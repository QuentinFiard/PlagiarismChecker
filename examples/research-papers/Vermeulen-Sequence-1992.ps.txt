

Sequence Semantics for Dynamic Predicate Logic

C.F.M. Vermeulen

January 1991
Revised July 1992

Abstract
In this paper a semantics for dynamic predicate logic is developed that uses sequence
valued assignments. This semantics is compared with the usual relational semantics for
dynamic predicate logic: it is shown that the most important intuitions of the usual semantics are preserved. Then it is shown that the refined semantics reflects out intuitions
about information growth. Some other issues in dynamic semantics are formulated and
discussed in terms of the new sequence semantics.1

1 Introduction
In Groenendijk and Stokhof[1991a] a relational semantics for predicate logic is developed.
The resulting Dynamic Predicate Logic (DPL) provides a synthesis of the insights of Discourse Representation Theory (DRT ) and the elegant formalism of predicate logic. The
value of such a synthesis is generally recognized, but at the same time some questions have
been raised. Some people have objected that the ideas of DRT are not represented correctly
in DP L. We do not intend to deal with their criticism in this paper. But also some properties of the formalism have been discovered that suggest serious problems for DP L. These
problems will be the topic of this paper.

The points that we want to discuss were raised by Groenendijk and Stokhof in a paper
presented at the JELIA-meeting [1991b]. There they compare the relational semantics of
DP L with the update semantics of Veltman[1991]. Veltman provides a dynamic theory
of modalities. He specifies the meaning of a modal (propositional) language in terms of
operations on information states. In this approach the meaning of a formula is its potential
to update information states. Groenendijk and Stokhof reformulate the semantics of DP L
as an update semantics to make a comparison with Veltman's system possible. Their update
formulation has the following (defining) property.

Definition 1.1 Let ASS be the set of assignments. Let oe 2 ""(ASS) be an information
state. Let [[OE]]gs2 ASS \Theta  ASS be the interpretation of OE as a relation on assignments. Then
([OE])gs, the interpretation of OE as an update function, is defined by the following property:

oe([OE])gs = fg 2 ASS : 9f 2 oef [[OE]]gsgg
1In this paper some familiarity with the ideas and techniques of dynamic predicate logic is presupposed. We think that all that we presuppose can be found in Dynamic Predicate Logic, by Groenendijk
and Stokhof[1991a]. The topics in dynamic logic that we discuss will be properly introduced, so no prior
knowledge of them is required. The topics are not new however: they can be found for example in Veltman[1991], van Benthem[1989] and in Groenendijk and Stokhof[1991b].

1

It is this formulation of the semantics of DP L that is the basis for the comparison with
Veltman's update semantics. So the properties of the semantics are discussed in terms of
properties of the update functions. The following observations are made:

Proposition 1.2 ffl DP L-updates are distributive, i.e. we always have2

oe([OE])gs = Sf ff g([OE])gs : f 2 oeg.

ffl DP L-updates are not in general eliminative, i.e. for some OE and oe not: oe([OE])gs ` oe:

In Groenendijk and Stokhof's paper these two properties are the basis for the comparison
with Veltman's system. It is pointed out that:3

ffl Veltman's updates are not in general distributive, i.e. for some OE and oe not: oe([OE])v =S

f ff g([OE])v : f 2 oeg.

ffl Veltman's updates are eliminative, i.e. we always have oe([OE])v ` oe:

Groenendijk and Stokhof conclude that these are genuine differences between Veltman's
update semantics and their own dynamic semantics, that will have to be taken into account
when an attempt is made to incorporate a dynamic treatment of modalities along the lines
of Veltman in DP L. But they do not seem to be worried about the fact that their semantics
behaves as it does: non-eliminatively and distributively.

We have a different view on the meaning of these observations. The property of distributivity allows us to compute the result of a function on a set pointwise. For our update
functions this means that the effects of the formula on an information state can be computed "locally". We can compute the result of an update on the whole information state
by just looking at its elements. It is convenient that this holds for the operations in the
DP L-semantics, but it cannot be expected to hold for all extensions. There seems to be no
intuitively compelling reason why sentence meanings should be computable pointwise. The
dynamic modality that Veltman considers typically is a case where one would not expect
this. (See also section 5:2.)

For the eliminativity property things are somewhat different. In update semantics the
ordering ' on information states corresponds to growth of information. The eliminative
functions are the functions that are monotone along this order. Hence if an operation
on information states is eliminative this means that there is an increase of information.
And if an operation is not eliminative this means that somehow we have lost some of the
information that we used to have. Therefore eliminativity is a property that we always
expect if we process information, as long as typically non-monotonic features are kept out of
consideration. DP L is not concerned with typically non-monotonic aspects of information
processing: DP L is used for the representation of simple narrative sentences. Therefore we
would not only expect eliminativity for Veltman's system, but also -- and perhaps even
more so -- for DP L.

2The notation that we use here is not the one used by Groenendijk and Stokhof or Veltman, but we hope
that no confusion will arise. The subscripts v and gs are used to distinguish Veltman's update functions
from the DP L-updates. We will use [[.]]-brackets for relational interpretations and ([:])- brackets for update
interpretations throughout the paper.

3We do not need the definition of Veltman[1991]'s update semantics in this paper. We just give the

properties of his system to illustrate the differences that Groenendijk and Stokhof[1991b] have found.

2

Thereby for us the non-distributivity of Veltman's system simply is a fact of life, while
the fact that DP L-updates are not monotone is a problem that has to be solved.

This is so for the intuitive reason expressed above, but it is also essential for a different
reason. If we accept non-monotonicity in the formal semantics, while intuitively information
grows, then we seem to accept a situation in which it is not possible to talk about information
in the DP L-formalism. This would be a very unfortunate situation. In fact, if DP Lsemantics is not about information, then it is no longer clear what it is that DP L-semantics
is about. This means that we do not know what to do with the formal system. Which
properties do we have to check, what does it mean if some property does or does not hold?
If we don't know what it is we are working on, then we don't know what to expect.

So we cannot afford to accept the non-monotonicity of DP L-semantics. We will propose
a slight modification of the semantics (section 2), called sequence semantics for dynamic
predicate logic. We will show that in the modified semantics the spirit of the usual relational
interpretation is preserved (section 2:3). The modified semantics will give rise to an improved
notion of information state (section 3) and will allow for a formulation as monotone update
semantics (section 5).

The new notion of information state will be explored further (in section 4) and we will
use it to shed a new light on familiar topics in dynamic semantics. First we discuss the
notion of monotonicity that comes with the new notion of information state, then we attack
some of the problems that the DP L-notion of inference gives rise to. Finally we will discuss
a new option that becomes available in the sequence approach: down-dating (section 5:3).

It turns out that not all problems are solved simply by using sequence semantics. But
the fact that some of the problems that seemed inevitable in the old formulation can be
solved easily in the sequence formulation, suggests that there is hope for dynamic semantics.
It encourages us to keep looking for better formalizations of the ideas of DP L until we have
a system that allows us to discuss all the important issues in formal semantics properly.
Then dynamic semantics will have become a serious alternative for any other formal system.

2 Sequence semantics
2.1 The language of dynamic predicate logic
In this section we present the language DP L. Usually in dynamic predicate logic, the
language of (standard) predicate logic is used. We prefer to work in a slightly different
language, which is very similar to the language of predicate logic but reflects the basic
intuitions of dynamic semantics better. This language, DP L, is defined inductively as
follows:

Definition 2.1 Let V AR be a countable set of discourse markers, P REDn the set of n-ary
predicate symbols (n * 0).

AT OM , the set of atomic formulas of DP L, is the smallest set such that:

1. ? 2 AT OM ;
2. for each n * 0, each P 2 P REDn, x1; : : : ; xn 2 V AR, P (x1; : : : ; xn) 2 AT OM ;
3. for each x 2 V AR we have 9x 2 AT OM .

DP L, the language of dynamic predicate logic, is the smallest set such that:

3

1. AT OM ` DP L;
2. OE 2 DP L ^  2 DP L ) OE: 2 DP L;
3. OE 2 DP L ^  2 DP L ) (OE ! ) 2 DP L;

Examples of DP L-formulas are: 9x:9y, P (x; y; z), 9x:P (x):Q(x; x), (9y ! P (y)). We
have conjunction (.) and implication (!) as logical connectives. Negation (:) is defined as:
:(OE) j (OE ! ?).

As one can see, we prefer to treat the existential quantifier as an atomic formula. One
reason for this is methodological: since 9x has a clear interpretation as a relation in DPL,
there is no reason not to regard it as a distinct syntactic unit. We prefer to have an
independent representation on the syntactic level of everything that plays an independent
role in the semantics. Another reason is that this way DP L-formulas look more like DRS0s
which makes a comparison of DP L and DRT easier. To us the language DP L seems to be
a medium in which both DP L and DRT could be studied.

Universal quantification can be defined as follows: 8x(OE) j (9x ! OE). The possibility of
defining universal quantification in this way, is another advantage of DP L over the language
of predicate logic. This definition of 8 clearly reflects the intuition that we have about the
meaning of 8: 8x(OE) simply means that for any x we could come up with, OE will hold.4

It should be noted that our definition of the . introduces a structural ambiguity. The
formula OE::O/ can be constructed in two ways: either as the conjunction of OE with :O/ or
as the conjunction of OE: and O/. We will see to it that this does not cause problems for the
interpretation of .-formulas (conjunctions): both "bracketings" will have the same effect in
the semantics. We think that this kind of (harmless) ambiguity is also present in natural
language. Also when we are considering, say, three sentences, it is not clear whether this is
the result of adding the third sentence with the first two, or whether the conjunction of the
last two sentences has been added to the first sentece. Therefore we feel no need to change
the formal definition.

Working in this language also makes some proofs a bit easier. But it can be checked that
the results that we present do not depend on the choice of DP L instead of the language of
predicate logic.

2.2 The refined relational semantics
The refined version of dynamic semantics that we present is very similar to the usual relational semantics. The inductive definition of the relations that are the interpretations of the
formulas of DP L is almost identical that of Groenendijk and Stokhof [1991a]. The refinement that we propose is obtained by using a richer notion of assignment. Instead of working
with assignments that assign one value in the domain to each discourse referent, we use assignments that assign to each discourse marker a sequence of values. In the usual semantics
we are forced to forget the current value of the variable x if we encounter the formula 9x.
If we consider, for example, the formula P (x):9x:Q(x):9x:R(x), a pair of assignments (f; g)
that is in the relational interpretation [[P (x):9x:Q(x):9x:R(x)]]gs will only reveal a value
f (x) that is in the interpretation of P , and a value g(x) that is in the interpretation of R,
but we have no way of remembering a value in the domain that is in the interpretation of
Q.

4This is also the intuition that is behind the proof rule for 8I in natural deduction for example.

4

We think that this is where the usual semantics goes wrong. If we throw away values
of some variable, then we lose track of the restrictions on the values of this variable. These
restrictions on the value of a variable are the way in which information gets represented
in DP L-semantics. And once we have thrown away this information, there is no way of
recovering it.

If we work with assignments that have sequences as values, we can see to it that the value
that is in Q is remembered. For example, if we find a pair (k; h) in the refined relational
interpretation [[P (x):9x:Q(x):9x:R(x)]], such that k(v) = h: : : ; pi and h(v) = h: : : ; p; q; ri:
this tells us that p 2 I(P ), q 2 I(Q) and r 2 I(R). This way no information is lost. We will
call these assignments that have sequences as values, sequence valued assignments.

Definition 2.2 1. SASS, the set of sequence valued assignments, is the set that consists

of all functions f : V AR ! DOM \Lambda . (Here DOM is the domain of interpretation, and
DOM \Lambda  = SfDOM n : n * 0g.)

2. For a sequence hd1; : : : ; dni 2 DOM \Lambda  we call end(hd1; : : : ; dni) = dn.
3. For a sequence hd1; : : : ; dni 2 DOM \Lambda  we call pd(hd1; : : : ; dni) = hd1; : : : ; dn\Gamma 1i.
4. For two sequences hd1; : : : ; dni, he1; : : : ; emi 2 DOM \Lambda , we define their concatenation:

hd1; : : :; dni \Lambda  he1; : : : ; emi = hd1; : : : ; dn; e1; : : :; emi.

It should be noted that the last component of the sequence f (x), end(f (x)), is the
current value of x. We also use the notation pd(f (x)), the predecessor of f (x), for the
sequence that is left if we remove end(f (x)). So we have f (x) = pd(f (x)) \Lambda  hend(f (x))i. If
the empty sequence is assigned to x (f (x) = hi), then we call f (x) undefined and we say
end(f (x)) = hi. The fact that we allow the empty sequence as a value introduces some form
of partiality into our semantics. Therefore we sometimes call the elements of SASS partial,
sequence valued assignments.

It is also possible to use partial assignments in the usual relational semantics. Instead
of working with total assignments, Groenendijk and Stokhof could have used partial assignments f : V AR ,! DOM . (Here the partiality is indicated by the ,!.) Let's call the set of
partial assignments P ASS.5

In such an approach new questions concerning partiality have to be answered. For
example, if we want to define the truth of a formula, OE say, in such an approach, we have
two options:

OE is true iff for any partial assignment f , there is a partial assignment g such
that...;

or

OE is true iff for any partial assignment f defined on the free variables of OE, there
is a partial assignment g... .

If we always choose for the second kind of answer, this gives us a semantics virtually equivalent to the usual relational semantics. We do not give the details of this approach: it
requires but a straightforward adaptation of the usual semantics. We will use the notation

5Note that ASS ` P ASS ` SASS.

5

[[OE]]pgs to refer to this version of the relational semantics.6 Following the first option will
give us something quite different. It might be interesting to see what happens in that kind
of semantics, but we will not do that here.

Note that the use of partial assignments instead of total assignments is a step in the
right direction: if we use partial assignments, we only have to consider the variables that
actually occur in the formula. This avoids confusion about the role that the values of other
variables play. We can see about which variables the formula actually gives information.

We can also extract this kind of information from the interpretation with total assignments. For example, if in the interpretation of a formula OE the value of x is restricted,
then it is clear that x occurs in OE. And also if in the interpretation of OE the value of x can
change, we can conclude that x occurs in OE. But the total assignments do not reveal all the
information about the occurrence of variables.

For example, if OE = (P (x) ! P (x)), then OE is a statement about x. We admit that
the information that OE gives about x is trivial, but still OE is a statement about x and not
about any other variable. In the interpretation with total assignments we cannot see this:
[[OE]]gs simply contains all pairs (f; f ). But if we use partial assignments then this information
becomes available in the semantics: if (f; f ) 2 [[OE]]pgs, then f will have to be defined on x,
but not necessarily on any other variable. So now we can see from the interpretation of OE
that x occurs in OE. In other words, if we use partial assignments the interpretation can tell
us about which variables formulas make a statement.7

But clearly the use of partial assignments is not enough to get a good representation of
all the information a formula can contain: we do not only want to make a difference between
the variables that occur and the variables that do not occur, but about the variables that
occur, we also want to know in how many different roles they occur.8

Now before we give the refined version of the relational semantics, where this information
is present, we introduce an important technical notion: we define what it means for one
(sequence valued) assignment to be a x-variant of another (sequence valued) assignment.

Definition 2.3 Let x 2 V AR, V 2 V AR\Lambda , f; g 2 SASS be given.

1. We say that g is a x-variant of f ,

f hhxiig iff 9d 2 DOM : g(x) = f (x) \Lambda  hdi ^ 8y : (y 6= x ! f (y) = g(y)).
2. We define the notion V -variant for sequences of variables V as follows:

f hh hxi iig , f hhxiig; f hh V \Lambda  hxi iig , 9h 2 SASS : f hh V iih ^ hhhxiig:
The relation hhxii allows us to choose a new value for x. We do not throw away the
old value: we build up a sequence of values instead. Note that the x-variant relation is not
symmetric. In fact we have f hhxiig ! : ghhxiif . Another important property of these
relations is that hhV ii is the same as hhV 0ii whenever V 0 is a permutation of V . So we have,
for example, hh hx; y; xi ii = hh hy; x; xi ii.

Now we will define the refined relational semantics.
6See for example Kamp and Reyle[1990], Dekker[1992] or Fernando[1991] for a presentation of semantics

with partial assignments. Kamp and Reyle [1990] use partial assignments for the semantics of DRT .

7Here the analogy between formulas and computer programs might be helpful: even if you want to run a

trivial program, you have to make sure all variables that actually occur are properly declared.

8In Dekker[1992] we see that even in the presence of partial functions strong syntactic restrictions are

required to obtain eliminativity. In Dekker's approach all formulas in which a variable occurs in more than
one role (such as 9x:P (x):9x:Q(x)) are regarded as meaningless.

6

Definition 2.4 1. For each OE 2 DP L we define [[OE]]` SASS \Theta  SASS, the meaning

of OE as a relation on sequence valued assignments. Let f; g; h; k be sequence valued
assignments, I an interpretation function for the predicates of DP L.

f [[P (x1; : : : ; xn)]]g , f = g ^ (end(f (x1); : : : ; end(f (xn))) 2 I(P )
f [[9x]]g , f hhxiig
f [[OE:]] , 9h f [[OE]]h[[]]g
f [[(OE ! )]]g , f = g ^ 8h : f [[OE]]h ) 9k : h[[]]k

2. We define valid inference as follows: let OE; OE1; : : : ; OEn;  2 DP L. Then:

OE j=[[  , 8f; g : f [[OE]]g ) 9h : g[[]]h

and
OE1; : : :; OEn j=[[  , OE1: : : : :OEn j=[[ :

The definition is very similar to the original definition by Groenendijk and Stokhof.
The main difference is the kind of assignments that is used. This has some interesting
consequences. Note for example that in the definition of [[OE:]] the h such that f [[OE]]h[[]]g is
uniquely determined by f; g.

Now if we look at [[OE]], we can find almost all the information that is revealed by OE: if
we look carefully at the variables on which pairs (f; g) that are in [[OE]] are defined, then we
can find out which variables are free in OE, which variables occur quantified and how often
a variable is quantified over. For example, if we consider [[P (x):9x:Q(x)]], we will see that
for all pairs (f; g) that are in this relation, we have that both f and g are defined on x (i.e.
f (x) 6= hi),which means that x must occur in the formula; the fact that f will always be
defined on x, means that there is a free x in the formula. We will see that the length of the
sequence g(x) is at least two, which means that x occurs in the formula in two (possibly
different) roles. Since x already occurs freely, we can infer from this that there must be
exactly one occurrence of 9x in the formula. We will also see that for any other variable y,
there is a pair (f; g) where f (y) = g(y) = hi, indicating that y does not occur in our formula.

It is also possible to make this information explicit in the relational semantics. If we add
two components to our indices, we can build up the set of the free variables of a formula
and the sequence of the quantified variables quite easily. Then [[.]] would become a relation
on triples (A; S; f ) where A contains the free variables and S gives the information about
the quantified variables and f 2 SASS. We will not make this precise here, but the reader
can see how it could be done in the static semantics that we will present later.

2.3 A comparison
In this section we compare the refined relational semantics with the usual relational semantics. We will show that for any OE the partial relational interpretation, [[OE]]pgs, can be
constructed out of the relational interpretation [[OE]]. This way we also establish a relation
between [[OE]] and [[OE]]gs, since [[OE]]gs is just the restriction of [[OE]]pgs to total assignments.

We establish the relation between [[OE]] and [[OE]]pgs with a mapping \Phi  that is defined as
follows:

Definition 2.5 We define \Phi  : ""(SASS \Theta  SASS) ! ""(P ASS \Theta  P ASS) as follows: \Phi (R) =
f(g; f ) : 9(k; h) 2 R : 8x 2 V AR : f (x) = end(h(x)) ^ g(x) = end(k(x))g.

7

As one can see, we construct a relation between partial functions out of a relation between
sequence valued functions by restricting our attention to the current values of the sequence
valued functions. The result of such a restriction is in general not a total function since we
allow the value hi for sequence valued functions.9

Now our claim about \Phi  is the following:

Proposition 2.6 For all OE 2 DP L we have \Phi ([[OE]]) =[[OE]]pgs.

The proof of proposition 2:6 is an induction on the complexity of OE. It can be found in
the appendix.

The relation that \Phi  establishes does not enable us to construct [[OE]] from [[OE]]pgs for all
OE: it is a homomorphism rather than a isomorphism. As an example of this we consider
the formula OE = 9x:P (x):9x:Q(x). If we know [[OE]]pgs, we only can see which values in the
domain are such that they have property I(Q) (by checking which values of x can occur as
f (x) in a pair (g; f ) 2 [[OE]]pgs). If we want to have [[OE]], we also have to know which part of
the domain has property I(P ).

It is possible, however, to define the relation [[:]] in terms of the relation [[:]]pgs. In other
words: if we know [[OE]]pgs for all OE, then we can give [[OE]] for all OE. As an example again consider
9x:P (x):9x:Q(x). Knowing [[9x:P (x):9x:Q(x)]]gs does not suffice to find [[9x:P (x):9x:Q(x)]].
But if we know both [[9x:P (x)]]gs and [[9x:Q(x)]]gs then we do have enough information to
construct [[9x:P (x):9x:Q(x)]]. In general, if we know how OE decomposes into conjuncts,
then we can construct the sequence valued interpretation of OE from the sequence valued
interpretation of the conjunctions. This weaker correspondence suffices to guarantee that
the notions of validity for [[.]], [[.]]pgs and [[.]]gs are equivalent. Therefore we have but one
notion of validity, in spite of the subtle differences between the different semantics for DP L.
First we give the definitions of valid inference for [[.]]gs and [[.]]pgs.

Definition 2.7 For OE;  2 DP L we define:

1. OE j=gs  , 8f; g 2 ASS : f [[OE]]gsg ! 9h 2 ASS : g[[]]gsh;
2. OE j=pgs  , 8f; g 2 P ASS :

f [[OE]]pgsg and g is defined on the free variables of  ! 9h 2 P ASS : g[[]]pgsh.

All definitions of OE j=x  are based on the same idea: if we are able to make a OE-step,
then we are always able to make a -step. Now our claim is that also formally all notions
of valid inference coincide, i.e. we claim:

Corollary 2.8 For any OE;  the following clauses are equivalent:

1. OE j=[[ ;
2. OE j=pgs ;
3. OE j=gs .
9In this context we mean by f(x) = end(hi) that f(x) is undefined.

8

The proof can be found in the appendix.
In the proof 2 ) 1 we need more than just [[]]pgsto be able to construct the required
sequence valued assignment h: we needed all the [[i]]pgs (where the i are the formulas of
which  is the conjunction). This confirms our remark above about the relation between
[[]] and [[]]pgs not being an isomorphism.

By now we think that we have given sufficient proof of the fact that in our refinement
of the relational semantics the spirit of the Groenendijk and Stokhof semantics is preserved:
we can reconstruct the original relations from our refined relational semantics and we have
preserved the original notion of entailment.

3 Static semantics
3.1 Information structures
In this section we will introduce the notion of an information structure. Then we will
interpret DP L-formulas as information structures and this will be our static semantics of
DP L. We will show that this interpretation is compatible with the relational interpretation
of the preceding section: we will be able to construct the relational interpretation from the
static interpretation and we will see that the information structure that we associate with a
formula is just the set of possible outputs of the relation [[.]].

At first sight the fact that dynamic DP L allows for a static semantics may seem odd.
Does this mean that dynamic semantics is static after all?10 It might be helpful to think
about the situation in terms of representation or coding. The information structures that
we will define can be used to represent (or code) the relations that we have defined above.
Not all relations on SASS will have such a representation, but it turns out that all DP Linterpretations can be represented by some information structure. We already have a similar
situation in the original formulation of the DP L-semantics. There conditions, such as P (x),
can be represented by a set, the set of assignments that satisfy the condition. Given this set
the relational interpretation of the condition can be constructed. For example, P (x) can be
represented by the set P= ff 2 ASS : f (x) 2 I(P )g. Then we can define [[P (x)]]gs= f(f; g) :
f = g ^ g 2 Pg. But for other formulas such a representation as a set is not available.

With the new definition of the relational semantics and the new notion of information
structure a similar reconstruction will be possible for all DP L-interpretations. But still the
dynamic perspective is useful. First because it is the relational formulation that has led
to our definition of information structure in the first place. And second because there are
still interesting notions in dynamic semantics that cannot be represented statically, even by
the new information structures. Here one can think, for example, of Veltman's dynamic
modalities (also see section 5:2).

What is the information revealed by a formula? The answer to this question will motivate
our notion of information structure. Surely, a formula gives information about the discourse
markers that occur in the formula. We represent this by considering all the values that
such a discourse marker can take, i.e. our information structures will contain assignments of
values to the discourse markers that occur in the formula. The range of values of a discourse
marker is restricted by the conditions that we find in the formula. So we will not find all
assignments in the information structure, but only those that satisfy the restrictions that
are expressed by the formula.

10I would like to thank an anonymous referee for pointing out this source of confusion.

9

Some of these discourse markers are introduced explicitly in the formula: they occur in
the scope of a quantifier. For these variables a value gets set whenever we interpret the
formula. But a formula also might say something about discourse markers that it does not
introduce itself: a formula might contain free variables. It would be unreasonable to treat
these free variables on a par with the quantified variables. The formula does not instruct
us to assign a value to them, it simply assumes that we have done this already: a formula
asks for a value on its free variables. The information about the free variables that the
formula gives specifies further which value we should have assigned to them. We include
this information into our information structures by restricting the assignments according
to these conditions. To make sure that we do not lose sight of the different status that
the restrictions on free variables have, we include in our structures components that tell us
which variables occur freely and which variables are quantified over in the formula.

If the set of assignments that we have in our information structures really has to give
some information about the discourse markers of the formula, it has to have, in some sense,
this set of discourse markers as its "domain". It is not yet clearly defined what it means
for an assignment to have that property. To make the notion precise we give the following
definitions.

Definition 3.1 1. With each sequence S 2 V AR\Lambda  we associate a partial function ffS :

N ! V AR as follows:

ffhxi(0) = x;
ffhxi(n + 1) = undefined for n 2 N ;
ffhxi\Lambda S(0) = x;
ffhxi\Lambda S(n + 1) = ffS(n) f or n 2 N .

2. We define the length of a sequence S as follows:

length(S) = maxfn 2 N : ffS(n) is def inedg + 1.
3. For each sequence S 2 V AR\Lambda  we define what it means for a sequence valued assignment

f 2 SASS to be defined on S.

f is defined on hxi iff length(f (x)) * 1;
f is defined on S \Lambda  hxi iff f is defined on S and

length(f (x)) ? minflength(g(x)) : g is def ined on Sg.

4. For a set V ` V AR we write seq(V ) for the sequence made out of the elements of

V . (Here we use some standard enumeration of V AR to order the elements of V in
seq(V ). Such an enumeration is available since V AR is countable.)

5. We define the relation ^l on assignments in SASS:

Let f; g 2 SASS. We say f ^l g iff 8x : 9oex : oex \Lambda  f (x) = g(x).
6. If f ^l g, then we say that f is a restriction of g.

In definition 3.1.1 we associate a function defined on an initial segment of N with each
sequence. The function ffS gives for n 2 N the nth element of the sequence S. The

10

relation of such a function to the sequence is the same as the relation of a characteristic
function to a set. They are interchangeable and we only distinguish them notationally to
avoid confusion.11 Note how ffS can be used to give a concise definition of the length of a
sequence (3.1.2).

The other clauses are concerned with making clear how we check whether a sequence S
is in the domain of a sequence valued function f . Definition 3:1:5 makes precise a notion
of the extension of a sequence valued assignment. If f ^l g, then g(x) may be longer than
f (x), but the extra values of g(x) are added in front. We also call such an extension an
irrelevant or left extension. If g extends f to the left, we have for each x that f (x) and g(x)
agree on the relevant values, i.e. the last few values.12

As an example, consider a model where I(P ) = fag and an assignment f with f (x) = hai.
Then f satisfies the condition P (x): the current value of x is in I(P ). If we extend f to
the left to g such that g(x) = hb; c; d; e; b; d; ai, then for the formula P (x) this extension of
the value of x is irrelevant. g is as good as f even though the new values on x are not in
I(P ). The current value of x still is a, so g also satisfies the condition. We can cut of an
irrelevant part of g(x) to get for example g0(x) = hb; d; ai. Usually we will only be interested
in the values of a sequence valued assignment on the variables that actually occur in some
formula. For example, in the case of P (x) we are only interested in the current value of x,
in the case of P (x):9x:Q(x) we want to know the last two values of x, etc.

This leads to the following definition of information structures:

Definition 3.2 IN F O ` ""(V AR) \Theta  V AR\Lambda  \Theta  ""(SASS) such that (A; S; F ) 2 IN F O iff

1. f 2 F ) f is defined on seq(A) \Lambda  S;
2. f 2 F ^ f ^l g ) g 2 F ;
3. f 2 F ^ g ^l f ^ g is defined on seq(A) \Lambda  S ) g 2 F .
(Here A is for asking, S is for setting and F is for function.)

In an information structure (A; S; F ) we want the assignments in F to contain information about A and S. This means that all the f 2 F have to be defined on both A and
S: seq(A) \Lambda  S is the minimal domain for the f 2 F (condition 1). It also means that the
values outside this minimal domain should not matter. Hence the f 2 F should at least be
defined on A and S, but any extension outside this minimal domain should be allowed. Furthermore the values outside the minimal domain should not be restricted: value restrictions
code information and we only want information about A and S, not about other variables.
Conditions 2 and 3 have this effect: given an f 2 F defined on the minimal domain, 2 says
that all its (left-)extensions also are in F and 3 says that if g 2 F , then also all restrictions
of g are in F , as long as they are defined on seq(A) \Lambda  S.

The sequence S will give the exact order of all the quantifiers in a formula. These are the
variables for which the formula sets a value. Since we will only be interested in the number
of times a quantifier 9x occurs in a formula -- and not in the question whether 9x occurs
before 9y or after -- we do not really need a sequence as a second component: a multiset
gives all the information we want. But using multisets would complicate things technically:

11Sometimes finite sequences are simply defined to be functions defined on initial segments of N.
12In the appendix we give a lemma that says exactly what we mean when we say that left extensions are

irrelevant.

11

if we work at the same time with sets (in the first component), multisets (in the second
component) and sequences (from the third component), we would have to introduce even
more auxiliary definitions and technical tricks. Already the use of both sets and sequences
has forced us to introduce more auxiliary techniques than we would like: the notion of the
range of a sequence (range(ffS)) and a function seq to make sequences out of sets.

So we allow ourselves some extra structure in the second component, but this is for
convenience only.13

The set A will contain the variables that occur freely in the formula. A formula cannot
by itself set these variables to the right value: it has to take over the value that the context
provides. One could say that the formula -- and therefore also the information structure --
asks the context for a value for such a variable.

Given an information structure (A; S; F ) it is possible to reconstruct seq(A) \Lambda  S from
F :14 we just have to find the minimal domain of the f 2 F . Therefore it is possible, given
F and S to find A and given F and A to find S.15 This suggests that we only need two and
not three components in an information structure. But that is not the case: it is impossible
to define the domain conditions on the F in an information structure and distinguish the
status of the A-like and the S-like part of the minimal domain without actually mentioning
all three components. It is true that -- once the definition is given -- the last two of the
three components suffice to find the first.

Our notion of information structure is motivated solely by considering formulas in DP L.
Therefore it might seem rather artificial. To give some more motivation we want to point
out the similarity to discourse representation structures (DRSs). A DRS also consists of a
component of discourse markers and a set of assignments that is to express the restrictions on
these discourse markers that we find in discourse. The set A that we have in our information
structures can be compared to the set of anchored discourse markers.16 In DRT these
anchors are mainly used for deictic expressions. This is also something that we can use the
A-variables for.

For further motivation for the presence of this third component in our information structures we compare our situation with the practice of computer programming. In a program
we are not allowed to work with undefined variable names. Nevertheless names can occur
in a program that are not declared in the program itself. We call these names constants,
but this does not mean that they never have to be defined. Instead of being defined in the
program, a constant can be said to be defined in the program environment. Such a situation
can also occur in discourse theory, where we may want to study a discourse fragment and
not the whole of discourse. We then assume some discourse environment in which the free
variables are defined. Free variables could stand for proper names, for example. We simply
assume that proper names have been introduced properly. We would not want to be forced
to introduce them in every bit of discourse in which we want to use them; they are declared
in the discourse environment.

It is also possible that we are simply not able to introduce the free variables properly. Such
a situation arises, for example, if one overhears part of a conversation. Suppose there is a
discussion going on between two people about a third person. To the two conversationalists
it probably is clear who this third person is. Therefore he (or she) will probably not be

13We do not mean that it is really difficult to use multisets, but we feel it would only confuse the issue.
14Up to permutation!
15Again, up to permutation.
16An anonymous referee pointed out this analogy of the A-variables and anchors.

12

represented as a free variable in their representation of the discourse. But if we miss the
introduction of this discourse marker, overhearing only a part of the conversation, we will
be forced to represent this person as a free variable. We will assign all the properties that
are assigned to this person to our free variable. But our understanding of the discourse
is incomplete until we find out who it was that they were talking about; we only have a
partial understanding of that piece of discourse, because our discourse environment is not
rich enough.

3.2 Static interpretation
Now we will define the interpretation of DP L-formulas as information structures. We will
assign an element of INFO to each formula OE, that we will call (AOE; SOE; FOE):

Definition 3.3 1. For each DP L-formula OE, we define the static interpretation of OE, [OE],

inductively as follows:

[P (x)] = (fxg; hi; ff : end(f (x)) 2 I(P )g)
[9x] = (;; hxi; ff : 9g : ghhxiif g)
[OE:] = (AOE [ (Anrange(SOE)); SOE \Lambda  S; ff 2 F : 9g 2 FOE : ghhSiif g)
[(OE ! )] = (AOE [ (Anrange(SOE)); hi; ff : 8g 2 FOE : f hhSOEiig ! 9h 2 FghhSiihg)

2. We define valid inference as follows: OE j=[  , 8g 2 FOE 9h 2 F : ghhSiih

The first component of [OE] is the set of the free variables in OE.17 In the second component
of [OE] we build up the sequence of the quantified variables. That allows us to keep track of
the order in which variables are (re)introduced.18

Example:
1. [P (z):9x] = (fzg; hxi; : : :);
2. [Q(x; y):9x:R(y; x)] = (fx; yg; hxi; : : :);
3. [P (z):9x:Q(x; y):9x:R(y; x)] = (fzg[(fx; ygnfxg); hxi\Lambda hxi; : : :) = (fz; yg; hx; xi; : : :).

Here 3 is the conjuction of 1 and 2. We see that the x that is free in 2, is not free in the
conjunction; it is bound by the 9x in 1. Therefore is has to be removed from the set of free
variables. Note that a variable can be free only once, while it can occur many times in the
sequence of the quantified variables.

The assignments in the third component will all be defined on the free and the quantified
variables. It is also easy to check that the third components of the interpretations satisfy the
other conditions in the definition of IN F O. This means that [OE] 2 IN F O for any OE 2 DP L.

There is an obvious correspondence between this static semantics and the refined relational semantics of the preceding section:

17We have compared the variables in A with anchors, but while free variables can get bound in larger
contexts, anchors will always remain anchors. So it would be better to consider the A-variables as temporary
anchors.

18Note that here [9x:P (x):9y:P (y)] 6= [9y:P (y):9x:P (x)]. If we use multisets these two information structures become equal.

13

Proposition 3.4 Let [OE] = (AOE; SOE; FOE). Then:

1. f [[OE]]g , g 2 FOE ^ f hhSOEiig.
2. OE j=[  , OE j=[[ .

(The proof is omitted.) If we had used a version of [[.]] that has three components (cf.
p.7), this would mean that: [OE] = range([[OE]]) (or "[[OE]] in the notation of Groenendijk and
Stokhof[1991b]). The proposition has the following corollary:

Corollary 3.5 For any OE;  the following clauses are equivalent:

1. OE j=[[ ;
2. OE j=[ ;
3. OE j=pgs ;
4. OE j=gs ;

Proposition 3:4 and corollary 3:5 guarantee that our interpretation of formulas in IN F O
preserves the spirit of dynamic semantics. This means that a relational formulation is not
essential for the dynamic semantics of DP L. This may seem surprising at first, but maybe
not if we recall that already in DRT we have a kind of static semantics that covers almost
the same evidence as DP L.

Also recall our discussion in 3:1 where it is pointed out that for certain extensions of
dynamic semantics a relational formulation will be essential.19

4 Topics in dynamic semantics
4.1 The general point
Now that we have an improved notion of information, we want to discuss some topics
in dynamic semantics in terms of it. One of the issues we will discuss is monotonicity.
Remember that we said (section 2:2) that probably the defect of the original formulation of DP L was that we were sometimes forced to forget the value of a variable. If we
consider [[P (x):9x:Q(x):9x:R(x)]]gs, we will only find the p such that p 2 I(P ) and the
r 2 I(R), but no q 2 I(Q). This means that if there is any q 2 I(Q), we would have
[[P (x):9x:R(x)]]gs=[[P (x):9x:Q(x):9x:R(x)]]gs. This makes it impossible to have an intuitively acceptable notion of information based on these relations. Indeed, it makes it impossible to answer questions about information by looking at the semantics.

In our set up it is possible to define an ordering of the semantic objects, information
structures, that corresponds to our basic intuitions about the informativity of the formulas.
We will define this ordering and show how it works. This is the ordering along which we
want to check the monotonicity of our semantics.

Then we will try to use our ordering for the study of dynamic inference. This will cause
some problems that suggest that our ordering is not "the right one", at least not for all
purposes.

19Also see section 5.

14

4.2 Ordering information structures
We will introduce an ordering of information structures, that will be motivated by the
intuition that more discourse contains more information. We will discuss the basic properties
of this ordering. There are basically two ways in which we can give more information in
discourse. First we can say more about the objects that we already were talking about.
We would then add restrictions on discourse markers, and as a consequence eliminate some
assignments. The other way in which we can add information, is by introducing a new object
in discourse.

It is difficult to find a piece of discourse that does just this. One could think of the
indefinite article 'a',20 but larger discourse fragments that only introduce and do not restrict
are hard to find. Usually as soon as we introduce some object we say something about it as
well. But maybe we can illustrate the two kinds of information be contrasting the following
two sentences.

Example:
1. There is a unicorn.
2. There is a man.

The first sentence clearly is informative in both senses. An object is introduced for us
to talk about and a very interesting claim about the object is made. From this sentence
we can infer that unicorns exist. Probably this is what the speaker wants to say with this
sentence.

From the second sentence we can infer that men exist. But probably this will not be
what the speaker is trying to tell us in this sentence. Here this claim is merely a side effect.
The main goal of the speaker probably is to introduce an object that he wants to talk about.
The first claim about it is already made -- that it is a man -- but, surely, more will follow.
Such introductory acts are essential in the chain of information exchange.

In other words, although both sentences are informative in both ways, the second sentence
mainly serves to introduce the object for further discussion. The first sentence also makes a
remarkable claim about the object it introduces.21

Other, more artificial examples can be found in in mathematics. In proving a theorem,
in arithmetic for example, a phrase such as

Let n be given.
is often used. Clearly the purpose of this phrase is just to make it possible to talk about
some number.

In DP L-syntax these two ways of giving information correspond, roughly, to two kinds
of formulas: the first kind of information is typically represented by DP L-conditions, and
the second kind goes together with the DP L-quantifier. We should be careful with this
correspondence: when we find a formula 9x in DP L, we are not sure that this x will really
stand for a new object, it might just be another name for an old object.22 Nevertheless, it
is the formula 9x that makes it possible to talk about new objects and therefore it seems

20Even for 'a' the situation is not so easy. Sometimes 'a' does more then just introduce an object. Think,
for example, of generic uses of the indefinite article.

21I want to thank the referees for helping me out with this example.

22Also see section 4:4.

15

reasonable to say that 9x contains positive information (in this sense). In our semantics the
two ways of giving information can be represented as follows:

Definition 4.1 We define an ordering * on IN F O. Let (A; S; F ), (A0; S0; F 0) 2 IN F O be
given. Then (A; S; F ) * (A0; S0; F 0) iff

1. S0 = S \Lambda  S00 for some S00;

2. A0 ` A;
3. 8f 0 2 F 0 : 9f 2 F : f hhS00iif 0.

We have defined * in such a way that the smaller information structures are more
informative. In this definition 1 represents (part of) the second idea about increase of
information: the more informative state defines all the discourse markers that are defined
in the larger state and maybe some more.

If we have a situation in which S00 = hi, the third clause reads as F 0 ` F . This is
the easiest way to see that the first idea is also reflected by the definition: if no more
discourse markers are introduced, giving more information simply means eliminating some
assignments. What 3 actually says is that every assignment in F 0 should be an extension
(to the right, so not irrelevant!) of an assignment in F . This corresponds to a more refined
notion of the elimination of assignments: we do not want to say that an assignment is
eliminated if it has "grown". An assignment is not eliminated if it gets a larger domain: it
can only be said to be eliminated if it does not reoccur at all, not even with a larger domain.
If we want to consider assignments as possibilities, it is this refined notion of elimination
that corresponds to the idea of eliminating a possibility. The sequence valued assignments
are like possible histories of information. Just as history will not have to be revised, because
of developments in the future, assignments are not eliminated if they become "longer". Now
we can see that 3 corresponds to this notion of elimination of possibilities: every f 0 2 F 0 is
an extension of an f 2 F , but some assignments f 2 F might not reoccur in F 0, they are
eliminated. Note that 2 usually follows from 1 and 3; the only exception is F = F 0 = ;.
(We have (A; S; ;) 2 IN F O for all A; S.) It would be interesting to understand what this
exception means. It seems to say that when a contradiction arises, i.e. F = ;, we have to
make a choice: either we say that all contradictions contain the same information or some
give different information from others. Here we have chosen the last option, not for any
intuitive reason but because it makes the formalism easier to handle.

Before we show what this way of ordering information structures means for our DP Linterpretations, we give some abstract properties of the ordering.

Proposition 4.2 * is a partial order, i.e.

1. (A; S; F ) * (A0; S0; F 0) ^ (A0; S0; F 0) * (A00; S00; F 00) ) (A; S; F ) * (A00; S00; F 00);

2. (A; S; F ) * (A0; S0; F 0) ^ (A0; S0; F 0) * (A; S; F ) , (A0; S0; F 0) = (A; S; F ).

Proof:
2 `(' is obvious. `)': Clearly the antecedent implies S = S0 and A = A0. Therefore we

find both F ` F 0 and F 0 ` F , i.e. F = F 0.

1 S00 = S0 \Lambda  S\Lambda  for some S\Lambda , and S0 = S \Lambda  Sffi for some Sffi. So S00 = S \Lambda  Sffi \Lambda  S\Lambda . Also, if

f 00 2 F 00, we have f 0 2 F 0 such that f 0hhS\Lambda iif 00, and for this f 0 we have f 2 F such
that f hhSffiiiF 0, and therefore f hhSffi \Lambda  S\Lambda iif 00.2

16

4.3 Monotonicity
In this section we show that our ordering of information structures works fine for the easiest
way of giving more information in discourse: we show that larger discourse fragments are
more (or: not less) informative in our sequence based interpretation. We call this property
of our semantics monotonicity. As was explained in the introduction it is our view that any
good semantics for ordinary, narrative discourse should have this property. It is simply true
that we do not lose information if we continue our story. Of course, this does not mean that
we never forget information, but forgetting is not the result of ordinary narrative discourse.
And it is this kind of discourse that we want to represent in DP L. So we want the following
property for our semantics:

Proposition 4.3 Let OE;  2 DP L be given. Then we have [OE] * [OE:].

Proof: [OE:] = (AOE [ (Anrange(SOE)); SOE \Lambda  S; ff 2 F : 9g 2 FOE : ghhSiif g),
[OE] = (AOE; SOE; FOE). Now the proposition holds by definition of *.2

At this point we can see in which way our semantics is an improvement of the usual
relational semantics for DP L. By using sequence valued assignments, we have enriched the
semantics in a natural way. As a result we get the possibility of a systematic and abstract
discussion of information in DP L-semantics. We now have semantic objects that are rich
enough to make such an approach possible. The definition of information structures and
our ordering of these structures are examples of these new possibilities. They have enabled
us to see that DP L-semantics is monotone.23 It is not claimed here that these two notions
are all we will ever need, but they show what kinds of things are possible in this richer
environment, using sequence valued assignments.

4.4 Inference
4.4.1 Dynamic inference in general
Inference is a notoriously difficult topic in dynamic semantics. Different branches of dynamic
semantics have given rise to different notions of inference and some have even produced more
than one. The DP L-notion of inference is defined as follows:

OE j=  , 8f; g : f [[OE]]gsg ! 9h : g[[]]gsh:
We have seen that this notion of inference is preserved in all the reformulations of DP Lsemantics that we have considered.

Update semantics has produced its own notions of inference. One of them reads as
follows24:

OE j=v  , 8oe : oe([OE])v ` oe([OE:])v
Both notions of inference make sense in the context in which they arise. So it seems that
dynamic semantics in general does not have one notion of inference. Instead we find a whole
spread of inference relations. Of course, this gives rise to the question what the common

23We will get back to this point in section 5.1.
24Notation as in introduction: v stands for Veltman.

17

features of these relations are. Or -- to put it ironically -- is there any relation that is not
a dynamic inference relation?

This turns out to be a surprisingly difficult question.25 But here we do not aim at solving
this general problem. Instead we restrict ourselves to the DP L-notion of inference.26 We
are especially interested in the way in which the inference relation fits into our algebra
of information structures. We know that for all sorts of formal systems there is a nice
fit between the algebraic semantics and the inference relation. For propositional logic j=
coincides with the ordering in Boolean (or, in the intuitionistic case, Heyting) algebra. For
predicate logic and modal (propositional) logic it is the inclusion relation on satisfaction
sets. For linear logic the situation is less straightforward: here j= can be defined in terms of
\Phi .27 Now what kind of relation holds between the DP L-inference relation and the ordering
that we have defined on IN F O?

4.4.2 Inference and information
The idea that there might be a connection between the inference relation of DP L and the
ordering on IN F O, is not just inspired by the fact that this is so for other formal systems.
There also is a clear intuition that inference and information are related concepts. It seems
that if we can infer  from OE, this must be because OE gives all the information that we need
to conclude that . And, conversely, if  contains no more information than , then, surely,
we should be able to infer  from OE.

In this paper we have been concerned with the information contained in DP L-formulas
from a different perspective. We have been trying to model the idea that "more discourse
contains more information". This has given rise to a notion of information structure. Of
course it is our hope that this notion of information will also allow us to say something
about inference.

There are different relations between j= and ^ that we could discuss. We will start with
the most obvious one and we will consider some other options as we proceed. The first guess
is that the inference relation simply is the ^-relation on information structures:

OE j=  , [OE] ^ [].
One example which supports this is:

9x:P (x) j= P (x).
But unfortunately the first counterexample is not far away. If we turn around the j=-sign in
the above example we get a case where OE j=  but not [OE] ^ [].

A somewhat weaker relation, which is still possible in view of these examples, is:

OE j=  ( [OE] ^ [].
For, it is not the case that [P (x)] ^ [9x:P (x)], so the counterexample that we had, is not a
counterexample for this weaker correspondence. This weaker correspondence would suggest

25More about this issue can be found in Van Benthem[1991].
26Note that the fact that this notion of inference has survived the reformulations in this paper, is evidence

that it is indeed at the core of DP L, even if in the more general picture it is just one of a number of
candidates.

27A similar situation exists in Pratt's action logic (Pratt[1991]).

18

that our ordering on IN F O is too strong: if [OE] ^ [], then OE j= , but even if not [OE] ^ [],
OE j=  can still hold.

But this is not the case: we can have OE and  such that [OE] ^ [] but not OE j= , as the
following example shows.

[P (x):9x::(P (x))] ^ [P (x)]; but not: P (x):9x::(P (x)) j= P (x):
That the first relation holds is true in view of the monotonicity result that we established
in the previous section: more discourse contains more information. But it is also clear that
not P (x):9x::(P (x)) j= P (x); since this would imply that the same instance of x would
both have property P and not have property P .

It is clear what goes wrong in this example: in OE j=  the binding between the antecedent
OE and the consequent  makes that we are talking about the same x both having and not
having property P . But the x in the antecedent that has property P is not the same as the
x that has property not P .

We could try to prevent this kind of anaphoric confusion as follows. Instead of comparing
[OE] and [], we compare [OE] and [OE:]. If we do this, we can take into account the bindings
between OE and  already if we are looking at ^. So this should help to prevent the unpleasant
surprises that these bindings, that are essential for the j=-relation, cause in cases as the above.
Therefore our next guess is that:

OE j=  , [OE] ^ [OE:]:
Note that this guess reflects the same intuition about the relation between information and
inference: if OE j= , then what we learn from OE: is no more than what we learn from
OE.28 In other words: given OE,  contains no new information. Also note that for most
of the systems that we mentioned above the two guesses coincide formally. For example,
for propositional logic and Boolean algebras we have that for two propositions P1 and P2,
[P1] ^ [P2] iff [P1^P2] ^ [P1]. It might be the case that in our situation this formulation with
the conjunction is simply more suitable, since in DP L anaphoric bindings are so important.

In the new situation the counterexample that we had no longer works. For now [OE:] =
[P (x):9x::(P (x)):P (x)] has to be compared with [OE] = [P (x):9x::(P (x))]: We see that
not [OE] ^ []. Therefore we would not expect OE j=  in the first place.

But again there is a counterexample. The counterexample is embarrassingly simple --
and in fact it also defeats our earlier guesses -- but also very instructive.

9x:P (x) j= 9y:P (y).29
Here we see that not [OE] ^ [OE:], simply because OE: says something about more variables
than OE alone. Since the introduction of new variables counts as an increase of information
according to ^, we do not find [9x:P (x)9y:P (y)] * [9x:P (x)].

Here we see the main problem for the comparison of j= and ^. ^ is based on the idea
that as a rule new variables are introduced to give new information. But the variables that
are introduced in the consequent of j= typically are not introduced for this purpose. They
are there to make claims about old information.30 If we say 9x:P (x) j= 9y:P (y), than we

28Also note the similarity with Veltman's notion of entailment.
29Note that also 9x:P (x) j= 9x:P (x) is a counterexample. So the choice of the variable in the consequent

is not important.

30The same holds for the variables in the consequent of an implication.

19

do not mean that some unknown object y has the property P , but we claim that when we
know 9x:P (x), we already know an object with property P .

It seems that we are at a dead end: in order to model the idea that more discourse
contains more information, we had to count the introduction of variables as informative
acts. This was an important motivation for our definition of ^. We have seen that in the
context of inference we typically consider discourse that is not supposed to contain new
information, but nevertheless can contain new variables. These are conflicting requirements
on the ordering of information.

4.4.3 Multi-dimensional information algebras
The conflict that we have seen seems irrepairable: we cannot think of a way in which it
could be solved. We think that in DP L information is given by conditions on variables.
These conditions can be represented as restrictions of the values of the variables. Any
sensible notion of information state for DP L should show the variables that the information
is about as well as the restrictions on these variables that embody the information. So the
problem that we have sketched will arise for any sensible notion of information structure
that one might come up with. Always the same question will arise: do more variables mean
more information or not? And always the answer will be both yes and no.

Therefore our conclusion must be that there is more than one way to look at the information of a DP L-formula. These different ways give rise to different orderings of the
information structures. We have already seen two perspectives on information that give rise
to two different orderings of information structure. One is the perspective where we consider
OE and OE: and ask ourselves which of the two we would prefer to hear. Of course we would
choose OE:, since it contains more information than OE alone.

In the other perspective we imagine that we are in a situation that we have heared OE
and we wonder whether we still want to hear . Of course we only want hear , if given
OE it contains new information. This situation, in which we consider OE and  in a specific
order and not as unordered alternatives, gives rise to a different ordering of information
structures. We call this the diachronic information order. This is the ordering that should
correspond directly to j=. The situation where we can choose between OE now or OE: now
gives rise to a synchronic ordering of information structures. This is the situation that we
have considered in section 4:2.31

We can illustrate the difference between these two ways of looking at information with
an example from DP L. Consider the formula OE j :P (x):9x:P (x). This is an example of
a DP L-formula that does not entail itself. Therefore it will behave funnily in a diachronic
information ordering. But in a synchronic information ordering it will not behave funnily:
synchronically each formula is of course as informative as itself.32

We can think of these different orderings as the dimensions of the information algebra.
So the conclusion is that we have to work in a multi-dimensional information algebra. Of
course this cannot be the final word about information orderings. Just as with the study
of dynamic inference the fact that there are several sensible information orderings gives rise
to further questions. We would like to know what kind of relations count as information
orderings, i.e. how many dimensions there are in our information algebra. Is there any
relation on information structures that is not an information ordering?

31The terminology is taken from Visser[1991], who applies the distinction in a slightly different context.
32This example is due to one of the referees.

20

At the moment it seems to us that there is a feature that all ordering relations should have
in common. Let's assume that our information structures contain both a set of variables33
-- the variables that the information is about -- and a set of functions embodying the
restrictions on the values of the variables. This is not only true for our information structures
but also -- as we have argued -- for any reasonable alternative. Comparing the information
contained in these information structures always amounts to comparing the restrictions on
variables that we find in the different information structures. If for every variable in one
structure, oe say, we can find a variable in the other structure, oe0, that is at least as severely
restricted, then we are inclined to say that oe0 contains more information than oe.

This could be tested with a mapping from the variables in oe0 to the variables in oe. If we
can find a mapping such that the restrictions on the images of the variables are at least as
severe as the restrictions in oe, then we would say that oe0 is more informative than oe. The
severity of the restrictions can, of course, be compared by looking at the assignments. We
intend to develop this general idea about the ordering of information states elsewhere. Here
we just check how the two information orderings that we have seen relate to this general
idea.

We find that both the synchronic and the diachronic information ordering embody this
idea. But both notions have some extra conditions on the variables that we are allowed to
compare, conditions on the mapping from oe0 to oe as it were. If we test whether (A; S; F ) ^
(A0; S0; F 0), we check whether for any f 0 2 F 0 there is an f 2 F that has the same values
for the variables in S0 as f itself. Here this comparison of variables in different information
structures is effectuated by the condition f hhS00iif 0 (where S00 is such that S = S0 \Lambda  S00).
This condition tells us which variable occurrences have to be compared. For example, if
(A; S; F ) = (;; hx; xi; F ) and (A0; S0; F 0) = (;; hxi; F 0), then we will compare the last value
of f 0(x), not with the last value of f (x), but with the value before last: f (x) = f 0(x) \Lambda  hdi
for some d.

For j= the relation with our general information ordering is less straightforward. The
relation between entailment and the general ordering can be made precise, but the technical
details would take us beyond the scope of this paper. Suffice it to make the connection
intuitively clear.

In checking intuitively that something like 9x:P (x):9y:Q(y):R(y) j= 9z:Q(z) holds, we
try to find (for z) in the antecedent a variable that satisfies at least the condition Q. Here
this variable is y. There are no restrictions on the variable that we can choose: any variable
will do as long as it satisfies Q.

If we have a free variable in the conclusion (as in 9x:P (x) j= P (x)), then the situation
is different. This time we do not have a free choice at all. A free variable in the conclusion
can be bound by a variable in the antecedent. If this is the case, then this is the variable we
should choose. So in the example we can only compare the x in the conclusion with the x
in the antecedent. If the variable is not bound by the antecedent (as in j= (P (x) ! P (x))),
then the conclusion has to hold for all values of x. So we can see that also for j= we have to
compare variables, taking into account some restrictions.

The conclusion of this section is that there is no straightforward algebraic relation between ^ and j=. In this respect DP L-semantics is less well behaved then the formal systems
we mentioned above. But the orderings on information structures that ^ and j= lead to,

33Strictly speaking we should say "a set of variable instances", for we will have to distinguish different
occurrences of variables (just as we do in sequence semantics), according to the different roles one variable
can play.

21

seem to be instances of one general scheme for the comparison of information. This general
information order will get more attention elsewhere.

5 Update semantics
5.1 Update semantics
In the preceding sections we have given interpretation of DP L in terms of assignments that
have sequences as values. We have checked that our semantics is faithful to the original
DP L-semantics (in section 2:3) and we have seen that we can give both a relational and
a static formulation of our semantics. In this section we formulate sequence semantics as
update semantics. We will see that such a formulation is available. Then we will discuss
the issue of eliminativity again. We have addressed this issue already in terms of the static
semantics (section 3:3), but if we formulate the notion of monotonicity in update style, this
will make the comparison with the original discussion by Groenendijk and Stokhof[1991b]
more straightforward.

First we define an operation on information states, that we call the merger.

Definition 5.1 We define the merger of information structures, ffl : IN F O ! IN F O, as
follows:

(A; S; F ) ffl (A0; S0; F 0) = (A [ (A0n(range(ffS)); S \Lambda  S0; ff 2 F 0 : 9g 2 F : ghhS0iif g.

We use this operation to define the interpretations of formulas as update functions on
information structures.

Definition 5.2 For a DP L-formula OE we define the update function ([OE]) : IN F O ! IN F O
as follows:34

(A; S; F )([OE]) = (A; S; F ) ffl [OE]:

Here we have defined ([OE]) in terms of [OE], but it is an easy exercise to show that we can
also define ([OE]) directly in such a way that the defining property holds.

Note that the update functions make it possible to built up the static interpretation: for
example, if (A; S; F ) = [], then (A; S; F; )([OE]) = [OE:].35 Hence we can find the interpretation of a conjunction by updating the state of no information, (;; hi; SASS), step by step.
In other words:

Proposition 5.3 Let OE0; : : : ; OEn 2 DP L be given. Then we have:

[OE0: : : : :OEn] = (: : : ((;; hi; SASS)([OE0])) : : :)([OEn]):

We can also give a definition of inference in terms of update functions.
Definition 5.4 Let (;; hi; SASS)([OE]) = (A; S; F ) and (;; hi; SASS)([]) = (A0; S0; F 0).
Then we define:

OE j=up  , 8f 2 F : 9g 2 F 0 : f hhS0iig:
34We use postfix notation for update functions.
35This explains the notation for the merger as ffl: it is the semantic analogue of 0:0 .

22

Because of the close relationship with the static interpretation that we have established
in proposition 5:3, it can be checked easily that this notion of inference coincides with the
one(s) discussed before.

Corollary 5.5 OE j=up  , OE j= :

Now we can formulate the monotonicity property in terms of update functions.
Proposition 5.6 Let OE 2 DP L be given. Then ([OE]) is monotone decreasing, i.e.:

(A; S; F ) * (A; S; F )([OE]) for all (A; S; F ) 2 IN F O.

The proof of the proposition again relies on the correspondence between the static and
the update interpretation. Because of this correspondence the result simply follows from the
monotonicity result for the static interpretation (section 4.3).

We see that for the improved notion of information state the DP L-updates are monotone,
or -- in the terminology of Groenendijk and Stokhof[1991b] -- eliminative. Now that we
have discovered this improved notion of monotonicity, we can check what this property
amounts to in terms of the relational semantics. We find, by a careful reconstruction, the
following reformulation of monotonicity:

Proposition 5.7 Let oe 2 ASS; OE;  2 DP L be given. Then:36
Monotonicity 8f 2 oe([OE:])gs9g 2 oe([OE])gs such that ghhrange(ffS)iif:
Truth property oe([OE:])gs 6= ; ) oe([OE])gs 6= ;:

Hence the truth of OE: implies the truth of OE.

Proof: [Monotonicity] From the monotonicity property for our update semantics we
learn that oe ffl [OE] ffl [] ^ oe ffl [OE]. For oe = (;; hi; SASS), this means that all f 0 in the static
interpretation of OE: are extensions of some g0 in the static interpretation of OE. Because
of the relation between the sequence semantics and the original relational semantics we see
that this means that for any f 2 range([[OE:]]gs) there is a g 2 range([[OE]]gs) that differs from
f only on the variables occurring in . This proves the proposition for oe = (;; hi; SASS).
The proof of the general case is completely analogous.

[Truth property] Follows immediately from the Monotonicity property.2

Of course, this is not anything like the eliminativity property that Groenendijk and
Stokhof considered. Since they were not careful in the choice of their notion of information
state, they defined an inappropriate notion of eliminativity, one that did not correpond
to the notion of information growth. Since for Veltman's system this is exactly what the
eliminativity property is about, the resulting comparison of DP L and Veltman's update
semantics was confused. Now we are in a position to clear up the confusion and we find the
monotonicity property that we would expect for DP L.

36Here we abuse the notation hh:ii: we use it for a set instead of a sequence. Of course we mean the usual
notion of resetting a function here, where fhhXiig allows us to reset all the values of the variables in the set
X.

23

5.2 Might
Veltman does not introduce update semantics just as a nice way to present dynamic semantics. He has some substantial applications in mind (see Veltman[1990]) in the dynamic
semantics of modalities. The simplest system that Veltman applies the techniques of update semantics to is propositional logic with an operator 3, might. The update semantics
enables us to give a dynamic interpretation to might. What we mean by this is shown by
the following example:

Example:
1. It might be raining. ... It is not raining.
2. It is not raining. ... It might be raining.

The first sentence says that we first think that it might be raining and later find out
that it is not raining. This is all right. But in 2 we still think that it might rain after we
have found out that it is not raining. That is nonsense. This example shows that a dynamic
treatment of might is needed. Only a dynamic might could explain why the first sentence
seems acceptable, and the second not. Veltman has succeeded in giving an elegant semantics
for propositional logic with 3 that deals with this phenomenon.

In this section we will see whether an easy extension of our system with a dynamic
modality is available. We will show that it is possible to define a dynamic might operator in
our system. The semantics of this operator, 3, cannot be given "pointwise". We mean that
it is not possible to compute the effect of 3OE in some complex state, by first computing its
effect on the atomic substates and then simply adding the results to find the effect on the
complex state. This is in contrast with what we have seen so far: for our DP L-updates we
have:

Proposition 5.8 Update functions for DP L-formulas are "pointwise":

(A; S; F )([OE]) = (A0; S0; SfGf : f 2 F g),
where for all f 2 F (A; S; ff g)([OE]) = (A0; S0; Gf).

The fact that such a result cannot be obtained for the semantics of 3 is not a defect
of our semantics: it is an essential property of the meaning of 3. 3OE induces a test on
our current state of information: the test succeeds if OE is compatible with our information.
Then it leaves the state of information unchanged. If OE is incompatible with what we already
know, the test fails. Then the result 3OE is total confusion: the information of 3OE gives a
contradiction.

If we try to perform such a test bit by bit, we will (possibly) throw away some information,
since it is incompatible with OE, while we leave other bits of information intact. Then, if we
add up the resulting bits of information, we could only retrieve some of the information that
we started with. This is in contradiction with the test character of 3OE. Hence a pointwise
approach to the semantics of might does not stand a chance.

We can define the concept of acceptability or compatibility that is associated with 3, as
follows:

Definition 5.9 OE is acceptable in (A; S; F ) iff 9f 2 F : 9g 2 FOE : f hhSOEiig.

Remember that we defined :

24

OE is valid in (A; S; F ) iff 8f 2 F : 9g 2 FOE : f hhSOEiig.
So, while validity says that OE should hold in all possible cases, acceptability says that OE
should hold in at least one possible case. In this context we can think of the set F as the
set of possibilities (or possible information histories).

Now we can explain the meaning of 3OE as follows: checking whether 3OE holds in a
situation (A; S; F ) means checking whether OE is acceptable in (A; S; F ). So we define:

Definition 5.10 For each OE we define ([3OE]) as follows:37

(A; S; F )([3OE]) = (A; S; F ) if OE is acceptable in (A; S; F );
(A; S; F )([3OE]) = (A; S; ;) else.

It can happen that for some formula OE, (3OE)::(OE) is acceptable, while :(OE):(3OE) is not
acceptable, just as in the example above. Consider, for example, the formula P (x). We find
that, in a model where there is some p 2 I(P ), while not I(P ) = DOM :

(;; hxi; ff 2 SASS : length(f (x)) * 1g)([3P (x)])([:P (x)]) =

(;; hxi; ff 2 SASS : length(f (x)) * 1g)([:P (x)]) =
(;; hxi; ff 2 SASS : length(f (x)) * 1 ^ not : end(f (x)) 2 I(P )g),
but also:

(;; hxi; ff 2 SASS : length(f (x)) * 1g)([:(P (x))])([3P (x)]) =

(;; hxi; ff 2 SASS : length(f (x)) * 1 ^ not : end(f (x)) 2 I(P )g)([3P (x)]) =,
(;; hxi; ;).
This confirms that what we have defined is a dynamic might operator.

However, the non-commutativity of ([OE]), which is the clue to its dynamic character, holds
only for a restricted class of formulas. We find the non-commutativity only if OE contains
free variables. This can be understood as follows:

Suppose that OE contains no free variables, i.e. AOE = ;. We know that

OE is acceptable in (A; S; F ) iff 9f 2 F 9g 2 FOE : f hhSOEiig:
But if AOE = ; then this is the case exactly if:

8f 2 F 9g 2 FOE : f hhSOEiig:
(This follows from the irrelevance lemma that we proof in the appendix. The remark there
about free variables is important here.) But this is the definition of validity. So formulas
without free variables are acceptable iff they are valid.

And if OE is valid, then :(OE) is not acceptable, so (A; S; F )([:(OE)]) = (A; S; ;). But then
we find that in case OE does not contain free variables

37Note that ([3OE]) is not representable as an information structure!

25

OE is acceptable in (A; S; F ) ,
(A; S; F )([3OE]) = (A; S; F ) )
(A; S; F )([3OE])([:(OE) ]) = (A; S; F )([:(OE)]) = (A; S; ;).

Hence:

([3OE::(OE)]) = ([:(OE):3OE]).
This restricts the applicability of our 3 as a dynamic might operator. We can understand
the restriction from the technical point of view, but it does not seem to make sense intuitively.
If something might exist, then usually it does not follow that it does exist. Still the fact
remains that 3 gives us a consistency test for DP L as an operation on information structures.
And for a non-trivial fragment of the language this consistency test has a dynamic character.

5.3 Down-dating
In the section on monotonicity we explained why the semantics of DP L should be monotone:
DP L is to be the language of ordinary discourse in which more and more information is
revealed by the speaker and gathered by the hearer. We also said that for some other
situations it might be handy to have a language and a semantics of forgetting or downdating, as we will call it. In this section we extend DP L with atomic formulas xE -- read as
"x exit" -- for any variable x, that will be interpreted as an instruction to forget the current
value of x. We will see that down-dating helps us to formulate old ideas more elegantly.

The interpretation of xE is essentially relational: xE does not give information, it is
purely an action.38 We define the relational interpretation of xE as follows:

Definition 5.11 [[xE]] = f(f; g) : pd(f (x)) = g(x) ^ (y 6= x ! g(y) = f (y))g: (again
pd(hi) = hi).39

In this extension of the language we can have local variables: for example in 9x:OE:xE.
We can also give an update formulation of the meaning of a down-date.

Definition 5.12 (A; S; F )([xE]) = (A0; S0; fg : 9f 2 F : ghhxiif g),

where S0 is obtained from S by removing the last x. If there is no occurrence of x in S,
S0 = S and we remove x from A: A0 = Anfxg. If x does not occur in A either: A0 = A.

Note that ([xE]) works best if there is at least one x available in (A; S; F ). If there is no
x, then (A; S; F )([xE]) = (A; S; F ).

Now it is possible to establish another relation between [[OE]] and [[OE]]gs. As one can see:
[[9x]]pgs=[[xE:9x]]"P ASS \Theta  P ASS and [[9x]]gs=[[xE:9x]]"ASS \Theta  ASS. This is no surprise:
the original interpretation of 9x told us to replace a value of x. In the refined semantics
this can be established by two seperate actions: first throw away the old value of x with
xE, then add the new value with 9x. Thereby we are able to translate ordinary dynamic
predicate logic into the enriched language by replacing all quantifiers 9x by xE:9x. If we
call this translation ffi, we find:

Proposition 5.13 Let f; g 2 P ASS. Then: f [[OEffi]]g , f [[OE]]pgsg.

38This means that it can not be represented by some information structure.
39pd is defined in definition 2.2.3.

26

Corollary 5.14 Let f; g 2 ASS. Then: f [[OE]]gsg , f [[OEffi]]g.

(The proof is omitted.) The restriction that f; g 2 P ASS is added because [[.]]pgs is
defined on P ASS. The corollary follows immediately from the proposition, since [[.]]gs is the
restriction of [[.]]pgs to total assignments.

It is also possible to use down-dating to give an elegant definition of dynamic validity,
j=. First we introduce some notation:

Notation: We will write # hxig for the assignment f such that g[[xE]]f . For
# hxni(: : : (# hx1i(g)) : : :) we write # hx1; : : : ; xni(g) and if G is a set of partial
assignments, we write # hx1; : : :; xni(G) for f# hx1; : : : ; xni(g) : g 2 Gg.

We define:
Definition 5.15 1. o/ is a relation on states defined by: (A; S; F ) o/ (A0; S0; F 0) ,

F `# S0(F 0).

2. For any state (A; S; F ), # (A; S; F ), the projection (or domain) of (A; S; F ), is defined

by: # (A; S; F ) = (A; hi; # S(F )).

So to find # (A; S; F ), we simply forget the values of the variables in S. This way the
functions in # S(F ) are exactly the ones that have an hhSii-extension in F .

Now we have that for any OE, # [OE] is the input state, or domain, for OE: OE asks for values
on the variables in the first component of # [OE] and accepts only those assignments that are
in the third component of # [OE]. # S(FOE) is for [OE], what dom([[OE]]gs) is for [[OE]]gs.

We claim that 5:15:1 is in fact a definition of dynamic validity. I.e. we claim:

Proposition 5.16 For any OE;  : [OE] o/ [] , OE j= .

Proof: By comparing the definition of o/ and, for example, j=[, using the fact that
f hhSiig ,# S(g) = f .2

This result seems rather strong, but in fact it is already known for the original DP Lsemantics that

OE j=  , range([[OE]]gs) ` dom([[]]gs):
Here we see that the same relation holds, but now it is possible to define domains in terms
of a more primitive notion: the down-date operator. In the extended language it is even
possible to give for each formula an expression that gives the domain of the formula: we
simply add the right amount of xE's for each variable. If we call this expression for the
domain of OE, # OE, we get for example:

Example:
# P (x) = P (x);
# (9x:R(x; y)) = 9x:R(x; y):xE
# (9x:R(x; y):9x:P (x)) = 9x:R(x; y):9x:P (x):xE:xE

27

The fact that we are able to model down-dating in sequence semantics, shows how
powerful sequence semantics really is. In the applications that we have given, we have
shown that some familiar notions in the semantics can be redefined elegantly in terms of
the down-date operator. It remains to be seen how down-dating can be used in the study of
phenomena that are genuinely non-monotonic. Probably xE could be used to model some
cases of belief revision, but that would take us beyond the scope of this paper.

6 Conclusion
The main conclusion of this paper is that it is possible to give a formalization of the ideas of
Groenendijk and Stokhof[1991a] in which the information content of a formula can be represented formally. This means that interesting questions about information can be discussed
in DP L-semantics.

We have obtained the improved representation by using sequence valued assignments.
The use of these assignment inspired a suitable notion of information structure. We discovered that different ways of looking at information in DP L lead to different orderings on the
information structures. In further work we hope to improve our understanding of the ways
in which information can be compared in dynamic semantics. We think that the general
idea that we have developed about comparing information will be of use here.

We were also able to define a down-date operator in our semantics. This operator is an
instruction to forget the value of a variable. We have looked at the relation of down-dating
with the original semantics for DP L and with the DP L-notion of valid inference. But maybe
downdating can also be used to model genuinely non-monotonic phenomena, such as belief
revision, that fall ouside the scope of this paper.

7 Acknowledgements
I would like to thank Albert Visser for encouragement and inspiration. Thanks are also due
to my colleagues at the O.T.S (Research Institute for Language and Speech) in Utrecht,
Holland, and my colleagues from the AIO-network TLI who have provided useful criticism
on an earlier version of this paper. I am particularly grateful to Emiel Krahmer for pointing
out numerous small errors and typos in the earlier version. I would also like to thank the
referees, whose detailed comments have been very helpful.

8 Appendix
In this appendix we present a proof of proposition 2:6 and corollary 2:8. In the proofs we
will use the following lemma:

Lemma(Irrelevance):

1. k[[OE]]h) k0[[OE]]h0 for all k0; h0 such that for all x k0(x) = oex \Lambda  k(x) and h0(x) = oex \Lambda  h(x)

(for some sequence oex).

2. k[[OE]]h ) k00[[OE]]h00 for all k00; h00 such that for all x k(x) = oex\Lambda k00(x) and h(x) = oex\Lambda h00(x)

(for some sequence oex) and k00(x) extends end(k(x)).

28

The lemma says that if (k; h) is in the relation [[OE]], only those values of k and h are
relevant that occur after end(k(x)). This corresponds to the fact that we always add the
current value of a variable at the end. (end(k(x)) itself is relevant if x is free in OE.) We will
not prove the lemma: the proof is an easy induction.

Now we will prove proposition 2:6:

Proposition 8.1 For all OE 2 DP L we have \Phi ([[OE]]) = [[OE]]pgs.

Remember that \Phi  is defined by:
Definition 8.2 ( 2:5 ) We define \Phi  : ""(SASS \Theta  SASS) ! ""(P ASS \Theta  P ASS) as follows:
\Phi (R) = f(g; f ) : 9(k; h) 2 R : 8x 2 V AR : f (x) = end(h(x)) ^ g(x) = end(k(x))g.

Proof (of 2:6): We have to prove that:
1 (g; f ) 2 \Phi ([[OE]]) ) g[[OE]]pgsf
2 g[[OE]]pgsf ) (g; f ) 2 \Phi ([[OE]]pgs).

We prove this by a simultaneous induction on the complexity of OE. We will need 1 as
induction hypothesis for the !-clause of 2 and vice versa.

1. P (x) Suppose (g; f ) 2 \Phi ([[P (x)]]). Then there are h; k such that k = h ^ end(h(x)) 2

I(P ) ^ 8y : g(y) = end(k(y)) ^ f (y) = end(h(y)). Hence f = g and end(h(x)) =
f (x) 2 I(P ), i.e. g[[P (x)]]pgsf .

9x Suppose (g; f ) 2 \Phi ([[9x]]). Then there are h; k k[hxi]h ^ 8y : f (y) = end(h(y)) ^

g(y) = end(k(y)). Obviously g[[9x]]pgsf .

:O/ Suppose (g; f ) 2 \Phi ([[:O/]]). Then there are h; k; l such that: k[V \Lambda  VO/]h ^ h 2

FO/ ^ l[VO/]h ^ l 2 F ^ 8y : g(y) = end(k(y)) ^ f (y) = end(h(y)). Define m such
that m(y) = end(l(y)). Then (m; f ) 2 \Phi ([O/]). Also, since k[V]l, (g; m) 2 \Phi ([]).
Hence (ind. hyp.): g[[:O/]]pgsf .

( ! O/) Suppose (g; f ) 2 \Phi ([[( ! O/)]]). Then there are h; k such that k[[( !

O/)]]h ^ 8y : g(y) = end(k(y)) ^ f (y) = end(h(y)). Then f = g, k = h. Now let
f [[]]pgsm. Then 2 gives us l[[]]n. Now the lemma gives us n0 such that h[[]]n0.
By assumption this means that there is a n00 n0[[O/]]n00. By the definition of \Phi ,
this gives a p such that (m; p) 2 \Phi ([[O/]]). Now the induction hypothesis for 1
guarantees m[[O/]]pgsp. Hence g[[( ! O/)]]pgsf .

2. P (x) Suppose g[[P (x)]]pgsf . Then g = f ^ end(f (x)) 2 I(P ). So we can choose

h = k = f to prove that (g; f ) 2 \Phi ([[P (x)]]).

9x Suppose g[[9x]]pgsf . Then 8y : (g(y) = f (y) ^ (y = x ^ f (x) is defined)). Choose

h such that h(x) = g(x) \Lambda  hf (x)i and h(y) = f (y) for all other y, and choose k
such that k(x) = pd(h(x)) and k(y) = h(y) for all other y. These h; k guarantee
that (g; f ) 2 \Phi ([[9x]]).

( ! O/) Suppose g[[( ! O/)]]pgsf . Then f = g ^ 8m : g[[]]pgsm 9n : m[[O/]]pgsn.

Let k = f = g = h. We prove that k[[( ! O/)]]h: suppose k[[]]l. This gives
(g; p) 2 \Phi ([[]]) for p such that p(y) = end(l(y)) for all y. Hence, by 1, g[[]]pgsp.
But then, by assumption, there must be a q such that p[[O/]]pgsq. By induction
hypothesis for 2 we get h0; l0 such that l0[[O/]]h0 ^ 8y : end(h0(y)) = q(y) ^
end(l0(y)) = p(y) = end(l(y)). Using the lemma we find that there is a h00 such
that l[[O/]]h00.

29

:O/ Suppose g[[:O/]]pgsf . It suffices to consider the cases in which O/ is not of the form

O/0:O/00.

a O/ j P (x): Then g[[]]pgsf ^ f (x) 2 I(P ). By induction hypothesis for 2 we

get h; k such that k[[]]h ^ 8y : f (y) = end(h(y)) ^ g(y) = end(k(y)). But
then end(h(x)) 2 I(P ). So these h; k also do the job for :P (x).

b O/ j 9x: Then there is a f 0 such that f 0 does not differ from f on variables

other than x and g[[]]pgsf 0. By the induction hypothesis for 2, there are
k; h0 such that k[[]]h0 ^ 8y : end(h0(y)) = f 0(y) ^ end(k(y)) = g(y). Now
we define h such that h(x) = h0(x) \Lambda  hf (x)i and h(y) = h0(y) for all other
variables. Then h; k do the job for :9x.

c O/ j (ae ! o/ ): Then g[[]]pgsf ^ f [[(ae ! o/ )]]pgsf . By the induction hypothesis

for 2 there are k; h; h0 such that k[[]]h ^ h0[[(ae ! o/ )]]h0, where for all x
end(h(x)) = end(h0(x)) = f (x). By the lemma we find that also h[[(ae ! o/ )]]h.
But then (g; f ) 2 \Phi ([[OE]]). 2

Note that the proof of the proposition is not difficult. It is just hard work. The same
holds for the proof of corollary 2:8. This time we will provide less details.

Proof (corollary 2.8): We sketch the proof of 1 , 2 and leave 2 , 3 to the reader.
1 ) 2 Assume OE j=[[ . Let f; g 2 P ASS be given such that f [[OE]]pgsg. We have to find

h 2 P ASS such that g[[]]pgsh. Now, since P ASS ` SASS, it follows from the
assumption that we find a h0 2 SASS such that g[[]]h0. But then \Phi  gives a h 2 P ASS
such that g\Phi ([[]])h, i.e. g[[]]pgsh.

2 ) 1 Assume OE j=pgs . Let f; g 2 SASS be given such that f [[OE]]g. We have to find

h 2 SASS such that g[[]]h. \Phi  gives for f; g f 0; g0 2 P ASS such that f 0[[OE]]pgsg0. By
the assumption this gives an h0 2 P ASS such that g0[[]]pgsh0. If we decompose  into
atoms 0; : : : ; n, then we can see all the changes of values that we need to build the
h 2 SASS such that g[[]]h.2

9 References
Benthem, J. van "Semantic parallels in natural language and computation", in: H. Ebbinghaus et al. (eds.), Logic Colloquium '87, Amsterdam,1989

Benthem, J. van "General Dynamics", Theoretical Linguistics, pp. 159-201, 1991
Dekker, P. "An update semantics for dynamic predicate logic", to appear in: P. Dekker,

M. Stokhof (eds.) Proceedings of the 8th Amsterdam Colloquium, 1992

Fernando, T. Transition Systems, manuscript, 1991 (to be presented at JELIA 1992,

Berlin)

Groenendijk, J. and M. Stokhof "Dynamic Predicate Logic", Linguistics and Philosophy 14, pp. 39-100, 1991a

Groenendijk, J. and M. Stokhof "Two theories of dynamic semantics", in: J. van Eijck

(ed.), Logics in AI -- European Workshop JELIA '90, pp. 55-64, Berlin, 1991b

30

Kamp, H. "A Theory of Truth and Semantic Representation", in: Groenendijk et al. eds.,

Truth, Interpretation and Information, Dordrecht, 1984

Kamp, H. and U. Reyle From Discourse to Logic, manuscript, Institute for Computational Linguistics, University of Stuttgart, 1990

Pratt, V. "Action Logic and Pure Induction" in: J. van Eijck (ed.), Logics in AI -- European Workshop JELIA '90, pp. 55-64, Berlin, 1991

Veltman, F. "Defaults in update semantics", manuscript, Department of Philosophy, University of Amsterdam, 1991 (to appear in: Journal of Philosophical Logic)

Visser, A. Actions under Presupposition in: Logic Group Preprint Series, no.76, 1992
Zeevat, H. "A Compositional Approach to Discourse Representation Theory", Linguistics

and Philosophy 12, pp.95-131

31