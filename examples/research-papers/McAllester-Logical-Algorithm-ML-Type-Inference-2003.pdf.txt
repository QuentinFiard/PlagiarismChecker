

A Logical Algorithm for ML Type Inference

David McAllester
Toyota Technological Institute at Chicago

mcallester@tti-c.org

Abstract. This paper gives a bottom-up logic programming formula-
tion of the Hindley-Milner polymorphic type inference algorithm. Weshow that for programs of bounded order and arity the given algorithm

runs in O(n\Lambda (n) + dn) time where n is the length of the program, d is
the "scheme depth" of the program, and \Lambda  is the inverse of Ackermann'sfunction. It is argued that for practical programs

d will not exceed 5
even for programs with hundreds of module layers. This formulation ofthe Hindley-Milner algorithm is intended as a case study in "logical algo-

rithms", i.e., algorithms presented and analyzed as bottom-up inference
rules.

1 Introduction
This paper is a case study in the use of bottom-up logic programming for thepresentation and analysis of complex algorithms. The use of bottom-up logic
programming for algorithm presentation has been developed in a recent seriesof papers whose main contributions are theorems governing the running time of
these programs [6,1,2]. This paper explores the use of these run time theoremsfor the presentation and analysis of the Hindley-Milner type inference algorithm
used in the ML and Caml programming languages [7]. It is known that forprograms where procedures can take an unbounded number of arguments and
be of unboundedly high order (procedures which take procedures which takeprocedures ...) the type inference problem is complete for exponential time [4,5].
In practice human written procedures never take more than twenty argumentsand are never more than fifth order. It is known that for bounded order and arity
the type inference problem can be done in polynomial time [3]. However, I amnot aware of a published analysis giving a particular polynomial running time
for this problem. This paper gives an inference rule presentation of a version ofthe Hindley-Milner type inference algorithm and shows that this version runs
in time O(n\Lambda (n) + dn) where d is the "scheme depth" of the program. Thealgorithm presented here is very similar to the one described in [8] although no
formal analysis of running time is given there. Section 5 defines scheme depthand argues that it can be expected to be less than 5 even for programs with
hundreds of module layers.

R. Nieuwenhuis (Ed.): RTA 2003, LNCS 2706, pp. 436-451, 2003.c\Delta  Springer-Verlag Berlin Heidelberg 2003

A Logical Algorithm for ML Type Inference 437
2 The LDP Programming Language

Here we use (essentially) the logic programming language and run time modelspecified in [2]. For convenience we will here refer to this programming language
as LDP for Logic programs with Deletions and Priorities. It is important to notethat deletions and priorities have been widely discussed in the logic programming
literature. The main contribution of [2] is the definition of an abstract notion ofrunning time and a proof that this abstract definition of running time can be
implemented on a conventional random access computer. This run time result isstated in section 3. We now define an LDP program to be a set of inference rules
where an inference rule is specified by the category r in the following grammar.

N \Delta  i | n | N1 + N2 | N1 * N2
H \Delta  x | f(ff1, . . . , ffk)

ff \Delta  H | N
A \Delta  P (ff1, . . . , ffk) | N1 <= N2
C \Delta  P (ff1, . . . , ffk) | del(P (ff1, . . . , ffk))

r \Delta  A1, . . . , An \Pi  C1, . . . , Ck with priority N

Every variable in the priority N occurs in A1.
If Ai = P (t1, . . . tk) then Ai does not contain + or *.
If Ai = N1 <= N2 then every variable in Ai occurs in some Aj for j < i.
Every variable in Ci occurs in some Aj.

This is two-sorted grammar with a sort for Herbrand terms (H) and a separatesort for integers (

N ). There are two sorts of variables -- Herbrand variables suchas
x in the grammar for H and integer variables such as i in the grammar for
N . In the grammar for integers, n ranges over any integer constant. We allowpredicates and functions to take either sort as arguments although we assume

that each predicate and function symbol has a specified arity and a specifiedsort for each argument and all applications of predicate and function symbols
must respect this specification to be well formed. The function symbol f in thegrammar for Herbrand terms should be viewed as a data constructor.

The inference rules are to be run bottom-up. For rules not involving dele-tion, a state of the system is a database of assertions which grows monotonically
by adding new assertions derivable from some rule and assertions already inthe database. When deletion is involved each rule states that its conclusion ac-
tions are to be taken taken whenever its antecedents are satisfied. The actionsare either additions to, or removals from, a database of assertions of the form
P (ff1, . . . , ffn). A particular use (instance) of a rule involves particular valuesfor all the variables of that rule. Given values for the variables in the first an-
tecedent, a rule is associated with an integer priority where the integer 1 is thehighest priority (priority 1 rules run first). Priorities smaller than 1 are treated

438 David McAllester

source(v)
(D1,1)

dist(v, 0)

dist(v, d)
dist(v, d\Delta )
d\Delta  < d
(D2,1) del(

dist(v, d))

dist(v, d)
E(v, c, u)
(D3,d+2)

dist(u, d + c)

Fig. 1. Dijkstra Shortest Path.

as equivalent to 1 but (instances of) rules with larger priority values run only ifno higher priority rule instance can run.

Figure 1, taken from [2], gives an LDP implementation of the Dijkstra short-est path algorithm. The input is an initial database containing assertions of the
form E(v, c, u) stating that there is an edge from v to u of cost c plus an assertionof the form source(

v) stating that v is the source node. In the figure each rule isgiven a name and a priority. For example, rule D2 has priority 1 while rule D3

has priority d+2 where the integer variable d occurs in the first antecedent of therule. Different uses of the rule D3 run at different priorities. This has the effect
that distances are established in a shortest-first order as in the normal Dijkstrashortest path algorithm. The general run time model for inference rules given
in section 3 implies that these rules run in O(e log e) where e is the number ofinitial assertions. Note that

d\Theta  < d can be written as d\Theta  + 1 <= d.We now consider the operational semantics an arbitrary set of LDP rules

in more detail. A ground assertion is one not containing variables or arith-metic operations. A program state is a set

S of ground assertions of the form
P (t1, . . . , tk) and deletion assertions of the form del(P (t1, . . . , tk)). Deletionactually adds an assertion of the form del(

P (t1, . . . , tk)) so that the set D growsmonotonically over the execution of the algorithm. However, only the positive

elements of D that have not been deleted are "visible" to the antecedents of theinference rules. Once a ground assertion has been both asserted and deleted new
assertions or deletions of that ground assertion do not change the state of thedatabase. Let

r be the rule A1, . . . , An \Pi  C1, . . . , Cm at priority N . Let oe be asubstitution assigning (ground) values to all the variables in the rule

r. The pair\Sigma 
r, oe\Upsilon  will be called an instance of r. The instance \Sigma r, oe\Upsilon  is called pending atstate

S if each antecedent oe(Ai) holds in the state S and firing the rule wouldchange the state, i.e.,

S \Phi = S \Psi  {oe(C1), . . . , oe(Cm)}. We say that state S can

make an R-transition to state S\Theta , written S R\Pi  S\Theta , if there exists a pending ruleinstance \Sigma 

r, oe\Upsilon  with r fi R such that no pending stance of a rule in R has higherpriority and

S\Theta  is S \Psi  {oe(C1), . . . , oe(Cm)} where C1, . . . , Cm is the conclusion of
r. We say that W is a final state for R on input S if no rule instances are pend-ing in

W and W is reachable from S by a sequence of R-transitions. One shouldthink of

R as a (don't care) nondeterministic procedure for mapping input S tooutput
W .

A Logical Algorithm for ML Type Inference 439
3 The Running Time of LDP Programs
We say that R terminates on input S if there is no infinite sequence S1, S2, S3,
. . . such that S = S1 and Si R\Pi  Si+1. The Dijkstra shortest path algorithm infigure 1 terminates even if we assign all rules priority 1 so that they can fire

in any order. However, we are interested here in much more refined analysis ofrunning time when the rule set is implemented on a conventional random access
computer. To make statements about conventional running time we introducethe notion of a prefix firing. A prefix instance of

r with n antecedents is a triple\Sigma 
r, i, oe\Upsilon  where 1 <= i <= n and oe is a ground substitution defined on (only) thevariables occurring in the antecedent prefix

A1, . . . , Ai of r. Intuitively, a prefixinstance is a way of instantiating the first
i antecedents of the rule. Let W bea final state of input
S for rule set R. An atomic formula P (t1, . . . , tk) in Wwill be said to have been asserted and will be said to have been deleted if

Wcontains del(
P (t1, . . . , tk)). An assertion is visible to the antecedents of rules ifit has been asserted but has not been deleted.

Definition: A prefix firing of a rule r over a final state W is a prefixinstance \Sigma 

r, i, oe\Upsilon  of r such that oe(A1), . . ., oe(Ai) have all been asserted(and possibly also deleted) where

A1, . . ., Ai are the first i antecedentsof
r.

The basic result in [6] is that for a rule set R in which all rules have priority1 (and can therefore fire in any order), and where no rule involves deletion, the
(unique) final state can be computed time proportional to the number of prefixfirings over the final state. This bound also holds for rules with fixed priorities
(no variables in the priority expressions) and deletions. However the run timebound from this "naive" count of prefix firings is typically quite loose for rule sets
involving deletions and priorities. Rather than count all prefix firings we wantto only count the "visible" prefix firings. To define the visible prefix firings we
consider computation histories. A complete R-computation from S is a sequence
S1, S2, . . ., ST such that S1 = S, Si R\Pi  Si+1 and ST is a final value of S, i.e.,there are no instances of rules in

R that are pending in ST .

Definition: A state S is said to be visible to a prefix instance \Sigma r, i, oe\Upsilon if no pending instance of a rule in

R has priority higher than the priorityof \Sigma 
r, i, oe\Upsilon . A prefix firing \Sigma r, i, oe\Upsilon  over the final state of a computationis said to be visible in a computation if either

i = 1 and r is variablepriority or the computation contains a state
S visible to \Sigma r, i, oe\Upsilon  suchthat
oe(A1), . . . oe(Ai) all hold in S (have been asserted but not deleted)where

A1, . . ., Ai are the first i antecedents of r.

Every visible prefix firing is a prefix firing over the final state, but the vis-ible prefix firings are usually only a small fraction of the prefix firings. A first-
antecedent prefix firing is just a prefix firing \Sigma r, i, oe\Upsilon  with i = 1. The following isthe main theorem of [2].

440 David McAllester
Theorem 1. If the rule set R terminates on initial state S then a complete R-computation from

S can be computed on a RAM machine extended with constanttime hash table operations in time proportional to |

S| + P1 + P2 log N where |S|is the number of assertions in
S, P1 is the number of visible prefix firings offixed priority rules;
P2 is the number of visible prefix firings of variable priorityrules; and
N is the number of distinct priorities of first-antecedent prefix firingsof variable priority rules.

To see how this theorem can be applied we now give a running time analysisof the Dijkstra shortest path algorithm shown in Figure 1. Note that for a state
to be visible to (an instance of) a rule there must not be any pending higherpriority rule. States where no high-priority rule is pending often satisfy certain
invariants. The priority of a state is the priority of a highest-priority (lowestinteger) pending rule instance (or +fl if there are no pending rule instances).
The rule D2 ensures that in any state with priority 2 or larger we have at mostone distance associated with each node. So this invariant holds in any state
visible to the rule D3. The priority of an instance of rule D3 is determined bythe value of the integer variable

d appearing in the first antecedent -- shorterdistances get higher priority. Note that when an instance of D3 runs all pending

rule firings involve priorities at least as large as d. Since an invocation of rule D3must generate a bound at least as large as

d, all future bounds will be at least aslarge as
d. Hence, in any state with priority d +2 or greater, any visible assertionof the form

dist(u, d) must have the property that d is the actual shortest distanceto the node

u. Hence, for a given node u there is only one value of d used in afull firing of rule D3. This implies that the total number of firings of the first two

antecedents of D3 is at most the number of edges in the graph. Each firing of D2occurs in a state immediately following a firing of D3 and for each firing of D3
there is at most one firing of D2. Hence the total number of firings of D2 is also atmost the total number of edges. Furthermore, this implies that the total number
of assertions of the form dist(u, d) ever asserted is at most e. All of these "ever-asserted" assertions must be included as prefix firings of the first antecedent of
the variable priority rule D3. However, for the two antecedent prefix firings ofD3 we need only consider a single value of

d for each value of u. Hence the totalnumber of prefix firings is
O(e). The number of distinct priorities associated withrule D3 is at most
e. So the abstract run time of rule D3 is O(e log e). The runtime of D3 dominates and the algorithm runs in

O(e log e) time.

4 Union-Find
Figure 2, taken from [2], gives an LDP implementation of the union-find algo-rithm. The algorithm is not particularly elegant, but it does show that LDP with
its associated abstraction notion of run time is sufficiently powerful to express auseful implementation of union-find. Note that these rules all have priority either
1 or 2. The rules take as input assertions of the form union(x, y) and produce asoutput assertions of the form

find(x, f). Any state in which no union-find rulesare pending, e.g., any state with priority 3 or lower (numerically larger), satisfies

A Logical Algorithm for ML Type Inference 441
union(x, y)
(UF1,1)

nofind(x)
nofind(y)

find(x, y)
(UF2,1)

del(nofind(x))

find(x, y)
find.arc(y, z)
(UF3,1)

find(x, z)del(

find(x, y))

union(x, y)
find(x, z)
find(y, z)
(UF4,1)

del(union(x, y))

nofind(x)
(UF5,2)

find(x, x)
size(x, 1)

union(x, y)
find(x, x\Delta )
find(y, y\Delta )
(UF6,2)

merge(x\Delta , y\Delta )

merge(x, y)
size(x, s1)
size(y, s2)
s1 <= s2
(UF7,1)

del(merge(x, y))
find.arc(x, y)
del(size(y, s2))
size(y, s1 + s2)

merge(x, y)
size(x, s1)
size(y, s2)
s2 < s1
(UF8,1)

del(merge(x, y))
find.arc(y, x)
del(size(x, s1))
size(x, s1 + s2)

Fig. 2. Union-Find

E(x, c, y)
find(x, z)
find(y, z)
(ST1,3)

del(E(x, c, y))

E(x, c, y)
(ST2,c+4)

union(x, y)
out(x, c, y)

Fig. 3. Minimum Spanning Tree

the invariant that for any x there is at most one y satisfying find(x, y) and thatthe find map defined by the find relation implements the equivalence relation
defined by the input union assertions. This implementation uses greedy pathcompression. The number of visible prefix firings (the abstract running time for
fixed priority rules) is O(n log n) where n is the number of union operations.Figure 3, also taken from [2], shows an

O(n log n) LDP implementation of aminimum spanning tree algorithm based on this implementation of union-find.

The input is a set of assertion of the form E(x, c, y) and the output spanningtree is a set of assertions of the form out(

x, c, y). An O(n\Lambda (n)) implementation

442 David McAllester
of union-find can also be given in LDP but requires that union assertions carrytime stamps and that a time-stamped find request is made each time one accesses
the find relation. This makes it difficult to write rules, such as rule ST1 in fig-ure 3, whose antecedents notice changes in the find map. In many applications
of union-find, such as the spanning tree algorithm in figure 3, the union-findprocessing time is dominated by other computations (such as the priority queue
implicit in rule ST2) and the O(n log n) implementation of union-find suffices.

5 Polymorphic Type Inference
Polymorphic type inference can be formulated for the lambda calculus extendedwith an explicit let constructor and a constant for the numeral zero. The lambda
calculus with let and zero is the set of expressions defined by the followinggrammar.

x \Delta  termvar(id)

e \Delta  x | 0 | lambda(x, e) | let(x, e1, e2) | apply(e1, e2)

Note that this grammar represents each expression of the lambda calculus ex-plicitly as an LDP data structure. Variables are represented by LDP data struc-

tures of the form termvar(id) where id is the "identifier" of the variable and canbe any LDP data structure (ground term). It is interesting to note that one
can easily write LDP programs that "gensym" new variables to avoid variablecapture in operations such as substitution. More specifically, in implementing
substitution on the lambda calculus one must be able to construct a variable
x that is guaranteed not to occur in a given expression e. But note that thevariable

termvar(e) cannot occur in e. I will write (e1 e2) as an abbreviationfor apply(

e1, e2) and write (f e1, e2, . . . , en) as an abbreviation for the "Cur-ried application" (

. . . ((fe1)e2) . . . en). I will also sometimes write O/x. e as anabbreviation for lambda(

x, e). I will also write O/x1x2. e for O/x1. O/x2. eThe type inference algorithm described here runs in

O(n\Lambda (n) + dn) timewhere
d is the scheme depth of the input program. To define scheme depth wefirst say that the position of

e1 in the let expression let(x, e1, e2) is a schemeformation position. As described below, these scheme formation positions are

the position in the program where polymorphic type inference creates a typescheme. The scheme depth of a position

 in the program is the number ofscheme formation positions above
. For example, one might have a sequenceof module definitions where each module can use definitions from the previous

module. Conceptually, the sequence of module definitions is analogous to thefollowing pattern of let bindings.

let(m1, M2, let(m2, M2, let(m3, M3 . . . )))
Each module Mi might consist of a sequence of procedure definitions analogousto the following let bindings.

Mi = let(f1, F1, let(f2, F2, let(f3, F3, . . . )))

A Logical Algorithm for ML Type Inference 443
Each procedure might be defined with a sequence of let expressions as follows

fi = O/x1, . . . xn let(y1, e1, let(y2, e2, let(y3, e3, . . .)))
Typically the expressions ei that give the values of let-bound variables insideindividual procedure definitions do not themselves contain let expressions. So an

arbitrarily long sequence of dependent models, each consisting of an arbitrarilylong sequence of dependent procedure definitions, each consisting of an arbitrar-
ily sequence of dependent let expressions, only has scheme depth 3. It seemsunlikely that the scheme depth ever exceeds 5 in human written programs.

Hindley-Milner type inference is a process by which the various variables andsubexpressions of a given input expression are assigned types. Here we consider
only one primitive data type -- the type of integers. Finite (nonrecursive) typesare generated by the following grammar.

ff \Delta  \Lambda  | int | arrow(ff1, ff2)
In this grammar \Lambda  ranges over type variables. The type arrow(ff1, ff2) is the typeassigned to procedures which take as an argument a value in the type

ff1 andproduce as an output a value in the type
ff2. I will often write ff1 \Pi  ff2 as analternate notation for
arrow(ff1, ff2) and write ff1 * * * * * ffn \Pi  oe as an alternatenotation for
ff1 \Pi  (ff2 \Pi  * * * oe). If ff is the type oe \Pi  $ then I will sometimeswrite
domain(ff ) for the type oe and range(ff) for the type $.The basic constraint in type inference is that in each application (

e1, e2) theexpression
e1 must be assigned a type of the form ff \Pi  oe where e2 is assigned thetype
ff and the application (e1, e2) is assigned the type oe. Type inference is doneby unification of type expressions. For each application (

e1 e2) we introduce thefollowing constraints.

type.of(e1, 1) = domain(type.of(e1, 1)) \Pi  range(type.of(e1, 1))
type.of(e2, 2) = domain(type.of(e1, 1))
type.of((e1 e2), 3) = range(type.of(e1, 1))

Expressions of the form type.of(e, ) are treated as type variables. In poly-morphic type inference different occurrences of the same expression can be as-

signed different types as will be explained below. In a type variable of the form
type.of(e, ) we have that  names a position in the input expression at whichthe expression

e occurs. Unification is used to construct the most general solutionto the type constraints.

Figure 4 gives the top level structure of the polymorphic type inference pro-cedure. The procedure definitions are abbreviations for inference rules given in
figure 5. The input to the top level procedures in figure 4 is a single expressionof the form do(analyze(

e, root)) where e is a closed lambda expression whosevariables have been renamed so that each variable is bound at only one position

in e. The basic constraints for application are installed by the first procedurein figure 4. The input expression

e is typable by polymorphic recursive typesif and only if the rules in figures 4, 6, 7, 8, and 9 do not generate clash. These

444 David McAllester
procedure analyze(apply(e1, e2), j)

[analyze(e1, down(j, 1));

analyze(e2, down(j, 2));
assert.declare.arrow(type.of(e1, down(j, 1)));
assert.eq.types(type.of(e2, down(j, 2)),

domain(type.of(e1, down(j, 1))));
assert.eq.types(type.of(apply(e1, e2), j),

range(type.of(e1, down(j, 1))))]

procedure analyze(0, j)[

assert.int.type(type.of(0, j))]

do(analyze(termvar(id), j))
monomorphic(termvar(id))
(A1, 4)

eq.types(type.of(termvar(id), j), type.of(termvar(id)))
done(analyze(termvar(id), j))

do(analyze(termvar(id), j))
polymorphic(termvar(id), s)
(A2, 4)

make.copy(s, j)
eq.types(type.of(termvar(id), j), , copy(s, j))
done(analyze(termvar(id), j))

procedure analyze(lambda(x, e), j)

[assert.monomorphic(x);

analyze(e, down(j, 2));
assert.declare.arrow(type.of(lambda(x, e), j));
assert.eq.types(type.of(x), domain(type.of(lambda(x, e), j)));
assert.eq.types(type.of(e, down(j, 2)), range(type.of(lambda(x, e), j))) ]

procedure analyze(let(x, e1, e2), j)

[analyze(e1, down(j, 2));

assert.make.scheme(type.of(e1, down(j, 2)), j);
assert.polymorphic(x, sch(type.of(e1, down(j, 2)), j));
analyze(e2, down(j, 3));
eq.types(type.of(let(x, e1, e2), j), type.of(e2, down(j, 3))) ]

Fig. 4. Hindley-Milner Type Inference: Fundamental Structure

figures give a complete machine-executable implementation of the polymorphictype inference procedure. This procedure allows recursive types by not perform-
ing any occurs-check in the unification procedure. Recursive types can be viewedas infinite type expressions with a finite number of distinct subexpressions. For

A Logical Algorithm for ML Type Inference 445
do(f(t1, . . . , tn))
(P1,4)

do([a1; . . . ; an])

do(f(t1, . . . , tn))
done([a1; . . . ; an])
(P2,4)

done(f(t1, . . . , tn))

do(sequence(a1, a2))
(P3,4)

do(a1)

do(sequence(a1, a2))
done(a1)
(P4,4)

do(a2)

do(sequence(a1, a2))
done(a1)
done(a2)
(P5,4)

done(sequence(a1, a2))

do(assert.P(x1, . . . , xn))
(P6 1)

P (x1, . . . , xn)
done(assert.P(x1, . . . , xn))

Fig. 5. Implementing Procedures. Defining f(t1, . . . , tn) to be [a1; . . . ; an] abbreviates
the rules P1 and P2 above. The notation [a1; a2] abbreviates sequence(a1, a2) and[

a1; a2; . . . ; an] abbreviates [a1; [a2; . . . ; an]]. Rule P6 is assumed to be present for every
action constructor of the form assert.P.

eq.types(_, *)
(U1,1)

type.var(_)
type.var(*)
union(_, *)

declare.arrow(_)
(U2, 1)

type.var(_)
type.var(domain(_))
type.var(range(_))
decl.struct(_, domain(_), range(_))

int.type(_)
(U3, 1)

type.var(_)

type.var(\Lambda )
(U4,1)

no.struct(\Lambda )

struct(\Lambda , ,, ss)
(U5,1)

del(no.struct(\Lambda ))

decl.struct(\Lambda , ,, ss)
no.struct(\Lambda )
(U6,2)

struct(\Lambda , ,, ss)

decl.struct(\Lambda , ,, ss)
struct(\Lambda , ,\Delta , ss\Delta )
(U7,1)

union(,, ,\Delta )
union(ss, ss\Delta )

struct(\Lambda , ,, ss)
find.arc(\Lambda , \Lambda \Delta )
(U8,1)

decl.struct(\Lambda \Delta , ,, ss)

int.type(_)
find.arc(_, *)
(U9,1)

int.type(*)

struct(\Lambda , ,, ss)
int.type(\Lambda )
(U10,1)

clash

Fig. 6. Type Unification

446 David McAllester
example there is a recursive type ff satisfying the following fixed point equation.

ff = int \Pi  ff
It is possible to add occurs-check rules if one wishes to avoid recursive types.In Hindley-Milner type inference, expressions can have polymorphic types
-- different occurrences of a procedure can have different types. For example,consider a composition operator comp satisfying the following equation.

((comp f g) x) = (f (g x))
For any types ff1 and ff2 the composition operator can be assigned the type(

ff1 \Pi  ff2) * (ff1 \Pi  ff2) \Pi  (ff1 \Pi  ff2). For the polymorphic composition operatorthe types

ff1 and ff2 can be different for different uses of the operator. For example,the same composition operator can be used to compose functions from integers

to integers in one place and to compose functions on floating points in anotherplace (in a language with floating point numbers). The type of the polymorphic
composition operator is often written as follows.

comp : i\Lambda , &, (\Lambda  \Pi  &) * (\Lambda  \Pi  &) \Pi  (\Lambda  \Pi  &)
In Hindley-Milner type inference polymorphism is restricted to let-boundvariables. If the top level expression does not involve let then the rules in figures
4 and 6 suffice for type inference. Figures 7, 8, and 9 handle polymorphism. Thebasic idea is that when the procedure analyzes a let expression it makes the type
of the let-bound variable polymorphic. This is done by converting the type to atype scheme which is then instantiated with fresh type variables for each use of
the let bound variable.It is instructive to first consider the case of expressions without let. In this
case we can ignore rule A2 and the last procedure definition in figure 4. Sincethe rules for polymorphic type inference do not involve variable-priority rules, to
analyze the run time of the procedure it suffices to count the number of visibleprefix firings. The number of prefix firings of the rules implicit in figure 4 is
proportional to the number of expressions analyzed, i.e., the number of positionsoccupied in the input expression. The number of occupied positions corresponds
to the written length of the expression and I will simple call this number "thesize of the input" and denote it by

n. To finish the analysis it suffices to countthe number of visible prefix firings in the union-find rules in figure 2 and of

the unification rules in figure 6. The input to the unification module consistsof assertions of the form declare

.arrow(oe), int.type(oe) and eq.types(oe, ff ). Theoutput is the assertion
clash if the constraints are unsatisfiable and otherwisethe output consists of the union-find equivalence relation on the set of type

expressions plus assertions of the form struct(\Lambda , &, $) which implies that \Lambda  =
& \Pi  $. The rules maintain the invariant that for any \Lambda  there is at most oneassertion of the form struct(

\Lambda , &, $). Note that, with the exception of ruleU2, the unification rules do not introduce new expressions. In fact the number

of expressions generated by the rules is no more than linear in the number

A Logical Algorithm for ML Type Inference 447
type.var(\Lambda )
(MS1,1)

unbound(\Lambda )

arrow.type(\Lambda )
(MS2,1)

del(unbound(\Lambda ))

int.type(\Lambda )
(MS3,1)

del(unbound(\Lambda ))

make.scheme(_, j)
find(_, _\Delta )
struct(_\Delta , *, ss)
(MS4,1)

sch.struct(sch(_, j), sch(*, j), sch(ss, j))
make.scheme(*, j)
make.scheme(ss, j)

make.scheme(_, j)
find(_, _\Delta )
int.type(_\Delta )
(MS5,1)

int.scheme(sch(_, j))

make.scheme(_, j)

find(_, _\Delta )
unbound(_\Delta )
scheme.depth(_\Delta , i)
scheme.depth(j, j)
i < j
(MS6,2)

free.var(sch(_, j), _\Delta )

make.scheme(_, j)
find(_, _\Delta )
unbound(_\Delta )
scheme.depth(_\Delta , i)
scheme.depth(j, j)
j <= i
(MS7,2)

bound.var(sch(_, j), sch(_\Delta , j))

Fig. 7. Making a Type Scheme

make.copy(s, j)
sch.struct(s, t, v)
(C1,3)

declare.arrow(copy(s, j))
eq.types(domain(copy(s, j)), copy(t, j))
eq.types(range(copy(s, j)), copy(v, j))
make.copy(t, j)
make.copy(v, j)

make.copy(s, j)
int.scheme(s)
(C2,3)

int.type(copy(s, j))

make.copy(s, j)
free.var(s, _\Delta )
(C3,3)

eq.types(copy(s, j), _\Delta )

make.copy(s, j)
bound.var(s1, s2)
(C4,3)

eq.types(copy(s1, j), copy(s2, j))

Fig. 8. Copying a Type Scheme at Program Position j

448 David McAllester

(D1,1)

scheme.depth(root, 0)

do(analyze(let(x, e1, e2), j))
scheme.depth(j, d)
(D2,1)

scheme.depth(down(j, 2), d + 1)
scheme.depth(down(j, 3), d)

do(analyze(lambda(x, e), j))
scheme.depth(j, d)
(D3,1)

scheme.depth(down(j, 2), d)

do(analyze(apply(e1, e2), j))
scheme.depth(j, d)
(D4,1)

scheme.depth(down(j, 1), d)
scheme.depth(down(j, 2), d)

type.var(type.of(e, j))
scheme.depth(j, d)
(D5,1)

scheme.depth(type.of(e, j), d)

type.var(copy(s, j))
scheme.depth(j, d)
(D6,1)

scheme.depth(copy(s, j), d)

scheme.depth(x, d1)
scheme.depth(x, d2)
d1 < d2
(D7,1)

del(scheme.depth(x, d2))

scheme.depth(x, d)
find.arc(x, y)
(D8,2)

scheme.depth(y, d)

scheme.depth(\Lambda , d)
struct(\Lambda , ,, ss)
(D9,3)

scheme.depth(,, d)
scheme.depth(ss, d)

Fig. 9. The Computation of Scheme Depth

of assertions input to the unification process. This implies that the numberof (uncompressed) underlying find arcs in the union-find assertions is also no
more than linear in the number of inputs to unification. The number of prefixfirings of the unification rules can be seen to be proportional to the number
of find arcs and hence the number of prefix firings of the unification rules isproportional to the number of input assertions. In let-free programs the number
of inputs to unification is proportional to the number of positions occupied inthe input expression. Because the unification rules use only the uncompressed

A Logical Algorithm for ML Type Inference 449
find arcs a lazy path compression unification algorithm can be used. This gives
n\Lambda (n) type inference procedure for let-free programs. The lazy path compressionalgorithm can also be used with the full polymorphic type inference procedure.

While linear time unification is appropriate for let-free programs, in the presenceof polymorphism one must alternate unification processing with type scheme
construction and this alternation prevents the use of linear time unification.To understand the subtleties of the polymorphic case consider the following
expression.

O/y.let(f, O/x.y, e)

In this case multiple occurrences of f in e can have different domain types butmust all have the same range type. The range type of

f is the type of thelambda-bound variable
y which can be determined from the contexts into whichthe overall lambda expression is embedded. In the Hindley-Milner type inference

algorithm we assign f and y the following type.

y : &
f : i\Lambda , \Lambda  \Pi  &

Note that in this type \Lambda  is a bound type variable while & is a free type variable.In general, when the procedure constructs a type scheme it must distinguish type

variables which can be universally quantified (the bound type variables) fromthe type variables which must be left free. Each type variable has a position in
the program where it is created. In a let expression let(x, e1, e2) type variablescreated in

e1 are candidates for quantification in the type scheme. However, theexpression

e1 usually contains free variables such as y in the above example.The types associated with the free variables in

e1 cannot be generalized. A typethat is provably equal to a range type or a domain type of an ungeneralizable

type is also ungeneralizable. To determine which types are generalizable we usthe notion of scheme depth defined earlier. The formal definition of the scheme
depth of a position is given by rules D1 through D4 in figure 9. We first saythat a type

\Lambda  is provably a subexpression of a type & if \Lambda  is provably equalto
& (the union-find structure equates them) or \Lambda  is of the form domain($) orrange(

$) where $ is provably a subexpression of &. Each type variable has acreation depth and an inferred depth. The inferred depth of

\Lambda  is the creationdepth of the shallowest type variable
& such that \Lambda  is provably a subexpressionof
&. Rules D5 and D6 in figure 9 install the creation depth of type variablesand rules D7, D8, and D9 compute inferred depth. A variable can be bound by

the universal quantifier at scheme creation time as long as its inferred depth isno less than the depth of position at which the scheme is being created.

Figure 7 gives rules for creating type schemes. We leave it to reader to deci-pher the meaning of these rules. One point concerning their complexity should
be noted however. The find assertions in the antecedents of these rules obvi-ously should only be instantiation with the find values that are in place when
the scheme is constructed. This can be achieved by adding an antecedent of theform

active() to each rule where this assertion is only true during the computa-tion of the scheme for position

. With this modification it is easy to check that

450 David McAllester
the number of visible prefix firings of the rules in figure 7 is proportional to thenumber of nodes in the scheme created. For programs of bounded order and arity
there is a constant upper bound on the number of nodes in each scheme. Therules for making a scheme are consistent with the use a lazy path compression
version of union-find.The rules in figure 8 copy a type scheme at a particular program location.
The number of visible prefix firings of these rules is proportional to the size of thescheme being copied. If schemes have a bounded size then the total time taken
to create and copy schemes is proportional to the size of the input program (thenumber of occupied positions). In this case the total number of type variables
is also proportional the size of the input program so the time spent inside alazy path compression version of union-find is

O(n\Lambda (n)). The number of visibleprefix firings of rule D7, D8, and D9 in bounded by the size of the program (the

number of type variables) times the number of depths, i.e., the scheme depth ofthe deepest scheme position. This gives a total running time of

O(n\Lambda (n) + dn)where
n is the length of the program and d is the scheme depth of the program.

6 Conclusions
This paper is a case study in the use of bottom-up logic programming extendedwith priorities and deletions in presenting and analyzing algorithms. Efficient
Hindley-Miler type inference seems to be a challenging algorithm to both formu-late and analyze. The algorithm presented here is essentially the one used in ML
and Caml implementations. Although beauty is in the eye of the beholder, it canat least be argued that the inference rule presentation of the algorithm is clearer
both respect to its correctness and with respect to its running time than is anypresentation of the algorithm using traditional control structures. Of course the
top level procedure in figure 4 uses classical procedural control structures. How-ever, the implementation of unification and the depth propagation seems easier
in inference rules that classical control structures. Also, inference rules provide adifferent form of modularity than is possible in classical programs. The unifica-
tion algorithm can be given as a separate module that uses a union-find modulewithout modification. Depth propagation can be done with yet another module.
The complexity analysis of these modules can be done separately (modularly).This modularization seems difficult with classical control structures. The use of
inference rules also seems to facilitates the treatment of recursive types.In comparing the algorithm in figures 4, 6, 7, 8, and 9 to other presentations
it is important to keep in mind that the these figures contain machine readablecode. Machine readable code is usually less clear than informal descriptions
of algorithms written in English. A fair comparison would require comparingthese figures to a machine-readable implementation in an some more traditional
programming language.Bottom-up logic programming is clearly not the best tool for all applications.
However, it is hoped that this case study demonstrates that presenting and

A Logical Algorithm for ML Type Inference 451
analyzing complex algorithms as bottom-up logic programs is indeed feasibleand perhaps provides greater clarity than traditional approaches.

References
1. H. Ganzinger and D. McAllester. A new meta-complexity theorem for bottom-uplogic programs. In Proc. International Joint Conference on Automated Reasoning,

volume 2083 of Lecture Notes in Computer Science, pages 514-528. Springer-Verlag,2001.
2. H. Ganzinger and D. McAllester. Logical algorithms. In Proc. International Con-

ference on Logic Programming (ICLP), 2002.3. F. Henglein. Type inference with polymorphic recursion. Transactions on Program-

ming Languages and Systems (TOPLAS), 15(2):253-289, 1993.4. A. Kfoury, J. Tiuryn, and P. Urzyczyn. Ml typability is dexptime complete. In
Proceedings of the 15th Colloquium on Trees in Algebra and Programming (CAAP),pages 468-476. ACM, 1990.
5. H. Mairson. Deciding ml typability is complete for deterministic exponential time.

In ACM Symposium on Principles of Programming Languages. Association for Com-puting Machinery, 1990.

6. David McAllester. On the complexity analysis of static analyses. Journal of theACM (JACM), 49(4):512-537, 2002. A short version appeared in SAS99.
7. Robin Milner. A theory of type polymorphism in programming. JCSS, 17:348-375,

1978.8. Didier R'emy. Extending ML type system with a sorted equational theory. Research

Report 1766, Institut National de Recherche en Informatique et Automatisme, Roc-quencourt, BP 105, 78 153 Le Chesnay Cedex, France, 1992.