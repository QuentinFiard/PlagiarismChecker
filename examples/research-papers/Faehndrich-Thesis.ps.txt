

BANE: A Library for Scalable Constraint-Based Program Analysis

by
Manuel Alfred F"ahndrich

B.E. (Ecole Polytechnique F'ed'erale de Lausanne) 1993

M.S. (University of California at Berkeley) 1995

A dissertation submitted in partial satisfaction of the

requirements for the degree of

Doctor of Philosophy

in
Computer Science

in the
GRADUATE DIVISION

of the
UNIVERSITY of CALIFORNIA at BERKELEY

Committee in charge:

Professor Alexander Aiken, Chair
Professor Susan Graham
Professor Hendrik Lenstra

1999

The dissertation of Manuel Alfred F"ahndrich is approved:

Chair Date

Date
Date
University of California at Berkeley

1999

BANE: A Library for Scalable Constraint-Based Program Analysis

Copyright 1999

by
Manuel Alfred F"ahndrich

1
Abstract

BANE: A Library for Scalable Constraint-Based Program Analysis

by
Manuel Alfred F"ahndrich
Doctor of Philosophy in Computer Science

University of California at Berkeley

Professor Alexander Aiken, Chair

Program analysis is an important aspect of modern program development. Compilers use
program analysis to prove the correctness of optimizing program transformations. Static error detection tools use program analysis to alert the programmer to the presence of potential
errors. This dissertation focuses on the expressiveness and implementation of constraintbased program analyses, i.e., analyses that are expressed as solutions to a system of constraints. We show that structuring the implementation of program analyses around a library
of generic constraint solvers promotes reuse, gives control over precision-efficiency tradeoffs,
and enables optimizations that yield orders of magnitude speedups over standard implementations.

The first part of the dissertation develops the formalism of mixed constraints which
provides a combination of several constraint formalisms with distinct precision-efficiency
tradeoffs. We provide a semantics for constraints and constraint resolution algorithms. The
second part of the dissertation describes an implementation of mixed constraints and a
number of novel techniques to support the practical resolution of large constraint systems.
We give empirical results supporting the claims of scalability, reuse, and choice of precisionefficiency tradeoffs provided by the mixed constraint framework.

Professor Alexander AikenDissertation Committee Chair

iii
To my wife Barbara,
for the patience and support.

iv
Contents
List of Figures vii
List of Tables ix
1 Introduction 1
2 Background 6

2.1 An Ideal Model for Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 Set Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

2.2.1 Term-Set Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2.2 An Ideal Model for Set Expressions . . . . . . . . . . . . . . . . . . . 9

I Mixed Constraints 13
3 Mixed Constraints 15

3.1 A Motivating Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.2 Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.3 Sorts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

4 Semantics 22

4.1 Value Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.2 Interface Paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
4.3 Type Collections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
4.4 Semantic Relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.5 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.6 Discussion and Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . 39

5 Constraint Resolution 45

5.1 Inductive Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
5.2 Set Constraint Resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

5.2.1 Upward-Closure and Negation . . . . . . . . . . . . . . . . . . . . . 46
5.2.2 L-Intersection Simplification . . . . . . . . . . . . . . . . . . . . . . . 52
5.2.3 Set Resolution Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
5.3 Term and FlowTerm Resolution . . . . . . . . . . . . . . . . . . . . . . . . . 55

v
5.4 Row Resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

5.4.1 Domain Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
5.4.2 General Row Resolution . . . . . . . . . . . . . . . . . . . . . . . . . 60
5.4.3 Splitting of Closed Rows . . . . . . . . . . . . . . . . . . . . . . . . . 62
5.4.4 Splitting of Minimal and Maximal Rows . . . . . . . . . . . . . . . . 62
5.5 Inductive Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

5.5.1 Level Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
5.5.2 Contractive Systems of Equations . . . . . . . . . . . . . . . . . . . 75
5.5.3 Contractive Term-Equations . . . . . . . . . . . . . . . . . . . . . . . 77
5.5.4 Generating Set and FlowTerm-Equations . . . . . . . . . . . . . . . . 78
5.5.5 Solutions For Row-Constraints . . . . . . . . . . . . . . . . . . . . . 85

6 Practical Aspects of Constraint Resolution 87

6.1 Conditional Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
6.2 Graph Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92

6.2.1 Transitive Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . 99
6.2.2 Condition resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.3 Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
6.4 Discussion and Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . 103

II BANE 107
7 BANE: An Implementation of Mixed Constraints 109

7.1 Constraint Graph Representation . . . . . . . . . . . . . . . . . . . . . . . . 109

7.1.1 Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
7.1.2 Edges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.1.3 Alias Edges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
7.1.4 Expression Hashing and Bound Representation . . . . . . . . . . . . 113
7.1.5 Online Cycle Detection and Elimination . . . . . . . . . . . . . . . . 114
7.1.6 General Constraint Resolution Algorithm . . . . . . . . . . . . . . . 120
7.2 Set Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122

7.2.1 Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
7.2.2 Projection Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
7.3 Term Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
7.4 FlowTerm Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
7.5 Row Sorts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
7.6 Polymorphic Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130

7.6.1 Multiple Constraint Graphs . . . . . . . . . . . . . . . . . . . . . . . 131
7.6.2 Quantification and Instantiation . . . . . . . . . . . . . . . . . . . . 132
7.6.3 Simplification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132

vi
8 Example Analyses 136

8.1 Points-to Analysis for C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136

8.1.1 Re-Formulation using Standard Set Constraints . . . . . . . . . . . . 138
8.1.2 Constraint Generation . . . . . . . . . . . . . . . . . . . . . . . . . . 139
8.1.3 A Complete Points-to Example . . . . . . . . . . . . . . . . . . . . . 142
8.2 ML Exception Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145

8.2.1 Type and Constraint Generation . . . . . . . . . . . . . . . . . . . . 149
8.2.2 An Example Inference . . . . . . . . . . . . . . . . . . . . . . . . . . 153
8.2.3 Exception Inference for Full SML . . . . . . . . . . . . . . . . . . . . 155
8.2.4 Precision-Efficiency Variations . . . . . . . . . . . . . . . . . . . . . 157
8.2.5 Related Exception Work . . . . . . . . . . . . . . . . . . . . . . . . . 158

9 Experiments 159

9.1 Measurement Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
9.2 Online Cycle Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160

9.2.1 Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
9.3 Projection Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
9.4 Standard Form v.s. Inductive Form . . . . . . . . . . . . . . . . . . . . . . . 168

9.4.1 Standard Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
9.4.2 Inductive Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
9.4.3 Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
9.4.4 TLB on demand . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
9.5 Precision-Efficiency Tradeoffs . . . . . . . . . . . . . . . . . . . . . . . . . . 177

10 Related Work 184

10.1 The Cubic-Time Bottleneck . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
10.2 Sub-Cubic Time Formalisms . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
10.3 Program Analysis Frameworks . . . . . . . . . . . . . . . . . . . . . . . . . 186

11 Conclusions 188
Bibliography 190

vii
List of Figures

1.1 Constraint-based approach to program analysis . . . . . . . . . . . . . . . . 2
3.1 Set operations in \Pi Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2 Operations \Pi Row(s) for sort Row(s) . . . . . . . . . . . . . . . . . . . . . . . 20

4.1 Development of lattices (Ss; `s) . . . . . . . . . . . . . . . . . . . . . . . . . 23
5.1 L-intersection simplification . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
5.2 Resolution rules for Set-constraints. . . . . . . . . . . . . . . . . . . . . . . . 56
5.3 Resolution of FlowTerm-constraints. . . . . . . . . . . . . . . . . . . . . . . 56
5.4 Resolution of Term-constraints. . . . . . . . . . . . . . . . . . . . . . . . . . 57
5.5 Resolution of Row-constraints (simple cases). . . . . . . . . . . . . . . . . . 67
5.6 Resolution of Row-constraints (complex cases). . . . . . . . . . . . . . . . . 68
5.7 Simplification of domain constraints (complete) . . . . . . . . . . . . . . . . 69

6.1 Transforming constraints with conditional expressions into conditional constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
6.2 Structural rewrite rule for conditional constraints . . . . . . . . . . . . . . . 91
6.3 Structural rewrite rule for inconsistent constraint . . . . . . . . . . . . . . . 91
6.4 Unnecessary edges in full graph closure . . . . . . . . . . . . . . . . . . . . . 97

7.1 Example frequency of bound sizes . . . . . . . . . . . . . . . . . . . . . . . . 113
7.2 Cycle detection in an example graph . . . . . . . . . . . . . . . . . . . . . . 116
7.3 The two kinds of cycles detected . . . . . . . . . . . . . . . . . . . . . . . . 117
7.4 Possible 3-cycles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
7.5 Transitive edges to projection patterns . . . . . . . . . . . . . . . . . . . . . 123
7.6 Specialized Term resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
7.7 Specialized FlowTerm-resolution . . . . . . . . . . . . . . . . . . . . . . . . . 129

8.1 Example points-to graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
8.2 Constraint generation for Andersen's analysis . . . . . . . . . . . . . . . . . 140
8.3 More complex C example . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
8.4 Points-to graph of program in Figure 8.3 . . . . . . . . . . . . . . . . . . . . 142
8.5 Type and exception inference rules for expressions . . . . . . . . . . . . . . 148
8.6 Type and exception inference rules for patterns . . . . . . . . . . . . . . . . 149

viii

8.7 Example Mini-ML program . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
9.1 IF without cycle elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
9.2 Analysis times with cycle detection and oracle . . . . . . . . . . . . . . . . . 162
9.3 Speedups through online cycle detection . . . . . . . . . . . . . . . . . . . . 163
9.4 Graph Schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
9.5 Paths from sources to Y to sinks . . . . . . . . . . . . . . . . . . . . . . . . 165
9.6 Paths added through projection merging . . . . . . . . . . . . . . . . . . . . 166
9.7 Speedups through projection merging . . . . . . . . . . . . . . . . . . . . . 168
9.8 Example constraints in SF and IF . . . . . . . . . . . . . . . . . . . . . . . . 170
9.9 SF and IF without cycle elimination . . . . . . . . . . . . . . . . . . . . . . 173
9.10 Relative execution times of Shapiro and Horwitz's SF implementation of C

points-to analysis (SH) over SF-Plain . . . . . . . . . . . . . . . . . . . . . . 173
9.11 Analysis times with cycle detection and oracle . . . . . . . . . . . . . . . . . 174
9.12 Speedups through online cycle detection . . . . . . . . . . . . . . . . . . . . 174
9.13 Speedups through inductive form . . . . . . . . . . . . . . . . . . . . . . . . 175
9.14 Fraction of variables on cycles found online . . . . . . . . . . . . . . . . . . 175
9.15 Final graph sizes of SF and IF . . . . . . . . . . . . . . . . . . . . . . . . . . 175
9.16 Comparison of the Base experiments . . . . . . . . . . . . . . . . . . . . . . 183

ix
List of Tables

9.1 Benchmark data common to all experiments . . . . . . . . . . . . . . . . . . 160
9.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
9.3 Benchmark data for IF-Plain and IF-Oracle . . . . . . . . . . . . . . . . . . 162
9.4 Benchmark data for IF-Online . . . . . . . . . . . . . . . . . . . . . . . . . . 163
9.5 Cycle detection statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
9.6 Benchmark data for IF-PM . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
9.7 Points-to Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
9.8 Benchmark data for SF-Plain and SF-Oracle . . . . . . . . . . . . . . . . . . 172
9.9 Benchmark data for SF-Online . . . . . . . . . . . . . . . . . . . . . . . . . 174
9.10 Benchmark data for on demand TLB . . . . . . . . . . . . . . . . . . . . . . 176
9.11 Precision-efficiency variations for exception analysis . . . . . . . . . . . . . . 177
9.12 ML Benchmarks for exception analysis . . . . . . . . . . . . . . . . . . . . . 179
9.13 Experiments for exception analysis . . . . . . . . . . . . . . . . . . . . . . . 179
9.14 Benchmark data of all implementations and experiments . . . . . . . . . . . 180
9.15 Benchmark data for Term-Set Base experiment with Cartesian-closed constructors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182

x

Acknowledgements
My deepest thanks go to my advisor Alex Aiken, without whose support and encouragement
this dissertation would have never seen the light of day. His never wavering optimism and
cheerfulness kept me going through those unavoidable lows of graduate school and I hope I
was able to take on some of these qualities myself.

I'm also deeply indebted to my Berkeley fellow graduates Jeff Foster and Zhendong
Su for their role as early adopters of BANE. Their work, discussions, and feedback helped
me focus on the important aspects of BANE and spurned me on to find yet another factor of
ten. Other Berkeley people I'd like to thank for generating ideas through various discussions
are David Gay, Raph Levien, Ben Liblit, Boris Vaysman, Remzi Arpaci, and Andrew Begel.

Finally, I'd like to thank my other committee members Sue Graham and Hendrik
Lenstra for their time and useful feedback.

1
Chapter 1
Introduction

This dissertation supports the thesis that structuring the implementation of program analyses around a library of generic constraint solvers promotes reuse, gives control
over precision-efficiency tradeoffs, and enables optimizations that yield orders of magnitude
speedups over standard implementations.

The work described here is part of an ongoing research project on program analysis at the University of California at Berkeley under the supervision of Professor Alexander
Aiken. The goals of this project are to advance the state of the art of program analysis
by 1) developing novel sophisticated program analyses for compiler optimizations or error
detection, and 2) finding novel ways of implementing program analyses (new and old) so as
to improve their scaling behavior. My involvement in this research has resulted in BANE,
a library for constraint-based program analysis. BANE addresses the need of our research
group to rapidly develop prototypes of program analysis ideas and perform experimentation, without spending a huge effort on implementation. Without respect to efficiency
and scalability, prototypes could in principle be developed reasonably cheaply. However,
such naive implementations of prototypes do not answer interesting questions like whether
in practice an analysis scales. Furthermore, poor prototype efficiency substantially slows
down experimentation since the measurement cycle becomes intolerably long.

Experimentation is important, since paper designs of program analyses often do
not prove practical for two reasons. First, the computed information may be too approximate on real programs to be useful. Second, the running time and especially the scaling
behavior of the analysis may make it impractical. (Analysis designers sometimes ignore the
asymptotic complexity of their algorithms, hoping that the worst case bound is not met
in practice.) Shortcomings in precision and efficiency are usually only found after a substantial effort has been invested in writing and tuning an implementation. At that point,
it may be difficult to change the precision and or efficiency of the analysis, since tuning
often results in monolithic, obscure code that is hard to change. For the same reason, such
implementations are also hard to maintain and reuse.

Constraint-based implementations of program analyses need not fall into this trap.
Figure 1.1 shows graphically the organization of a constraint-based program analysis. The
goal of any static program analysis is to infer information about a program given in some
source representation. In constraint-based program analysis, this information is expressed

2

Constraint
Generator

Static
Info Mapping

?

Solver`

Source

Solutions

Figure 1.1: Constraint-based approach to program analysis
as the solutions to a system of constraints derived from the program source. Constraintbased program analysis thus proceeds in three steps: 1) constraints are generated according
to a set of rules from the source, 2) constraints are solved, and 3) the static information
is extracted from the solutions. A constraint-based program analysis is uniquely defined
by the constraint generation rules and the mapping from solutions to the desired static
information. The analysis is therefore independent of the constraint resolution which is the
bulk of the implementation. The constraint resolution can be thought of as a black box,
where constraints are entered on one side, and solutions come out on the other side. The
black box interface is well defined, namely by the constraint language and the meaning of
the constraints. As a result, the black box can be implemented completely independently
of any particular program analysis, as long as the implementation produces the solutions
to the given constraints. It is this clean separation of constraint generation and constraint
resolution that naturally supports reuse and non-intrusive tuning. Constraint resolution
engines can be factored into libraries and their code tuned independently of any particular
constraint generation code. The narrow interface to the constraint engines further enables
more aggressive tuning than in monolithic implementations. For example, the representation of constraints and the resolution algorithm can be completely changed without affecting
any client analyses.

BANE is based on a novel constraint formalism called mixed constraints. Mixed
constraints provides inclusion constraints between mixed expressions. Mixed expressions

3
are a combination of expressions from a number of base constraint formalisms. Mixed
constraints then synthesizes constraint relations out of the constraint relations of the base
formalisms. The synthesis of the mixed constraint relations is parameterized by constructor
signatures that specify how expressions of the base formalisms are combined into mixed
expressions. If the base formalisms have distinct precision-efficiency characteristics, then
it possible to choose the precision-efficiency tradeoffs of mixed constraints explicitly via a
suitable set of constructor signatures.

One of the base constraint formalisms provided in BANE is set-constraints. Many
well-known program analyses can be expressed in this formalism. Resolution of set constraints in the simplest case has cubic worst case time complexity. Implementations in the
past have shown that analyses in this constraint class are feasible on small programs. Unfortunately, the resolution of set-constraint problems for larger programs quickly becomes
intractable on today's hardware. This scaling problem has resulted in a shift towards less
precise but computationally cheaper algorithms for program analyses. One contribution
of this dissertation is to provide a transition path from precise but expensive analyses to
coarser but faster analysis through the mixed constraint formalism.

Another major contribution of the work presented here is a number of implementation techniques that substantially improve practical performance of set-constraint
resolution. Even though these techniques do not lower the worst-case computational complexity, they do improve the "common" case scaling behavior of set constraints. Using these
techniques, we can solve set-constraint problems that are orders of magnitude larger than
the largest previously reported problems. Our hope is that by showing how to structure
implementations for better scaling, more sophisticated program analyses become feasible
for very large programs.

BANE has been used in three large analysis applications with several variations
produced by our research group. Furthermore, it has been used by a number of graduate
student groups to realize projects in a class on programming language semantics taught
at Berkeley. This dissertation describes the architecture of BANE, motivating engineering
tradeoffs and new implementation techniques with a series of experiments.

Consumers of Program Analysis
Program analysis computes information about a program's runtime behavior. Program
analysis can be either static, dynamic, or a combination of the two. In static program
analysis, information about all executions of a program are inferred without actually running
the program. Dynamic program analysis takes the form of trace gathering during particular
executions of programs. In this dissertation program analysis always refers to static program
analysis.

Program analysis is an important component of software development tools. There
are two main uses of program analysis. Traditionally, program analyses is an integral
component of optimizing compilers. Program optimizations are semantics preserving code
transformations. Before a compiler can perform any particular optimization, it must prove
that the transformation is semantics preserving in the particular context. For all but trivial
transformations, this proof is provided by a program analysis. For example, a procedure

4
call with late binding semantics may be transformed into a static direct call whenever the
receiver of the call can be uniquely identified through a closure or receiver class analysis.
Unfortunately, most properties of programs are undecidable in general. Program analysis
is thus always concerned with computing approximations to the actual program properties.
One often hears that an analysis is conservative, meaning that the analysis errs on the side
of safety w.r.t. the use of the inferred properties. For example, if receiver class analysis is
used to replace late bindings with static calls, the analysis must over-estimate the potential
receivers, or the program transformation may not be safe.

The use of program analysis in compilers started with the earliest optimizing Fortran compilers. Since then, the use of program analysis has become ubiquitous in compilers
across the whole language spectrum. As new programming paradigms develop, new optimization opportunities arise that result in novel program analyses. Examples that come to
mind are strictness analysis for lazy functional languages, receiver class analysis for object
oriented languages, array use analysis for functional languages. Although program analyses
in compilers have become more sophisticated since the Fortran era, the main criteria for
such analyses is efficiency, both absolute speed and scaling behavior. Many sophisticated
program analyses found in the literature have not made their way into commercial compilers
(or even into research compilers) due to bad scaling behavior.

The second use of program analysis is error detection. This application has become
progressively more important in the last decade and, at least in the author's view, will overshadow the optimization use of program analysis by far in the future. The most common
manifestation of this use is in type checkers and type inference systems. In this scenario,
program analysis guarantees that a program produces no runtime errors arising from applying primitive operations to unsuitable arguments. In contrast to its use in optimizations
where program analyses always have to be conservative by under-estimating properties,
program analyses in error detection may also err on the other side, i.e., over-estimate properties, giving rise to two camps of error detection. In the first camp, error detection proves
the absence of errors. In the second camp, it proves the existence of errors. The difference
between the two camps arises from the approximation inherent in program analyses. Error detection can be viewed as verifying a partial program specification. If a conservative
program analysis proves that a particular program satisfies a partial specification, then it
proves the absence of errors w.r.t. the specification. However, if the conservative analysis
fails to prove the specification, it is not known whether the program actually behaves in
unspecified ways, or whether the proof failed due to the approximations of the analysis. A
program analysis trying to prove the existence of an error also verifies whether a program
satisfies a particular specification. However, if the analysis finds an unspecified behavior
w.r.t. the specification, then the error is a guaranteed error. On the other hand, if no errors
are found, it is not known whether the program satisfies the specification, or whether the
absence of errors is a result of over-estimating program properties.

Program analysis in error detection can also be distinguished from program analysis for optimizations in terms of the efficiency requirements. Scaling is still an important
issue, but absolute speed is not. For example, it may be well worth running a sophisticated
analysis of a large program overnight if the analysis detects bugs or proves their absence.
Compilation cannot tolerate long analysis times because it disrupts the edit-compile-test

5
cycle and thus programmer productivity.
Outline of the Dissertation
The dissertation is organized into two parts. The first part describes the mixed constraint
formalism and its constraint resolution and is more theoretical in nature. The second part
describes implementation techniques used in BANE and experiments to validate our claims
of improved scaling. The next chapter introduces background information needed in the
first part of the dissertation.

6
Chapter 2
Background

This chapter provides background information on the ideal model for types and
set constraints.

2.1 An Ideal Model for Types
The ideal model of types was proposed by MacQueen, Plotkin, and Sethi [56, 57]. This
section summarizes the results of that work used in the development of the mixed constraint
formalism.

Types are used in programming languages to characterize certain subsets of all
runtime values. If a program expression e has a type o/ , then it is understood that any
evaluation of e results in a value v, where v is an element of the set of values denoted by
o/ . To make these notions precise, the set of runtime values V, and the subsets of V that
form the denotations of types must be characterized. This approach is referred to as the
denotational model of program semantics. Other models, such as operational models or
axiomatic models, have different notions of types.

The set of values V is usually characterized by an equation of the form

V = B [ fwrong; ?g [ (V ! V) [ (V \Theta  V) [ (V + V) (2.1)
where B stands for a set of base values, for example the natural numbers or the truth
values. The distinguished value wrong is used to give meaning to evaluations that result in
an error, for example applying a function to a value outside its domain. The special value
bottom ? is the value of a non-terminating computation. The set V ! V is the set of
functions from V to V. The set of V \Theta  V is the set of pairs of values from V, and finally
the coalesced sum V + V is the set of pairs hi; vi, where v 2 V \Gamma  f?g, and i = 1; 2. The
index1 i specifies from which set in the sum the value v is taken.

Sets V that satisfy such equations have been shown to exist by Scott (see for
example Gunter and Scott [36]) and are commonly referred to as domains. Domains are

1The index set is disjoint from V.

7
essentially complete partial orders (cpo), where the order is given by

? ^ v

f ^ f 0 () 8x 2 V:f x ^ f 0 x
hv1; v2i ^ hv01; v02i () v1 ^ v01 ^ v2 ^ v02

hi; vi ^ hi; v0i () v ^ v0

Domains D are constructed as the limit of a series. In the case of V, the series is V0 = f?g
and

Vn+1 = B [ fwrong; ?g [ (Vn ! Vn) [ (Vn \Theta  Vn) [ (Vn + Vn) (2.2)
Elements of domains are either finite or infinite. An element v 2 V is finite, if whenever v
is less than the lub (least upper bound) of a directed set A (a set containing upper-bounds
for all finite subsets), v is less than some element v0 2 A. Infinite elements are the least
upper bounds of increasing sequences.

A subset I of D is an ideal, iff

1. I 6= ;
2. 8v 2 I:8w 2 D:w ^ v =) w 2 I
3. for all increasing sequences hvii in I, F vi 2 I
Ideals are the non-empty, downward-closed subsets of D that are closed under lubs of
increasing sequences. The collection of ideals of a domain D is written I(D). If a set
satisfies only the first two requirements, then it is called an order ideal. Remarkably,
when restricting the domain D to its finite elements Dffi, the collection of ideals I(Dffi) is
isomorphic to I(D). It is thus possible to consider only the order ideals and the finite
elements of D. The collection of ideals I(D) forms a complete lattice under set-inclusion,
where the meet is given by set-theoretic intersection, and the lub is defined by MacQueen
et al. [57] as G

*

I*!

ffi

= [

*

Iffi*

The subsets of a domain D that give meaning to type expressions are the ideals
of D that do not contain wrong. Let E be a language of type expressions formed by the
following grammar

E ::= B j t j E ! E j E \Theta  E j E + E j E " E j E [ E j 8t:E j 9t:E
where t are type variables and B is a set of base types. A type expression E is said to be
contractive in t, if

ffl E is b for some base type b 2 B
ffl E = t0 with t 6= t0

8

ffl E is of the form E1 ! E2, E1 \Theta  E2, or E1 + E2
ffl E is of the form E1 " E2 or E1 [ E2 and both E1 and E2 are contractive in t
ffl E is of the form 8t0:E or 8t0:E, and either t = t0 or E is contractive in t
MacQueen, Plotkin, and Sethi's main result [56] then states that a system of type equations
of the form

t1 = E1

...

tn = En
has unique solutions in I(V) as long as the expressions Ei are contractive in t1::tn. This
result is derived in five steps.

ffl A function fl(I; J ) is defined that measures the "closeness" of two ideals I, J from

I(D). The closeness function is parameterized by a rank function r(I) that associates
a natural number with each finite element in D.

fl(I; J ) = ae 1 if I = Jminfr(v) j v 2 (I [ J \Gamma  I " J )g otherwise (2.3)

ffl The collection of ideals I(D) is shown to form a complete metric space under the distance metric ffi(I; J ) = 2\Gamma fl(I;J). Series called Cauchy sequences converge in complete
metric spaces.

Definition 2.1 A sequence of ideals hIii is a Cauchy sequence, if given any real ffl ? 0,
there exists an n, such that for all n1; n2 * n, the distance ffi(In1 ; In2 ) ! ffl.

ffl The rank function r(v) is defined for elements in v 2 V as the minimal i, such that

v 2 Vi in the series (2.2).

ffl The semantic counter-parts of the type constructors !, +, and \Theta  are shown to be

contractive in the metric ffi and the operations [ and " are shown to be non-expansive
in the following sense.

Definition 2.2 Expressed in terms of the closeness function fl, an n-ary function f
over ideals is contractive if

fl(f (I1; : : : ; In); f (J1; : : : ; Jn)) ? mini fl(Ii; Ji)

and f is non-expansive if

fl(f (I1; : : : ; In); f (J1; : : : ; Jn)) * mini fl(Ii; Ji)

ffl Each system of contractive type equations is then a contractive map over I(D). The

main result follows from the Banach fix-point theorem which states that contractive
maps over complete metric spaces have unique solutions.

9
2.2 Set Constraints
Set constraints express inclusion relations between set expressions. Set expressions in turn
denote sets of elements in some underlying domain. Set expressions E are formed by the
grammar below. A set expression is either a set-variable X from the collection V , the
empty set 0, the universal set 1, a constructor c from a collection of uninterpreted function
symbols \Sigma  applied to n set expressions (where n is the arity of c), a union, an intersection,
or a negation.

E := X j 0 j 1 j c(E1; : : : ; En) j E1 [ E2 j E1 " E2 j :E
Constants are just a special case of constructors with arity 0. The arity of a constructor
c 2 \Sigma  is written a(c).

An inclusion constraint E1 ` E2 between set expressions E1 and E2 expresses that
the set of elements denoted by E1 must be a set-theoretic subset of the set of elements denoted by E2. The next two subsections describe two models for interpreting set expressions
and characterizing their solutions.

2.2.1 Term-Set Model
The simplest interpretation of set expressions is as sets of elements in a Herbrand universe
H. This model is commonly known as the term-set model [52, 51]. The universe H is
formed by applying constructors from the same set \Sigma  that is used to build constructed set
expressions.

H = fc(v1; : : : ; va(c)) j vi 2 H; c 2 \Sigma g
In this model, each set expression E denotes a subset of H. A variable assignment oe is a
map from variables to subsets of H. The interpretation or denotation of set expressions in
the term-set model under a given variable assignment oe is then given by _:

_[[X ]]oe = oe(X )

_[[0]]oe = ;
_[[1]]oe = H
_[[c(E1; : : : ; En)]]oe = fc(v1; : : : ; vn) j vi 2 _[[Ei]]oeg

_[[E1 [ E2]] = _[[E1]]oe [ _[[E2]]oe
_[[E1 " E2]] = _[[E1]]oe " _[[E2]]oe

_[[:E]]oe = H \Gamma  _[[E]]oe

A solution of a system of constraints fE1 ` E01; : : : ; En ` E0ng is a variable assignment oe,
such that the inclusions _[[Ei]]oe ` _[[E0i]]oe hold in the model for i = 1::n. The complexity
and algorithms for solving systems set constraints (and extensions with projections) have
been widely studied [74, 48, 41, 10, 5, 6, 13, 14].

2.2.2 An Ideal Model for Set Expressions
If the set constraint language is extended with expressions E1 ! E2 (standing for a set of
functions taking arguments from the set E1 and producing results in the set E2), then the

10
meaning of set expressions can no longer be adequately captured by the term-set model.2
Instead, a model for set constraints can be built based on the ideal model of types discussed
in the previous section. The semantic domain V is defined in terms of the equation

V = f?g [ (V ! V) [ [

c2\Sigma 

c(V \Gamma  f?g; : : : ; V \Gamma  f?g) [ fwrongg

where Sc2\Sigma  c(V \Gamma  f?g; : : : ; V \Gamma  f?g) can be expressed as the coalesced sum of smash
products [36]. More precisely, each value c(v1; : : : ; vn) of a n-ary constructor c is an element
of the coalesced sum

V + \Delta  \Delta  \Delta  + V-- -z ""

j\Sigma j times

where the index set used in the coalesced sum are the constructors c 2 \Sigma . Thus, the value
c(v1; : : : ; vn) has the form hc; vi, where v is an element of the smash product

V \Omega  \Delta  \Delta  \Delta  \Omega  V-- -z ""

n times

The smash product differs from ordinary product in that elements hv1; : : : ; vni, where vi = ?
for some i, are identified with ?. This property is commonly referred to as being strict in
?. Constructors in \Sigma  are then said to be strict.

Note that ? and the element *x:? of V ! V are distinct values. The former denotes a non-terminating computation, the latter denotes a function that is non-terminating
when applied.

The domain T is the domain V without wrong. Set expressions are then interpreted
as ideals of T, i.e., elements of I(T). Let oe be a variable assignment mapping set variables
to elements of I(T). The meaning function _ for set expressions in the ideal model is then

_[[X ]]oe = oe(X )

_[[0]]oe = ;
_[[1]]oe = T
_[[E1 ! E2]]oe = ff j x 2 _[[E1]]oe =) f x 2 _[[E2]]oeg
_[[c(E1; : : : ; En)]]oe = fc(v1; : : : ; vn) j vi 2 _[[Ei]]oe \Gamma  f?gg

_[[E1 [ E2]] = _[[E1]]oe [ _[[E2]]oe
_[[E1 " E2]] = _[[E1]]oe " _[[E2]]oe

_[[:E]]oe = T \Gamma  _[[E]]oe [ f?g

Complement expressions :E pose a problem in this model since the set complement of
an ideal in T is not an ideal. Even when adding f?g to the complement, the result may
not be an ideal, unless _[[E]]oe is upward-closed. An ideal I is upward-closed in V if 8v 2
I \Gamma  f?g:8w 2 V:w * v =) w 2 I. This problem leads to restrictions on well-formed set
expressions and the form of set constraints that can be solved. These restrictions guarantee
that negations only appear on upward-closed expressions. Since these same restrictions
apply to mixed constraints, we will discuss them in detail in Section 5.2.1.

2An alternative approach to the ideal model has been studied by Flemming Damm [19].

11
The main result of Aiken and Wimmers [3] then states that every set of constraints
S satisfying the restrictions due to negations is equivalent to a set of inductive systems and
that each inductive system has solutions. In order to define inductive systems, we assume
an arbitrary total order on the set-variables appearing in the constraints. The index i in
this order is written as a subscript to the variable, as in Xi. To define solutions inductively,
we need the notion of a top-level variable.

Definition 2.3 The set of top-level variables TLV(E) of an expression E is defined by

TLV(0) = fg TLV(1) = fg
TLV(X ) = fX g TLV(c(: : : )) = fg
TLV(E1 [ E2) = TLV(E1) [ TLV(E2)
TLV(E1 " E2) = TLV(E1) [ TLV(E2)

A constraint set S is an inductive system, iff

1. S has the form L1 ` X1 ` U1; : : : Ln ` Xn ` Un where Li and Ui are set expressions

without negations.

2. The top-level variables appearing in Li and Ui have index strictly less than i, i.e.,

TLV(Li) [ TLV(Ui) ` fX1; : : : ; Xi\Gamma 1g for all i.

3. The system S is closed under transitive constraints Li ` Ui. More formally, the

following property holds for all solutions oe of S and for all i, j:

8k ! i:_j[[Lk]]oe ` _j[[Xk]]oe ` _j[[Uk]]oe ^
8k * i:_j\Gamma 1[[Lk]]oe ` _j\Gamma 1[[Xk]]oe ` _j\Gamma 1[[Uk]]oe =) _j[[Li]]oe ` _j[[Ui]]oe

where _j is the meaning function up to level j defined by

_j[[E]]oe = _[[E]]oe " T0j
and T0j is the downward-closure of the finite domain approximation Tj of the sequence
T0; T1; : : : used to define the domain T.

The third condition of inductive systems states that if oe satisfies the constraints up to level
j for all variables with index less than i, and oe satisfies all constraints up to level j \Gamma  1 for
all indices greater or equal to i, then the transitive constraint Li ` Ui is also satisfied up
to level j.

An inductive system Li ` Xi ` Ui is equivalent to the set of equations

X1 = L1 [ Y1 " Ui

...

Xn = Ln [ Yn " Un

12
where the variables Yi are fresh. This system in turn is equivalent to the following system
of equations E

X1 = L01 [ Y1 " U 0i

...

Xn = L0n [ Yn " U 0n
where

ffl L0i is equal to Li where all top-level variables Xj of Li are replaced with L0j [ Yj " U 0j,

and

ffl U 0i is equal to Ui where all top-level variables Xj of Ui are replaced with L0j [ Yj " U 0j
Note that the above transformation is well-defined, since the top-level variables appearing
in Li and Ui have index strictly less than i due to Condition 2 of inductive systems. The
transformation can thus be performed starting with L1 and U1 which have no top-level
variables, and proceeding through Ln, Un in order. The resulting system of equations E has
no top-level variables from fX1; : : : ; Xng. Therefore E is contractive in X1::Xn in the sense
of Section 2.1. Every arbitrary assignment oe mapping the variables Yi to elements of I(T),
induces a unique solution of the equations E.

13
Part I
Mixed Constraints

15
Chapter 3
Mixed Constraints

This part introduces the formalism used to specify and solve constraints in BANE.
We define the syntax of mixed expressions and constraints. Chapter 4 defines their semantics
using a denotational model and characterizes when constraints have solutions. Chapter 5
gives an algorithm for solving the constraints. Chapter 6 discusses practical aspects of
constraint resolution.

Building an analysis framework on a single constraint formalism leads to a tension
between the generality and the efficiency of the chosen formalism. Generality is needed for
the framework to be useful in more than a couple of specialized cases, whereas efficiency is
crucial for the framework to be useful in practice. The constraint formalism at the base of
BANE--set constraints--satisfies the generality requirement. Indeed, many program analyses can be expressed within set constraints, including but not limited to closure analysis,
pointer analysis, receiver class analysis, and subtype inference.

The directionality of set constraints allows expressing analyses with subtyping.
Thus set constraints cover a much larger class of analyses than, for example, equality constraints. On the efficiency side, set constraints have not fared well in the past. Published accounts of implementations report practical performance on small problems, but
report bad performance on large problems or don't report performance on large problems
at all [68, 3, 4, 39, 24, 31, 58, 30]. Scaling is the main problem of such analyses. The scaling
problem of set-constraints has led to less precise program analyses based on equality constraints solvable using unification [75] in nearly linear time. Mixed constraints addresses the
tension between precision and scalability by integrating a number of specialized constraint
formalisms with distinct precision-efficiency tradeoffs.

To motivate the development, consider using set constraints for specifying and
implementing the resolution of equality constraints arising in Hindley-Milner type inference [60]. A standard set constraint solver can do this task. However, the efficiency will
fall short of the standard unification-based implementation. In this case it is clear that a
unification-based implementation is preferable. There are two important aspects that make
the equality constraints for Hindley-Milner type inference simpler to solve than general set
constraints. The first aspect is that equalities induce equivalence classes which are more
efficient to represent and merge than the partial orders that arise from inclusions. The second aspect is the solution domain of expressions appearing in the equality constraints. Each

16
type expression denotes a single tree of the form c(: : : ), with a unique head constructor c.
Set expressions on the other hand denote sets of terms with a variety of head constructors.
The general observation is that if the shape or domain of solution is restricted in certain
ways, a specialized and more efficient algorithm should exist.

Given an analysis problem, it is natural to select the most specialized class of
constraints in which the problem is expressible. This idea can be pursued by building a
library implementing a variety of specialized constraint solvers. However, mixed constraints
goes beyond this idea. Analysis problems may not be uniform in their precision requirements. It can well be that part of an analysis requires general sets for precision, and other
parts may only need restricted sets built from a single head constructor. With a library
of independent, specialized solvers such a problem must be expressed uniformly in a set
constraint formalism, without taking advantage of the single head constructor restrictions.
The idea of mixed constraints is to combine several constraint formalisms into a common
framework that enables the construction of analyses where precision-efficiency tradeoffs are
made explicit.

3.1 A Motivating Example
This section motivates mixed constraints with a concrete example analysis: Uncaught exception inference for a subset of the ML language. Exception inference is an interesting
problem for mixed constraints because it can be expressed as a minimal refinement of standard Hindley-Milner type inference, while still making essential use of set expressions.

We begin by illustrating the problem of types that are more general than needed
if the exception inference is uniformly expressed in a set constraint formalism. In ML, the
type of an exception value v is simply exn--no indication is given of the possible exception
constructors of v. Consider a refinement of the ML type system that models exception types
with an explicit annotation of the set of exception constructors. For example, the type of
the exception constructor Subscript is modeled as exn(Subscript). A possible inference
rule for if-expressions based on inclusion constraints is

A ` p : bool
A ` e1 : T1
A ` e2 : T2
T1 ` X X fresh
T2 ` X

A ` if p then e1 else e2 : X [IF]

The rule says that the result type must contain the types of both branches. The conditional
expression

if p then Subscript else x

returns either the exception value Subscript (exceptions are first-class), or the
value of the program variable x. Assuming x has type Y, applying the inference rule to this
expression gives the type X along with two lower bounds, written

X where exn(Subscript) ` X ^ Y ` X

17
There are many solutions for X and Y satisfying these constraints. One possible solution is

Y 7! int
X 7! exn(Subscript) [ int

For many programming languages (and in particular for ML), this solution is uninteresting,
because the union of an integer and an exception cannot be used anywhere. We are really
only interested in solutions where the type of the else branch is also an exception. However,
we cannot simply require both branches to have the same type as in a standard ML type
system, because the else branch may contribute an exception other than Subscript. For
example, if the else branch returns exn(Match), we would like to infer that the entire if
returns exn(Subscript) or exn(Match). Thus we have two conflicting goals: On one hand
we need the generality of inclusion constraints to allow different exception constructors in
the branches of the conditional, and on the other hand we do not want the full generality of
inclusion constraints, since they admit many uninteresting solutions. In the example, the
interesting solutions all have the form

Y = exn(Z)
X = exn(Subscript [ Z)

which clarifies that the if-expression and both branches return exceptions and that the set
of exception constructors of the result includes the Subscript exception and any exceptions
contributed by the else-branch. In summary, the example illustrates two points:

ffl For particular analyses, inclusion constraints may admit more solutions than required.
ffl Set types are needed to express sets of values with more than one head constructor

(e.g. Subscript [ Z).

3.2 Syntax
Mixed constraints are a combination of constraints in several specialized constraint formalisms, which we refer to as sorts. Each sort in a collection of sorts S is characterized by
a constraint language, a constraint relation, a solution domain, and a resolution algorithm.
Expressions for a sort s are built from a collection of s-variables Vs, s-constructors \Sigma s,
and s-operations \Pi s. Each constructor c 2 \Sigma s and operation op 2 \Pi s is equipped with a
signature. A signature

c : '1 \Delta  \Delta  \Delta  'k ! s

specifies the constructor sort s, the number of arguments k, the sort of arguments 'j, and
their variance ('j is t or t for some t 2 S, overlined sorts mark contravariant arguments, the
rest are covariant). Signatures also specify the strictness of the constructor in each argument. To keep the notation readable, we omit the strictness annotations. The signatures
for operations are fixed, whereas the signatures for constructors parameterize the language.
We refer to \Sigma s for the set of s-constructors along with their signatures. The arity of a
constructor c is written a(c).

Let V = Ss2S Vs be the set of variables of all sorts. The language Ls of expressions
of sort s is formed by the term algebra T\Sigma s[\Pi s(V).

18
Definition 3.1 An expression E 2 Ls is well-formed if it is

ffl a variable of sort s (E 2 Vs),
ffl of the form c(E1; : : : ; Ek), where c 2 \Sigma s has signature '1 \Delta  \Delta  \Delta  'k ! s, and expression

Ej is either an expression of sort s (if 'j = s), or a variable from V'j 1

ffl of the form op(E1; : : : ; Ek), where op 2 \Pi s has signature '1 \Delta  \Delta  \Delta  'k ! s, and Ej is

either an expression of sort s (if 'j = s), or a variable from V'j .

Depending on the signature, some arguments to a constructor c of sort s may be of sort t
distinct from s. An s-constructor c is called mixed if at least one argument of c is of a sort
other than s. Otherwise the constructor is called pure. The constraint relation for sort s
is written `s. Constraints observe sorts, i.e., a constraint E1 `s E2 is well-formed if both
E1 and E2 are s-expressions. Expressions and constraints are assumed to be well-formed
unless otherwise mentioned.

We use the following terminology: Expressions on the left of a constraint `s are
said to occur in an L-context, and expressions on the right occur in an R-context. Subexpressions occur in the same context (L- or R-) as their immediately enclosing expression,
unless the sub-expression is the ith argument to a constructor that is contravariant in i.
In that case its context is inverse w.r.t. the enclosing expression. We call an expression
occurring in an L-context (R-context) an L-expression (R-expression).

For example, let Term be a sort of equality constraints between tree expressions,
formed by a set of constructors \Sigma Term and variables VTerm and no additional operations.
Pure Term-expressions are defined by giving each constructor c 2 \Sigma Term the signature

c : Term \Delta  \Delta  \Delta  Term-- -z ""

a(c)

! Term

3.3 Sorts
For concreteness, the development of mixed constraints focuses on the set of sorts provided
in BANE. There are three base sorts: Set, Term, and FlowTerm, abbreviated by s,t, and ft.

The Set-sort is a language of set expressions with operations \Pi s given in Figure 3.1
including joins t, meets u, the empty set (written 0), and negation. The universal set, which
we refer to as 1, is simply shorthand for :fg. Note that we write unions and intersections of
Set-expressions with t and u instead of the usual set-theoretic union and intersections [ and
". We make this distinction because the semantics of mixed expressions we give in Chapter 4
shows that the meet of two Set-expressions is not exactly set-theoretic intersection, but
may in fact be smaller. Similarly, the join of two Set-expressions may be larger than settheoretic union. Constraints E1 `Set E2 are inclusions between set expressions E1 and
E2. The resolution rules are given in full detail in Chapter 5. Here we only show the rule

1When we introduce the operations of each sort, we also allow 0t (the smallest expression of sort t) and
1t (the largest expression of sort t) to appear as arguments of mixed constructors.

19
t : Set Set ! Set
u : Set Set ! Set

0 : Set
:fc1; : : : ; cng : Set for any set of Set-constructors ci 2 \Sigma Set

Figure 3.1: Set operations in \Pi Set
involving constructors to give insight into how constraints involving mixed constructors
connect constraints of different sorts

S [ fc(E1; : : : ; Ek) `s c(E01; : : : ; E0k)g () S [ fEj `'j E0jg

where c : '1 \Delta  \Delta  \Delta  'k ! s

Depending on the signature c : '1 \Delta  \Delta  \Delta  'k ! s, the constraints Ej `'j E0j may be on expressions
of sort 'j distinct from s. For contravariant arguments, the constraint relation is flipped,
i.e., Ej `s E0j j E0j `s Ej.

The Term sort provides inclusion constraints between restricted sets. There are
two constant operations in \Pi Term: 0 stands for the empty set, and 1 for the set of all Term
denotations. Sets are restricted in that they are built from a single head constructor (besides
0 and 1). On single head constructor expressions, the relation `t is actually equality. Thus,
the resolution rule for constructors is symmetric in the constraints on constructor arguments

S [ fc(E1; : : : ; Ek) `t c(E01; : : : ; E0k)g () S [ fEj `'j E0j; E0j `'j Ejg

where c : '1 \Delta  \Delta  \Delta  'k ! t

The third base sort, FlowTerm, also provides inclusion constraints over restricted
sets. It has the same language as Term, i.e., \Pi FlowTerm contains only 0 and 1. This sort
distinguishes itself from the Term sort in that the constraint relation is still directed for
single head constructor expressions. The constructor rule is therefore not symmetric.

S [ fc(E1; : : : ; Ek) `ft c(E01; : : : ; E0k)g () S [ fEj `'j E0jg

where c : '1 \Delta  \Delta  \Delta  'k ! ft

The FlowTerm sort can be thought of as a combination of the Set and Term sort, inheriting
the directionality from Set-constraints and the solution domain from the Term sort.

For each base sort s there is a corresponding row-sort Row(s), abbreviated by r(s).
Expressions of sort Row(s) model record types of labeled fields of sort s. Such records denote
sets of partial functions from an infinite set of labels L to elements of sort s. There are no
constructors for row expressions. Instead all row expressions are formed from the operations
in Figure 3.2. The constants 1 and 0 denote the set of all records and the least record. The
syntax hl : Elil2A stands for a sequence hl1 : El1 ; : : : ; ln : Elni where A = fl1; : : : ; lng. Rowexpressions can be composed with ffi. Composition is restricted to the case where the left
side is an explicit finite enumeration of labels. Furthermore, we are principally interested
in strict extensions, i.e., the domain of the Row-expression on the right does not contain

20

1 : Row(s)
0 : Row(s)
hi : Row(s)

hl : \Delta il2A ffi \Delta  : s \Delta  \Delta  \Delta  s-- -z ""

jAj times

Row(s) ! Row(s)

for any finite set A ` L

Figure 3.2: Operations \Pi Row(s) for sort Row(s)
any labels from A. We will make this notion precise in Chapter 5 when we address the
resolution of the constraints. Besides associativity, a number of equivalences hold for row
expressions. The empty record hi is a neutral element for composition.

E1 ffi (E2 ffi E3) = (E1 ffi E2) ffi E3 (3.1)

hi ffi E1 = E1 ffi hi = E1 (3.2)
hl : Elil2A ffi hl : Elil2A0 = hl : Elil2A]A0 (3.3)

Row expressions obtained by permuting the order of label-expression pairs are also considered equivalent. Using the above equivalences, every Row-expression is equivalent to a
Row-expression of the form hl : Elil2A ffi E, where E = hi, E = X , E = 1, or E = 0.
Such Row-expressions are normalized and we assume that we work only with normalized
Row-expressions. Where necessary, we disambiguate the field sort s by adding the sort as a
superscript to a Row-expression as in hl : Elisl2A.

Our definition of mixed expressions requires that arguments of sort t to constructors and operations of sort s, where t 6= s are variables from Vt. For convenience, we
relax this restriction now for the constants 0 and 1 (of sort t), which may also appear as
arguments in these cases.

Coming back to our motivating example on exception inference, we can now give
signatures to the constructors for exceptions, and exception names:

exn : s ! ft
Subscript : s

\Delta  \Delta  \Delta 

Types are modeled by FlowTerm expressions, i.e., expressions having a single head constructor. To distinguish exceptions by name, the exception constructor exn is refined to a
unary constructor, embedding sets of exception names into the FlowTerm sort. Exception
names are constants of sort Set. Some exceptions constructors in ML carry a value. Such
exception constructors c can be given the signature

c : ft ! s
making clear that the value is of sort FlowTerm and the resulting exception is of sort Set.

21
The set of sorts S is now fixed fs; ft; t; r(s); r(ft); r(t)g. The constants 0 and 1 are
overloaded, but their sort should always be apparent from the context. Every choice of
signatures \Sigma s, \Sigma t, and \Sigma ft generates the languages for mixed expressions Ls, Lt, Lft. We
refer to \Sigma s, \Sigma t, and \Sigma ft as fixed, but unknown signature sets.

22

Chapter 4
Semantics

This chapter formalizes the semantics of mixed expressions and constraints by
defining semantic domains for the different sorts, the semantic constraint relations, and the
notion of solutions.

We provide a denotational semantics, i.e., a semantics where each expression denotes a subset of values from an underlying value domain. Constraints between expressions
are then interpreted as inclusion constraints between these values sets. The novelty of mixed
constraints is that these inclusions are actually stronger for some sorts than set-theoretic
inclusion. The motivation for a denotational semantics stems from the desire to make the
constraints suitable for quick prototyping of program analyses. The model in mind is that
given a programming language and an analysis problem, mixed expressions are used to
conservatively approximate the sets of values of each program expression. Constraints are
used to conservatively model the value flow in the program, i.e., if values from a program
expression e1 may flow to an expression e2 where the values of e1 are abstracted by a mixed
expression E1 and those of e2 by E2, then there should be a constraint E1 `s E2. If all
value-flows in a program are conservatively approximated by constraints, then one obtains a
sound analysis. An analysis is sound if for any program expression e and possible evaluation
of e to v, the denotation of the mixed expression E approximating e contains v.

In the above naive formulation, we have ignored the evaluation context. If different
approximations of an expression are used according to the context, then the soundness
argument must be refined. The idea here is to give an intuitive approach to modeling value
flows at a range of abstractions. Without a denotational model and constraints that imply
set inclusion, it becomes harder to reason about the appropriateness of an analysis.

We quickly summarize the development of the mixed constraints semantics. We use
the ideal model of types as outlined in Chapter 2 to assign semantics to mixed expressions.
The semantics is parameterized in two ways. The first parameterization is a collection
of semantic constructors which are used to construct a semantic domain V. The second
parameterization of our semantics is given by the syntactic constructors appearing in mixed
constraints, and their signatures. Each n-ary constructor comes with an interpretation
function that maps sequences of n ideals to ideals. We use these constructor interpretations
to partition the domain V into sub-domains Vs for each sort s. This partitioning induces a
partitioning of the collection of ideals I(V) into collections of ideals I(Vs) for each sort s.

23
Vs
(I(Vs); `)

Vt
(I(Vt); `)

Vft
(I(Vft); `)

Vr(s)
(I(Vr(s)); `)

Vr(t)
(I(Vr(ft)); `)

Vr(ft)
(I(Vr(t)); `)
(Ss; `s) (St; `t) (Sft; `ft) (Sr(s); `r(s)) (Sr(ft); `r(ft))(Sr(t); `r(t))

V

Figure 4.1: Development of lattices (Ss; `s)
We then select only a subset Ss of each collection I(Vs) which we call the type-collection Ss
of sort s. The ordering on ideals I(Vs) is set-theoretic inclusion. The relations expressed in
mixed constraints are in general stronger than set-theoretic inclusion. We define the order
relations `s on elements of Ss and show that (Ss; `s) forms a lattice. Figure 4.1 summarizes
the development in graphical form.

We use the following notational convention. The symbols [ and " refer to settheoretic union and intersection, or to joins and meets of ideals in the complete lattice of
ideals I(V) (Section 2.1). Symbols u and t are used for meet and join operations of ideals
w.r.t. the type collections Ss constructed in this chapter.

4.1 Value Domains
The semantics of mixed expressions is given in a common domain V similar to the ideal
domain described in Chapter 2. The main differences from the semantics of set constraints
given there are

ffl The semantic constructors from which the domain is built are disjoint from the syntactic constructors used in the constraints.

ffl The relation between syntactic and semantic constructors is captured by a collection of constructor interpretations OEc : I(V) \Delta  \Delta  \Delta  I(V) ! I(V) that parameterize the
semantics.

ffl The domain V contains total functions over an infinite label set L for modeling records.

There is a special value abs in the range of these functions to also model partial
functions which correspond more closely to actual record values.

24

ffl Not all ideals I(V) are denotations of mixed expressions. The constructor signatures

and their interpretations impose more structure.

To distinguish semantic constructors from syntactic constructors, we refer to the set of
semantic constructors with the letter C and to individual semantic constructors with the
letter ^. The semantic domain V is the solution of the domain equation

V = f?g [ [

^2C

^(V \Gamma  f?g; : : : ; V \Gamma  f?g) [

V ! (V [ fwrongg) [ L ! (V [ fabsg)
The domain of values V ! (V [ fwrongg) are the strict continuous functions from V to
V [ fwrongg. The definition contains functions returning wrong. This provision is different
from the development by Aiken and Wimmers [3], where functions are restricted to V ! V.
Including functions that return wrong models function implementations more realistically
(applying a function to an argument outside its domain may generate an error) and has the
advantage of getting rid of the following equivalence (assuming I 6= f?g)

I ! V = f?g ! V
If functions may return wrong, this equivalence does not hold since the right side contains
functions that return wrong for x 2 I, whereas the left side does not.

The set of values L ! (V [ fabsg) is the set of functions from labels to elements
in V or abs. The idea of abs is to model partial functions. A record with finite domain
A ae L is modeled as returning abs on all labels not in A. The domain dom(f ) of a function
f is thus the set

dom(f ) = fl j f l 6= absg

We say that an ideal is a Row-ideal if it is an element of I(L ! (V [ fabsg)). The domain
of a Row-ideal I written dom(I) is the intersection of the domains of all its elements

dom(I) = "

f2I\Gamma f?g

dom(f )

Furthermore, we write I(l) for the set

I(l) = [

f2I\Gamma f?g

f l

Given a finite subset of labels A

fin` L, the composition of two functions f and g, where

A ` dom(f ), is written f ffiA g and corresponds to the function

f ffiA g = *l:if l 2 A then f l else g l
Given two Row-ideals I and J such that dom(I) is finite, the composition I ffi J is the set

I ffi J = ff ffidom(I) g j f 2 I \Gamma  f?g ^ g 2 J \Gamma  f?gg [ f?g (4.1)

25
We now motivate the parameterization of constructor interpretations and then
proceed by partitioning V into sub-domains Vs for each sort s. Most published accounts
of set constraints interpret constructor expressions c(E1; : : : ; En) as sets of strict tuples
labeled by c, where the syntactic constructors and the semantic constructors stand in a
one-to-one relationship

_[[c(E1; : : : ; En)]] = fc(t1; : : : ; tn) j ti 2 _[[Ei]] \Gamma  f?gg [ f?g
(_ is the function mapping expressions to their denotation). The arrow constructor for sets
of functions cannot be modeled this way and is treated specially. Expressions E1 ! E2
have the interpretation

ff j x 2 _[[E1]] =) f x 2 _[[E2]]g [ f?g
Other standard type constructors, e.g. list also need to be modeled as special cases in such
an interpretation. For example, the standard interpretation for a type list(E) is

_[[list(E)]] = X where X is defined by the equation
X = fnil ; ?g [ fcons(v; w) j v 2 _[[E]] \Gamma  f?g ^ w 2 Xg

which is rather different from a set of tuples labeled by list, and has the additional property
of being non-strict in E.

Having numerous special cases is impractical and interpreting all constructors
as labeled tuples severely limits the applicability of the formalism developed here. Our
approach is to assume as little as possible about the interpretation of constructors. We
assume only that there exists an interpretation function OEc : I(V)n ! I(V) for each n-ary
constructor c in any given signature set \Sigma s. Each OEc gives meaning to expressions with head
constructor c

_[[c(E1; : : : ; En)]] = OEc(_[[E1]]; : : : ; _[[En]])

The mappings OEc must satisfy a number of axioms. These properties are used in the
remainder of the development.

Axiom 4.1 The mapping OEc for constructor c must satisfy the following conditions.

i. [Ideals] OEc is a mapping of ideals to ideals, i.e., OEc : I(V)n ! I(V)
ii. [Contractive] OEc must be contractive according to the Definition 2.2.
iii. [Variance] OEc must observe the variance of the declared signature c : '1 \Delta  \Delta  \Delta  'n ! s.

Formally,

Ij ` J =) ae OEc(I1; : : : ; In) ` OEc(I1::Ij\Gamma 1; J; Ij+1::In) 'j = t 2 SOE

c(I1::Ij\Gamma 1; J; Ij+1::In) ` OEc(I1; : : : ; In) 'j = t 2 S

iv. [Disjoint] The interpretations of any two distinct constructors c, d of some sort s must

be disjoint, i.e., OEc(I1; : : : ; Ia(c)) " OEd(J1; : : : ; Ja(d)) = f?g, for any sets Ii and Jj.

26

v. [Non-empty] The interpretation of any constructor c is non-empty (besides ?) when

applied to non-empty arguments, i.e., OEc(I1; : : : ; Ia(c)) oe f?g for all sets Ii oe f?g.

vi. [Strictness] Every constructor interpretation is either strict in all arguments, or nonstrict in all arguments.

vii. [Injective] The interpretation OEc of non-strict constructors must be injective.

To simplify the development in the rest of the dissertation, we require all Term and FlowTerm
constructors to be non-strict. We believe that this requirement can be relaxed.

The following equations define the domain of values for each sort, splitting up the
common domain V.

Vt = [fOEc(I1; : : : ; Ia(c)) j c : '1 \Delta  \Delta  \Delta  'a(c) ! t ^ Ij 2 I(V'j ); j = 1::a(c)g
Vft = [fOEc(I1; : : : ; Ia(c)) j c : '1 \Delta  \Delta  \Delta  'a(c) ! ft ^ Ij 2 I(V'j ); j = 1::a(c)g

Vs = [fOEc(I1; : : : ; Ia(c)) j c : '1 \Delta  \Delta  \Delta  'a(c) ! s ^ Ij 2 I(V'j ); j = 1::a(c)g)
Vr(s) = f?g [ L ! (Vs [ fabsg)

Since the mappings OEc are contractive, the above equations are also contractive and thus
have unique solutions.

Mixed expressions allow Set-expressions to include e.g., embedded Term-expressions.
Thus, inclusion on Set-ideals may actually be a stronger relation than set-theoretic inclusion, because the embedded Term-ideals require equality between constructor arguments
and not inclusions. To make precise what inclusion relations on mixed expressions mean,
we introduce the notion of interface paths and interface-path projections.

4.2 Interface Paths
Let an access path be a sequence formed by constructor-index pairs, i.e., P = (\Sigma  \Theta  N)\Lambda 
where \Sigma  = \Sigma s [ \Sigma t [ \Sigma ft. An access path p 2 P is well-formed of sort s if

1. p = ffl (p is empty), or
2. p = (c; j)p0, 0 ^ j ! a(c), c : '1 \Delta  \Delta  \Delta  'a(c) ! s, and p0 is well-formed and of sort 'j.

Definition 4.2 The following definitions characterize useful properties of paths.

1. The sort of a non-empty path p is sort (p). Where convenient, we write ps when p is

non-empty and sort (p) = s.

2. A path p is uniform of sort s iff every prefix path of p is of sort s.
3. The interface if(p) of a non-empty, well-formed path p is t, if p = p0(c; j), c :

'1 \Delta  \Delta  \Delta  'a(c) ! s, and 'j = t or 'j = t for some t 2 S. Additionally, we say that
the interface is covariant if 'j = t and contravariant if 'j = t. Where convenient, we
write pt whenever p is non-empty, well-formed, and if(p) = t.

27
4. A path is non-strict if all constructors in the path are non-strict, otherwise the path

is strict.

5. A non-empty path p is called an interface path if p is uniform of sort s, and if(p) 6= s.
6. Finally, a path p is called even ( odd), iff it the number of distinct prefix paths of p

with contravariant interface is even (odd).

Unless stated explicitly, paths are always well-formed and finite, and interface paths are
always non-strict.

We now define what it means for a path to be present in a an ideal I(V).
Definition 4.3 A path p is present in I, written present (p; I), iff p = ffl or p = (c; j)q with
c : '1 \Delta  \Delta  \Delta  'a(c) ! s and 9I1 2 I(V'1)::Ia(c) 2 I(V'

a(c)) such that f?g ae OEc(I1; : : : ; I

a(c)) ` I

and present (q; Ij ).

Definition 4.4 The projection p\Gamma 1(I) of an ideal I w.r.t. to a path p is defined as follows:

p\Gamma 1(I) = ae I if p = fflq\Gamma 1(OE\Gamma 1

c (I; j)) if p = (c; j)q

where c : '1 \Delta  \Delta  \Delta  'a(c) ! s and

OE\Gamma 1c (I; j) = ae SfIj j f?g ae OEc(I1; : : : ; I

a(c)) ` Ig 'j = t for some t 2 ST

fIj j f?g ae OEc(I1; : : : ; Ia(c)) ` Ig 'j = t for some t 2 S

with Ik 2 I(V'k ) for k = 1::a(c), Sfg = f?g, and Tfg = Vt, if 'j = t.
The projection of non-present even paths is f?g and the projection of non-present odd paths
of interface t is Vt. (Note that I(Vs) is a complete lattice and that the above intersections
and unions are defined for infinite collections.)

We now prove that projections of non-strict paths distribute over unions and intersections. We'll need the following lemmas.

Lemma 4.5 For any ideals I, J , constructor c and index 1 ^ j ^ a(c), such that c is
non-strict and covariant in j, we have

OE\Gamma 1c (I; j) " OE\Gamma 1c (J; j) = OE\Gamma 1c (I " J; j)
Proof: First consider the case '. Since c is covariant in j, we have

OE\Gamma 1c (I " J; j) ` OE\Gamma 1c (I; j) and also
OE\Gamma 1c (I " J; j) ` OE\Gamma 1c (J; j) thus
OE\Gamma 1c (I " J; j) ` OE\Gamma 1c (I; j) " OE\Gamma 1c (J; j)

Consider the case `. Suppose t 2 OE\Gamma 1c (I; j) " OE\Gamma 1c (J; j). Then

t 2 [fYj j f?g ae OEc(Y1; : : : ; Ya(c)) ` Ig and also
t 2 [fY 0j j f?g ae OEc(Y 01 ; : : : ; Y 0a(c)) ` J g

28
Thus 9Y1::Ya(c) and 9Y 01::Y 0a(c) such that

f?g ae OEc(Y1; : : : ; Ya(c)) ` I ^ t 2 Yj
f?g ae OEc(Y 01; : : : ; Y 0a(c)) ` J ^ t 2 Y 0j

Since OEc observes variance and is non-strict, f?g ae OEc(Y1 " Y 01 ; : : : ; Ya(c) " Y 0a(c)) ` I " J ,
where

" = ae " for covariant arguments[ for contravariant arguments

Since t 2 Yj " Y 0j , we have t 2 OE\Gamma 1c (I " J; j). 2

The next lemma states the dual case.
Lemma 4.6 For any sets I, J , constructor c and index j ^ a(c), such that c is non-strict
and contravariant in j, we have

OE\Gamma 1c (I; j) [ OE\Gamma 1c (J; j) = OE\Gamma 1c (I " J; j)
The proof is symmetric to the proof of Lemma 4.5. In similar spirit one can prove the
following

Lemma 4.7 For any sets I, J, non-strict constructor c and index j ^ a(c) we have

OE\Gamma 1c (I [ J; j) = ae OE

\Gamma 1c (I; j) [ OE\Gamma 1c (J; j) c covariant in j

OE\Gamma 1c (I; j) " OE\Gamma 1c (J; j) c contravariant in j

The next lemma states that intersections and unions distribute over projections of
non-strict paths.

Lemma 4.8 For any sets I, J, and non-strict path p,

p\Gamma 1(I) " p\Gamma 1(J ) = ae p

\Gamma 1(I " J ) p even

p\Gamma 1(I [ J ) p odd

and

p\Gamma 1(I) [ p\Gamma 1(J ) = ae p

\Gamma 1(I [ J ) p even

p\Gamma 1(I " J ) p odd

Proof: By induction on the length of p. If p = ffl, the lemma holds. We prove the
induction step for intersections. The case for unions is analogous. Suppose p = (c; j)q. By
definition and the induction hypothesis we have

p\Gamma 1(I) " p\Gamma 1(J ) = q\Gamma 1 \Gamma OE\Gamma 1c (I; j)\Delta  " q\Gamma 1 \Gamma OE\Gamma 1c (J; j)\Delta 

= ae q

\Gamma 1 \Gamma OE\Gamma 1c (I; j) " OE\Gamma 1c (J; j)\Delta  q even

q\Gamma 1 \Gamma OE\Gamma 1c (I; j) [ OE\Gamma 1c (J; j)\Delta  q odd

Note that since p is non-strict c is non-strict. Suppose that p is even. There are two cases. If
q is even, then c is covariant in j. By Lemma 4.5 we have OE\Gamma 1c (I; j)"OE\Gamma 1c (J; j) = OE\Gamma 1c (I "J; j),

29
and thus q\Gamma 1(OE\Gamma 1c (I; j) " OE\Gamma 1c (J; j)) = q\Gamma 1(OE\Gamma 1c (I " J; j)). Now suppose that q is odd. Then c
is contravariant in j and by Lemma 4.6 we have OE\Gamma 1c (I; j)[OE\Gamma 1c (J; j) = OE\Gamma 1c (I "J; j), and thus
q\Gamma 1(OE\Gamma 1c (I; j)[OE\Gamma 1c (J; j)) = q\Gamma 1(OE\Gamma 1c (I "J; j)). In either case q\Gamma 1(OE\Gamma 1c (I "J; j)) = p\Gamma 1(I "J ).

Now assume that p is odd. We have again two cases. If q is even, then c
is contravariant in j. By Lemma 4.7 OE\Gamma 1c (I; j) " OE\Gamma 1c (J; j) = OE\Gamma 1c (I [ J; j), and thus
q\Gamma 1(OE\Gamma 1c (I; j)"OE\Gamma 1c (J; j)) = q\Gamma 1(OE\Gamma 1c (I [J; j)). Now suppose that q is odd, then c is covariant
in j. By Lemma 4.7 OE\Gamma 1c (I; j)[OE\Gamma 1c (J; j) = OE\Gamma 1c (I[J; j), and thus q\Gamma 1(OE\Gamma 1c (I; j)[OE\Gamma 1c (J; j)) =
q\Gamma 1(OE\Gamma 1c (I [ J; j)). In either case q\Gamma 1(OE\Gamma 1c (I [ J; j)) = p\Gamma 1(I [ J ). 2

4.3 Type Collections
This section defines for each sort s the collections Ss ` I(Vs) of ideals denoted by mixed
expressions of sort s (see also Figure 4.1). We call these collections type collections. Whereas
in the work of Aiken and Wimmers [3] every ideal of I(V) is a possible denotation of a
set-expression, in mixed constraints we impose more structure on the denotations that we
consider. For example, Term and FlowTerm expressions only denote ideals of the domain
obtained by applying a single head constructor. The interpretation functions OEc make it
convenient to specify the collections of ideals Ss.

Before we define the type collections Ss, we characterize three kinds of Row-ideals
denoted by Row-expressions.

Definition 4.9 Each Row-ideal I is fully characterized by the ranges I(l) for all labels
l 2 L. Thus, for all labels l, if f 2 I, then for each v 2 I(l), there exists f 0 2 I, such that
f 0(l) = v, and f 0(l0) = f (l) for all labels l0 6= l. Furthermore, a Row-ideal I is called

1. [Maximal] iff dom(I) is finite and 8l 62 dom(I):I(l) = Vs [ fabsg.
2. [Closed] iff dom(I) is finite and 8l 62 dom(I):I(l) = f?; absg.
3. [Minimal] iff dom(I) = L and there are finitely many l 2 L such that I(l) 6= f?g.

The three kinds of Row-denotations correspond to the three Row-expressions that
can appear as the right-most expression of a Row-composition: 1, hi, and 0. Maximal Rows
are denoted by expressions of the form hl : EliA ffi 1, closed Rows by expressions of the form
hl : EliA = hl : EliA ffi hi, and minimal Rows by expressions of the form hl : EliA ffi 0. We
define the minimal domain dom?(I) of any Row-ideal I to be

dom?(I) = ae fl j I(l) 6= f?gg if I minimaldom(I) otherwise
Definition 4.9 implies that dom?(I) is finite for all Row-ideals we consider. Even though
we won't discuss any applications of Row-expressions until Section 4.6 and Chapter 8 we
briefly give some intuition about the three different kinds of Row-ideals in our model.
Width-subtyping is common in type systems and refers to the ability to forget the presence
of certain fields in record types, or add new fields in variant types. Width-subtyping is
reflected in the relation of the minimal domains of two Row-ideals. The semantic relation

30
I `r(s) J on Row-ideals that we define later in Section 4.4 captures three distinct widthsubtyping relations, depending on the kinds of Row-ideals I and J . If I and J are minimal
Row-ideals, then I `r(s) J only if dom?(I) ` dom?(J ). If I and J are closed Row-ideals,
then I `r(s) J only if dom?(I) = dom?(J ). Finally, if I and J are maximal Row-ideals,
then I `r(s) J only if dom?(I) ' dom?(J ).

The following invariant specifies the properties of the type collections Ss that make
them distinct from I(Vs).

Invariant 4.10 (Type Collections) The collections Ss must satisfy the following conditions.

1. All elements (besides ? and ?, defined below) of Stn and Sftn are of the form OEc(I1; : : : ; In)

for some constructor c : '1 \Delta  \Delta  \Delta  'k ! ft; ftg and elements Ij 2 S'jn\Gamma 1.

2. For every finite non-strict interface path pts and every element I 2 Ssn the projection

p\Gamma 1(I) is an element of Stn\Gamma jpj. (By convention Ssi = Ss0 for negative i).

3. For every I 2 Sr(s)i and every label l in the domain of I, the set I(l) is an element of

Ssi\Gamma 1, and I is either closed, maximal, or minimal according to Definition 4.9.

Condition 1 is motivated by the desire to restrict the ideals of Term and FlowTerm-expressions
to the form OEc(: : : ), i.e., ideals built from a unique head constructor c. Condition 2 guarantees that for any Set-ideal I in Ss, the projection p\Gamma 1(I) of any non-strict interface path
pt is an element of St. We use this property in Section 4.4 to define the semantic relation
I `s J to be stronger than set-theoretic inclusion I ` J by also requiring that for any
non-strict interface path pt, p\Gamma 1(I) `t p\Gamma 1(J ). Condition 3 is similar to Condition 2, but
for Row-ideals. Note that Row-composition defined by (4.1) preserves Condition 3.

The collections Ss ` I(Vs) are now defined inductively for each sort, along with
binary operations us and ts. These operations will serve as the meet and join when we
define the semantic relations on the type collections. The meet and join also serve as
the generalized intersection and union operations for mixed Set-expressions. We use two
abbreviations ?s and ?s to denote the smallest and the largest ideal for sort s. For sorts
s 2 fs; ft; tg, the smallest set is ?s = f?g. For Row-sorts, the smallest ideal is ?r(s) =
f?; *x:?g. The largest ideal ?s for sort s is simply Vs. The type-collections Ss are defined
inductively as the smallest fix-point of a series Ss0; Ss1; Ss2; : : : . For notational convenience,
we say that ?s = ?s and ?s = ?s. Where the sort s is understood, we omit it. In the base
case, type collections merely contain ?s and ?s.

St0 = f?t; ?tg
Sft0 = f?ft; ?ftg
Sr(s)0 = f?r(s); ?r(s)g

Ss0 = f?s; ?sg

At level n of the induction, we define Ssn in terms of two simpler collections Tsn
and Osn. Tsn is the collection of ideals built by applying constructors of sort s to elements of

31
Stn\Gamma 1, and Osn is built from Tsn by adding ideals built using operations of sort s (besides meet
and join operations). Finally, Ssn is built from Osn by closing under finite meets and joins
(defined below). The following equality defines how Tsn is built from the collections Stn\Gamma 1.
(Note that for Row-sorts, this collection is empty since Row-sorts have no constructors).

Tsn = fOEc(I1; : : : ; Ik) j c : '1 \Delta  \Delta  \Delta  'k ! s; Ij 2 S'jn\Gamma 1g (4.2)
Osn is obtained from Tsn by adding the denotations of constants 0 and 1 for all sorts.
Additionally, denotations for Row-constant hi and Row-composition are added to sorts r(s),
and negations of single constructors are added to the Set-sort.

Osn = Tsn [ f?s; ?sg [ 8???!???:

; s 2 ft; ftg
faeA(g) ffi K j A

fin` L ^ g 2 A ! St

n\Gamma 1g s = r(t)

f?s \Gamma  OEc(?'1 ; : : : ; ?'

a(c) ) j c : '1 \Delta  \Delta  \Delta  'a(c) ! sg s = s

(4.3)

where K is ?r(s), absr(s), or ?r(s), and

aeA(g) =ff 2 L ! (Vs [ fabsg) j l 2 A =) f l 2 g l ^ l 62 A =) f l 2 f?; absgg

[ f?g (4.4)

and

absr(s) = ff 2 L ! (Vs [ fabsg) j f l 2 f?; absgg [ f?g

= L ! f?; absg

Note that absr(s) is in Or(s)n since ae;(g) ffi absr(s) = absr(s).

Finally, collections Ssn are obtained from Osn by closing under finite meets and joins
(uns , tns defined below)

Ssn = min X ' Osn such that for all non-empty finite subsets Y of Xl

ns Y 2 X ^ G ns Y 2 X

We now show that the inductive definition of the type collections Ss satisfy Invariant 4.10. Conditions 1-3 of Invariant 4.10 hold for the base collections Ss0. Assuming they
hold for Stn\Gamma 1, then by equations (4.2) and (4.3) they hold for Tsn and also for Osn. We now
inductively define the two binary operations uns and tns that perform meets and joins of
elements of Ssn and show that the invariants also hold for Ssn provided they hold for Ssn\Gamma 1.
At the same time we show that for all sorts s and all induction levels n,

I uns J ` I " J (4.5)
I tns J ' I [ J (4.6)

i.e., meets are smaller than set-theoretic intersections and joins are larger than set-theoretic
unions.

32

For the Term-sort the meet and join operations are particularly simple. The join
of two non-bottom elements of Otn is ? unless the elements are equal, or one is ?. Similarly,
the meet of two non-top elements is ? unless they are equal, or one is ?.

I ut J = 8??!??:

I if I = J
I if J = ?
J if I = ?
? otherwise

I tt J = 8??!??:

I if I = J
I if J = ?
J if I = ?
? otherwise

(4.7)

In the first three cases I ut J = I " J and I tt J = I [ J . In the last case, ?t = I ut J ` I " J
and ?t = I tt J ' I [ J. Thus (4.5) and (4.6) are satisfied for all n. The definition directly
implies that for all I; J 2 Otn we have I ut J 2 Otn and I tt J 2 Otn. Thus Stn = Otn and
Condition 1 of Invariant 4.10 is satisfied.

For notational convenience we define us = ts and ts = us.

I unft J = 8????!????:

OEc(I1 un\Gamma 1'1 J1; : : : ; Ik un\Gamma 1'k Jk) c : '1 \Delta  \Delta  \Delta  'k ! ft

I = OEc(I1; : : : ; Ik) ^ J = OEc(J1; : : : ; Jk)
I if J = ?
J if I = ?
? otherwise

(4.8)

I tnft J = 8????!????:

OEc(I1 tn\Gamma 1'1 J1; : : : ; Ik tn\Gamma 1'k Jk) c : '1 \Delta  \Delta  \Delta  'k ! ft

I = OEc(I1; : : : ; Ik) ^ J = OEc(J1; : : : ; Jk)
I if J = ?
J if I = ?
? otherwise

(4.9)

The meet and join for FlowTerm-ideals are defined in terms of the meets and joins of constructor arguments. The base cases (n = 0) degenerate to the cases for Term-ideals. Note
that Ij; Jj 2 S'jn\Gamma 1. Since S'jn\Gamma 1 is closed under meets and joins, we have Ij tn\Gamma 1'j Jj 2 S'jn\Gamma 1.

Therefore for all I; J 2 Oftn we have I unft J 2 Oftn and I tnft J 2 Oftn . Thus Sftn = Oftn and
Condition 1 is satisfied. By induction on (4.5) and (4.6) and the fact that OEc observes
variance, we conclude that I unft J ` I " J and I tnft J ' I [ J .

To simplify the cases for the meet and join of Row-ideals, we use the following
definition.

Definition 4.11 The kind of a Row-ideal I 2 Sr(s)n --written k(I)--is

k(I) = 8!:

?r(s) if I is maximal
?r(s) if I is minimal
absr(s) if I is closed

(4.10)

Note that ?r(s) ae absr(s) ` ?r(s) and thus these kinds are closed under union and intersection. The meet and join for elements in row type-collections is defined in terms of meets
and joins of the range of each label and the meet and join of the kinds.

I unr(s) J = aedom(I)[dom(J)(*l:I(l)

\Lambda un\Gamma 1

s J (l)) ffi ku(I; J ) (4.11)

I tnr(s) J = aedom(I)"dom(J)(*l:I(l) tn\Gamma 1s J (l)) ffi kt(I; J ) (4.12)

33
where

X

\Lambda un\Gamma 1

s Y = (X \Gamma  fabsg) un\Gamma 1s (Y \Gamma  fabsg)

and

ku(I; J) = 8????!????:

?r(s) if k(I) = k(I) " k(J) = absr(s)

and dom(J ) 6` dom(I)
?r(s) if k(J ) = k(I) " k(J ) = absr(s)

and dom(I) 6` dom(J)
k(I) " k(J ) otherwise

kt(I; J) = 8????!????:

?r(s) if k(I) = k(I) [ k(J) = absr(s)

and dom?(J ) 6` dom(I)
?r(s) if k(J ) = k(I) [ k(J ) = absr(s)

and dom?(I) 6` dom(J )
k(I) [ k(J ) otherwise

In order for these operations to be meets and joins of the constraint relation `r(s) which we
will define shortly, we require that if the meet of I and J is closed, then its domain must be
equal to the domain of the closed arguments (I, J , or I and J ), and similarly for the join.
The extra cases for ku and kt take care of this by making the meet minimal (join maximal),
if these domain constraints are not satisfied. The special cases for the meet are furthermore
needed to guarantee (4.6). Consider for example two closed Row-ideals I and J , such that
l 2 dom(I), I(l) 6= f?g, and l 62 dom(J ). Then there exists an element f of I, such that
f l 62 f?; absg. Note that l 62 dom(I t J), thus (I t J )(l) = f?; absg if k(I t J ) = absr(s).
But then f 62 I t J. The definition makes sure that in this case the kind of I t J becomes
?r(s) to guarantee that f 2 I t J .

We show that for all I; J 2 Or(s)n , I tnr(s) J is an element of Or(s)n . In the base
case (n = 0) the (minimum) domains of I and J are empty and the meet and join reduce to intersections and unions of kinds. Otherwise, I(l); J(l) 2 Ssn\Gamma 1 for l 2 dom(I) "
dom(J ). By induction, I(l) tn\Gamma 1s J (l) 2 Ssn\Gamma 1. Suppose dom(I) " dom(J ) is finite. Then,

aedom(I)"dom(J)(*l:I(l) tn\Gamma 1s J (l)) ffi kt(I; J) is an element of Or(s)n by definition. Otherwise, I
and J are minimal. In this case, dom?(I) [ dom?(J ) ae dom(I) " dom(J ) is finite and thus
I t J is equivalent to

aedom?(I)[dom?(J)(*l:I(l) tn\Gamma 1s J (l)) ffi ?r(s)

which is in Or(s)n . For unr(s) the argument is similar with the additional observation that if
abs 2 I(l) for some I 2 Sr(s)n , then I(l) \Gamma  fabsg 2 Ssn\Gamma 1. Thus Sr(s)n = Or(s)n and Condition 3
is satisfied. For (4.5) note that if f 2 I unr(s) J , then

l 2 dom(I) [ dom(J ) =) f l 2 I(l) \Lambda un\Gamma 1s J(l)
^ l 62 dom(I) [ dom(J ) =) f l 2 (k(I) " k(J ))(l)

By induction, I(l)

\Lambda un\Gamma 1

s J (l) ` I(l) " J (l) and thus f 2 I and f 2 J . The argument for (4.6)is given above.

34

The crux of the definition comes with the Set-sort. If we use set-theoretic intersection and union for the meet and join operations, we cannot guarantee Condition 2,
since projections of non-strict interface paths pt (of length jpj ^ n) yield arbitrary unions
and intersections of elements in Stn\Gamma jpj which themselves are not necessarily in Stn\Gamma jpj. We
therefore adjust the set-theoretic union and intersection to establish the invariant.

I uns J = max X 2 I(Vs) st. X ` I " J

and for all finite non-strict ptsae

p\Gamma 1(X) = p\Gamma 1(I) ukt p\Gamma 1(J ) if p even, k = n \Gamma  jpj
p\Gamma 1(X) = p\Gamma 1(I) tkt p\Gamma 1(J ) if p odd, k = n \Gamma  jpj

(4.13)

I tns J = min X 2 I(Vs) st. X ' I [ J

and for all finite non-strict ptsae

p\Gamma 1(X) = p\Gamma 1(I) tkt p\Gamma 1(J ) if p even, k = n \Gamma  jpj
p\Gamma 1(X) = p\Gamma 1(I) ukt p\Gamma 1(J ) if p odd, k = n \Gamma  jpj

(4.14)

To see that the operations uns and tns are well-defined we give an explicit construction of
the maximal (minimal) X. We will use the following auxiliary definitions. For any finite
non-strict path p, and element Y of Ssi\Gamma jpj, define

minp (Y ) = 8????!????:

Y if p = ffl
OEc(?'1 ::?'j\Gamma 1 ; minq(Y ); ?'j+1 ::?'

a(c)) if p = (c; j)qcovariant in j

OEc(?'1 ::?'j\Gamma 1 ; maxq(Y ); ?'j+1 ::?'

a(c)) if p = (c; j)q

contravariant in j

(4.15)

maxp (Y ) = 8????!????:

Y if p = ffl
?s \Gamma  OEc(?'1::?'

a(c)) [ OEc(?'1 ::?'j\Gamma 1 ; maxq(Y ); ?'j+1 ::?'a(c)) if p = (c; j)q

covariant in j

?s \Gamma  OEc(?'1::?'

a(c)) [ OEc(?'1 ::?'j\Gamma 1 ; minq(Y ); ?'j+1 ::?'a(c) ) if p = (c; j)q

contravariant in j

(4.16)

Note that maxp(Y ) 2 Sn. Using these min and max ideals, we can define uns and tns
explicitly (p ranges over non-strict paths present in I and J where k = n \Gamma  jpj * 0).

I uns J = I " J

" "

even pt

maxp ip\Gamma 1(I) ukt p\Gamma 1(J)j

" "

odd pt

maxp ip\Gamma 1(I) tkt p\Gamma 1(J )j

(4.17)

I tns J = I [ J

[ [

even pt

minp ip\Gamma 1(I) tkt p\Gamma 1(J )j

[ [

odd pt

minp ip\Gamma 1(I) ukt p\Gamma 1(J )j

(4.18)

We illustrate these definitions with an example.

35
Example 4.12 Consider two constant Term-constructors bool : t and int : t which denote
boolean and integer values, and a covariant unary mixed Set-constructor list : t ! s for
lists. Assuming semantic constructors nil and cons, the meaning function OElist is defined by

OElist(A) = X where X = fnil g [ fcons(h; t) j h 2 A \Gamma  f?g ^ t 2 Xg
Suppose we form the join of two Set-expressions

list(bool) t list(int)
and consider the natural denotation I of the above expression given by

I = OElist(OEbool) ts OElist(OEint)
If ts were simply Set-theoretic union, then the projection p\Gamma 1(I) of interface path p =
(list; 1) results in the ideal J = OEbool [ OEint. However, J 62 St, since it is not of the form
OEc(: : : ), i.e., there is no unique head constructor c for J . Our definition of ts avoids this
problem by raising the above join, such that p\Gamma 1(J ) = OEbool tt OEint = ?t. In particular,

OElist(OEbool) ts OElist(OEint) = OElist(OEbool) [ OElist(OEint) [ [

even pt

minp \Gamma p\Gamma 1(I) tt p\Gamma 1(J )\Delta 

= OElist(OEbool) [ OElist(OEint) [ min(

list;1) (OEbool tt OEint)

= OElist(OEbool) [ OElist(OEint) [ OElist(?t)
= OElist(?t)

where the second to last step follows from the definition of tt and minp, and the last
step follows from covariance of OElist. The projection p\Gamma 1(I) is thus ?t, which is the only
Term-ideal larger than both OEbool and OEint.
\Xi 

To see that construction (4.17) for I uns J satisfies requirement (4.13) in general, note that
we have for all finite even interface paths pt,

p\Gamma 1(I uns J ) = p\Gamma 1(I) " p\Gamma 1(J ) " p\Gamma 1 0@ "

even qt

nmax

q iq

\Gamma 1(I) ukt q\Gamma 1(J )j1A

= p\Gamma 1(I) " p\Gamma 1(J ) " p\Gamma 1 ` nmaxp ip\Gamma 1(I) ukt p\Gamma 1(J )j'
= p\Gamma 1(I) " p\Gamma 1(J ) " p\Gamma 1(I) ukt p\Gamma 1(J )
= p\Gamma 1(I) ukt p\Gamma 1(J )

The first equality follows from Lemma 4.8, the second simply chooses the path q = p from
the intersection Teven q, and the last equality follows from the inductive hypothesis that

I uks J ` I " J for all sorts s and k = n \Gamma  jpj ! n. The case for odd paths is analogous, and
similarly for tns .

36

Note that by Conditions iii, vi and vii of Axiom 4.1, minp(Y ) is by construction the
smallest set X such that p\Gamma 1(X) = Y . Assume that Z is another set satisfying (4.14), then
minp(p\Gamma 1(I)ttp\Gamma 1(J )) ` Z for all finite even non-strict interface paths pt and minp(p\Gamma 1(I)ut
p\Gamma 1(J )) ` Z for all finite odd non-strict interface paths pt. Thus Z ' I ts J and I ts J is
minimal. The argument for the maximality of I us J is analogous. By construction we have
I uns J ` I " J and I tns J ' I [ J . Condition 2 is satisfied by the definition.

The type-collections Ss are then defined as the limits of the series Ss0; Ss1; : : : .

Theorem 4.13 The limits Ss exist.
Proof: Each series Ss0; Ss1; : : : is monotonically increasing in the powerset lattice of
ideals P(I(Vs)). This lattice is complete, and thus the series Ss0; Ss1; : : : has a limit, which
is given by the least upper bound of the series. 2

4.4 Semantic Relations
Having defined our type-collections, we are ready to define the semantic relations `s and
show that they form lattices over Ss with meet and join being us and ts respectively. A
difficulty in defining the semantic relations for mixed constraints not present in non-mixed
formalisms is that the constraint relations are interdependent. For example, the constraint
relation for mixed Set-constraints is not simply set-theoretic inclusion. The relation depends
on the constraint relations of other sorts that appear in mixed Set-expressions.

We now define the semantic relation `s for each sort s and show that I `s J =)
I `Vs J, i.e., the semantic relations `s imply set-theoretic inclusion in the underlying value
domain Vs. For notational convenience, we state that I `s J () J `s I.

The semantic relation for the Term-sort is

? `t I

I `t ?
I `t I

(4.19)

Note that I `t J implies I ` J . For the FlowTerm-sort, the semantic relation is

? `ft I

I `ft ?
OEc(I1; : : : ; In) `ft OEc(J1; : : : ; Jn) () Ij `'j Jj

(4.20)

Note that I `ft J implies I ` J . The first two cases are trivial. For the last case note that
OEc observes the declared variance of constructor c (Axiom 4.1, Condition iii). For rows, the
semantic relation is

I `r(s) J () I `Vr(s) J ^

I and J closed =) dom(I) = dom(J ) ^
forall l 2 dom(J) I(l) `s J (l)

(4.21)

37
Two Row-ideals I and J stand in I `r(s) relation iff I is a set-theoretic subset of J (written
I `Vr(s) J), the domains of I and J agree if both are closed, and for all labels l in the
domain of J , the projection I(l) is related to J (l) according to `s. Note that I `Vr(s) J
implies dom(I) ' dom(J ).

The semantic relation `s has two parts, a standard set-theoretic inclusion on the
domain elements (written `Vs for clarity), and a set of interface constraints using interface
paths. Our definition of Ss guarantees that for any interface path pt and any ideal I 2 Ss,
p\Gamma 1(I) 2 St and thus the interface constraints are well-defined.

I `s J () I `Vs J ^ forall finite non-strict interface paths ptae

p\Gamma 1(I) `t p\Gamma 1(J ) p even
p\Gamma 1(J) `t p\Gamma 1(I) p odd

(4.22)

Note how the relations are stronger than the set-theoretic inclusions in the underlying
domain. This fact is not surprising, since the major goal of mixed constraints is to narrow
the set of possible solutions such that more specialized and efficient algorithms can be
applied.

Example 4.14 Reconsider the covariant mixed Set-constructor list : t ! s from Example 4.12. Suppose I and J are two Term-ideals from St and consider the set-theoretic
relation

OElist(I) ` OElist(J ) (4.23)
By covariance of OElist, this relation is satisfied iff I ` J . Now consider the relation `s

OElist(I) ` OElist(J )
which requires (4.23) and in addition

I `t J (4.24)
From the definition of `t, the relation (4.24) is satisfied if I = J , unless I = ?t or J = ?t.
The equality constraints arising through the use of Term-expressions can be represented
and solved more efficiently than inclusion constraints (Section 7.3).
\Xi 

Lemma 4.15 The operations us and ts are the meet and join operations of `s.
Proof: By induction on Ssi . For Ss0 the lemma holds, since meet and join degenerate to
intersection and union in that case.

For I tt J we have I `t I tt J and J `t I tt J from the definition. Suppose there
exists another upper-bound Z. If Z is ?t or ?t we are done. Otherwise, Z = I = J = I tt J
for all n. The case for ut is similar.

For I tft J we have by induction Ij `ft Ij tn\Gamma 1'j Jj and Jj `ft Ij tn\Gamma 1'j Jj. Thus
by variance of OEc, we have I `ft I tnft J and J `ft I tnft J . For the inductive step of
minimality, suppose there exists another upper-bound Z 2 Sft of I and J . If Z is ?ft or

38
?ft we are done. Otherwise, since constructor interpretations are disjoint, Z must be of the
form OEc(Z1; : : : ; Zk) with Ij `'j Zj and Jj `'j Zj. By induction, Ij tn\Gamma 1'j Jj `'j Zj, and
thus by variance of OEc, I tnft J `ft Z. The case for uft is similar.

For I tr(s) J we need to show that I `r(s) I tr(s) J, i.e., 1) I `Vr(s) I tr(s) J , 2) if
I, I tr(s) J are closed then dom(I) = dom(I tr(s) J ), and 3) that for all l 2 dom(I tr(s) J )
I(l) `s (I tr(s) J )(l). We have I tr(s) J = aedom(I)"dom(J)(*l:I(l) tn\Gamma 1s J (l)) ffi kt(I; J).
Since I tr(s) J ' I [ J ' I, we have the first part. For the second part, suppose I and
I tr(s) J are closed. Then by definition of tr(s) we have that J is not maximal, and that

dom?(J ) ` dom(I). If J is minimal, then dom(I tr(s) J ) = dom(I) " dom(J) = dom(I).
If J is closed, then we have dom(J ) ` dom(I) and symmetrically, dom(I) ` dom(J ). For
the third part we have for all l 2 dom(I tr(s) J ) = dom(I) " dom(J) that (I tr(s) J)(l) =

I(l) tn\Gamma 1s J (l). By induction, I(l) `s I(l) tn\Gamma 1s J (l) and thus for all l 2 dom(I tnr(s) J ) we
have I(l) `s (I tnr(s) J )(l) as desired. The case for unr(s) is analogous.

To show minimality, suppose there exists another upper-bound Z 2 Sr(s) of I and
J . We have I [ J `Vr(s) Z implying that dom(Z) ` dom(I [ J ) = dom(I) " dom(J) =
dom(I tr(s) J ). Suppose dom(Z) = dom(I tr(s) J ), otherwise we are done. Clearly, k(Z) '

kt(I; J ), otherwise Z is not an upper-bound. Assume k(Z) = kt(I; J ), otherwise we are
done. Then for all l 2 dom(Z) we have I(l) `s Z(l) and J (l) `s Z(l). By induction,
I(l) ts J (l) `s Z(l) and thus (I tr(s) J )(l) `s Z(l). The case for ur(s) is similar.

Finally, for I ts J we have by definition that I `Vs I ts J and J `Vs I ts J . Also
by definition, for all even interface paths pt, p\Gamma 1(I) `t p\Gamma 1(I tns J ) = p\Gamma 1(I) tkt p\Gamma 1(J ) and
for all odd interface paths pt, p\Gamma 1(I tns J ) = p\Gamma 1(I) ukt p\Gamma 1(J ) `t p\Gamma 1(I). Similarly for J .
Thus by induction (k ! n), we have I `s I tns J and J `s I tns J. Suppose there exists
another upper-bound Z. Then Z ' I [ J and p\Gamma 1(I) `t p\Gamma 1(Z) and p\Gamma 1(J ) `t p\Gamma 1(Z)
for even non-strict interface paths pt, and p\Gamma 1(Z) `t p\Gamma 1(I) and p\Gamma 1(Z) `t p\Gamma 1(J ) for odd
non-strict interface paths pt. By induction, p\Gamma 1(I) tt p\Gamma 1(J) `t p\Gamma 1(Z) for even p and
p\Gamma 1(Z) `t p\Gamma 1(I) ut p\Gamma 1(J ) for odd p. Thus by variance of constructor interpretations,
I ts J `s Z. The case for us is analogous. 2

Corollary 4.16 For all n, the collections Ssn with relations `s form lattices with meets and
joins defined as above; thus for all s

I ts J `s Z () I `s Z ^ J `s Z
I `s J us Z () I `s J ^ I `s Z

The motivation for our construction of the lattices (Ss; `s) was to exclude certain
undesired ideals in I(Vs) from Ss, and to make the relations `s stronger than set-theoretic
inclusion. These two points where illustrated by Example 4.12 and Example 4.14. Another
reason for the non-standard definition of Ss is to rule out sets that cannot be built using
constructor application, union, and intersection. Reconsider the standard interpretation of
a list constructor given in Example 4.12

OElist(A) = X where X = fnil g [ fcons(h; t) j h 2 A \Gamma  f?g ^ t 2 Xg

39
The set of lists of length greater than 0 (not containing nil ) is an ideal of Vs, but it cannot
be constructed by applying OElist. Note that a list constructor is not the only way to refer to
ideals for modeling lists. More precise ideals for lists can be denoted by using two distinct
constructors nil and cons. Using these constructors it is possible to distinguish non-empty
lists from all lists, a distinction that cannot be captured with the list constructor above.

4.5 Solutions
To give meaning to mixed constraints we define a meaning function, mapping expressions
to types, and then say that the solutions to a set of syntactic constraints are the solutions
of their semantic counterparts.

Variable assignments oe are maps from variables to elements of Ss. A variable
assignment oe is well-sorted, if for all sorts s and all X 2 Vs we have X 2 dom(oe) =)
oe(X ) 2 Ss. We consider only well-sorted variable assignments. Given a variable assignment
oe, we define the meaning _ of mixed expressions by

_[[X ]]oe = oe(X )
_[[0s]]oe = ?s
_[[1s]]oe = ?s
_[[c(E1; : : : ; En)]]oe = OEc(_[[E1]]oe; : : : ; _[[En]]oe)

_[[E1 u E2]]oe = _[[E1]]oe us _[[E2]]oe
_[[E1 t E2]]oe = _[[E1]]oe ts _[[E2]]oe

_[[:fc1; : : : ; cng]]oe = ?s \Gamma  [

c2fc1;::: ;cng

OEc(?'1 ; : : : ; ?'

a(c) ) c : '1 \Delta  \Delta  \Delta  'a(c) ! s

_[[his]]oe = absr(s)
_[[hl : Eliflg ffi E]]oe = aeflg(*l:_[[El]]oe) ffi _[[E]]oe

Since Ss is closed under meet and join, and Sr(s) closed under composition, we have that _
is well-sorted under any well-sorted variable assignment oe, i.e., E 2 Ls implies _[[E]]oe 2 Ss.

We can now define the meaning of mixed constraints. Under a given set of constructor signatures \Sigma , a mixed constraint problem S is the conjunction of constraint problems
Ss, where each Ss is a collection of s-constraints fE1 `s E01; : : : ; En `s E0ng. A solution to
such a system S is a well-sorted variable assignment oe, such that

_[[Ei]]oe `s _[[E0i]]oe forall Ei `s E0i 2 S

4.6 Discussion and Related Work
We start our discussion of the semantics of mixed constraints with a number of examples
that illustrate how the choice of constructor signatures influences the precision of mixed
constraints. Assume list is the unary constructor for lists with the semantic interpretation
OElist given in Example 4.12, and bool and int are constant constructors denoting the set of
boolean values and the set of integers. Consider the two constraints

list(bool) `s X

list(int) `s X (4.25)

40
We now explore the possible minimal solution for X under a number of constructor signatures for list; bool and int. First consider the signatures

list : s ! s
bool : s

int : s

defining all constructors as Set-constructors. Then the constraints (4.25) have the minimal
solution

X = OElist(OEbool) ts OElist(OEint)

= OElist(OEbool) [ OElist(OEint)

expressing that X is the set of integer and boolean lists, where each list contains either only
booleans, or only integers. Next consider the variation on the signatures where list is made
a FlowTerm-constructor.

list : s ! ft
bool : s

int : s

In this case the minimal solution of constraints (4.25) is

X = OElist(OEbool) tft OElist(OEint)

= OElist(OEbool ts OEint)
= OElist(OEbool [ OEint)

expressing that X is the set of lists where each element is either a boolean or an integer.
This set is strictly larger than the minimal solution under the previous signatures. Another
coarsening can be obtained by choosing all constructors to be of sort FlowTerm.

list : ft ! ft
bool : ft

int : ft

In this case the minimal solution of constraints (4.25) is

X = OElist(OEbool) tft OElist(OEint)

= OElist(OEbool tft OEint)
= OElist(?ft)

expressing that X is the set of all lists. A final coarsening can be obtained by making list
a Term-constructor.

list : ft ! t
bool : ft

int : ft

41
In this case the minimal solution of constraints (4.25) is

X = OElist(OEbool) tt OElist(OEint)

= ?t

expressing that X is the set of all values.

Condition 2 of Invariant 4.10 only applies to non-strict interface paths. To illustrate why, we show that the definition of meet and join for Set-expressions does not extend
to strict interface paths. Consider the following signatures.

a : t

b : t
c : t t ! s

Let X = _[[c(a; b)tc(b; a)]] be the join of c(a; b) and c(b; a). If c is non-strict, the construction
of the join adds the sets min(c;1)(attb) and min(c;2)(btta) so that the projections (c; 1)\Gamma 1(X)
and (c; 2)\Gamma 2(X) yield elements of St. We relied on the minimum and maximum sets minp
and maxp to force the projection of interface paths of the join and meet to be elements of the
appropriate type collection. The existence of such minimal and maximal sets is guaranteed
by Axiom 4.1 (Conditions iii,vi,vii). If c is strict however, then the meaning function OEc
is not injective and the construction min(c;1)(X) is always f?g for strict constructors with
more than one argument. Thus (c; 1)\Gamma 1(min(c;1)(X)) 6= X for all X strictly larger than f?g.

Most common constructor interpretations satisfy Axiom 4.1. Unfortunately, the
standard function constructor interpretation does not satisfy Condition vii of Axiom 4.1
since it is non-injective if the first argument is f?g. Instead of the standard function
interpretation, we can use a non-standard function interpretation that is injective. Consider
the interpretation,

OE!(X; Y ) = f(f; r) j f 2 (?t ! ?t [ fwrongg) [ ?; r 2 Y ^ x 2 X =) f x 2 Y g [ f?g

which pairs each function with a witness r from its range. Now for A ae ?t the set OE!(?t; A)
is distinct from OE!(?t; ?t) since the first set only contains pairs (f; r) where f is any
function, and r 2 A. Such an interpretation is still valid for the purposes of modeling
sets of functions in programs. It merely provides distinctions that are only needed for the
technical development and not observable by any actual denotations of program expressions.

Note that it is not simply our choice of the construction for the set maxp that
causes this problem. For the standard function constructor, there exists no maximal set
max(!;2)(X) such that (!; 2)\Gamma 1(max(!;2)(X)) = X. To see that this set does not exist,
note that our semantic domain V contains only functions strict in ?. Thus the set of
functions that map ? to an element in X is the set of all functions. To guarantee that the
projection of the range is X, we would need to add at least one element v to the domain,
restricting the set of functions to map v to an element of X. There are infinitely many
choices for v and no minimal set exists.

42
Applicability of the Model
The design goal for the model of mixed constraints is to provide an intuitive meaning
to mixed constraints that allows analysis designers to reason about the correctness of a
constraint-based program analysis. This goal motivates the choice of a denotational model
where mixed expressions denote certain sets of values, and where a mixed constraint E1 `s
E2 implies that the denotation of E1 is a set-theoretic subset of E2. If an analysis models
all flows of values using mixed constraints, then in principle, one obtains a sound overapproximation of the possible runtime values of any expression.

Consider an expression E used to over-approximate a set of values V . We say
that in this case E occurs in an L-context (Chapter 3). Note that if E contains any
sub-expressions in R-contexts, then those occur in argument positions of contravariant constructors. Clearly, if E is to be an over-approximation of V , then the projection of any
odd path p present in E must under -approximate the projection p\Gamma 1(V ). In general, expressions in L-contexts must over-approximate sets, and expressions in R-contexts must
under-approximate sets. Our choice of L- and R-contexts coincides with the occurrence of
such expressions in mixed constraints. If expressions E1 and E2 appear in a mixed constraint E1 `s E2, then we say that E1 appears in an L-context and E2 appears in an
R-context. The fact that E1 over-approximates a set V1 and E2 under-approximates a set
V2 makes the constraint harder to satisfy than the set-theoretic inclusion V1 ` V2. Thus, if
the mixed constraint E1 `s E2 is satisfied in a solution, then so is the constraint V1 ` V2.

Our definitions of meet and join are such that meets are no larger than set-theoretic
intersection and joins are not smaller than set-theoretic unions. If two expressions E1 and
E2 over-approximate the set of values at a given program point, then the join also overapproximates this set. Similarly, if E1 and E2 under-approximate a set, then so does their
meet. On the other hand, the meet of two expressions E1 and E2 that over-approximate
a set V does not necessarily over-approximate V , and dually for the join. As a result,
meets may be used in L-contexts only if they are guaranteed to coincide with set-theoretic
intersection, and joins may be used in R-contexts only if they are guaranteed to coincide
with set-theoretic union. This principle serves as a guide when designing an analysis.

Abstract Interpretation
The general theory of program analysis has been set on well-defined mathematical grounds
in the work on abstract interpretation originated by Cousot and Cousot [17]. In the view of
abstract interpretation, a program analysis is an evaluation of a program over an abstract
semantics. The abstract semantics of a programming language conservatively approximates
its concrete semantics. The conservative approximation is formalized by a Galois connection
(ff; fl) between the values of the concrete semantics (V) and the values of the abstract
semantics, drawn from an abstract domain A. The function ff : V ! A maps each concrete
value v to the abstract value a that "best" approximates it. The function fl : A ! 2V
maps abstract values a to the set of concrete values v that it approximates. The functions
ff and fl form a Galois connection, if for all v 2 V, v 2 fl(ff(v)), and for all a 2 A,
a = Ffff(v) j v 2 fl(a)g. The abstract domain A must form a join semi-lattice.

It is conjectured that any program analysis can be seen as an abstract interpre43
tation. In certain cases it may be necessary to consider a concrete semantics that is much
richer than the standard semantics of a programming language. Such cases arise when an
analysis infers information about a program execution that is not captured by the standard
concrete semantics, such as for example its memory allocation strategy [22].

Abstract interpretation is sometimes mistaken to stand only for the particular iterative fix-point approach proposed by Cousot and Cousot [17] to compute the solution of
an abstract interpretation. That algorithm is only one possible implementation strategy.
In general, abstract interpretations can be viewed as minimal solutions to systems of constraints between expressions denoting abstract values. These expressions involve meets and
joins and other monotone operations on abstract values.

The type-collections Ss for mixed expressions can thus be seen as abstract domains
for the concrete value domain V. Mixed expressions then provide syntax to denote abstract
values in these abstract domains, and mixed constraints provide lattice constraints between
these abstract values. The constructors and their signatures appearing in mixed expressions
synthesize the abstract domains. This synthesis has been recognized previously in the
context of pure set-expressions by Heintze [38] and been described by Cousot and Cousot
for set-expressions [18] and for type-expressions [16].

The Row Model
The row part of the model is to the best of our knowledge novel in its ability to combine
three distinct width-subtype relations for rows with depth-subtyping. Given two row types
r1 = hl : o/lil2A and r2 = hl : o/ 0l il2B, width-subtyping refers to the relation between the
domains of A and B of the rows, whereas depth subtyping refers to the subtype relationship
between the types o/l and o/ 0l at certain labels l.

If the rows model record types (finite functions from labels to types), widthsubtyping allows the right-side r2 to have a smaller domain than r1, and depth-subtyping
allows each type o/l to be less than o/ 0l for l 2 B. This notion of record subtyping is most
common in the literature [86, 12, 78]. Constraints between maximal rows of sort r(s) provide the width-subtyping of record types. Depth-subtyping is provided to the degree that
sort s provides subtyping. If s = t, only depth-subtyping involving ? and ? is present.

The constraint relation between closed rows has only depth-subtyping, but no
width-subtyping, since the domains on either side of the constraint must agree. This row
relation is not common in the literature. Such a relation is useful however for an analysis
that extends an ML-like type system with subtyping, but where subtyping on records is to
be limited to depth subtyping to match the absence of width-subtyping in ML.

Labeled variants are the dual of record types. A labeled variant is a label paired
with a value hl; vi. A variant type [l : o/l]l2A then denotes the set of pairs hl; vi, where l 2 A
and v 2 o/l. Variant types are treated in the literature as rows with a different subtype
relation than rows used for records. The width-subtyping relation is reversed, i.e., [l : o/l]l2A
is a subtype of [l : o/ 0l ]l2B iff A ` B and also o/l ^ o/ 0l for l 2 A. The mixed constraint
relation between minimal rows matches the constraint relation used for variant types. The
denotation given in terms of finite functions does not match up with the intuitive meaning
of variants as unions. But there is a simple isomorphism between a minimal Row-ideal J

44
and the set-interpretation of variants:

J j fhl; vi j l 2 L ^ v 2 J (l) \Gamma  f?gg
There are models for record types and subtype relations that are not based on a
denotational model. For example, Bruce and Longo describe models for subtyping of record
types (and other types) based on partial equivalence relations [11].

45
Chapter 5
Constraint Resolution

Constraint resolution is the process of deciding whether there exist any solutions to
a given system of constraints, and making the solutions explicit. Here we adapt techniques
developed by Aiken and Wimmers [3]. Constraints are solved by a collection of resolution
rules that transform the constraints into a canonical form.

An application of a resolution rule can be thought of as a rewrite step S ) S0,
transforming the constraints S into the constraints S0. A resolution rule is sound, if every
solution oe of S0 is also a solution of S, and a resolution rule is complete, if every solution
oe of S is also a solution of S0. If resolution rules may introduce fresh variables, the above
statement can be relaxed to say that a rule is sound, if for every solution oe0 of S0, there
exists a solution oe of S, such that oe0 and oe agree on the variables occurring in S, and
similarly for completeness.

All the resolution rules we consider preserve sorts, i.e., given a constraint E1 `s E2
of sort s where E1 and E2 are expressions of sort s, the resolution rules only generate wellformed constraints E `t E0 where the sorts of E and E0 agree with the constraint sort t.
This fact follows directly from the well-formedness of mixed expressions.

The chapter first defines the notion of inductive constraints. The subsequent sections show how to transform constraints of all sorts into inductive constraints through the
use of resolution rules. Section 5.5 defines the notion of an inductive system of constraints
and shows how to transform a system of constraints into an collection of inductive systems.
Inductive systems consist of inductive constraints and are closed under transitivity. Inductive systems contain no inconsistent constraints (constraints that cannot be satisfied in any
solution).

5.1 Inductive Constraints
It is necessary to distinguish variables occuring in the constraints at top-level (e.g. X `s
Y) from variables occurring inside constructors. The following definition (an extension of
Definition 2.3 to Row-expressions) makes the notion of top-level variables precise.

46
Definition 5.1 The set of top-level variables TLV(E) of an expression E is defined by

TLV(0) = fg TLV(1) = fg
TLV(X ) = fX g TLV(hi) = fg
TLV(c(: : : )) = fg TLV(hl : Eli ffi E) = TLV(E)
TLV(E1 t E2) = TLV(E1) [ TLV(E2) TLV(:fc1; : : : ; cng) = fg
TLV(E1 u E2) = TLV(E1) [ TLV(E2)

Note that the right side of a Row-composition expression appears at top-level.

In Section 5.5 we will show how to build solutions of the constraints using a double
induction over the level of the series Ss0; Ss1; : : : and an arbitrary, but fixed, sequence of the
variables. For this purpose, we assume a total ordering on variables characterized by an
injective mapping o : V ! N. Where convenient, we write the index o(X ) = i as a subscript
on the variable, as in Xi. We now define when a constraint is inductive.

Definition 5.2 (Inductive Constraint) A constraint E `s Xi or Xi `s E is inductive,
iff TLV(E) ` fX1; : : : ; Xi\Gamma 1g.

5.2 Set Constraint Resolution
This section describes the resolution of Set-constraints, i.e., the systematic transformation
of a system of constraints into inductive constraints. Before giving all the resolution rules
we discuss the difficult cases in the following subsections.

5.2.1 Upward-Closure and Negation
There are two forms of Set-constraints that are difficult to decompose during resolution:
E1 uE2 ` E3 and E1 ` E2 tE3. In pure set theory, the constraint E1 ` E2 [E3 is equivalent
to E1 " :E2 ` E3 (we use " and [ when talking about intersections and unions of pure
Set-expressions). However, if Set-expressions are interpreted as ideals (downward-closed
sets of values) as we do, the complement of a type is not necessarily a downward-closed
set, and thus not an ideal. For example, the complement of the function type 1 ! 0
contains every function except the least function *x:?, but the least function is part of
every function type. Even though general negation is not closed in the semantic domain,
L-intersections (intersections in L-contexts) and R-unions (unions in R-contexts) need not
be dropped entirely since there is a subset of the domain for which negation is closed,
namely the upward- and downward-closed subsets. A set X is upward-closed, if for all
x 2 X \Gamma  f?g; y 2 D we have y * x =) y 2 X.

Aiken and Wimmers identified simple restrictions on L-intersections and R-unions
so that negations only arise on upward-closed sets during the constraint resolution. The
two restrictions are

ffl R-unions E1 [ E2 must be disjoint, i.e., _[[E1]]oe " _[[E2]]oe = f?g in all solutions oe.

47
ffl L-intersections must be of the form E " M , where E is an L-expression subject to the

same restrictions and M is a ground expression (no variables) denoting an upwardclosed set. We call such expressions M-expressions.

More formally, we define when expressions are compatible with L-contexts (L-compatible)
or R-contexts (R-compatible). An expression that is both L- and R-compatible is said to
be LR-compatible.

ffl 0, 1, and null-ary constructors are LR-compatible.
ffl A variable X is LR-compatible.
ffl A constructed expression c(E1; : : : ; En) is

- L-compatible if covariant arguments are L-compatible and contravariant arguments are R-compatible.

- R-compatible if covariant arguments are R-compatible and contravariant arguments are L-compatible.

ffl An intersection E1 u E2 is

- L-compatible if E1 is L-compatible and E2 is an M-expression (or vice versa).
- R-compatible if E1 and E2 are R-compatible.

ffl A union E1 t E2 is

- L-compatible if E1 and E2 are L-compatible.
- R-compatible if E1 and E2 are disjoint in all interpretations.

In well-formed constraints, all expressions appearing in L-contexts are L-compatible, expressions appearing in R-contexts are R-compatible. Since we are only interested in well-formed
constraints, we can refer to L-compatible (R-compatible) expressions as L-expressions (Rexpressions).

Given only well-formed pure Set-constraints, Aiken-Wimmers showed that the
problematic forms can be rewritten using the following two rules:

E1 ` E2 [ E3 , E1 " :E2 ` E3 ^ E1 " :E3 ` E2 (5.1)

E1 " M ` E2 , E1 ` (E2 " M ) [ :M (5.2)

Expression E is the smallest M-expressions s.t. _[[E]]oe ' _[[E]]oe for all variable assignments
oe. The type expression :E denotes (V \Gamma  _[[E]]oe) [ f?g, which is the complement of E. (To
see this, note that _[[E]]oe [ _[[:E]]oe = V and _[[E]]oe " _[[:E]]oe = f?g.)

However, note that these rules don't make any progress in simplifying the constraints unless there is an additional requirement: L-intersections E1 " :E2 and E1 " :E3
arising in the first rule must be simplified to L-expressions containing only intersections on
variables of the form X " M . To do so, it must be possible to move intersections under
constructors, i.e.,

c(E1; : : : ; En) " c(M1; : : : ; Mn) = c(E01; : : : ; E0n)

48
for some expressions E0i. Thus, M-expressions in L-intersections must be explicit, i.e.,
a union of constructor expressions. Consequently, the ability to compute the two type
expressions E and :E explicitly for any expression E is required.

In order to generalize this approach to mixed constraints we must address the
following questions.

ffl Are the above rewrite rules sound and complete for `s? Given that we use us and ts

instead of " and [, this is not obvious.

ffl Given that constructor interpretations are abstract, can we compute explicit forms of

upward-closure and negation?

ffl What are the restrictions on constructor interpretations so that we can simplify Lintersections syntactically?

We proceed as follows. We first review the approach of Aiken-Wimmers for computing
upward-closure and negation expressions and show that it is unsuitable for mixed constraints. We then introduce an abbreviation expression and new resolution rules that serve
our purpose and show that the resolution rules are sound and complete for `s. The problem
of simplifying intersections is deferred to the following section.

The algorithms for computing E and :E given by Aiken and Wimmers [3] are
syntax-directed and depend crucially on the interpretation of constructors as labeled tuples.
The table below reproduces these rules. (For simplicity, we avoided the extra cases to keep
the resulting unions of M-expressions disjoint.)

X = 1 0 = 0
c(E1; : : : ; En) = c(E1; : : : ; En) 1 = 1

E1 [ E2 = E1 [ E2 E1 " E2 = E1 " E2
:fc1; : : : ; cng = :fc1; : : : ; cng

:0 = 1 :1 = 0
:c(M1; : : : ; Mn) = :fcg [ [

j=1::n

c(1; : : : ; 1; :Mj; 1; : : : ; 1)

:(M1 [ M2) = :M1 " :M2
:(:fc1; : : : ; cng) = [

j=1::n

cj(1; : : : ; 1)

For example, the negation of an M-expression c(a) is :fcg [ c(:fag). The rules for :M
assume that constructors are strict and that

c(M1; : : : ; Mn) [ [

j=1::n

c(1; : : : ; 1; :Mj ; 1; : : : ; 1) = c(1; : : : ; 1)

Having given a semantics to mixed expressions based on ideals I(V), we face the
same limitations as Aiken-Wimmers concerning L-intersections and R-unions. However,

49
there are additional complications in the context of mixed constraints due to the fact that
we have abstracted the constructor interpretations in the semantics. Without knowing the
interpretation of a constructor c, an expression c(E1; : : : ; Ea(c)) cannot automatically be
transformed into its upward-closure or negation. To see this, consider a standard function
constructor \Delta  ! \Delta  with the interpretation

OE!(X; Y ) = ff j x 2 X =) f x 2 Y g [ f?g
Since the least function *x:? is less than any other function according to the domain ordering ^ it is an element of every downward-closed set containing functions. As a result, the
upward-closure of any downward-closed set containing some function contains all functions.
In other words, the expression E1 ! E2 is 0 ! 1 (the set of all functions1) for any E1 and
E2.

Even if we are given an explicit M-expression its negation can still not be computed
as above because the second assumption used by the negation algorithm does not hold in
general. As an example, reconsider the interpretation of a list constructor given in Example 4.12 and suppose true denotes the set ftrue; ?g containing the value true. Observe that
the expression list(true) is upward-closed, (the semantic constructor cons used by the interpretation is strict, and there are no functions involved). However, list(true) [ list(:ftrueg) is
not equal to list(1). To see this note that the 2-element list [true; false] has one element from
true and one element not in true and is therefore neither part of list(true) nor list(:ftrueg).
This problem does not arise because OElist is non-strict in its argument. Consider the unary
constructor pair and its interpretation given by

OEpair(A) = fp(t1; t2) j t1; t2 2 A \Gamma  f?gg [ f?g
Even though OEpair is strict, the union pair(true) [ pair(:ftrueg) is not equal to pair(1). To
see this, note that the element p(true; false) 2 pair(1) is neither part of pair(true) nor of
pair(:ftrueg).

Given that the semantics of constructors in mixed constraints is a priori not known,
we cannot hope to compute the upward-closure and negation of expressions automatically.
Fortunately, inspection of the two Rules 5.1 and 5.2 shows that the set of M-expressions
required during resolution is fixed by the initial constraints. Therefore, we can circumvent
the problem by putting the constraints in a form that makes all required M-expressions
explicit in the initial system of constraints. We define an abbreviation Pat as follows.2

Pat[E; M ] = (E u M ) t :M
where M is an M-expression. With Pat we reformulate the resolution rules for L-intersections
as follows.

10 ! 1 is the set of all functions in V since V contains only strict functions, i.e., mapping ? to ?. If V
also contains non-strict functions, then the set ff j f ? = wrongg is not in 0 ! 1. Strictness however has
the undesired property of inducing a set of equivalences: 0 ! X j 0 ! 1 for all downward-closed sets X.

2Pat stands for pattern, since the expressions it abbreviates are used most frequently in constraints

generated for pattern matching.

50
Theorem 5.3 The following rule is sound and complete.

E1 u M `s E2 , E1 `s (E2 u M ) t :M (5.3)

j E1 `s Pat[E2; M ] (5.4)

Rule 5.3 reduces to rule 5.2 if \Sigma s contains only pure constructors. We now prove its soundness and completeness under any \Sigma s. We need the following observation and lemma.

Observation 5.4 The definition of mixed expressions guarantees that M-expressions do
not contain subexpressions of the form c(: : : ), where c is a mixed constructor. This follows
from the requirement that M-expressions are ground, and that other-sorted arguments to
mixed constructors must be variables, 0, or 1 (Definition 3.1).

Lemma 5.5 Given an M-expression M of sort s, we have p\Gamma 1(M ) = ?t for all even interface paths pt present in M and p\Gamma 1(M ) = ?t for all odd interface paths present in M .

Proof: By Observation 5.4, the only subexpressions of M whose denotations contain
interface paths are of the form E = :fc1; : : : ; cng. Furthermore, since M is upward-closed,
such subexpressions occur in covariant contexts. Thus for all interface paths pt present in
M , there exists a sub-expression :fc1; : : : ; cng of M and a path q such that p = qr. Since
the sub-expression appears in a covariant context, q must be even. Thus r has the same
variance as p. We can thus restrict ourselves to the case where M = :fc1; : : : ; cng. We
proceed by induction on the length of p. Let pt be an interface path present in M . Base
case: p = (c; j), where c is a Set-constructor not in fc1; : : : ; cng. If c is covariant in j then
p is even and p\Gamma 1(M ) = ?t. Otherwise p is odd and p\Gamma 1(M ) = ?t. Induction: p = (c; j)r
with r 6= ffl. Then c's jth argument is of sort s. If c is covariant in j then (c; j)\Gamma 1(M ) = ?s.
If p is even, then r is even and by induction, p\Gamma 1(M ) = r\Gamma 1(?s) = r\Gamma 1(_[[:fg]]) = ?t. If p is
odd, then r is odd and p\Gamma 1(M ) = ?t by the same reasoning. If c is contravariant in j, then
(c; j)\Gamma 1(M ) = ?s, but since r 6= ffl, r cannot be present in ?s and thus there is no such p. 2

From Lemma 5.5 we immediately obtain the following
Corollary 5.6 For all expressions E, and variable assignments oe,

_[[E u M ]]oe = _[[E]]oe us _[[M ]]oe = _[[E]]oe " _[[M ]]oe
and also

_[[E t M ]]oe = _[[E]]oe ts _[[M ]]oe = _[[E]]oe [ _[[M ]]oe

By the above corollary we are free to write E " M and E [ M instead of E u M and E t M
without causing confusion as to the operation being performed. We now prove Theorem 5.3.
Proof: of Theorem 5.3 First we deal with the inclusion on the underlying domain Vs.
To simplify the notation, we write E to mean _[[E]]oe for some fixed oe. By Corollary 5.6 we
have E1 us M = E1 " M `Vs E2, and thus by set theory E1 `Vs E2 " M [ :M . Applying
Corollary 5.6 again, we obtain E1 `Vs E2 us M ts :M .

We now prove that the interface constraints hold as well. Direction =) : Assume
E1 " M `s E2. Let pts be a non-strict interface path present in E1. Assume p is even. There

51
are two cases. First assume p is present in M . By assumption we have p\Gamma 1(E1 " M ) `t
p\Gamma 1(E2). By Lemma 4.8 we have p\Gamma 1(E1) " p\Gamma 1(M ) `t p\Gamma 1(E2). By Lemma 5.5 we have
p\Gamma 1(M ) = Vt. Since M and :M are disjoint and p non-strict it follows that p is not present
in :M and p\Gamma 1(:M ) = f?g. Thus p\Gamma 1(E1) `t p\Gamma 1(E2) " p\Gamma 1(M ) [ p\Gamma 1(:M ). Applying
Lemma 4.8 once more, we obtain p\Gamma 1(E1) `t p\Gamma 1(E2 " M [ :M ) and by Corollary 5.6
p\Gamma 1(E1) `t p\Gamma 1(E2 us M ts :M ). We still need to show the above for all even paths
p present in E1, but not in M . Since M and :M are disjoint, present (p; :M ). Then
p\Gamma 1(:M ) = Vt and p\Gamma 1(E2 " M [ :M ) = Vt. Thus p\Gamma 1(E1) `t p\Gamma 1(E2 " M [ :M ) holds
for all even p present in E1.

Now assume p is odd. There are again two cases. Assume p is present in M .
By assumption we have p\Gamma 1(E2) `t p\Gamma 1(E1 " M ). By Lemma 5.5 we have p\Gamma 1(M ) =
f?g. Also, since p is non-strict and M disjoint from :M , p is not present in :M and
thus p\Gamma 1(:M ) = Vt. By Corollary 5.6 and Lemma 4.8 we have \Gamma p\Gamma 1(E2) [ p\Gamma 1(M )\Delta  "
p\Gamma 1(:M ) `t p\Gamma 1(E1 " M ). Applying Lemma 4.8 and canceling p\Gamma 1(M ) on the right,
we obtain p\Gamma 1(E2 " M [ :M ) `t p\Gamma 1(E1). A final application of Corollary 5.6 yields
p\Gamma 1(E2 us M ts :M ) `t p\Gamma 1(E1). We still need to show the above for all odd paths
p present in E1, but not in M . Since M and :M are disjoint, present (p; :M ). Then
p\Gamma 1(:M ) = f?g and p\Gamma 1(E2 " M [ :M ) = f?g. Thus p\Gamma 1(E2 us M ts :M ) `t p\Gamma 1(E1)
holds for all odd p present in E1.

Now we prove (: Assume p non-strict and present in E1 " M . Thus p present in
E1. Assume first that p is even. Then by assumption, we have p\Gamma 1(E1) `t p\Gamma 1(E2 " M [
:M ). Applying Corollary 5.6 and Lemma 4.8 we obtain p\Gamma 1(E1) `t p\Gamma 1(E2) " p\Gamma 1(M ) [
p\Gamma 1(:M ). Since p is even and present in M we have p\Gamma 1(M ) = Vt by Lemma 5.5. Since
p is non-strict, it is thus not present in :M and thus p\Gamma 1(:M ) = f?g. We thus have
p\Gamma 1(E1) " p\Gamma 1(M ) `t p\Gamma 1(E2). Applying Lemma 4.8 and Corollary 5.6 once more, we
obtain the desired conclusion p\Gamma 1(E1 usM ) `t p\Gamma 1(E2). Now assume that p is odd. Then by
assumption we have p\Gamma 1(E2usM ts:M ) `t p\Gamma 1(E1). Applying Corollary 5.6 and Lemma 4.8
we obtain (p\Gamma 1(E2) [ p\Gamma 1(M )) " p\Gamma 1(:M ) `t p\Gamma 1(E1). Since p is odd and present in M we
have p\Gamma 1(M ) = f?g by Lemma 5.5. Since p is non-strict, it is thus not present in :M and
thus p\Gamma 1(:M ) = Vt. We thus have p\Gamma 1(E2) `t p\Gamma 1(E1) [ p\Gamma 1(M ). Applying Lemma 4.8
and Corollary 5.6 once more, we obtain the desired conclusion p\Gamma 1(E2) `t p\Gamma 1(E1 us M ).
2

Using Rule 5.4 the negation :M no longer appears in the abbreviation and there
is thus no need to compute it. Representing arbitrary disjoint R-unions is more complex.

Lemma 5.7 If A; B 2 Ss such that for all even paths p, either p\Gamma 1(A) = ? or p\Gamma 1(B) = ?
and for all odd paths p either p\Gamma 1(A) = ? or p\Gamma 1(B) = ?, then

A us B = A " B
Similarly, if A; B 2 Ss such that for all even paths p, either p\Gamma 1(A) = ? or p\Gamma 1(B) = ?
and for all odd paths p either p\Gamma 1(A) = ? or p\Gamma 1(B) = ?, then

A ts B = A [ B

52
Proof: Simply note that in the first case, for all even pt, p\Gamma 1(A"B) = p\Gamma 1(A)"p\Gamma 1(B) =
p\Gamma 1(A) ut p\Gamma 1(B). For odd pt, p\Gamma 1(A " B) = p\Gamma 1(A) [ p\Gamma 1(B) = p\Gamma 1(A) tt p\Gamma 1(B). The
proof for ts is analogous. 2

Let E1 and E2 denote disjoint types. Observe that by applying Corollary 5.6
numerous times

Pat[E1; E1] us Pat[E2; E2] us Pat[0; :(E1 [ E2)]

= (E1 " E1 [ :E1) us (E2 " E2 [ :E2) us (E1 [ E2) def. of Pat
= (E1 [ :E1) us (E2 [ :E2) us (E1 [ E2) E " E = E
= \Gamma (E1 [ :E1) us (E1 [ E2)\Delta  us \Gamma (E2 [ :E2) us (E1 [ E2)\Delta  associativity
= \Gamma (E1 [ :E1) " (E1 [ E2)\Delta  us \Gamma (E2 [ :E2) " (E1 [ E2)\Delta  Corollary 5.6
= \Gamma E1 " (E1 [ E2) [ :E1 " (E1 [ E2)\Delta  us\Gamma 

E2 " (E1 [ E2) [ :E2 " (E1 [ E2)\Delta  distribute
= (E1 [ E2) us (E2 [ E1) E " E = E; E1 us E2 = 0
= (E1 [ E2) " (E2 [ E1) Lemma 5.7
= E1 [ E2 E1 " E2 = 0

To represent an R-union E1 t E2 we therefore need the M-expressions for the upwardclosures E1 and E2, as well as the M-expression for the complement :(E1 [ E2). As long as
these M-expressions exist, the R-union can be represented. Below, we show the resolution
of a constraint involving E1 t E2:

E ` E1 t E2 = E ` Pat[E1; E1] u Pat[E2; E2] u Pat[0; :(E1 [ E2)]

, E ` Pat[E1; E1] ^ E ` Pat[E2; E2] ^ E ` Pat[0; :(E1 [ E2)]
, E " E1 ` E1 ^ E " E2 ` E2 ^ E " :(E1 [ E2) ` 0

Only R-unions of M-expressions remain, and these are themselves M-expressions only appearing in second positions of Pat where they need never be decomposed.

To summarize, the two rules (5.2) and (5.1) for rewriting L-intersections and Runions are replaced with the new rule (5.4) using the abbreviation Pat. The implicit negation
present in Pat avoids the need to form negations during resolution.

5.2.2 L-Intersection Simplification
Constraint simplification as presented by Aiken-Wimmers requires simplifying intersections
between L-expressions and M-expressions. The basic axiom that enables this simplification
in their model is

c(X1; : : : ; Xn) " c(Y1; : : : ; Yn) = c(X1 " Y1; : : : ; Xn " Yn)
for any constructor c. In mixed constraints, constructors may be contravariant in certain
arguments. The natural generalization of the above axiom is thus

c(X1; : : : ; Xn) " c(Y1; : : : ; Yn) = c(X1 " Y1; : : : ; Xn " Yn)
where

" = ae " for covariant arguments[ for contravariant arguments

53
Unfortunately, this generalization is incorrect even for standard function constructors. Consider the type expression

true ! false " false ! true

The boolean negation function is certainly an element of this type. However, under the
axiom, the above type should be equivalent to (true [ false) ! 0, which does not contain
the boolean negation function.

We can weaken the axiom since we know that c(Y1; : : : ; Yn) must be an Mexpression c(M1; : : : ; Mn). Suppose c is contravariant in argument j. Thus Mj co-varies
with the domain of a function set in the interpretation. For that set to be upward-closed,
the domain must be f?g, and thus c(M1; : : : ; Mn) = c(M1; : : : ; Mj\Gamma 1; 0; Mj+1; : : : ; Mn).
This motivates the following weakening.

Axiom 5.8 (LI-Simplification) For any Set-constructor c, L-expression c(E1; : : : ; Ea(c))
and any M-expression c(M1; : : : ; Ma(c)), interpretation OEc must satisfy

OEc(E1; : : : ; Ea(c)) " OEc(M1; : : : ; Ma(c)) = OEc(E01; : : : ; E0a(c))
where

E0j = ae Ej " Mj c covariant in jE

j c contravariant in j

By Corollary 5.6 we immediately have

OEc(E1; : : : ; Ea(c)) us OEc(M1; : : : ; Ma(c)) = OEc(E01; : : : ; E0a(c))
where

E0j = ae Ej us Mj c covariant in jE

j c contravariant in j

Under this weakening, standard function interpretation satisfies the axiom, since
the only upward-closed set of functions is OE!(f?g; Vs) (all functions):

OE!(X; Y ) us OE!(f?g; Vs) = OE!(X; Y )

= OE!(X; Y us Vs)

The abstraction of constructor interpretations in mixed constraints leaves enough
freedom to violate even this weaker axiom. As an example, consider the following interpretation

OEc(A; B) = fc(t) j t 2 (A [ B) \Gamma  f?gg [ f?g

Note that c(false; true) is an M-expression. Yet,

c(true; false) " c(false; true) = fc(true); c(false ); ?g " fc(true); c(false); ?g

= fc(true); c(false ); ?g
6= f?g
= c(0; 0)
= c(true " false; false " true)

54

Constructor interpretations that violate Axiom 5.8 cannot be used in the mixed
constraint system we present, and it is up to the analysis designer to ensure that no such
constructor interpretations are used. We have not found this restriction to be significant in
practice--standard interpretations satisfy Axiom 5.8.

Given that M-expressions are pure Set-expressions, driving intersection below constructors only produces non-trivial intersections on Set-expressions. We need the following
Lemma for the correctness of our simplification rules.

Lemma 5.9 For any M-expression M , and any expressions E1 and E2,

(E1 t E2) " M = E1 " M t E2 " M
Proof: We again write E for the denotation _[[E]]oe for a fixed oe. By Corollary 5.6
(E1 ts E2) us M = (E1 ts E2) " M . By the definition of ts, this is the smallest set X greater
than (E1 [ E2) " M , such that for all non-strict interface paths ptae

p\Gamma 1(X) = (p\Gamma 1(E1) tt p\Gamma 1(E2)) " p\Gamma 1(M ) p even
p\Gamma 1(X) = (p\Gamma 1(E1) ut p\Gamma 1(E2)) [ p\Gamma 1(M ) p odd

There are two cases, first assume p present in M , then since p\Gamma 1(M ) = ?t for even p and
p\Gamma 1(M ) = ?t for odd p, we haveae

p\Gamma 1(X) = p\Gamma 1(E1) " p\Gamma 1(M ) tt p\Gamma 1(E2) " p\Gamma 1(M ) p even
p\Gamma 1(X) = (p\Gamma 1(E1) [ p\Gamma 1(M )) ut (p\Gamma 1(E2) [ p\Gamma 1(M )) p odd

But this is exactly the condition on (E1 " M ) ts (E2 " M ). Now suppose p is not present
in M . Since p\Gamma 1(M ) = ?t for even p and p\Gamma 1(M ) = ?t for odd p, we haveae

p\Gamma 1(X) = ?t p even
p\Gamma 1(X) = ?t p odd

which is equivalent toae

p\Gamma 1(X) = p\Gamma 1(E1) " p\Gamma 1(M ) tt p\Gamma 1(E2) " p\Gamma 1(M ) p even
p\Gamma 1(X) = (p\Gamma 1(E1) [ p\Gamma 1(M )) ut (p\Gamma 1(E2) [ p\Gamma 1(M )) p odd

Thus in either case, the smallest set X greater than (E1 [ E2) " M is equal to the smallest
set greater than E1 " M [ E2 " M satisfying the above, which is E1 " M ts E2 " M . By
Corollary 5.6 we conclude (E1 ts E2) us M = E1 us M ts E2 us M . 2

We now give the rules for simplifying L-intersections. L-expressions and Mexpressions adhere to the productions

L ::= 0 j X j X " M j L1 [ L2 j c(E1; : : : ; Ea(c)) j :fc1; : : : ; cng
M ::= 0 j M1 [ M2 j c(M1; : : : ; Ma(c)) j :fc1; : : : ; cng

Note that M-expressions are a subset of L-expressions. Figure 5.1 shows the equations
governing the simplification of L-intersections and M-intersections. Symmetric cases and
the case covered by Axiom 5.8 are omitted. The rules are correct by Corollary 5.6 and
Lemma 5.9.

55
0 " M = 0

L " 0 = 0
(X " M1) " M2 = X " (M1 " M2)

(L1 [ L2) " M = L1 " M [ L2 " M
L " (M1 [ M2) = L " M1 [ L " M2

c(: : : ) " d(: : : ) = 0 c 6= d

c(E1; : : : ; Ea(c)) " :fc1; : : : ; cng = ae 0 if c 2 fc1; : : : ; cngc(E

1; : : : ; Ea(c)) otherwise

:fc1; : : : ; cng " :fd1; : : : ; dmg = :fc1; : : : ; cn; d1; : : : ; dmg

Figure 5.1: L-intersection simplification

5.2.3 Set Resolution Rules
Figure 5.2 shows the complete set of resolution rules for Set-constraints. The rules transform a system of constraint sets into a simpler system of constraint sets. More than a single
constraint set is required when constraints can be decomposed in different ways. For example, Rule 5.7, which simplifies constraints between strict constructor expressions, produces
two constraint systems. The first corresponds to the case where the expression on the left
is assumed to be non-zero. The second constraints set captures the solutions when the
expression on the left is zero.

The rules should be read as left-to-right rewrite rules. Rules 5.5-5.12 are from
Aiken-Wimmers. The correctness of Rules 5.9 and 5.10 follows from the fact that us and
ts are meet and join operations for `s. Correctness of Rule 5.12 follows from Corollary 5.6.
Rule 5.7 is sound but not complete in the presence of strict interface paths.

The rules below the horizontal line in Figure 5.2 are new rules. Rule 5.13 and
Rule 5.14 are proven correct by Theorem 5.3. The rules involving negations are straightforward.

5.3 Term and FlowTerm Resolution
The rules in Figure 5.3 for FlowTerm resolution are a subset of the rules for the Set-sort. The
rules in Figure 5.4 for Term constraints are similar, except that Rule 5.27 for constructor
resolution is symmetric in the constructor arguments. The soundness and completeness
argument is analogous to the one for Set-constraints.

5.4 Row Resolution
Recall from Section 4.3 that every Row-expression denotes one of three kinds of Row-ideals:
a maximal, closed, or minimal Row-ideal. This section gives complete resolution rules for
Row-constraints involving all three kinds of Row-expressions. However, the implementation
described in the second part of this dissertation only implements a fraction of the resolution rules, namely all rules involving Row-expressions without Row-variables, and all rules

56

\Gamma ; S [ f0 `s Eg j \Gamma ; S (5.5)
\Gamma ; S [ fc(E1::En) `s c(E01::E0n)g j \Gamma ; S [ fEj `'j E0j j c : '1 \Delta  \Delta  \Delta  'n ! sg

c non-strict (5.6)
\Gamma ; S [ fc(E1::En) `s c(E01::E0n)g j \Gamma ; S [ fEj `'j E0j j c : '1 \Delta  \Delta  \Delta  'n ! sg;

S [ fc(E1::En) `s 0g c strict (5.7)

\Gamma ; S [ fc(E1::En)g `s E j ae \Gamma  c : '1 \Delta  \Delta  \Delta  'n ! s non-strict\Gamma ; S [ fE

1 `'1 0g; : : : ; S [ fEn `'n 0g c strict

if E is 0 or d(::) where c 6= d (5.8)

\Gamma ; S [ fE1 [ E2 `s Eg j \Gamma ; S [ fE1 `s E; E2 `s Eg (5.9)
\Gamma ; S [ fE `s E1 " E2g j \Gamma ; S [ fE `s E1; E `s E2g (5.10)

\Gamma ; S [ fX `s X g j \Gamma ; S (5.11)
\Gamma ; S [ fX " M `s X g j \Gamma ; S (5.12)

\Gamma ; S [ fE1 `s Pat[E2; M ]g j \Gamma ; S [ fE1 " M `s E2g (5.13)

\Gamma ; S [ fX " M `s Eg j \Gamma ; S [ fX `s Pat[E; M ]g (5.14)

\Gamma ; S [ f:fc1::cng `s Eg j 8!:

\Gamma ; S if E = :fd1::dmg and

fd1::dmg ` fc1::cng
\Gamma  otherwise

(5.15)

\Gamma ; S [ fc(E1::En)g `s :fc1::ckg j ae \Gamma  c non-strict\Gamma ; S [ fc(E

1::En) `s 0g c strict

if c 2 fc1::ckg (5.16)

\Gamma ; S [ fc(::) `s :fd1::dmgg j \Gamma ; S if c 62 fd1::dmg (5.17)

\Gamma ; S [ fE1 `s E2g j \Gamma ; S [ fE2 `s E1g (5.18)

Figure 5.2: Resolution rules for Set-constraints.

\Gamma ; S [ f0 `ft Eg j \Gamma ; S (5.19)
\Gamma ; S [ fE `ft 1g j \Gamma ; S (5.20)
\Gamma ; S [ fc(E1::En) `ft c(E01::E0n)g j \Gamma ; S [ fEj `'j E0j j c : '1 \Delta  \Delta  \Delta  'n ! ftg (5.21)

\Gamma ; S [ fc(E1::En)g `ft E j \Gamma  if E is 0 or d(::) where c 6= d (5.22)

\Gamma ; S [ fX `ft X g j \Gamma ; S (5.23)
\Gamma ; S [ fE1 `ft E2g j \Gamma ; S [ fE2 `ft E1g (5.24)

Figure 5.3: Resolution of FlowTerm-constraints.

57
\Gamma ; S [ f0 `t Eg j \Gamma ; S (5.25)
\Gamma ; S [ fE `t 1g j \Gamma ; S (5.26)
\Gamma ; S [ fc(E1::En) `t c(E01::E0n)g j \Gamma ; S [ fEj `'j E0j; E0j `'j Ej j c : '1 \Delta  \Delta  \Delta  'n ! tg

(5.27)
\Gamma ; S [ fc(E1::En)g `t E j \Gamma  if E is 0 or d(::) where c 6= d (5.28)

\Gamma ; S [ fX `t X g j \Gamma ; S (5.29)
\Gamma ; S [ fE1 `t E2g j \Gamma ; S [ fE2 `t E1g (5.30)

Figure 5.4: Resolution of Term-constraints.

involving Row-variables that denote closed Row-ideals. This section may safely be skipped
on a first reading.

Before we develop the core resolution rules for Row-constraints, we make a simplifying assumption. We assume that the kind of each Row-variable in a system of constraints
S is fixed a priori.3 Recall from Definition 4.11 that the kind k(X) of a Row-ideal is either
?r(s), ?r(s) or absr(s). In other words, we associate a unique kind with each Row-variable.
We make this kind explicit where needed by superscripts on variables. Thus X c is a variable which denotes only closed Row-ideals in all solutions. We simply say that X is closed.
Minimal Row-variables are written X ? and maximal Row-variables are written X ?. Where
convenient, we use the same superscripts on expressions. Given the explicit kinds of Rowvariables, we consider only solutions oe that agree with the kind of each variable, formally

k(oe(X ?)) = ?r(s)

k(oe(X c)) = absr(s)
k(oe(X ?)) = ?r(s)

Note that the kind annotation on variables fixes the kind of every Row-expression E.

The main problem faced in the resolution of Row-constraints is that the right
side of a Row-composition expression appears at top-level. In order for a constraint hl :
EliA ffi X `r(s) Y to be inductive, the index of X must be less than the index of Y, i.e.,
o(X ) ! o(Y). If that is not the case, we need a way to break up the composition. We
proceed as follows. The next section introduces domain constraints which are used to justify
a number of resolution rules. Domain constraints are furthermore used in the following
section to argue the termination of the resolution of constraints when both sides are closed
Rows. Finally we show how to split Row-constraints in the remaining cases.

3The resolution under fixed kinds is easier to formulate. In principle, complete resolution of Rowconstraints without fixed kinds is possible by solving the constraints under each kind assignment.

58
5.4.1 Domain Constraints
Consider the following resolution rule for transforming a constraint of the form

hl : Elil2A ffi E `r(s) hl : E0lil2B ffi E0
where A " B 6= ;, into a set of simpler constraints.

hl : Elil2A ffi E `r(s) hl : E0lil2B ffi E0 () ^

l2A"B

El `s E0l ^

hl : Elil2A\Gamma B ffi E `s hl : E0lil2B\Gamma A ffi E0 (5.31)
The rule is sound, i.e., if the constraints on the right of () have a solution, then that
solution also satisfies the original constraint. However, for the simplification to be complete,
i.e., a solution to the first constraint is also a solution to the second set of constraints, we
need to know how E and E0 behave on the range of labels in A " B. If E(l) and E0(l)
can be arbitrary for l 2 A " B and l 2 dom(E0), then the constraints E(l) `s E0(l) may
not be satisfied, even though the original constraint is. We thus establish an invariant that
specifies what is known about E, if E appears on the right side of a composition expression
hl : EliA ffi E.

Invariant 5.10 (Row Composition) For any Row-expression of the form hl : Elil2A ffi E,
the (minimum) domain of E does not contain any labels from A in all solutions. Formally,

E(l) = rng(k(E)) 8l 2 A (5.32)
Note that the range of k(X) is the unique set Y , such that there exist infinitely many l0 2 L
for which X(l0) = Y . Precisely,

rng(k(X)) = 8!:

f?g if X is minimal
f?; absg if X is closed
?s [ fabsg if X is maximal

The invariant restricts the denotation of E such that for each label l 2 A and each function
f 2 E, it holds that f l 2 rng(k(E)). Furthermore, for each l 2 A and each value v 2
rng(k(E)), there exists a function f 2 E, such that f l = v.

Invariant 5.10 guarantees that Rule 5.31 is complete, i.e., direction =) holds.
However, the rule is no longer sound, since not all solutions for the right-hand side establish
the above invariant for the original constraint. In order to obtain a rule that is sound
and complete, we need to make the restrictions on the domain of Row-expressions explicit
through a new form of constraints called domain constraints.

Definition 5.11 The domain-complement of a Row-ideal I, written ff(I) is the set of labels
not in the minimum domain of I. Formally,

ff(I) = L \Gamma  dom?(I)

59
A domain variable ffX stands for the set of labels that must be "absent" from the minimum
domain of Row-variable X in all solutions. For any solution oe of the constraints, the
solution oe(ffX ) of the domain variable and the solution oe(X ) of the associated Row-variable
are related by.

oe(ffX ) = ff(oe(X c)) if X closed
oe(ffX ) ` ff(oe(X )) otherwise (5.33)

Note that if X is closed, then ffX is exactly the domain complement of X . Otherwise,
ffX is only a lower bound of the domain-complement of X . In general domain-complement
expressions have the form

ffi ::= N j ffX j ff(E) j ffi [ N j ffi " N
N ::= :A j A

where N is either a finite or co-finite set of labels. The domain-complement ff(E) of a
Row-expression E is a lower-bound (equal if E is closed) of the domain-complement of E
in all solutions oe, i.e.,

_[[ff(Ec)]]oe = ff(_[[Ec]])oe

_[[ff(E)]]oe ` ff(_[[E]])oe

Variable assignments oe are extended to map domain variables to subsets of L, and _ is
extended over domain-complement expressions as follows.

_[[N ]]oe = N
_[[ffX ]]oe = oe(ffX )
_[[ff(hl : EliA ffi E?)]]oe = (_[[ff(E?)]]oe \Gamma  fl j _[[El]]oe 6= ?sg)

_[[ff(hl : EliA ffi E)]]oe = (_[[ff(E)]]oe \Gamma  A) E 6= E?

_[[ff(0)]]oe = L
_[[ff(hi)]]oe = L

_[[ff(1)]]oe = L
_[[ffi [ N ]]oe = _[[ffi]]oe [ N
_[[ffi " N ]]oe = _[[ffi]]oe " N

Domain constraints now have the form

N `d ffi
ffX `d ffi
ff(Ec) `d ffi

Domain constraints are a special form of set constraints. The notions of top-level variables
and inductive constraints are easily adapted to domain constraints. Using the simplification
rules given in Figure 5.7, domain constraints can be transformed into inductive constraints

60
of the form

N `d ffX
ffX " N `d ffY

ffX `d ffY [ N
ffX `d N
ffX `d ff(hl : EliA ffi 0)

Domain constraints enable us to restrict the minimum domain of a Row-variable to
exclude certain labels. A domain constraint has the form A `d ffX , where A is a set of labels.
We extend domain constraints to Row-expressions E of sort r(s) by writing A `d ff(E).
The constraint states that the minimum domain of E must not contain any labels from A.
Invariant 5.10 is now made explicit using a domain constraint.

Invariant 5.10 (Row Composition Revised) If a system of constraints S contains any
sub-expression of the form hl : Elil2A ffi E, then there are domain constraints in S implying
that A `d ff(E) (written S j= A `d ff(E)).

Furthermore, we can now state a sound and complete version of Rule 5.31.
Resolution Rule 5.12 (Common Labels)

S [ fhl : Elil2A ffi E `r(s) hl : E0lil2B ffi E0g ()

S [ fEl `s E0l j l 2 A " Bg [
fhl : Elil2A\Gamma B ffi E `s hl : E0lil2B\Gamma A ffi E0g

where S j= A `d ff(E) ^ B `d ff(E0)

The above rule is sound because the constraints A `d ff(E) and B `d ff(E0) implied by
S on the right establish Invariant 5.10 on the left. Similarly, the rule is complete, because
Invariant 5.10 on the left implies the domain constraints explicit on the right.

5.4.2 General Row Resolution
Given the three kinds of Row-expressions in our language, there are nine possible combinations for the kinds of E1 and E2 in a constraint E1 `r(s) E2. Since maximal rows are never
a subset of closed rows, and closed rows are never a subset of minimal rows, three cases are
easily identified as having no solutions.

\Gamma ; S [ fE? `r(s) Ecg j \Gamma 
\Gamma ; S [ fE? `r(s) E?g j \Gamma 

\Gamma ; S [ fEc `r(s) E?g j \Gamma 

61
Next we consider the base cases, where the sides are equal, the left side is 0, or the right
side is 1.

\Gamma ; S [ fE `r(s) Eg j \Gamma ; S

\Gamma ; S [ f0 `r(s) Eg j \Gamma ; S
\Gamma ; S [ fE `r(s) 1g j \Gamma ; S

Now we consider the cases where one side does not contain a variable. The following cases
assume that A " B = ;, A " C = ;, B " C = ;, and neither A nor B is empty but C may be
empty. Note that Rule 5.12 handles the cases when the intersection A " B is non-empty.

\Gamma ; S [ fhl : EliA ffi 0 `r(s) hl : E0liB ffi Eg j \Gamma ;S [ fhl : EliA ffi 0 `r(s) Eg (5.34)

\Gamma ; S [ fhl : EliA ffi E `r(s) hl : E0liC ffi 0g j \Gamma ;S [ fEl `s 0 j l 2 Ag [

fE `r(s) hl : E0liC ffi 0g
\Gamma ; S [ fhl : EliC ffi hi `r(s) hl : E0liB ffi Eg j \Gamma  (5.35)
\Gamma ; S [ fhl : EliA ffi E `r(s) hl : E0liC ffi hig j \Gamma ;S [ fEl `s 0 j l 2 Ag [

fE `r(s) hl : E0liC ffi 0g
\Gamma ; S [ fhl : EliC ffi 1 `r(s) hl : E0liB ffi Eg j \Gamma 
\Gamma ; S [ fhl : EliA ffi E `r(s) hl : E0liC ffi 1g j \Gamma ;S [ fE `r(s) hl : E0liC ffi 1g (5.36)

We illustrate the correctness of equivalence (5.35), the other cases are argued similarly.
Note that B is non-empty and disjoint from C. Thus, in all solutions oe, there exists l0 2 B,
such that (_[[hl : EliC ffi hi]]oe)(l0) = f?; absg. But, the right-side of the constraint maps l0
to (_[[hl : E0lil2B ffi E]]oe)(l0) = _[[E0l0]]oe 2 Ss, and abs is not an element of any ideal in Ss for
any sort s (abs only appears in the range of functions modeling Row-ideals).

We are left with cases where both sides contain variables. Again, A " B = ; and
at least one of A or B is non-empty.

hl : EliA ffi X `r(s) hl : E0liB ffi Y
There are three cases. Either X = Y, or A is non-empty and o(X ) ? o(Y), or B is nonempty and o(Y) ? o(X ). Otherwise the constraint is inductive. The first case (X = Y)
splits into three cases again, depending on the kind of X . If X is maximal or closed, the
constraint is inconsistent. To see why, note that by Invariant 5.10, B must be absent from
the domain of X . Thus the left side contains functions returning abs for some l 2 B,
whereas the right side contains no such functions. If X is minimal, X maps l 2 A [ B to ?
by Invariant 5.10. Thus the constraint is satisfied as long as El `s 0 for l 2 A.

\Gamma ; S [ hl : EliA ffi X c `r(s) hl : E0liB ffi X c j \Gamma 
\Gamma ; S [ hl : EliA ffi X ? `r(s) hl : E0liB ffi X ? j \Gamma  (5.37)
\Gamma ; S [ hl : EliA ffi X ? `r(s) hl : E0liB ffi X ? j \Gamma ; S [ fEl `s 0 j l 2 Ag (5.38)

The remaining cases are harder and dealt with in the following subsections. First we consider
the case where both sides are closed.

62
5.4.3 Splitting of Closed Rows
Consider the constraint between closed rows hl : EliA ffi X `r(s) hl : E0liB ffi Y where X and
Y are distinct. Since both sides are closed, the constraint is only satisfied if the domains of
both sides are equal. Suppose B is non-empty and o(Y) ? o(X ) (the case where A is nonempty and o(X ) ? o(Y) is symmetric). In all solutions, X must be of the form hl : XliB ffi X 0
where Xl `s E0l for l 2 B and hl : EliA ffi X 0 `r(s) Y. This can be captured by the rule

Resolution Rule 5.13 (Closed Row Split)

S [ fhl : EliA ffi X `r(s) hl : E0liB ffi Yg j S [ fB `d ffX 0g [

fhl : XliB ffi X 0 `r(s) X ; X `r(s) hl : XliB ffi X 0g [

fXl `s E0l j l 2 Bg [ fhl : EliA ffi X 0 `r(s) Yg

where X 0 is a fresh closed Row-variable of sort r(s) and Xl with l 2 B are fresh variables of
sort s. If we choose the index of X 0 less than the index of X , the resulting constraints on
X and Y are inductive, since o(X 0) ! o(X ) ! o(Y).

Since this rule introduces fresh variables, we need to argue that the rule is only
applied finitely many times given a finite initial system of constraints. The argument
assumes that no other sorts introduce fresh variables. Call the variable X 0 a child of X .
Note the domain constraint B `d ffX 0 associated with the fresh Row-variable X 0. The
domain of X 0 is strictly smaller than the domain of X , since everything absent in X must
be absent in X 0, and additionally B is absent from X 0, whereas B is contained in the domain
of X . Since there are only finitely many distinct labels in the constraints, X 0 the number
of generations generated from X 0 is finite. Furthermore, since the index of X 0 and all of its
descendants have smaller index than X , they cannot cause the same rule to trigger another
split of X . Thus Rule 5.13 can be applied only finitely many times.

5.4.4 Splitting of Minimal and Maximal Rows
The splitting-approach of closed Row-expressions does not work for minimal and maximal
rows because the domains of the two sides need not be equal in this case. Instead we
transform Row-constraints involving a minimal or maximal row using the following two
basic transformations.

Resolution Rule 5.14 (Split-Left)

hl : EliA ffi X `r(s) E () hl : EliA ffi 0 `r(s) E ^

hl : 0il2A ffi X `r(s) E

The correctness of this rule follows from the fact that

hl : EliA ffi X = hl : EliA ffi 0 tr(s) hl : 0iA ffi X
Note that we depend crucially on the definition of tr(s). If tr(s) were set-theoretic union,
the equivalence would not hold. Similarly, the following rule splits rows on the right side of
the inclusion.

63
Resolution Rule 5.15 (Split-Right)

E `r(s) hl : EliA ffi X () E `r(s) hl : EliA ffi 1 ^

E `r(s) hl : 1il2A ffi X

The correctness of this rule follows from the fact that

hl : EliA ffi X = hl : EliA ffi 1 ur(s) hl : 1iA ffi X
These rules are correct for all Row-kinds. Note that we only apply these rules if E contains
a top-level variable Y such that o(X ) ? o(Y). Otherwise, one of the rules in Section 5.4.2
can be applied.

Rules 5.14 and 5.15 are only the first step in obtaining inductive constraints.
Consider the constraint hl : 0il2AffiX `r(s) E obtained by applying Rule 5.14. This constraint
is still not inductive. Intuitively, we need an expression E0 that is equivalent to E, except
that the set A is masked out from the domain, and such that hl : 0iA ffiX `r(s) E is equivalent
to X `r(s) E0. The constraint X `r(s) E0 is then inductive.

In general, we need to extend our Row-expression language with two forms of
expressions.

Definition 5.16 Let A be a finite subset of the set of labels L. If E is a well-formed
L-compatible minimal (maximal) r(s)-expression, then E ^A 0 is also a well-formed Lcompatible minimal (maximal) r(s)-expression, with the meaning

E ^A 0 = ff j l 2 A =) f l 2 ?s ^ l 62 A =) f l 2 (_[[E]]oe)(l)g (5.39)
Furthermore, if E is a well-formed minimal (maximal) r(s)-expression, then E .A 1 is also
a well-formed minimal (maximal) r(s)-expression. The meaning of E .A 1 is

_[[E .A 1]]oe = 8??!??:

ff j l 2 A =) f l 2 ?s ^ l 62 A =) f l 2 (_[[E]]oe)(l)g

if E is minimal
ff j l 2 A =) f l 2 ?s [ fabsg ^ l 62 A =) f l 2 (_[[E]]oe)(l)g

if E is maximal

(5.40)

We call expressions of the form E ^A 0 L-masks and expressions of the form E .A 1 Rmasks. Intuitively, E ^A 0 is equivalent to E, except that the rows in E ^A 0 map all labels
in A to ?. Similarly, E .A 1 is equivalent to E, except that for l 2 A, no assumptions on
the rows of E .A 1 are made. This last requirement also motivates why we do not define
E .A 1 when E is closed. The denotation of E .A 1 when E is closed must be the same
as the one when E is maximal. However, that denotation is not a closed Row-ideal, since
(E .A 1)(l) = ?s [ fabsg for l 2 A. For minimal and maximal Rows the denotations of the
new expression are already present in our type-collections.

The introduction of L-masks and R-masks is governed by the following two rules.

Resolution Rule 5.17 If E0 is not closed, then the following equivalence holds.

hl : 0iA ffi E `r(s) E0 ^ A `d ff(E) () E `r(s) E0 .A 1 ^ A `d ff(E)

64
Proof: Note that A `d ff(E) implies by (5.16) that _[[E]]oe = _[[E .A 1]]oe for all oe. To
simplify the notation, we drop the _[[\Delta ]]oe notation in the proof. Furthermore to simplify the
proof, we assume that I .A 1 is defined for closed row-ideals as for maximal row-ideals (5.16).
Note that for any row-ideal I, I `Vr(s) I .A 1, and for any row-ideal I, hl : 0iA ffi E `Vr(s) E.
Also note that \Delta  .A 1 as a unary operation on ideals (5.16) is monotone it its argument,
and so is row-composition in its right argument. Direction =) : We first show that
E `Vr(s) E0 .A 1, and then show that the interface constraints are satisfied. We have that
E `Vr(s) E .A 1 = (hl : 0iA ffi E) .A 1 `r(s) E0 .A 1, where the last step follows from
monotonicity of \Delta  .A 1. For the interface constraints, consider l 2 dom(E0) \Gamma  A. Clearly
l 2 dom(E0), and thus E(l) = (hl : 0iA ffi E)(l) `s E0(l) = E0 .A 1(l). Direction (=: Note
that hl : 0iA ffi (E0 .A 1) `Vr(s) E0. By monotonicity of ffi, we have hl : 0iA ffi E `Vr(s) hl :
0iA ffi (E0 .A 1) and thus hl : 0iA ffi E `Vr(s) E0. For the interface constraints, consider
l 2 A. Clearly (hl : 0iA ffi E)(l) `s ?s `s E0(l). Otherwise l 2 dom(E0) \Gamma  A. We have
(hl : 0iA ffi E)(l) = E(l) `s (E0 .A 1)(l) `s E0(l). 2

Resolution Rule 5.18 If E is not closed, then the following equivalence holds.

E `r(s) hl : 1iA ffi E0 ^ A `d ff(E0) () E ^A 0 `r(s) E0 ^ A `d ff(E0)
The proof is similar to the previous resolution rule.

Finally, we need one more rule that allows us to move the mask from the left of
an inclusion to the right and vice-versa.

Resolution Rule 5.19 If E and E0 are not closed, then the following equivalence holds.

E ^A 0 `r(s) E0 () E `r(s) E0 .A 1
Proof: Direction =) : We have 8f:f 2 E ^A 0 =) f 2 E0 we proof 8f:f 2 E =)
f 2 E0 .A 1. Suppose f 2 E. If f 2 E ^A 0 we have f 2 E0 and thus f 2 E0 .A 1. Otherwise,
there exits f 0 2 E ^A 0, such that for all l 62 A we have f 0(l) = f (l). Since f 0 2 E0, we have
f 2 E0 .A 1. For the interface constraints, first assume E0 is maximal. Then dom(E0 .A 1) =
dom(E0)\Gamma A, and for all l 2 dom(E0)\Gamma A, we have E(l) = (E ^A 0)(l) `s E0(l) = (E .A 1)(l).
If E0 is minimal, then the above argument also holds. In addition, for all l 2 A, we have
E(l) `s (E0 .A 1)(l) = ?s. Direction (: We have 8f:f 2 E =) f 2 E0 .A 1 we proof
8f:f 2 E ^A 0 =) f 2 E0. Suppose f 2 E ^A 0. Then f 2 E and thus f 2 E0 .A 1.
By Definition 4.9 and (5.16), we have that f 2 E0, since for all l 2 A, f (l) = ?, and
? 2 E0(l) for all labels l. For the interface constraints, suppose l 2 dom(E0). If l 2 A, then
(E ^A 0)(l) = ?s `s E0(l). Otherwise, (E ^A 0)(l) = E(l) `s (E0 .A 1)(l) = E0(l). 2

Before we show how to apply the above rules to transform the remaining constraints into inductive form, we state a number of equivalences involving ^A0 and .A1.

1 ^A 0 = hl : 0iA ffi 1
0 ^A 0 = 0
(E ^B 0) ^A 0 = E ^A[B 0
(hl : Elil2B ffi E) ^A 0 = hl : Elil2B\Gamma A ffi (E ^A 0)

65
1 .A 1 = 1
0 .A 1 = hl : 1iA ffi 0
(E .B 1) .A 1 = E .A[B 1
(hl : Elil2B ffi E) .A 1 = hl : Elil2B\Gamma A ffi (E .A 1)

These equivalences suggest that Row-expressions involving a Row-variable X can always be
normalized to the form

hl : EliA ffi [X ]B
where [X ]B is X if B = ;, or

[X ]B = ae X ^B 0 if [X ] appears to the left of `r(s)X .

B 1 if [X ] appears to the right of `r(s)

Where convenient, we drop the label set subscript. The remaining non-inductive constraints
are then of the form

hl : EliA ffi [X ] `r(s) hl : E0liB ffi [Y]
where A " B = ;, at least one of A and B is non-empty, and not both X and Y are closed.
By combining Rule 5.14 with Rule 5.17, we obtain

Resolution Rule 5.20 If A " B = ;, A is non-empty, o(X ) ? o(Y), and Y is not closed,
then the following equivalence holds.

S [ fhl : Elil2A ffi [X ] `r(s) hl : E0lil2B ffi [Y]g ()

S [ fhl : EliA ffi 0 `r(s) hl : E0liB ffi [Y]g [

f[X ] `r(s) hl : E0lil2B\Gamma A ffi ([Y] .A 1)g

where S j= A `d ff([X ])

The resulting constraint involving X and Y is either inductive ([X ] = X ), or inductive after
applying Rule 5.19. The constraint involving only Y can be transformed into an inductive
constraint using (5.34) and Rule 5.19.

The analogous rule in the case where o(Y) ? o(X ) is obtained by combining
Rule 5.15 and Rule 5.18.

Resolution Rule 5.21 If A " B = ;, B is non-empty, o(Y) ? o(X ), and X is not closed,
then the following equivalence holds.

S [ fhl : Elil2A ffi [X ] `r(s) hl : E0lil2B ffi [Y]g ()

S [ fhl : EliA ffi [X ] `r(s) hl : E0liB ffi 1g [

fhl : EliA\Gamma B ffi ([X ] ^B 0) `r(s) [Y]g

where S j= B `d ff([Y])

66
The resulting constraint involving X and Y is inductive ([Y] = Y), or inductive after
applying Rule 5.19. The constraint involving only X can be transformed into and inductive
constraint using (5.36) and Rule 5.19.

Since we needed to restrict the above rules to the cases where the lower indexed
variable is not closed, we introduce the following invariant.

Invariant 5.22 The index of any closed Row-variable X c is larger than the index of any
minimal or maximal Row-variable Y.

Since it is always possible to choose a variable order satisfying the invariant, we do not
restrict the family of constraints that can be solved.

Due to the introduction of masks, there are still some constraints that we haven't
dealt with. The equivalences (5.37) and (5.38) need to be extended to the cases

S [ fhl : EliA ffi [X ?]B `r(s) hl : E0liC ffi [X ?]Dg
S [ fhl : EliA ffi [X ?]B `r(s) hl : E0liC ffi [X ?]Dg

where A " B. Furthermore, since we never form new composition expressions involving
masks, we know by Invariant 5.10 that S implies the domain constraints A `d ffX and
C `d ffX . Applying Rule 5.14 followed by (5.34), Rule 5.17, and Rule 5.19 we obtain the
equivalent constraints

S [ fhl : EliA\Gamma D ffi 0 `r(s) X ; X `r(s) hl : E0liC\Gamma B ffi (X .A[B[D 1)g

where S j= A [ C `d ffX

If X is minimal, then the domain constraints imply that X (l) = f?g in all solutions for
all l 2 A [ C. Thus the second constraint is always satisfied and the first is equivalent to
fEl `s 0 j l 2 A \Gamma  Dg.

If X is maximal, then the domain constraints imply that X (l) = ?s [ fabsg in
all solutions for all l 2 A [ C. Thus the first constraint is always satisfied and the second
constraint is satisfied if and only if C ` B. The complete set of rules is summarized in
Figures 5.5-5.7.

5.5 Inductive Systems
The resolution rules presented so far allow us to transform an arbitrary system of constraints
into an equivalent collection of constraint systems consisting only of inductive constraints.
This section defines inductive systems, which are systems of inductive constraints closed
under transitivity. We show how to transform an original system of constraints into an
equivalent collection of inductive systems. If this collection is empty, then the original
constraints have no solutions. Otherwise, the original constraints satisfy the constraints up
to any finite level of the type collections Ss0; Ss1; Ss2; : : : (Section 4.3). We conjecture that
inductive systems always have solutions.

To simplify the presentation, we initially leave out Row-sorts, i.e., we assume that
the constraints contain no Row-variables. We say that a constraint E `s X is L-inductive,

67
\Gamma ; S [ fE? `r(s) Ecg j\Gamma  (5.41)
\Gamma ; S [ fE? `r(s) E?g j\Gamma  (5.42)

\Gamma ; S [ fEc `r(s) E?g j\Gamma  (5.43)

\Gamma ; S [ fE `r(s) Eg j\Gamma ; S (5.44)

\Gamma ; S [ f0 `r(s) Eg j\Gamma ; S (5.45)
\Gamma ; S [ fE `r(s) 1g j\Gamma ; S (5.46)
\Gamma ; S [ fX `r(s) X g j\Gamma ; S (5.47)
\Gamma ; S [ fE1 `r(s) E2g j\Gamma ; S [ fE2 `r(s) E1g (5.48)

The rules below assume that A " B = ;; A " C = ;; B " C = ;, A and B are non-empty,
and C may be empty.

\Gamma ; S [ fhl : EliA ffi 0 `r(s) hl : E0liB ffi Eg j\Gamma ; S [ fhl : EliA ffi 0 `r(s) Eg (5.49)
\Gamma ; S [ fhl : EliA ffi E `r(s) nhl : E0liC ffi 0g j\Gamma ; S [ fEl `s 0 j l 2 Ag [ (5.50)

fE `r(s) hl : E0liC ffi 0g
\Gamma ; S [ fhl : EliC ffi hi `r(s) hl : E0liB ffi Eg j\Gamma  (5.51)

\Gamma ; S [ fhl : EliA ffi E `r(s) hl : E0liC ffi hig j\Gamma S [ fEl `s 0 j l 2 Ag [ (5.52)

fE `r(s) hl : E0liC ffi 0g
\Gamma ; S [ fhl : EliC ffi 1 `r(s) hl : E0liB ffi Eg j\Gamma  (5.53)

\Gamma ; S [ fhl : EliA ffi E `r(s) hl : E0liC ffi 1g j\Gamma ; S [ fE `r(s) hl : E0liC ffi 1g (5.54)
\Gamma ; S [ hl : EliA ffi X c `r(s) hl : E0liB ffi X c j\Gamma  (5.55)
\Gamma ; S [ fhl : EliA ffi [X ?]B `r(s) hl : E0liC ffi [X ?]Dg j\Gamma ; S C ` B (5.56)
\Gamma ; S [ fhl : EliA ffi [X ?]B `r(s) hl : E0liC ffi [X ?]Dg j\Gamma  C 6` B (5.57)
\Gamma ; S [ fhl : EliA ffi [X ?]C `r(s) hl : E0liB ffi [X ?]Dg j\Gamma ; S [ fEl `s 0 j l 2 A \Gamma  Dg (5.58)

Figure 5.5: Resolution of Row-constraints (simple cases).

68
The rule below assumes A " B 6= ; and that S j= A `d ff(E) ^ B `d ff(E0)

\Gamma ; S [ fhl : Elil2A ffi E `r(s) hl : E0lil2B ffi E0g j\Gamma ; S [ fEl `s E0l j l 2 A " Bg [ (5.59)

fhl : Elil2A\Gamma B ffi E `s hl : E0lil2B\Gamma A ffi E0g

The two rules below assume o(X c) ? o(Yc), A " B = ;, A non-empty, and that S j= A `d
ffX ^ B `d ffY . Variable X 0 is a fresh closed variable, Xl for l 2 B are fresh variables of
sort s.

\Gamma ; S [ fhl : EliA ffi X c `r(s) hl : E0liB ffi Ycg j\Gamma ; S [ fB `d ffX 0g [ (5.60)

fhl : XliB ffi X 0 `r(s) X cg [
fX c `r(s) hl : XliB ffi X 0g [
fXl `s E0l j l 2 Bg [
fhl : EliA ffi X 0 `r(s) Ycg
\Gamma ; S [ fhl : EliB ffi Yc `r(s) hl : E0liA ffi X cg j\Gamma ; S [ fB `d ffX 0g [ (5.61)

fhl : XliB ffi X 0 `r(s) X cg [
fX c `r(s) hl : XliB ffi X 0g [
fEl `s Xl j l 2 Bg [
fYc `r(s) hl : E0liA ffi X 0g

The following rules assume o(X ) ? o(Y), Y is not closed, A " B = ;, and A non-empty.
S j= A `d ff([X ])

\Gamma ; S [ fhl : Elil2A ffi [X ] `r(s) hl : E0lil2B ffi [Y]g j\Gamma ; S [ fhl : EliA ffi 0 `r(s) hl : E0liB ffi [Y]g [

f[X ] `r(s) hl : E0liB\Gamma A ffi ([Y] .A 1)g (5.62)
\Gamma ; S [ fhl : Elil2B ffi [Y] `r(s) hl : E0lil2A ffi [X ]g j\Gamma ; S [ fhl : EliB ffi [Y] `r(s) hl : E0liA ffi 1g [

fhl : EliB\Gamma A ffi ([Y] ^A 0) `r(s) [X ]g (5.63)
\Gamma ; S [ fX ^A 0 `r(s) Eg j\Gamma ; S [ fX `r(s) E .A 1g (5.64)
\Gamma ; S [ fE `r(s) X .A 1g j\Gamma ; S [ fE ^A 0 `r(s) X g (5.65)

Figure 5.6: Resolution of Row-constraints (complex cases).

69
\Gamma ; S [ f; `d ffig j\Gamma ; S (5.66)
\Gamma ; S [ fffi1 `d ffi2 [ :;g j\Gamma ; S (5.67)
\Gamma ; S [ fffi `d ff(0) [ N g j\Gamma ; S (5.68)
\Gamma ; S [ fffi `d ff(hi [ N )g j\Gamma ; S (5.69)

\Gamma ; S [ fffi `d ff(1) [ N g j\Gamma ; S (5.70)
\Gamma ; S [ fffi `d ff(X ) [ N g j\Gamma ; S [ fffi `d ffX [ N g (5.71)

\Gamma ; S [ fffX `d ffY [ N g j\Gamma ; S [ fffX " N `d ffY g o(X ) ! o(Y) (5.72)

\Gamma ; S [ fff(hi) `d ffig j\Gamma ; S [ f:; `d ffig (5.73)
\Gamma ; S [ fff(hl : EliA ffi Ec) `d ffig j\Gamma ; S [ fff(Ec) `d ffi [ :Ag (5.74)

\Gamma ; S [ fN1 `d N2g j\Gamma ; S N1 ` N2 (5.75)
\Gamma ; S [ fN1 `d N2g j\Gamma  N1 6` N2 (5.76)
\Gamma ; S [ fN1 `d ffi [ N2g j\Gamma ; S [ fN1 \Gamma  N2 `d ffig (5.77)
\Gamma ; S [ fffi `d ff(hl : EliA ffi E?) [ N g j\Gamma ; S [ fffi `d ff(hl : EliA\Gamma N ffi 0); ffi `d ff(E?) [ N g

(5.78)

\Gamma ; S [ fN `d ff(hl : EliA ffi 0)g j\Gamma ; S [ fEl `s 0 j l 2 A " N g (5.79)
\Gamma ; S [ fffi `d ff(hl : EliA ffi E) [ N g j\Gamma ; S [ fffi `d :A [ N; ffi `d ff(E) [ N g E 6= E?

(5.80)

\Gamma ; S [ fffi `d ff(E? .A 1) [ N g j\Gamma ; S [ fffi `d ff(E?) [ (N [ A)g (5.81)

\Gamma ; S [ fffi `d ff(E? .A 1)g j\Gamma ; S [ fffi `d :A [ N; ffi `d ff(E?) [ N g (5.82)
\Gamma ; S [ fffi `d ff(E? ^A 0)g j\Gamma ; S [ fffi `d :A [ N; ffi `d ff(E?) [ N g (5.83)
\Gamma ; S [ fffi `d ff(E? ^A 0) [ N g j\Gamma ; S [ fffi `d ff(E?) [ (N [ A)g (5.84)

Figure 5.7: Simplification of domain constraints (complete)

70
if it is inductive and o(X ) ? o(Y) for any Y 2 TLV(E). Similarly, a constraint X `s E
is R-inductive, if it is inductive and o(X ) ? o(Y) for any Y 2 TLV(E). The following
definitions characterize inductive systems.

Definition 5.23 A system of inductive constraints S is equivalent to a system S [ fE1 `s
E2g, if applying the resolution rules of Figures 5.2-5.7 presented in the previous sections to
the system S [ fE1 `s E2g yields the collection of systems S1; S2; : : : ; Sn with n * 1, and
S = Si for some i = 1::n.

Definition 5.24 (Inductive System) A system S of constraints is inductive, if every
constraint E `s E0 in S is inductive, and furthermore for each pair of L-inductive constraint
E1 `s X and R-inductive constraint X `s E2 in S, S is equivalent to S [ fE1 `s E2g
according to Definition 5.23.

The algorithm below transforms an arbitrary system S of constraints into a collection of inductive systems.

Algorithm 5.25 Let \Gamma  = S. Repeat the following steps until no new constraint is added.

1. If \Gamma  contains any system with a constraint that is not inductive, apply the earliest

resolution rule appearing in one of the Figures 5.2-5.7 and let \Gamma  be the new collection
of constraint systems.

2. If S 2 \Gamma  contains an L-inductive constraint E1 `s X and an R-inductive constraint

X `s E2, add the transitive constraint E1 `s E2 to S, unless it has been added
before.

The algorithm terminates, since the number of distinct inductive constraints is bounded by
the size of the original constraints S. Inductive systems have a pleasing property that each
constraint forms either a lower or an upper-bound on a variable. Note that a constraint
X `s Y between two variables can either act as a lower-bound on Y, or as an upperbound on X . We disambiguate these cases using the ordering o(\Delta ) on variables by stating
that all L-inductive constraints are lower-bounds, and all R-inductive constraints are upper
bounds. Thus a constraint X `s Y is a lower-bound on Y if o(X ) ! o(Y), otherwise it is
an upper-bound on X . An inductive system S then has the form

L1;1 t L1;2 t \Delta  \Delta  \Delta  t L1;l1 `sX1 `s U1;1 u U1;2 t \Delta  \Delta  \Delta  u U1;u1
L2;1 t L2;2 t \Delta  \Delta  \Delta  t L2;l2 `sX2 `s U2;1 u U2;2 t \Delta  \Delta  \Delta  u U2;u2

...

Ln;1 t Ln;2 t \Delta  \Delta  \Delta  t Ln;ln `sXn `s Un;1 u Un;2 t \Delta  \Delta  \Delta  u Un;un

(5.85)

where each constraint Li;j `s Xi is L-inductive (j = 1::li) and each constraint Xi `s Ui;j is
R-inductive (j = 1::ui). Note the use of joins and meets to combine the lower and upperbounds on each variable. Even though not all sorts have syntax for these operations, they
are semantically well defined for all sorts, and two lower-bounds L1 `s X , L2 `s X have
the effect of constraining X to be larger than L1 t L2 in all solutions, and similarly for

71
meets of upper-bounds. We thus use u and t as syntax for meets and joins of all sorts in
this section. The meaning function _ is extended naturally by adding the cases

_[[E1 us E2]]oe = _[[E1]]oe us _[[E2]]oe
_[[E1 ts E2]]oe = _[[E1]]oe ts _[[E2]]oe

Let Li be the set of lower-bounds of variable Xi and Ui be the set of upper-bounds
of Xi. Without loss of generality, we can assume that for all Set-variables Xi, no L 2 Li
contains any top-level unions and no U 2 Ui contains any top-level intersections (a top-level
union L1 [ L2 in Li can be split into separate expressions L1 and L2 of Li, and similarly
for intersections in Ui).

We briefly review the technique of Aiken-Wimmers used to show the existence of
solutions for Set-constraints alone (Section 2.2.2). Consider the family of equations obtained
from (5.85) by equating each variable Xi with the join of its lower-bounds, joined with the
meets of its upper-bounds and a fresh auxiliary variable Yi (meets have precedence over
joins)

X1 =L1;1 t L1;2 t \Delta  \Delta  \Delta  t L1;l1 t Y1 u U1;1 u U1;2 u \Delta  \Delta  \Delta  u U1;u1
X2 =L2;1 t L2;2 t \Delta  \Delta  \Delta  t L2;l2 t Y2 u U2;1 u U2;2 u \Delta  \Delta  \Delta  u U2;u2

...

Xn =Ln;1 t Ln;2 t \Delta  \Delta  \Delta  t Ln;ln t Yn u Un;1 u Un;2 u \Delta  \Delta  \Delta  u Un;un

(5.86)

Each assignment for the auxiliary variables Y1::Yn selects an assignment for the variables
X1::Xn in between their lower and upper-bounds. To show that inductive systems have
solutions, it is sufficient to show that there exist assignments for the auxiliary variables
Y1::Yn such that the equations (5.86) have solutions for X1::Xn, and that these solutions
satisfy the constraints of the inductive system S from which the equations were constructed.

We first show that for every choice of Y1::Yn, the induced assignments for Xi satisfy the constraints up to any finite level j. Section 5.5.2 then discusses contractive systems
of equations and shows that--unlike for pure Set-constraints (Section 2.2.2)--not all assignments of Y1::Yn induce contractive equations for inductive systems of mixed constraints.
We form a conjecture that there always exist assignments for the auxiliary variables Y1::Yn
such that the equations induced by (5.86) have solutions. We provide support for this conjecture in Section 5.5.3 and Section 5.5.4. Finally, Section 5.5.5 briefly sketches how the
results of Section 5.5.1 can be extended to Row-sorts.

5.5.1 Level Semantics
The following definitions formalize what we mean by satisfying the constraints "up to level
j".

Definition 5.26 (Assignment Sequence) A variable assignment sequence is a sequence
of variable assignments hoeji, such that the domains of the oej agree, and for each variable
Xi of sort s in that domain, oej maps Xi to an element of Ssj, i.e., oej(Xi) 2 Ssj.

72
Definition 5.27 For every level j, assignment sequence hoeii defined up to level j, and
mixed expression E of sort s, the meaning function _j of the family of meaning functions
defined below, assigns to E an element of Ssj, i.e., _j[[E]]hoeii 2 Ssj.

_j^0[[Es]]hoeii = ?s

_j[[1s]]hoeii = ?s
_j[[0s]]hoeii = ?s
_j[[X s]]hoeii = oej(X )
_j[[c(E1; : : : ; En)]]hoeii = OEc(_j\Gamma 1[[E1]]hoeii; : : : ; _j\Gamma 1[[En]]hoeii)

_j[[E1 u E2]]hoeii = _j[[E1]]hoeii us _j[[E2]]hoeii
_j[[E1 t E2]]hoeii = _j[[E1]]hoeii ts _j[[E2]]hoeii

_j[[:fc1; : : : ; cng]]hoeii = ?s \Gamma  [

c2fc1;::: ;cng

OEc(?'1 ; : : : ; ?'

a(c) ) c : '1 \Delta  \Delta  \Delta  'a(c) ! s

_j[[his]]hoeii = absr(s)
_j[[hl : Eliflg ffi E]]hoeii = aeflg(*l:_j\Gamma 1[[El]]hoeii) ffi _j[[E]]hoeii

Note that our definition of _j[[E]] is careful in assigning meaning to each subexpression E0 of E according to the nesting depth of E0 within E.

Definition 5.28 (Inductive Assignment Sequence) Let S be an inductive system over
a set of variables X1::Xn. Consider the system of equations (5.86) obtained from S by
introducing auxiliary variables Y1::Yn and let hoeii be an assignment sequence for Y1::Yn.

The inductive assignment sequence IAS(S; hoeii) of S with respect to hoeii is the
sequence hoe0ii defined inductively by

oe00(Xj) = ?s Xj of sort s

oe00(Yj) = oe0(Yj) (5.87)

and

oe0i(Yj) = oei(Yj)
oe0i(Xj) = _i[[G sLi ts Yi us l sUi]]hoe0ii Xj has sort s (5.88)

Note that equation (5.88) in the definition of IAS(S; hoeii) is non-recursive since the top-level
variables appearing in Lj and Uj have indices smaller than j and appear already in oei.

Definition 5.29 An assignment sequence hoeii satisfies a system of inductive constraints S
up to level j, if for each k ^ j, and each constraint E1 `s E2 in S

_k[[E1]]hoeii `s _k[[E2]]hoeii
Theorem 5.30 If S is inductive, then the inductive variable assignment IAS(S; hoeii) satisfies the constraints of S up to any finite level j for any auxiliary assignment sequence
hoeii.

73
Proof: By induction on the level j and the variable order o(\Delta ). Let hoe0ii = IAS(S; hoeii).
For j = 0, _j[[E]]hoe0i = ?s for any expression E of sort s, thus the constraints are satisfied
at level 0. Suppose the constraints are satisfied up to level j \Gamma  1. To make the non-recursive
nature of hoe0ii in Definition 5.28 explicit, consider the assignment oe0j;i defined as follows.

oe0j;i(Yk) = oej(Yk)
oe0j;i(Xk) = ae _j[[F Lk t Yk u d Uk]]hoe0; : : : ; oej\Gamma 1; oej;i\Gamma 1i if k ^ iundefined otherwise

At the boundary case, let oej;0 be undefined for all Xk. To simplify the notation, let hoej;ii
denote the sequence hoe0; : : : ; oej\Gamma 1; oej;ii. Consider variable X1 and oe0j;1(X1) defined by

oe0j;1(X1) = _j[[G L1 t Y1 u l U1]]hoej;0i
The definition is well-formed since no expression in L1 or U1 contains any top-level variables
and thus no lookup on oej;0 is performed. We need to show that

1. the lower-bound constraints are satisfied at level j, i.e.,

_j[[G L1]]hoe0j;1i `s oe0j;1(X1)
and that
2. the upper-bound constraints are satisfied at level j, i.e.,

oe0j;1(X1) `s _j[[l U1]]hoe0j;1i

The lower-bound constraints are satisfied, since

_j[[G L1]]hoe0j;1i = _j[[G L1]]hoe0j;0i

`s _j[[G L1]]hoej;0i t _j[[Y1 u l U1]]hoej;0i
= _j[[G L1 t Y1 u l U1]]hoej;0i
= oe0j;1(X1)

For the upper-bounds, we have that

oe0j;1(X1) = _j[[G L1 t Y1 u l U1]]hoej;0i

= _j[[G L1]]hoej;0i t _j[[Y1 u l U1]]hoej;0i

Since I t J `s K () I `s K ^ K `s K, we can break up the relation into two relations

_j[[G L1]]hoej;0i `s _j[[l U1]]hoe0j;1i
_j[[Y1 u l U1]]hoej;0i `s _j[[l U1]]hoej;1i

74
Since _j[[d U1]]hoe0j;1i = _j[[d U1]]hoe0j;0i, the second relation is trivially satisfied. For the first
relation, we need to show that for all E 2 L1 and E0 2 U1,

_j[[E]]hoe0j;0i `s _j[[E0]]hoe0j;0i
Since S is inductive, S is equivalent to S [ fE `s E0g according to Definition 5.23. Thus
if E = 1, then either U1 is empty, or E0 = 1. Similarly if E0 = 0. If E = 0 or E0 = 1, the
constraint is trivially satisfied. We split up the remaining cases according to the sort of X1.

ffl Suppose X1 is of Term-sort. Then E = c(E1; : : : ; En) and E0 = c0(E01; : : : ; E0m).

Since S is inductive, S is equivalent to S [ fc(E1; : : : ; En) `t c0(E01; : : : ; E0m)g. Thus
c = c0 and S contains constraints equivalent to Ek `'k E0k and E0k `'k Ek for all
k = 1::n (by Rule 5.27). Since the constraints are satisfied at level j \Gamma  1, we know
that _j\Gamma 1[[Ek]]hoe0j;0i = _j\Gamma 1[[E0k]]hoe0j;0i for k = 1::n, and thus _j[[c(E1; : : : ; En)]]hoe0j;0i =
_j[[c(E01; : : : ; E0n)]]hoe0j;0i, which is equivalent to _j[[E]]hoe0j;0i `t _j[[E0]]hoe0j;0i (Definition
of `t in Section 4.4).

ffl If X1 is of FlowTerm-sort, then E = c(E1; : : : ; En) and E0 = c0(E01; : : : ; E0m). Since S is

inductive, S is equivalent to S[fc(E1; : : : ; En) `t c0(E01; : : : ; E0m)g. Thus c = c0 and S
contains constraints equivalent to Ek `'k E0k for all k = 1::n (by Rule 5.21). Since the
constraints are satisfied at level j \Gamma  1, we know that _j\Gamma 1[[Ek]]hoe0j;0i `'k _j\Gamma 1[[E0k]]hoe0j;0i
for k = 1::n, and thus _j[[c(E1; : : : ; En)]]hoe0j;0i `ft _j[[c(E01; : : : ; E0n)]]hoe0j;0i (Definition
of `ft in Section 4.4).

ffl If X1 is of Set-sort, then there are a number of cases for the forms of E and E0.

We can focus on the cases where E or E0 contains variables, since S is equivalent
to S [ fE `s E0g and constraints between ground expressions are either satisfied in
all solutions or in none. We focus on the most interesting case E = c(E1; : : : ; En)
and E0 = Pat[c0(E01; : : : ; E0m); M;]. This constraint is equivalent to E " M `s E0 by
Rule 5.4. There are two cases. If the intersection c(?'1 ; : : : ; ?'n)"M is empty, then the
constraint is trivially satisfied. Otherwise the intersection is equal to c(M1; : : : ; Mn),
and by Axiom 5.8 the constraint is equivalent to

c(E1 " M1; : : : ; En " Mn) `s c0(E01; : : : ; E0m)
If c 6= c0, then by Rule 5.8, c must be strict and S contains constraints equivalent
to Ek " Mk `'k 0 for some k. Since the constraints are satisfied at level j \Gamma  1, we
have _j\Gamma 1[[Ek " Mk]]hoe0j;0i = ?'k and _j[[c(E1 " M1; : : : ; En " Mn)]]hoe0j;0i = ?s. Thus
the constraint is satisfied at level j. Otherwise, we have c = c0 and by Rule 5.8, we
are either in the above case, or S contains constraints equivalent to Ek " Mk `'k E0k
for k = 1::n. Since the constraints are satisfied at level j \Gamma  1, we have _j\Gamma 1[[Ek "
Mk]]hoe0j;0i `'k _j\Gamma 1[[E0k]]hoe0j;0i and thus by Axiom 4.1 (c's interpretation observes the
declared variance) _j[[c(E1 " M1; : : : ; En " Mn)]]hoe0j;0i `Vs _j[[c(E01; : : : ; E0n)]]hoe0j;0i.
If c is non-strict, then for any interface path p = (c; k)qt, if q = ffl, then the interface constraint (c; k)\Gamma 1(_j\Gamma 1[[E]]hoe0j;0i) `'k (c; k)\Gamma 1(_j\Gamma 1[[E0]]hoe0j;0i) is equivalent to
_j\Gamma 1[[Ek " Mk]]hoe0j;0i `'k _j\Gamma 1[[E0k]]hoe0j;0i and is satisfied. Otherwise, we know that the
interface constraint q is satisfied by _j\Gamma 1[[Ek " Mk]]hoe0j;0i `'k _j\Gamma 1[[E0k]]hoe0j;0i and thus
the interface constraint on path p is satisfied.

75
We proceed by induction on the variables X2::Xn. Assume that oe0j;i\Gamma 1 satisfies the constraints
up to level j \Gamma  1 for all variables, and up to level j for variables X1::Xi\Gamma 1. Consider the case
for variable Xi. The only additional case not covered above for variable X1 is when E or E0
contain a top-level variable. Suppose that E = Xk. Then since S is inductive, a constraint
equivalent to Xk `s E0 is in S. There are two cases, if the index of Xk is larger than the
index of any top-level variables of E0, then S contains the inductive constraint Xk `s E0.
Since k ! i and hoe0j;i\Gamma 1i satisfies the constraints up to level j for all variables Xk with index
less than i, the constraint is satisfied at level j. Otherwise, E0 contains a top-level variable
Xk0 with index higher than Xk. Then the constraint is equivalent to an inductive constraint
E00 `s Xk0 and since k0 ! i, the constraint is also satisfied at level j. For Set-sorts, E can
be of the form Xk " M and E0 can be of the form Pat[Xk0; M ], but the basic argument does
not change. 2

Definition 5.31 A series of assignments hoeii is Cauchy, if for every variable X in the
domain of the series, the sequence oe0(X ); oe1(X ); oe2(X ); : : : is Cauchy with respect to Definition 2.1 and the metric on ideals given in Section 2.1.

Theorem 5.32 (Solutions) If IAS(S; hoeii) is Cauchy, then the limit of IAS(S; hoeii) is a
solution of S.

Proof: Note that the expressions in S have a maximal height, say k. By definition of
_j, _j[[E]]hoeii only requires assignments hoej\Gamma k; : : : ; oeji (assuming j ? k). If IAS(S; hoeii)
converges to oe, then _k+1[[E]]h oe; : : : ; oe-- -z ""

k + 1 times

i is equal to _[[E]]oe (Section 4.5), and thus oe is a

solution of the constraints S. 2

5.5.2 Contractive Systems of Equations
Aiken-Wimmers prove the existence of solutions for pure Set-constraints by showing that
the solutions of an inductive system of Set-constraints are equivalent to the solutions of a
family of contractive equations. We describe why this proof cannot be adapted to mixed
constraints and then provide support for a conjecture that inductive systems of mixed
constraints nevertheless have solutions.

Consider again the family of equations (5.86) obtained from an inductive system
S. Each assignment of the auxiliary variables Y1::Yn selects a system of cascading equations

fX1 = RHS1; : : : ; Xn = RHSng
from (5.86). An equation Xi = RHSi is cascading, if every top-level variable Xk in RHSi has
lower index than Xi (o(Xk) ! o(Xi)). The equations are cascading since they were obtained
from the inductive constraints of S and thus the top-level variables in Li;k and Ui;k have
indices strictly lower than o(Xi). Due to this property, we can transform cascading equations
into equations without top-level variables. To see this, order the equations according to the
variable order o(\Delta ), i.e., let the indices of the variables match the order o(\Delta ). Then, clearly

76
RHS1 has no top-level variables. Assume that we have transformed all RHSk into RHS0k
where RHS0k has no top-level variables for k = 1::i \Gamma  1. Then RHSi can be transformed into
RHS0i without top-level variables by replacing each top-level variable Xk in RHSi by RHS0k.
Since the equation Xi = RHSi is cascading, each top-level variable Xk in RHSi has index
lower than Xi and thus RHS0k has already been computed.

The only operations in RHS0i are joins, meets, and constructor applications. If all
expressions denote pure Set-ideals, the joins and meets are set-theoretic union and intersection. These operations are non-expansive (Section 2.1). Since all variables appear inside
constructors in RHS0i and constructor interpretations are contractive (Axiom 4.1), the system of equations Xi = RHS0i is contractive and thus has a unique solution (Section 2.1).

Unfortunately, in the case of mixed constraints, the meet and join operations are
not always non-expansive, i.e., for some arguments, the meet and join operations can be
expansive. The resulting equations are thus not contractive. The culprits are the meet and
join operations for Term-ideals I ut J and I tt J which are expansive if the two operands
are of the form I = OEc(I1; : : : ; In), and J = OEc(J1; : : : ; Jn), and Ik differs from Jk for some
k. In this case, I tt J = ?t and I ut J = ?t. These operations are expansive in I and
J , meaning that if I and I0 have distance d, then the distance between between the join
I tt J and the join I0 tt J may be larger than d, and similarly for the meet and the second
argument (Section 2.1). The expansiveness of the meet and join for Term-ideals then induces
expansiveness for the meet and join of other sorts. As an example, consider the inductive
system S

c(X1) tt c(X2) `t X1

?t `t X2 (5.89)

where c : t ! t is a unary Term-constructor. The family of equations associated with S are

X1 = c(X1) tt c(X2) tt Y1
X2 = ?t tt Y2 (5.90)

The cascading equations obtained from the above family through the auxiliary variable
assignment Y1 = ?t and Y2 = ?t is

X1 = c(X1) tt c(X2)
X2 = ?t

This system is not contractive, since the join c(X1) tt c(X2) is expansive for some assignments of X1 and X2. To illustrate the problem with non-contractive equations, consider the
function

F (oe)(Xi) = _[[RHS0i]]oe (5.91)
transforming any variable assignment oe into a new variable assignment oe0 through the
equations. A fix-point of F is a solution of the equations. Contractiveness guarantees that
F has a fix-point, and that the fix-point is reached by the sequence of variable assignments
oe0; oe1 = F (oe0); oe2 = F (F (oe0)); : : : ; oei = F i(oe0); : : : starting from an arbitrary assignment
oe0. The sequence of variable assignments obtained in our example starting with [X1 7!
?; X2 7! ?] is

77
Assignment oe0 oe1 oe2 oe3 oe4 oe5
X1 ? c(?) ? c(?) ? c(?) \Delta  \Delta  \Delta 
X2 ? ? ? ? ? ? \Delta  \Delta  \Delta 

which doesn't converge. However, there are different choices for the auxiliary variable
assignment of Y1 so that the equations obtained from (5.90) are contractive and thus have
solutions. For example, choosing Y1 = ?, results in the equations

X1 = ?t
X2 = ?t

which clearly define a solution of the inductive system (5.89). Thus, in the case of mixed
constraints, not all auxiliary variable assignments Yi induce contractive equations and solutions. However, in order for the series hF i(oe0)i to converge, contractiveness is not necessary.
It is sufficient if the series hF i(oe0)i is Cauchy according to Definition 5.31.

We conjecture that there always exist choices for Yi such that the equations induced
by the family (5.86), generate a series of assignments that is Cauchy.

Conjecture 5.33 If S is an inductive system of constraints, then there exists an auxiliary
variable assignment for Yi, such that the equations induced by the family (5.86) generate a
series of assignments that is Cauchy.

To support this conjecture, we proceed as follows. Section 5.5.3 shows how to
eliminate all joins and meets appearing directly in bounds of Term-variables in an inductive system S by generating contractive equations for Term-variables. We show that these
equations together with the equations (5.86) for the FlowTerm and Set variables satisfy
the constraints up to any finite level for any auxiliary variable assignment. Section 5.5.4
then proposes a construction of auxiliary variable assignments Y1::Yn for each FlowTerm
and Set-variable Xi such that the induced equations (5.86) combined with the equations on
Term-variables should generate a series of variable assignments for Xi that is Cauchy and
thus converges towards a solution of the constraints.

5.5.3 Contractive Term-Equations
This subsection shows how to eliminate meets and joins appearing directly in bound of
Term-variables. We proceed by induction on the order of the Term-variables, creating a
contractive equation Xi = RHS0i from the upper and lower bounds Ui and Li, such thatF

Li `t Ei `t d Ui. Consider the Term-variable X1 of minimum index with lower bounds
L1 and upper bounds U1 (Without loss of generality, we can assume that Term-variables
have indices 1::m \Gamma  1). Recall that no expression L in L1 or U in U1 contains any top-level
variables. Thus each L is of the form 0, 1, or c(: : : ) and similarly for each U in U1. Let U 01 be
U1\Gamma f1g, and let L01 = L1\Gamma f0g. The removed expressions 1 from U1 and removed expressions
0 from L1 do not constrain the solutions for X1. Now, if L01 = ;, we set RHS01 = ?. Clearly,
? `t d U1, and also F L1 `t ?. Analogously, if U 01 = ;, we set RHS01 = ?. Otherwise,
L01 and U 01 contain only constructed expressions (if 1 2 L1, then by transitivity, U 01 = ;,
and similarly, if 0 2 U1). Thus, consider the expressions ci(Ei;1; : : : ; Ei;a(ci)) in L01 for some

78
set of indices i 2 I, and the expressions c0j(E0j;1; : : : ; E0j;

a(c0j)) in U 01 for some indices j 2 J .

Since our system S is inductive, it is closed under transitivity, and we can conclude that
ci = c0j for all i and j. Then by Rule 5.27 for Term-constructors, S contains constraints
equivalent to Ei;k `sk E0j;k and E0j;k `sk Ei;k for all k = 1::a(ci), i 2 I, and j 2 J. Suppose
we have an assignment sequence hoe0j\Gamma 1i that satisfies the constraints up to level j \Gamma  1. Then
we have _j\Gamma 1[[Ei;k]]hoe0j\Gamma 1i = _j\Gamma 1[[E0j;k]]hoe0j\Gamma 1i for all k = 1::a(ci), i 2 I, and j 2 J. Thus,
expressions ci(Yi;1; : : : ; Yi;a(ci)) and c0j(Y0j;1; : : : ; Y0j;

a(c0j)) are equal at level j. We can thus

set RHS01 = ci(Ei;1; : : : ; Ei;a(c0)) for an arbitrary i 2 I, and X1 will satisfy the constraints
at any level j, provided all other variables satisfy the constraints up to level j \Gamma  1. Now
the same construction can be performed for the next Term-variable X2 in the order, using
RHS01 to expand any appearance of X1 at top-level in L2 or U2. Proceeding in order on all
Term-variables, we obtain a system of Term-equations

X1 =t RHS01
X2 =t RHS02

...

Xm\Gamma 1 =t RHS0m\Gamma 1

(5.92)

where all RHS0i are of the form 0, 1, or ci(: : : ) for some constructor ci. Since these equations
contain no meets or joins and all variables appear inside constructors, the equations are
contractive. Similarly to X1, the assignment of Xi for i = 2::m \Gamma  1 satisfy the constraints
at any level j, provided that all variables satisfy the constraints up to level j \Gamma  1.

5.5.4 Generating Set and FlowTerm-Equations
Unfortunately, even if we eliminate all meets and joins in bounds of Term-variables as
outlined in the previous subsection, we still don't have contractive equations for all variables.
Joins and meets on Term-ideals may appear in joins and meets of FlowTerm and Set ideals.
As an example, consider the inductive system

c(X3) `t X1 `t c(X3)
c(X4) `t X2 `t c(X4)
d(X1) [ d(X2) `s X3

d(1) `s X4

where X1 and X2 are Term-variables, X3 and X4 are Set-variables, c is a Term-constructor,
and d a Set-constructor with signatures

c : s ! t
d : t ! s

Using the construction above, we transform the constraints on the Term-variables X1 and
X2 into the contractive equations

X1 = c(X3)
X2 = c(X4)

79
Choosing the auxiliary variable assignment [Y3 7! ?; Y4 7! ?] we obtain the system of
equations

X1 = c(X3)
X2 = c(X4)
X3 = d(X1) ts d(X2)
X4 = d(1)

(5.93)

This system is not contractive, since the join d(X1) ts d(X2) involves the join X1 tt X2 which
is expansive for some assignments of X1 and X2. The sequence of variable assignments
obtained starting with [Xi 7! ?] for i = 1::4 is

oe0 oe1 oe2 oe3 oe4 oe5 oe6 oe7 oe8
X1 ? c(?) c(d(?)) c(d(c(?))) c(d(?)) c(d(?)) c(d(c(d(?)))) c(d(c(d(?)))) c(d(?)) \Delta  \Delta  \Delta 
X2 ? c(?) c(d(?)) c(d(?)) c(d(?)) c(d(?)) c(d(?)) c(d(?)) c(d(?)) \Delta  \Delta  \Delta 
X3 ? d(?) d(c(?)) d(?) d(?) d(c(d(?))) d(c(d(?))) d(?) d(?) \Delta  \Delta  \Delta 
X4 ? d(?) d(?) d(?) d(?) d(?) d(?) d(?) d(?) \Delta  \Delta  \Delta 

which clearly doesn't converge, since assignments 4-7 are repeated ad infinitum.

Observe that we can obtain a system of equations generating a converging series
of assignments in the example of the previous subsection, if we choose a different auxiliary
variable assignment where Y3 = d(?). In that case, the induced equations are

X1 = c(Y1)
X2 = c(Y2)
X3 = d(X1) ts d(X2) ts d(?t)
X4 = d(1)

The right-side of equation X3 is now contractive, since it results in d(?t) for any assignment
of X1 and X2.

In this section we construct auxiliary variable assignments for each Yi where Xi is
a FlowTerm or Set-variable. The assignments for Yi are defined through their own system
of equations. The basic idea is that if we choose Y1::Yn to contain only ?t and ?t for all
projections of Term-interface paths p, then all problematic meets and joins of Term-ideals
arise from Term-variables syntactically present in the upper and lower-bounds of FlowTerm
and Set-variables. This follows from Definition 3.1 which restricts the Term-arguments to
FlowTerm and Set-constructors to be either 1t, 0t, or a Term-variable X . Since Term-meets
and Term-joins involving 1 and 0 are non-expansive, the potentially expansive meets and
joins involve the denotation of a Term-variable.

Our construction is based on the following idea. For every FlowTerm or Set-variable
Xi, we choose Yi such that the projection p\Gamma 1(Yi) is ?t for every even Term-path p where
p\Gamma 1(FL2Li L) is determined in part by a Term-variable X , and X is defined by an equation
X = c(E1; : : : ; Ea(c)), i.e., X is non-empty in all solutions (recall from Axiom 4.1 that
Term-constructors are non-strict). If Xi is defined by

Xi = G

L2Li

L ts Yi us l

U2Ui

U

80
then for any even path p with interface Term, the projection p\Gamma 1(Xi) depends on p\Gamma 1(dU2Ui U ).
If p\Gamma 1(dU2Ui U ) is ?t in all solutions, i.e., the upper-bounds of Xi do not constrain the solution of Xi at interface p, then

p\Gamma 1(Xi) = p\Gamma 1( G

L2Li

L) tt p\Gamma 1(Yi) ut p\Gamma 1( l

U2Ui

U )

= p\Gamma 1( G

L2Li

L) tt ?t ut ?t

= ?t
Otherwise, the upper-bounds Ui constrain the solution of Xi at interface p to be less than
some Term-variable X 0, and since p\Gamma 1(FL2Li L) `t p\Gamma 1(dU2Ui U ) (transitivity of inductive
systems), X 0 is defined by an equation X 0 = c(E01; : : : ; E0a(c)) involving the same constructor
as the equation of X which constrains Xi at interface p from below. Furthermore, the
inductive system S constrains the arguments of the constructed expressions c(E1; : : : ; Ea(c))
and c(E01; : : : ; E0

a(c)) to be equal (Ek = E0k for k = 1::a(c)).4 Since this argument holds for

all pairs of such variables X and X 0, we know that these variables denote equal Term-ideals

I in all solutions and their meet or join is thus non-expansive. The projection of Xi at
interface p is then

p\Gamma 1(Xi) = p\Gamma 1( G

L2Li

L) tt p\Gamma 1(Yi) ut p\Gamma 1( l

U2Ui

U )

= I tt ?t ut I
= I

The idea for odd paths is similar. To simplify the presentation, we assume that the original
constraints are in normal form, where the arguments Ei of each constructed expression
c(E1; : : : ; En) are variables, unless the entire expression is an M-expression M . Since the
only new constructor expressions introduced by the resolution rules of the previous sections
arise in simplifications of L-intersections c(E1; : : : ; En) " M to c(E1 " M1; : : : ; En " Mn),
we can assume that each argument Ei of a constructed expression c(E1; : : : ; En) is either
a variables X , or a variable intersected with an M-expression X " M . The latter case only
arises for Set-variables.

Definition 5.34 A non-strict Term-path p is a path with interface t (Section 4.2). We
say that p is syntactically present in variable Xi of an inductive system S and leads to
Term-variable Xj (written present S(Xj; p; Xi) when S is understood), if one of the following
conditions hold.

1. p = ffl and Xi is a Term-variable, thus present S(Xi; ffl; Xi).
2. p = (c; k)q and Xi is a FlowTerm or Set variable, and Li contains an expression E

such that one of the following cases applies:

4Note that X 0 cannot be 0, for otherwise the transitive constraints through Xi lead to the inconsistent
constraint c(E1; : : : ; Ea(c)) `t 0 and inductive systems do not contain any inconsistent constraints.

81
(a) E = c(E1; : : : ; Ea(c)), where c is non-strict, Ek = Xi0, and present S(Xj; q; Xi0).

(b) E = c(E1; : : : ; Ea(c)), where c is non-strict, Ek = Xi0 " M , q\Gamma 1(M ) = ?t, and

present S(Xj; q; Xi0 ).

(c) E = Xi0 and present S(Xj; p; Xi0 ).
(d) E = Xi0 " M , p\Gamma 1(M ) = ?t, and present S(Xj; p; Xi0).

If a path p is syntactically present in Xi of inductive system S and leads to Termvariable Xj, then the projection p\Gamma 1(oe(Xi)) is determined in part by oe(Xj) for any solution
oe

Definition 5.35 The expansive interface variables of Xi w.r.t. non-strict Term-path p is
the set

EIF(Xi; p) = fXj j present S(Xj; p; Xi) ^ RHS0j = c(: : : )g

i.e., the set of Term-variables Xj syntactically present in Xi w.r.t. path p, and for which the
equation Xj = RHS0j derived in Section 5.5.3 is of the form Xj = c(: : : ) for some constructor
c.

The expansive interface variables EIF(Xi; p) are the Term-variables whose join (if p is even)
or meet (if p is odd) may be expansive. In our example equations (5.93) at the beginning
of this subsection, the syntactic interface variables for X3 and path (d; 1) are

EIF(X ; (d; 1)) = fX1; X2g
which are defined by the equations

X1 = c(X3)
X2 = c(X4)

and it is the join of c(X3) tt c(X4) which is expansive for some assignments of X3 and X4.
Note that Term-variables Xj such that RHS0j = 1 or RHS0j = 0 are not considered expansive,
since meets and joins involving 0 and 1 are never expansive.

Our goal is now to construct the auxiliary assignments Yi, such that for every
path p for which EIF(Xi; p) is non-empty, p\Gamma 1(Yi) = ?t if p is even, and p\Gamma 1(Yi) = ?t if p is
odd. In essence, the projection of every even Term-path p is raised to ?t and the projection
of every odd Term-path p is lowered to ?t through the assignment Yi. As a result, the
combined join F EIF(Xi; p) t p\Gamma 1(Yi) = F EIF(Xi; p) t ?t = ?t is non-expansive (for odd
paths p the meet is ?t). The complete argument needs to take the upper-bounds Ui of Xi
into account, since Xi is defined by

Xi = G Li t Yi u l Ui
and the meet Yi u d Ui may eliminate some raised or lowered projections p\Gamma 1(Yi). However,
if p\Gamma 1(d Ui) 6= ?t for some even path p, then either p\Gamma 1(Yi) = ?t, or the denotation of
variables EIF(Xi; p) are all equal. In either case, the meet or join of these variables is
non-expansive, and similarly for odd paths p.

82

Before we construct the auxiliary variable assignments Yi, we need to know which
FlowTerm-variables Xk are ?ft in all solutions. This information is easily extracted from
the inductive system S by creating a map Minft, associating with each FlowTerm-variable a
unique constructor c, 0, or 1. Start with the FlowTerm-variable Xf1 of lowest index. Lf1 has
no top-level variables, so Minft(Xf1 ) = 0 if Lf1 contains only 0. If Lf1 contains only 0 and
expressions c(: : : ), then Minft(Xf1) = c. Otherwise, Lf1 contains 1 or two expressions c(: : : )
and c0(: : : ) where c 6= c0. In that case Minft(Xf1 ) = 1. Proceed according to the ordering
o(\Delta ) for all FlowTerm-variables. Assume we have constructed Minft(Xk) for all FlowTermvariables Xk, such that o(Xk) ! o(Xi). Construct Minft(Xi) as for Xf1 , using the mapping
Minft to replace all top-level variables Xk appearing in Li with 1 if Minft(Xk) = 1, with 0 if
Minft(Xk) = 0, and with c(1; : : : ; 1) if Minft(Xk) = c.

We now construct equations for Yi. We require an extra set of auxiliary variables
Y\Gamma i to provide distinct projections for odd paths.

Definition 5.36 Each auxiliary variable Yi associated with a FlowTerm or Set-variable Xi
is defined by the equation

Yi = G

E2Li

TI+(E) (5.94)

where TI+(E) is defined on expressions of all sorts s as follows:

TI+(Xj) = 8!:

1t Xj is a Term-variable with equation Xj =t d(: : : )
0t Xj is a Term-variable with equation Xj =t 0 or Xj =t 1
Yj Xj is a FlowTerm or Set-variable

TI+(Xj " M ) = Yj " M

TI+(c(E1; : : : ; Ea(c))) = ae 0

s if c strict or a(c) = 0

c(TIz1(E1); : : : ; TIz

a(c)(E

a(c)))

TI+(E) = 0s otherwise

where zk = + if c is covariant in k, and zk = \Gamma  if c contravariant in k.

Similarly, we define Y\Gamma i by the equations

Y\Gamma i = ae 1

ft if Xi is of sort FlowTerm and Minft(Xi) = 1d

E2Li TI\Gamma (E) otherwise (5.95)
where TI\Gamma (E) is defined on expressions of all sorts s as follows:

TI\Gamma (Xj) = 8!:

0t Xj is a Term-variable with equation Xj =t d(: : : )
1t Xj is a Term-variable with equation Xj =t 0 or Xj =t 1
Y\Gamma j Xj is a FlowTerm or Set-variable

TI\Gamma (Xj " M ) = Pat[Y\Gamma j ; M ]

TI\Gamma (c(E1; : : : ; Ea(c))) = 8!:

1s if c strict or a(c) = 0
Pat[E0; c(1'1 ; : : : ; 1'

a(c) )] if c : '1 \Delta  \Delta  \Delta  '

a(c) ! s

E0 if c : '1 \Delta  \Delta  \Delta  'a(c) ! ft

TI\Gamma (E) = 1s otherwise

83
where E0 = c(TIz1(E1); : : : ; TIz

a(c)(E

a(c))) and zk = \Gamma  if c is covariant in k, and zk = + if

c contravariant in k.

The equations for Yi and Y\Gamma i of Definition 5.36 define an assignment sequence hoeji
constructed analogously to the inductive assignment sequence built in Definition 5.28.

Now consider the system of equations below where we assume that Term-variables
have indices 1::m\Gamma 1 and FlowTerm and Set-variables have indices m::n, and that the indices
match the variable order o(\Delta ) of X1::Xn. The equations for Term-variables are the equations
described in the previous subsection.

X1 =t RHS01

...

Xm\Gamma 1 =t RHS0m\Gamma 1

Ym = G

E2Lm

TI+(E)

...
Yn = G

E2Ln

TI+(E)

Y\Gamma m = l

E2Lm

TI\Gamma (E)

...
Y\Gamma n = l

E2Ln

TI\Gamma (E)

Xm = G Lm t Ym u l Um

...

Xn = G Ln t Yn u l Un

(5.96)

Let the relative order of variables Yi be given by their indices and similarly for variables
Y\Gamma i . Note that each top-level variable in the right-hand side of an equation Yi FE2Li TI+(E)

contains only top-level variables Yj with j ! i, and similarly for Y\Gamma i . This follows from the
the fact that any top-level variable Xj in Li has index j ! i and that the indices of variables Yj and Y\Gamma j in the construction in Definition 5.36 mirror the indices of Xj. Thus

the right-hand side of equation Ym has no top-level variables. Let EYm = FE2Lm TI+(E).
Now proceed in the order Ym::Yn. Assume we have constructed expressions EYk equivalent to FE2Lm TI+(E) but without top-level variables for k = m::j \Gamma  1. Now let EYj =

84F

E2Lj TI+(RHS(E)), where

RHS(Yk) = EYk
RHS(Yk " M ) = EYk " M
RHS(Pat[Yk; M ]) = Pat[EYk ; M ]

RHS(E) = E otherwise

Since each top-level variable Yk of E 2 Lj has index lower than Yj, we have already
constructed EYk and the expansion RHS(E) is well-defined.

Similarly, the right-hand side of equation Y\Gamma m has no top-level variables and we
can construct EY\Gamma 

k without top-level variables for k = m::n. Finally, the right-hand sidesof X

m::Xn can be transformed into EXk without top-level variables by noting that F Lm t
Ym u d Um contains only Ym as a top-level variable. Since we have computed EYm , we

can replace Ym by EYm in the right-hand side of Xm, thus obtaining EXm without top-level
variables. In a similarly fashion, we can obtain EXm+1 ::EXn.

We have thus transformed the set of equations 5.96 into the equivalent system
below, where no top-level variables appear in the right-hand sides.

X1 =t RHS01

...

Xm\Gamma 1 =t RHS0m\Gamma 1

Ym = EYm

...

Yn = EYn
Y\Gamma m = EY\Gamma m

...

Y\Gamma n = EY\Gamma n
Xm = EXm

...

Xn = EXn

(5.97)

We conjecture that the assignment series generated by these equations is Cauchy and thus
defines a solution of the constraints. Note that by Definition 5.36, the expressions EYm ::EYn
and EY\Gamma m::EY\Gamma n contain no variables besides Ym::Yn and Y\Gamma m::Y\Gamma n . Since the auxiliary variables are of sorts FlowTerm and Set, we know that for any Term interface path p, the
projections p\Gamma 1(Yk) and p\Gamma 1(Y\Gamma k ) result in meets and joins involving only ?t and ?t and
are thus non-expansive. Thus the equations for the auxiliary variables Yi and Y\Gamma i are
contractive.

85
5.5.5 Solutions For Row-Constraints
We briefly sketch how the result of Section 5.5.1 adapts to constraint systems with Rowconstraints.

In light of our addition of domain constraints and the assumption that each Rowvariable has a fixed kind, we refine our definition of a well-sorted variable assignment given
in Section 4.5, to well-kinded assignments.

Definition 5.37 A well-sorted variable assignment oe is well-kinded, iff for every Rowvariable X of kind K, oe(X ) is of kind K, and furthermore,

dom(_[[X ]]oe) = L \Gamma  oe(ffX ) if X is closed
dom?(_[[X ]]oe) ` L \Gamma  oe(ffX )

A well-sorted and well-kinded variable assignment oe is a solution of a system of
constraints S, if for every constraint E1 `s E2 in S, the relation _[[E1]]oe `s _[[E2]]oe is
satisfied (the semantic relation for `d is simply set-theoretic inclusion) The notion of an
inductive system is extended to guarantee that well-kinded assignments for Row-variables
exist.

Definition 5.38 (Inductive System with Row-Constraints) A system S of constraints
is inductive, if every constraint E `s E0 in S is inductive, and furthermore for each pair of
L-inductive constraint E1 `s X and R-inductive constraint X `s E2 in S, S is equivalent
to S [ fE1 `s E2g according to Definition 5.23. Furthermore, S is equivalent to each of the
following systems:

1. S [ fff(X ) `d ff(E1)g if X is a minimal or closed Row-variable.
2. S [ fff(X ) `d ff(E2)g if X is a closed or maximal Row-variable.
3. S [ fff(E1) `d ff(X )g if X is a closed Row-variable and E1 is a closed Row-expression.
4. S [ fff(E2) `d ff(X )g if X is a closed Row-variable and E2 is a closed Row-expression.
For minimal Row-variables, Condition 1 guarantees that the labels ffX are absent from the
minimal domain of all lower-bounds E1 on X . This condition is necessary since _[[E1]]oe `r(s)
oe(X ) in all solutions oe, implying that dom?(_[[E1]]oe) ` dom?(oe(X )). Similarly, for maximal
Row-variables, Condition 2 guarantees that the labels ffX are absent from the domain of all
upper-bounds E2 on X . This condition is necessary since oe(X ) `r(s) _[[E2]]oe in all solutions
oe, implying that dom(_[[E2]]oe) ` dom?(oe(X )). Similarly, Conditions 1-4 guarantee that
the domain constraints for closed Row-variables are satisfiable.

If we choose the auxiliary assignments Yi such that for all Row-variables Xi, the
minimal domain of Yi satisfies the domain constraints on Xi, i.e.,

dom(_[[Yi]]oe) = L \Gamma  oe(ffXi ) if Xi is closed
dom?(_[[Yi]]oe) ` L \Gamma  oe(ffXi )

86
we can show that the equations induced by Yi on the family (5.86) satisfy the constraints
up to any finite level j, analogously to the development in Section 5.5.1. Definition 5.27 is
easily extended over domain-complement expressions as follows.

_i^0[[ffi]]hoej i = L

_i[[ffX ]]hoeji = oei(ffX )
_i[[ff(hl : EliA ffi E?)]]hoeji = (_i[[ff(E?)]]hoeji \Gamma  fl j _i\Gamma 1[[El]]hoeji 6= ?sg)

_i[[ff(hl : EliA ffi E)]]hoeji = (_i[[ff(E)]]hoej i \Gamma  A) E 6= E?

_i[[ff(0)]]hoej i = L
_i[[ff(hi)]]hoej i = L

_i[[ff(1)]]hoej i = L
_i[[ffi [ N ]]hoeji = _i[[ffi]]hoej i [ N
_i[[ffi " N ]]hoeji = _i[[ffi]]hoej i " N

87
Chapter 6
Practical Aspects of Constraint
Resolution

The transformation of constraint systems into a collection of inductive systems
as presented in the previous chapter involves splitting of constraint systems. Such an
approach is completely impractical for an implementation since the split systems share most
constraints and a large fraction of their resolution is duplicated. Furthermore, the rewrite
formulation of constraint resolution serves mainly a didactic purpose but is not practical as
an implementation strategy. This chapter addresses these two issues. Section 6.1 introduces
conditional constraints as a way to delay splitting of constraint systems during resolution.
Section 6.2 presents constraint resolution as the construction of a closed constraint-graph
and characterizes when constraint-graphs are consistent. The chapter concludes with a
discussion of constraint resolution and related work.

6.1 Conditional Constraints
To avoid splitting constraint systems during resolution, we encode splits using conditional
expressions. Conditional set-expressions were originally introduced by Reynolds [74]. Aiken,
Wimmers, and Lakshman [4] use conditional set-expressions to capture certain aspects of

control flow in a soft-typing system. A conditional expression has the form C

l=) E, where

E is an L-compatible conditional expression, and C is a condition. The conditions used
by Aiken et al. [4] are simply L-compatible expressions. Given a solution oe, the meaning
function _ is extended to conditional expressions as follows:

_[[C

l=) E]]oe = ae _[[E]]oe if _[[C]]oe 6= f?g

f?g otherwise

i.e., the meaning of C

l=) E is the meaning of E if C is non-empty, and f?g if C is

empty. The resolution rule proposed by Aiken et al. [4] for constraints involving conditional
expressions is

\Gamma ; S [ fC

l=) E1 `

s E2g j \Gamma ; S [ fC `s 0g; S [ fE1 `s E2g (6.1)

88
The system S [ fC

l=) E1 `

s E2g is split into two systems, one where the condition is

false S [ fC `s 0g, and one where the inclusion holds S [ fE1 `s E2g.

Some of the resolution rules we have presented in the previous sections introduce splitting. We can reformulate these rules without splitting by introducing conditional
expressions. The resolution rule for resolving constraints between strict constructor expressions (Rule 5.7)

\Gamma ; S [ fc(E1::En) `s c(E01::E0n)g j\Gamma ; S [ fEj `'j E0j j c : '1 \Delta  \Delta  \Delta  'n ! sg;

S [ fc(E1::En) `s 0g c strict

is reformulated as

\Gamma ; S [ fc(E1::En) `s c(E01::E0n)g j \Gamma ; S [ fc(E1::En)

l=) c0(E1::E

n) `s c0(E01::E0n)g

where c0 is an auxiliary constructor with the same signature as c, but non-strict in all arguments. This rule in conjunction with the rule for splitting conditional expressions (6.1) and
the resolution for non-strict constructors (Rules 5.6, 5.21, and 5.27) has the same behavior
as the original rule for strict constructors: either the left-side is empty (the condition is
false), or the inclusions between the corresponding constructor arguments hold.

The only other rule that splits constraint systems is Rule 5.8 which has the form

\Gamma ; S [ fc(E1::En) `s Eg j ae \Gamma  c : '1 \Delta  \Delta  \Delta  'n ! s non-strict\Gamma ; S [ fE

1 `'1 0g; : : : ; S [ fEn `'n 0g c strict

where E is 0 or d(::) where c 6= d

If c is strict, then one of E1; : : : ; En must be 0. This choice can be encoded by the following
constraint

(E1 ^ E2 ^ \Delta  \Delta  \Delta  ^ En\Gamma 1)

l=) E

n `s 0

where the conjunction E1 ^ E2 \Delta  \Delta  \Delta  En\Gamma 1 is a condition that is true if all of E1; : : : ; En\Gamma 1
are true. Note that a conjunction of two conditions E1 ^ E2 is equivalent to the condition
p(E1; E2), where p is an arbitrary strict constructor. Conjunctions in this context are thus
merely a syntactic convenience and we make free use of them. We reformulate Rule 5.8 as
follows

\Gamma ; S [ fc(E1::En) `s Eg j ae \Gamma  c : '1 \Delta  \Delta  \Delta  'n ! s non-strict\Gamma ; S [ f(E

1 ^ \Delta  \Delta  \Delta  ^ En\Gamma 1)

l=) E

n `'n 0g c strict

where E is 0 or d(::) where c 6= d

(6.2)

Applying Rule 6.1 n times to the system

S [ f(E1 ^ E2 ^ \Delta  \Delta  \Delta  ^ En\Gamma 1)

l=) E

n `s 0g

produces the same collection of constraint systems as the original rules

S [ fE1 `'1 0g; : : : ; S [ fEn `'n 0g
Now the only rule that splits constraint systems is Rule 6.1 for conditional expressions.

89
Before proceeding, we introduce a normal form for conditions. Conditions are
conjuncts of atomic conditions which take the form X " M , i.e. a variable intersected with
an M-expression. An atomic condition X " M is true for a solution oe, if oe(X ) " _[[M ]]oe
is non-empty. By non-empty, we mean a set distinct from f?g, since every denotation
contains ?. Thus, no atomic conditions involving Row-variables need to be formed, since
Row-variables are always non-empty. Furthermore, if X is a Term or FlowTerm variable,
then M = 1. An empty list of conjuncts represents the trivial condition true. A conjunct
of conditions C1 ^ C2 ^ \Delta  \Delta  \Delta  ^ Cn is true if each individual condition is true. Note that
this form of conditions is more restrictive than allowing any L-compatible expression to
be a condition. In particular, L-compatible expressions as conditions can directly express
disjuncts: for example assuming c is a strict constructor, c(E1 [ E2) is true if E1 or E2 is
non-empty. However, by normalizing c(E1 [ E2) to c(X ), where X is a fresh variable with
the associated constraint E1 [ E2 `s X the same condition can still be expressed. Moreover,
if L

l=) E is a conditional expression and L an arbitrary L-compatible expression, then

we can form an equivalent union Si(Ci

l=) E) of conditional constraints, where the C

i

adhere to our restriction and without the need to introduce fresh variables. The conditions

Ci are simply the disjuncts of the condition L.

Conditions are introduced every time one of our reformulated resolution rules is
applied to a strict constructor. Instead of generating the conditions implicitly in these resolution rules, we can add the normalized conditions directly to the expressions involving strict
constructors in the original constraints. Replace all expressions c(E1; : : : ; En) appearing in
an L-context of the original constraints and where c is strict, by the conditional expression
(E1 ^ \Delta  \Delta  \Delta  ^ En)

l=) c0(E1; : : : ; E

n), where c0 is again a constructor with the same signature

as c, but non-strict in all arguments. Replace all remaining occurrences of c by c0 in the

constraints. If we perform this step for all strict constructors in the original constraints,
and the resolution does not produce new constructor expressions, then we obtain the same
inductive systems. However, L-intersection simplification may introduce new constructor
expressions, i.e., when simplifying c0(E1; : : : ; En) " M to c0(E1 " M1; : : : ; En " Mn) and c
was originally strict, the simplified L-intersection should be transformed into the conditional
expression (C1 ^ \Delta  \Delta  \Delta  ^ Cn)

l=) c0(E1 " M1; : : : ; E

n " Mn), where Ci is the normalized condition for Ei " Mi. Note that this process terminates, since there are finitely many atomic

conditions X " M that can be formed. As a final step, we can remove the cases for strict
constructors from the resolution rules above and modify the rule for splitting constraints
with conditional expressions to deal with conjuncts directly:

\Gamma ; S [ f(C1 ^ \Delta  \Delta  \Delta  ^ Cn)

l=) E1 `

s E2g j\Gamma ; S [ fC1 `s 0g;

S [ fC2 ^ \Delta  \Delta  \Delta  ^ Cn)

l=) E1 `

s E2g

(6.3)

Since C1 is atomic, the constraint C1 `s 0 does not require the use of Rule 6.2. Thus, by
putting conditions into normal form and adding conditions explicitly to the original constraints, we can avoid the introduction of conditional expressions during resolution (except
when intersections of strict constructors are simplified), and the only rule that splits constraint systems is the rule for conditional expressions. Special resolution rules for strict
constructors are no longer needed.

We now show how to avoid splitting constraint systems by delaying the use of

90
Rule 6.3. The basic idea for avoiding the splitting is to transform constraints with conditional expressions into conditional constraints and vice-versa. Before proceeding, we extend
our conditional expressions to R-compatible expressions. If R is an R-compatible expression,
then C

r=) R is a conditional R-compatible expression with the meaning

_[[C

r=) E]]oe = ae _[[E]]oe if _[[C]]oe is true

? otherwise

Conditional constraints now have the form C =) (L `s R). A solution oe satisfies such a
conditional constraint if either _[[C]]oe is false, or if _[[L]]oe `s _[[R]]oe. The basic idea is then
to use the equivalences

(C

l=) L) `

s R j C =) (L `s R)

j L `s (C

r=) R)

to transform constraints with conditional expressions into conditional constraints, and vice
versa. The above equivalences are trivial: suppose C is true, then all three forms are
equivalent to L `s R. If C is false, then we have

(C

l=) L) `

s R j 0 `s R

j "no constraint"

j L `s 1
j L `s (C

r=) R)

Thus a conditional constraint true =) (L `s R) is equivalent to L `s R. These equivalences motivate the replacement of Rule 6.1 with the two transformation rules depicted in
Figure 6.1.

The new transformations introduce conditional constraints and we need the structural rewrite rule of Figure 6.2 to deal with them. Intuitively, given the constraint C =)
(L `s R), we strip off condition C and solve the non-conditional constraint in the standard
way. If this constraint is equivalent to fC1 =) (L1 `s1 Rn); : : : ; Cn =) (Ln `sn Rn)g,
then we add the condition C back onto each individual constraint, yielding fC ^ C1 =)
(L1 `s1 R1); : : : ; C ^ Cn =) (Ln `sn Rn)g. It is immediate that this rule is sound, but
not complete in the case where L `s R is inconsistent. We will address this case shortly.

The algorithm for transforming constraints into inductive form can now be adapted
to constraints with conditional expressions. Instead of transforming constraints into inductive constraints, we transform them to conditional inductive constraints, i.e. constraints of
the form C =) (L `s R) where L `s R is inductive. To solve conditional constraints,
repeat the following two steps until either an inconsistency is found, or all constraints are
conditional inductive.

ffl For any constraint that is not conditional inductive, apply one of the equivalences in

Figure 6.1, 6.2, or the standard resolution rules.

ffl For any pair of conditional inductive constraints C1 =) (L `s X ) and C2 =)

(X `s R), add the transitive constraint C1 ^ C2 =) (L `s R).

91
(C

l=) L) `

s R) j C =) (L `s R)

L `s (C

r=) R) j C =) (L `

s R)

Figure 6.1: Transforming constraints with conditional expressions into conditional
constraints

fL `s Rg j fC1 =) (L1 `s1 R1); : : : ; Cn =) (Ln `sn Rn)g
S [ fC =) (L `s R)g j S [ fC ^ C1 =) (L1 `s1 R1); : : : ; C ^ Cn =) (Ln `sn Rn)g

Figure 6.2: Structural rewrite rule for conditional constraints

fL `s Rg j Inconsistent
S [ f(Vi=0:::n Ci) =) (L `s R)g j S [ f(Vi=1::n Ci) =) (C0 `s 0)g

Figure 6.3: Structural rewrite rule for inconsistent constraint
The transitive constraints combine the conditions of the two constraints involved. The
justification behind this rule is as follows: If both conditions are true, then we have an
ordinary transitive constraint. Otherwise, the transitive constraint need not be satisfied.

As in the original algorithm, the final form of a solved system of constraints can
be expressed as lower and upper bounds on variables Li `s Xi `s Ui, where Li and Ui are

Li = fC

l=) L j C =) (L `

s Xi) is L-inductiveg

Ui = fC

r=) R j C =) (X

i `s R) is R-inductiveg

Thus in the final result, no conditional constraints remain, only conditional upper and lower
bounds on variables.

As described, the algorithm gives up as soon as an inconsistent constraint is found,
no matter whether the inconsistent constraint was conditional or not. E.g. any system
containing the constraint C =) (c `s d) is inconsistent under this algorithm, provided
constructor c is distinct from d. This approach does not find all solutions, for in the above
example, there could still be solutions where C is false and the constraint satisfied.

The form of conditions is simple enough so that we can encode the fact that a
condition is false with appropriate set-constraints. If C is of the form Vi=1:::n Xi " Mi, then
we can express the fact that C is false with the constraint C0 =) (Xj " Mj `s 0), where
Xj " Mj is an arbitrary atomic condition of C, and C0 = Vi=1:::j\Gamma 1;j+1:::n Xi " Mi. In other
words, either Xj " Mj is empty implying that C is false, or C0 is false. If C is true, then
the constraints have indeed no solution. Figure 6.3 contains the structural rewrite rule to
handle inconsistent systems.

The conditional inductive system is obtained without applying Rule 6.1 for splitting systems with conditional expressions. As a result, we can transform a constraint system

92
into a single conditional inductive constraint system. If the original inductive systems are
desired, the splitting rule can be applied to this system. Thus we essentially delay the
splitting until the constraints are almost in inductive form. Applying the splitting rule only
introduces additional constraints of the form E `s 0.

6.2 Graph Formulation
This section describes a graph-based representation of constraints and an algorithm to
transform a constraint system into a fully-closed constraint graph. We also characterize when
fully-closed constraint graphs are consistent, in which case, they correspond to a conditional
inductive system. Inconsistent constraint graphs and their associated constraints have no
solutions.

Even though the previous section presented constraint resolution as a rewrite system, constraint resolution is usually presented as a graph closure. The graph formulation
presented here makes it convenient to study different constraint resolution algorithms based
on graph closure.

In principle, given a set of mixed constraints S, the fully-closed constraint graph
corresponding to S is a collection of disjoint graphs Gs, one for each sort s. To simplify the
presentation here, we only show the graph construction for Set-constraints. A restriction of
this construction can be applied to the Term and FlowTerm-constraints. For Row-constraints,
the graph formulation is similar but sets of labels are used in place of M-expressions.

Definition 6.1 (Set-Constraint Graph) A constraint graph G = (V; A) consists of a set
of nodes V which are Set-expressions E with the restrictions that E is of the form

E ::= X j c(X1; : : : ; Xn) j 1 j 0 j :fc1; : : : ; cng
i.e., V contains no unions or intersections, and the arguments to constructors are variables.
An edge (or arrow) of A is a quadruple of V \Theta  V \Theta  M \Theta  C, where M is an M-expression,

and C a normalized condition. We write E1 M;C\Gamma \Gamma \Gamma ! E2 for the edge (E1; E2; M; C). A nonvariable node is called a source if it occurs to the left of an arrow, and is called a sink, if
it occurs to the right of an arrow.

Definition 6.2 (Graph Path) We say that there exists a path of length n in G from E
to E0 under M-expression M and condition C, if there exists a sequence of edges in G

E M1;C1\Gamma \Gamma \Gamma \Gamma ! X1 M2;C2\Gamma \Gamma \Gamma \Gamma ! X2 \Delta  \Delta  \Delta  Xn\Gamma 1 M

n;Cn\Gamma \Gamma \Gamma \Gamma ! E0

and

C = ^

i=1::n

Ci

M = "

i=1::n

Mi

If the intermediate nodes are not of interest, we refer to such a path using the notation
E M;C\Gamma \Gamma \Gamma !\Lambda E0.

93
We now show how to transform a system of constraints S into its initial constraint graph. The first step is to normalize constructor expressions c(E1; : : : ; En) where
some Ei are not variables. Replace each expression c(E1; : : : ; En) in the constraints with
c(X1; : : : ; Xn) where X1::Xn are fresh variables, and add the constraints Ei `'i Xi to S
if the occurrence c(E1; : : : ; En) being replaced appears in an L-context in the constraint.
Otherwise (the occurrence appears in an R-context), add the constraints Xi `'i Ei to S.

Constraints E1 `s E2 of S where E1 and E2 adhere to the node-restrictions in
the definition above can be directly added to the graph, by adding E1 and E2 and their

sub-expressions to V , and adding the edge E1 1;true\Gamma \Gamma \Gamma ! E2 to A. The remaining constraints
are of the forms

E1 t E2 `s E3

E1 `s E2 u E3
X " M `s E

E1 `s Pat[E2; M ]

C

l=) E1 `

s E2

E1 `s C

r=) E2

We show how to normalize each of these in turn:

ffl [L-union] If E1 t E2 `s E3 is in S, remove it and add E1 `s E3 and E2 `s E3 to S.
ffl [R-inter] If E1 `s E2 u E3 is in S, remove it and add E1 `s E2 and E1 `s E3 to S.
ffl [L-inter] If X " M `s E is in S, and E adheres to the node-restriction, then the

constraint is normalized. Otherwise, apply one of the other rules matching E.

ffl [R-Pat] If E1 `s Pat[E2; M ] is in S, and E1 and E2 adhere to the node-restriction,

then the constraint is normalized.

Otherwise, apply the other rules to normalize E1. If E1 adheres, but E2 does not
adhere to the node-restriction, remove the constraint from S and add the constraint
E1 " M `s E2 to the constraints, normalizing the intersection E1 " M if necessary.

ffl [L-cond] If C

l=) E1 `

s E2 is in S and E1 or E2 do not adhere to the node-restriction,

remove the constraint from S and add E1 `s E2 to a new system S0 and normalize

it. Then for each normal constraint Ci =) (Ei `s E0i) in S0, add the conditional
constraint C ^ Ci =) (Ei `s E0i) back to S.

ffl [R-cond] If E1 `s C

r=) E2 is in S and E1 or E2 do not adhere to the noderestriction, remove the constraint from S and add E1 `s E2 to a new system S0 and
normalize it. Then for each normal constraint Ci =) (Ei `s E0i) in S0, add the
conditional constraint C ^ Ci =) (Ei `s E0i) back to S.

94
After normalization, S contains constraints of the form

C =) (X " M `s E2)

C =) (E1 `s Pat[E2; M ])
C =) (E1 `s E2)

where each E1 and E2 adhere to the node-restriction, and C is a conjunction of atomic
conditions. (Recall that the empty conjunct is equivalent to true). These constraints are
then added to G using the following three rules

ffl [L-inter] If C =) (X " M `s E) is in S, remove it and add X , E, and their

sub-expressions to V , and add the edge X M;C\Gamma \Gamma \Gamma ! E to A.

ffl [R-Pat] If C =) (E1 `s Pat[E2; M ]) is in S, remove it and add E1, E2, and their

sub-expressions to V and add the edge E1 M;C\Gamma \Gamma \Gamma ! E2 to A.

ffl [Cond] If C =) (E1 `s E2) is in S and E1 and E2 adhere to the node-restriction,

remove the constraint and add E1, E2, and their sub-expressions to V , and add the

edge E1 1;C\Gamma \Gamma ! E2 to A.

We now present the rules for closing a constraint graph under transitivity and
structural constraints.

Transitive Closure Rule If E1 M1;C1\Gamma \Gamma \Gamma \Gamma ! X and X M2;C2\Gamma \Gamma \Gamma \Gamma ! E2 are edges in G, add the

transitive edge E1 M1"M2;C1^C2\Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma ! E2 to the edges of G.

The structural closure rules cover all combinations of source-sink pairs.

Structural Closure Rules If E1 M;C\Gamma \Gamma \Gamma ! E2 is an edge of G between a source E1 and a sink

E2, apply the closure rule that applies to E1; E2 below.

ffl c(X1; : : : ; Xn) M;C\Gamma \Gamma \Gamma ! c(Y1; : : : ; Yn) and M "c(?'1; : : : ; ?'n) = Sj c(Mj1; : : : ; Mjn)

where each c(Mj1; : : : ; Mjn) 6= 0, then add edges Xi

Mji;C^C0j\Gamma \Gamma \Gamma \Gamma \Gamma \Gamma ! Y

i for all j, where

C0j = ae Vi Xi " Mji if c stricttrue otherwise

ffl c(X1; : : : ; Xn) M;C\Gamma \Gamma \Gamma ! :fc1; : : : ; cng and c 2 fc1; : : : ; cng, then add the edge

c(X1; : : : ; Xn) M;C\Gamma \Gamma \Gamma ! 0
ffl c(X1; : : : ; Xn) M;C\Gamma \Gamma \Gamma ! E2 where E2 = 0, or E2 = d(: : : ) with c 6= d.

95
1. if n ? 0, c strict, and M " c(?'1 ; : : : ; ?'n) = Sj c(Mj1; : : : ; Mjn), where each

c(Mj1; : : : ; Mjn) 6= 0, then add edges 1

1;C^C0j\Gamma \Gamma \Gamma \Gamma ! 0 for all j, where

C0j = ^

i

Xi " Mji

2. otherwise add the edge 1

M"c(?'1;::: ;?'n);C\Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma ! 0.

ffl :fc1; : : : ; cng M;C\Gamma \Gamma \Gamma ! E2, and E2 is c(: : : ), 0, or :fd1; : : : ; dng, then add the edge

1 :fc

1;::: ;cng"M;C\Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma ! E2

ffl 1 M;C\Gamma \Gamma \Gamma ! c(X1; : : : ; Xn) add the edge 1 M":fcg;C\Gamma \Gamma \Gamma \Gamma \Gamma \Gamma ! 0, and if M " c(?'1 ; : : : ; ?'n) =S

j c(Mj1; : : : ; Mjn) where each c(Mj1; : : : ; Mjn) 6= 0, then also add edges

1

Mji;C\Gamma \Gamma \Gamma \Gamma ! X

i

for all j.
ffl 1 M;C\Gamma \Gamma \Gamma ! :fc1; : : : ; cng, and n 6= 0, add the edge 1 M"M

0;C\Gamma \Gamma \Gamma \Gamma \Gamma ! 0, where

M 0 = [

c2fc1;::: ;cng

c(?'1 ; : : : ; ?'n)

ffl 1 M;C\Gamma \Gamma \Gamma ! 0, M 6= 0, and C = C1 ^ \Delta  \Delta  \Delta  ^ Cn, n ? 0, then add the edge

X1 M1;C

0\Gamma \Gamma \Gamma \Gamma ! 0

where

C1 = X1 " M1

C0 = C2 ^ \Delta  \Delta  \Delta  ^ Cn

Definition 6.3 (Graph Consistency) A constraint graph is consistent, if its closure does
not contain any edge of the form 1 M;true\Gamma \Gamma \Gamma \Gamma ! 0, where M 6= 0.

If a graph is consistent, then there exists an equivalent conditional inductive system S,
obtained as follows:

ffl if X M;C\Gamma \Gamma \Gamma ! E is in G and E is a sink, then add the inductive constraint X `s C

r=)

Pat[E; M;] to S.

ffl if E M;C\Gamma \Gamma \Gamma ! X is in G and E is a source, then add the inductive constraint C

l=)

(E " M ) `s X to S.

96

ffl if X M;C\Gamma \Gamma \Gamma ! Y is in G and o(X ) ? o(Y), then add the inductive constraint X `s C

r=)

Pat[Y; M;] to S.

ffl if X M;C\Gamma \Gamma \Gamma ! Y is in G and o(Y) ? o(X ), then add the inductive constraint C

l=)

(X " M ) `s Y to S.

The resulting system S is inductive, since all constraints above are inductive, and the
transitive closure rule of the graph adds all edges added by Algorithm 5.25.

On the other hand, if the closure of the constraint graph G corresponding to the
normalized constraint set S is inconsistent, then applying Algorithm 5.25 to S also results in
no inductive systems. To see this, observe that the structural graph closure rules correspond
to the resolution rules given earlier, and that Algorithm 5.25 has the following property.

Property 6.4 If E and E0 are expressions without top-level variables and there is a path

E M1;C1\Gamma \Gamma \Gamma \Gamma ! X1 M2;C2\Gamma \Gamma \Gamma \Gamma ! X2 \Delta  \Delta  \Delta  Xn\Gamma 1 M

n;Cn\Gamma \Gamma \Gamma \Gamma ! E0

in the initial graph G corresponding to the normalized constraint system S, then Algorithm 5.25 applied to S generates the constraint C =) (E " M ) `s E0, where

C = ^

i=1::n

Ci

M = "

i=1::n

Mi

Proof: If n = 1 the result is immediate. Note by edge E M1;C1\Gamma \Gamma \Gamma \Gamma ! X1 we have
that S contains the conditional inductive constraint constraint C1 =) E " M1 `s X1.

Call this constraint the source constraint. Similarly by edge Xn\Gamma 1 M

n;Cn\Gamma \Gamma \Gamma \Gamma ! E0 we have that

S contains the conditional inductive constraint Cn =) Xn\Gamma 1 `s Pat[E0; Mn]. Call this
edge the sink constraint. If n = 2, then X1 = Xn\Gamma 1, and Algorithm 5.25 adds the transitive
constraint C1 ^C2 =) E "M1 `s Pat[E0; M2], which is equivalent to the sought constraint.
We now proceed by induction on the length of the path. Assume the property holds for
paths of length k. We show that it also holds for paths of length k + 1. Suppose n = k + 1.
There are three cases to consider.

1. If the index o(X1) is greater than the index o(X2), then by edge X1 M2;C2\Gamma \Gamma \Gamma \Gamma ! X2 we

know that S contains the conditional inductive constraint C2 =) X1 `s Pat[X2; M2].
This constraint together with the source constraint produces the transitive constraint
C1 ^C2 =) E "M1 `s Pat[X2; M2], which is transformed to the conditional inductive
constraint C1 ^ C2 =) E " (M1 " M2) `s X2. But now there exists a path E M;C\Gamma \Gamma \Gamma !\Lambda E0
of length k and by induction the constraint C =) E " M `s E0 is added.

2. If the index o(Xn\Gamma 1) is greater than the index o(Xn\Gamma 2), then by edge Xn\Gamma 2 M

n\Gamma 1;Cn\Gamma 1\Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma !

Xn\Gamma 1 we know that S contains the conditional inductive constraint Cn\Gamma 1 =) Xn\Gamma 2 "
Mn\Gamma 1 `s Xn\Gamma 1. This constraint together with the sink constraint produces the transitive constraint Cn\Gamma 1 ^ Cn =) Xn\Gamma 2 " Mn\Gamma 1 `s Pat[E0; Mn], which is transformed

97
X
list

Y
list

Figure 6.4: Unnecessary edges in full graph closure
to the conditional inductive constraint Cn\Gamma 1 ^ Cn =) X `s Pat[E0; M1 " M2].
But now there exists a path E M;C\Gamma \Gamma \Gamma !\Lambda E0 of length k and by induction the constraint
C =) E " M `s E0 is added.

3. Otherwise o(X2) ? o(X1) and o(Xn\Gamma 2) ? o(Xn\Gamma 1). Thus there exists a variable

with maximum index among X2::Xn\Gamma 2. Assume this variable is Xj. Then the edges

Xj\Gamma 1

Mj;Cj\Gamma \Gamma \Gamma \Gamma ! X

j and Xj

Mj+1;Cj+1\Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma ! X

j+1 correspond to the initial conditional inductive constraints Cj

l=) X

j\Gamma 1 " Mj `s Xj and Xj `s Cj+1

r=) Pat[X

j+1; Mj+1].

Thus Algorithm 5.25 adds the transitive constraint (C1 ^ C2) =) Xj\Gamma 1 " Mj\Gamma 1 `s

Pat[Xj+1; Mj+1], which is transformed into the conditional inductive constraint (C1 ^
C2) =) (Xj\Gamma 1 " (Mj\Gamma 1 " Mj+1) `s Xj+1) if o(Xj+1) ? o(Xj\Gamma 1), or into the conditional inductive constraint (C1 ^ C2) =) Xj\Gamma 1 `s Pat[Xj+1; Mj\Gamma 1 " Mj+1] if
o(Xj\Gamma 1) ? o(Xj+1). In either case there is now a path E M;C\Gamma \Gamma \Gamma !\Lambda E0 of length k and by
induction the constraint C =) E " M `s E0 is added.

2
The full closure of the constraint graph contains many edges that are not really
necessary to decide whether the graph is consistent or not. Edges between sources and
sinks that are not inconsistent and for which the corresponding structural closure rule has
been applied can be removed from the graph. Consider the example in Figure 6.4. The
graph represents the closure of the constraint list(X ) `s list(Y), where the vertical edges
relate constructors with their arguments and horizontal edges are inclusion edges. Applying
the structural rule to to the inclusion edge between the two list constructor nodes adds the
dashed inclusion edge between X and Y. After this step, the inclusion edge between the
two list constructors is never needed again and can be deleted.

This optimization is easy to implement. Simply apply the structural rules immediately whenever a new source-sink edge is to be added. Furthermore, the transitive closure
rule can be restricted so as to add only edges between a source E1 and a sink E2, if there
is a path E1 M;C\Gamma \Gamma \Gamma !\Lambda E2 in the graph. In fact the transitive edge needs only be added, if the
M is non-empty and the condition C is true in all solutions. Transitive edges where one
end-point is a variable can be avoided since they do not trigger any structural rules and do
not make the graph inconsistent. One can thus formulate a minimal transitive closure rule:

Minimal Transitive Closure Rule If there exists a path E M;C\Gamma \Gamma \Gamma !\Lambda E0 between a source E

and a sink E0 in G, and M 6= 0, and C is true in all solutions of the constraints, then

add the edge E M;C\Gamma \Gamma \Gamma ! E0 to G.

98
The minimal transitive closure rule does not yield a practical algorithm however, since
whenever a new edge is added to the graph by the structural closure rules, all paths would
need to be reexamined. Furthermore, deciding whether a condition is true in all solutions
requires information about paths from sources to variables, and this information may be
expensive to compute and maintain in itself.

The distinguishing feature between different algorithms for computing constraint
graph consistency is thus the transitive closure rule. Practical algorithms usually use a local
rule, where only a small constant number of edges need to be considered, instead of paths of
arbitrary length. We have already seen one alternative transitive closure rule that is local,
namely the one applied by the algorithm computing inductive systems.

Inductive Transitive Closure Rule If G contains two edges E1 M1;C1\Gamma \Gamma \Gamma \Gamma ! X and X M2;C2\Gamma \Gamma \Gamma \Gamma !

E2 such that any top-level variables of E1 and E2 have indices lower than o(X ), then

add the transitive edge E1 M1"M2;C1^C2\Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma ! E2 to the graph.

It is instructive to examine where exactly sources and sinks meet on a path
E M;C\Gamma \Gamma \Gamma !\Lambda E0 using the above rule. From the proof of Property 6.4 we know that the closure rule adds transitive edges between variables so as to guarantee that there exists a
path

E M1;C1\Gamma \Gamma \Gamma \Gamma ! X1 M2;C2\Gamma \Gamma \Gamma \Gamma ! X2 \Delta  \Delta  \Delta  Xn\Gamma 1 M

n;Cn\Gamma \Gamma \Gamma \Gamma ! E0

where no variable Xj among X2::Xn\Gamma 2 has index larger than its neighbors, i.e., o(Xj) 6?
o(Xj\Gamma 1) and o(Xj) 6? o(Xj+1). Consider thus the sequence of indices o(X1); o(X2); : : : ; o(Xn)
and the variable Xj with minimal index. Then the sequence o(X1); : : : ; o(Xj) is strictly
decreasing, and the sequence o(Xj); : : : ; o(Xn) is strictly increasing. As a result, the edges

Xi\Gamma 1 Mi;Ci\Gamma \Gamma \Gamma ! Xi with i ^ j are R-inductive and the source E is propagated forward along this
path to each Xi up to the minimal indexed variable Xj. Similarly, the edges Xi M

i+1;Ci+1\Gamma \Gamma \Gamma \Gamma \Gamma \Gamma !

Xi+1 with i * j are R-inductive and the sink E0 is propagated backward along this path
to each Xi down to the minimal indexed variable Xj. Thus the inductive transitive closure
rule produces the two inductive constraints

(C1 ^ \Delta  \Delta  \Delta  ^ Cj) =) E " (M1 " \Delta  \Delta  \Delta  " Mj) `s Xj
(Cj+1 ^ \Delta  \Delta  \Delta  ^ Cn) =) Xj `s Pat[E0; Mj+1 " \Delta  \Delta  \Delta  " Mn]

from which a final application of the rule produces the edge E M;C\Gamma \Gamma \Gamma ! E0.

The inductive closure rule is rather non-standard in that sources move forwards
and sinks backwards along certain, but not all paths. Most published algorithms for this
kind of graph closure use something akin to the following rule, which we thus call the
standard transitive closure rule.

Standard Transitive Closure Rule If G contains two edges E1 M1;C1\Gamma \Gamma \Gamma \Gamma ! X and X M2;C2\Gamma \Gamma \Gamma \Gamma !

E2 such that E1 is a source, then add the transitive edge E1 M1"M2;C1^C2\Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma ! E2 to the
graph.

This standard closure works rather differently from the inductive closure rule. The rule
uniquely propagates sources forward along paths until they meet up with a sink. One could

99
imagine a dual rule that propagates sinks backwards until they meet a source. However,
the standard rule has the advantage of computing an explicit form of the transitive lower
bound for each variable which we discuss in the following section. Empirical results on the
practical tradeoffs of using the inductive transitive closure rule over the standard transitive
closure rule are studied in more detail in Chapter 9.

6.2.1 Transitive Lower Bound
In many analyses, it is often desirable to answer queries such as, "does X contain an ideal
OEc(I1; : : : ; In) 6= f?g for some Ij in all solutions of the constraints?" To answer such queries
easily, the transitive lower bound of X (written TLB(X )) can be computed.

Definition 6.5 (Transitive Lower Bound) The transitive lower bound of X , consists of
the union of all non-empty expressions in the intersection E " M where E M;C\Gamma \Gamma \Gamma !\Lambda X is a path
in G, and C is true in all solutions. Formally,

TLB(X ) = [fTLB(C

l=) E " M ) j E M;C\Gamma \Gamma \Gamma !\Lambda X g

where E = c(X1; : : : ; Xn), M " c(?'1 ; : : : ; ?'n) = Sj c(Mj1; : : : ; Mjn), and

TLB(C

l=) E " M ) = fc(X1 " M

j1; ::; Xn " Mjn) j C ^ Cj is true in all solutionsg

Cj = ae true c non-strictX

1 " Mj1 ^ \Delta  \Delta  \Delta  ^ Xn " Mjn otherwise

Computing TLB(X ) thus involves deciding which conditions are true in all solutions. An
atomic condition Y "M is true in all solutions, if TLB(Y)"M is non-empty. Thus, computing
the transitive lower bound and resolving conditions that are true in all solutions depend on
each other. We tackle both problems by first considering the conditional transitive lower
bound of X (CTLB(X )) which essentially consists of all conditional expressions C

l=) E

where E is a candidate for TLB(X ) if C is true in all solutions. The transitive lower bound
TLB(X ) is related to CTLB(X ) by

c(E1; : : : ; En) 2 TLB(X ) () C

l=) c(E1; : : : ; E

n) 2 CTLB(X ) ^

C true in all solutions

Definition 6.6 (Conditional Transitive Lower Bound) The conditional transitive lowerbound of X consists of the union of all conditional expressions in CTLB(C

l=) E " M )

where E is a source and there is a path E M;C\Gamma \Gamma \Gamma !\Lambda X in G. Formally,

CTLB(X ) = [fCTLB(C

l=) E " M ) j E M;C\Gamma \Gamma \Gamma !\Lambda X g

CTLB(C

l=) E " M ) = [

j i

C ^ Cj

l=) c(X1 " M

j1; ::; Xn " Mjn)j

where E = c(X1; : : : ; Xn), M " c(?'1 ; : : : ; ?'n) = Sj c(Mj1; : : : ; Mjn), and

Cj = ae true c non-strictX

1 " Mj1 ^ \Delta  \Delta  \Delta  ^ Xn " Mjn otherwise

100
The next section describes how to identify conditions that are true in all solutions of the
constraints using the conditional transitive lower bounds. In the remainder of this section
we show how to compute the conditional transitive lower bounds when constraint graphs
are closed under the standard transitive closure rule, and when constraint graphs are closed
under the inductive transitive closure rule.

Suppose G is closed under the standard transitive closure rule (STCR). Recall that
STCR propagates sources forward along all paths. If there is a path E M;C\Gamma \Gamma \Gamma !\Lambda X in G, then

G also contains the edge E M;C\Gamma \Gamma \Gamma ! X . Therefore, to compute CTLB(X ) for a variable X from
G it suffices to compute CTLB(C =) E " M ) for all edges E M;C\Gamma \Gamma \Gamma ! X in G.

Now suppose G is closed under the inductive transitive closure rule (ITCR). Recall
from above that if G is closed under ITCR and E M;C\Gamma \Gamma \Gamma !\Lambda X is a path from a source E to X ,
then G contains a path

E M1;C1\Gamma \Gamma \Gamma \Gamma ! X1 M2;C2\Gamma \Gamma \Gamma \Gamma ! X2 \Delta  \Delta  \Delta  Xn\Gamma 1 M

n;Cn\Gamma \Gamma \Gamma \Gamma ! X

where the sequence of indices o(X1); o(X2); : : : ; o(Xn\Gamma 1); o(X ) are strictly increasing. This
suggests an inductive strategy to compute CTLB(X ):

ffl Compute CTLB(Y) for all Y with index lower than X .
ffl Let

CTLB(X ) = [fCTLB(C

l=) E " M ) j E M;C\Gamma \Gamma \Gamma ! X 2 G; E a sourceg

[ [fCTLB(C ^ C0

l=) E " M ) j Y M;C\Gamma \Gamma \Gamma ! X ^ o(Y) ! o(X ) ^

C0

l=) E 2 CTLB(Y)g

The strategy is well founded since if X is the variable with minimal index, then for every
path E M;C\Gamma \Gamma \Gamma !\Lambda X in G where E is a source, there is an edge E M;C\Gamma \Gamma \Gamma ! X in G, and CTLB(X )
does not depend on the CTLB of any other variable.

The CTLB computation on a graph closed under ITCR is similar to computing the
transitive closure of an acyclic graph. Consider the simple case where all conditions in G are
true, all M-expressions are 1, and all constructors are non-strict. Then computing CTLB(X )
corresponds exactly to computing the transitive closure of the acyclic subgraph G0 formed

by all variables and sources of G, and containing all edges E 1;true\Gamma \Gamma \Gamma ! X of G where E is a
source, or E = Y and o(Y) ! o(X ), i.e., CTLB(X ) = fE j E 1;true\Gamma \Gamma \Gamma !\Lambda X 2 G0g. There exist
efficient algorithms for computing the transitive closure of an acyclic graph with worst-case
time complexity O(ne\Gamma ), where n is the number of nodes of G0, and e\Gamma  is the number of
edges in the transitive reduction of G [84]. We now present an algorithm for the general
case that has this complexity in the above restricted case.

The algorithm computes a set R(Xj) for each variable Xj, which is a set of triples
(E; M; C) such that there is a path E M;C\Gamma \Gamma \Gamma !\Lambda Xj in G0. The conditional transitive lower bound
CTLB(Xj) can then be computed from R(Xj), by

CTLB(Xj) = [fCTLB(C

l=) E " M ) j (E; M; C) 2 R(X

j) ^ E a sourceg

101
The extra elements of R(Xj) that are ignored for CTLB(Xj) are used to prune transitive
edges in G0, and thus to avoid merging redundant sets into R(Xj).

Algorithm 6.7 Suppose R(Xk) has been computed for all Xk with index less than o(Xj).
Compute R(Xj) as follows.

Initially, let R(Xj) = fg. Consider each edge E M;C\Gamma \Gamma \Gamma ! Xj in G0 in turn, ordered by
decreasing top-level variable index of E (expressions without top-level variables come last
in no particular order). If (E; M; C) 2 R(Xj), ignore this edge. Otherwise add (E; M; C)
to R(Xj) and if E = Xk let I = f(E0; M 0 " M; C0 ^ C) j (E0; M 0; C0) 2 R(Xk)g and add set
I to R(Xj) as well. Note that if M = 1 and C = true, then I = R(Xk).

We now show how the algorithm avoids transitive edges Xi M;C\Gamma \Gamma \Gamma ! Xj when there are
edges Xi M;C\Gamma \Gamma \Gamma ! Xk, and Xk 1;true\Gamma \Gamma \Gamma ! Xj in G0. Suppose we are computing R(Xj). Then
the above edges imply that any triple (E; M 0; C0) added to R(Xj) through Xi M;C\Gamma \Gamma \Gamma ! Xj
is also an element of R(Xk). The algorithm avoids merging these elements twice into

R(Xj) by considering edge Xk 1;true\Gamma \Gamma \Gamma ! Xj before Xi M;C\Gamma \Gamma \Gamma ! Xj since o(Xi) ! o(Xk). After merging R(Xk) into R(Xj), R(Xj) contains the triple (Xi; M; C). Thus when next

considering the edge Xi M;C\Gamma \Gamma \Gamma ! Xj, the algorithm skips it correctly, since all elements
f(E; C ^ C0; M " M 0) j (E; C0; M 0) 2 R(Xi)g are already present in R(Xj).

6.2.2 Condition resolution
Given a system of constraints S, certain conditions C will be true in all solutions of S. Such
information may be equally interesting to a program analysis as the explicit bounds on set
variables themselves. This section describes how to resolve the status of all conditions in a
closed constraint graph G. It is possible to classify conditions into two categories: 1) either
a condition is true in all solutions, or 2) it is false in some solution. It is interesting to
note that for the conditions of the 2nd category, there exist solutions of G for which these
conditions are all simultaneously false.

For notational convenience, we define CTLB(X ) " M = SfCTLB(C

l=) E " M ) j

C

l=) E 2 CTLB(X )g. Assume that we have computed CTLB(X

j) for all variables Xj.

Recall that a condition Vi Xi " Mi is true in all solutions if each atomic condition Xj " Mi

is true in all solutions. An atomic condition X " M in turn is true, if the set CTLB(X ) " M

contains an element true

l=) E, or some element C l=) E0 where C is true. We use

this observation to build a condition dependency graph GC as follows. Let the nodes of
GC be the set of all conditions (conjuncts and atomic) appearing in CTLB(Xj) for some Xj.

There is an implication edge from condition C to atomic condition X " M , iff C

l=) c(: : : )

appears in CTLB(X ) " M . Furthermore, there is a containment edge from atomic condition
X " M to every condition C0 containing X " M as a conjunct.

Algorithm 6.8 The following naive algorithm marks all conditions that are true in every
solution:

Repeat until no more conditions are marked

102

For each condition C in GC

if C is not marked then

if C is atomic of the form X " M then

if CTLB(X ) " M contains true

l=) E, then

mark C
fi
else (C is not atomic)

inspect all C0 s.t. C0 ! C is a containment edge.
If all C0 are marked, then mark C fi
fi
else (C is marked)

for all implication edges C ! C0, mark C0
fi
endfor
endrepeat

Clearly, if the algorithm marks a condition C, then C is true in all solutions. On the other
hand, suppose there is an atomic condition X " M not marked by the algorithm. Then we

can add the edge X M;true\Gamma \Gamma \Gamma \Gamma ! 0 to G without making the graph inconsistent.

There is clearly a better algorithm which visits each edge in GC at most once.

Algorithm 6.9 Associate a counter cnt (C) with each condition C. For atomic conditions,
set the counter to 1. For non-atomic conditions, set the counter to the number of its atomic
conjuncts.

dec-and-propagate(C) =

if cnt(C) ? 0 then

cnt(C) := cnt(C) - 1
if cnt(C) == 0 then

mark C
for each outgoing edge C ! C0 in GC

dec-and-propagate(C0)
endfor
fi
fi

For each atomic condition C of the from X " M

if CTLB(X ) " M contains an element true

l=) E, then

dec-and-propagate(C)
fi

The identification of conditions that are true in all solutions can be performed
incrementally and at the same time that the constraint graph closure is computed. It is
then possible to take advantage of the information about conditions to add only edges

E M;true\Gamma \Gamma \Gamma \Gamma ! E0 to the graph where the condition is true. Heintze describes such an algorithm
for computing the results of Set-Based Analysis in his dissertation [38]. This algorithm
is based on a variation of the standard transitive closure rule that adds only edges whose
condition is true. The algorithm requires substantial book-keeping of suspended edges, i.e.,

103
edges E M;C\Gamma \Gamma \Gamma ! E0 that would normally be added through the transitive or structural closure,
but where C is not known to be true. Furthermore, whenever a new edge is added to the
graph, the condition of some suspended constraint may become true, and thus suspended
edges need to be examined.

The implementation of BANE described in the second part of this dissertation
does not suspend conditional constraints. It is based on the inductive transitive closure
rule (ITCR) and solves conditional constraints as described in Section 6.1. Algorithm 6.9 is
used to explicitly solve conditions when computing the transitive lower bounds TLB from
the conditional transitive lower bounds CTLB. As we show in Section 9.4, this approach
enables computing the TLB on-demand, resulting in substantially smaller constraint graphs
than obtained using the standard algorithm for Set-Based Analysis.

6.3 Complexity
We briefly discuss the worst-case time complexity for deciding the consistency of set of mixed
constraints. The general problem of deciding the consistency of the closure of a constraint
graph is at least NEXPTIME. This lower bound follows from a result by McAllester and
Heintze [59], who show complexity bounds for various refinements of Set-Based Analysis
(SBA). The hardest form of SBA they study is NEXPTIME-complete. Since all their SBA
variations can be formulated using the Set-constraint part of mixed constraints, resolution
of mixed constraints is at least NEXPTIME.

6.4 Discussion and Related Work
The use of inclusion constraints in program analysis goes back to an early paper by Reynolds.
Reynolds [74] describes an analysis for inferring the shape of data-structures in Lisp programs. It provides the basic idea of later analyses based on inclusion constraints [48, 2, 41],
although the same ideas were independently rediscovered in some cases.

Jones and Muchnick [48] describe an abstract interpretation approach to inferring a
description of list data-structures at each program point of imperative flow-chart programs.
Their system is very similar to the one of Reynolds, but is not expressed using constraints.

Decidability and complexity results for a number of variations of set constraints
have been studied by numerous researchers [40, 33, 34, 10, 5, 13, 14, 6]. Heintze popularized set-constraints for program analysis through his work on set-based analysis (SBA) of
logic and functional programs [38]. He expresses set-based analysis using constraints of the
form op(X1::Xn) ` X , where the right-hand side of all constraints is a set-variable, and
the left-hand side is an arbitrary monotone set-operator op. His formulation of solutions to
the constraints is somewhat non-standard, since the interpretation of certain set-operators
constrains the solution. As a result, his formulation only applies to the least solution of
the constraints. Least solutions always exist since all set-operators are monotone. The constraint resolution proposed by Heintze differs from our approach in that it uses the standard
transitive closure rule we described earlier. We will contrast our implementation of Setconstraints with the standard set-constraint resolution described in Heintze's dissertation
further in Chapter 9, where we show that constraint resolution based on standard transitive

104
closure is severely limited in its ability to scale to large constraint problems due to the fact
that it explicitly computes the transitive lower bound of all variables.

Independently of Heintze, Aiken and Wimmers developed set-constraint decision
procedures for type inference of FL [2, 3]. The Set-constraint resolution of mixed constraints
is based on their work, which we already described in Chapter 2.

FlowTerm-constraints can be viewed as constraints arising in the type inference system PTB_+int studied by Palsberg and O'Keefe [67] and Palsberg, Wand, and O'Keefe [69].
The system PTB_+int stands for partial types with top and bottom and integer types. It
is an extension of Thatte's system of partial types [81] with bottom. Constraints arising
in this system are non-structural subtyping relations, meaning that two types can be related even if they don't have the same structure. The non-structural relation matches our
inclusion relation on FlowTerm-ideals. Palsberg et al. [67] show that their type system is
equivalent to a flow analysis and give a cubic time algorithm to infer the flow relations. The
algorithm is essentially the full constraint graph closure we described in this chapter.

Term-constraints are most similar to equality constraints between first-order terms,
but augmented with top and bottom. Without top, Term-constraints correspond to the
conditional unification constraints proposed by Steensgaard [79]. A constraint E1 ^ E2
in his system is satisfied, if either E1 is bottom, or E1 = E2, but there is no top in his
language. Steensgaard describes a nearly linear time algorithm for solving such constraints.
The constraints proposed by Henglein for inference of global tagging optimization of Lisp
programs [46] and for binding-time analysis in partial evaluation [45] are similar to Termconstraints with top, but no bottom. A constraint E1 ^ E2 in these systems is satisfied if
either E2 is top, or E1 = E2. However, in certain variations of the tagging and bindingtime analyses, further constraints are placed on E1 if E2 is top, namely that if E1 has the
form c(E01; ::; E0n), then E0i = ? for i = 1::n. Henglein shows that such constraints can be
solved in nearly linear time. These extra constraints on the sub-expressions of E1 cannot
be expressed in our current formulation of mixed constraints.

The notion of row types was introduced by Wand [86]. He later pointed out
that his algorithm for solving equality constraints between type and row expressions may
not terminate due to the introduction of fresh row variables [87]. The fix to Wand's nontermination problem is similar to the termination argument we used for splitting closed rows.
Both ideas are based on the formalization of record types by R'emy [73]. R'emy proposed an
extension of the ML type system with extensible record types. R'emy recognized that Rowvariables need an annotation indicating the set of labels that may not be present. He writes
this annotation as a superscript O/L, whereas we introduced domain constraints. Type
inference for R'emy's record extensions are based on equality constraints between closed
rows. Record subtyping is obtained in his system solely through parametric polymorphism.
Jategaonkar and Mitchell [47] consider a type system and type inference for an extension
of ML with records and atomic subtyping. They provide an inference algorithm based
on unification [75] and atomic subtype resolution [63]. Ohori [65] presents a type system
similar to the one of R'emy for adding extensible records. Type inference is again based on
equality constraints and implemented through an extension of unification. Record subtyping
is obtained through bounded parametric polymorphism. So far, all record type inference
systems described are based on equality constraints that are solved through an extension to

105
the unification algorithm. The only type inference system we are aware of that uses inclusion
constraints between records is by Stansifer [78]. He studied type inference in the presence
of record and variant types. The constraints arising in his type derivations between variant
rows correspond to constraints between minimal Row-expressions, and the constraints on
record types correspond to constraints on maximal Row-expressions. Stansifer did not give
any constraint resolution rules or consistency criteria for the generated constraints. The
resolution of minimal and maximal Row-constraints presented here can be seen as filling in
that gap.

Particular instances of mixed constraint systems have been described in the literature in the past. Type systems with record types have already been described above. They
fall into the category of mixed Term and Term-Row expressions and constraints. Another
category of mixed expressions and constraints arise in effect systems [54, 55, 49, 80, 82]. Effect systems generalize type systems in that they also infer a description of some behavioral
aspects of evaluation, usually side effects. Examples of side effects are reads and writes
to the store, sending or receiving messages, and non-local control transfers (for example
exceptions). Effects are naturally described as sets. Classic effect systems are instances of
mixed Term and Set expressions and constraints, with the additional property that there
are no mixed Set-constructors, i.e., Term expressions don't appear in effects. Lucassen and
Gifford [55] describe type and effect inference rules using a subset relation on types induced
by the subset relation of effect sets contained in the types. Such constraints correspond to
mixed FlowTerm-Set constraints. However, they do not show how to solve such constraints
and, in fact, in a later paper drop the subset constraints for equality constraints, obtaining
a Term-Set instance of mixed constraints, which they solve with generalized unification [49].
Similarly, Tofte and Talpin [82] use a mixture of types and sets in an effect system to infer allocation and deallocation points of memory regions at compile-time. Their inference
rules are also based on equality constraints which they solve using a generalized unification
procedure. In Section 8.2 we describe an effect system for computing uncaught exceptions
in Standard ML programs based on three different mixed constraint systems, one of which
we have published earlier [25]. The exception inference system generalizes previous effect
systems in that effect sets also contain mixed constructors.

Mossin [64] independently studied a form of mixed constraints in the context of
flow analysis for higher order functional programs. In his formulation, flow analysis computes information about how values flow through a program. Every expression constructing
a value is tagged with a label. The result of a flow analysis is then a mapping of program
points to sets of labels. He formulates flow analysis as a type system, where standard types
for functional programs are augmented with label annotations. Constraints between annotated types are similar to mixed FlowTerm-Set constraints. However, Mossin's formulation
separates the type inference from the flow analysis. During flow analysis, the type structure
of the program is already known, and only constraints between the label annotations are
generated and solved. Furthermore, his resolution algorithm is based on the fact that the
label sets only contain constants and variables and that no non-trivial upper bounds on
label sets are added as constraints. As a result, the label set constraint graph is trivially
consistent and needs not be closed under transitive or structural rules. This property results
in an pseudo-linear time algorithm for computing the flow graph. Pseudo-linear refers to

106
the fact that the algorithm is linear in the size of the type structure of a program, but the
type structure itself can be exponential in the program size. However, many researchers
have argued that in practice programmers don't write programs with huge types, for such
programs would be hard to understand. Thus in practice, the size of the type structure is
often of the same order as the program size. Individual queries about labels sets can be
answered in linear time by a depth-first search in the flow graph. Computing all queries
results in a quadratic algorithm.

Independently, Heintze and McAllester [42] developed essentially the same idea
for performing closure analysis of ML programs. Each lambda expression is labelled and
types are augmented with label annotations. They compute a flow graph similar to Mossin
in pseudo-linear time and are able to answer individual queries in linear time.

There is a large body of work on atomic subtyping constraints starting with
Mitchell [63] and later Fuh and Mishra [32], which we do not review here since the current
formulation of mixed constraints does not have a notion of atomic subtyping. It would
however be entirely plausible to add another sort for expressions with an atomic subtype
relation.

107
Part II
BANE

109
Chapter 7
BANE: An Implementation of
Mixed Constraints

This chapter describes the implementation of the BANE library, discussing important engineering decisions and algorithmic ideas supporting the resolution of large constraint
problems. BANE (the Berkeley ANalysis Engine) implements a resolution engine for mixed
constraints based on the algorithm for solving conditional constraints presented in Chapter 6.

BANE is implemented as a library in Standard ML of New Jersey [9] and encompasses about 20,000 lines of code.

Section 7.1 describes implementation aspects common to all sorts, in particular
the representation used for constraint graphs. The subsequent sections describe aspects
specific to the implementation of each sort. Finally, Section 7.6 describes BANE's support
for polymorphic constraint-based analysis.

7.1 Constraint Graph Representation
This section describes BANE's representation of constraint graphs. Recall from Chapter 5
and Chapter 6 that constraints can be simplified to inductive constraints on variables. In
terms of graph edges, each inductive constraint represents an edge between two mixed expressions, where at least one endpoint is a variable. This fact enables a graph representation
where edges are stored as adjacency lists on variable nodes. Note that graph representations
based on adjacency matrices are not practical since they require space quadratic in the number of nodes even if the graph is sparse. BANE's basic assumption is that the final (closed)
constraint graph is sparse, i.e., the number of edges will be roughly linear in the number of
nodes. Constraint problems with inductively closed graphs containing a quadratic number
of edges are not expected to scale well. As we show in later sections, BANE uses techniques
to restrain the blowup of edges and maintains sparse graphs for many constraint problems
that would otherwise exhibit a quadratic number of edges.

We use ML datatype declarations to specify the shape of the data structures
described in this chapter. An ML datatype declaration has the form

110

datatype dty = C1 [of ty1] : : : Cn [of tyn]

where dty is the name of the declared datatype, and the Ci are value constructors.
Value constructors are either constants, or they carry an argument of type tyi. A value of
a datatype dty is either one of the constants Ci, or a constructor Ci applied to a value of
type tyi.

The next subsection describes the representation of nodes in our constraint graphs,
followed by a subsection on the representation of edges. Section 7.1.3 describes alias edges,
a special kind of edge to represent equality constraints. Section 7.1.4 describes how sets
of mixed expressions are represented efficiently. Section 7.1.5 describes a technique for
performing online cycle elimination in the constraint graph which forms one of the core
techniques of BANE for scaling to large constraint problems.

7.1.1 Nodes
Nodes are essentially mixed expressions. The datatype for mixed expressions used by BANE
has the following form

datatype me = Var of varinfo

-- Cons of -con:constructor, args: me list""
-- Row of -fields: -l:label, e:me"" list, rest:me""
-- EmptyRow
-- Zero
-- One
-- LUnion of me list
-- RInter of me list
-- LInter of -v : me, m : me""
-- RPat of -e : me, m : me""
-- Cond -co:condition, e : me""
-- Neg of constructor list

The value constructors for mixed expressions consist of a variable constructor Var
with associated information of type varinfo which we describe in the next section, a value
constructor for constructor expressions Cons with an argument record containing a value
for the constructor con and the list of argument mixed expressions (args). A record in
ML fl1 : ty1, : : : , ln : tyng is similar to a struct in C with fields l1..ln containing
values of types ty1..tyn. An ML type of the form ty list is a datatype for lists of elements
of type ty. The value constructor Row is used to represent Row-composition hl : EliA ffi E,
where fields is the list of label-expression pairs (l : El) and rest holds the composed
expression E. The constants EmptyRow, Zero, and One represent the empty Row hi, the
minimal expression 0, and the universal expression 1. Value constructor LUnion forms Lunions for Set-expressions, i.e., unions appearing in L-contexts. Similarly, RInter forms
intersections appearing in R-contexts. Value constructor LInter represents L-intersections.
This constructor thus makes the invariant explicit that L-intersections must be of the form
X " M , i.e., a variable intersected with an M-expression. Similarly, constructor RPat forms
R-patterns Pat[E; M ] which abbreviate the R-union E " M [ :M . Value constructor Cond

is used to represent conditional expressions C

l=) E and C r=) E. The particular kind

111
is always apparent from the context. Finally, constructor Neg represents cofinite negations
of Set-constructors :fc1; : : : ; cng. We don't give the explicit form of constructor values,
conditions, and labels. Constructor values simply contain the name and the signature of a
constructor, and conditions are sets of atomic conditions of the form X " M .

A couple of remarks about the expression representation are in order. The actual
implementation uses a less verbose representation, where the constants EmptyRow, Zero, and
One are represented as special cases of the other constructors. L-unions and R-intersections
formed by LUnion and RInter are never empty, i.e., 0 and 1 are uniquely represented by
Zero and One. M-expressions are formed using the constructors Zero, One, Cons, Neg, and
LUnion. BANE uses a common datatype to represent expressions for all sorts. Expressions
for a particular sort only use a subset of all the value constructors given above. The
use of the common datatype is motivated by the need to avoid code blowup. There is
a good amount of code that can be shared among the sorts s; ft; t, and among the Rowsorts r(s); r(ft); r(t). If each sort is given a separate datatype, we end up with six distinct
mutually recursive datatypes and code for one sort cannot be applied to expressions of other
sorts. Since constructor arguments can refer to any other sort, an extra common datatype
that represents expressions of any sort is needed nevertheless. Although a clean separation
of sorts at the ML-typelevel would be desirable, ML's lack of mutually recursive modules
makes such an approach impractical.

For similar reasons, we do not use separate datatypes to represent L-compatible
and R-compatible expressions. It is important to note that the common representation of
mixed expressions is only visible internally in BANE. At the library interface, expressions
of different sorts are given distinct ML types. Thus an analysis written using BANE as a
library enjoys the benefits of ML-typechecking for avoiding programming errors related to
the confusion of distinct sorts.

The nodes of constraint graphs are now represented by mixed expression values of type me, with the restriction that Set-expressions do not contain L-unions and Rintersections at top-level. Constraints involving L-unions and R-intersections are readily
broken up into constraints without L-unions and R-intersections using the resolution rules
given earlier

E1 [ E2 `s E3 () E1 `s E3 ^ E2 `s E3
E1 `s E2 " E3 () E1 `s E2 ^ E1 `s E3

7.1.2 Edges
As outlined before, edges are represented as adjacency lists on variable nodes. The adjacency
lists are stored in the variable information varinfo associated with each variable node.

datatype varinfo = LU of -lb : meset, ub : meset""

The field lb contains a set of mixed expressions (type meset) representing lower
bounds on the variable, i.e., if E 2 lb of a variable X , then the edge E ! X is present.
Similarly, ub contains a set of mixed expressions representing upper bounds on the variable.

112

Each graph edge is represented uniquely as either a predecessor edge, or a successor
edge. Recall from the previous chapter that edges are either of the form E M;C\Gamma \Gamma \Gamma ! X where
E is a source, X M;C\Gamma \Gamma \Gamma ! E where E is a sink, or the constraint contains top-level variables on
both sides. (Recall that sources and sinks E are mixed expression of the form 0, 1, c(: : : ),
hl : EliA ffi K, or :fc1; : : : ; cng, where K is 0, 1, or hi.)

The first form is always represented by placing the expression C

l=) E " M

in the set of lower bounds of X . Similarly, the second form is represented by placing
C

r=) Pat[E; M ] in the set of upper bounds of X .

Recall that we associate an index (a unique integer) o(X ) with each variable X
thus creating a total order on the variables. The representation of edges where both sides
contain top-level variables depends on the respective order of the top-level variables. These

remaining edges are of the form hl : EliA ffi [Y] 1;C\Gamma \Gamma ! X , X 1;C\Gamma \Gamma ! hl : EliA ffi [Y], and X M;C\Gamma \Gamma \Gamma !
Y, where [Y] stands for Row-mask expressions involving variable Y (Section 5.4.4). The
resolution of Row-constraints in Section 5.4 guarantees that we can transform constraints

into inductive constraints, i.e., o(Y) ! o(X ) in the edges hl : EliA ffi [Y] 1;C\Gamma \Gamma ! X , and
X 1;C\Gamma \Gamma ! hl : EliA ffi [Y]. Thus, the first form is represented by adding C

l=) hl : E

liA ffi [Y] to

the lower bounds of X , and the second form is represented by adding C

r=) hl : E

liA ffi [Y]

to the upper bounds of X .

Finally, an edge X M;C\Gamma \Gamma \Gamma ! Y is represented by adding C

l=) X " M to the lower

bounds of Y, if o(X ) ! o(Y), and by adding C

r=) Pat[Y; M ] to the upper bounds of

X , if o(Y) ! o(X ). We call this representation of the edges the inductive form (IF) of the
graph. Inductive form graphs have the property that the top-level variables Y in any lower
or upper bound expression of variable X have indices strictly less than X . We thus call all
edges in an inductive graph inductive edges since they correspond to inductive constraints.

The order o(\Delta ) on variables used by BANE is simply the order in which the variables
are generated, and we call this order the gen order. The particular choice of o(\Delta ) influences
the number of edges in the closed graph. In practice, we have compared the gen order to
randomly generated orders and found that there was no significant difference. The gen order
however has some pleasing properties that we exploit to prove termination when introducing
fresh variables.

7.1.3 Alias Edges
In the subsequent sections it is convenient to use an additional kind of graph edge called
an alias edge. An alias edge X =\Gamma ! E is a directed edge from a variable X to an expression
E. An alias edge X =\Gamma ! E is inductive, if each top-level variable Y 2 TLV(E) has index
lower than o(X ). We only consider constraint graphs with inductive alias edges. An alias
edge abbreviates an equality between X and E that could be represented with two edges
E ! X and X ! E. Note that if the alias edge is inductive, so are the two constraints it
abbreviates. The advantage of alias edges however does not come from saving a single edge
in the graph. Instead, we maintain an invariant on X, that if X =\Gamma ! E is an inductive alias
edge in a constraint graph G, then there are no other alias edges X =\Gamma ! E0, no L-inductive
edges E0 ! X and no R-inductive edges X ! E0 in G. If there exists an alias edge X =\Gamma ! E

113
1
10
100
1000
10000
100000

1 10 100 1000 10000Bound size

FrequencyPercent Total Edges

Figure 7.1: Example frequency of bound sizes
in G, then we say that X is aliased in G.

This invariant is enforced by the varinfo datastructure on variables by adding a
variant Alias containing the aliased expression E.

datatype varinfo = LU of -lb : meset, ub : meset"" -- Alias of me

The advantage of alias edges comes from the possibility to compress alias paths.
An alias path X =\Gamma !\Lambda E is a sequence of alias edges and variables X1 : : : Xn, such that X = X1,
and Xi =\Gamma ! Xi+1, and Xn =\Gamma ! E. If G contains an alias path X =\Gamma !\Lambda E, then the unique alias
edge X =\Gamma ! X2 may be replaced by X =\Gamma ! E without changing the solutions of the constraints
represented by G. This path compression is equivalent to Tarjan's path compression in the
union-find algorithm.

For the moment we only consider non-recursive aliases that are L- and R-compatible.

Definition 7.1 An alias edge X =\Gamma ! E is non-recursive, if E is non-recursive in X . We
say that E is non-recursive in X if X does not appear in E, and for all aliased variables Y
in E with alias paths Y =\Gamma !\Lambda E0, E0 is non-recursive in X .

Assuming that X is not aliased in the current graph, and its lower bounds are
lb(X ) and its upper bounds are ub(X ), an alias edge X =\Gamma ! E can be introduced into
a graph by replacing the variable info associated with X by Alias E, and adding the
constraint L `s E for each L 2 lb(X ) and the constraint E `s U for each U 2 lb(X ).

7.1.4 Expression Hashing and Bound Representation
Edges in the constraint graph are represented as sets of mixed expressions in the upper
and lower bounds of variables. During constraint resolution, some of these sets grow to
substantial size. Even if we assume that the final graph is sparse, there can still be many
nodes whose lower or upper bounds have size proportional to the number of nodes in
the graph. Such cases do happen in practice, and it is thus crucial that operations for
membership test and element addition on these sets be constant amortized time. Figure 7.1

114
shows an example edge distribution for a constraint graph obtained with the points-to
analysis described in Section 8.1. The graph contains 59600 variables nodes and 209139
edges. The x-axis represents the size of a variable bound (number of expressions in lower
and upper-bounds). The frequency plot represents the number of variables in the graph
with a particular bound size. There are numerous variables with bound sizes larger than
100 and even larger than 1000 expressions. Note that there is one variable with a bound
size of 4500. The second plot (Percent Total Edges) shows that most edges appear on
variables with small bounds. A point (x; y) of the second plot states that y percent of the
total number of edges appear in bounds of variables with bound size less or equal to x.
For example, 12% of edges appear on variables with only one expression in their upper and
lower-bounds.

Since expressions themselves can be large, we do not want to structurally compare
them. Instead, we associate a unique integer id(E) with each mixed expression E built
in BANE. The association of expressions and their id's is stored as an extra field on the
expression itself. In order to generate the same id if the same expression is built twice,
BANE uses hash-consing, i.e., each syntactically distinct expression is represented at most
once in memory. Hash consing guarantees the unique mapping between expressions and their
id's, and furthermore supports maximal sharing of expressions, thereby reducing memory
requirements. The expression hash table is implemented using weak pointers, (pointers that
do not keep an object alive when otherwise unreachable) in order to reclaim storage of
expressions no longer in use. The hash table grows dynamically when full by doubling the
table size and rehashing.

The unique id associated with each expression E enables the use of hash-tables to
represent sets of expressions. A set is represented as a pair of a list and a small hash-table.
The list contains the elements in the set (no duplicates), and the hash-table contains the
unique ids of all expressions in the set. To test membership of E in the set, it suffices to
test whether id(E) is in the hash-table. Adding an expression E to a set reduces to testing
membership of E, and if not present, consing E onto the head of the list and adding id(E)
to the hash-table. The hash tables used for sets are initially very small (2 elements) and
grow dynamically by doubling the table size when full and rehashing.

The use of individual hash tables per bound instead of using a global hash table
representing the presence of edges in the constraint graph is motivated by the desire to
prune unreachable variables from constraint graphs efficiently (Section 7.6). Using local
hash tables, unreachable variables and their edges can be left to the garbage collector. In
the presence of a global hash table, edges incident on unreachable variables would have to
be removed individually.

7.1.5 Online Cycle Detection and Elimination
The performance of constraint resolution can be improved by simplifying the constraint
graph at various times during the resolution. Prior work on constraint graph simplification
has shown that periodic simplification performed during resolution helps to scale to larger
analysis problems [24, 30, 58], but absolute performance is still unsatisfactory. One problem
is deciding the frequency at which to perform simplifications to keep a well-balanced costbenefit tradeoff. Simplification frequencies in past approaches range from once for an entire

115
module to once for every program expression.

BANE contains a novel technique for a particular constraint graph simplification,
namely detecting and eliminating cycles. As we will show in the chapter on experiments,
cycles in the constraint graph are a key inhibitor for scaling to large Set-constraint problems.
This section describes the algorithm used in BANE for detecting and eliminating cycles in
constraints. The algorithm is applicable to all sorts.

Cycles in the constraint graph are sequences of edges X1 1;true\Gamma \Gamma \Gamma ! X2 1;true\Gamma \Gamma \Gamma ! \Delta  \Delta  \Delta  1;true\Gamma \Gamma \Gamma !
Xn where X1 = Xn. Such cycles imply that X1 = X2 = \Delta  \Delta  \Delta  = Xn\Gamma 1 in all solutions of the
constraints. In general, let V be a strongly connected component in the graph, i.e., V is a
set of variables such that for all pairs (X ; Y) of variables in V , there is a path X 1;true\Gamma \Gamma \Gamma !\Lambda Y
and a path Y 1;true\Gamma \Gamma \Gamma !\Lambda X in G. Then the variables of V are equal in all solutions. The idea is
to collapse the strongly connected component V to a single representative variable of V .

BANE takes the extreme approach to simplification frequency by performing cycle
detection and elimination online, i.e., at every update of the constraint graph. At first
glance, this approach seems overly expensive, since the best known algorithm for online
cycle detection performs a full depth-first search for half of all edge additions [77]. The
detection algorithm used by BANE is actually only partial and does not detect all cycles.
It is the partial aspect of it that makes it practical. Section 9.2 will show that our partial
detection costs only a small constant overhead per edge addition by only traversing a few
edges during the search. Nevertheless, many cycles are detected, boosting performance by
more than an order of magnitude for large constraint problems.

Note that it is not sufficient to use the well-known linear time algorithm to eliminate cycles in the initial unclosed constraint graph. The structural rules used during
constraint graph closure add new edges to the graph which may form cycles that are not
apparent in the initial constraint graph. Thus cycle detection needs to be performed during
the resolution.

We first show how to perform the detection of cycles, and then how cycles are
collapsed.

Partial Detection Algorithm
We would like to know whether adding an edge X 1;true\Gamma \Gamma \Gamma ! Y to a constraint graph G closes a
cycle. Thus we need to know whether G contains a path Y 1;true\Gamma \Gamma \Gamma !\Lambda X . This question may be
answered in several ways. If we maintained transitive edges between all variables, then we

could simply query the presence of edge Y 1;true\Gamma \Gamma \Gamma ! X . However, maintaining full transitive
closure is expensive to compute and requires quadratic space in practice. Another way to
answer the query is to perform a depth-first search on the graph, starting at Y, searching
for X , or doing a depth-first-search on the reverse graph starting at X and searching for
Y. Doing a full depth-first search however is still expensive, since on every edge addition a
very large part of the graph may be explored. What we would like is a way to restrict our
search by traversing only certain kinds of edges.

Fortunately, the inductive graph representation provides a very natural restriction.

Recall that each edge X 1;true\Gamma \Gamma \Gamma ! Y is stored as a lower bound on Y if o(X ) ! o(Y), or
as an upper bound on X if o(Y) ! o(X ). Thus, inductive form doesn't even allow a

116

U W
Z

\Delta  \Delta  \Delta 

\Delta  \Delta  \Delta 
X Y

Figure 7.2: Cycle detection in an example graph
standard depth-first search, since not all successors of a variable X are apparent in the
upper bounds of X . Only successors with lower index than X are present. Similarly, only
predecessors with lower index than X are present in its lower bounds. Performing a forward
depth-first search in the inductive graph using only the inductive edges corresponds to a
restricted forward depth-first search in the graph, where an edge X \Gamma ! Y is only traversed
if o(Y) ! o(X ). Similarly, performing a backward depth-first search in the inductive graph
using only inductive edges corresponds to a depth-first search in the reversed graph, where
again an edge X \Gamma ! Y is only traversed if o(Y) ! o(X ).

Let an inductive path from Y to X be a sequence of edges Z1 \Gamma ! Z2 \Gamma ! \Delta  \Delta  \Delta  Zn
such that Y = Z1, Zn = X , and the sequence of indices o(Z1); o(Z2); : : : ; o(Zn) is strictly
decreasing. Similarly, let a reverse inductive path from Y to X be a sequence of edges Z1 \Gamma !
Z2 \Gamma ! \Delta  \Delta  \Delta  Zn such that Y = Z1, Zn = X , and the sequence of indices o(Z1); o(Z2); : : : ; o(Zn)
is strictly increasing.

Algorithm 7.2 The observation on depth-first search in inductive graphs translates into
the following strategy for detecting cycles when adding an edge X \Gamma ! Y.

ffl If o(X ) ! o(Y) search for an inductive path Y\Gamma !\Lambda X along successor or alias edges,

starting from the upper bounds of Y. Prune the search whenever a variable Z is
reached with index lower than o(X ), since in that case there are no inductive paths
from Z to X .

ffl If o(Y) ! o(X ) search for a reverse inductive path Y\Gamma !\Lambda X , but search for the path

in reverse, i.e., starting from X , following predecessor or alias edges to lower indexed
variables. Prune the search whenever a variable Z is reached with index lower than
o(Y), since in that case there are no reverse inductive paths from Y to Z.

We illustrate the search algorithm using the example graph of Figure 7.2. The
edge X \Delta \Delta \Delta \Delta \Delta \Delta -Y is the edge we are about to add and which triggers the cycle detection. Plain
edges in the graph represent successor edges, i.e., edges Y\Gamma \Gamma \Gamma -Z where the index o(Y) is
larger than the index o(Z). Dotted edges represent predecessor edges, i.e., edges U \Delta \Delta \Delta \Delta \Delta \Delta -W
where the index o(U ) is smaller than o(W). The example assumes that the index o(X ) is
smaller than o(Y). We are thus in the first case of Algorithm 7.2, searching for an inductive
path Y\Gamma !\Lambda X along successor edges. In our notation, this path corresponds to a path using

117
X3
Xn\Gamma 1 X2

X1Xn

(1) (2)

X3
Xn\Gamma 1 X2

X1Xn

Figure 7.3: The two kinds of cycles detected
only plain arrows from Y to X . The search finds such a path through Z. Note that the
search also explores node U , since it is reachable along a plain edge from Y. However, node
W and it successors are not explored, since o(W) ? o(U ).

The above strategy only detects cycles of the two following kinds, illustrated in
Figure 7.3.

1. G contains an inductive path X1\Gamma !\Lambda Xn and we add edge Xn \Gamma ! X1. In this case

o(Xn) ! o(X1) and we find the inductive path by searching along successor edges
starting from X1.

2. G contains a reverse inductive path X1\Gamma !\Lambda Xn and we add edge Xn \Gamma ! X1. In this case

o(X1) ! o(Xn) and we find the reverse inductive path by searching along predecessor
edges starting from Xn.

Clearly cycles of length 2 are always detected. For larger cycles, this approach
seems overly restrictive, since the probability given a random index assignment of closing one
of the above cycles decreases exponentially in the size of the cycle. However, the inductive
transitive closure rule (ITCR) used to close the constraint graph adds some transitive edges
between variables. In fact, if X1::Xn form a cycle that has not been detected with the above
strategy, then ITCR adds the transitive edge Xj\Gamma 1 \Gamma ! Xj+1, through Xj, where Xj is the
variable with maximum index on the cycle. As a result, the cycle length is shortened by one
edge, giving cycle detection another chance. In the worst case, a cycle of length 2 will be
found between the two variables of minimum index on the cycle. Thus, inductive transitive
closure and the above cycle detection mechanism always finds a sub-cycle of every strongly
connected component in the graph.

We illustrate this mechanism for cycles of length 3. Every 3-cycle has the form
of one of the cycles in Figure 7.4. In the figure, the variable order assumed is given by
the variable indices. Variable X3 is thus the variable with maximum index on these cycles.
Consider the 3-cycle on the left of Figure 7.4. Since X2 `s X3 is an L-inductive constraint
and X3 `s X1 is an R-inductive constraint, inductive transitive closure adds the transitive
edge X2\Gamma \Gamma \Gamma -X1 to the graph. This edge addition generates a 2-cycle X1; X2 which is de118

X2X1

X3 X3

X1X2

Figure 7.4: Possible 3-cycles
tected. Similarly, inductive transitive closure adds the edge X1 \Delta \Delta \Delta \Delta \Delta \Delta -X2 to the right graph of
Figure 7.4, forming a 2-cycle that is detected.

Collapsing Cycles
Once a cycle X1::Xn is found, it can be collapsed to avoid performing redundant work on
the cycle's edges in the future. Collapsing the cycle requires choosing a representative Xr
among X1::Xn and introducing alias edges from the remaining variables on the cycle to the
representative. Furthermore, the lower bounds of each variable on the cycle must be added
as a lower bound constraint on Xr and similarly for the upper bounds.

The choice of the representative Xr for inductive graphs is given by the invariant
that alias edges must be inductive, i.e., X =\Gamma ! Xr implies that the index of X is larger than
o(Xr). Thus the representative must be the variable with minimum index on the detected
cycle.

For the cycles we detect with the above strategy, the representative is the lower
indexed variable of the edge Xn \Gamma ! X1 that triggered the detection. Furthermore, there is
an important optimization that we can do when adding the bounds of the variables on the
cycle to the representative. If X1::Xn is the cycle we detected while adding edge Xn \Gamma ! X1,
then either o(Xn) ! o(X1), in which case Xn is the representative and the lower bounds of
the variables on the cycle are already present on Xn or o(X1) ! o(Xn), in which case X1 is
the representative and the upper bounds of the variables on the cycle are already present
on X1. The reasoning is as follows:

ffl If o(Xn) ! o(X1) then there is an inductive path X1\Gamma !\Lambda Xn and Xn is the variable with

minimum index on the cycle. The inductive path implies that for any edge E M;C\Gamma \Gamma \Gamma ! Xj
of a source E to a variable Xj on the cycle, there is already an edge E M;C\Gamma \Gamma \Gamma ! Xn in
the graph (Section 6.2).

Similarly, for any edge Z M;C\Gamma \Gamma \Gamma ! Xj where o(Z) ! o(Xn) the transitive closure has
already added the edge Z M;C\Gamma \Gamma \Gamma ! Xn. For any edge Z M;C\Gamma \Gamma \Gamma ! Xj where o(Z) ! o(Xj),
but o(Z) ? o(Xn), there exists a variable Xi on the cycle with index larger or equal

to o(Xn) and index less than o(Xj), such that the edge Z M;C\Gamma \Gamma \Gamma ! Xi is R-inductive,
i.e., represented as an upper bound on Z. Since Xi is aliased to Xn after the cycle is

collapsed, it is not necessary to add the R-inductive edge Z M;C\Gamma \Gamma \Gamma ! Xn involving the
representative explicitly.

119
The above discussion establishes that when collapsing a cycle involving an inductive
path to the minimum indexed variable, only the upper bounds of the variables on the
cycle must be added to the representative. The lower bounds are already present.

ffl otherwise, o(X1) ! o(Xn) in which case there is a reverse inductive path X1\Gamma !\Lambda Xn and

X1 is the variable with minimum index on the cycle. Analogously to the above case
one can establish that all upper bounds of variables on the cycle are already added as
upper bound constraints on X1 and only lower bounds must be added to X1.

On Searching for Cycles
The choice of the minimum indexed variable as the representative for the cycle was given
by our invariant on inductive alias edges. Here we argue that this choice is actually crucial
to keep the cycle detection cost low. Consider for each variable X in a constraint graph G
the set of reachable successors and the set of reachable predecessors defined as follows. A
variable Y is a reachable successor of X if o(Y) ! o(X ) and there is a successor edge X \Gamma ! Y
or alias edge X =\Gamma ! Y in G, or there exists a variable Z with index o(Y) ! o(Z) ! o(X )
and an edge X \Gamma ! Z or alias edge X =\Gamma ! Z, and Y is a reachable successor of Z. Reachable
predecessors are defined analogously, i.e., a variable Y is a reachable predecessor of X if
o(Y) ! o(X ) and there is a predecessor edge Y \Gamma ! X or alias edge X =\Gamma ! Y in G, or there
exists a variable Z with index o(Y) ! o(Z) ! o(X ) and an edge Z \Gamma ! X or alias edge
X =\Gamma ! Z, and Y is a reachable predecessor of Z. The cycle detection cost is essentially
defined by the number of reachable successors and predecessors of each variable in the
graph. When adding an edge Y \Gamma ! X and X is the higher indexed variable, cycle detection
visits each variable Z of index equal to or higher than o(Y) in the reachable successors of X .
Similarly, when adding an edge X \Gamma ! Y and X is the higher indexed variable, cycle detection
visits each variable Z of index equal to or higher than o(Y) in the reachable predecessors
of a variable X .

Collapsing cycles to the minimum indexed variable on the cycle decreases the size
of the reachable predecessor or successor sets of the variables on the cycle. Suppose we
detect a cycle X1::Xn when adding edge Xn ! X1 and the index of X1 is less than the index
of Xn. Then variables X1::Xn form an anti-inductive path. We add alias edges from each Xi
to X1. Since X1 is in the predecessor set of every Xi on the cycle, the set of predecessors of
Xi is reduced to the set of predecessors of X1. Consider the predecessor edges that are added
to X1 as a result of collapsing the cycle. These edges are the predecessor edges Z \Gamma ! Xi of
variables Xi on the cycle where o(Z) ! o(X1). These edges are also added as predecessor
edges on X1 if we add the edge Xn \Gamma ! X1 instead of collapsing the cycle, since the inductive
transitive closure will add edges Xi \Gamma ! X1 for each Xi on the cycle.

Now consider the reachable successors of each Xi. After collapsing the cycle, the
reachable successors of each Xi are equal to the reachable successors of X1. We've already
seen that closing the graph under the edge Xn \Gamma ! X1 instead of collapsing the cycle adds
edges Xi \Gamma ! X1 for each Xi. Thus the set of reachable successors of each Xi includes X1 in
that case. Collapsing the cycle can only make the set smaller.

The argument for the case where we collapse a cycle X1::Xn and o(X1) ? o(Xn)
is analogous. Now consider what happens, if we collapse the cycle to the highest indexed

120
variable Xn instead of X1. Collapsing the cycle involves adding all successor edges of each
Xi to Xn. Since o(Xn) ? o(Xi), the reachable successors of Xn after collapsing the cycle is
the union of reachable successors of any Xi. This set is potentially very large. A similar
argument applies to the reachable predecessors when collapsing the dual kind of cycles to
the maximal index variable.

7.1.6 General Constraint Resolution Algorithm
This section briefly describes how constraints are transformed into inductive constraint
graphs. The algorithm given incorporates cycle detection and alias edges, topics not discussed in Chapters 5 or 6.

Algorithm 7.3 Given a constraint system S, let the constraint graph G be initially empty.
Apply the function resolve below to each constraint C =) E1 `s E2 in S.

resolve(C,E1,E2)

if E1 `s E2 is L-inductive (E2 = X2) then

if X2 is aliased to E02, then

resolve(C,E1,E02)
else if E1 is a variable X1 and C is true then

run cycle detection on X1 \Gamma ! X2
if there is a cycle Xi1 ::Xin then

for each Xj 6= X1 in Xi1 ::Xin do

add an alias edge Xj =\Gamma ! X1;
add upper-bounds of Xj to X1
endfor
else

update.lower.bound(C,X1,X2)
endif
else (E1 is a source)

update.lower.bound(C,E1,X2)
endif

else if E1 `s E2 is R-inductive (E1 = X1) then

if X1 is aliased to E01, then

resolve(C,E01,E2)
else if E2 is a variable X2 and C is true then

run cycle detection on X1 \Gamma ! X2
if there is a cycle Xi1 ::Xin then

for each Xj 6= X2 in Xi1 ::Xin do

add an alias edge Xj =\Gamma ! X2;
add lower-bounds of Xj to X2
endfor
else

update.upper.bound(C,X1,X2)
endif
else (E2 is a sink)

update.upper.bound(C,X1,E2)

121
endif
else (E1 `s E2 is not inductive)

apply a resolution rule of Chapter~5
if the constraint is inconsistent, stop.
else

E1 `s E2 is equivalent to a set of simpler
constraints C1 =) (E11 `s1 E21); : : : ; Cn =) (E1n `sn E2n)
for each i = 1::n do

resolve(C ^ Ci,Ei1,Ei2)
endfor

update.lower.bound(C,E,X )

if C

l=) E not in lb(X ) then

add C

l=) E to lb(X )

for each U in ub(X ) do

resolve(C,E,U )
endfor
endif

update.upper.bound(C,X ,E)

if C

r=) E not in ub(X ) then

add C

r=) E to ub(X )

for each L in lb(X ) do

resolve(C,L,E)
endfor
endif

Our restriction to inductive alias edges guarantees that the constraint E1 `s E2
that results from expanding an aliased variable X has top-level variables with indices strictly
lower than o(X ). Thus, if the constraint has any top-level variables left, further rewrite
steps must eventually transform all constraints involving top-level variables into inductive
constraints. For constraints without top-level variables either the constraint is trivially
satisfied, the constraint has no solution, or one of the structural resolution rules 5.7, 5.59,
5.21, or 5.27 is applied, which generate constraints on sub-expressions of E1 and E2. The
restriction to non-recursive aliases guarantees that any variables in the sub-expressions of
E1 and E2 can be expanded as well without generating an infinite expansion.

In practice, BANE implements a specialized resolve function for each sort. The
specific details of these functions are discussed in the following sections. The resolve
functions are mutually recursive due to the need to invoke resolution of constraints of other
sorts at sort interfaces. To avoid having to write the resolve functions as a set of mutually
recursive functions in BANE (which would hinder extension and modularity of the code),
constructor signatures contain a set of function pointers, akin to a dispatch table in objectoriented language implementations.

122
7.2 Set Sort
This section describes implementation features of BANE specific to the resolution of Setconstraints. The implementation of Set-constraint resolution corresponds closely to Algorithm 7.3 given above. Besides online cycle detection that is applicable to all sorts, BANE
contains another novel technique called projection merging that improves the scaling behavior of Set-constraints even further.

We first introduce projections and then describe the projection merging algorithm.

7.2.1 Projections
Projections come up frequently in program analysis problems where they correspond to
selecting a particular component of a data-structure.

A projection c\Gamma i(E) denotes the set Sf_[[Ai]]oe j c(A1; : : : ; Aa(c)) ` Eg. Projections are not primitive expressions in BANE, but can be expressed indirectly by a translation of a projection c\Gamma i(E) into a fresh variable X , along with the constraint E `s
Pat[c(1'1 ; : : : ; X ; : : : ; 1'n ); c(1'1 ; : : : ; 1'n )], where c : '1 \Delta  \Delta  \Delta  'n ! s and X appears in the
ith position. The notation 1'j stands for 1 if 'j = t, i.e., c is covariant in the jth argument,
and 0 if 'j = t, i.e., c is contravariant in the jth argument. If c is covariant in the projected
position (i), the constraint on the fresh variable X only requires X to be a superset of
the desired projection. In practice, this is not a limitation as long as X is only used in
L-contexts. To see this, note that if X appears only in L-contexts, then every constraint on
X is of the form X `s E, constraining X by an upper-bound. Thus, the lower-bounds on
X arise solely from the projection. Similarly, if c is contravariant in the projected position
(i), then the constraint on X only requires it to be a subset of the projection. Again, this
is not a limitation as long as X is used only in R-contexts.

The resolution of the Pat constraint proceeds by filtering out all sets of the form
c(E1; : : : ; En) from E and making them a subset of c(1'1; : : : ; X ; : : : ; 1'n).

E `s Pat[c(1'1 ; : : : ; X ; : : : ; 1'n); c(1'1 ; : : : ; 1'n )]
() E " c(1'1 ; : : : ; 1'n) `s c(1'1 ; : : : ; X ; : : : ; 1'n )

Given a constraint c(E1; : : : ; En) ` E, the following sequence of constraints is generated:

c(E1; : : : ; En) " c(1; : : : ; 1) `s c(1'1 ; : : : ; X ; : : : ; 1'n )
() c(E1; : : : ; En) `s c(1'1 ; : : : ; X ; : : : ; 1'n)
() Ei `'i X

The Pat formulation of projections imposes a fair amount of overhead. BANE thus introduces
an abbreviation for projection patterns of the form Pat[c(1; : : : ; E; : : : ; 1); c(1; : : : ; 1)] with
the syntax ProjPat(c; i; E), along with the resolution rule:

d(E1; : : : ; Ea(d)) ` ProjPat(c; i; E) () ae Ei `'j E if c = d0 `

s 1 otherwise

The resolution of the abbreviation avoids an intersection and only accesses the position that
is being projected.

123
E1
Ek
E2

U W

Z

c\Gamma i(W0)Y X

c\Gamma i(U0)

c\Gamma i(Z0)
c\Gamma i(Y0)

E1
Ek
E2

U W

Z

c\Gamma i(W0)Y X

c\Gamma i(U0)

c\Gamma i(Z0)
c\Gamma i(Y0)

Figure 7.5: Transitive edges to projection patterns
7.2.2 Projection Merging
As shown in the previous section, a projection c\Gamma i(E) is translated by BANE into a fresh
variable X , and the constraint E ` ProjPat(c; i; X ). Since ProjPat(c; i; X ) is a sink, it
may propagate backwards in the constraint graph along reverse inductive paths through
the inductive transitive closure rule (ITCR). As a result, large numbers of these projection
patterns may accumulate on a single variable. Together with large numbers of forward
propagated lower bounds, this accumulation can cause a quadratic work and space blowup.

Figure 7.5 depicts the backward propagation of projection patterns. The graphs
contain a number of sources E1::Ek, variables, and projection patterns. Projection patterns
ProjPat(c; i; E) are abbreviated in the graphs by c\Gamma i(E). The top graph depicts the situation
before transitive closure is applied. The relative order of the variables is again shown by
using dotted or plain edges. The bottom graph shows some of the edges added by the
inductive transitive closure rule. Note how the closure adds edges from variable X to
all projection patterns, since there are reverse inductive paths from X to all variables
with projection patterns in their upper bounds. On the other hand, the sources E1::Ek
are propagated forward along edge Y\Gamma \Gamma \Gamma -X . The bottom graph does not yet show the
transitive constraints through variable X that are now added between all sources E1::Ek
and all projection patterns. The apparent redundancy is that the projection of X w.r.t. c
and argument i is computed multiple times, once for each projection pattern.

124

The basic idea behind projection merging is that a set of projection constraints
fX `s ProjPat(c; i; E1); X `s ProjPat(c; i; E2); : : : ; X `s ProjPat(c; i; En)g
is satisfied if all constraints c\Gamma 1(X ) `'i Ej for j = 1::n are satisfied. Observe that for
any solution oe, the projection of a fixed variable w.r.t. a fixed constructor c and index i is
unique, and that we can replace the above system of constraints with the following system:

fX `s ProjPat(c; i; X 1); X 1 `'i E1; X 1 `'i E2; : : : ; X 1 `'n Eng
where X 1 is a fresh variable generated as a function of X . The number of constraints is
the same, but consider the transitive constraints that ensue. Given lower-bounds Y1 `
X ; : : : ; Yk ` X , the original system propagates the n projection patterns to all Yi where
o(Yi) ! o(X ). In the new constraint system, the only projection pattern propagated to
each Yi is ProjPat(c; i; X 1).

This transformation introduces fresh variables. Consequently, the termination of
the resolution algorithm must be proven with the addition of this new rule. We first need
to be more precise in the formulation of the new resolution rule. Assume that each variable
X is part of a family X s, where s is a sequence of constructor-index pairs.

S [ fX s ` ProjPat(c; i; E)g () S [ fX s ` ProjPat(c; i; X s(ci)); X s(ci) ` Eg (7.1)
This rule applies before any transitive rules so that ProjPat(c; i; E) is not propagated. Note
that since the rule generates the variable X s(ci) as a function of X , constructor c and index
i, there is only a single projection pattern per variable, constructor, index triple.

For the proof of termination, we restrict ourselves to the case where projection
expressions only contain variables ProjPat(c; i; Y). The following invariant relates the indices
of variables appearing in projection constraints:

Lemma 7.4 Let S0 be a system of initial constraints. All variables in S0 are assumed
to have indices ^ 0. Further assume that variables generated by Rule 7.1 are assigned
consecutive indices starting from 1. Let S be any system of constraints obtained from S0 by
applying some of the resolution rules. The relation (o(X ) ? 0.o(Y) ? 0) =) o(X ) ! o(Y)
holds for any constraint X ` ProjPat(c; i; Y) in S.

Proof: Note that the lemma holds for S0, since o(X ) ^ 0 for all X . We show that any
new constraints involving a projection maintain the invariant.

Rule 7.1 We have o(X s) ! o(X s(ci)) for the constraint X s ` ProjPat(c; i; X s(ci)) generated

from X s ` ProjPat(c; i; Y), since X s(ci) is generated after X .

Transitivity: From X ` Z ` ProjPat(c; i; Y) transitivity generates X ` ProjPat(c; i; Y)

whenever o(X ) ! o(Z). By assumption, (o(Z) ? 0 . o(Y) ? 0) =) o(Z) ! o(Y),
thus (o(X ) ? 0 . o(Y) ? 0) =) o(X ) ! o(Y).2

2 Say that Y is a parent of X s(ci) whenever Rule 7.1 is applied with E = Y. Let the
ancestor relation anc(X ; Ys(ci)) be the transitive closure of the parent relation.

125
Lemma 7.5 Given a constraint X ` ProjPat(c; i; Ys(ci)) it holds that o(X ) ! o(Z) for any
ancestor Z of Ys(ci) with o(Z) ? 0.

Proof: By induction on the length of the ancestor relation ancn.
Base: Z is a parent of Ys(ci): Then Ys ` ProjPat(c; i; Z), and o(Ys) ! o(Z) by Lemma 7.4.

If X 6= Ys, then we must have X ` Ys and also o(X ) ! o(Ys) for transitivity to apply.
In any case o(X ) ! o(Z).

Induction: Assume there exists an ancestor Z, such that ancn+1(Z; X ), and not ancn(Z; X ).

Then there exists a parent Wt(dj) of Ys(ci) such that ancn(Wt(dj); Z). The constraint
Ys ` ProjPat(c; i; Wt(dj)) must result from transitivity of Ys ` Wt and the constraint
Wt ` ProjPat(d; j; Wt(dj) ). This implies o(Ys) ! o(Wt) and by induction we have
o(Wt) ! o(Z) and thus o(Ys) ! o(Z). If X 6= Ys then X ` Ys and o(X ) ! o(Ys). In
either case, o(X ) ! o(Z).2

2 Now let V0 be the set of variables appearing in the original
finite constraint system S0, and let I be the set of initial generated variables X , i.e., the set
of variables fX j 9Y 2 V0; s.t. Y parent of X g. Since for each occurrence of an expression
ProjPat(c; i; Y) in the original constraints, there is exactly one variable X , such that Y is
a parent of X , the set I is finite. Also note that every generated variable has an ancestor
in I. Now consider any constraint X ` ProjPat(c; i; Z) obtained during resolution. From
Lemma 7.5, we know that the index of X is less than the index of Z and all ancestors of
Z, and thus there exists W 2 I, such that o(X ) ! o(W). But this implies that X 2 I [ V0.
Termination follows directly.

Theorem 7.6 (Termination of Projection Merging) The number of variables generated through projection merging during resolution is bounded by 2AjS0j \Delta  jCj, where C is the
set of constructors appearing in S0 and A is the maximum arity of any constructor in C.

Note that termination is also guaranteed in the case where the resolution of other sorts
introduces fresh Set-variables (for example Rule 5.13), since any fresh variable X will have
index o(X ) ? 0 and the only constraints the resolution of other sorts can add on X is
a variable constraint X `s Y or Y `s X . Then Lemma 7.4 still holds, since a constraint
X `s ProjPat(c; i; Z) on the fresh variable X can only be generated by applying the inductive
transitive closure to X `s Y and Y `s ProjPat(c; i; Z), in which case Y has higher index
than o(X ).

Section 9.3 shows the impact of projection merging on constraint resolution times.
For very large constraint problems, projection merging can improve resolution times by an
order of magnitude.

7.3 Term Sort
BANE's implementation of Term-constraints makes two simplifying assumptions.

ffl Solutions where a Term-variable must be ?t are not of interest and can be discarded.

126

ffl Conditions on conditional constraints are assumed to be eventually true in all solutions

and are discarded.

These two assumptions enable a faster constraint resolution algorithm which is essentially
the algorithm proposed by Steensgaard for conditional unification [79].

Note that the above assumptions retain soundness of the resolution, but discard
completeness in favor of efficiency. These assumptions represent only the current implementation of BANE and are not fundamental limitations of the mixed constraint approach. To
avoid unexpected effects due to the assumption on conditions, BANE resolves the condition
status of any conditional Term-constraint using the algorithm given in Section 6.2.2 and
issues a warning if the condition cannot be proven true in all solutions.

To take advantage of Steensgaard's algorithm, Term-constraints are not represented in inductive form. Instead, constraints of the form X `t E are always represented
as a successor edge in the graph, by adding E to the upper bound set of X (ub(X )). The
algorithm essentially assumes that each variable X is ? until proven otherwise by a lower
bound of the form c(: : : ). A constraint of the form c(E1; : : : ; En) `t X causes X to be
equal to c(E1; : : : ; En) by the second assumption above. The constraint resolution thus
replaces the varinfo of X with an alias edge to c(E1; : : : ; En) and adds the constraints
c(E1; : : : ; En) `t U for all U 2 ub(X ). An occurs check guarantees that the alias is nonrecursive.1

Figure 7.6 gives the pseudo-code for Term-constraint resolution. Note that the
only transitive constraints are generated when an alias edge is introduced. No lower bounds
are ever added to any variable. For a variable X , the algorithm traverses each successor
edge of X at most once, i.e., exactly when an alias edge for X is introduced.

The non-inductive representation defies the cycle detection strategy outlined earlier. If desired, inductive edges can be added to lower bounds solely for cycle detection. So
far, we have not encountered the need to add cycle elimination to Term-constraints.

Constraints of the form c(E1; : : : ; En) `t c(E01; : : : ; E0n) are simplified by generating equality constraints Ei ='i E0i for i = 1::n. The equality constraints between Ei and E0i
are implemented using Robinson's unification algorithm whenever possible [75]. Since Term
and FlowTerm expressions can be identified structurally, unification applies to them in all
cases. For Set-expressions, equality can be implemented using unification as long as both
sides are structurally equal up to variables. When the structure doesn't match, symmetric
constraints Ei `s E0i and E0i `s Ei are asserted.

7.4 FlowTerm Sort
BANE's implementation of FlowTerm-constraints makes similar simplifying assumptions as
for Term constraints.

ffl Solutions where a FlowTerm-variable must be ?ft or ?ft are deemed uninteresting and

are discarded.

1If recursive unification is used for the structural constraints and no mixed constructors of sort Term are
used, then recursive aliases can be permitted.

127
term-resolve(C,E1,E2)

if C not true in current constraints, issue warning endif
if E1 = X1 then

if X1 is aliased to E01 then

term-resolve(true,E01,E2)
else

add E2 to ub(X )
endif
else (E1 = c(E11; : : : ; E1n))

if E2 = X2 then

if X2 is aliased to E02 then

term-resolve(true,E1,E02)
else

add alias edge X2 =\Gamma ! E1
for each U 2 ub(X2) do

term-resolve(true,E1,U )
endif
endif
else (E2 = d(E21; : : : ; E2m))

if c 6= d stop, no solution
else

for each i = 1::n

s is sort of ith argument of c
dispatch-unify(E1i,E2i) to sort s
endfor
endif
endif
endif

Figure 7.6: Specialized Term resolution

.

128

ffl Conditions on conditional constraints are assumed to be true in all solutions and are

discarded.

Again, these assumptions present implementation choices and are not fundamental. As
for Term-constraints, BANE issues a warning for conditional FlowTerm-constraints whose
condition cannot be proven true.

The first assumption above (discarding ?ft and ?ft solutions) enables the use of
the following resolution rules:

c(E1; : : : ; En) `ft X () X = c(X c1; : : : ; X cn) ^ Ei `'i X ci
X `ft c(E1; : : : ; En) () X = c(X c1; : : : ; X cn) ^ X ci `'i Ei

where the variables X ci are fresh variables similar to the fresh variables introduced during
projection merging. These variables are uniquely determined by variable X , constructor c,
and index i. The first resolution rule above is justified by the fact that in all solutions where
X 6= ?ft, X must be of the form c(X c1; : : : ; X cn). The second rule is justified similarly.

The equality constraint on X can be represented using an alias edge as outlined in
Figure 7.7. Termination of the resolution using the specialized rules can be proven similarly
to the case of projection merging, by showing that only a finite number of fresh variables
are generated. Termination hinges crucially on the fact that we expand the aliasing of a
variable X only when an inductive constraint on X is found and not when X is merely
an upper or lower bound on some other higher indexed variable Y. Consider the example
constraint c(X ) `ft X which forces a naive algorithm into an infinite loop. The resolution
of this constraint proceeds as follows: (assuming no other initial constraints)

ffl The constraint is L-inductive, so the algorithm in Figure 7.7 generates the fresh variable X c1 and adds the alias edge X =\Gamma ! c(X c1).

ffl Then the constraint X `ft X c1 is generated. Since X c1 was generated after X , we

have o(X c1) ? o(X ) and the constraint is L-inductive. Thus the algorithm checks the
aliasing of X c1, which is not aliased and we add X to the lower bounds lb(X c1) of
X c1.

If the algorithm expanded the aliasing of the lower indexed variable X instead, the algorithm
would enter an infinite loop.

The intuitive termination argument is that whenever a variable X is found to be of
the form c(X c1; : : : ; X cn), the addition of the alias edge X =\Gamma ! c(X c1; : : : ; X cn) only causes
variables Y with lower index than X to be expanded as well, since all upper and lower
bounds of X in inductive form have lower indices.

The above algorithm is novel to the best of our knowledge. Similar previously
published constraint resolution algorithms based on the expansion of a variable X to
c(X c1; : : : ; X cn) are restricted to non-recursive constraints [42, 64].

7.5 Row Sorts
BANE implements only one kind of Row-variable for each of the three Row-sorts r(s),r(t),
and r(ft), namely closed Row-variables. Minimal and maximal Row-expressions can still be

129
flowterm-resolve(C,E1,E2)

if C not true in current constraints, issue warning endif
if E1 `ft E2 is L-inductive then

E2 is a variable X2
if X2 is aliased to E02 then

term-resolve(true,E1,E02)
else

if E1 = c(E11; : : : ; E1n) then

add alias edge X2 =\Gamma ! c(X c1; : : : ; X cn) and transitive constraints
for each i = 1::n

s is the sort of the ith argument of c
dispatch-resolve(true,E1i,X ci)
endfor
else

update.lb(true,E1,X )
endif
endif

else if E1 `ft E2 is R-inductive then

E1 is a variable X1
if X1 is aliased to E01 then

term-resolve(true,E01,E2)
else

if E2 = c(E21; : : : ; E2n) then

add alias edge X1 =\Gamma ! c(X c1; : : : ; X cn) and transitive constraints
for each i = 1::n

s is the sort of the ith argument of c
dispatch-resolve(true,X ci,E2i)
endfor
else

update.ub(true,X ,E2)
endif
endif

else (E1 = c(E11; : : : ; E1n) and E2 = d(E21; : : : ; E2m))

if c 6= d stop, no solution
else

for each i = 1::n

s is sort of ith argument of c
dispatch-resolve(true,E1i,E2i) to sort s
endfor
endif
endif

Figure 7.7: Specialized FlowTerm-resolution

130
used, but are always of the forms hl : EliA ffi 0 and hl : EliA ffi 1. Conditional constraints are
handled as for Term and FlowTerm-constraints.

Besides these assumptions, the resolution of Row-constraints is implemented using
the general resolution algorithm given above and the resolution rules described in Section 5.4. The only implementation optimization is the use of an alias edge when a variable
X is split into hl : XliAX 0. Note that the absence of minimal and maximal Row-variables
obviates the need to represent Row-masks.

Domain constraints are not currently implemented in BANE. With the absence
of minimal and maximal Row-variables, termination can be guaranteed if the original constraints observe the following invariant: If a Row-variable X appears in two composition
expressions hl : EliA ffi X and hl : E0liB ffi X in the original constraints, then A = B. This is
an idea of R'emy (for example [70]).

7.6 Polymorphic Analysis
BANE provides support for polymorphic constraint-based program analysis through polymorphic constrained mixed expressions. Polymorphic constrained expressions have the form

8X1::Xn:EnS
where X1::Xn are the quantified variables, E is the underlying mixed expression, and S is
the system of constraints of the polymorphic expression.

Polymorphic constrained expressions are the natural generalization of polymorphic
types. Polymorphic types arise in languages supporting parametric polymorphism. For
example, in ML one can write a function length that returns the length of lists containing
any type of elements. The type of such a length function is 8ff:ff list ! int, where ff is a
type variable standing for the type of elements of the list. The type of the length function is
polymorphic in the element type ff. One way to think about the type of the length function
is that it has all types o/ list ! int for any type o/ . The justification for the polymorphic
type of length is that the length function does not assume anything about the element
types. In other words, the type of the elements is not constrained. One thus refers to the
quantified variables of such polymorphic type as universally quantified variables.

In languages with parametric polymorphism and type inference, polymorphic types
are usually inferred for let-bound variables, i.e., variables x bound in expressions of the form

let x = e
in

e0
end

Here, the x is the let-bound program variable, e is the let-bound expression, and
e0 the let-body. The standard semantics of such let-expressions is that x is bound to the
value v which is the result of evaluating e. Occurrences of x within the let-body e0 then
refer to the value v. The let-expansion of the body e0 is the expression e0[e=x], i.e., e0 where
all occurrences of x are replaced with e (while avoiding name capture). In conventional
programming language semantics, the let-expansion is equivalent to the let-expression

131
provided that e has no side-effects. Type systems for such languages generally state that a
let-expression is well-typed if and only if the let-expansion is well-typed (assuming e has
no side effects).

The basic idea of polymorphic type inference is to infer that the let-expansion
of a let-expression is well-typed without repeatedly performing the type inference for the
let-bound expression e at all occurrences of x in e0. This is achieved by inferring the most
general polymorphic type 8:~ff:o/ for e and instantiating this type to possibly different types
o/i at each occurrence of x within e0. Instantiating a polymorphic type means replacing
the quantified type variables ~ff with actual types in the underlying type o/ . Coming back
to our example of the length function, instantiating the polymorphic type 8ff:ff list ! int
corresponds to selecting the appropriate type o/ list ! int among all types of length such
that o/ is the type of the elements to which length is actually applied in the particular
occurrence.

For improved accuracy, program analyses are sometimes performed on the letexpansion of a program. The let-expansion of a program may in principle be exponential
in size w.r.t. to the original program size, although in practice such blowups seem rare (at
least for standard type systems). The idea of polymorphic type systems can be carried
over to constraint-based program analysis. In this scenario, the analysis of the let-bound
expression e may yield a mixed expression E along with a constraint system S. Let V
be the set of variables in S that were generated as part of the analysis of e and are not
shared with the analysis of any other part of the program. Call V the local variables of
S. If we performed the same analysis on the let-expansion of the let-expression, we would
infer a mixed expression Ei and constraint system Si for every occurrence i of x in the
body e0. Furthermore, Ei; Si and Ej; Sj will differ in exactly the local variables only. Thus,
we can achieve the same effect if we generate the quantified expression 8V:EnS for the
analysis of the let-bound expression e and instantiate the quantified expression at each
occurrence of x in the body. In this case, instantiation refers to replacing each quantified
variable v 2 V with a fresh variable in E and S. Note that this step is only valid if the
program semantics are such that the let-expansion is actually equivalent to the original
let-expression. Furthermore, the constraint system S inferred for the let-bound expression
e must usually be consistent.

To support this style of polymorphic analysis, BANE provides the following features:

ffl Multiple constraint graphs and automatic tracking of local variables.
ffl Quantification and instantiation
ffl Constraint simplification
The next subsections deal with each of these aspects in turn.

7.6.1 Multiple Constraint Graphs
If a mixed expression E and associated constraints S are to be generalized it is convenient
to keep the constraints S separate from other constraints S0 that may arise in other parts of

132
an analysis. To guarantee that constraints S have a solution, they need to be solved before
quantification. Solving the constraints means building a closed inductive constraint graph
for S. In order to keep several systems S and S0 in solved form, BANE provides the ability
to maintain several distinct constraint graphs.

BANE's approach to multiple constraint graphs is imperative. BANE has the notion of a current constraint graph (CCG). Constraints are always added to the current
constraint graph. There are operations to create fresh empty constraint graphs and to
make a constraint graph current. The information associated with a constraint graph is a
set of variable-varinfo pairs, where the variable information associated with each variable
X contains the predecessor and successor edges of X or an alias edge.

Each variable node has a special field called the varinfo-cache. A variable X is
deemed current, if its varinfo-cache points to the varinfo data associated with X in the
current constraint graph. The varinfo-cache enables direct access to the edges of the CCG
through variable nodes without the need for a lookup operation. A constraint graph G
is made current by making each variable of G current, i.e., for each variable-varinfo pair
(X ; vi) in G the varinfo-cache of X is set to vi.

Each variable in a constraint graph G is either local or free w.r.t. to G. When a
fresh variable X is created, it is added to the current constraint graph and is then local to
the CCG. When a constraint E `s E0 is to be added to the current constraint graph, each
variable occurring within E or E0 that is not current must be added to the CCG as a free
variable with an initially empty varinfo structure. Thus the local variables of G are the
variables created while G is the current constraint graph.

7.6.2 Quantification and Instantiation
BANE provides a function to form a polymorphic constrained expression 8V:EnS from a
given mixed expression E and the current constraint graph. The quantified variables V are
the local variables of the CCG, and the constraints S are simply the constraints represented
by the edges of the CCG.

It is not always desirable to quantify all local variables, for example variables that
are later inspected to extract results from the analysis should not be quantified. Variables
to be excluded from quantification can be explicitly mentioned during the quantification
step.

Instantiation of a polymorphic constrained expression 8V:EnS proceeds by creating a substitution oe from V to a set of fresh variables V 0, adding the constraint oe(E) `s
oe(E0) to the current constraint for each constraint E `s E0 in S, and returning the instantiation oe(E). The set of fresh variables V 0 are local to the current constraint graph.

7.6.3 Simplification
Naively quantifying mixed expressions and their constraint graphs obtained from the analysis of let-bound expressions and instantiating the resulting polymorphic expressions at each
occurrence of the let-bound variable in the let-body as outlined at the beginning of this section is often not practical. The constraint graphs associated with polymorphic expressions

133
may be large, containing many quantified variables. Since each instantiation generates fresh
variables, it is fairly simple to obtain an exponential blowup of variables and graph edges.

This blowup is reduced by simplifying the set of constraints S extracted from
the current constraint graph when quantifying over the local variables. Simplification of a
polymorphic constraint system 8V:EnS w.r.t. the set of variables V involves computing a
hopefully smaller constraint system S0 that is equivalent to S w.r.t. the free variables of
8V:EnS, i.e., variables occurring in S or E but not in V . We say that S and S0 are equivalent
w.r.t. to a set of free variables F , if for any solution oe of S, there exists a solution oe0 for S0
that agrees with oe on F , and for any solution oe0 of S0 there exists a solution oe of S such
that oe and oe0 agree on F .

We make use of an alternative formulation of equivalent constraint systems. We
characterize a constraint system S not by its solutions, but by the local (or quantified)
variables and the possible consistent extensions of S. We say that S and S0 are equivalent
w.r.t. a set of local variables V , if for any set of constraints S00 such that no variable of V
occurs in S00, the constraints S [ S00 are consistent if and only if the constraints S0 [ S00 are
consistent.

Simplification of constraint systems has been studied in the past by several researchers, including the author [24, 29, 72, 7, 58]. BANE takes a less sophisticated approach
to constraint system simplification than for example Flanagan [29] or Pottier [72] in that no
entailment relation of constraint systems is used. BANE only prunes unreachable variables
and constraints, and performs minimization-maximization.

Reachability classifies each variable in the current constraint graph as unreachable,
L-reachable, R-reachable, or both L- and R-reachable. We represent these four choices by
two bits associated with each variable, an L-bit and an R-bit. The reachability of a variable
X in a polymorphic constrained expression 8V:EnS states in what contexts variable X may
appear in any extension of the constraints S with S00 not involving any variables of V .

If a variable X is unreachable in a consistent system S, then no future constraints
can further bound X . Thus, X has no influence on whether an extension S [S00 is consistent
or not. Thus, S is equivalent to S0 where X and all edges involving X have been deleted.

If variable X is L-reachable, but not R-reachable, then future constraints can
only add upper bounds on X . In this case X may be minimized without affecting the
consistency of any extensions S [ S00. Similarly, if X is R-reachable, but not L-reachable,
future constraints can only lower bound X . In this case X may be maximized without
affecting the consistency of any extensions S [ S00.

The following algorithm computes the reachability of variables for a polymorphic
constrained expression 8V:EnS obtained from a constraint graph G with local variables V
and expression E.

Algorithm 7.7 Let L-mark and R-mark be the two procedures defined below. Initially,
mark the L- and R-bit of each free variable and call L-mark and R-mark on E. Then apply
the following steps until no more variables can be marked.

ffl If X is L-reachable, then call L-mark on each lower bound L in lb(X ) or on E if X is

aliased to E.

134

ffl If X is R-reachable, then call R-mark on each upper bound R in ub(X ) or on E if X

is aliased to E.

where

L-mark(E)

case E of

X =? mark L-bit of X
-- c(E1; : : : ; En) =?

for each i = 1::n do

if c is a Term-constructor then

call L-mark(Ei) and R-mark(Ei)
else

if c is covariant in i then

call L-mark(Ei)
else (c contravariant in i)

call R-mark(Ei)
endif
endif
endfor
-- hl : EliA ffi E =?

call L-mark(E);
for each l 2 A do

call L-mark(El)
endfor
-- X " M =? call L-mark(X )
-- Pat[E; M ] =? call L-mark(E)
-- . =? ;
endcase

and R-mark is the dual of L-mark.
If we know that each instantiation E0 of the polymorphic constrained expression 8V:EnS
generated from G will only appear on the left of a constraint, then we don't need to call
R-mark on E, and analogously if each instantiation only appears on the right.

The marking correctly captures the reachability of variables in the constraint graph
since L-mark(E) correctly classifies the reachability of variables during the resolution of a
constraint E `s E0 using the structural resolution rules, and similarly for R-mark. Furthermore, the repeat steps correspond to applying the inductive transitive closure rule.

To minimize a variable X , add an alias edge X =\Gamma ! F lb(X ) from X to the union
of its lower bounds, provided this union is expressible in the sort of X and F lb(X ) is
not recursive in X . Minimization is always possible for Set-variables. Term variables never
have lower bounds, so they can be minimized to 0. FlowTerm and Row-variables can be
minimized only if their lower bound is empty or contains a single expression E. In the first
case the variable is aliased to 0, in the second case to E.

Similarly, to maximize a variable X , add an alias edge X =\Gamma ! d ub(X ) from X to
the intersection of its upper bounds, provided this intersection is expressible in the sort of
X and d ub(X ) is not recursive in X . Maximization is always possible for Set-variables.
Term, FlowTerm and Row-variables can be maximized only if their upper bound is empty

135
or contains a single expression E. In the first case the variable is aliased to 1, in the second
case to E.

136

Chapter 8
Example Analyses

This chapter describes two example program analyses expressed using mixed constraints and implemented using the BANE library. The first analysis is a points-to analysis for C based on Andersen's algorithm [8]. It uses inclusion constraints between Setexpressions and Row-expressions. The second analysis is an exception inference for ML
developed by the author [25, 26]. We study three distinct versions of this analysis. The
versions differ in their use of particular sorts and constraints. Experiments involving these
analyses are used in the next chapter to evaluate the scaling ability of BANE and the impact
of particular implementation techniques.

8.1 Points-to Analysis for C
Points-to analysis for C computes for each expression in a program a set of abstract memory
locations (variables and heap) that the expression may evaluate to. From this information,
a points-to graph can be derived. Graph nodes represent abstract memory locations and
edges represent points-to relations. More precisely, an edge from a location l1 to a location
l2 in the points-to graph states that the abstract location l1 may at some point during
evaluation of the program contain a pointer to abstract location l2. Abstract locations refer
to program variables or syntactic occurrences of applications of memory allocation functions (for example malloc). Figure 8.1 shows the points-to graph computed by Andersen's
analysis for a simple C program.

Points-to analysis has been well studied (for example [88, 15, 53, 44, 23, 79]). Here
we examine the particular points-to analysis proposed by Andersen [8], which is based on
Set-constraints. Our interest in this study is not primarily the precision of the points-to
information computed, but the execution time required to compute the information. Past
implementations of points-to analyses based on Set-constraints suggest that the approach
does not scale to large programs. As we will show through extensive measurements in
Chapter 9, points-to analysis derived by solving a system of set constraints is practical even
for very large programs.

Andersen's points-to analysis is formulated as a non-standard type inference system, based on a collection of constants representing abstract locations fl1; ::; lng and a
collection of Set-variables, one per location fX1; ::; Xng. Set-constraints are used to model

137
a = &b;
a = &c;
*a = &d;

a

b

d
c

iiii1PPP

Pq

PPPPq
iiii1

Figure 8.1: Example points-to graph
the flow of abstract locations through the program. The analysis infers a non-standard type
from the following language for each expression e in the program.

E ::= l j Xl j *E
A type is either a location l, a type variable Xl associated with a location l, or a dereference type *E. Type variables Xl denote the set of locations pointed-to by location l. A
dereference type *E refers to the set of locations SfXl j l 2 Eg, i.e., to locations l0, such
that there exists l in the set of locations of E, and l points to l0 (or l0 2 Xl). The type of an
expression denotes the set of locations that the expression may evaluate to. The minimal
solution of the example in Figure 8.1 using Andersen's formulation is

Xla = flb; lcg
Xlb = fldg
Xlc = fldg

Andersen's analysis is a global context and flow -insensitive analysis. The analysis
is global in that it requires the entire program to derive the pointer relationships. Context
insensitive means that the effect of a function call on the points-to relation does not depend
on the particular call-site. In other words, the effect of all calls to a particular function
are merged. Context insensitivity is generally faster than context-sensitive approaches but
may result in less precise information, since information from one call site flows back to all
other call-sites of the same function. Flow-insensitive means that the analysis treats the
statements of a C program as an unordered collection, evaluated in no particular order.

The crux of any points-to formulation using Set-constraints is in the handling of
indirect assignments of the form

*e1 = e2;

Andersen's formulation uses a non-standard Set-expression *E to refer to the locations
pointed to by E. The above statement is typed in Andersen's formulation using the following
type rule:

e1 : E1
e2 : E2
E2 ` *E1

*e1 = e2 : E2
where the constraint E2 ` *E1 expresses the intuitive meaning of assignment, that any
location e2 should be in the points-to set of each location pointed to by e1. The presence of

138
the non-standard Set-expression *E forces Andersen's algorithm for solving the constraints
to be specialized to this application. Andersen uses thus a non-standard resolution rule
associated with *E for closing constraint systems:

E1 ` \Lambda E2 ^ l ` E2 =) E1 ` Xl
The rule should be read as follows. If E1 ` *E2 and l ` E2 are constraints in S, then add
the constraint E1 ` Xl to S. As is apparent in this rule, Andersen's formulation contains
an implicit association between each location l and the Set-variable Xl that represents the
points-to set of l. It is this implicit association that results in a specialized resolution
algorithm for Andersen's points-to analysis. To express Andersen's analysis in BANE, we
first show how to reformulate the types and constraints such that BANE's generic Setconstraint resolution can be applied. Section 8.1.2 then describes the constraint generation.
Finally, Section 8.1.3 illustrates the points-to analysis with a complete example.

8.1.1 Re-Formulation using Standard Set Constraints
The association between a location li and its points-to set Xli is implicit in Andersen's
formulation and results in an ad-hoc resolution algorithm. We use a different formulation
that makes this association explicit and enables the use of BANE's generic Set-constraint
solver. We model locations by pairing location names and points-to set variables with a
constructor ref(li; Xli ) akin to reference types in languages like ML [61].

Unlike the type system of ML, which is equality-based, Andersen's points-to analysis uses inclusion constraints. It is well known that subtyping of references is unsound
in the presence of update operations (e.g., Java arrays [35]). A sound approach is to turn
inclusions between references into equality for their contents: ref(L1; X ) ` ref(L2; Y) ,
L1 ` L2 ^ X = Y.

We adapt this technique to a purely inclusion-based system using a novel approach.1 We intuitively treat a reference lx as an object with a location name and two
methods get : void ! Xlx and set : Xlx ! void, where the points-to set of the location
acts both as the range of the get function and the domain of the set function. Updating a
location corresponds to applying the set function to the new value. Dereferencing a location
corresponds to applying the get function.

Translating this intuition, we make ref a ternary constructor. The first argument
captures a set of location names, the second argument corresponds to the get function (represented by the range), and the third argument corresponds to the set function, represented
by its domain. Since functions are contravariant in the domain, ref is contravariant in
this third argument. Our location constructor is thus a pure Set-constructor with three
arguments having the signature

ref : s s s ! s

A location for a variable x is then represented by an expression ref(lx; Xlx ; Xlx ) (to improve
readability we overline contravariant arguments). We now show how to dereference and
update these locations using constraints. Dereferencing an expression E representing a set

1It has recently come to our attention that this idea has also been suggested to Pottier by Cardelli and
independently by Trifonov and Smith as described in Pottier's dissertation [72], page 154.

139
of locations is equivalent to computing the projection of the second argument to the ref
constructor. We have already seen how to model projection in BANE (Section 7.2.1) by
generating a fresh variable T (in this section we use T for temporary variables that are not
associated directly with abstract locations) and adding the constraint

E `s Pat[ref(1; T ; 0); ref(1; 1; 0)]
The constraint makes T an upper bound on all locations pointed to by E. As explained in
Section 7.2.1, BANE provides a specialized abbreviation for this common pattern, called a
projection pattern. We can thus use the equivalent constraint

E `s ProjPat(ref; 2; T )
Updating the points-to set of each location represented by an expression E with a set
of locations represented by E0 is similar to a dereference, but involves the contravariant
argument of the ref constructor. It suffices to assert the constraint

E `s Pat[ref(1; 1; E0); ref(1; 1; 0)]
To illustrate how the above constraint adds E0 to the points-to set of any location in E,
consider the following example. Suppose E = T and ref(lx; Xlx ; Xlx ) `s E. Then the
transitive constraint ref(lx; Xlx ; Xlx ) `s ref(1; 1; E0) generated through T and the above
constraints is equivalent to E0 `s Xlx (due to contravariance), which is the desired effect.
As above for dereference, we can express an equivalent constraint with a projection pattern

E `s ProjPat(ref; 3; E0)
To extract the points-to set as a set of location names of a location ref(lx; Xlx ; Xlx )
it suffices to project the first field of all ref constructors of Xlx. Let Ylx stand for the location
names in the points-to set of location lx. Add the constraint

Xlx `s ProjPat(ref; 1; Ylx )
The location names in the points-to set of x is the minimal solution of variable Ylx which
coincides with the transitive lower bound TLB(Ylx ) in this case, since the solution only
contains location constants.

8.1.2 Constraint Generation
Types and constraints are generated by applying the type rules in Figure 8.2 to the abstract
syntax tree of a C program. The rules assign a type (Set-expression) to each program
expression and generate a system of Set-constraints as side conditions. C has the notion of land r-values, where an l-value is a location that can be updated, whereas r-values are values
that cannot be updated. Quantities with l-values (such as variables) are automatically
converted to r-values if the context requires it. For example the assignment y = x; of a
variable x to a variable y implicitly converts the location of x (an l-value) to the value stored
in location x (an r-value) whereas y is not converted, since it is the location y that must

140

x : ref(lx; Xlx ; Xlx ) (Var)

e : E
&e : ref(0; E; 1) (Addr)

e : E E `s ProjPat(ref; 2; T ) T fresh

\Lambda e : T (Deref)

e1 : E1 e2 : E2
E2 `s ProjPat(ref; 2; T ) E1 `s ProjPat(ref; 3; T )

T fresh
e1=e2 : E2

(Asst)

f : ref(lf; Xlf ; 0) xi : ref(lxi ; Xlxi ; Xlxi )

lam(Af; Rf) `s Xlf
Af `r(s) hai : Xlxi ii=1::n ffi 1

Af; Rf fresh
f(x1,: : : ,xn) f...g : 0

(Fun)

e : E E `s ProjPat(ref; 2; T )

T `s Rf

T fresh
return(e) : 0

(Ret)

e0 : E0 ei : Ei
Ei `s ProjPat(ref; 2; Ti) i = 0::n
T0 `s ProjPat(lam; 2; T 0) T0 `s ProjPat(lam; 1; A)

hai : Tiii=1::n ffi 0 `r(s) A

Ti; T 0; A fresh
e0(e1,: : : ,en) : ref(0; T 0; 1)

(App)

Figure 8.2: Constraint generation for Andersen's analysis
be updated. To avoid separate rules for l- and r-values the type rules in Figure 8.2 lift all
values to l-values and infer types for l-values only. Where necessary, the types of l-values
are converted explicitly to the types of the corresponding r-values by adding dereference
constraints. The (Var) rule for example gives x the type ref(lx; Xlx ; Xlx ) which is the type
of the location of x (and thus the type of the l-value of x).

We now describe the remaining rules in Figure 8.2. Rule (Addr) for typing the
address-of operator (Addr) creates a reference to the type of its operand E by nesting E
inside a ref constructor. Note that an expression &e can never be used as an l-value in
C, but we nevertheless lift its type. Since the resulting value can only appear in a context
requiring an r-value, the type ref(0; E; 1) will be explicitly converted back to the type of
the corresponding r-value, namely E. Thus the ref type assigned in (Addr) has no location

141
name and the contravariant argument is 1, since these fields will be ignored by the l-to-r
conversion step. The type rule (Deref) for the dereferencing operator does the opposite,
removing a ref constructor by projecting the covariant points-to set argument of E through
a projection constraint as discussed above. The first constraint in the assignment rule (Asst)
transforms the type E2 of the right-hand l-value e to the type T of its corresponding r-value
as in (Deref). The second constraint E1 `s ProjPat(ref; 3; T ) is the assignment constraint
described above, which makes T a subset of the points-to set of E1 and expresses exactly
the intuitive meaning of assignment: the points-to set E1 of the left-hand side contains at
least the points-to set E2 of the right-hand side.

C functions are given types formed by the binary mixed constructor lam (for
lambda) with signature

lam : r(s) s ! s

which is analogous to the standard function type constructor \Delta  ! \Delta . The first argument of
lam is contravariant of sort Set-Row and stands for the domain of a function, in this case
a record of the formal parameters, where the ith argument is labeled by ai. The second
argument of lam is covariant and represents the range or return value of a function. Since
C allows function pointers, we also lift all functions to l-values. Rule (Fun) describes how
a function declaration is typed. The type for the l-value of the function f is a location
ref(lf; Xlf ; 0) labeled by lf with points-to set Xlf . The location of f cannot be updated,
so the third argument is 0. The location of f only points to the function f and thus the
points-to set Xlf only contains the set lam(Af; Rf) which is expressed with the constraint
lam(Af; Rf) `s Xlf. lam(Af; Rf) is the function type of f, Af is a Row-variable standing
for the sequence of formal parameters of f and Rf is the type of the r-value returned by f.
The Row-constraint Af `r(s) hai : Xlxi ii=1::n ffi 1 models the flow of actual argument values

to the formal parameters. The l-types of the formal parameters xi are ref(lxi ; Xlxi ; Xlxi ).
Thus any lower bound on the domain Af of f's function type will end up as a lower bound
on hai : Xlxi ii=1::n ffi 1 and consequently, each individual argument flows into the points-to
set Xlxi of the corresponding formal parameter. The use of a maximal Row(ffi1) serves the
purpose of ignoring any extraneous arguments that might be passed to f.2 Note that the
types of the formal arguments are updatable locations, since in the body of function f, the
formal parameters are assignable like any other variable. The correspondence between the
function return type Rf and the body of f is dealt with in the (Ret) rule for typing return
statements. The rule assumes that the return statement appears in the body of function
f. The type E of the l-value of e is converted to the type of the corresponding r-value T
using the now familiar projection constraint. The constraint T `s Rf then constrains the
return set Rf to contain T . Since the rule is applied to all return statements in the body
of f, the return set Rf contains the union of all possible return values.

Finally, consider the rule for function application. Here e0 is the function expression, i.e., an expression whose L-value is a location pointing to a function.3 The l-values of

2We have found numerous C programs containing function applications that pass more arguments than
the function expects.

3C actually contains implicit dereferencing for function pointers appearing in the function position of an

application. Here we take the opposite approach, lifting constant functions to l-values and always performing
the dereference at the application point.

142
(1) f(r) - return(r); ""
(2) g(p,q,h) - *p = (*h)(q); ""
(3) a = &b;
(4) a = &c;
(5) g(a, &d, &f);

Figure 8.3: More complex C example

a
p

b
c

r
d
q
h f-Q
QQ

Qsjjj

j3

-

PPPPq
iiii1

?

6

Figure 8.4: Points-to graph of program in Figure 8.3
the actual arguments ei have types Ei. The first set of constraints in the rule converts the
types of the l-values of e0::en to the types Ti of the corresponding r-values. The constraint
T0 `s ProjPat(lam; 2; T 0) projects the return set of the function type T0 into T 0 and the
constraint T0 `s ProjPat(lam; 1; A) makes the fresh Row-variable A a lower bound on the
domain of the function (recall that lam is contravariant in the first argument). The final
constraint hai : Tiii=1::n ffi 0 `r(s) A models the flow of the actual arguments Ti to the domain of the function (through variable A). We use a minimal Row-expression (ffi0) to avoid
inconsistent constraints when functions are applied to fewer arguments than expected. The
final type of the function application is T 0 lifted to an l-value as in the rule (Addr).

8.1.3 A Complete Points-to Example
To illustrate the type and constraint generation rules, consider Figure 8.3 which contains an
example C program. The function pointers and indirect assignments generate the points-to
graph of Figure 8.4 which contains the points-to graph of the simple example of Figure 8.1
as a sub-graph. Line (1) declares an identity function f. Line (2) declares function g with
three arguments p, q, and h. The body of g applies h (which is thus a function parameter)
to q and assigns the result indirectly through p. Lines (3) and (4) assign the address of
variables b and c to a. Line (5) calls g, passing a as p, the address of variable d as q, and
the identity function f as h.

143
We now give a detailed account of the type judgments and constraints generated
for this example. The type judgments and constraints generated for line (1) in Figure 8.3
are

f : ref(lf; Xlf ; 0) r : ref(lr; Xlr ; Xlr )
lam(Af; Rf) `s Xlf Af `r(s) ha1 : Xlr i ffi 1 (8.1)
ref(lr; Xlr ; Xlr ) `s ProjPat(ref; 2; T1) T1 `s Rf

The constraints on the second line arise from applying Rule (Fun) and the constraints on
the last line arise from applying Rule (Ret). Note that the second to last constraint implies
Xlr `s T1, which together with the last constraint implies

Xlr `s Rf (8.2)
stating that the return set of f contains whatever r points-to. The type judgments and
constraints for line (2) are as follows. Applying Rule (Fun) to g generates

g : ref(lg; Xlg ; 0) p : ref(lp; Xlp ; Xlp )
q : ref(lq; Xlq ; Xlq ) h : ref(lh; Xlh ; Xlh )
lam(Ag; Rg) `s Xlg Ag `r(s) ha1 : Xlp ; a2 : Xlq ; a3 : Xlhi ffi 1 (8.3)

Applying (Deref) and (App) to (*h)(q) generates

ref(lh; Xlh ; Xlh ) `s ProjPat(ref; 2; T2) T2 `s ProjPat(ref; 2; T3) (8.4)
ref(lq; Xlq ; Xlq ) `s ProjPat(ref; 2; T4)

T3 `s ProjPat(lam; 1; A1) ha1 : T4i ffi 0 `r(s) A1 (8.5)
T3 `s ProjPat(lam; 2; T 01 ) (*h)(q) : ref(0; T 01 ; 1) (8.6)

which imply among other constraints

Xlh `s T2 (8.7)
Xlq `s T4 (8.8)

Applying (Deref) to *p and applying (Asst) to the entire assignment we obtain

ref(lp; Xlp ; Xlp ) `s ProjPat(ref; 2; T5) *p : T5

ref(0; T 01 ; 1) `s ProjPat(ref; 2; T6) T5 `s ProjPat(ref; 3; T6)

which imply

Xlp `s ProjPat(ref; 3; T6) (8.9)

T 01 `s T6 (8.10)

The type judgments and constraints for line (3) are

b : ref(lb; Xlb ; Xlb ) &b : ref(0; ref(lb; Xlb ; Xlb ); 1)
a : ref(la; Xla ; Xla ) ref(0; ref(lb; Xlb ; Xlb ); 1) `s ProjPat(ref; 2; T7)

ref(la; Xla ; Xla ) `s ProjPat(ref; 3; T7)

144
which imply that ref(lb; Xlb ; Xlb ) `s T7 `s Xla by contravariance of the third argument of
ref. The constraints generated for line (4) are similar and we thus have

ref(lb; Xlb ; Xlb ) `s Xla
ref(lc; Xlc ; Xlc ) `s Xla (8.11)

Finally, line (5) generates the following type judgments and constraints.

g : ref(lg; Xlg ; 0) a : ref(la; Xla ; Xla )
d : ref(ld; Xld ; Xld ) &d : ref(0; ref(ld; Xld ; Xld ); 1)
f : ref(lf; Xlf ; 0) &f : ref(0; ref(lf; Xlf ; 1); 0)
ref(la; Xla ; Xla ) `s ProjPat(ref; 2; T8)
ref(0; ref(ld; Xld ; Xld ); 1) `s ProjPat(ref; 2; T9)

ref(0; ref(lf; Xlf ; 1); 0) `s ProjPat(ref; 2; T10)

ref(lg; Xlg ; 0) `s ProjPat(ref; 2; T11) T11 `s ProjPat(lam; 1; A2) (8.12)
ha1 : T8; a2 : T9; a3 : T10i ffi 0 `r(s) A2 (8.13)

These constraints imply

Xla `s T8 (8.14)
ref(ld; Xld ; Xld ) `s T9 (8.15)

ref(lf; Xlf ; 0) `s T10 (8.16)

Xlg `s T11 (8.17)

By (8.3), (8.12), and (8.17) we obtain lam(Ag; Rg) `s ProjPat(lam; 1; A2) and thus A2 `r(s)
Ag by contravariance of the first argument of lam. Then by (8.13) and (8.3) we obtain

T8 `s Xlp (8.18)
T9 `s Xlq (8.19)
T10 `s Xlh (8.20)

connecting up the actual arguments with the formal parameters of g. By (8.16), (8.20) we
obtain

ref(lf; Xlf ; 0) `s Xlh (8.21)
stating that h points to function f. Furthermore, by (8.7), and (8.4), we have ref(lf; Xlf ; 0) `s
ProjPat(ref; 2; T3) and thus Xlf `s T3. By (8.1), (8.5), and (8.6) we obtain lam(Af; Rf) `s
ProjPat(lam; 1; A1) connecting the argument Row, and lam(Af; Rf) `s ProjPat(lam; 2; T 01 )
connecting the result set, which are equivalent to

A1 `r(s) Af (8.22)
Rf `s T 01 (8.23)

145
By (8.22), (8.5), and (8.1) we obtain ha1 : T4i ffi 0 `r(s) ha1 : Xlr i ffi 1 and thus T4 `s Xlr ,
which together with (8.8) implies

Xlq `s Xlr (8.24)
stating that r points-to whatever q points to. By (8.2), (8.23), and (8.10) we also have

Xlq `s T6 (8.25)
Variable T6 is used in the assignment of *p = (*h)(q); and we now follow argument a in
the call to g. By (8.11), (8.14), and (8.18) we have

ref(lb; Xlb ; Xlb ) `s Xlp
ref(lc; Xlc ; Xlc ) `s Xlp (8.26)

Furthermore by (8.9), we obtain ref(lb; Xlb ; Xlb ) `s ProjPat(ref; 3; T6) and ref(lc; Xlc ; Xlc ) `s
ProjPat(ref; 3; T6) which are equivalent to T6 `s Xlb and T6 `s Xlc. But then by (8.25) we
obtain

Xlq `s Xlb
Xlq `s Xlc (8.27)

which establishes that b and c point-to whatever q points to. It remains to track the actual
argument &d to formal parameter q. By (8.15) and (8.19) we obtain

ref(ld; Xld ; Xld ) `s Xlq (8.28)
and thus

ref(ld; Xld ; Xld ) `s Xlr
ref(ld; Xld ; Xld ) `s Xlb
ref(ld; Xld ; Xld ) `s Xlc

(8.29)

The final points-to graph is directly read off relations (8.11), (8.26), (8.28), (8.29), and (8.21).
Note the edge h ! f in the resulting points-to graph of Figure 8.4 showing that function
parameter h may point to function f. This formulation of points-to analysis thus performs
a control-flow analysis simultaneously with the pointer analysis.

8.2 ML Exception Inference
This section describes an exception inference for ML developed by the author. The standard
ML type system gives no information about the set of exceptions that an expression may
raise. Knowing only the types, a programmer must assume that each expression e has the
worst possible effect: Every imaginable exception may be raised during evaluation of e. The
exception inference described here gives the programmer more precise information about
possible exceptions. We present our analysis for Mini-ML but discuss implementation issues
for full SML.

146

The syntax of Mini-ML is a typed lambda calculus with exceptions, raise and
handle expressions, pairs, and case expressions with pattern matching.

e ::= x j c j d(e) j (e1; e2) j fn x =? e j e1 e2 j

e0 handle x =? e1 j raise e j case e0 of p =? e1 [] x =? e2 j
let x = e1 in e2 end
p ::= x j c j d(p) j (p1; p2)

The language has a standard call-by-value semantics which we outline informally. Exception values are built from exception constructors, where c stands for a constant exception
and d for a unary exception constructor that can be applied to a value of fixed type (we
use d without an argument to refer to either a constant or a unary exception constructor).
Exception values built from c and d are first class values that can be passed around before
being raised in a raise-expression or pattern matched in a case-expression. Handle expressions evaluate e0 while catching any exception. If an exception is caught, it is bound to
identifier x and the handler body e1 is evaluated. Raise-expressions evaluate the argument
e (which must be an exception value) and raise it.4 Case expressions evaluate e0 and match
the result against pattern p. If the resulting value matches, expression e1 is evaluated,
otherwise, the value is bound to identifier x and the default branch e2 is evaluated. Finally,
a let-expression binds variable x to the value of e1 and evaluates e2. We use let-expressions
to illustrate polymorphic analysis. Patterns p consist of variable patterns x (matching any
value and binding it to the identifier x), constant patterns c, constructor patterns d(p) with
argument pattern p and pair patterns (p1; p2). We assume that patterns are linear, i.e.,
every identifier occurs at most once in any given pattern.

Our implementation of the exception inference deals with the entire ML language
and our examples in this section do use ML features not present in the above language.
The exception inference is formulated as a non-standard type and effect system [54] which
infers a type expression T and an effect expression E for each program expression e of the
source program. The effect E of an expression e denotes the set of exceptions that may be
raised during evaluation of e. The following grammar defines type and effect expressions.

T ::=exn(E) j hT1; T2i j T1 E\Gamma ! T2 j T
E ::=E j 0 j c j d(T ) j E " E j E [ E j :fdg

where T ranges over type variables and E ranges over effect variables. The type T1 E\Gamma ! T2 is
a function type with domain T1, range T2 and effect E, where E represents the effect that
results from applying a function of this type. Effect expressions consist of constant exception
constructors c, and unary exception constructors d, one for every exception constructor in
the source program. Because exceptions can carry values, the grammar for type and effect
expressions is mutually recursive, a feature not commonly seen in effect systems.

We map these type and effect expressions onto mixed expressions by choosing
to represent types with FlowTerm-expressions and effects with Set-expressions using the

4For clarity, we made raise expressions explicit even though raise could be treated as a function with
a type binding in the initial environment.

147
following constructors and signatures:

exn : s ! ft
pair : ft ft ! ft

\Delta  \Delta \Gamma ! \Delta  : ft s ft ! ft

c : s for all constant exceptions c
d : ft ! s for all unary exceptions d

Pair types hT1; T2i are represented using a binary constructor pair, but we will write the
more convenient form hT1; T2i. The type expressions T correspond to the standard type
expressions of ML (for our subset of the language) except for the addition of the exception

annotations E in the types exn(E) and T1 E\Gamma ! T2. If we erase these exception annotations,
we recover the standard ML types. We will also use polymorphic constrained expressions
Q = 8V:T nS to analyze let-bound expressions polymorphically. We use the convention that
if V and S are empty, then Q = T .

In the standard ML type system, all exception values have type exn giving no
indication of the constructors used to create the exception. Our exception annotation E
on exn(E) provides information about the exception constructors of a particular exception
value. For example the constant exception Subscript (raised when accessing an array
outside its domain), can be given the type exn(Subscript), where Subscript is the constant
Set-constructor corresponding to the Subscript exception. Similarly, the annotation on
function types provides information about the exceptions that may be raised when applying
a function. If sub is the function that returns an array element at a given index and sub

may raise the Subscript exception, then we assign sub the type hT array; inti Subscript\Gamma \Gamma \Gamma \Gamma \Gamma ! T ,
where T array is the type of an array containing elements of type T and int is the type of
integers. (We discuss later in this section how such types and ML datatypes are added to
our inference system).

Since exception values and functions are first-class values in ML that can be passed
as arguments and stored in data-structures, our type and effect language must be rich
enough to express dependencies between exception values and effects. We illustrate these
aspects with a few more examples. Consider the ML function raise, which is used to
raise an exception. Its ML type is raise : exn ! T . Using our refined type language,

the type becomes raise : exn(E) E\Gamma ! T , capturing the fact that applying raise to an
exception of type exn(E) causes the observable effect E. The variables T and E are implicitly
quantified here, but we omit the quantifier. In general, we can infer constrained polymorphic
expressions as discussed in Section 7.6 for let-bound expressions that observe the so-called
value restriction [90, 62]. We assume that all let-bound expressions in valid Mini-ML
programs are values and that their types can therefore be generalized. Let-expressions that
do not satisfy the value restriction can be eliminated by replacing them with (fn x =?
e2) e1.

Consider a function catchSubscript that calls a function argument, and if the
Subscript exception is raised, returns the default value d.

exception Subscript

148

A(x) = 8V:T nS
fresh substitution oe on V

A ` x : oe(T ) ! 0; oe(S) [VAR]

A ` c : exn(c) ! 0; ; [CON0]

A ` e : T ! E; S
A ` d(e) : exn(d(T )) ! E; S [CON1]

A ` e1 : T1 ! E1; S1
A ` e2 : T2 ! E2; S2

A ` (e1; e2) : hT1; T2i ! E1 [ E2; S1 [ S2 [PAIR]

A[x 7! T ] ` e : T ! E; S
A ` fn x =? e : (T E\Gamma ! T ) ! 0; S

[ABS]

A ` e1 : T1 ! E1; S1
A ` e2 : T2 ! E2; S2

S3 = fT1 `ft T2 E\Gamma ! T g
A ` e1 e2 : T ! E1 [ E2 [ E; S1 [ S2 [ S3 [APP]

A ` e : T ! E; S
S0 = fT `ft exn(E)g

A ` raise e : T ! E [ E; S [ S0 [RAISE]

A ` e0 : T0 ! E0; S0
A[x 7! exn(E0)] ` e1 : T1 ! E1; S1
S2 = fT0 `ft T ; T1 `ft T g

A ` e0 handle x =? e1 : T ! E1; S0 [ S1 [ S2 [HANDLE]

A ` e0 : T0 ! E0; S0

`p p : (Tp; R2::Rn; Ap; Sp)
S = fT0 `ft Tpg
A; Ap ` e1 : T1 ! E1; S1
A[x 7! Ri] ` e2 : Ti ! Ei; Si i = 2::n
S0 = fTi `ft T j i = 1::ng

A ` case e0 of p =? e1 [] x =? e2 : T ! Si=0::n Ei; S [ S0 [ Si=0::n Si [CASE]

A ` e1 : T1 ! E1; S1
Q = 8V:T1nS1 V local to S1
A[x 7! Q] ` e2 : T2 ! E2; S2

A ` let x = e1 in e2 end : T2 ! E1 [ E2; S1 [ S2 [LET]

Figure 8.5: Type and exception inference rules for expressions

149
`p x : (T ; fT 0g; [x 7! T ]; ;) [PVAR]
`p c : (exn(E); fexn(E " :fcg)g; []; ;) [PCON0]

`p p : (T; fR1::Rng; A; Sp)
S = Sp [ fE `s Pat[d(T ); d(1)]g

`p d(p) : (exn(E); fexn(E " :fdg [ d(R1) [ :: [ d(Rn))g; A; S) [PCON1]

`p p1 : (T1; fR1::Rng; A1; S1)
`p p2 : (T2; fR01::R0n0 g; A2; S2)
R = f(T1; R01); ::; (T1; R0n0 ); (R1; T2); ::; (Rn; T2)g

`p (p1; p2) : (hT1; T2i; R; A1 \Phi  A2; S1 [ S2) [PPAIR]

Figure 8.6: Type and exception inference rules for patterns
let catchSubscript = fn f =? fn d =?

f () handle Subscript =? d
in ... end

Note our use of the abbreviation e handle p =? e0 for the expression e handle
x =? case x of p =? e' [] y =? raise y. The value () is a dummy value of type unit
and is used as an argument (result) of functions that have no non-trivial argument (result).
We can assign the type

catchSubscript : (unit E\Gamma ! T ) 0\Gamma ! T E":fSubscriptg\Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma ! T
to catchSubscript. The type illustrates the dependencies between the exceptions carried by the function argument f and the exceptions of catchSubscript. Given a function

f : unit E\Gamma ! T which may raise an exception from the set E, we know that the expression f()
has type T and effect E. The handle expression prevents the Subscript exception from escaping the body of catchSubscript. As a result, we know that evaluating catchSubscript
can result in any exceptions raised by the argument function, except Subscript which is
expressed with the Set-expression E " :fSubscriptg. Set expressions make it convenient to
describe such types concisely.

8.2.1 Type and Constraint Generation
Figure 8.5 shows the type inference rules for expressions. The rules assign types, effects,
and a system of constraints to each expression in the source program. Judgments have the
form A ` e : T ! E; S, meaning that under the type assumptions A (mapping identifiers
to type expressions), expression e has type T and may raise the exceptions denoted by E.

150
Both T and E are constrained by the constraint system S. All type and effect variables
appearing in the rules are assumed to be fresh.

Figure 8.6 contains type rules for patterns. Judgments for patterns have the form
`p p : (T; fR1::Rkg; A; S) meaning that pattern p has type T , values not matched by the
pattern have any one of the remainder types R1::Rk, and any variable x occurring in the
pattern has type A(x) as given by the bindings A. Again, types T , R1::Rn, and the range
of A are constrained by the constraints in S.

We now discuss the type rules. Except for the handle expression, exceptions
propagate from sub-expressions to enclosing expressions. The effect of an expression is
thus the union of the effect of its sub-expressions. Identifiers are typed using Rule [VAR] by
looking up the type assumption for x in the type environment A. The possibly polymorphic
type 8V:T nS is instantiated through a substitution oe mapping all variables V to fresh
variables. The result type is oe(T ) with associated constraints oe(S). No exceptions are raised
by referring to an identifier, thus the effect is 0. If V and S are empty, the substitution is
the identity and the result type is T . Rule [CON0] types constant exceptions c by assigning
the type exn(c). There are no associated constraints and the effect is 0, since no exceptions
are raised. Unary exception constructors d(e) are typed using Rule [CON1]. If the inferred
type and effect of the argument expression e is T and E constrained by S, then the type
of the constructor application is exn(d(T )) with effect E and constraints S. Type inference
for a pair expression (e1; e2) proceeds by typing e1 and e2, yielding Ti, Ei, and Si (i = 1; 2).
The result type is hT1; T2i and the effect is the union E1 [ E2, since the exceptions raised
by the pair expression are either the exceptions raised by e1 or by e2. The type and effect
are constrained by S1 [ S2. Rule [ABS] infers the type for a lambda abstraction fn x =? e
by typing the body e where the type assumptions A are extended with a binding for x to a
fresh variable T . If under this assumption the type and effect of the body is T and E, then

the lambda abstraction has function type T E\Gamma ! T . Evaluating a lambda expression does not
raise any exceptions, since the effects of the body are delayed and are only observable when
the function is applied. Function application e1 e2 is typed by Rule [APP] by inferring types

and effects for e1 and e2 and constraining the type T1 of e1 to be a sub-type of T2 E\Gamma ! T ,
where E and T are fresh variables. The constraint makes E an upper bound on the effects
of function T1 and T an upper bound on the result type of T1. Since function types are
contravariant in the domain, the constraint makes the argument type T2 a lower bound on
the domain of T1. The effect of the application is the union of the effect E1 of e1, E2 of
e2, and the effect E produced by applying the function (and thus evaluating its body). The
above constraint is standard for type systems with sub-typing. The sub-typing present in
our system is the subtyping of mixed FlowTerm and Set-constraints.

Rule [RAISE] infers type T and effect E for the argument e. The constraint
T `ft exn(E) makes the effect variable E an upper bound on the set of exceptions carried
by the argument e. The type of the raise expression is a fresh type variable T which is
akin to saying that the raise expression has any type (we could have used 0 instead). The
effect of the expression is the effect E of evaluating e unioned with the set of exceptions E
extracted from the exception type T . Rule [HANDLE] for handle expressions works in the
opposite way. If the type and effect of expression e0 is T0 and E0, then the handler body is
typed under assumptions A extended with a binding for x to exn(E0). The set of exceptions

151
E0 that may be raised by e0 is effectively converted to the exception type exn(E0). The
effect of the handle expression is simply the effect E1 of the handler-body, since the effects
of e0 are caught.

Before we describe the rule for case expressions, we discuss the pattern rules in
Figure 8.6. Recall that pattern judgments have the form `p p : (T; fR1::Rng; A; S) assigning
pattern p a type T , remainder types R1::Rn, a variable environment A, and constraints S.
The type T of the pattern is intended as an upper bound on the type of the value against
which the pattern is matched. The remainder types R1::Rn are possible types for the values
not matched by this pattern. We use a set instead of a single remainder type, since not
all remainder types can be expressed using a single type expression. We will discuss this
issue in the context of the pair pattern. Assumptions A bind pattern variables to types and
constraints S constrain the pattern type T , R1::Rn, and the types in A. Pattern variables
are assigned a fresh type variable T by Rule [PVAR]. The remainder type, i.e., the type of
values not matched by this pattern is T 0--a fresh unconstrained variable--since a variable
pattern matches everything (we could have used 0 instead). The assumptions returned by
[PVAR] binds x to T . Rule [PCON0] assigns type exn(E) to constant exception patterns c.
The remainder type is exn(E ":fcg) expressing that if we match c against some value of type
T `ft exn(E), then the type of the values not matched by c is exn(E " :fcg), i.e., exceptions
of T , except the c exception. Rule [PCON1] types patterns d(p) by inferring a type T and
remainder types R1::Rn for p. The type of d(p) is then exn(E), where E is constrained by
E `s Pat[d(T ); d(1)]. The constraint makes d(T ) an upper bound on the exceptions E of
the value to be matched. It also constrains the matched type against the pattern argument
type T , thus indirectly constraining the remainder types R1::Rn. The remainder type is
exn(E " :fdg [ d(R1) [ :: [ d(Rn)), expressing that any exceptions E " :fdg are not matched
by the pattern, as well as any exceptions d(Ri), where the argument pattern p does not
match. Pair patterns (p1; p2) are typed with the pair type hT1; T2i where T1 and T2 are
inferred for p1 and p2. Since our patterns are linear, the variable bindings A1 and A2 have
disjoint domains and can be composed into A1 \Phi  A2.

The remainder types of pair patterns are tricky. Suppose p1 had a single remainder
type R1 and p2 a single remainder type R2. The remainder type of the pair pattern (p1; p2)
is not hR1; R2i. The remainder hR1; R2i expresses the types that match neither p1 nor p2.
However, the pair pattern only matches if both p1 and p2 match. Thus the remainder types
must be either hR1; T2i (if p1 doesn't match), or hT1; R2i (if p2 doesn't match). These two
types cannot be expressed as a single type without losing precision. The only single safe
remainder type is hT1; T2i which is the same type as the type for the pair pattern. Using a
single remainder type thus results in no filtering when pair patterns are used.

We now return to the type rule for case expressions. Rule [CASE] infers type T0
and effect E0 for expression e0 which is to be matched against p. If the type for the pattern
is Tp with remainder types R2::Rn, and variable bindings Ap, then we constrain the type of
the expression to be matched with the pattern type T0 `ft Tp, and infer type T1 and effect
E1 for the branch expression e1 under the assumptions A extended with the bindings for
the pattern variables Ap. The default branch is typed once per remainder type R2::Rn, by
binding x to Ri. The type of the entire case expression is the fresh type variable T which
is an upper bound of T1::Tn. Similarly, the effects of the case expression are the effects

152

exception Subscript
exception Fail of exn
let substFail = fn f =? fn d =?

(f d) handle x =?

case x of

Fail(y) =? raise y
-- z =? raise z
in ..
end

Figure 8.7: Example Mini-ML program

of e0, e1, and e2 (under all remainder types). Using this type rule becomes impractical if
the number of remainder types is large. Furthermore, precision is only recovered by using
multiple remainder types, if pair and exception constructors are treated as strict, a feature
not currently supported by BANE for FlowTerm-constructors. Thus our implementation
uses a single remainder type by approximating the remainder of pair patterns as described
above.

The final Rule [LET] concerns let expressions. If we infer type T1, effect E1
under constraints S1 for the let-bound expression e1, then we can form the polymorphic
constrained type 8V:T1nS1 representing all possible typings of e1. V is the set of local
variables of S (in this case local variables are all variables generated during the type and
constraint generation of e1). The let-body e2 is then typed using assumptions A extended
with a binding for x to 8V:T1nS1. The type of the let expression is the type of the
let-body T2 and the effect is the union of effects E1 and E2. If we compare this rule to
standard inference rules of let-polymorphic type systems, we notice the absence of any
reference to the assumptions A in choosing the type variables V to be quantified over. In
the standard inference rules of Hindley-Milner type inference expressed using Robinson's
unification algorithm to solve equality constraints and explicit substitutions, the type T1 can
only be generalized over type variables not appearing in the assumptions A. This restriction
is necessary because the equality constraints arising in Hindley-Milner type inference are
eliminated before the quantification. If we constrained a fresh variable X to be equal to a
variable Y appearing in the assumptions, and X appears in type T1, then the unification of X
and Y either substitutes X into A, or Y into T2. In either case, the variable appearing in T2
is not quantified and with reason, since the environment can further constrain it. However,
if we left the equality constraint X = Y unsolved and formed the constrained quantified
type 8X :T2nfX = Yg instead, the reference to the assumptions A is not necessary, since
no variable in A can be local to S. Furthermore, the constraint X = Y appearing in the
polymorphic type guarantees that any further constraints on Y will be reflected on X in
all instantiations of the polymorphic type. In our inference we use inclusion constraints
instead of equality, but the argument is the same.

153
8.2.2 An Example Inference
This subsection uses the example program in Figure 8.7 to illustrate the type and constraint
generation rules. The example program assumes that Subscript is a constant exception
and Fail is an exception constructor with an exception argument. The function substFail
expects a function argument f and an argument d and proceeds by applying f to d. Any
exceptions raised by the application and matching the Fail(y) pattern cause the exception
argument y to be raised. Other exceptions are re-raised. We derive the polymorphic
constrained type

substFail :8T1; Td; E1; E5:(Td E1\Gamma ! T1) 0\Gamma ! Td E

6[E1":fFailg\Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma ! T1n

E1 `s Pat[Fail(exn(E5)); Fail(1)]

for this function. The type expresses that if the argument f has type Td E1\Gamma ! T1, the
argument d type Td, then the result of substFail is T1. So far this corresponds to the
type (ff ! fi) ! ff ! fi that the standard ML type system would derive. The exception
annotations however tell us that if applying f raises exceptions E1, then the exceptions raised
by substFail are the exceptions E1, except for the Fail exceptions, plus any exceptions
carried by a Fail exception of E1.

We break up the body of substFail into expressions

esubstFail = fn f =? fn d =? ehandle

ehandle = eapp handle x =? ecase

eapp = f d
ecase = case x of Fail(y) =? raise y -- z =? raise z

Let A = [f 7! Tf; d 7! Td] be the assumptions resulting from applying Rule [ABS] twice to
esubstFail. Rule [HANDLE] first derives the judgment for expression eapp under A

A ` f : Tf ! 0; ;
A ` d : Td ! 0; ;

A ` f d : T1 ! E1; Sapp

where Sapp = fTf `ft Td E1\Gamma ! T1g and the premisses are obtained by applying Rule [VAR] to
f and d. From this judgment, Rule [HANDLE] constructs assumptions A0 = A[x 7! exn(E1)]
to be used for the inference of ecase. Applying Rule [CASE] to ecase proceeds by deriving
A0 ` x : exn(E1) ! 0; ; using Rule [VAR], and the pattern judgment

`p Fail(y) : (exn(E2); fexn(E2 " :fFailg [ Fail(T 0y ))g; Ap; Spat)
where Ap = [y 7! Ty] and Spat = fE2 `s Pat[Fail(Ty); Fail(1)]g. This judgment in turn is
derived from the pattern judgment `p y : (Ty; T 0y ; Ap; ;) of y. Rule [CASE] proceeds by
typing the branch expression raise y under assumption A0; Ap using [RAISE] resulting in

A0; Ap ` raise y : T3 ! E3; Sraise1

154
where Sraise1 = fTy `ft exn(E3)g. Then the default branch is typed under assumptions
A00 = A0[z 7! exn(E2 " :fFailg [ Fail(T 0y ))] where z is bound to the remainder type obtained
from the pattern judgment of Fail(y). Applying Rule [Raise] again results in

A00 ` raise z : T4 ! E4; Sraise2
where Sraise2 = fexn(E2 " :fFailg [ Fail(T 0y )) `ft exn(E4)g which is equivalent to fE2 "
:fFailg [ Fail(T 0y ) `s E4g. Rule [CASE] now concludes with the judgment

A0 ` ecase : T5 ! E3 [ E4; Scase
where Scase = Spat [ Sraise1 [ Sraise2 [ fexn(E1) `ft exn(E2); T3 `ft T5; T4 `ft T5g. The last
two constraints simply make the result type T5 of ecase an upper bound on the result types
T3 and T4 of the branches. Constraint exn(E1) `ft exn(E2) results from relating the type of
x to be matched with the type of the pattern. This constraint is equivalent to E1 `s E2.
The [HANDLE] Rule now concludes with

A ` eapp : T1 ! E1; Sapp
A0 ` ecase : T5 ! E3 [ E4; Scase

A ` ehandle : T2 ! E3 [ E4; Shandle

where Shandle = Sapp [ Scase [ fT1 `ft T2; T5 `ft T2g and the last two constraints make the
result type T2 of ehandle an upper bound of the type T1 of eapp and T5 of ecase. The final
judgment for esubstFail is obtained by concluding the initial [ABS] Rules and results in

[] ` esubstFail : Tf 0\Gamma ! Td E3[E4\Gamma \Gamma \Gamma ! T1 ! 0; Shandle
The final inductive constraints Shandle are summarized by

T3 `ft T5 T4 `ft T5 T5 `ft T2
T1 `ft T2 E1 `s E2 E2 `s Pat[Fail(Ty); Fail(1)]

E2 " :fFailg [ Fail(T 0y ) `s E4 Ty `ft exn(E3) Tf `ft Td E1\Gamma ! T1

Using the constraint on Ty we can introduce the fresh variable E5 and set Ty = exn(E5)
with the constraint E5 `s E3. Applying minimization and maximization simplification
(Section 7.6.3) to these constraints we obtain

T3 min) 0 T4 min) 0 T5 min) 0
T2 min) T1 T 0y min) 0 E2 min) E1

E4 min) E1 " :Fail E3 min) E5 Tf max) Td E1\Gamma ! T1

resulting in the final polymorphic constrained type

substFail :8T1; Td; E1; E5:(Td E1\Gamma ! T1) 0\Gamma ! Td E

5[E1":fFailg\Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma \Gamma ! T1n

E1 `s Pat[Fail(exn(E5)); Fail(1)]

155
To conclude our example, assume we apply substFail to arguments

f : exn(E6) E6\Gamma ! int
d : exn(Fail(Subscript))

Applying Rule [APP] twice results in the constraints

exn(E6) E6\Gamma ! int `ft Td E1\Gamma ! T1
exn(Fail(Subscript)) `ft Td

E1 `s Pat[Fail(exn(E5)); Fail(1)]

which in inductive form are

Td `ft exn(E6 int `ft T1 E6 `s E1
exn(Fail(Subscript)) `ft Td E1 `s Pat[Fail(exn(E5)); Fail(1)]

Applying transitivity results in the constraints

exn(Fail(Subscript)) `ft exn(E6)

Fail(Subscript) `s E6
Fail(Subscript) `s Pat[Fail(exn(E5)); Fail(1)]

Subscript `s E5

The result type of the application is T1 which can be minimized to int. Similarly, the overall
effect is E5 [ E1 " :fFailg, which can be minimized to Subscript. The example shows that
the type of substFail contains enough information to relate the type of the argument d
with the exceptions raised by f and resulting in the filtering of Fail and extraction of the
argument exception Subscript carried by Fail.

8.2.3 Exception Inference for Full SML
Some remarks about extending the described exception inference to full SML are in order.

ffl Standard ML has parameterized modules called functors. Our exception inference

cannot directly analyze functors due to unresolved exception aliasing. We use a tool
to expand all functor applications prior to performing the analysis.

ffl Exception declarations in SML are generative, meaning that a declaration produces

a fresh exception constructor (distinct from all other constructors) at every evaluation. Exception declarations within let expressions can therefore give rise to an
unbounded number of distinct exceptions, all sharing the same name. Since our exception inference matches exceptions by constructor name, the filtering of exceptions
is only sound if a particular exception name refers to a unique exception constructor.
Consequently, only exceptions declared at top-level can safely be filtered by name,
since such exception declarations give rise to exactly one exception constructor. Our

156

analysis implementation classifies exceptions as either top-level, in which case the exception can be filtered, or as generative, in which case the exception is never filtered.
A special case in the pattern rules generates remainder types for generative exception
patterns that do not filter the exception. In practice, we find that practically all exception declarations appear at top-level or can safely be moved to top-level without
changing the semantics of the program. It is worth noting that this problem does
not arise in the CAML dialect of ML, since CAML disallows exception declarations
within let expressions [89].

ffl ML record types are modeled using closed Row-expressions.
ffl In practice it is necessary to know where an exception is raised in a program. Effects

are thus modeled as a set of pairs E@p, where E is a Set-expression describing a set
of exceptions, and P is a Set-expression describing a set of program source positions.
In terms of the implementation, this amounts to adding a binary Set-constructor
@ : s s ! s and adjusting Rule [Raise] where source positions are introduced, and
Rule [Handle], where source positions are removed.

ffl Datatypes hide the internal structure of values. We must ensure that exceptions do not

"disappear" into datatypes. To this end, we extend datatypes containing exception
values (directly or through functions) with a single extra type parameter to capture
these exceptions. To illustrate this technique, consider the following excerpt from a
hash-table implementation of the SML/NJ library.

datatype 'a hash.table =

HT of -not.found : exn,

table : ...,
n.items : ...""

fun mkTable (sizeHint,notFound) =

(HT -not.found = notFound,

table = ...,
n.items = ..."")

The function mkTable is used to create an empty hash-table. It takes an exception
argument notFound, which is stored as part of the hash-table data structure. This
exception value is raised during lookup and remove operations on keys that are not
part of the table. In order to correctly report the exception raised by lookup or
remove, we need to attach the exception used when creating the hash-table to the
type of the hash-table. In general, we augment types with exception information
by parameterizing types with an extra exception argument. In the example, the
hash table type constructor takes a second argument denoting the set of exceptions
potentially stored in the hash-table data-structure. Thus, the type of mkTable is

hint; exn(E)i 0\Gamma ! (T ; E) hash table

157
which states that mkTable can be applied to a pair consisting of an integer and an
exception (whose exception names are bound to E), and it returns a hash-table datastructure containing elements of type T and exceptions E. The dependency between
the hash table type and the exceptions raised by the lookup function appears clearly
in the type of lookup:

(T ; E) hash table ! key E@P\Gamma \Gamma ! T
Any exceptions carried by the hash-table may be raised when calling lookup (where
P is the position of the raise expression within lookup).

In general, we infer for each datatype whether or not it carries any exception names
and effects, and whether these effects appear covariantly, contravariantly, or nonvariantly in the datatype. Our inference conflates all covariant (contravariant) exceptions carried by a datatype.

ffl Mutable references in ML are treated using the same trick applied in the Points-to

analysis of Section 8.1. A reference is treated as an object with a get-method of type
unit ! X and a set-method of type X ! unit, where X represents the contents of the
reference cell. Such a cell is represented using a constructor with signature

ref : ft ft ! ft
where the first FlowTerm-argument refers to the range of the get-method, and the
second FlowTerm-argument refers to the domain of the set-method (and is thus contravariant). The types assigned by our system to ML operations on references are

ref : T ! ref(T ; T )

! : ref(T ; 0) ! T
:= : href(1; T ); T i ! unit

The first function ref creates a fresh reference with contents T . The dereference
operator ! expects a ref type, but extracts only the covariant argument, corresponding
to a call to the get-method. Dually, the assignment function := expects a ref type and
an argument of type T and updates the cell by accessing the contravariant argument
of ref, corresponding to a call to the set-method.

We have successfully applied an implementation of our exception inference combined with a visualization tool to detect two previously unknown bugs caused by uncaught
exceptions in the tools ML-Lex and ML-Burg distributed with the SML/NJ compiler [27].

8.2.4 Precision-Efficiency Variations
We briefly discuss two variations of our analysis, showing how the mixed constraint formalism of BANE helps evaluate precision-efficiency tradeoffs. We refer to the system we
have presented so far as the FlowTerm-Set exception inference system, since it is based on
FlowTerm and Set expressions.

158

A coarsening of the analysis can be obtained by replacing the use of the FlowTermsort for the type structure of our inference with the Term-sort, and strengthening the
FlowTerm-inclusion constraints `ft to equality constraints between Term-expressions =t.
This coarsening results in less precise exception information due to the back-flow of exception information resulting from the use of equality constraints. We refer to this system as
the Term-Set system.

Going in the direction of more expressive, but also more expensive sorts, we can
replace the FlowTerm-sort with the Set-sort, thus using Set-expressions for both the type
and the effect structure. We call this system Set-Set system. Note that there is no gain
in precision by going to this variation unless we refine the types further to take advantage
of the Set-structure. A possible refinement would be to model constructors of a datatype
separately.

Chapter 9 characterizes the precision-efficiency tradeoffs of the three variations of
exception inference by running experiments on all systems and characterizing the number
of exceptions found and the overall performance of the analysis.

8.2.5 Related Exception Work
We are aware of two earlier approaches to uncaught exception detection for ML. Guzm'an
and Su'arez [37] describe an extended type system for ML similar to, but less powerful
than the one presented here. They do not treat exceptions as first class values, and they
ignore value-carrying exceptions. Yi [91] describes a collecting interpretation for estimating
uncaught exceptions in ML. His analysis is presented as an abstract interpretation [17] and
is much finer grained than Guzm'an and Su'arez' approach [37] or the system described here,
but is also slow in practice.

Independently, Yi [93] developed an approach to exception inference of ML based
on a control-flow analysis, followed by an exception analysis based on set constraints. Since
control-flow depends on exception information, this approach conservatively approximates
exception information during the control-flow analysis. This step is justified since exceptions
rarely carry function arguments. The resulting analysis is comparable in precision and
performance to the one presented here.

More recently, Pessaux and Leroy [70] have proposed an exception inference for
CAML based on types and Row-expressions. Their work can be viewed as yet another
refinement expressible in BANE, where exception sets are represented by labels present in
Row-expressions instead of Set-expressions.

159
Chapter 9
Experiments

In this chapter we evaluate the implementation strategies employed in BANE and
the benefits provided by a reusable mixed constraint framework. We show the impact of
BANE's implementation strategies by running a number of experiments exercising each implementation feature in isolation. The experiments show that BANE enables the resolution
of constraint problems that are orders of magnitude larger than previously reported results,
and that precision-efficiency variations can be studied through appropriate sort selections.

Furthermore we experimentally contrast BANE's inductive transitive closure (for
computing the consistency of constraint graphs) with the standard transitive closure algorithm.

The chapter is organized as follows. Section 9.1 discusses general aspects concerning the measurement methodology. Section 9.2 shows that for our experiments, Setconstraint graphs contain large strongly connected components (SCC) and that these components are a major cost during resolution. The section then shows the effectiveness of
BANE's novel partial online cycle elimination strategy. Section 9.3 evaluates the effect of
projection merging on constraint resolution. We show that projection merging speeds up
the analysis of the largest programs by an order of magnitude, but has little effect on small
programs. Section 9.4 studies how the use of inductive form (IF) compares to the standard
form (SF) used traditionally for solving inclusion constraints. We show that SF is superior
to IF only in the presence of cycles. When cycles are eliminated using our online strategy, then IF performs much better than SF without cycle elimination. We also show that
BANE's strategy for cycle detection can be applied to SF resulting in similar speedups as
for IF. Section 9.4 concludes with a discussion of a clear advantage of IF over SF, namely
the ability to compute the transitive lower-bound on demand.

9.1 Measurement Methodology
All experiments were performed using a single processor on a SPARC Enterprise-5000 with
2GB of memory. Reported execution times are best out of three runs on a lightly loaded
machine.

The absolute execution times include the execution of a fair amount of instrumentation code present in BANE. Removal of that code may yield better absolute times.

160

Strongly Connected Components (SCC)
AST Total Initial Initial graph Final graph
Benchmark Nodes LOC #Vars Nodes Edges #Vars #SCC #Max #Vars #SCC #Max

allroots 700 426 126 74 110 5 2 3 20 5 11
diff.diffh 935 293 186 133 184 6 3 2 13 6 3
anagram 1078 344 209 127 191 10 5 2 24 9 5
genetic 1412 323 226 137 196 4 2 2 14 5 4
ks 2284 574 326 225 329 6 3 2 39 3 34
ul 2395 441 196 146 192 4 2 2 6 3 2
ft 3027 1180 393 278 392 0 0 -- 68 5 55
compress 3333 651 251 189 293 17 7 4 26 9 6
ratfor 5269 1532 605 565 747 20 7 6 89 8 65
compiler 5326 1888 442 597 725 12 6 2 35 8 20
assembler 6516 2980 971 849 942 9 4 3 88 10 27
ML-typecheck 6752 2410 794 722 976 29 9 9 248 15 144
eqntott 8117 2266 978 699 1066 50 15 12 177 22 55
simulator 10946 4216 1433 1261 1350 6 3 2 237 5 213
less-177 15179 11988 1848 1751 2195 30 12 4 273 18 202
li 16828 5761 3223 2653 3384 24 10 4 1267 6 1255
flex-2.4.7 29960 9345 3749 3363 4377 57 19 11 325 19 248
pmake 31148 18138 3312 3189 3938 63 24 8 669 21 592
make-3.72.1 36892 15213 4737 3781 5150 156 63 7 869 64 665
inform-5.5 38874 12957 4402 4535 5338 34 15 6 505 22 407
tar-1.11.2 41035 18293 4158 2957 5073 347 89 29 848 82 527
sgmls-1.1 44533 30941 4253 3953 5268 91 37 9 910 43 767
screen-3.5.2 49292 23919 6516 4849 6050 108 46 6 914 39 824
cvs-1.3 51223 31130 6971 5358 6936 340 87 31 743 95 379
espresso 56938 21537 6327 4855 7839 333 149 10 1396 171 976
gawk-3.0.3 71140 28326 6237 4653 7134 400 58 86 1343 45 1087
povray-2.2 87391 59689 7712 5863 8391 198 87 9 1509 75 1245

Table 9.1: Benchmark data common to all experiments
However, all experiments were run with all instrumentation code in place.

For the Points-to experiments, library functions were assumed to have no effect on
the pointer graph, a naive assumption. For the exception inference experiment, conservative
signatures of all library functions were provided.

9.2 Online Cycle Elimination
In this section we show that cycle elimination in the constraint graph is a crucial step to
making inclusion constraint analyses scale to large problems with good performance. Cyclic
constraints have the form X1 ` X2 ` X3 ` : : : ` Xn ` X1 where the Xi are variables. All
variables on such a cycle are equal in all solutions of the constraints, and thus the cycle can
be collapsed to a single variable. Cycles arise and can be eliminated in the constraints of
all sorts in BANE. The experiments here focus on Set-constraints.

We show that the partial online cycle detection employed in BANE is both efficient
in practice with a small constant time overhead on every edge addition, and still able to
find and eliminates on average 90% of all variables involved in cycles. For our benchmarks,
this approach radically improves the scaling behavior, making points-to analysis of large
programs practical. For example, online cycle elimination provides speedups of points-to
analysis of large programs (more than 10000 lines) of up to a factor of 50.

The reason why cycle elimination produces such large speedups is that cycles may
induce a non-linear amount of work that can be avoided when the cycle is collapsed. To see
this suppose that V is a strongly connected component of size n and there are edges from a
set of sources E1::Em to some variables in V . In the worst case, closing the constraint graph
will add edges from each source Ei to each variable in V , i.e., mn edges. Furthermore,

161
Experiment Description
IF-Plain Inductive form, no cycle elimination
IF-Oracle Inductive form, with full (oracle) cycle elimination
IF-Online Inductive form, with online cycle elimination

Table 9.2: Experiments

if there are redundant paths within V , then the closure may try to add the same edge
redundantly up to n times. If on the other hand the component V is collapsed to a single
variable before adding edges E1::Em incident on V , then the work to add them is reduced
to m steps.

9.2.1 Measurements
The measurements in this section use our Set-constraint formulation of Andersen's Pointsto analysis, run on the C benchmark programs shown in Table 9.1. For each benchmark,
the table lists the number of abstract syntax tree (AST) nodes, the number of lines in the
preprocessed source, the number of set variables, the total number of distinct nodes in the
graph (sources, variables, and sinks), and the number of edges in the initial constraints
(before closing the graph). Furthermore, the table contains the combined size of all nontrivial strongly connected components (#Vars), the number of components (#SCC), and
the size of the largest component (#Max), both for the initial graph (before closure) and for
the final graph (in any experiment). The difference in the combined size of SCCs between the
initial and the final graph shows the need for online cycle elimination. Cycles are formed
dynamically through new edges added by the structural closure rules. If all cycles were
present in the initial graph, online cycle elimination would be unnecessary. For example,
for the benchmark povray, only 13% of all variables involved in cycles are apparent in the
initial constraint graph.

We performed the three experiments shown in Table 9.2. The first experiment,
IF-Plain, runs the points-to analysis without performing cycle elimination. Experiment IFOracle precomputes the strongly connected components of the final graph and uses that
information as an oracle during the analysis. Whenever a fresh Set-variable is created, the
oracle predicts to which strongly connected component the variable will eventually belong
in the final graph. We designate a unique witness variable of that component and add an
alias edge from the fresh variable to the witness. As a result, the oracle experiment uses only
a single variable (witness) for each strongly connected component, and thus the constraint
graphs are acyclic at all times. Since the oracle experiment avoids all unnecessary work
related to cycles in the constraint graph (perfect cycle elimination), it provides a lower
bound1 for the last experiment--IF-Online--which uses BANE's online cycle detection and
elimination algorithm. (The prefix IF- of these experiments refers to the use of the inductive
closure for the constraint graph. In Section 9.4 will we examine similar experiments using
the standard transitive closure SF.)

1In fact, the oracle as implemented has true future knowledge, since it combines variables even before
the cycle the variables belong to is present in the graph.

162

IF-Plain IF-Oracle
Benchmark Edges Work Time(s) Edges Work Time(s)

allroots 291 34 0.13 522 35 0.14
diff.diffh 404 46 0.18 723 44 0.20
anagram 444 45 0.19 702 11 0.21
genetic 438 25 0.21 895 23 0.22
ks 2649 6134 0.50 1519 305 0.44
ul 306 51 0.31 1220 51 0.36
ft 3480 10496 0.71 1798 117 0.54
compress 463 67 0.35 1658 57 0.44
ratfor 4986 10558 1.20 4257 265 1.34
compiler 1268 265 0.72 3323 82 1.07
assembler 3173 2514 1.30 5479 444 1.92
ML-typecheck 18284 219631 7.07 6528 931 1.84
eqntott 5614 6908 1.41 5858 369 1.59
simulator 15061 292900 7.14 7179 1128 2.25
less-177 32237 455486 12.97 11894 2456 3.74
li 1146729 156695062 3577.03 15883 46302 9.63
flex-2.4.7 76662 1625239 37.32 29205 1160 13.49
pmake 390037 30756702 649.46 20622 25735 7.44
make-3.72.1 800101 104545496 2370.49 26811 91328 12.71
inform-5.5 196431 13856788 269.09 26491 20474 10.68
tar-1.11.2 248733 16414402 316.73 26133 15207 9.20
sgmls-1.1 998305 178723881 3690.27 45302 92659 21.22
screen-3.5.2 894383 96327887 1995.84 33965 64764 14.79
cvs-1.3 171643 5258119 123.59 35821 14243 12.46
espresso 916001 81191353 1671.40 39131 80024 17.25
gawk-3.0.3 1041525 106585396 2314.35 43717 57869 18.65
povray-2.2 2171746 343496620 8058.49 66258 150098 29.55

Table 9.3: Benchmark data for IF-Plain and IF-Oracle

0.1

1
10
100
1000
10000

1000 10000 100000
Time(s)

AST nodes
Figure 9.1: IF without cycle elimination

0.1

1
10
100

1000 10000 100000
Time(s)

AST nodes

IF-OnlineIF-Oracle

Figure 9.2: Analysis times with cycle detection and oracle

Table 9.3 shows the results for the first two experiments. For each benchmark
and experiment, we report the number of edges in the final graph, the total number of
edge additions (Work) including redundant ones, and the execution time in seconds. The
reported CPU times are best out of three runs. The reported times include the time to
compute the transitive lower bounds of all points-to variables to compute the points-to
sets. Note the large number of redundant edge additions for IF-Plain. The low numbers
for the oracle runs IF-Oracle in Table 9.3 show that the bulk of work and execution time
is attributable to strongly connected components in the constraint graph. Without cycles,
the points-to analysis scales very well. For example, the analysis time for povray-2.2 with
IF-Oracle is over 250 times faster than with IF-Plain.

163
SCC IF-Online
Benchmark AST Elim. Edges Work Time(s) %CD

allroots 700 15 265 311 0.11 18.18%
diff.diffh 935 13 369 423 0.14 0.00%
anagram 1078 23 372 415 0.14 0.00%
genetic 1412 11 424 451 0.18 5.56%
ks 2284 33 1027 1909 0.34 11.76%
ul 2395 6 286 337 0.23 8.70%
ft 3027 54 897 1370 0.40 7.50%
compress 3333 24 397 487 0.32 0.00%
ratfor 5269 64 1758 2515 0.85 5.88%
compiler 5326 22 1047 1240 0.62 1.61%
assembler 6516 69 2174 2897 1.19 3.36%
ML-typecheck 6752 227 2858 7924 1.85 12.97%
eqntott 8117 150 2496 3805 1.10 7.27%
simulator 10946 177 3652 8243 2.14 14.49%
less-177 15179 235 7040 15377 3.72 19.62%
li 16828 1013 10100 100410 15.80 27.34%
flex-2.4.7 29960 281 7531 10052 6.14 2.93%
pmake 31148 582 10629 53327 9.97 20.26%
make-3.72.1 36892 798 18123 238148 30.66 36.24%
inform-5.5 38874 422 14139 51621 10.47 17.10%
tar-1.11.2 41035 745 15366 48610 10.79 19.46%
sgmls-1.1 44533 815 17358 208348 35.03 29.35%
screen-3.5.2 49292 823 19707 159340 26.42 25.47%
cvs-1.3 51223 652 20289 49522 11.82 14.47%
espresso 56938 1208 21671 141052 27.19 24.20%
gawk-3.0.3 71140 1122 18613 111359 21.93 20.79%
povray-2.2 87391 1355 51216 286044 50.76 28.09%

Table 9.4: Benchmark data for IF-Online

0.1

1
10
100

0.1 1 10 100 1000 10000
Speedups

Absolute time(s) IF-Plain

IF-Online/IF-Plain

Figure 9.3: Speedups through online cycle
detection

Figure 9.1 plots the analysis time for IF-Plain without cycle elimination against
the number of AST nodes of the parsed program. As the size exceeds 15000 AST nodes
there are many benchmarks where the analysis becomes impractical.

Table 9.4 reports the measurement results for the online cycle elimination experiment IF-Online. In addition to the information shown for the plain and oracle experiments,
the table contains the number of variables that were eliminated through cycle detection and
the fraction of analysis time spent in cycle detection (%CD). As the execution times and
work counts show, online cycle elimination is very effective for medium and large programs.
Figure 9.2 plots the analysis times for online cycle elimination and the oracle experiment

164

Detection Coverage
Benchmark Vst Hit% Len Total% Avg% Max%

allroots 2.03 9.84% 2.00 75.00% 87.27% 36.36%
diff.diffh 1.53 7.30% 2.00 100.00% 100.00% 100.00%
anagram 1.66 26.16% 2.12 100.00% 100.00% 100.00%
genetic 1.59 5.37% 2.50 78.57% 85.00% 75.00%
ks 1.88 7.50% 2.53 86.84% 95.10% 85.29%
ul 1.24 16.66% 1.00 100.00% 100.00% 100.00%
ft 2.24 8.13% 2.60 79.41% 70.06% 83.64%
compress 1.68 17.95% 2.21 92.31% 96.30% 66.67%
ratfor 2.19 5.94% 2.82 77.11% 88.89% 69.49%
compiler 1.46 5.13% 2.36 62.86% 88.33% 40.00%
assembler 1.67 7.50% 2.35 78.41% 83.75% 70.37%
ML-typecheck 2.15 4.67% 3.23 93.42% 93.10% 95.80%
eqntott 2.32 10.52% 2.73 87.21% 91.09% 94.23%
simulator 2.29 3.71% 2.39 76.96% 94.27% 71.36%
less-177 2.38 2.28% 2.63 86.08% 95.11% 83.66%
li 1.91 2.04% 2.84 80.72% 96.73% 80.37%
flex-2.4.7 1.85 9.57% 2.74 88.92% 94.74% 88.48%
pmake 1.70 1.31% 2.65 88.18% 88.39% 88.68%
make-3.72.1 1.90 0.49% 3.13 93.22% 94.17% 93.61%
inform-5.5 1.60 1.51% 2.62 90.17% 94.34% 90.86%
tar-1.11.2 1.98 1.83% 3.31 90.63% 95.72% 89.55%
sgmls-1.1 1.65 0.47% 2.67 94.44% 94.40% 94.89%
screen-3.5.2 1.45 0.84% 2.86 95.37% 96.78% 94.44%
cvs-1.3 1.90 2.17% 2.52 90.06% 97.31% 85.83%
espresso 1.84 2.71% 2.68 87.92% 99.49% 83.28%
gawk-3.0.3 1.65 1.41% 2.73 87.04% 91.52% 86.45%
povray-2.2 2.25 0.60% 2.61 93.26% 95.89% 92.77%

Table 9.5: Cycle detection statistics

(note the scale change w.r.t. Figure 9.1). IF-Online stays relatively close to the oracle times
indicating that while our cycle detection algorithm is not perfect, there is not much room
for improvement.

Figure 9.3 shows the total speedup of IF-Online over IF-Plain. To show that our
technique helps scaling, we plot the speedups vs. the absolute execution time of IF-Plain.
As we go to larger problems that take longer to run without cycle elimination, the speedups
also grow. Cycle elimination thus helps scaling and does not simply improve the execution
time by a constant factor. For very small programs, the cost of cycle elimination outweighs
the benefits, but the absolute extra cost is small (! 1 second). For medium and large
programs, online cycle elimination improves analysis times substantially, for large programs
by more than an order of magnitude.

The absolute speedups of IF-Online over IF-Plain should be taken with a grain
of salt. The inductive transitive closure actually performs very badly in the presence of
cycles, due to the fact that it adds transitive edges between variables as well as transitive
edges between sources and sinks. Constraint graph closure based on the standard transitive
closure rule performs a little better in the presence of large strongly connected components
than IF, but the scaling behavior is essentially the same. We will study the relationship
between inductive form IF and standard form SF in more detail in Section 9.4.

Table 9.5 shows statistics on the cycle detection performed during the points-to
analyses. For each benchmark, the table reports the average number of variables visited
during a cycle detection (Vst), the hit rate of finding an actual cycle (Hit%), and the average
cycle length (Len). Furthermore, we give three measures for the quality of the detection.
The overall detection fraction (Total%) is the ratio between the number of variables in the
final graph that we found to be on a cycle, over the total number of variables in non-trivial
strongly connected components in the final graph. The second measure (Avg%) gives the

165
E1 E01

En E0mG

Figure 9.4: Graph Schema

c(X1)

c(Xn0)

Y

c\Gamma 1(Zm0)

c\Gamma 1(Z1)

Figure 9.5: Paths from sources to Y to sinks
average detection fraction per strongly connected component, i.e., for each SCC V in the
final graph, the fraction jV 0j=jV j, where V 0 is the largest single component we detected
within V . Finally, the third measure (Max%) gives the detection fraction for the largest
component in the final graph, i.e., if V is the largest component, the fraction jV 0j=jV j,
where V 0 is the largest single component within V that we detected. The numbers clearly
show that even though the average cycle length detected is small, detected cycles combine
into larger detected components so that the overall coverage is high. Recall from Table 9.1
that the largest strongly connected component in the final graph is substantial for each
benchmark. For example, the largest SCC in the final graph of povray-2.2 contains 1299
variables. The 92.77% detection of the maximal component for povray-2.2 in Table 9.5
states that we detected a single component consisting of 1205 variables within this largest
component.

9.3 Projection Merging
This section takes a close look at the impact of the projection merging technique of BANE
(Section 7.2.2) on constraint graph size and analysis execution times. We explain how
projection merging can in the best case prevent the addition of a quadratic number of
edges to a graph and show through measurements of the points-to analysis that for large
constraint problems, projection merging results in speedups of another order of magnitude
over cycle elimination alone.

To gain insight into how projection merging changes the structure of a constraint
graph during closure, consider the schematic constraint graph in Figure 9.4. Expressions
E1::En are sources and E01::E0m are sinks and we assume that the graph has size O(n + m)
and that there exist paths in the graph connecting each source Ei with each sink E0j.
Furthermore, assume that each source Ei has the form c(Xi) for some fixed unary constructor

166

c(X1)
c(Xn0 )

Y

c\Gamma 1(Zm0)

c\Gamma 1(Z1)
c\Gamma 1(Y0)

Figure 9.6: Paths added through projection merging
c, and that each sink E0j has the form ProjPat(c; 1; Zj ). Closing this graph under any of the
transitive closure rules and structural closure rules we have presented results in the addition
of the nm edges Xi \Gamma ! Zj, for i = 1::n, j = 1::m. The closure of a graph of size O(n + m)
may thus produces a graph of size O(nm). Now observe that if the original graph has size
O(n + m), and there exists paths from each source Ei to each sink Ej, then these paths
must share some variable nodes Y. In fact there must exist variable nodes Y, such that
there are paths from a fraction O(n) of the sources Ei to Y and paths from Y to a fraction
O(m) of the sinks E0j. This situation is depicted in Figure 9.5, where we assume that n0 of
the sources Ei have paths to Y and there are paths from Y to m0 of the sinks E0j (we use
straight edges to denote graph edges and wavy lines to denote paths. We also abbreviate
ProjPat(c; i; Zi) in the graph by c\Gamma 1(Zi)). If our graph is being closed under the inductive
transitive closure rule and only O(n + m) transitive edges between variables are added2,
we can furthermore assume without loss of generality that variable Y is the variable with
minimum index on all paths Ei\Gamma !\Lambda E0j. Without projection merging, the inductive transitive
closure rule eventually adds direct edges Ei \Gamma ! Y and Y \Gamma ! E0j for all paths Ei\Gamma !\Lambda Y and
Y\Gamma !\Lambda Zj. The resulting transitive constraints Ei `s Ej on Y then generate the n0m0 edges
Xi \Gamma ! Zj.

Now consider the impact of projection merging in the situation of Figure 9.5.
Projection merging will create a new sink ProjPat(c; i; Y0) with an edge Y \Gamma ! ProjPat(c; i; Y0),
and paths Y0\Gamma !\Lambda Zj for the m0 sinks ProjPat(c; i; Zj) reachable from Y. We still end up
with edges Ei \Gamma ! Y added by the inductive transitive closure, but now we only add the
n0 transitive constraints Ei `s ProjPat(c; i; Y0), which results in the n0 edges Xi \Gamma ! Y0.
As a result, we have created paths Xi\Gamma !\Lambda Zj for the n0 sources c(Xi) and the m0 sinks
ProjPat(c; i; Zj) connected through Y (depicted in Figure 9.6). The sharing in these new
paths mirrors the sharing present in the paths Ei\Gamma !\Lambda Ej, since all the new paths Xi\Gamma !\Lambda Zj pass
through the fresh variable Y0. As a result, we can expect the graph closed using projection
merging to still have O(n + m) edges, assuming again that the inductive transitive closure
rule only adds a linear number of edges directly between variables.

2We describe a theoretical model for estimating the cost of closing constraint graphs under the inductive
transitive closure rule in a separate publication [28]. For random graphs and any ordering o of the variables,
the expected number of transitive edges added between variables is linear.

167
IF-PM Addtl Reduction w.r.t. IF-Online
Benchmark AST #Vars Edges Work Time(s) #Vars Edges Work Time

allroots 700 212 365 59 0.16 86 0.73 0.58 0.69
diff.diffh 935 277 514 46 0.17 100 0.72 1.00 0.82
anagram 1078 316 502 18 0.20 108 0.74 1.39 0.70
genetic 1412 438 707 23 0.25 212 0.60 1.04 0.72
ks 2284 582 1288 321 0.52 266 0.80 2.13 0.65
ul 2395 258 326 83 0.28 67 0.88 0.61 0.82
ft 3027 598 1084 190 0.53 209 0.83 1.45 0.75
compress 3333 393 582 149 0.40 152 0.68 0.43 0.80
ratfor 5269 845 2158 356 1.01 292 0.81 1.51 0.84
compiler 5326 667 1276 199 0.91 258 0.82 0.83 0.68
assembler 6516 1494 3227 869 1.74 528 0.67 0.63 0.68
ML-typecheck 6752 1252 3184 1405 2.06 473 0.90 2.39 0.90
eqntott 8117 1608 3260 657 1.57 688 0.77 1.44 0.70
simulator 10946 2106 4452 666 2.08 703 0.82 6.15 1.03
less-177 15179 2441 6921 1812 3.22 607 1.02 4.11 1.16
li 16828 4061 11567 4475 6.54 872 0.87 19.21 2.42
flex-2.4.7 29960 5318 9811 1751 7.43 1664 0.77 1.01 0.83
pmake 31148 5304 12182 3997 6.86 2032 0.87 10.11 1.45
make-3.72.1 36892 7164 33820 150028 43.89 2558 0.54 1.38 0.70
inform-5.5 38874 8197 18920 3966 9.66 4161 0.75 8.85 1.08
tar-1.11.2 41035 5576 17282 3607 7.69 1562 0.89 8.57 1.40
sgmls-1.1 44533 6085 24181 13465 13.92 2245 0.72 13.69 2.52
screen-3.5.2 49292 8032 17309 5236 10.79 1760 1.14 25.39 2.45
cvs-1.3 51223 10618 23478 6223 12.20 3729 0.86 4.39 0.97
espresso 56938 10377 23186 6460 12.69 4247 0.93 17.65 2.14
gawk-3.0.3 71140 8563 20007 6296 14.54 2558 0.93 13.94 1.51
povray-2.2 87391 11812 38242 22654 23.91 4329 1.34 9.80 2.12
mume 312458 68233 218772 102215 170.24 16720 2.47 123.16 10.94
gs 504724 115976 245064 75254 272.73 19353 177.85 46.19 192.90
gcc 1168907 199424 707881 854424 808.27 63748 0.93 94.03 14.55

Table 9.6: Benchmark data for IF-PM

Table 9.6 contains the results of experiment IF-PM which consists of running the
Points-to analysis of Section 8.1 on all the C benchmarks we used in the previous section
and two additional large benchmarks, ghostscript (gs), and the GNU C-compiler (gcc).
Each run used cycle elimination and projection merging. Projection merging without cycle
elimination does not perform well at all, which is not surprising. As we have outlined above,
projection merging creates paths between variables appearing in sources and variables appearing in sinks and these paths mirror the structure of the paths connecting the source
and the sink. If the original paths contain cycles, projection merging results in similar
additional cycles and we have already established in the previous section that cycles slow
constraint resolution significantly.

Table 9.6 gives the total number of edges, total redundant work, and the total
execution time in seconds for all benchmarks. Furthermore, we show reduction factors
of these quantities with respect to the equivalent experiment without projection merging,
and the number of additional variables generated by projection merging. The number of
additional variables generated by projection merging varies, but is generally between 25-
30%. This relatively large increase of variables explains that except for two benchmarks,
projection merging results in a slight increase of the final graph size. The redundant work
however is reduced between 1 and 100 fold for all but the small benchmarks which translates
into substantial speedups in execution times. The speedups are thus primarily due to the
reduced redundant work and not to reduction in the overall graph size. Redundant work
results from redundant paths in the constraint graph. Benchmark gs seems to be a special
case. Projection merging reduces the graph size of gs substantially more than for the other
benchmarks. We currently have no explanation for this effect.

168

1
10
100
1000
10000

1000 10000 100000 1e+06
Speedups

AST size

IF-PM/IF-PlainIF-PM/IF-Online

Figure 9.7: Speedups through projection merging
To see how projection merging helps reduce redundant work, consider again the
schematic graph in Figure 9.4 and suppose that there are k variables Y1::Yk for which the
situation depicted in Figure 9.5 holds. In other words, there are k distinct paths between
each source Ei and sink E0j. Without projection merging, edge Xi \Gamma ! Zj is added through
each Yk, for i = 1::n0 and j = 1::m0, resulting in (k \Gamma  1)n0m0 redundant work or O(knm).
Contrast this with the graph produced using projection merging. The structure of the
paths created between Xi and Zj mirror the paths between Ei and E0j, and thus we create
redundant paths through the fresh variables Y01::Y0k. But the number of edges added is
roughly kn0 + km0 or O(kn + km) without redundant work. The savings in redundant work
can thus dwarf the savings in graph edges by a factor k. Our understanding of the exact
effect of projection merging on the savings in redundant work is incomplete. We believe that
there are positive interactions between projection merging and cycle elimination. Studying
these effects however is tricky and time consuming, since the effects only show up on large
constraint problems and cannot easily be reproduced in micro-benchmarks.

Figure 9.7 plots the speedup obtained through projection merging over cycle elimination alone, and the total speedup of projection merging and cycle elimination over IF-Plain.
As is the case for cycle elimination, the speedups grow as we solve larger constraint problems, showing that projection merging substantially helps scaling. Since the largest three
programs ran out of space for the IF-Plain experiment, projection merging and cycle elimination not only result in substantial speedups, but enable the analysis of large programs
that cannot otherwise be analyzed.

9.4 Standard Form v.s. Inductive Form
In this section we compare the commonly used implementation strategy of set-based analysis [38], which represents constraint graphs in standard form (SF), with the inductive form
(IF) used by BANE. Using our formulation of Andersen's points-to analysis as the example,

169
we show the following four points.

1. Without cycle elimination, standard form performs better on the set of benchmarks

than inductive form.

2. Inductive form with cycle elimination performs substantially better than standard

form without cycle elimination.

3. Cycle detection and elimination for standard form is also effective. With cycle elimination, standard form performs somewhat worse than inductive form with cycle elimination.

4. Standard form computes the transitive lower bound (TLB) of all Set-variables explicitly. Often, the transitive lower bound of only a fraction of all variables needs to be
inspected to extract useful information from an analysis. Inductive form enables the
computation of TLB on demand which yields substantial overall speedup due to the
fact that the total size of the TLB tends to grow quadratically with the size of the
constraint problem.

The next subsections briefly recall from Section 6.2 the properties of the associated constraint graphs for the two representations under study SF and IF. Both forms use
adjacency lists to represent edges. Every edge (X ; Y) in a graph is represented exclusively
either as a predecessor edge (X 2 pred(Y)) or as a successor edge (Y 2 succ(X )).

9.4.1 Standard Form
Standard form (SF) represents edges in constraint graphs as follows:

X ` Y X \Gamma \Gamma \Gamma -Y successor edge
E ` X E \Delta \Delta \Delta \Delta \Delta \Delta -X predecessor edge (E is a source)
X ` E X \Gamma \Gamma \Gamma -E successor edge (E is a sink)

We draw predecessor edges in graphs using dotted arrows and successor edges
using plain arrows. New edges are added by the standard transitive closure rule (STCR).
STCR can be expressed in terms of predecessor and successor edges:

L \Delta \Delta \Delta \Delta \Delta \Delta -X \Gamma \Gamma \Gamma -R , L `s R
Given a predecessor edge L \Delta \Delta \Delta \Delta \Delta \Delta -X and a successor edge at X \Gamma \Gamma \Gamma -R, a new constraint L ` R
is generated. We generate a constraint instead of an edge because only atomic constraints
(constraints on variables) are represented in the graph. The constraint is thus first transformed into a set of atomic constraints using the structural resolution rules given in earlier
chapters. Note that L is always a source in all transitive constraints L `s R generated for
SF. This follows from the fact that the only predecessor edges in the constraint graph are
rooted at sources.

SF makes the transitive lower bound of all variables explicit by propagating sources
forward to all reachable variables via the closure rule. The particular choice of successor

170

Li ` X i = 1::k
Z ` Ri i = 1::m

X ` Yi
Yi ` Z oe i = 1::l

SF IF

Y1 R1L1

RmYlLk
ZX

Y1 R1L1

RmYlLk
ZX

Y1 R1L1

RmYlLk
Z

Y1 R1L1

RmYlLk
ZX X

Close Close

Figure 9.8: Example constraints in SF and IF
and predecessor representation is motivated by the need to implement the transitive closure
rule locally. Given a variable X , the closure rule must be applied exactly to all combinations
of predecessor and the successor edges of X .

Figure 9.8 shows an example system of constraints, the initial SF graph, and the
resulting closed SF graph (left). The example assumes that set expressions L1 : : : Lk are
sources and R1 : : : Rm are sinks. The closure of the standard form adds transitive edges
from each source Li to all variables reachable from X i.e., Y1 : : : Yl; Z. Note that the edges
from L1 : : : Lk to Z are added l times each, namely along all l edges Yi\Gamma \Gamma \Gamma -Z. The total
work of closing the graph is 2kl edge additions, of which k(l \Gamma  1) additions are redundant,
plus the work resulting from the km constraints Li ` Rj (not shown).

To see why cycle elimination can asymptotically reduce the amount of work to
close a graph, suppose there is an extra edge Z\Gamma \Gamma \Gamma -X in Figure 9.8, forming a strongly
connected component X ; Y1; : : : ; Yl; Z. If we collapse this component before adding the
transitive edges Li \Delta \Delta \Delta \Delta \Delta \Delta -Yj, none of the 2kl transitive edge additions Li \Delta \Delta \Delta \Delta \Delta \Delta -Yj are performed
(the km constraints Li ` Rj are still produced of course).

9.4.2 Inductive Form
Inductive form (IF) exploits the fact that a variable-variable constraint X ` Y can be
represented either as a successor edge (Y 2 succ(X )) or as a predecessor edge (X 2 pred(Y)).
The representation for a particular edge is chosen as a function of a fixed total order

171
o : V ! N on the variables. Edges in the constraint graph are represented as follows:

X ` Y X \Gamma \Gamma \Gamma -Y a successor edge if o(X ) ? o(Y)
X ` Y X \Delta \Delta \Delta \Delta \Delta \Delta -Y a predecessor edge if o(X ) ! o(Y)
E ` X E \Delta \Delta \Delta \Delta \Delta \Delta -X predecessor edge (E is a source)
X ` E X \Gamma \Gamma \Gamma -E successor edge (E is a sink)
The choice of the order o(\Delta ) can have substantial impact on the size of the closed constraint
graph and the amount of work required for the closure. Choosing a good order is hard, and
we have found that the gen order employed by BANE performs as well or better than other
orders we have tried. For example, picking a random order performs about as well as the
gen order, but the gen order is of course easier to generate.

The inductive transitive closure rule (ITCR), expressed in terms of predecessor
and successor edges, is surprisingly the same as STCR. Transitive constraints are generated
between L and R through X , if there is a predecessor edge L \Delta \Delta \Delta \Delta \Delta \Delta -X and a successor edge
X \Gamma \Gamma \Gamma -R.

L \Delta \Delta \Delta \Delta \Delta \Delta -X \Gamma \Gamma \Gamma -R , L ` R
Notice that here L may be a source or a variable--unlike SF, where L is always a source. In
IF the closure rule can therefore directly produce transitive edges between variables. (This
is not to say that the closure of SF does not produce new edges between variables, but for
SF such edges always involve the the application of a structural closure rule.)

Unlike SF, the transitive lower bounds of all variables are not explicit in IF. As
was shown in Section 6.2.1, the transitive lower bounds can be computed efficiently for IF.
Unless otherwise stated, all execution times reported in the rest of this section include the
time to compute the transitive lower bounds of all variables.

The right side of Figure 9.8 shows the initial and final graph for the example
constraints using IF. Note that some variable-variable edges in IF are predecessor edges
(dotted), whereas all variable-variable edges in SF are successor edges (solid). The ordering
on the variables assumed in the example is o(X ) ! o(Z) ! o(Yi). Note the extra variablevariable edge X \Delta \Delta \Delta \Delta \Delta \Delta -Z added by the closure rule for IF. As a result of this edge, the closure of
IF adds edges from X to all Ri. Each of the variables Y1; : : : ; Yl; Z has a single predecessor
edge to X , and thus their transitive lower bound is equal to TLB(X ) = fL1; : : : ; Lkg. The
total work of closing the graph is l+m edge additions, of which l\Gamma 1 additions are redundant,
namely the addition of edge X \Delta \Delta \Delta \Delta \Delta \Delta -Z through all Yi, plus the work for the km transitive
constraints Li ` Rj (not shown). The work to compute the TLB is proportional to l.

9.4.3 Measurements
Our measurements use the same C benchmark programs shown in Table 9.1. We performed
the three experiments shown in Table 9.7 which are analogous to the ones performed for
inductive form in Section 9.2. SF-Plain corresponds to classic implementations of set-based
analyses and does not perform any cycle elimination. The SF-Oracle experiment is similar
to IF-Oracle in that it avoids all unnecessary work induced by cycles in the constraint

172

Experiment Description
SF-Plain Standard form, no cycle elimination
SF-Oracle Standard form, with full (oracle) cycle elimination
SF-Online Standard from, using IF online cycle elimination

Table 9.7: Points-to Experiments

SF-Plain SF-Oracle
Benchmark Edges Work Time(s) Edges Work Time(s)

allroots 222 20 0.10 480 20 0.10
diff.diffh 335 41 0.13 690 45 0.17
anagram 365 24 0.15 672 11 0.18
genetic 366 29 0.15 864 26 0.20
ks 1271 1059 0.30 1698 506 0.31
ul 278 54 0.20 1209 53 0.30
ft 1051 496 0.34 1826 153 0.41
compress 387 47 0.28 1627 48 0.45
ratfor 2308 1112 0.69 4561 403 1.16
compiler 1849 293 0.62 3827 131 0.90
assembler 2465 569 1.08 5739 564 1.53
ML-typecheck 16803 129255 2.93 9234 2478 1.77
eqntott 2707 1052 0.96 5668 436 1.45
simulator 30011 266797 5.14 16610 4232 2.49
less-177 40610 279440 6.22 27004 9809 3.65
li 1356287 95149841 1199.05 389792 339648 21.06
flex-2.4.7 12659 5266 5.85 32711 1933 9.67
pmake 276802 9151132 117.40 122188 107489 11.96
make-3.72.1 697032 50125520 606.20 313423 353384 22.08
inform-5.5 260795 8831354 111.25 152261 81206 13.72
tar-1.11.2 270995 6215213 82.10 139649 65246 11.57
sgmls-1.1 1084322 113925061 1346.18 499575 601541 37.67
screen-3.5.2 664085 37987203 461.43 306694 257300 24.55
cvs-1.3 119323 925859 23.21 92380 57411 13.69
espresso 712612 25996610 360.47 387566 331837 29.07
gawk-3.0.3 754367 32595925 439.16 344836 215576 28.20
povray-2.2 2057472 162483247 2148.99 1108347 767024 69.55

Table 9.8: Benchmark data for SF-Plain and SF-Oracle
graph by eliminating cycles before they arise, using the precomputed strongly connected
components of the final graph. Finally, SF-Online uses BANE's strategy for cycle detection
and elimination applied to constraints in standard form.

Table 9.8 shows the results for the first two experiments. Figure 9.9 plots the
analysis time for SF-Plain without cycle elimination against the number of AST nodes of
the parsed program. For comparison, the graph also includes the numbers for IF-Plain, i.e.,
the corresponding experiment for inductive form. Without cycle elimination, SF generally
outperforms IF because cycles add many redundant variable-variable edges in IF that lead
to redundant work. However, the scaling trend is roughly the same, showing that SF also
does not scale well. As is the case for IF, strongly connected components in the constraint
graph are a scaling inhibitor for SF as shown by the low numbers for SF-Oracle in Table 9.8.

To validate that our results are not a product of our particular implementation of
SF, we compare our measurements against the times of an independent implementation of
the same points-to analysis written in C by Shapiro and Horwitz [76]. Their implementation (SH) corresponds to SF without cycle elimination, and we empirically verify that our
implementation of SF produces the same trend on our benchmark suite. The scatter plot
in Figure 9.10 shows that our implementation of SF without cycle elimination is usually
between 2 times faster and 2 times slower than SH (horizontal lines) on a subset of the

173
0.1

1
10
100
1000
10000

1000 10000 100000
Time(s)

AST nodes

IF-PlainSF-Plain

Figure 9.9: SF and IF without cycle
elimination

0.1
0.5

1
2

10

100 1000 10000 100000
Time ratio SH/SF-Plain

AST nodes
Figure 9.10: Relative execution times of
Shapiro and Horwitz's SF implementation
of C points-to analysis (SH) over SF-Plain

benchmarks3 with a few exceptions where our implementation is significantly faster (flex,
li, cvs, inform), and one program where our implementation is substantially slower (tar).

Table 9.9 reports the measurement results for SF-Online. In addition to the information shown for the plain and oracle experiments, the table contains the number of
variables that were eliminated through cycle detection and fraction of analysis time spent
in cycle detection (%CD).

Figure 9.11 plots the execution time of SF-Online against the number of AST
nodes. For comparison, the graph also plots SF-Oracle, IF-Online, and IF-Oracle. The
fastest analysis times are achieved by IF-Oracle, followed by SF-Oracle, IF-Online, and SFOnline. We can use the oracle experiments SF-Oracle and IF-Oracle to directly compare the
graph representations of IF and SF, independently of cycle elimination. Figure 9.11 shows
that IF-Oracle does better than SF-Oracle overall, and thus closing acyclic graphs using the
inductive transitive closure rule is more efficient than using the standard transitive closure
rule. This observation not only applies to the particular experiment described here. We
showed elsewhere [28] that this result can be derived analytically by studying the average
case behavior of STCR and ITCR on random acyclic graphs.

Figure 9.12 shows the speedup obtained through online cycle elimination applied
to standard form (SF-Online over SF-Plain). For comparison, the graph also contains the
speedup of IF-Online over SF-Plain, showing that with cycle elimination, inductive form
does better, as predicted by comparing SF-Oracle and IF-Oracle.

The performance benefit of inductive over standard form is illustrated more clearly
in Figure 9.13. In this plot, we can see that IF-Online is consistently faster than SF-Online
for medium and large-sized programs (at least 10,000 AST nodes).4 For large programs the
difference is significant, with IF-Online outperforming SF-Online by over a factor of 2.5 for
the largest program. For very small programs, IF is at most 50% slower than SF, which in

3Not all benchmarks ran through SH.
4The outlier is the program flex; although flex is a large program, it contains large initialized arrays.

Thus as far as points-to analysis is concerned, it actually behaves like a small program.

174

0.1

1
10
100

1000 10000 100000
Time(s)

AST nodes

SF-OnlineSF-Oracle

IF-OnlineIF-Oracle

Figure 9.11: Analysis times with cycle detection and oracle

0.5

1
2

5
10
20

50
100

0.01 0.1 1 10 100 1000 10000
Speedups

Absolute time(s) SF-Plain

IF-Online/SF-PlainSF-Online/SF-Plain

Figure 9.12: Speedups through online cycle
detection

SCC SF-Online
Benchmark AST #Vars Elim. Edges Work Time(s) %CD

allroots 700 20 11 214 316 0.09 11.11%
diff.diffh 935 13 13 319 455 0.13 7.69%
anagram 1078 23 23 333 438 0.14 0.00%
genetic 1412 14 6 366 490 0.16 6.25%
ks 2284 38 24 957 2511 0.30 6.67%
ul 2395 6 6 268 360 0.20 0.00%
ft 3027 67 27 865 1420 0.40 12.50%
compress 3333 26 21 366 474 0.31 0.00%
ratfor 5269 83 35 1913 3521 0.72 5.56%
compiler 5326 35 12 1560 1983 0.61 0.00%
assembler 6516 88 46 2385 3380 1.08 2.78%
ML-typecheck 6752 243 162 8333 26557 1.79 12.29%
eqntott 8117 173 93 2175 4215 1.11 5.41%
simulator 10946 230 108 19142 112887 4.61 22.13%
less-177 15179 274 150 27998 69334 3.99 10.03%
li 16828 1256 872 635866 2348798 60.85 9.22%
flex-2.4.7 29960 315 177 10859 15621 6.68 7.19%
pmake 31148 661 446 144126 443113 19.16 15.08%
make-3.72.1 36892 858 642 329665 1357871 46.67 19.78%
inform-5.5 38874 468 343 99451 306814 15.74 19.70%
tar-1.11.2 41035 822 666 126487 348520 15.88 11.90%
sgmls-1.1 44533 863 744 368084 2198616 62.53 15.70%
screen-3.5.2 49292 864 737 282551 864976 39.07 17.12%
cvs-1.3 51223 724 554 78151 190880 14.59 11.45%
espresso 56938 1374 834 428677 1066940 48.22 20.01%
gawk-3.0.3 71140 1290 910 365473 1072313 44.81 15.09%
povray-2.2 87391 1456 1024 1182741 4557948 138.39 16.16%

Table 9.9: Benchmark data for SF-Online
absolute times means only fractions of seconds.

We can explain the performance difference of IF and SF by comparing the fraction
of variables on cycles found by IF-Online and SF-Online (Figure 9.14). Throughout, SF
finds only about half as many variables on cycles as IF, and the remaining cycles slow down
SF. One reason for this difference is that for SF, the cycle detection only searches successor
chains. The analog to predecessor chains in SF are increasing chains. Searching increasing
chains in SF results in a higher detection rate (57%), but the much higher cost outweighs
any benefits.

Projection merging hurts standard form, since every extra variable will induce
extra source edges and extra redundant work. Furthermore, since sinks are not propagated

175
0.66

1

1.5

2
2.5

3
3.5

4
4.5

5

0 20000 40000 60000 80000
Time ratio

AST nodes
Figure 9.13: Speedups through inductive
form

0
0.2
0.4
0.6
0.8

1
1.2

0 20000 40000 60000 80000
Fraction of variables on cycles found online

AST nodes

IF-OnlineSF-Online

Figure 9.14: Fraction of variables on cycles
found online

100
1000
10000
100000

1e+06
1e+07
1e+08

1000 10000 100000 1e+06
#Graph Edges

AST nodes

SF-OnlineIF-Online

Figure 9.15: Final graph sizes of SF and IF
backwards, projection merging isn't needed.
9.4.4 TLB on demand
One clear advantage of inductive form over standard form is space usage during the resolution. At each step in the resolution process, the constraint graph for standard form
contains the transitive lower bounds of each variable explicitly. Since the total size of the
TLBs becomes fairly large for large constraint problems, the space usage during resolution
under SF is a scaling inhibitor. Figure 9.15 plots the final graph size of IF and SF vs. the
benchmark size. The diagonal line plots y = x, showing that for IF, the final graph size is
smaller than the number of AST nodes, with the exception of mume where the graph size is
about a third larger. The graph sizes under standard form however exceed the number of
AST nodes by orders of magnitude for medium to large programs. In fact, the largest two
benchmarks (gs and gcc) did not run to completion with 2GB of main memory under SF.

The transitive lower bound (TLB) under IF is computed using Algorithm 6.7 which

176

IF-Online
Benchmark AST Edges QTLBsize FTLBsize BTime(s) QTime(s) FTime(s) Speedup

allroots 700 265 16 34 0.10 0.01 0.00 0.91
diff.diffh 935 369 60 77 0.13 0.01 0.01 1.00
anagram 1078 372 48 89 0.14 0.00 0.01 1.07
genetic 1412 424 52 95 0.18 0.00 0.01 1.06
ks 2284 1027 171 448 0.33 0.01 0.02 1.03
ul 2395 286 24 28 0.23 0.00 0.00 1.00
ft 3027 897 158 215 0.40 0.00 0.01 1.02
compress 3333 397 33 44 0.31 0.01 0.01 1.00
ratfor 5269 1758 610 829 0.84 0.01 0.03 1.02
compiler 5326 1047 398 679 0.61 0.01 0.02 1.02
assembler 6516 2174 681 972 1.16 0.03 0.04 1.01
ML-typecheck 6752 2858 3092 3833 1.73 0.12 0.10 0.99
eqntott 8117 2496 316 605 1.08 0.02 0.05 1.03
simulator 10946 3652 8600 12448 2.03 0.11 0.20 1.04
less-177 15179 7040 16278 19933 3.62 0.10 0.30 1.05
li 16828 10100 453422 538661 15.18 0.62 5.01 1.28
flex-2.4.7 29960 7531 3858 6545 6.03 0.11 0.32 1.03
pmake 31148 10629 90576 118190 9.68 0.29 1.00 1.07
make-3.72.1 36892 18123 159226 273020 30.07 0.59 4.12 1.12
inform-5.5 38874 14139 31044 85711 10.08 0.39 0.78 1.04
tar-1.11.2 41035 15366 64543 108111 10.29 0.50 0.85 1.03
sgmls-1.1 44533 17358 219934 329640 34.28 0.75 4.69 1.11
screen-3.5.2 49292 19707 150763 248691 25.84 0.58 4.69 1.16
cvs-1.3 51223 20289 40274 62274 11.61 0.21 0.90 1.06
espresso 56938 21671 197535 351298 25.19 2.00 3.47 1.05
gawk-3.0.3 71140 18613 152493 306369 21.36 0.57 3.42 1.13
povray-2.2 87391 51216 624500 986988 48.14 2.62 13.07 1.21
mume 312458 539536 17897718 31474473 (113.16) 1800.66 (56.91) 61.89 938.07 (6.08) 1.47
gs 504724 43583903 52728554 - 51360.90 1014.02 1 1
gcc 1168907 659584 87997663 - 11485.26 272.06 1 1

Table 9.10: Benchmark data for on demand TLB

reduces to computing the transitive closure on an acyclic graph. Redundant work due to
transitive edges can be avoided during this computation, whereas during closure under
SF, the same redundant work cannot be avoided. Algorithm 6.7 can also be used for ondemand computation of the TLB by computing only the TLBs of variables reachable from the
variables of interest. As a result, the full TLBs of the entire graph need never be explicitly
represented in memory for IF and as little of the TLB can be computed as necessary. This
last point is of interest in that for most analyses, the TLB is needed for only a fraction of all
variables present in the constraints. In Points-to analysis for example, the points-to relation
is mainly of interest at dereference expressions in the program. Our experiments actually
compute the TLB for every program variable. To quantify the benefits of computing the
TLB on demand under IF we also computed the full TLB explicitly in memory for each
benchmark under the IF-Online experiment, i.e., using cycle elimination, but not projection
merging. Table 9.10 contains for each benchmark the size of the constraint graph under IF,
the size of the TLB computed on demand (QTLBsize), the size of the full TLB (FTLBsize),
the time to compute the closed graph before computing any TLBs (BTime), the time to
compute the query-based TLB (QTime), and the time to compute the full TLB (FTime).
The last column shows the speedup obtained by using the query-based TLB. The full TLB
is roughly twice the size of query-based TLB for many benchmarks under this experiment.
Computing the query-based TLB however is often substantially cheaper, indicating that
the total size may not be a good indicator for performance. Computing the TLB involves
merging sets and eliminating duplicates, which induces costs not apparent in the final TLB
size. Even though computing the query-based TLB is roughly five times cheaper for most
benchmarks, the overall speedups obtained through the query-based TLB for IF-Online are

177
Implementation Description
Term-Set Exception inference using Term-expressions for the type

structure and Set-expressions for exception annotations.
Based on equality constraints between Term-expressions.
FlowTerm-Set Exception inference using FlowTerm-expressions for the

type structure and Set-expressions for exception annotations. Based on inclusion constraints.
Set-Set Exception inference using Set-expressions for both the type

structure and the exception annotations. Based on inclusion constraints.

Table 9.11: Precision-efficiency variations for exception analysis

moderate, ranging from 1 to 1.5. The time to close the graph dominates the time to compute
the TLBs.

The situation changes when we look at the execution times for closing the graph
using projection merging. Projection merging reduces the time to compute the closed
graph substantially, but has no direct influence on the time to compute the TLB. Thus, the
speedup obtained with the query-based TLB computation is more important when using
projection merging. The numbers in parentheses for the mume benchmark show the times
and speedup obtained w.r.t. IF-PM. Note that with projection merging, the time to close
the graph and compute the query-based TLB for mume is less than 15 that of computing the
full TLB alone on the final graph. Since standard form computes the full TLB in all cases,
it can not compete with inductive form and query-based TLB on large constraint problems.

9.5 Precision-Efficiency Tradeoffs
This section evaluates BANE with respect to the precision-efficiency tradeoffs provided
through mixed constraints. We implemented three versions of the exception inference system described in Section 8.2 using different sorts for the inference of the type structure.
Table 9.11 describes the three variations. In theory, the precision increases from the TermSet to the FlowTerm-Set systems. The precision difference between these systems stems
from the use of equality constraints vs. the use of inclusion constraints between type expressions. Inclusion constraints model the direction of the flow of values through a program,
whereas equalities model the flow of values undirectionally. As a result, the use of equality
constraints may lead to back-flow of information. As an example of an expression where
the two systems differ, consider the following program fragment:

let g = fn x =? fn y =? fn f =? (case ... of .. =? f x -- .. =? f y, x)
in

...

Here g is a function of three curried arguments x, y, and f. The result of applying
g is a pair hv1; v2i, where v1 is the result of applying f to either x or y, and v2 is identical
to argument x. An analysis based on equality constraints (Term-Set) equates the types of

178
arguments x and y with the domain of function argument f. As a result, the type of the
second component of the return value (v2) is equal to the type of both x and y, and it
appears as if the second argument y could be returned by g. Thus the type of g obtained
by Term-Set is

g : T1 ! T1 ! (T1 E\Gamma ! T2) E\Gamma ! hT2; T1i
On the other hand, consider the type of g inferred by the FlowTerm-Set system.

g :T1 ! T2 ! (T3 E\Gamma ! T4) E\Gamma ! hT4; T1inT1 `ft T3 ^ T2 `ft T3
Here the types of arguments x and y are not equated. The fact that f is potentially applied
to x and y is reflected in the constraints T1 `ft T3 and T2 `ft T3 expressing that either
argument flows into the domain of function f. The result type of g states precisely that the
second component is of type T1, the type of x.

The effect of the different types for g is best observed by considering applying g
to u, v, and w with the following types:

u : exn(E1)nSubscript `s E1
v : exn(E2)nFail `s E2

w : T 0\Gamma ! T

In words, u is an exception value carrying at least exception Subscript, v is an exception
value carrying at least exception Fail, and w is function, returning a value of the same type
as its argument. Under Term-Set, the result type of the application is hT ; T i with the
constraints

T = exn(E) ^ Subscript `s E ^ Fail `s E
If we extract the exception in the second position of the result and raise it we obtain the
potential exception Fail, even though the application could never result in the Fail exception.

Under FlowTerm-Set, the result type of the above application still correctly models
the possible value flows in the program. We obtain the pair type hT4; T1i with the constraints

T4 = exn(E3) Subscript `s E3 Fail `s E3
T1 = exn(E4) Subscript `s E4

As the example suggests, these differences only show up in higher order programs.

Even though in theory there is a precision difference between the FlowTerm-Set
and the Set-Set systems, our formulation of exception inference for ML does not expose it.
The Set-Set system can in principle deal with types of the form bool [ int, i.e., a union of
an integer and a boolean, whereas the FlowTerm-Set system would have to express this type
as ?. No valid ML programs contain such types however, and thus the difference does not
show up. Every ML type is formed by applying a single head constructor, and the FlowTerm
sort can express such types accurately. It should be noted here that the use of a sort like
FlowTerm or Term requires the analysis to be as polymorphic as the ML type system, for

179
Benchmark Lines ML Description
kb 630 An implementation of the Knuth-Bendix completion algorithm.
lex 1320 The Standard ML/NJ lexer generator.
yacc 2978 The Standard ML/NJ parser generator.
burg 8320 The Standard ML/NJ tree match generator.
pta 30226 The complete Points-to implementation including the BANE

library.

Table 9.12: ML Benchmarks for exception analysis

Experiment Description
Base Exception inference using cycle elimination, projection

merging, and min-max simplification of polymorphic constrained expressions.
NoCycle Same as Base, but no cycle elimination or projection merging.
NoSimp Same as Base, but no min-max simplification.

Table 9.13: Experiments for exception analysis

otherwise, the single head constructor assumption no longer holds. The Set-Set system on
the other hand can analyze ML programs using less polymorphic types than are needed to
infer the ML types. We have not explored this aspect further.

On the efficiency side, constraint resolution for the Term-Set system should be
cheaper than for the FlowTerm-Set system through the use of equality constraints and the
more compact representation that use enables. Similarly, resolution of constraints in the
FlowTerm-Set system should be cheaper than for the Set-Set system, since relations between
types containing no exception annotations can be represented by equalities in the FlowTerm
system. For example, suppose we have the constraints

T1 `ft T2 T2 `ft T3 T2 `ft T4
and we add the constraint bool `ft T1. This constraint forces T1::T4 to be equal to bool
in our implementation, which can be compactly represented. On the other hand, the same
constraints under the Set sort cannot be simplified.

We measured the actual precision and efficiency of the three exception systems on
the five benchmarks shown in Table 9.12. The benchmarks span two orders of magnitude
in terms of size, ranging from 600 lines to 30000 lines. The number of lines is the number
of non-blank, non-comment lines after functor applications have been expanded. For each
system and each benchmark, we performed the experiments shown in Table 9.13. The
Base experiment uses all optimizations present in BANE. The NoCycle experiment evaluates
the efficiency difference of the three systems when no cycle elimination is performed, and
the last experiment--NoSimp--evaluates the efficiency difference of the three systems when
polymorphic constrained expressions are not simplified using the min-max approach of
Section 7.6.3.

180

Exception Counts Analysis Time (s) Avg. # quantified vars
Name LTP LTE LTL MFP MFE MFL Base NoCycle NoSimp Base NoCycle NoSimp

Term-Set
kb 0 0 0 1 1 1 2.04 1.93 2.91 2.36 2.73 9.33
lex 0 0 0 16 7 10 9.12 9.03 18.02 21.05 21.49 52.71
burg 5 4 5 48 17 32 11.15 13.44 11.40 6.02 6.17 8.25
yacc 6 4 6 36 21 36 16.21 15.76 16.53 2.16 2.36 3.16
pta 22 11 22 261 41 180 7894.47 1 1 23.58 - -

FlowTerm-Set
kb 0 0 0 1 1 1 2.10 2.01 5.60 1.27 1.26 20.35
lex 0 0 0 16 7 10 3.05 2.91 10.43 0.35 0.35 28.88
burg 5 4 5 48 17 32 12.39 12.52 25.61 2.72 2.94 26.09
yacc 6 4 6 36 21 36 24.36 23.90 41.26 1.55 1.63 15.11
pta 22 11 22 261 41 180 198.30 1 1 2.10 - -

Set-Set
kb 0 0 0 1 1 1 2.15 2.42 5.14 1.29 1.37 14.96
lex 0 0 0 16 7 10 2.89 4.22 8.77 0.55 0.71 13.51
burg 5 4 5 48 17 32 9.44 11.64 20.36 1.83 2.21 12.05
yacc 6 4 6 36 21 36 27.12 39.56 49.00 1.56 2.04 11.59
pta 22 11 22 261 41 180 319.37 1 1 2.75 - -

Table 9.14: Benchmark data of all implementations and experiments

Table 9.14 contains the results of our experiments. We measured the relative
precision of the exception inference systems in terms of the number of reported uncaught
exception-location pairs for the main function of each program (MFP). Recall from Section 8.2 that we infer sets of uncaught exception-location pairs e@l, where e is an exception,
and l a program point. The pair expresses that the uncaught exception e may originate
from location l. It is thus possible to infer two pairs e@l1, and e@l2 involving the same
exception e, but two different locations l1 and l2. To clarify the differences in the precision
of just the set of exceptions e and locations l, we also give the number of distinct exceptions
(MFE) and locations (MFL) appearing in all exception-location pairs.

We also report the number of uncaught load-time exceptions, i.e., exceptions that
are potentially raised when loading and evaluating initialization code, but before calling the
main function of the program (LTP, LTE, and LTL). The execution times reported are in
seconds and are best out of three runs, using one processor of a lightly loaded 8 processor
Enterprise-5000 machine with 2GB of main memory. Entries marked with 1 ran out of
space or did not run to completion within 24 hours.

The last three columns report the average number of quantified variables per polymorphic constrained expression inferred for each experiment. As we will see, the size of the
quantified types has a strong influence on execution time.

As is apparent from the exception counts, the precision loss of the Term-Set implementation does not manifest itself. All implementations report exactly the same uncaught
exceptions for the main function and the load-time exceptions. A likely explanation for this
fact is that the absence of subtyping in Term-Set is counteracted by let-polymorphism. On
the efficiency side there are even more surprises. The expected efficiency grade from TermSet to FlowTerm-Set only shows up for the three benchmarks kb, burg, and yacc, although
the running times for kb are essentially the same. The most surprising result is that for
the largest benchmark, pta, the Term-Set system performs extremely poorly. We will study
the reason for this anomaly below. The efficiency grade between FlowTerm-Set and Set-Set
shows up only for the two largest benchmarks. For smaller benchmarks, the overhead introduced by fresh variables arising during the resolution of FlowTerm constraints may annul
any advantages of the potential savings provided by the more compact representation of

181
FlowTerm constraints.

If we look at the average number of quantified variables for the Base experiments,
we notice that the averages for the Term-Set system are up to ten times larger than for
the other two systems, indicating that the efficiency advantage of the Term-Set system did
not show up due to large polymorphic constrained expressions. This fact is supported
by the NoSimp experiment where the expected efficiency grade is more apparent. The
purpose of the min-max simplification is to reduce the size of polymorphic constrained
expressions so as to avoid overhead when the expression is instantiated multiple times.
Except for lex and pta, the execution times for the Term-Set system without simplification
are barely different from the Base experiment, whereas for the other systems, execution
times double without simplification. These facts suggest that min-max simplification in
the Term-Set systems does not work well. Clearly, type expressions of sort Term cannot be
minimized or maximized, since we assert equality constraints between type expressions in
the Term-Set system. However, the bounds on Set-variables appearing in the constraints
can be minimized or maximized, except for variables appearing inside Term-constructors
(Section 7.6.3). Examining the types and constraints arising during the inference of pta
with the Term-Set system, we notice that many constrained types contain a large number
of constraints of the form:

IO(exn(E1))@l1 `s E
IO(exn(E2))@l2 `s E
IO(exn(E3))@l3 `s E

...

where the IO exception constructor is used by the input-output subsystem of SML and
signals that an exception was caught during an input-output operation. The exception
that was caught is provided to any handler as an argument carried by the IO-constructor.
The distinct Set-variables E1; E2; E3; : : : all have similar if not the same bounds, and the
locations l1; l2; : : : are also mostly the same. These constraints on E cannot be minimized
by the min-max simplification, since the variables E1; E2; E3; : : : appear inside the Termconstructor exn and future constraints can result in equality constraints on these variables.
Note that the FlowTerm-Set and Set-Set systems can in this same situation minimize the
variables E1; E2; : : : which results in many equivalent lower-bounds on E (since the variable
Ei have similar lower bounds). As a result, large numbers of such bounds do not appear in
these systems.

The NoCycle experiments show that cycle elimination introduces some overhead
for the smaller benchmarks, but is absolutely necessary for the largest benchmark pta. This
largest benchmark exercises the higher order nature and the imperative features of ML more
fully than the other benchmarks and presents a realistic real world analysis problem. The
scaling behavior of the three systems should thus be judged mainly by this benchmark.

A first conclusion from the above experiments is that the expected precisionefficiency tradeoffs do not necessarily arise in practice. Unexpected interactions (in this
case between Term constructors, and simplification) may annul the efficiency advantage of
certain sorts. On the other hand, the precision of a cheaper system need not be worse in

182

Exception Counts Base Experiment
Name LTP LTE LTL MFP MFE MFL Time(s) avg.QV
kb 0 0 0 1 1 1 1.41 2.36
lex 0 0 0 59 7 12 3.93 3.85
burg 5 4 6 415 17 34 7.94 2.89
yacc 6 4 6 493 21 36 15.65 1.91
pta 54 11 22 3541 41 198 127.09 2.50

Table 9.15: Benchmark data for Term-Set Base experiment with Cartesian-closed
constructors.

practice. Being able to express these different systems in a common framework as provided by BANE allowed us to identify why one system performs better than another. As
a result, we added another precision dial to BANE to deal with the performance problem
of the Term-Set system described above. The precision dial allows Set-constructors to be
marked as Cartesian-closed. This option tells BANE that if c : s t ! s is a Cartesian-closed
constructor, then the union of two constructor expressions c(E1; E2) [ c(E3; E4) is to be
treated as equivalent to c(T1; T2) along with the constraints

E1 `s T1 E2 `s T1 E3 `t T2 E2 `t T2
where T1 is a fresh temporary variable of sort s, T2 is a fresh temporary variable of sort
t. In other words, the union of two constructor expressions with the same cartesian-closed
constructor is transformed into a single constructor expression where the arguments are
unioned (the example assumes that both arguments to c are covariant). Applying this option
to the constructor @ combining exception names and locations allows us to simplify the large
numbers of constraints arising in the Term-Set system. In the case of the IO constructor
bounds shown above, the simplification results in equality constraints between E1; E2; E3; : : :
and we end up with a single lower bound IO(exn(E1))@T2 `s E. The optimization however
may result in a severe loss of precision since the association between an exception name and
the location where it was raised is essentially lost.

Table 9.15 shows the results of the Base experiment for the Term-Set system when
Cartesian-closed constructors are used. The last column shows that the number of quantified
variables per polymorphic constrained expression inferred has been reduced ten-fold w.r.t.
to the earlier Base experiment on Term-Set. The numbers are now of the same order as
for the FlowTerm-Set and Set-Set systems. As a result, the execution times have been
reduced, in particular for pta. The expected efficiency and scaling advantage of the TermSet system finally shows. However, we have traded the efficiency gain for a loss in precision.
The number of exception-location pairs inferred for pta jumped from 261 to 3541. Similar
experiments for the FlowTerm-Set and Set-Set systems produced execution times that did
not differ much from the times reported in Table 9.14 for the Base experiments. This fact
is not surprising, since simplification for these systems already produced small polymorphic
constrained expressions. Figure 9.16 summarizes the results. It shows the Base analysis
times for all systems and benchmarks. The Term-Set-CC system refers to the Term-Set
system with Cartesian-closed constructors. No system stands out with a clear performance
advantage over the other. The maximum difference is a factor of 2 between Term-Set-CC and
Set-Set on the pta benchmark. We expect that for even larger benchmarks, the efficiency
relations seen on pta will prevail.

183
1
10
100
1000
10000

100 1000 10000 100000
Time(s)

Lines ML

Set-SetFlowTerm-Set
Term-SetTerm-Set-CC

Figure 9.16: Comparison of the Base experiments
The sequence of experiments described in this section have shown that experimentation is crucial to figuring out practical precision-efficiency tradeoffs of program analyses.
The expected tradeoffs have not shown-up initially but have uncovered a scaling inhibitor
for the Term-Set system.

184
Chapter 10
Related Work

This chapter discusses related work not covered in Section 4.6 or Section 6.4. The
related work is divided into three sections. The first section covers work on practical aspects
of constraint resolution implementations of cubic or worse complexity. The second section
covers sub-cubic time analyses and resolution techniques. Finally, the third section covers
other approaches to program analysis tools and frameworks.

10.1 The Cubic-Time Bottleneck
Most work on program analysis focuses on the soundness and theoretical complexity of the
algorithms. Algorithms based on inequalities (in particular inclusion constraints) generally
require a form of dynamic transitive closure, causing such algorithms to have at least cubic
worst-case time-complexity. Heintze and McAllester [43] show that the problem of determining membership for languages defined by 2-way nondeterministic pushdown automata
(2NPDA) is linear time reducible to a standard flow analysis [67]. The best known algorithm for 2NPDA has cubic worst-case time complexity, showing that it is unlikely that
sub-cubic time algorithms for standard flow analysis exist.

This so-called cubic-time bottleneck has led many researchers away from algorithms based on dynamic transitive closure and towards cheaper, but potentially less precise
techniques. Relatively little work has focused on practical implementation aspects of cubic
or worse constraint resolution algorithms, probably because straight-forward implementations do indeed attain the worst-case complexity even for relatively small programs.

The work described in this dissertation shows that through clever representation
and constraint graph simplification, the cubic worst case complexity need not be attained
in practice. Furthermore, the mixed constraint formalism provides an escape mechanism
to use cheaper constraints, such as equality between Term expressions in cases where the
practical complexity is indeed high.

Work on set-based analysis [38, 30, 29] and inclusion constraint-based type inference [71, 24, 83, 58, 72] has mostly focused on techniques to simplify constraints to
achieve scalability. One exception is Heintze, who describes in his dissertation that hashconsing constructed expressions is important to efficiently test set-memberships, and that
sets should be represented as hash tables. Flanagan and Felleisen prove that their partic185
ular form of set-constraints have minimal normal forms, but that computing the normal
form is PSPACE-complete. They then develop a number of algorithms based on grammar
simplifications that achieve good reductions in constraint size without computing the minimal form. They apply their techniques in MrSpidey [31], a static debugger for Scheme
using set-based analysis. Their results focus mainly on the reduction of constraint system
sizes, showing that the simplifications enable the analysis of medium sized programs (17000
lines of Scheme), whose analysis otherwise exhausts heap space. It would be interesting to
combine inductive form and online cycle elimination with some of the more sophisticated
algorithms they developed.

Pottier develops heuristics for simplifying and thus reducing the size of constraint
systems. Pottier does not report measurements showing the benefit of his approach [72].
We examined similar techniques for simplifying constraint graphs at regular intervals [24];
we showed that the cost-benefit tradeoff of simplifications poses a problem in that frequent
simplification is too expensive for some benchmarks, but necessary for others to achieve
scalability.

Trifonov and Smith investigate the decidability of entailment for a class of set
constraints slightly distinct from those of Flanagan and Felleisen. They only derive an
approximation of entailment that could be used to simplify constraint systems, but do
not investigate practical aspects. Marlow and Wadler developed a soft-typing system for
Erlang [58] based on the set-expressions and set-constraints of Aiken and Wimmers [3].
They also report scaling problems and suggest several ways to simplify constraints that
help scaling. The reported analysis times of their system are still rather slow.

Except for our own system, all of the above systems are based on a standard form
representation of the constraints and standard transitive closure (so far as we know). As
we have shown in Section 9.4, standard form has serious limitations for scaling to large
problems, due to the explicit computation of the transitive lower bounds. Simplification
techniques may help reduce the size of the transitive lower bound, but cannot in general
compute a sparser closure than inductive form.

10.2 Sub-Cubic Time Formalisms
The lack of progress in achieving scalable implementations of algorithms based on dynamic
transitive closure has encouraged interest in asymptotically faster algorithms that are either
less precise or designed for special cases. An example of the former is Steensgaard's formulation of points-to analysis based on conditional unification [79]. Instead of a points-to
set for each program point, his analysis infers points-to equivalence classes. If a pointer is
found to point to two distinct classes, the classes are merged. This approach loses precision
but results in a nearly linear time algorithm.

Shapiro and Horwitz [76] study points-to analysis w.r.t. the precision-efficiency
tradeoff. They contrast an algorithm based on inclusion constraints [8] with the equality based algorithm of Steensgaard [79], and then describe a spectrum of algorithms with
sub-cubic time complexity in between. They conclude that while Andersen's analysis is
substantially more precise than Steensgaard's, its running time is impractical. However,
our implementation of Andersen's points-to analysis is generally competitive with their im186
plementation of Steensgaard's algorithm, suggesting that the precision-efficiency tradeoff
needs to be reexamined.

In Section 6.4 we already discussed the the pseudo-linear time flow analysis of
Mossin and the similar pseudo-linear time closure-analysis algorithm for functional programs
of Heintze and McAllester [64, 42]. These analyses are examples of the second class of
analyses, where particular properties of the program under analysis are exploited. In this
case the exploited property is the bounded type size.

Defouw, Grove, and Chambers study precision-efficiency tradeoffs in receiver-class
analysis of Cecil and Java programs [21]. They show that the receiver class analysis of
Palsberg and Schwartzbach (a cubic time algorithm) does not scale well and propose less
precise but sub-cubic time algorithms based on unification.

10.3 Program Analysis Frameworks
The desire to reuse substantial programming efforts in program analysis is not new. Frameworks for classic dataflow analysis (DFA) are numerous. These frameworks are specialized
for producing dataflow analyses in the context of an optimizing compiler.

Kildall [50] characterizes a broad class of "global" (single procedure) dataflow
analyses (DFA) as iterative fix-points in meet semi-lattices. Based on this formulation, he
built a tool to automatically generate DFA-algorithms from specifications. According to
Aho et al. [1] tools for automatically generated DFA-algorithms have not caught on because
the amount of development time saved was not significant.

Venkatesh [85] and Yi et. al. [92] describe generic frameworks based on abstract
interpretation [17]. Both frameworks allow semantic program analyses to be described in a
high level language, which is then translated into an implementation language and linked
with support libraries. Venkatesh gives no performance numbers, while Yi et. al. report
numbers for inter-procedural alias analysis and constant propagation of 4-100 minutes for
programs in the range of several thousand lines of C. An interesting aspect of the framework
described by Yi et. al. is the idea of a projection, which can be used to coarsen abstract
domains and thus trade precision for efficiency.

Dawson et al. [20] show the practicality of describing and implementing program
analyses as logic programs. An analysis is defined by a translation from the source language
to a Prolog program, where the evaluation of the Prolog program yields the analysis result.
They illustrate their approach with a groundness analysis for logic programs and a strictness
analysis for lazy functional programs. They claim their approach is also practical for classic
DFA of imperative programs. Unfortunately, the order of horn clauses in the generated
program has a direct influence on the efficiency of the analysis. They report reasonable
strictness analysis times of 3 seconds for a 595 line functional program. However, they do
not describe the scaling behavior. This approach corresponds essentially to solving equations
between terms, and can be used e.g. for Hindley-Milner type inference. The authors refer
to constraint logic programming (CLP) as an avenue to generalize the approach.

Attribute grammar frameworks provide a clean formalism to express semantic analyses of programs. A variety of approaches have been explored to overcome the circularity
restriction (Paakki gives a comprehensive survey [66]). For example, logic attributes can be

187
used to express Hindley-Milner type inference. Another approach is to compute iterative
fix-points akin to abstract interpretation.

188

Chapter 11
Conclusions

Expressing a program analysis as a constraint problem provides a clean separation between the analysis specification (constraint generation) and its implementation
(constraint resolution).

The first part of this dissertation developed the formalism of mixed constraints.
Mixed constraints combine different kinds of constraint formalisms into a coherent new
formalism. Mixed constraints give the program analysis designer more control over the
precision-efficiency tradeoffs of an analysis. The second part of the dissertation presents and
empirically evaluates an implementation of the mixed constraint formalism called BANE.

The clean separation of program analysis specification and implementation allows
tuning the constraint representation and resolution and enables powerful optimizations
that can be implemented completely independently from any particular program analyses.
Furthermore, these optimizations and representation choices benefit any future analyses
written using the same library.

We have demonstrated, for the first time, that program analyses based on set
constraints are very practical, even for large programs. As our benchmark, we used a
Points-to analysis for the C programming language, which computes an approximation of
the memory graph and the global control-flow of a program. Keeping the constraint-graph
size under control is crucial. The improved scaling results from a combination of three novel
techniques:

ffl A non-standard constraint-graph representation based on inductive constraints,
ffl Online detection and elimination of cyclic constraints, and
ffl Merging of projection constraints.
All three techniques help reduce the number of edges, even at the cost of introducing new
nodes (in the case of projection merging).

Some scaling obstacles appear only with very large constraint problems. Cyclic
constraints are a problem already on medium sized graphs, whereas our inductive graph
representation becomes important at much larger scales. Projection merging also is not
essential up to very large constraint problems. Our measurements show that inferring

189
the scaling behavior of an analysis implementation from a few small benchmarks is very
misleading. Unfortunately, such inferences are still very common in the literature.

We compared our constraint graph representation to the standard graph representation commonly used in implementations of Set-Based Analysis (SBA). We showed that
SBA implementations also benefit from cycle elimination, but that for very large constraint
problems the standard graph representation requires orders of magnitude more space in
practice than inductive constraint graphs for the same problem.

To evaluate the precision-efficiency choices provided by mixed constraints, we implemented and compared three versions of an exception inference system for Standard ML.
We demonstrated the importance of empirical experimentation in determining the practicality of an analysis, by showing that expected efficiency advantages of less precise, but
faster constraint formalisms may not manifest in a naive implementation.

190
Bibliography

[1] A. V. Aho, R. Sethi, and J. D. Ullman. Compilers Principles, Techniques, and Tools.

Addison Wesley, 1988.

[2] A. Aiken and E. Wimmers. Solving systems of set constraints. In Proceedings of the

7th Annual IEEE Symposium on Logic in Computer Science (LICS'92), pages 329-340.
IEEE Computer Society Press, June 1992.

[3] A. Aiken and E. Wimmers. Type inclusion constraints and type inference. In Proceedings of the 1993 Conference on Functional Programming Languages and Computer
Architecture, pages 31-41, Copenhagen, Denmark, June 1993.

[4] A. Aiken, E. Wimmers, and T.K. Lakshman. Soft typing with conditional types. In

Conference Record of the 21st Annual ACM SSymposium on Principles of Programming
Languages, pages 163-173, January 1994.

[5] Alexander Aiken, Dexter Kozen, Moshe Vardi, and Ed Wimmers. The complexity of set

constraints. In Computer Science Logic '93, volume 832 of Lecture Notes in Computer
Science, pages 1-17. Springer Verlag, September 1993.

[6] Alexander Aiken, Dexter Kozen, and Ed Wimmers. Decidability of systems of set

constraints with negative constraints. Information and Computation, 122(1):30-44,
1995.

[7] Alexander Aiken, Ed Wimmers, and Jens Palsberg. Optimal representations of polymorphic types with subtyping. In Proceedings of the International Symposium on
Theoretical Aspects of Computer Science, pages 47-76. Springer Verlag, September
1997.

[8] Lars Ole Andersen. Program Analysis and Specialization for the C Programming Language. PhD thesis, DIKU, University of Copenhagen, May 1994. DIKU report 94/19.

[9] Andrew Appel. Compiling with Continuations. Cambridge University Press, 1992.
[10] Leo Bachmair, Harald Ganzinger, and Uwe Waldmann. Set constraints are the monadic

class. In Proceedings of the 8th Annual IEEE Symposium on Logic in Computer Science
(LICS'93), pages 75-83. IEEE Computer Society Press, June 1993.

191
[11] Kim B. Bruce and Giuseppe Longo. A modest model of records. In C. A. Gunter and

J. C. Mitchell, editors, Thoretical Aspects of Object-Oriented Programming, Foundation
of Computing, chapter 6. MIT Press, 2nd edition, 1994.

[12] Luca Cardelli. A semantics of multiple inheritance. Information and Computation,

76:138-174, 1988.

[13] Witold Charatonik and Leszek Pacholski. Negative set constraints with equality.

In Proceedings of the 9th Annual IEEE Symposium on Logic in Computer Science
(LICS'94), pages 128-136. IEEE Computer Society Press, July 1994.

[14] Witold Charatonik and Leszek Pacholski. Set constraints with projections are in NEXPTIME. In Proceedings of the 35th Annual Symposium on Foundations of Computer
Science, pages 642-655, Los Alamitos, CA, USA, November 1994. IEEE Computer
Society Press.

[15] David R. Chase, Mark Wegman, and F. Kenneth Zadeck. Analysis of pointers and

structures. In Proceedings of the 1990 ACM SIGPLAN Conference on Programming
Language Design and Implementation, number 25:6 in SIGPLAN notices, pages 296-
310, June 1990.

[16] Patrick Cousot. Types as abstract interpretations. In Conference Record of the 24th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages,
pages 316-331, January 1997.

[17] Patrick Cousot and Radhia Cousot. Abstract interpretation: A unified lattice model

for static analysis of programs by contruction or approximation of fixed points. In Conference Record of the 4th ACM Symposium on Principles of Programming Languages,
pages 238-252, January 1977.

[18] Patrick Cousot and Radhia Cousot. Formal language, grammar and set-constraintbased program analysis by abstract interpretation. In Proceedings of the 1995 Conference on Functional Programming Languages and Computer Architecture, pages 170-
181, June 1995.

[19] Flemming M. Damm. Subtyping with union types, intersection types and recursive

types. In Proceedings of the International Symposium on Theoretical Aspects of Computer Science, pages 687-706. Springer Verlag, April 1994.

[20] Steven Dawson, C. R. Ramakrishnan, and David S. Warren. Practical program analysis using general purpose logic programming systems--A case study. In Proceedings
of the 1996 ACM SIGPLAN Conference on Programming Language Design and Implementation, number 31:5 in SIGPLAN notices, pages 117-126, May 1996.

[21] Greg DeFouw, David Grove, and Craig Chambers. Fast interprocedural class analysis.

In Conference Record of the 25th Annual ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages, pages 222-236, January 1998.

192
[22] Alain Deutsch. On the complexity of escape analysis. In Conference Record of the

24th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 358-371, January 1997.

[23] Maryam Emami, Rakesh Ghiya, and Laurie J. Hendren. Context-sensitive interprocedural points-to analysis in the presence of function pointers. In Proceedings of the 1994
ACM SIGPLAN Conference on Programming Language Design and Implementation,
number 29:6 in SIGPLAN notices, pages 242-256, June 1994.

[24] Manuel F"ahndrich and Alex Aiken. Making set-constraint based program analyses

scale. In First Workshop on Set Constraints at CP'96, Cambridge, MA, August 1996.
Available as Technical Report CSD-TR-96-917, University of California at Berkeley.

[25] Manuel F"ahndrich and Alexander Aiken. Program analysis using mixed term and set

constraints. In Proceedings of the 4th International Static Analysis Symposium, volume
1302 of Lecture Notes in Computer Science, pages 114-126. Springer Verlag, September
1997.

[26] Manuel F"ahndrich and Alexander Aiken. Refined type inference for ML. In Proceedings

of the 1st Workshop on Types in Compilation, 1997.

[27] Manuel F"ahndrich, Jeffrey S. Foster, Alexander Aiken, and Jason Cu. Tracking down

exceptions in Standard ML programs. Technical Report UCB//CSD-96-996, University
of California, Berkeley, 1998.

[28] Manuel F"ahndrich, Jeffrey S. Foster, Zhendong Su, and Alexander Aiken. Partial

online cycle elimination in inclusion constraint graphs. In Proceedings of the 1998
ACM SIGPLAN Conference on Programming Language Design and Implementation,
number 33:5 in SIGPLAN notices, pages 85-96, June 1998.

[29] Cormac Flanagan. Componential Set-Based Analysis. PhD thesis, Rice University,

1997.

[30] Cormac Flanagan and Matthias Felleisen. Componential set-based analysis. In Proceedings of the 1997 ACM SIGPLAN Conference on Programming Language Design
and Implementation, number 32:6 in SIGPLAN notices, pages 235-248, June 1997.

[31] Cormac Flanagan, Matthew Flatt, Shriram Krishnamurthi, Stephanie Weirich, and

Matthias Felleisen. Catching bugs in the web of program invariants. In Proceedings of
the 1996 ACM SIGPLAN Conference on Programming Language Design and Implementation, number 31:5 in SIGPLAN notices, pages 23-32, May 1996.

[32] You-Chin Fuh and Prateek Mishra. Type inference with subtypes. In Proceedings of

the 1988 European Symposium on Programming, pages 94-114, 1988.

[33] R'emi Gilleron, Sophie Tison, and Marc Tommasi. Solving systems of set constraints

using tree automata. In Proceedings of the 10th Annual Symposium on Theoretical
Aspects of Computer Science, pages 505-514, 1992.

193
[34] R'emi Gilleron, Sophie Tison, and Marc Tommasi. Solving systems of set constraints

with negated subset relationships. In Foundations of Computer Science, pages 372-380,
November 1993.

[35] James Gosling, Bill Joy, and Guy Steele. The Java Language Specification, chapter 10,

pages 199-200. Addison Wesley, 1996.

[36] Carl A. Gunter and Dana S. Scott. Semantic Domains, chapter 12, pages 633-674.

Elsevier & MIT Press, 1992. In J. van Leeuwen, editor, Handbook of Theoretical Computer Science, Volume B, Formal Models and Semantics.

[37] Juan Carlos Guzm'an and Asc'ander Su'arez. An extended type system for exceptions.

In Proceedings of the ACM SIGPLAN Workshop on ML and its Applications, pages
127-135, June 1994.

[38] Nevin Heintze. Set Based Program Analysis. PhD thesis, Carnegie Mellon University,

1992.

[39] Nevin Heintze. Set based analysis of ML programs. In Proceedings of the 1994 ACM

Conference on Lisp and Functional Programming, pages 306-17, June 1994.

[40] Nevin Heintze and Joxan Jaffar. A decision procedure for a class of Herbrand set

constraints. In Proceedings of the 5th Annual IEEE Symposium on Logic in Computer
Science (LICS'90), pages 42-51. IEEE Computer Society Press, June 1990.

[41] Nevin Heintze and Joxan Jaffar. A decision procedure for a class of set constraints

(extended abstract). In Proceedings of the 5th Annual IEEE Symposium on Logic in
Computer Science (LICS'90), pages 42-51. IEEE Computer Society Press, June 1990.

[42] Nevin Heintze and David McAllester. Linear-time subtransitive control flow analysis.

In Proceedings of the 1997 ACM SIGPLAN Conference on Programming Language
Design and Implementation, number 32:6 in SIGPLAN notices, pages 261-272, June
1997.

[43] Nevin Heintze and David McAllester. On the cubic bottleneck in subtyping and flow

analysis. In Proceedings of the 12th Annual IEEE Symposium on Logic in Computer
Science (LICS'97), pages 342-351. IEEE Computer Society Press, June 1997.

[44] Laurie J. Hendren, Joseph Hummel, and Alexandru Nicolau. Abstractions for recursive

pointer data structures: Improving the analysis of imperative programs. In Proceedings of the 1992 ACM SIGPLAN Conference on Programming Language Design and
Implementation, number 27:7 in SIGPLAN notices, pages 249-260, June 1992.

[45] Fritz Henglein. Efficient type inference for higher-order binding-time analysis. In 5th

ACM Conference Proceedings on Functional Programming Languages and Computer
Architecture, pages 448-72, 1991.

[46] Fritz Henglein. Global tagging optimization by type inference. In Proceedings of the

1992 ACM Conference on Lisp and Functional Programming, pages 205-215, June
1992.

194
[47] Lalita Jategaonkar and John Mitchell. ML with extended pattern matching and subtypes. In Proceedings of the 1988 ACM Conference on Lisp and Functional Programming, pages 198-211, July 1988.

[48] Neil D. Jones and Steven S. Muchnick. Flow analysis and optimization of LISP-like

structures. In Conference Record of the 6th Annual ACM Symposium on Principles of
Programming Languages, pages 244-256, January 1979.

[49] Pierre Jouvelot and David K. Gifford. Algebraic reconstruction of types and effects. In

Conference Record of the 18th Annual ACM Symposium on Principles of Programming
Languages, pages 303-310, January 1991.

[50] Gary Kildall. A unified approach to global program optimization. In Conference

Record of the ACM Symposium on Principles of Programming Languages, pages 194-
206, October 1973.

[51] Dexter Kozen. Logical aspects of set constraints. In CSL: 7th Workshop on Computer

Science Logic, pages 175-188. Springer Verlag, 1993.

[52] Dexter Kozen. Set constraints and logic programming. Information and Computation,

142(1), 1998.

[53] William Landi and Barbara G. Ryder. Safe approximate algorithm for interprocedural

pointer aliasing. In Proceedings of the 1992 ACM SIGPLAN Conference on Programming Language Design and Implementation, number 27:7 in SIGPLAN notices, pages
235-248, June 1992.

[54] John M. Lucassen. Types and Effects--Towards the Integration of Functional and

Imperative Programming. Ph.D. thesis, MIT Laboratory for Computer Science, August
1987.

[55] John M. Lucassen and David K. Gifford. Polymorphic effect systems. In Conference

Record of the 15th Annual ACM Symposium on Principles of Programming Languages,
pages 47-57, January 1988.

[56] David MacQueen, Gordon Plotkin, and Ravi Sethi. An ideal model for recursive polymophic types. In Conference Record of the 11th Annual ACM Symposium on Principles
of Programming Languages, pages 165-174, January 1984.

[57] David B. MacQueen, Gordon D. Plotkin, and Ravi Sethi. An ideal model for recursive polymorphic types. Information and Control, 71(1-2):95-130, October-November
1986.

[58] Simon Marlow and Philip Wadler. A practical subtyping system for Erlang. In Proceedings of the International Conference on Functional Programming (ICFP '97), number
32:8 in SIGPLAN notices, pages 136-149, June 1997.

[59] David McAllester and Nevin Heintze. On the complexity of set-based analysis. In

Proceedings of the International Conference on Functional Programming (ICFP '97),
number 32:8 in SIGPLAN notices, pages 150-63, June 1997.

195
[60] Robin Milner. A theory of type polymorphism in programming. Journal of Computer

and System Sciences, 17:348-375, 1978.

[61] Robin Milner, Mads Tofte, and Robert Harper. The Definition of Standard ML. MIT

Press, 1990.

[62] Robin Milner, Mads Tofte, Robert Harper, and David MacQueen. The Definition of

Standard ML (Revised). The MIT Press, 1997.

[63] John Mitchell. Coercion and type inference (summary). In Conference Record of the

11th Annual ACM Symposium on Principles of Programming Languages, pages 175-
185, January 1984.

[64] Christian Mossin. Flow Analysis of Typed Higher-Order Programs. PhD thesis, DIKU,

Department of Computer Science, University of Copenhagen, 1996.

[65] Atsushi Ohori. A polymorphic record calculus and its compilation. Transactions on

Programming Languages and Systems, 17(6):844-895, November 1995.

[66] Jukka Paakki. Attribute grammar paradigms--a high-level methodology in language

implementation. ACM Computing Surveys, 27(2):196-256, June 1995.

[67] Jens Palsberg and Patrick O'Keefe. A type system equivalent to flow analysis. Transactions on Programming Languages and Systems, 17(4):576-599, July 1995.

[68] Jens Palsberg and Michael I. Schwartzbach. Object-oriented type inference. In Proceedings of the ACM Conference on Object-Oriented programming: Systems, Languages,
and Applications, pages 146-61, October 1991.

[69] Jens Palsberg, Mitchell Wand, and Patrick O'Keefe. Type inference with non-structural

subtyping. Formal Aspects of Computing, 9(1):49-67, 1997.

[70] Fran,cois Pessaux and Xavier Leroy. Type-based analysis of uncaught exceptions. In

Conference Record of the 26th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 276-290, January 1999.

[71] Fran,cois Pottier. Simplifying subtyping constraints. In Proceedings of the SIGPLAN

'96 International Conference on Functional Programming (ICFP '96), number 31:6 in
SIGPLAN notices, pages 122-133, May 1996.

[72] Fran,cois Pottier. Type Inference in the Presence of Subtyping: From Theory to Practice.

PhD thesis, Universit'e Paris VII, July 1998.

[73] Didier R'emy. Typechecking records and variants in a natural extension of ML. In Conference Record of the Sixteenth Annual ACM Symposium on Principles of Programming
Languages, Austin, Texas, pages 60-76, January 1989.

[74] John C. Reynolds. Automatic Computation of Data Set Definitions, pages 456-461.

Information Processing 68. North-Holland, 1969.

196
[75] John A. Robinson. A machine-oriented logic based on the resolution principle. Journal

of the ACM, 12(1):23-41, 1965.

[76] Marc Shapiro and Susan Horwitz. Fast and accurate flow-insensitive points-to analysis.

In Conference Record of the 24th Annual ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages, pages 1-14, January 1997.

[77] Oded Shmueli. Dynamic cycle detection. Information Processing Letters, 17(4):185-

188, 8 November 1983.

[78] Ryan Stansifer. Type inference with subtypes. In Conference Record of the 15th Annual

ACM Symposium on Principles of Programming Languages, pages 88-97, January 1988.

[79] Bjarne Steensgaard. Points-to analysis in almost linear time. In Conference Record of

the 23rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, pages 32-41, January 1996.

[80] Jean-Pierre Talpin and Pierre Jouvelot. Polymorphic type, region and effect inference.

Journal of Functional Programming, 2(3):245-271, July 1992.

[81] Satish Thatte. Type inference with partial types. In Automata, Languages and Programming: 15th International Colloquium, pages 615-629. Springer-Verlag Lecture
Notes in Computer Science, vol. 317, July 1988.

[82] Mads Tofte and Jean-Pierre Talpin. Implementation of the typed call-by-value *-

calculus using a stack of regions. In Conference Record of the 21st Annual ACM
SSymposium on Principles of Programming Languages, pages 188-201, January 1994.

[83] Valery Trifonov and Scott Smith. Subtyping constrained types. In Proceedings of

the 3rd International Static Analysis Symposium, volume 1145 of Lecture Notes in
Computer Science, pages 349-365. Springer Verlag, September 1996.

[84] Jan van Leeuwen. Graph Algorithms, chapter 10, page 542. Elsevier & MIT Press,

1992. In J. van Leeuwen, editor, Handbook of Theoretical Computer Science, Volume
A, Algorithms and Complexity.

[85] G. A. Venkatesh. A framework for construction and evaluation of high-level specifications for program analysis techniques. In Proceedings of the 1989 ACM SIGPLAN
Conference on Programming Language Design and Implementation, number 24:7 in
SIGPLAN notices, pages 1-12, 1989.

[86] Mitchell Wand. Complete type inference for simple objects. In Proceedings of the 2nd

Annual IEEE Symposium on Logic in Computer Science (LICS'87), pages 37-44. IEEE
Computer Society Press, June 1987. Corrigendum, LICS'88, page 132.

[87] Mitchell Wand. Corrigendum: Complete type inference for simple objects. In Proceedings of the 3rd Annual IEEE Symposium on Logic in Computer Science (LICS'88),
page 132. IEEE Computer Society Press, July 1988.

197
[88] William E. Weihl. Interprocedural data flow analysis in the presence of pointers. In

Conference Record of the 7th Annual ACM Symposium on Principles of Programming
Languages, pages 83-94, January 1980.

[89] Pierre Weis, Mar'ia-Virginia Aponte, Alain Laville, Michel Mauny, and Asc'ander

Su'arez. The CAML reference manual, Version 2.6. Technical report, Projet Formel,
INRIA-ENS, 1989.

[90] Andrew K. Wright. Polymorphism for imperative languages without imperative types.

Technical Report 93-200, Rice University, February 1993.

[91] Kwangkeun Yi. Compile-time detection of uncaught exceptions for Standard ML programs. In Proceedings of the 1st International Static Analysis Symposium, volume
864 of Lecture Notes in Computer Science, pages 238-254. Springer Verlag, September
1994.

[92] Kwangkeun Yi and Williams Ludwell Harrison, III. Automatic generation and management of interprocedural program analyses. In Conference Record of the 20th Annual
ACM-SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages
246-259, January 1993.

[93] Kwangkeun Yi and Sukyoung Ryu. Towards a cost-effective estimation of uncaught

exceptions in SML programs. In Proceedings of the 4th International Static Analysis
Symposium, volume 1302 of Lecture Notes in Computer Science, pages 98-113. Springer
Verlag, September 1997.

198
Table of Notations

Notation Explanation
S the set of sorts
s,t a sort
c,d a syntactic constructor
X ,Y,Z a variable of any sort
E a mixed expression or set-expression
V a semantic domain
v; w an element of a semantic domain
f; g a function value of a semantic domain
I; J; K; X; Y; Z an ideal
I(D) the collection of ideals of D
L the universal set of labels
A; B; C a finite set of labels
N a finite or cofinite set of labels
S a constraint set
\Gamma  a collection of constraint sets
ffi a domain-complement expression
ffX a domain-complement variable
ff(E) the domain-complement of Row-expression E