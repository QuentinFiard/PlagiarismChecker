

STORING   A  SPARSE   TABLE

bY
Robert   Endre  Tarjan

STAN-CS-78-683DECEMBER   1978
COMPUTER   SCIENCE   DEPARTMENTSchool  of  Humanities  and  Sciences

STANFORD  UNIVERSITY

Storing a Sparse Table

Robert Andre Tarjan*f
Computer Science DepartmentStanford University

Stanford, California 94305

August, 1978

Abstract.

The problem of storing and searching large sparse tables arises in
compiling and in other areas of computer science. The standard technique

for storing such tables is hashing, but hashing has poor worst-case
performance. We consider good worst-case methods for storing a table of

n entries, each an integer between 0 and N-l . For dynamic tables,

in which look-ups and table additions are intermixed, the use of a trie
requires oh4 storage and allows o(logk(N/n)) worst-case access time,

where k is an arbitrary parameter. For static tables, in which the entire
table is constructed before any look-ups are made, we propose a method

which requires (  )RO(n log n) storage  and allows O(f log, N) access time,

where R is an arbitrary parameter. Choosing  R = log* n gives a method

with O(n) storage and O((log* n)(logn N)) access time.

CR Categories: 4.34, 3.74, 4.12, 5.25

f* This research was supported in part by National Science Foundation

grant MC!S75-22870-A02  and by Office of Naval Research contract~oool4-76-c-0688. Reproduction in whole or in part is permitted

for any purpose of the United States government.

1. Introduction.

The following table searching problem arises in many areas of computer
science. Given a universe of N names and an initially empty table, we
wish to be able to perform two operations on the table:

enter(x): Add name x (and possibly some associated information)

to the table.
lookup(x): Discover whether x is present in the table, and if it

is, retrieve the information associated with it.

Compilers require such a table to store names of variables  [2]. Methods

for LR parsing [2], sparse Gaussian elimination  [6], and finding
equivalent expressions [31, require such a table to store ordered pairs
of integers.

In considering this problem we shall distinguish between the dynamic- -
case, in which entries and  lookups are intermixed, and the static case
in which all entries occur before all  lookups. We shall use a random

access machine with uniform cost measure [1] as the computing model,
We assume that the names are integers in the range 0 through  ~-1 and

that each storage cell in the machine can store  an integer of magnitude
O(N)  '

An ideal solution to the table searching problem would be a method
which requires O(1) time per operation and which does not require

substantially more than oh-4 space, where n is the total number of
entries made in the table. If we use an array of size  N to store the
table, each operation requires O(1) time, but the storage is excessive

if  n<<N. (Note that the solution to exercise 2.12 in  [l] allows

us to avoid initializing the array.) If we use a balanced binary tree

[4] or similar structure to store the table, the storage is O(n) but
each operation requires O(log n) time. The best method in many practical
situations is the use of a hash table [4], which requires O(n) space to

store the table and achieves an O(1) time bound per operation on the
average, though not in the worst case.

Although for most practical purposes hashing solves the table lookup
problem, it is of interest to know how far the storage required for the

table can be reduced while maintaining an O(1) worst-case time bound
per table access. Reduction of the storage to O(n+ G) , for instance,
would allow storage of a & x & matrix with n entries in O(n+  & )

space with O(1) access time. If the method is simple enough we  rnw be
able to beat hashing for some applications. Surprisingly little work
has been done on this problem; see for example  [5].

In this paper we examine two good worst-case methods of storing sparse
tables. For the dynamic case, a trie data structure [4] requires m4

storage while allowing o(logk(N/n)) access time, where k is a parameter
whose value is chosen in advance. The method supports table deletions as
well as insertions. We discuss this method in Section 2.

In Section 3 we present a more sophisticated method which handles the
static case. By precomputing the storage scheme before beginning the

lookups,  the method achieves an O(n log (  1R n) f* storage bound with

O(8 log, N) access time, where R is a parameter whose value is fixed

* -f**in advance. By choosing  R = log n we get a method

*f 1% (1) = log2 n  ; log(j+')  n  = log(i)(log n)

2  l
**I- log* n = min{i. \ log (i> nzl}.

3

with O(n) storage and o((log* n)@gn N)) access time. The method

combines the trie structure discussed in Section 2 with repeated application
of a method for compressing tables by using double displacements. This
double displacement method is an elaboration of a single displacement method

suggested in [2,7] for compressing parsing tables.

In Section 4 we mention some applications of our results and make a
few additional remarks.

2. Storing a Dynamic Table.

To store a dynamic table, we use a trie [ 41 with n-way branching
at the root and k-way branching at every other node, where k > 2 isan integer whose value is selected in advance. Each node in the trie
contains one table name and either n or k pointers to nodes one level

deeper in the trie. (Some or all of the pointers may be null.) Figure 1
gives  an example of such a data structure.

[Figure  11
To look up a name x in the trie, we divide x by n and then
repeatedly by k . We use the successive remainders to specify a search
path in the  trie. For instance, to search for 190 in the trie of
Figure 1, we look for 190 in the root. Not finding it, we divide 190
by 8 , leaving 23 with remainder  6 , which leads us to node e .

Again not finding 190 , we divide  23 by 4 to get 5 with remainder  3 .

This leads us to node i , where we find 190 . To insert a name in the
trie, we first search for it. The search leads to an external node,
which we place a pointer to a new node containing the new name. See

Figure 1.

in

Our tries differ from those discussed by  Kinuth [4 ] only in that we
allow the root to have a higher branching factor than the other nodes;
this reduces the time required by the method without increasing the space
bound, but requires that we know n (at least approximately) before we

begin to construct the table. It is straightforward to implement the
method, and we leave the details as an exercise. Note that by choosing
the branching factors to be powers of two, we can replace division by

shifting, and we can allocate space for the pointers out of a single array,
avoiding initialization by using the solution to exercise 2.12 in 111.

5

The total space required by the method is o(~) in the worst case.
The time required for either a look-up or an insertion is proportional
to the length of the search path, which is rlogk(N/d  1 in the worst

case. On the  average> the method requires O(1) look-up and insertion
time, since it is at least as fast (ignoring constant factors) as hashing
with separate chaining  [4 1.

If we add to each trie node a list of the non-null pointers in it,
then our data structure will support deletions. To delete a given table

entry, we first search for the node containing it, say p . We then
locate some external node q which is a descendant of p . We replace
the entry in p by the entry in q and delete node q . If P itself

See Figure 1. With careful

(N' n )) time in the worst case
is an external node, we merely delete p .

implementation this method requires '(logk
for a deletion.

3. Storing a Static Table.

Section 2 shows that by using tries the worst-case time to access a
table can be decreased as much as desired, at the expense of additional

storage. If the table to be stored is static, i.e., all the entries take
place before all the look-ups, then we can improve the method of Section 2

substantially. We shall show that for an arbitrary value of  1 , it is
possible to store n entries selected out of N in (1)O(n log n) space
with o(a log, N) access time.

For simplicity we shall assume that N is a perfect square, i.e.,
N = m2 for some integer m . We can represent the table to be accessed
by an mxm array A . Position (iA> in the array corresponds to

name k , where i  = ik/m]+l  and j  = k mod m  + 1 . Position  (i,j)

contains the information associated with k if k is present in the
table and contains zero if k is absent from the table.

We shall describe a method for compressing A into a smaller
array C , by giving a mapping from positions in A to positions in C

such that no two non-zeros in A are mapped to the same position in C .
Our mapping is defined by a displacement r(i) for each row i  ;
position  (i,j) in A is mapped into position  r(i)+j in C . The

idea is to overlap the rows of A so that no two non-zeros end up in
the same position. See Figure 2.

[Figure  21
Each entry in C indicates the position in A (if any) mapped
to that position in C , along with any associated information. To

look up a name k , we compute i  = Lk/mJ+l and j = k mod m + 1 .

If C(r(i)+j) contains k , we retrieve the associated information.

If not, we know k is not in the table. The access time with this

7

method is

plus space

Ullman  [2]

00)  ; the storage required is m for the row displacements
proportional to the number of positions in C . Aho and

and Ziegler [7] advocate this scheme as a way of compressing
parsing tables, but they provide no analysis.

To use this method, we need a way to find a good set of displacements.
Ziegler suggests the following "first-fit" method: Compute the row
displacements for rows 1 through m one-at-a-time. Select as the
row displacement r(i) for row i the smallest value such that no
non-zero in row i is mapped to the same position as any non-zero in

a previous row. An even better method, also suggested by Ziegler, is
to sort the rows in decreasing order by their number of non-zeros and
then apply them first-fit. We shall employ this "first-fit decreasing"
method. See Figure 2.

Theorem 1. Suppose the array A has the following "harmonic decay"
property:

(H) For any 1 J the number of non-zeros in rows with more than  R

non-zeros is at most n/(1+1)  .

Then every row displacement r(i) computed for A by the first-fit
decreasing method satisfies 0 < r(i) < n .-

Proof. For any row i , consider the choice of r(i) . Suppose r(i)
contains 1 > 1 non-zeros.- By (H) the number of non-zeros in previous
rows is at most n/R . Each such non-zero can block at most ,t choices
for r(i) . Altogether at most n choices are blocked, and
0 < r(i) < n . a- -

8

The following algorithm is a straightforward implementation of the
first-fit decreasing method. Input to the algorithm is a list of the
non-zero positions in A .

First-Fit Decreasing Algorithm.

Step 1: for i := 1 untilm dofor each non-zero position do
-X one to count(i);  pZ j in list(i)  ,oA;

Step 2: p&c := 0 en Ebucket(c) := fi od;

for i := - -<- 1 until m  dz put i in  bucketcount( z;

Step 3: for k := W-.- 0 z n+m-1  do- entry(k) := false od;

for c := n step -1 until 0 do- e  Tw

for eaxin bucket(c) do-m r(i) := 0;
check overlap: for each j in list(i) doWW'

if entry(r(i)+j)  Ken- - - "

r(i) := r(i)+l;  go to check overlap fi od;.- ,- - *-
for each j in list(i) do-W entry(r(i)+j) := true od od od;rvvvvIw,-Ivv

After Step 1, list(i) is a list of the non-zero columns in row i- -
and count(i) is a count of these non-zeros. Step 2 is a radix sort of
the rows by their number of non-zeros. The initialization in Step 3

assumes that A has harmonic decay, which is the case in which we shall
be interested. If A does not have harmonic decay, more space must be

allocated for C .

Theorem 2. If A has harmonic decay, then the first-fit decreasing

algorithm requires  O(n2+m) time to compute row displacements for A ,

9

Proof. Steps 1 and 2 and the initialization in Step  3 require O(n+m)

time. For 1 < i < m , let row i contain  li non-zeros. Then the-  -
time to compute the displacement for row i is  O(n,k) , and the total

time to compute row displacements is O(iitjl  nQ+ m)  =  O(n2+m) .  a

If the array A has harmonic decay, then the row displacement
method provides O(1) -time table access while requiring only  n+2m-1

storage, not counting storage of the information associated with each
name. If A does not have harmonic decay, we must smooth out the

distribution of non-zeros among the rows of A before computing row
displacements. To accomplish this we apply to A a set of column
displacements c(j) , mapping each position (id into a new position

(i+c(j>,j! . This transforms A into a new array B with an increased
number of rows (namely max c(j) + m-l ) but with the same number ofi

columns. See Figure  3.

[Figure   31
We choose the column displacements so as to satisfy an exponential
decay condition defined as follows. Let B.3 be the array consisting of
the first j shifted columns of A . Let n. be the total number of3

non-zeros in B. . Let n.. be the number of non-zeros in B. which3 iJ 3

appear in rows of B. containing more than i non-zeros. Let b be3
an arbitrary integer.

Ej(b): For 0 < i < b , n..  < n./2

i(2-nj/n) .

- - iJ- J

Note that Em(410g2 n_]) implies  B = Bm has harmonic decay. To
satisfy Ej(b) for all j 9 we employ the first-fit method aa follows:

10

Compute the displacements for columns 1 through m one-at-a-time.

Select as the column displacement c(j) for column j the smallest
value such that B. satisfiesJ Ej(b)  ' See Figure  3.

Theorem 3. The set of column displacements c(j) computed by the first

fit method to satisfy  Ej(b)  for all j is such that
0 < c(j) <  4n log2 b + O(n) for  l<j<m.- - -  -

Proof. For any column j , consider the situation when c(j) is chosen.

In order for a possible choice of c(j) to violate  Ej(b)  , there must

be some i such that n i(2-nj/n)ij > nj/2 . Since E j l(b) holds,

i(2-n  /n)n.
lj-1 5  nj-l/ 2

j-l . Each row of B. with i non-zeros in the

J

first j-l colwnns and an additional non-zero in column j contributes

i+l to  n.  .-n Each row of B. with more than i non-zeros in1-J ij-1 ' J
the first j-l columns and an additional non-zero in column j contributes
1 to n..-n Thus there must be more than1-J ij-1 l

i(2-nj/n) i(2-n  /n)
nj/2 -  nj-lI 2 j-l (i+l) rows in B. with more thanJ

i-l non-zeros in the first j-l columns and an additional non-zero in
column j . Since column j contains exactly n -nj j-l non-zeros, i>O.
We also have

(

i(2-nj/n) i(2-n  /n)
nj/2 -  nj-lI 2 j-l )/(i+l)

i(n -n
( I

i(2-n  /n) j j-l )/n>
nj-l 2 j-l- )C Y2 - 1nj-l )/(i+l)

(

i(2-n
I j-l

`4 i(n -n )/n>

nj-l 2 >( 2 j j-l - 1- >/(i+l)

(

i(2-n  /n)>
nj-lI 2 j-l . > (  (- 1 nj-njWl)(ln  2)/n)  / (i+l)

> (in.J-l(nj-nj-l) In 2)' (2i(2-nj-1'n)n  (i+l)) .-
Consider the set of ordered pairs whose first element is a row of
Bj-l with more than i-l non-zeros and whose second element is a non-zero

of column j . There are at most ( )I .ni-lj-1  nj-nj-l IL such pairs. Each

choice of c(j) for which nij > nj/2 i(2-nj/n) accounts for more than

( .1 njel(nj-njSl)  In 2) A

i(2-n.  /n)2 '-' n  (i+l) distinct pairs. Thus

>

the number of choices of c(j) for which nij > nj/2 i(2-nj/n) is
bounded by

i(2-n  /n)
ni-lj-l  nj-nj-l( >2 j-l n (i+l)

i2 nj-l(nj-nj-l) In 2

<-

i(2-n  /n)
nj-l 2 j-l n (i+l)(i-l (2-n

2 j-l' 4 i2 nj-l In 2

bY  E-j-l(b)

12

3
(2 I  )<  2 -nj-l   n  n  i+l

i2 In 2 < (4

log2 e)n(i+l)/i2  .

c

Xumming over i, we find that at most

b
. C  (4  log2  e)n(i+l)/i2l=l 5 (4 log2 e)n(Jn b + 1 +  n2/6)

< 4n log2  b  + O(n.

I  6

choices of c(j) are blocked, and 0 < c(j)  < 4n log2 b + O(n) . 0-

It is not hard to implement the first-fit method so that it computes
column displacements to satisfy  Ej(b) for all j in  O(n2+m) time.
We leave the details to the reader.

By combining row and column displacements, we obtain the following
table storage scheme.

Table Construction.

Step 1. Construct a set of column displacements c(j) for array A

by using the first-fit method to satisfy  Ej(Llog2  n]) for

all j . Compute the transformed array  B .
Step 2. Construct a set of row displacements r(i) for B by using

the first-fit decreasing method. Construct the transformed

array C .

Table Look-up.

Let k be the name to be accessed. Compute i  = Lk/m]+l  ,

j = k mod m + 1 , and k*  = r(i+c(j))+j . If C(k*) contains k ,
retrieve the associated information. If not, k is not in the table.

13

With this method, the access time is O(1) , the storage is  m.
for the column displacements plus 4n log2 log2 n + m + O(n) for the
row displacements (by Theorem  3) plus n+m-1 for C (by Theorem l),

not including space required to store the information associated with

each name. The total space is thus 4n log2 log2 n + 3m + O(n) . The
time required to construct the storage scheme is  O(n2+m)  .

If we are willing to allow a little slower access time, we  can
further decrease the space required to store the table, We construct
not just one set of row and column displacements, but several. Each

set of displacements is used to compress a different part of the table.
To look up a name, we use each set of row and column displacements in
turn until either finding the name or running out of mappings to try.

The algorithm, described below, uses a parameter  ,k' whose value
determines the time-space trade-off.

Table Construction.

Initialization. 0)Let  bl  =  [log  nl and for 2 <h<  R,-  -

let bh = 2 uh-l . Let  Al be an array representing the table to be

stored. For h from 1 to 1 , repeat the following steps.

Step 1. Construct a set of column displacements  ch(j) for Ah

by using the first-fit method to satisfy  Ej(bh) for all j .

Compute the transformed array  Bh .
Step 2. For each row i of Bh containing more than b h

non-zeros, let rh(i) = fl . Construct a set of row displacements

r,(i) for the remaining rows of Bh (those containing at
most bh non-zeros) by using the first-fit method. Construct

the transformed array  Ch for these rows.

14

Step 3. Form a new array 52+1 by replacing with a zero each

non-zero in Ah mapped to a position in  Ch . Oh e non-zeros

replaced are exactly those mapped into rows of  Bh with at
most bh non-zeros.)

Table Look-up.

Let k be the name to be accessed. Compute i = Lk/m J+l and

j = k mod m + 1 . Let h be minimum such that  rh(i+ch(j)) f n .
Compute k*  = rh(i+ch(j))+j  . If Ch(k*) contains k , retrieve

the associated information. Otherwise, k is not in the table.

This multiple displacement method requires O(n2+ 1m) time to
construct the table and allows O(1) access time. The next theorem
bounds the space required.

Theorem 4. The multiple displacement method requires O(n log('+l)  n+ 1m)

space to store the table.

Proof. For  l<h<  R,  __  - bh > rlog('-h+l)  nl . In particular
bR 2 [log2 nl . Furthermore, since Bh satisfies Em(bh) , at most

bh42 non-zeros appear in rows of Bh containing more than bh

bhnon-zeros. This means
%-l+1 contains at most n/2 = n/bh+l non-zeros.

The storage required for the first set of displacements is m for

the row displacements plus O(n log c&+1) n)+m for the column

displacements (by Theorem  3) plus O(n)+m for  Cl . For  2<h<R,-  -
the storage required for the h-th set of displacements is m for the

row displacements plus o( (n/bh) 1% bh) + m for the column displacements

15

plus O(n/bh)+m for Ch . Summing over h we find that the total

storage is O(n log@+')  n + 1m) . Ll

We now combine the multiple displacement method with the tree
structure of Section 2 to obtain a static table storage method good
for arbitrary values of n and N . Our first step is to construct
a trie as in Section 2 with k  = L,n]-1 .r The trie has  O(logn N)
depth and contains n+(n-l)(L"i;;J  -1) < n312 pointers, of which

only n-l are non-null. We can regard the pointers in this trie as
consisting of a table of n-l entries selected from
names; O(logn N) look-ups of pointers in this table

n312 possible

are required
to look up an entry in the original table. We use the multiple
displacement scheme with m  = [n3/4 1 to store the pointer table.
We thus obtain a method which requires  O(n log (0 n) storage space
-and allows O(m logn N) access time. If m grows only polynomially

with n, the access time is  O(a) . Choosing 1 = log* n gives

an O(n) -space method with O((log*  n)(logn N)) access time.

16

4. Remarks.

There are several possible applications of our table storage schemes.
The dynamic algorithm of Section 2 can be used to keep track of the

fill-in when carrying out sparse Gaussian elimination  [6] and to keep
track of signatures when finding equivalent expressions  [3], The static

algorithm of Section  3 can be used to store tables for LR parsing  [2,7].

In all these applications N  = O(n2) . Although we have not studied
the practicality of our methods, they are simple enough to be competitive
with hashing in some situations. Indeed, the row displacement method

described in Section 3 has been proposed as a practical way to store
parsing tables  [2,7]. It is important to note that our bounds are
worst-case and that the worst cases are unlikely in practice.

Our algorithms make use of array storage; they cannot be implemented
using only list structures as storage. Thus they indicate a difference

in power between random access machines and pointer machines. They also

suggest a time-space trade-off for the table storage problem, at least
in the dynamic case. Whether such a time-space trade-off exists is a

question deserving further study. For the static case, an affirmative
answer to the following question would imply the existence of an

O(n) -space, O(logn  N) -access time storage scheme:

Is there a constant c such that, for any  mxm array A
containing n non-zeros, there is a set of column displacements

selected from rO,l,2,...,cnJ for which the transformed array  B
has harmonic decay?

17

Acknowledgment.

My thanks to Yossi Shiloach for extensive discussions which contributed
greatly to the ideas presented here.

18

3
References
[l] A. V. Aho, J. E. Hopcroft, and J. D. Pullman, The Design and Analysis

of Computer Algorithms, Addison-Wesley, Reading, Mass.,  1974.
[2] A. V.  Aho and J. D.  Ullman, Principles of Compiler Design,  AddisonWesley, Reading, Mass.,  1977.
[3] P. J. Downey, R. Sethi, and R. E. Tarjan, "Variations on the common

subeqression problem," submitted to Journal ACM.
[4] D. E. Knuth, The Art of Computer Programming, Volume  3: Sorting

and Searching, Addison-Wesley, Reading, Mass.,  1973.
[5] R. Sprugnoli, "Perfect hashing functions: a single probe retrieving

method for static sets," comrn. ACM 20  (1977), 841-849.
[6] R. E.  Tarjan, "Graph theory and Gaussian elimination," Sparse Matrix

Computations, J. R. Bunch and D.  J. Rose, eds., Academic Press,
New York  (1976), 3-22.
[7] S. F. Zeigler, "Smaller faster table driven parser," unpublished

manuscript (19771.

0
1
2
3
4
5
6
7

b

Fig
ure
 1.

A 
tri

e w

ith
 m 
= 5
12 
,
8-w
ay 
bra
nch
ing
 at
 th
e r
oot
, 4
-wa
y b
ran
chi
ng 
els
ewh
ere
, a
nd 
ent
rie
s

3) 
81
,1
30
,1
50
, 
17
4,1

90
, 

203

, 2
55
 . 

To
 in

ser
t 
30 

, 
we 

div
id
e 3

0 
by 

8 a
nd
 re

pe
ate

dly

6 3
 

0

by 
4 ,
 gi
vin
g 3
0 
-+ 3

 -+ 
0 -
+ 0
 .

The
 re
mai
nde
rs 
6

an
d 3

 le
ad
 to

 n
ode

 i 
, 
whe

re
 we

ins
ert
 a 
new
 no
de 
as 
ind
ica
ted
.

To 
del
ete
 15
0 ,

we 
loc
ate
 it
 in
 no
de

e ,
 lo
cat
e a
n

ext
ern
al 
nod
e w
hic
h i
s a
 de
sce
nda
nt 
of 
e ,
 sa
y h
 , 
rep
lac
e

15
0 
in

e 
by
 1
74
 ,
 a
nd

del
ete
 no
de 
h .

1  -*--*
2 *  *  - *  -
3  *-*-,

4  ,,-*-
5  *---*

A

*  -  - r(1) = 1
*  *  - *  - r(2) = 0

*  - *  -  - r(3) = 4-

I- *  - r(4) =  5

*  -  -  - r(5) = 7

I 5 61 8 10  4 12 20  13  - 724

C

Figure 2. Row displacements computed for an array using "first-fit

decreasing" strategy. Asterisks denote non-zeros;
dashes denote zeros. Each position in array C contains
the position in A (if any) mapped to that position

in C . Positions in A are numbered row-by-row starting
from zero. Row displacements are computed in the order

2,1,3,5,4 l

21

1 2 3  4  5
A

B

* *
* * *

* *

*
* *

*
*

:*

c(1) = 0

c(2) = 5
c(3) = 0

c(4) = 6

c(5) = 0

Figure  3. Column displacements computed using first-fit to satisfy

Ej(Llog2 n]) for all j . This constraint requires no
rows with more than one non-zero in B2 ' at most one

such row in B3 and B4 , and at most two such rows
in B5

l

22