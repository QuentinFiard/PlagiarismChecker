

A Certified Type-Preserving Compiler from Lambda Calculus toAssembly Language *

Adam Chlipala
University of California, Berkeley

adamc@cs.berkeley.edu

Abstract
We present a certified compiler from the simply-typed lambda cal-culus to assembly language. The compiler is certified in the sense

that it comes with a machine-checked proof of semantics preser-vation, performed with the Coq proof assistant. The compiler and
the terms of its several intermediate languages are given depen-dent types that guarantee that only well-typed programs are representable. Thus, type preservation for each compiler pass fol-lows without any significant "proofs" of the usual kind. Semantics
preservation is proved based on denotational semantics assigned tothe intermediate languages. We demonstrate how working with a
type-preserving compiler enables type-directed proof search to dis-charge large parts of our proof obligations automatically.

Categories and Subject Descriptors F.3.1 [Logics and meaningsof programs]: Mechanical verification; D.2.4 [Software Engineering]: Correctness proofs, formal methods, reliability; D.3.4 [Pro-gramming Languages]: Compilers

General Terms Languages, Verification
Keywords compiler verification, interactive proof assistants, de-pendent types, denotational semantics

1. Introduction
Compilers are some of the most complicated pieces of widely-usedsoftware. This fact has unfortunate consequences, since almost all

of the guarantees we would like our programs to satisfy dependon the proper workings of our compilers. Therefore, proofs of
compiler correctness are at the forefront of useful formal methodsresearch, as results here stand to benefit many users. Thus, it is
not surprising that recently and historically there has been muchinterest in this class of problems. This paper is a report on our foray
into that area and the novel techniques that we developed in theprocess.

One interesting compiler paradigm is type-directed compi-lation, embodied in, for instance, the TIL Standard ML compiler [TMC+96]. Where traditional compilers use relatively type* This research was supported in part by a National Defense Science and Engineering
Graduate Fellowship, as well as National Science Foundation grants CCF-0524784and CCR-0326577.

Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citationon the first page. To copy otherwise, to republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee.
PLDI'07 June 11-13, 2007, San Diego, California, USA.Copyright cfl 2007 ACM 978-1-59593-633-2/07/0006.. . $5.00.

impoverished intermediate languages, TIL employed typed inter-mediate languages such that every intermediate program had a typing derivation witnessing its safety, up until the last few phases ofcompilation. This type information can be used to drive important
optimizations, including nearly tag-free garbage collection, wherethe final binary comes with a table giving the type of each register
or stack slot at each program point where the garbage collector maybe called. As a result, there is no need to use any dynamic typing
scheme for values in registers, such as tag bits or boxing.Most of the intricacies of TIL stemmed from runtime passing
of type information to support polymorphism. In the work wepresent here, we instead pick the modest starting point of simplytyped lambda calculus, but with the same goal: we want to compilethe programs of this calculus into an idealized assembly language
that uses nearly tag-free garbage collection. We will achieve thisresult by a series of six type-directed translations, with their typed
target languages maintaining coarser and coarser grained types aswe proceed. Most importantly, we prove semantics preservation
for each translation and compose these results into a machine-checked correctness proof for the compiler. To our knowledge,
there exists no other computer formalization of such a proof fora type-preserving compiler.

At this point, the reader may be dubious about just how involveda study of simply-typed lambda calculus can be. We hope that the
exposition in the remainder of this paper will justify the interestof the domain. Perhaps the key difficulty in our undertaking has
been effective handling of variable binding. The POPLmark Chal-lenge [ABF

+05] is a benchmark initiative for computer formalization of programming languages whose results have highlighted justthat issue. In the many discussions it has generated, there has yet

to emerge a clear winner among basic techniques for representinglanguage constructs that bind variables in local scopes. Each of the
proposed techniques leads to some significant amount of overheadnot present in traditional proofs. Moreover, the initial benchmark
problem doesn't involve any component of relational reasoning,crucial for compiler correctness. We began this work as an investigation into POPLmark-style issues in the richer domain of rela-tional reasoning.

1.1 Task Description
The source language of our compiler is the familiar simply-typedlambda calculus, whose syntax is:

Types o/ ::= N | o/ ! o/
Natural numbers nVariables

x, y, zTerms

e ::= n | x | e e | *x : o/. e

Application, the third production for e, associates to the left,as usual. We will elaborate shortly on the language's semantics.

Our target language is an idealized assembly language, with in-finitely many registers and memory cells, each of which holds unbounded natural numbers. We model interaction with a runtime sys-tem through special instructions. The syntax of the target language
is:

Registers rOperands

o ::= r | n | new h~r, ~ri | read hr, niInstructions

i ::= r := o; i | jump r

Programs p ::= h~i, ii

As instruction operands, we have registers, constant naturals,and allocation and reading of heap-allocated records. The

newoperand takes two arguments: a list of root registers and a list of

registers holding the field values for the object to allocate. Thesemantics of the target language are such that

new is allowed toperform garbage collections at will, and the root list is the usual

parameter required for sound garbage collection. In fact, we willlet

new rearrange memory however it pleases, as long as the resultis indistinguishable from having simply allocated a new record,

from the point of view of the specified roots. The read instructiondetermines which runtime system-specific procedure to use to read
a given field of a record.A program is a list of basic blocks plus an initial basic block,
where execution begins. A basic block is a sequence of registerassignments followed by an indirect jump. The basic blocks are
indexed in order by the natural numbers for the purposes of thesejumps. We will additionally consider that a jump to the value 0
indicates that the program should halt, and we distinguish oneregister that is said to hold the program's result at such points.

We are now ready to state informally the theorem whose proofis the goal of this work:

THEOREM 1 (Informal statement of compiler correctness). Givena term

e of the simply-typed lambda calculus of type N, the compi-lation of

e is an assembly program that, when run, terminates withthe same result as we obtain by running

e.

Our compiler itself is implemented entirely within the Coqproof assistant [BC04]. Coq's logic doubles as a functional programming language. Through the program extraction process, themore exotic features of a development in this logic can be erased
in a semantics-preserving way, leaving an OCaml program that canbe compiled to efficient object code. In this way, we obtain a traditional executable version of our compiler. We ignore issues of pars-ing in this work, so our compiler must be composed with a parser.
Assuming a fictitious machine that runs our idealized assemblylanguage, the only remaining piece to be added is a pretty-printer
from our abstract syntax tree representation of assembly programsto whatever format that machine requires.

1.2 Contributions
We summarize the contributions of this work as follows:

* It includes what is to our knowledge the first total correctness

proof of an entire type-preserving compiler.*

It gives the proof using denotational semantics, in contrast tothe typical use of operational semantics in related work.

* The whole formal development is carried out in a completely

rigorous way with the Coq proof assistant [BC04], yieldinga proof that can be checked by a machine using a relatively

small trusted code base. Our approach is based on a generalmethodology for representing variable binding and denotation
functions in Coq.*
We sketch a generic programming system that we have devel-oped for automating construction of syntactic helper functions

over de Bruijn terms [dB72], as well as generic correctnessproofs about these functions. In addition, we present the catalogue of generic functions that we found sufficient for this work.*
Finally, we add to the list of pleasant consequences of mak-ing your compiler type-directed or type-preserving. In particular, we show how dependently-typed formalizations of type-preserving compilers admit particularly effective automated
proof methods, driven by type information.

1.3 Outline
In the next section, we present our compiler's intermediate lan-guages and elaborate on the semantics of the source and target languages. Following that, we run through the basic elements of im-plementing a compiler pass, noting challenges that arise in the process. Each of the next sections addresses one of these challengesand our solution to it. We discuss how to mechanize typed programming languages and transformations over them, how to usegeneric programming to simplify programming with dependentlytyped abstract syntax trees, and how to apply automated theoremproving to broad classes of proof obligations arising in certification of type-preserving compilers. After this broad discussion, wespend some time discussing interesting features of particular parts
of our formalization. Finally, we provide some statistics on our im-plementation, discuss related work, and conclude.

2. The Languages
In this section, we present our source, intermediate, and targetlanguages, along with their static and dynamic semantics. Our

language progression is reminiscent of that from the paper "FromSystem F to Typed Assembly Language" [MWCG99]. The main
differences stem from the fact that we are interested in meaningpreservation, not just type safety. This distinction makes our task
both harder and easier. Naturally, semantics preservation proofs aremore difficult than type preservation proofs. However, the fact that
we construct such a detailed understanding of program behaviorallows us to retain less type information in later stages of the
compiler.The mechanics of formalizing these languages in Coq is the subject of later sections. We will stick to a "pencil and paper formaliza-tion" level of detail in this section. We try to use standard notations
wherever possible. The reader can rest assured that anything thatseems ambiguous in the presentation here is clarified in the mechanization.

2.1 Source
The syntax of our source language (called "Source" hereafter) wasalready given in Section 1.1. The type system is the standard one

for simply-typed lambda calculus, with judgments of the form
\Gamma  ` e : o/ which mean that, with respect to the type assignmentto free variables provided by

\Gamma , term e has type o/.Following usual conventions, we require that, for any typing

judgment that may be stated, let alone verified to hold, all freeand bound variables are distinct, and all free variables of the term
appear in the context. In the implementation, the first condition isdischarged by using de Bruijn indices, and the second condition is
covered by the dependent typing rules we will assign to terms.Next we need to give Source a dynamic semantics. For this and
all our other languages, we opted to use denotational semantics.The utility of this choice for compiler correctness proofs has been
understood for a while, as reifying a program phrase's meaning asa mathematical object makes it easy to transplant that meaning to
new contexts in modeling code transformations.

Our semantics for Source is:

[[o/]] : types ! sets
[[N]] = N
[[o/1 ! o/2]] = [[o/1]] ! [[o/2]]

[[\Gamma ]] : contexts ! sets

[[*]] = unit
[[\Gamma , x : o/]] = [[\Gamma ]] * [[o/]]

[[e]] : [\Gamma  ` e : o/] ! [[\Gamma ]] ! [[o/]]
[[n]]oe = n

[[x]]oe = oe(x)
[[e1 e2]]oe = [[e1]]oe [[e2]]oe
[[*x : o/. e]]oe = *x : [[o/]]. [[e]](oe, x)

The type of the term denotation function (used in expressions ofthe form

[[e]]) indicates that it is a function whose domain is typingderivations for terms and whose range depends on the particular

\Gamma  and o/ that appear in that derivation. As a result, we definedenotations only for well-typed terms.

Every syntactic class is being compiled into a single meta lan-guage, Coq's Calculus of Inductive Constructions (CIC) [BC04].
However, we will not expect any special knowledge of this formalsystem from the reader. Various other type theories could be substituted for CIC, and standard set theory would do just as well forthis informal presentation.

We first give each Source type a meaning by a recursive trans-lation into sets. Note that, throughout this paper, we overload constructs like the function type constructor ! that are found in bothour object languages and the meta language CIC, in an effort to
save the reader from a deluge of different arrows. The variety ofarrow in question should always be clear from context. With this
convention in mind, the type translation for Source is entirely un-surprising. N denotes the mathematical set of natural numbers, as
usual.We give a particular type theoretical interpretation of typing
contexts as tuples, and we then interpret a term that has type o/ incontext

\Gamma  as a function from the denotation of \Gamma  into the denotationof
o/. The translation here is again entirely standard. For the sake ofconciseness, we allow ourselves to be a little sloppy with notations

like oe(x), which denotes the proper projection from the tuple oe,corresponding to the position

x occupies in \Gamma . We note that we usethe meta language's lambda binder to encode the object language's

lambda binder in a natural way, in an example closely related tohigher-order abstract syntax [PE88].

2.2 Linear
Our first compilation step is to convert Source programs into a formthat makes execution order explicit. This kind of translation is associated with continuation-passing style (CPS), and the compositionof the first two translations accomplishes a transformation to CPS.
The result of the first translation is the Linear language, and wenow give its syntax. It inherits the type language of Source, though
we interpret the types differently.

Operands o ::= n | x | *x : o/. eTerms

e ::= let x = o in e | throw hoi | x y z

Linear terms are linearized in the sense that they are sequencesof binders of primitive operands to variables, followed by either a

"throw to the current continuation" or a function call. The functioncall form shows that functions take two arguments: first, the normal

argument; and second, a function to call with the result. The func-tion and both its arguments must be variables, perhaps bound with
earlier lets.For reasons of space, we omit the standard typing rules for Linear and proceed to its denotational semantics. Recall, however, thatthe denotation functions are only defined over well-typed terms.

[[o/]] : types ! sets
[[N]] = N
[[o/1 ! o/2]] = [[o/1]] ! ([[o/2]] ! N) ! N

[[o]] : [\Gamma  ` o : o/] ! [[\Gamma ]] ! [[o/]]
[[n]]oe = n

[[x]]oe = oe(x)
[[*x : o/. e]]oe = *x : [[o/]]. [[e]](oe, x)

[[e]] : [\Gamma  ` o : o/] ! [[\Gamma ]] ! ([[o/]] ! N) ! N
[[let x = o in e]]oe = [[e]](oe, [[o]]oe)

[[throw hoi]]oe = *k. k([[o]]oe)

[[x y z]]oe = *k. oe(x) (oe(y)) (*v. k(oe(z)(v)))

We choose the natural numbers as the result type of programs.Functions are interpreted as accepting continuations that return

naturals when given a value of the range type. The let case relies onthe implicit fact that the newly-bound variable

x falls at the end ofthe proper typing context for the body
e. The most interesting caseis the last one listed, that for function calls. We call the provided

function with a new continuation created by composing the currentcontinuation with the continuation supplied through the variable

z.We use Linear as our first intermediate language instead of

going directly to standard CPS because the presence of distin-guished

throw terms makes it easier to optimize term representa-tion by splicing terms together. This separation is related to the

issue of "administrative redexes" in standard two-phase CPS trans-forms [Plo75].

2.3 CPS
The next transformation finishes the job of translating Linear intogenuine CPS form, and we call this next target language CPS.

Types o/ ::= Nat | ~o/ ! NOperands

o ::= n | x | *~x : ~o/. eTerms

e ::= let x = o in e | x ~y

Compared to Linear, the main differences we see are that func-tions may now take multiple arguments and that we have collapsed

throw and function call into a single construct. In our intended useof CPS, functions will either correspond to source-level functions
and take two arguments, or they will correspond to continuationsand take single arguments. Our type language has changed to reflect that functions no longer return, but rather they lead to finalnatural number results.

We omit discussion of the semantics of CPS, since the changesfrom Linear are quite incremental.

2.4 CC
The next thing the compiler does is closure convert CPS programs,hoisting all function definitions to the top level and changing those

functions to take records of their free variables as additional argu-ments. We call the result language CC, and here is its syntax.

Types o/ ::= N | ~o/ ! N | N ~o/ | ~o/ * ~o/ ! NOperands

o ::= n | x | hx, ~yi | ssixTerms

e ::= let x = o in e | x ~yPrograms
p ::= let x = (*~y : ~o/. e) in p | e

We have two new type constructors: N ~o/ is the multiple-argument product type of records whose fields have the types given
by ~o/. ~o/ * ~o/ ! N is the type of code pointers, specifying first thetype of the closure environment expected and second the types of
the regular arguments.Among the operands, we no longer have an anonymous function form. Instead, we have hx, ~yi, which indicates the creation ofa closure with code pointer

x and environment elements ~y. Theseenvironment elements are packaged atomically into a record, and

the receiving function accesses the record's elements with projec-tion operand form

ssix. This operand denotes the ith element ofrecord
x.While we have added a number of features, there are no surprises encountered in adapting the previous semantics, so we willproceed to the next language.

2.5 Alloc
The next step in compilation is to make allocation of products andclosures explicit. Since giving denotational semantics to higherorder imperative programs is tricky, we decided to perform "codeflattening" as part of the same transformation. That is, we move to
first-order programs with fixed sets of numbered code blocks, andevery function call refers to one of these blocks by number. Here is
the syntax of this language, called Alloc.

Types o/ ::= N | ref | ~o/ ! NOperands

o ::= n | x | hni | new h~xi | ssixTerms

e ::= let x = o in e | x ~yPrograms
p ::= let (*~y : ~o/. e) in p | e

The first thing to note is that this phase is the first in whichwe lose type information: we have a single type

ref for all heapreferences, forgetting what we know about record field types. To

compensate for this loss of information, we will give Alloc pro-grams a semantics that allows them to fail when they make "incorrect guesses" about the types of heap cells.Our set of operands now includes both natural number constants
n and code pointer constants hni. We also have a generic recordallocation construct in place of the old closure constructor, and we
retain projections from records.This language is the first to take a significant departure from its
predecessors so far as dynamic semantics is concerned. The keydifference is that Alloc admits non-terminating programs. While
variable scoping prevented cycles of function calls in CC and ear-lier, we lose that restriction with the move to a first-order form. This
kind of transformation is inevitable at some point, since our targetassembly language has the same property.

Domain theory provides one answer to the questions that arisein modeling non-terminating programs denotationally, but it is difficult to use the classical work on domain theory in the setting ofconstructive type theory. One very effective alternative comes in the
form of the co-inductive types [Gim95] that Coq supports. A gen-eral knowledge of this class of types is not needed to understand
what follows. We will confine our attention to one co-inductivetype, a sort of possibly-infinite streams. We define this type with
the infinite closure of this grammar:

Traces T ::= n | ? | ?, T
In other words, a trace is either an infinite sequence of stars ora finite sequence of stars followed by a natural number or a bottom

value. The first of these possibilities will be the denotation of a non-terminating program, the second will denote a program returning an
answer, and the third will denote a program that "crashes."Now we are ready to start modeling the semantics of Alloc.
First, we need to fix a representation of the heap. We chose anabstract representation that identifies heaps with lists of lists of

tagged fields, standing for the set of finite-size records currentlyallocated. Each field consists of a tag, telling whether or not it is
a pointer; and a data component. As we move to lower languages,these abstract heaps will be mapped into more conventional flat,
untyped heaps. We now define some domains to represent heaps,along with a function for determining the proper tag for a type:

C = {Traced, Untraced} * N
M = list (list C)
tagof(N) = Untraced
tagof(ref) = Traced
tagof(~o/ ! N) = Untraced

Next we give a semantics to operands. We make two differentchoices here than we did before. First, we allow execution of
operands to fail when a projection reads a value from the heapand finds that it has the wrong tag. Second, the denotations of
operands must be heap transformers, taking the heap as an extraargument and returning a new one to reflect any changes. We also
set the convention that program counter 0 denotes the distinguishedtop-level continuation of the program that, when called, halts the
program with its first argument as the result. Any other programcounter

n + 1 denotes the nth function defined in the program.

[[o]] : [\Gamma  ` o : o/] ! [[\Gamma ]] ! M ! (M * N) [ {?}
[[n]]oem = (m, n)

[[x]]oem = (m, oe(x))
[[hni]]oem = (m, n + 1)
[[new h~xi]]oem = (m \Phi  [oe(~x)], |m|)

[[ssix]]oem = if moe(x),i = (tagof(o/), v), then: (m, v)

else: ?

We write \Phi  to indicate list concatenation, and the notation oe(~x) todenote looking up in

oe the value of each variable in ~x, forming alist of results where each is tagged appropriately based on the type

of the source variable. The new case returns the length of the heapbecause we represent heap record addresses with their zero-based
positions in the heap list.Terms are more complicated. While one might think of terms as
potentially non-terminating, we take a different approach here. Thedenotation of every term is a terminating program that returns the
next function call that should be made, or signals an error with a ?value. More precisely, a successful term execution returns a tuple
of a new heap, the program counter of the function to call, and alist of natural numbers that are to be the actual arguments.

[[e]] : [\Gamma  ` e : o/] ! [[\Gamma ]] ! M!

(M * N * list N) [ {?}
[[let x = o in e]]oem = if [[o]]oem = (m0, v) then: [[e]](oe, v)m0

else: ?
[[x ~y]]oem = (m, oe(x), oe(~y))

Finally, we come to the programs, where we put our tracedomain to use. Since we have already converted to CPS, there is no
need to consider any aspect of a program's behavior but its result.Therefore, we interpret programs as functions from heaps to traces.
We write :: for the binary operator that concatenates a new elementto the head of a list, and we write

~xpc for the lookup operation thatextracts from the function definition list

~x the pcth element.

[[p]] : [\Gamma  ` let ~x in e] ! [[\Gamma ]] ! M ! Trace
[[let ~x in e]]oem = if [[e]]oem = (m0, 0, v :: ~n) then: v

else if [[e]]oem = (m0, pc + 1, ~n) then:

if ~xpc = *~y : ~o/. e0 and |~y| = |~n| then:

?, [[let ~x in e0]]~nm0
else: ?
else: ?

Though we have not provided details here, Coq's co-inductivetypes come with some quite stringent restrictions, designed to prevent unsound interactions with proofs. Our definition of programdenotation is designed to satisfy those restrictions. The main idea
here is that functions defined as co-fixed points must be "suffi-ciently productive"; for our trace example, a co-recursive call may
only be made after at least one token has already been added tothe stream in the current call. This restriction is the reason for including the seemingly information-free stars in our definition oftraces. As we have succeeded in drafting a definition that satisfies
this requirement, we are rewarded with a function that is not just anarbitrary relational specification, but rather a program that Coq is
able to execute, resulting in a Haskell-style lazy list.
2.6 Flat
We are now almost ready to move to assembly language. Our laststop before then is the Flat language, which differs from assembly

only in maintaining the abstract view of the heap as a list of taggedrecords. We do away with variables and move instead to an infinite
bank of registers. Function signatures are expressed as maps fromnatural numbers to types, and Flat programs are responsible for
shifting registers around to compensate for the removal of the built-in stack discipline that variable binding provided.

Types o/ ::= N | ref | \Delta  ! NTypings

\Delta  = N ! o/Registers

rOperands
o ::= n | r | hni | new h~ri | ssirTerms

e ::= r := o; e | jump rPrograms
p ::= let e in p | e

In the style of Typed Assembly Language, to each static pointin a Flat program we associate a typing

\Delta  expressing our expecta-tions of register types whenever that point is reached dynamically.

It is crucial that we keep this typing information, since we will useit to create garbage collector root tables in the next and final transformation.Since there is no need to encode any variable binding for Flat,
its denotational semantics is entirely routine. The only exception isthat we re-use the trace semantics from Alloc.

2.7 Asm
Our idealized assembly language Asm was already introduced inSection 1.1. We have by now already provided the main ingredients

needed to give it a denotational semantics. We re-use the trace-based approach from the last two languages. The difference we
must account for is the shift from abstract to concrete heaps, aswell as the fact that Asm is parametric in a runtime system.

We should make it clear that we have not verified any runtimesystem or garbage collector, but only stated conditions that they
ought to satisfy and proved that those conditions imply the cor-rectness of our compiler. Recent work on formal certification of
garbage collectors [MSLL07] gives us hope that the task we haveomitted is not insurmountable.

In more detail, our formalization of Asm starts with the defini-tion of the domain H of concrete heaps:

H = N ! N
A runtime system provides new and read operations. The readoperation is simpler:

read : H * N * N ! N
For a heap, a pointer to a heap-allocated record, and a constantfield offset within that record,

read should return that field's currentvalue. Runtime systems may make different decisions on concrete

layout of records. For instance, there are several ways of includinginformation on which fields are pointers. Note that

read is anarbitrary Coq function, not a sequence of assembly instructions,

which should let us reason about many different runtime systemdesign decisions within our framework.

The new operation is more complicated, as it is designed tofacilitate garbage collector operation.

new : H * list N * list N!

H * list N * N

Its arguments are the current heap, the values to use to initializethe fields of the record being allocated, and the values of all live

registers holding pointer values. The idea is that, in the course offulfilling the allocation request, the runtime system may rearrange
memory however it likes, so long as things afterward look thesame as before to any type-safe program, from the perspective
of the live registers. A return value of new gives the modifiedheap, a fresh set of values for the pointer-holding registers (i.e., the
garbage collection roots, which may have been moved by a copyingcollector), and a pointer to the new record in the new heap.

To state the logical conditions imposed on runtime systems, wewill need to make a definition based on the abstract heap model of
earlier intermediate languages:

DEFINITION 1 (Pointer isomorphism). We say that pointers p1, p2 2N are isomorphic with respect to abstract heaps

m1, m2 2 M iff:

1. The p1th record of m1 and p2th record of m2 have the samenumber of fields.

2. If the ith field of the p1th record of m1 is tagged Untraced, thenthe

ith field of the p2th record of m2 is also tagged Untraced,and the two fields have the same value.

3. If the ith field of the p1th record of m1 is tagged Traced, thenthe

ith field of the p2th record of m2 is also tagged Traced, andthe two fields contain isomorphic pointers.

The actual definition is slightly more involved but avoids this un-qualified self-reference.

To facilitate a parametric translation soundness proof, a candi-date runtime system is required to provide a concretization function:

fl : M ! H
For an abstract heap m, fl(m) is the concrete heap with whichthe runtime system's representation conventions associate it. We

also overload fl to stand for a different function for concretizingpointers. We abuse notation by applying

fl to various different kindsof objects, where it's clear how they ought to be converted from

abstract to concrete in terms of the two primary fl translations; and,while some of these

fl functions really need to take the abstractheap as an additional argument, we omit it for brevity where the

proper value is clear from context. The runtime system must comewith proofs that its

new and read operations satisfy the appropriatecommutative diagrams with these concretization functions.

To give a specific example, we show the more complicatedcondition between the two operations, that for

new.

THEOREM 2 (Correctness of a new implementation). For any ab-stract heap

m, tagged record field values ~v (each in C), and registerroot set values

~rs, there exist new abstract heap m0, new root values ~rs0, and new record address a such that:

1. new(fl(m), fl(~v), fl( ~rs)) = (fl(m0), fl( ~rs0), fl(a))
2. For every pair of values p and p0 in ~rs and ~rs0, respectively, pand

p0 are isomorphic with respect to m \Phi  [~v] and m0.
3. |m| and a are isomorphic with respect to m \Phi  [~v] and m0.

To understand the details of the last two conditions, recall that,in the abstract memory model, we allocate new records at the end
of the heap, which is itself a list of records. The length of the heapbefore an allocation gives the proper address for the next record to
be allocated.

3. Implementation Strategy Overview
Now that we have sketched the basic structure of our compiler,we move to introducing the ideas behind its implementation in the

Coq proof assistant. In this section, we single out the first compilerphase and walk through its implementation and proof at a high
level, noting the fundamental challenges that we encounter alongthe way. We summarize our solution to each challenge and then
discuss each in more detail in a later section.Recall that our first phase is analogous to the first phase of a
two-phase CPS transform. We want to translate the Source lan-guage to the Linear language.

Of course, before we can begin writing the translation, weneed to represent our languages! Thus, our first challenge is to
choose a representation of each language using Coq types. Oursolution here is based on de Bruijn indices and dependently-typed
abstract syntax trees. Our use of dependent typing will ensurethat representable terms are free of dangling variable references
by encoding a term's free variable set in its type. Moreover, wewill combine typing derivations and terms, essentially representing
each term as its typing derivation. In this way, only well-typed termsare representable.

Now we can begin writing the translation from Source to Lin-ear. We have some choices about how to represent translations in
Coq, but, with our previous language representation choice, it isquite natural to represent translations as dependently-typed Coq
functions. Coq's type system will ensure that, when a translationis fed a well-typed input program, it produces a well-typed output
program. We can use Coq's program extraction facility to produceOCaml versions of our translations by erasing their dependentlytyped parts.This strategy is very appealing, and it is the one we have chosen, but it is not without its inconveniences. Since we representterms as their typing derivations, standard operations like bringing
a new, unused variable into scope are not no-ops like they are withsome other binding representations. These variable operations turn
out to correspond to standard theorems about typing contexts. Forinstance, bringing an unused variable into scope corresponds to a
weakening lemma. These syntactic functions are tedious to writefor each new language, especially when dealing with strong dependent types. Moreover, their implementations have little to dowith details of particular languages. As a result, we have been able
to create a generic programming system that produces them au-tomatically for arbitrary languages satisfying certain criteria. We
not only produce the functions automatically, but we also produceproofs that they commute with arbitrary compositional denotation
functions in the appropriate, function-specific senses.

Now assume that we have our translation implemented. Its typeensures that it preserves well-typedness, but we also want to prove
that it preserves meaning. What proof technique should we use?The technique of logical relations [Plo73] is the standard for this
sort of task. We characterize the relationships between program en-tities and their compilations using relations defined recursively on
type structure. Usual logical relations techniques for denotationalsemantics translate very effectively into our setting, and we are able
to take good advantage of the expressiveness of our meta languageCIC in enabling succinct definitions.

With these relations in hand, we reach the point where mostpencil-and-paper proofs would say that the rest follows by "routine
inductions" of appropriate kinds. Unfortunately, since we want toconvince the Coq proof checker, we will need to provide considerably more detail. There is no magic bullet for automating proofs ofthis sophistication, but we did identify some techniques that worked
surprisingly well for simplifying the proof burden. In particular,since our compiler preserves type information, we were able to automate significant portions of our proofs using type-based heuris-tics. The key insight was the possibility of using greedy quantifier
instantiation, since the dependent types we use are so particularthat most logical quantifiers have domains compatible with just a
single subterm of a proof sequent.

4. Representing Typed Languages
We begin our example by representing the target language of itstransformation. The first step is easy; we give a standard algebraic

datatype definition of the type language shared by Source andLinear.

type : set

Nat : type
Arrow : type ! type ! type

Things get more interesting when we go to define our termlanguages. We want to use dependent types to ensure that only

terms with object language typing derivations are representable, sowe make our classes of operands and terms indexed type families
whose indices tells us their object language types. Coq's expressivedependent type system admits simple implementations of this idea.
Its inductive types are a generalization of the generalized algebraicdatatypes [She04] that have become popular recently.

We represent variables with de Bruijn indices. In other words,a variable is represented as a natural number counting how many
binders outward in lexical scoping order one must search to findits binder. Since we are using dependent typing, variables are more
than just natural numbers. We represent our de Bruijn contexts aslists of types, and variables are in effect constructive proofs that
a particular type is found in a particular context. A generic typefamily

var encapsulates this functionality.We define operands and terms as two mutually-inductive Coq

types. Below, the standard notation \Pi  is used for a dependentfunction type. The notation

\Pi x : o/1, o/2 denotes a function from
o/1 to o/2, where the variable x is bound in the range type o/2,indicating that the function's result type may depend on the value

of its argument. We elide type annotations from \Pi  types where theyare clear from context.

lprimop : list type ! type ! set

LConst : \Pi \Gamma , N ! lprimop \Gamma  Nat

LVar : \Pi \Gamma , \Pi o/, var \Gamma  o/ ! lprimop \Gamma  o/
LLam : \Pi \Gamma , \Pi o/1, \Pi o/2, lterm (o/1 :: \Gamma ) o/2!

lprimop \Gamma  (Arrow o/1 o/2)

Inductive sty : Set :=

| SNat : sty
| SArrow : sty -> sty -> sty.

Inductive sterm : list sty -> sty -> Set :=

| SVar : forall G t,

Var G t
-> sterm G t
| SLam : forall G dom ran,

sterm (dom :: G) ran
-> sterm G (SArrow dom ran)
| SApp : forall G dom ran,

sterm G (SArrow dom ran)
-> sterm G dom
-> sterm G ran
| SConst : forall G, nat -> sterm G SNat.

Fixpoint styDenote (t : sty) : Set :=

match t with

| SNat => nat
| SArrow t1 t2 => styDenote t1 -> styDenote t2
end.

Fixpoint stermDenote (G : list sty) (t : sty)

(e : sterm G t) {struct e}
: Subst styDenote G -> styDenote t :=
match e in (sterm G t)

return (Subst styDenote G -> styDenote t) with
| SVar _ _ v => fun s =>

VarDenote v s
| SLam _ _ _ e' => fun s =>

fun x => stermDenote e' (SCons x s)
| SApp _ _ _ e1 e2 => fun s =>

(stermDenote e1 s) (stermDenote e2 s)
| SConst _ n => fun _ => n
end.

Figure 1. Coq source code of Source syntax and semantics

lterm : list type ! type ! set

LLet : \Pi \Gamma , \Pi o/1, \Pi o/2, lprimop \Gamma  o/1!

lterm (o/1 :: \Gamma ) o/2 ! lterm \Gamma  o/2
LThrow : \Pi \Gamma , \Pi o/, lprimop \Gamma  o/ ! lterm \Gamma  o/

LApp : \Pi \Gamma , \Pi o/1, \Pi o/2, \Pi o/3, var \Gamma  (Arrow o/1 o/2)!

var \Gamma  o/1 ! var \Gamma  (Arrow o/2 o/3)!
lterm \Gamma  o/3

In Coq, all constants are defined in the same syntactic class, asopposed to using separate type and term languages. Thus, for instance, lprimop and LConst are defined "at the same level," thoughthe type of the former tells us that it is "a type."

As an example of the encoding, the identity term *x : N. throw hxiis represented as:

LLam [] Nat Nat (LThrow [Nat] Nat (LVar [Nat] Nat First))
where First is a constructor of var indicating the lexically inner-most variable.

To give an idea of the make-up of our real implementation, wewill also provide some concrete Coq code snippets throughout this
article. The syntax and static and dynamic semantics of our sourcelanguage are simple enough that we can show them here in their

entirety in Figure 1. Though Coq syntax may seem strange at firstto many readers, the structure of these definitions really mirrors
their informal counterparts exactly. These snippets are only meantto give a flavor of the project. We describe in Section 10 how to
obtain the complete project source code, for those who want a morein-depth treatment.

5. Representing Transformations
We are able to write the linearization transformation quite naturallyusing our de Bruijn terms. We start this section with a less formal

presentation of it.

We first define an auxiliary operation e1 u* e2 that, given twolinear terms

e1 and e2 and a variable u free in e2, returns a newlinear term equivalent to running

e1 and throwing its result to e2 bybinding that result to
u.

(let y = o in e1) u* e2 = let y = o in (e1 u* e2)

(throw hoi) u* e = let u = o in e

(x y z) u* e = let f = (*v. let g = (*u. e) in z v g)

in x y f

Now we can give the linearization translation itself.b

nc = throw hnib

xc = throw hxib

e1 e2c = be1c u* (be2c v* (let f = (*x. throw hxi) in u v f)))b
*x : o/. ec = throw h*x : o/. beci

This translation can be converted rather directly into Coq recur-sive function definitions. The catch comes as a result of our strong

dependent types for program syntax. The Coq type checker is notable to verify the type-correctness of some of the clauses above.

For a simple example, let us focus on part of the linearization
case for applications. There we produce terms of the form be1c u*
(be2c v* . . .). The term e2 originally occurred in some context \Gamma .However, here b

e2c, the compilation of e2, is used in a contextformed by adding an additional variable

u to \Gamma . We know that thistransplantation is harmless and ought to be allowed, but the Coq

type checker flags this section as a type error.We need to perform an explicit coercion that adjusts the de
Bruijn indices in be2c. The operation we want corresponds to aweakening lemma for our typing judgment: "If

\Gamma  ` e : o/, then
\Gamma , x : o/0 ` e : o/ when x is not free in e." Translating thisdescription into our formalization with inductive types from the last

section, we want a function:

weakenFront : \Pi \Gamma , \Pi o/, lterm \Gamma  o/ ! \Pi o/0, lterm (o/0 :: \Gamma ) o/

It is possible to write a custom weakening function for lin-earized terms, but nothing about the implementation is specific

to our language. There is a generic recipe for building weaken-ing functions, based only on an understanding of where variable
binders occur. To keep our translations free of such details, we haveopted to create a generic programming system that builds these syntactic helper functions for us. It is the subject of the next section.When we insert these coercions where needed, we have a direct
translation of our informal compiler definition into Coq code. As-suming for now that the coercion functions have already been generated, we can give as an example of a real implementation the Coqcode for the main CPS translation, in Figure 2. We lack the space
to describe the code in detail, but we point out that the Next and
First constructors are used to build de Bruijn variables in unaryform.

compose is the name of the function corresponding to our

informal u* operator. Lterm is a module of helper functions genFixpoint cps (G : list sty) (t : sty)

(e : sterm G t) {struct e}
: lterm G t :=
match e in (sterm G t) return (lterm G t) with

| SVar _ _ v => LThrow (LVar v)
| SConst _ n => LThrow (LConst _ n)
| SLam _ _ _ e' => LThrow (LLam (cps e'))
| SApp _ _ _ e1 e2 =>

compose (cps e1)
(compose (Lterm.weakenFront _ (cps e2))

(LBind

(LLam (LThrow (LVar First)))
(LApply

(Next (Next First))
(Next First)
First)))
end.

Figure 2. Coq source code of the main CPS translation

erated automatically, and it includes the coercion weakenFrontdescribed earlier. Let us, then, turn to a discussion of the generic
programming system that produces it.

6. Generic Syntactic Functions
A variety of these syntactic helper functions come up again andagain in formalization of programming languages. We have identified a few primitive functions that seemed to need per-languageimplementations, along with a number of derived functions that can
be implemented parametrically in the primitives. Our generic pro-gramming system writes the primitive functions for the user and
then automatically instantiates the parametric derived functions.We present here a catalogue of the functions that we found to be
important in this present work. All operate over some type family
term parameterized by types from an arbitrary language.The first primitive is a generalization of the weakening function

from the last section. We modify its specification to allow insertioninto any position of a context, not just the beginning. \Phi  denotes list
concatenation, and it and single-element concatenation :: are rightassociative at the same precedence level.

weaken : \Pi \Gamma 1, \Pi \Gamma 2, \Pi o/, term (\Gamma 1 \Phi  \Gamma 2) o/!

\Pi o/0, term (\Gamma 1 \Phi  o/0 :: \Gamma 2) o/

Next is elementwise permutation. We swap the order of twoadjacent context elements.

permute : \Pi \Gamma 1, \Pi \Gamma 2, \Pi o/1, \Pi o/2, \Pi o/, term (\Gamma 1 \Phi  o/1 :: o/2 :: \Gamma 2) o/!

term (\Gamma 1 \Phi  o/2 :: o/1 :: \Gamma 2) o/

We also want to calculate the free variables of a term. Herewe mean those that actually appear, not just those that are in

scope. This calculation and related support functions are criticalto efficient closure conversion for any language. We write P

(\Gamma ) todenote the type of subsets of the bindings in context
\Gamma .

freeVars : \Pi \Gamma , \Pi o/, term \Gamma  o/ ! P(\Gamma )
Using the notion of free variables, we can define strengthening,which removes unused variable bindings from a context. This operation is also central to closure conversion. An argument of type
\Gamma 1 ` \Gamma 2 denotes a constructive proof that every binding in \Gamma 1 isalso in

\Gamma 2.

strengthen : \Pi \Gamma , \Pi o/, \Pi e : term \Gamma  o/, \Pi \Gamma 0,

freeVars \Gamma  o/ e ` \Gamma 0 ! term \Gamma 0 o/

We have found these primitive operations to be sufficient foreasing the burden of manipulating our higher-level intermediate
languages. The derived operations that our system instantiates are:adding multiple bindings to the middle of a context and adding
multiple bindings to the end of a context, based on weaken; andmoving a binding from the front to the middle or from the middle
to the front of a context and swapping two adjacent multi-bindingsections of a context, derived from

permute.

6.1 Generic Correctness Proofs
Writing these boilerplate syntactic functions is less than half of thechallenge when it comes to proving semantics preservation. The semantic properties of these functions are crucial to enabling correct-ness proofs for the transformations that use them. We also automate
the generation of the correctness proofs, based on an observationabout the classes of semantic properties we find ourselves needing
to verify for them. The key insight is that we only require that eachfunction commute with any compositional denotation function in a
function-specific way.An example should illustrate this idea best. Take the case of the
weakenFront function for our Source language. Fix an arbitrarydenotation function

[[*]] for source terms. We claim that the correct-ness of a well-written compiler will only depend on the following

property of weakenFront, written with informal notation: For any
\Gamma , o/, and e with \Gamma  ` e : o/, we have for any o/0, substitution oe forthe variables of

\Gamma , and value v of type [[o/0]], that:

[[weakenFront \Gamma  o/ e o/0]](oe, v) = [[e]]oe
We shouldn't need to know many specifics about [[*]] to deducethis theorem. In fact, it turns out that all we need is the standard

property used to judge suitability of denotational semantics, com-positionality. In particular, we require that there exist functions
fconst, fvar, fapp, and flam such that:

[[n]]oe = fconst(n)

[[x]]oe = fvar(oe(x))
[[e1 e2]]oe = fapp([[e1]]oe, [[e2]]oe)
[[*x : o/. e]]oe = flam(*x : [[o/]]. [[e]](oe, x))

Our generic programming system introspects into user-supplieddenotation function definitions and extracts the functions that witness their compositionality. Using these functions, it performs auto-matic proofs of generic theorems like the one above about

weakenFront. Every other generated syntactic function has a similar cus-tomized theorem statement and automatic strategy for proving it for

compositional denotations.Though space limits prevent us from providing more detail here,
we mention that our implementation is interesting in that the codeand proof generation themselves are implemented almost entirely
inside of Coq's programming language, using dependent types toensure their soundness. We rely on the technique of reflective
proofs [Bou97] to enable Coq programs to manipulate other Coqprograms in a type-safe manner, modulo some small "proof hints"
provided from outside the logic.
7. Representing Logical Relations
We now turn our attention to formulating the correctness proofs ofcompiler phases, again using the linearization phase as our example. We are able to give a very simple logical relations argument forthis phase, since our meta language CIC is sufficiently expressive to
encode naturally the features of both source and target languages.The correctness theorem that we want looks something like the following, for some appropriate type-indexed relation '. For disam-biguation purposes, we write

[[*]]S for Source language denotations
and [[*]]L for Linear denotations.

Fixpoint val_lr (t : sty) : styDenote t

-> ltyDenote t -> Prop :=
match t

return (styDenote t -> ltyDenote t -> Prop) with
| SNat => fun n1 n2 =>

n1 = n2
| SArrow t1 t2 => fun f1 f2 =>

forall x1 x2, val_lr t1 x1 x2

-> forall (k : styDenote t2 -> nat),

exists thrown, f2 x2 k = k thrown

/\ val_lr t2 (f1 x1) thrown
end.

Figure 3. Coq source code for the first-stage CPS transform'slogical relation

THEOREM 3 (Correctness of linearization). For Source term esuch that

\Gamma  ` e : o/, if we have valuations oeS and oeL for [[\Gamma ]]S
and [[\Gamma ]]L, respectively, such that for every x : o/0 2 \Gamma , we have
oeS(x) 'o/0 oeL(x), then [[e]]SoeS 'o/ [[bec]]LoeL.

This relation for values turns out to satisfy our requirements:

n1 'N n2 = n1 = n2
f1 'o/1!o/2 f2 = 8x1 : [[o/1]]S, 8x2 : [[o/1]]L, x1 'o/1 x2! 9

v : [[o/2]]L, 8k : [[o/2]]L ! N,
f2 x2 k = k v ^ f1 x1 'o/2 v

We have a standard logical relation defined by recursion on thestructure of types.

e1 'o/ e2 means that values e1 of type [[o/]]S
and e2 of type [[o/]]L are equivalent in a suitable sense. Numbers areequivalent if and only if they are equal. Source function

f1 andlinearized function
f2 are equivalent if and only if for every pair ofarguments
x1 and x2 related at the domain type, there exists somevalue
v such that f2 called with x2 and a continuation k alwaysthrows

v to k, and the result of applying f1 to x1 is equivalent to vat the range type.

The suitability of particular logical relations to use in compilerphase correctness specifications is hard to judge individually. We
know we have made proper choices when we are able to composeall of our correctness results to form the overall compiler correctness theorem. It is probably also worth pointing out here that ourdenotational semantics are not fully abstract, in the sense that our
target domains "allow more behavior" than our object languagesought to. For instance, the functions

k quantified over by the func-tion case of the ' definition are drawn from the full Coq function

space, which includes all manner of complicated functions rely-ing on inductive and co-inductive types. The acceptability of this
choice for our application is borne out by our success in using thislogical relation to prove a final theorem whose statement does not
depend on such quantifications.Once we are reconciled with that variety of caveat, we find
that Coq provides quite a congenial platform for defining logi-cal relations for denotational semantics. The ' definition can be
transcribed quite literally, as witnessed by Figure 3. The set ofall logical propositions in Coq is just another type

Prop, and sowe may write recursive functions that return values in it. Contrast

our options here with those associated with proof assistants likeTwelf [PS99], for which formalization of logical relations has historically been challenging.

8. Proof Automation
Now that we have the statement of our theorem, we need to producea formal proof of it. In general, our proofs will require significant

manual effort. As anyone who has worked on computerized proofscan tell you, time-saving automation machinery is extremely welcome. In proving the correctness theorems for our compiler, wewere surprised to find that a very simple automation technique is
very effective on large classes of proof goals that appear. The ef-fectiveness of this technique has everything to do with the combination of typed intermediate languages and our use of dependenttypes to represent their programs.

As an example, consider this proof obligation that occurs in thecorrectness proof for linearization.

* Know: 8oeS, 8oeL, oeS '\Gamma  oeL ! 9v : [[o/1 ! o/2]]L, 8k :

[[o/1 ! o/2]]L ! N, [[be1c]]LoeL k = k v ^ [[e1]]SoeS 'o/1!o/2 v

* Know: 8oeS, 8oeL, oeS '\Gamma  oeL ! 9v : [[o/1]]L, 8k : [[o/1]]L !N

, [[be2c]]LsL k = k v ^ [[e2]]SoeS 'o/1 v

* Must prove: 8oeS, 8oeL, oeS '\Gamma  oeL ! 9v : [[o/2]]L, 8k :

[[o/2]]L ! N, [[be1 e2c]]LoeL k = k v ^ [[e1 e2]]SoeS 'o/2 v

It is safe to simplify the goal by moving all of the universalquantifiers and implications that begin it into our proof context as
new bound variables and assumptions; this rearrangement cannotalter the provability of the goal. Beyond that, Coq's standard automation support is stuck. However, it turns out that we can domuch better for goals like this one, based on greedy quantifier instantiation. Traditional automated theorem provers spend most oftheir intelligence in determining how to use universally-quantified
facts and prove existentially-quantified facts. When quantifiersrange over infinite domains, many such theorem-proving problems
are both undecidable and difficult in practice.However, examining our goal above, we notice that it has a very
interesting property: every quantifier has a rich type depending onsome object language type. Moreover, for any of these types, exactly one subterm of proof state (bound variables, assumptions,and goal) that has that type ever appears at any point during the
proving process! This observation makes instantiation of quanti-fiers extremely easy: instantiate any universal assumption or existential goal with the first properly-typed proof state subterm thatappears.

For instance, in the example above, we had just moved the vari-ables

oeS and oeL and the assumption oeS '\Gamma  oeL into our proof con-text. That means that we should instantiate the initial

oe quantifiersof the two assumptions with these variables and use modus ponens

to access the conclusions of their implications, by way of our newassumption. This operation leaves both assumptions at existential
quantifiers, so we eliminate these quantifiers by adding fresh vari-ables and new assumptions about those variables. The types of the
k quantifiers that we reach now don't match any subterms in scope,so we stop here.

Now it's time for a round of rewriting, using rules added by thehuman user to a hint database. We use all of the boilerplate syntactic function soundness theorems that we generated automatically asleft-to-right rewrite rules, applying them in the goal until no further
changes are possible. Using also a rewrite theorem expressing the
soundness of the u* composition operation, this process simplifiesthe goal to a form with a subterm

[[be1c]]L sL(*x : [[o/1 ! o/2]]L. ...).This lambda term has the right type to use as an instantiation for the

universally-quantified k in the first assumption, so, by our greedyheuristic, we make that instantiation. We perform "safe" propositional simplifications on the newly-exposed part of that assumptionand continue.

Iterating this heuristic, we discharge the proof obligation com-pletely, with no human intervention. Our very naive algorithm
has succeeded in "guessing" all of the complicated continua-tions needed for quantifier instantiations, simply by searching for

(* State the lemma characterizing the effect of

* the CPS'd term composition operator. *)
Lemma compose_sound : forall (G : list sty)

(t : sty) (e : lterm G t)
(t' : sty) (e' : lterm (t :: G) t') s
(k : _ -> result),
ltermDenote (compose e e') s k
= ltermDenote lin s

(fun x => ltermDenote lin' (SCons x s) k).
induction e; (* We prove it by induction on

* the structure of
* the term e. *)
equation_tac. (* A generic rewriting

* procedure handles
* the resulting cases. *)
Qed.

(* ...omitted code to add compose_sound to our

* rewriting hint base... *)

(* State the theorem characterizing soundness of

* the CPS translation. *)
Theorem cps_sound : forall G t (e : sterm G t),

exp_lr e (cps e).
unfold exp_lr; (* Expand the definition of the

* logical relation on
* expressions. *)
induction e; (* Proceed by induction on the

* structure of the term e. *)
lr_tac. (* Use the generic greedy

* instantiation procedure to
* discharge the subgoals. *)
Qed.

Figure 4. Snippets of the Coq proof script for the CPS correctnesstheorem

properly-typed subterms of the proof state. In fact, this same heuris-tic discharges all of the cases of the linearization soundness proof,
once we've stocked the rewrite database with the appropriate syn-tactic simplifications beforehand. To prove this theorem, all the
user needs to do is specify which induction principle to use and runthe heuristic on the resulting subgoals.

We have found this approach to be very effective on high-leveltyped languages in general. The human prover's job is to determine
the useful syntactic properties with which to augment those provedgenerically, prove these new properties, and add them to the rewrite
hint database. With very little additional effort, the main theoremscan then be discharged automatically. Unfortunately, our lowerlevel intermediate languages keep less precise type information,so greedy instantiation would make incorrect choices too often to
be practical. Our experience here provides a new justification insupport of type-preserving compilation.

Figure 4 shows example Coq code for proving the CPS cor-rectness theorem, with a few small simplifications made for space
reasons.

9. Further Discussion
Some other aspects of our formalization outside of the runningexample are worth mentioning. We summarize them in this section.

9.1 Logical Relations for Closure Conversion
Formalizations of closure conversion and its correctness, especiallythose using operational semantics, often involve existential types

and other relatively complicated notions. In our formalization, weare able to use a surprisingly simple logical relation to characterize
closure conversion. It relates denotations from the CPS and CClanguages, indicated with superscripts

P and C, respectively. Welift various definitions to vectors in the usual way.

n1 'N n2 = n1 = n2
f1 '~o/!N f2 = 8~x1 : [[~o/]]P , 8x2 : [[~o/]]C, x1 '~o/ x2!

f1 x1 'o/2 f2 x2

This relation is almost identical to the most basic logical relationfor simply-typed lambda calculus! The secret is that our meta

language CIC "has native support for closures." That is, Coq'sfunction spaces already incorporate the appropriate reduction rules
to capture free variables, so we don't need to mention this processexplicitly in our relation.

In more detail, the relevant denotations of CC types are:

[[~o/ ! N]] = [[o/1]] * . . . * [[o/n]] ! N
[[~o/1 * ~o/2 ! N]] = ([[o/11 ]] * . . . * [[o/1n]]) ! ([[o/21 ]] * . . . * [[o/2m]]) ! N

Recall that the second variety of type shown is the type of codepointers, with the additional first list of parameters denoting the

expected environment type. A CPS language lambda expression iscompiled into a packaging of a closure using a pointer to a fresh
code block. That code block will have some type ~o/1 *~o/2 ! N. Thedenotation of this type is some meta language type

T1 ! T2 ! N.We perform an immediate partial application to an environment

formed from the relevant free variables. This environment's typewill have denotation

T1. Thus, the partial application's type hasdenotation
T2 ! N, making it compatible with our logical relation.The effect of the closure packaging operation has been "hidden"

using one of the meta language's "closures" formed by the partialapplication.

9.2 Explicating Higher-Order Control Flow
The translation from CC to Alloc moves from a higher-order,terminating language to a first-order language that admits nontermination. As a consequence, the translation correctness proofmust correspond to some explanation of why CC's particular brand
of higher-order control flow leads to termination, and why theresulting first-order program returns an identical answer to the
higher-order original.The basic proof technique has two main pieces. First, the logical
relation requires that any value with a code type terminates with theexpected value when started in a heap and variable environment
where the same condition holds for values that should have codetype. Second, we take advantage of the fact that function definitions
occur in dependency order in CC programs. Our variable bindingrestrictions enforce a lack of cycles via dependent types. We can
prove by induction on the position of a code body in a programthat any execution of it in a suitable starting state terminates with
the correct value. Any code pointer involved in the execution eithercomes earlier in the program, in which case we have its correctness
by the inductive hypothesis; or the code pointer has been retrievedfrom a variable or heap slot, in which case its safety follows by
an induction starting from our initial hypothesis and tracking thevariable bindings and heap writes made by the program.

9.3 Garbage Collection Safety
The soundness of the translation from Flat to Asm depends on adelicate safety property of well-typed Flat programs. It is phrased

in terms of the register typings we have available at every program

point, and it depends on the definition of pointer isomorphism wegave in Section 2.7.

THEOREM 4 (Heap rearrangement safety). For any register typing
\Delta , abstract heaps m1 and m2, and register files R1 and R2, if:

1. For every register r with \Delta (r) = ref, R1(r) is isomorphic to

R2(r) with respect to m1 and m2.

2. For every register r with \Delta (r) 6= ref, R1(r) = R2(r).

and p is a Flat program such that \Delta  ` p, we have [[p]]R1m1 =
[[p]]R2m2.

This theorem says that it is safe to rearrange a heap if therelevant roots pointing into it are also rearranged equivalently.

Well-typed Flat programs can't distinguish between the old andnew situations. The denotations that we conclude are equal in the
theorem are traces, so we have that valid Flat programs returnidentical results (if any) and make the same numbers of function
calls in any isomorphic initial states.The requirements on the runtime system's

new operation allowit to modify the heap arbitrarily on each call, so long as the new

heap and registers are isomorphic to the old heap and registers. Theroot set provided as an operand to

new is used to determine the ap-propriate notion of isomorphism, so the heap rearrangement safety

theorem is critical to making the correctness proof go through.
9.4 Putting It All Together
With all of the compiler phases proved, we can compose the proofsto form a correctness proof for the overall compiler. We give the

formal version of the theorem described informally in the Introduc-tion. We use the notation

T # n to denote that trace T terminateswith result
n.

THEOREM 5 (Compiler correctness). Let m 2 H be a heap ini-tialized with a closure for the top-level continuation, let

p be apointer to that closure, and let
R be a register file mapping thefirst register to
p. Given a Source term e such that * ` e : N,
[[bec]]Rm # [[e]]().

Even considering only the early phases of the compiler, it isdifficult to give a correctness theorem in terms of equality for all

source types. For instance, we can't compose parametrically theresults for CPS transformation and closure conversion to yield a
correctness theorem expressed with an equality between denota-tions. The problem lies in the lack of full abstraction for our semantics. Instead, we must use an alternate notion of equality thatonly requires functions to agree at arguments that are denotations
of terms. This general notion also appears in the idea of pre-logicalrelations [HS99], a compositional alternative to logical relations.

10. Implementation
The compiler implementation and documentation are available on-line at:

http://ltamer.sourceforge.net/
We present lines-of-code counts for the different implementa-tion files, including proofs, in Figure 5.

These figures do not include 3520 lines of Coq code from aprogramming language formalization support library that we have
been developing in tandem with the compiler. Additionally, wehave 2716 lines of OCaml code supporting generic programming
of syntactic support functions and their correctness proofs. Devel-oping these re-usable pieces was the most time-consuming part of
our overall effort. We believe we improved our productivity an or-der of magnitude by determining the right language representation

File LoCSource 31
...to... 116Linear 56
...to... 115CPS 87
...to... 646CC 185
...to... 1321Alloc 217
...to... 658Flat 141
...to... 868Asm 111

File LoCApplicative dictionaries 119
Traces 96GC safety 741
Overall compiler 119

Figure 5. Project lines-of-code counts
scheme and its library support code, and again by developing thegeneric programming system. With those pieces in place, implementing the compiler only took about one person-month of work,which we feel validates the general efficacy of our techniques.

It is worth characterizing how much of our implementation mustbe trusted to trust its outputs. Of course, the Coq system and the
toolchain used to compile its programs (including operating systemand hardware) must be trusted. Focusing on code specific to this
project, we see that, if we want only to certify the behavior ofparticular output assembly programs, we must trust about 200 lines
of code. This figure comes from taking a backwards slice fromthe statement of any individual program correctness theorem. If
we want to believe the correctness of the compiler itself, we mustadd additionally about 100 lines to include the formalization of the
Source language as well.

11. Related Work
Moore produced a verified implementation of a compiler for thePiton language [Moo89] using the Boyer-Moore theorem prover.

Piton did not have the higher-order features that make our presentwork interesting, and proofs in the Boyer-Moore tradition have
fundamentally different trustworthiness characteristics than Coqproofs, being dependent on a large reasoning engine instead of a
small proof-checking kernel. However, the Piton work dealt withlarger and more realistic source and target languages.

The VLISP project [GRW95] produced a Scheme system witha rigorous but non-mechanized proof of correctness. They also
made heavy use of denotational semantics, but they dealt with adynamically-typed source language and so did not encounter many
of the interesting issues reported here related to type-preservingcompilation.

Semantics preservation proofs have been published before forindividual phases of type-preserving compilers, including closure
conversion [MMH95]. All of these proofs that we are aware ofuse operational semantics, forfeiting the advantages of denotational
semantics for mechanization that we have shown here.Recent work has implemented tagless interpreters [PTS02] using generalized algebraic datatypes and other features related todependent typing. Tagless interpreters have much in common with
our approach to denotational semantics in Coq, but we are notaware of any proofs beyond type safety carried out in such settings.

The CompCert project [Ler06] has used Coq to produce a cer-tified compiler for a subset of C. Because of this source language
choice, CompCert has not required reasoning about nested vari-able scopes, first-class functions based on closures, or dynamic allocation. On the other hand, they deal with larger and more realis-tic source and target languages. CompCert uses non-dependentlytyped abstract syntax and operational semantics, in contrast to ouruse of dependent types and denotational semantics; and we focus
more on proof automation.Many additional pointers to work on compiler verification can
be found in the bibliography by Dave [Dav03].

12. Conclusion
We have outlined a non-trivial case study in certification of com-pilers for higher-order programming languages. Our results lend

credence to the suitability of our implementation strategy: the en-coding of language syntax and static semantics using dependent
types, along with the use of denotational semantics targeting a richbut formalized meta language. We have described how generic programming and proving can be used to ease the development of type-preserving compilers and their proofs, and we have demonstrated
how the certification of type-preserving compilers is congenial toautomated proof.

We hope to expand these techniques to larger and more realis-tic source and target languages. Our denotational approach naturally extends to features that can be encoded directly in CIC, in-cluding (impredicative) universal types, (impredicative) existential
types, lists, and trees. Handling of "effectful" features like non-termination and mutability without first compiling to first-order
form (as we've done here in the later stages of the compiler) isan interesting open problem. We also plan to investigate further
means of automating compiler correctness proofs and factorizinguseful aspects of them into re-usable libraries.

There remains plenty of room to improve the developer experi-ence in using our approach. Programming with dependent types
has long been known to be tricky. We implemented our proto-type by slogging through the messy details for the small number of features that we've included. In scaling up to realistic lan-guages, the costs of doing so may prove prohibitive. Some recentlyproposed techniques for simplifying dependently-typed Coq pro-gramming [Soz06] may turn out to be useful.

Coercions between different dependent types appear frequentlyin the approach we follow. It turns out that effective reasoning about
coercions in type theory requires something more than the compu-tational equivalences that are used in systems like CIC. In Coq, this
reasoning is often facilitated by adding an axiom describing thecomputational behavior of coercions. Ad-hoc axioms are a convenient way of extending a proof assistant's logic, but their loose in-tegration has drawbacks. Coq's built-in type equivalence judgment,
which is applied automatically during many stages of proof check-ing, will not take the axioms into account. Instead, they must be applied in explicit proofs. New languages like Epigram [MM04] de-sign their type equivalence judgments to facilitate reasoning about
coercions. Future certified compiler projects might benefit from be-ing developed in such environments, modulo the current immaturity of their development tools compared to what Coq offers. Al-ternatively, it is probably worth experimenting with the transplantation of some of these new ideas into Coq.

Acknowledgments
Thanks to Manu Sridharan and the anonymous referees for helpfulcomments on drafts of this paper.

References
[ABF+05] Brian E. Aydemir, Aaron Bohannon, Matthew Fairbairn,J. Nathan Foster, Benjamin C. Pierce, Peter Sewell, Dimitrios Vytiniotis, Geoffrey Washburn, Stephanie Weirich, andSteve Zdancewic. Mechanized metatheory for the masses:
The POPLMARK challenge. In Proc. TPHOLs, pages 50-65,2005.

[BC04] Yves Bertot and Pierre Cast'eran. Interactive TheoremProving and Program Development. Coq'Art: The Calculus

of Inductive Constructions. Texts in Theoretical ComputerScience. Springer Verlag, 2004.

[Bou97] Samuel Boutin. Using reflection to build efficient andcertified decision procedures. In Proc. STACS, pages 515-

529, 1997.
[Dav03] Maulik A. Dave. Compiler verification: a bibliography.SIGSOFT Softw. Eng. Notes, 28(6):2-2, 2003.

[dB72] Nicolas G. de Bruijn. Lambda-calculus notation withnameless dummies: a tool for automatic formal manipulation

with application to the Church-Rosser theorem. Indag.Math., 34(5):381-392, 1972.

[Gim95] Eduardo Gim'enez. Codifying guarded definitions withrecursive schemes. In Proc. TYPES, pages 39-59. SpringerVerlag, 1995.
[GRW95] Joshua D. Guttman, John D. Ramsdell, and Mitchell Wand.VLISP: A verified implementation of Scheme. Lisp and

Symbolic Computation, 8(1/2):5-32, 1995.
[HS99] Furio Honsell and Donald Sannella. Pre-logical relations. InProc. CSL, pages 546-561, 1999.

[Ler06] Xavier Leroy. Formal certification of a compiler back-endor: programming a compiler with a proof assistant. In Proc.

POPL, pages 42-54, 2006.
[MM04] Conor McBride and James McKinna. The view from theleft. J. Functional Programming, 14(1):69-111, 2004.

[MMH95] Yasuhiko Minamide, Greg Morrisett, and Robert Harper.Typed closure conversion. Technical Report CMU-CSFOX-95-05, Carnegie Mellon University, 1995.
[Moo89] J. Strother Moore. A mechanically verified languageimplementation. J. Automated Reasoning, 5(4):461-492,

1989.
[MSLL07] Andrew McCreight, Zhong Shao, Chunxiao Lin, and LongLi. A general framework for certifying garbage collectors

and their mutators. In Proc. PLDI, 2007.
[MWCG99] Greg Morrisett, David Walker, Karl Crary, and Neal Glew.From System F to typed assembly language. ACM Trans.

Program. Lang. Syst., 21(3):527-568, 1999.
[PE88] F. Pfenning and C. Elliot. Higher-order abstract syntax. InProc. PLDI, pages 199-208, 1988.

[Plo73] G. D. Plotkin. Lambda-definability and logical relations.Memorandum SAI-RM-4, University of Edinburgh, 1973.
[Plo75] Gordon D. Plotkin. Call-by-name, call-by-value, and thelambda calculus. Theoretical Computer Science, 1:125-159,

1975.
[PS99] Frank Pfenning and Carsten Sch"urmann. System descrip-tion: Twelf - a meta-logical framework for deductive systems. In Proc. CADE, pages 202-206, 1999.
[PTS02] Emir Pasalic, Walid Taha, and Tim Sheard. Tagless stagedinterpreters for typed languages. In Proc. ICFP, pages

218-229, 2002.
[She04] Tim Sheard. Languages of the future. In Proc. OOPSLA,pages 116-119, 2004.

[Soz06] Matthieu Sozeau. Subset coercions in Coq. In Proc. TYPES,2006.
[TMC+96] D. Tarditi, G. Morrisett, P. Cheng, C. Stone, R. Harper, andP. Lee. TIL: a type-directed optimizing compiler for ML. In

Proc. PLDI, pages 181-192, 1996.