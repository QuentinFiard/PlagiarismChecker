

Methods and Logics for Proving
Programs  1

Patrick   COUSOT
E'cole Polytechnique, LIX
91128 Palaiseau Cedex (France)

Contents
1 . Introduction 2

1.1 A brief review of HOARE [1969] 2
1.2 Further work by C. A. R. Hoare on Hoare logic 3
1.3 Further work by C. A. R. Hoare on methods of reasoning about programs 4
1.4 Surveys on Hoare logic 6
1.5 Summary 6
1.6 Hints for reading this survey 9

2 . Logical, set and order theoretic notations 1 1
3 . Syntax and semantics of the programming language 1 4

3.1 Syntax 14
3.2 Operational semantics 18
3.3 Relational semantics 22

4 . Partial correctness of a command 2 5
5 . Floyd-Naur partial correctness proof method and some equivalent variants 2 6

1 Cousot, P.

Methods and Logics for Proving Program
In ``Handbook of Theoretical Computer Science '', J. van Leeuwen (Ed.), vol. B ``  Formal Models and Semantics '',
Ch. 15, pp. 843--993, Elsevier, 1990.

5.1 An example of partial correctness proof by Floyd-Naur method 26
5.2 The stepwise Floyd-Naur partial correctness proof method 28

5.2.1 Stepwise induction principle 28
5.2.2 Representing a global invariant on configurations by local invariants on states attached

to program points 30
5.2.3 Construction of the verification conditions for local invariants 31
5.2.4 Semantical soundness and completeness of the stepwise Floyd-Naur partial correctness

proof method 35
5.3 The compositional Floyd-Naur partial correctness proof method 36

5.3.1 Preconditions and postconditions of commands 36
5.3.2 Compositional verification conditions 38
5.3.3 Semantical soundness and completeness of the compositional Floyd-Naur partial

correctness proof method 39
5.4 Equivalence of stepwise and compositional Floyd-Naur partial correctness proofs 40

5.4.1 The compositional presentation of a stepwise Floyd-Naur partial correctness proof 41
5.4.2 The stepwise presentation of a compositional Floyd-Naur partial correctness proof 42
5.5 Variants of Floyd-Naur partial correctness proof method 44

6 . Liveness proof methods 4 6

6.1 Execution traces 46
6.2 Total correctness 47
6.3 Well founded relations, well orderings and ordinals 48
6.4 Termination proofs by Floyd's well-founded set method 49
6.5 Liveness 51
6.6 Generalization Floyd's total correctness proof method to liveness 52
6.7 Burstall total correctness proof method and its generalization 53

7 . Hoare logic 5 5

7.1 Hoare logic considered from a semantical point of view 55

7.1.1 General theorems for proof construction 55
7.1.2 Semantical soundness and completeness 57
7.1.3 Proof outlines 58
7.2 Hoare logic considered from a syntactical point of view 60

7.2.1 Syntax of predicates and correctness formulae 61
7.2.2 Deductive systems and formal proofs 63
7.2.3 Hoare's proof system  64
7.2.4 Hoare's proof system   for proof outlines 67
7.2.5 Syntactical rules of substitution 68

7.2.5.1 Variables appearing in a term, predicate, command or correctness formula 68
7.2.5.2 Bound and free variables appearing in a term, predicate, command or correctness

formula 69
7.2.5.3 Formal definition of substitution of a term for a variable in a term or predicate 70
7.3 The semantics of Hoare logic 71

7.3.1 Semantics of predicates and correctness formulae 71
7.3.2 Semantics of substitution 74
7.4 The link between syntax and semantics: soundness and completeness issues in Hoare logic 76

7.4.1 Soundness of Hoare logic 76
7.4.2 Relative completeness of Hoare logic 77

7.4.2.1 Completeness and incompleteness issues for Hoare logic 77
7.4.2.2 Abacus arithmetic 80

7.4.2.2.1 Inexpressibility of addition in abacus arithmetic 80
7.4.2.2.2 Decidability of abacus arithmetic 83
7.4.2.2.3 Nonstandard interpretations of abacus arithmetic 84
7.4.2.3 Incompleteness results for Hoare logic 85

7.4.2.3.1 Unspecifiable partially correct programs 85
7.4.2.3.2 Unprovable partially correct programs 86

7.4.2.3.2.1 Incompleteness of Hoare logic for an interpretation with decidable firstorder theory 86
7.4.2.3.2.2 Incompleteness of Hoare logic for an interpretation with undecidable

first-order theory 87
7.4.2.3.2.2.1 The set of provable Hoare formulae is recursively enumerable 87
7.4.2.3.2.2.2 The non-halting problem is not semi-decidable for Peano

arithmetic 89
7.4.2.3.2.2.3 The set of valid Hoare formulae for Peano arithmetic is not

recursively enumerable 90
7.4.2.3.2.2.4 Incompleteness of Hoare logic for Peano arithmetic 91
7.4.2.3.3 Unprovable valid predicates, mechanical proofs 92
7.4.2.4 Cook's relative completeness of Hoare logic 93

7.4.2.4.1 Expressiveness a` la Clarke 93
7.4.2.4.2 Relative completeness of Hoare logic 95
7.4.2.4.3 Expressiveness a` la Cook and its equivalence with Clarke's notion of

expressiveness 97
7.4.2.4.4 Relative completeness of Hoare logic for arithmetical while-programs and

nonstandard interpretations 99
7.4.2.4.5 On the unnecessity of expressiveness 101
7.4.2.5 Clarke's characterization problem 102

7.4.2.5.1 Languages with a relatively complete and sound Hoare logic have a decidable

halting problem for finite interpretations 103
7.4.2.5.2 The halting problem for finite interpretations is undecidable for Clarke's

languages 103
7.4.2.5.3 Languages with no sound and relatively complete Hoare logic 109
7.4.2.6 Nonstandard semantics and logical completeness 110

8 . Complements on Hoare logic 1 1 1

8.1 Data structures 111
8.2 Procedures 113

8.2.1 Recursive parameterless procedures 113

8.2.1.1 Syntax and relational semantics of a parameterless procedural language 113
8.2.1.2 The recursion rule based upon computational induction 115
8.2.1.3 The rule of adaptation based upon fixpoint induction 118
8.2.1.4 Hoare-like deductive systems with context-dependent conditions 124
8.2.2 Value-result parameters 126
8.2.3 Complements on variable parameters and procedures as parameters 130
8.3 Undefinedness 133
8.4 Aliasing and side effects 133
8.5 Block structured local variables 133
8.6 Goto statements 134
8.7 Functions and expressions (with side effects) 135
8.8 Coroutines 135
8.9 Parallel programs 137

8.9.1 Operational semantics of parallel programs with shared variables 137
8.9.2 A` la Floyd proof methods for parallel programs with shared variables 140

8.9.2.1 Using a single global invariant 142
8.9.2.2 Using an invariant on memory states for each control state 143
8.9.2.3 Using an invariant on memory states for each program point 144
8.9.2.4 Using an invariant on memory and control states for each program point 146

8.9.2.4.1 The strengthened Lamport and Owicki & Gries method 148
8.9.2.4.2 Newton's method 150
8.9.2.4.3 The lattice of proof methods including Lamport's method 151
8.9.2.5 Using an invariant on memory states with auxiliary variables for each program

point 152
8.9.2.5.1 A stepwise presentation of Owicki & Gries method 152
8.9.2.5.2 On the use of auxiliary variables 154

8.9.2.5.3 A syntax-directed presentation of Owicki & Gries method 156
8.9.3 Hoare logics for parallel programs with shared variables 156

8.9.3.1 Owicki & Gries logic 156
8.9.3.2 Stirling compositional logic 157
8.9.4 Hoare logics for communicating sequential processes 161
8.10 Total correctness 164

8.10.1 Finitely bounded nondeterminism and arithmetical completeness 164
8.10.2 Unbounded nondeterminism 165
8.10.3 Total correctness of fair parallel programs 167

8.10.3.1 Fairness hypotheses and unbounded nondeterminism 167
8.10.3.2 Failure of Floyd liveness proof method 167
8.10.3.3 The transformational approach 168
8.10.3.4 The intermittent well-foundedness approach 169
8.10.4 Dijkstra's weakest preconditions calculus 170
8.11 Examples of program verification 172
8.12 Other logics extending first-order logic with programs 172

9 . References 1 7 4

1 . I n t r o d u c t i o n

Formalizing ideas of  FLOYD [1967a] and NAUR [1966]  which, in essence, were already
present in  GOLDSTINE & VON  NEUMANN  [1947] and  TURING [1949]  (as recalled in
MORRIS & JONES [1984]), C. A. R. Hoare introduced in October 1969 an axiomatic method
for proving that a program is partially correct with respect to a specification ( HOARE [1969],
see the genesis and reprint of this paper in  HOARE & JONES  [1989,  pp. 45-58]). This paper
introduced or revealed a number of ideas which originated an evolution of programming
from arts and crafts to a science. Hoare logic had a very significant impact on program
verification and design methods. It was an essential step in the emergence of "structured
programming" in the 1970's. It is also an important contribution to the development of
formal semantics of programming languages. Understanding that programs can be a subject
of mathematical investigations was also crucial in the development of a theory of
programming. This is reflected in the fact that HOARE [1969] is one of the most widely cited
papers in computing science (see the bibliography of more than 350 references).

1 . 1 A brief review of HOARE [1969]

The "introduction" of HOARE [1969] claims that "computer programming is an exact
science" and calls for the development of a formal system for reasoning about programs.

The first part of HOARE [1969] is an attempt to axiomatize "computer arithmetic" in two
stages : first axioms are given for arithmetic operations on natural numbers which are valid
independently of their computer representation (such as "x + y = y + x", "x + 0 = x", etc)
and then choices of supplementary axioms are proposed for characterizing various possible
implementations. For example a finite representation of natural numbers (" \Delta  x. 0  <= x <=
maxint") can lead to various possible interpretations of overflow such as "~  \Lambda  x. (x =
maxint + 1)" i.e. program execution should be stopped, "maxint + 1 = maxint" i.e. the
result of an overflowing operation should be taken as the maximum value represented or
"maxint + 1 = 0" i.e. arithmetic operations should be computed modulo this maximum
value "maxint".

The second part of  HOARE [1969]  introduces an axiomatic definition of " program
execution". It defines Hoare correctness formulae "{P} C {Q}" where C is a program and
the precondition P and postcondition Q are logical formulae describing properties of data
manipulated by program C. Such a formula "{P} C {Q}" means that if execution of C is
started in any memory state satisfying precondition P and if this execution does terminate
then postcondition Q will be true upon completion (Hoare correctness formulae were
originally written "P {C} Q" but are now written as "{P} C {Q}" to emphasize the ro^le

of assertions P and Q as comments). The main contribution of  HOARE [1969]  is the
elucidation of a set of axioms and rules of inference which can be used in correctness
proofs (termed "partial" since termination is not involved). For example if X is a variable
identifier, E is an expression, P[X  \Xi  E] is obtained from P by substituting E for all
occurrences of X then "{P[X  \Xi   E]} X := E {P}" is an axiom for the assignment
command "X := E". Intuitively, it simply states that what is true of expression E before
the assignment is true of X after assignment of E to X. Another example is the sequential
composition "C 1 ;   C 2 " of commands C 1  and C 2 . From "{P} C 1 {Q}" and
"{Q} C2 {R}", one can infer "{P} C 1; C2 {R}". Intuitively this rule of inference states
that if Q is true after execution of C1 starting with P true and if R is true after execution of
C2 starting with Q true then R is true after the sequential execution of C 1 and C2 starting
with P true. This second part of HOARE [1969] ends with the formal partial correctness proof
of a program for computing the Euclidian division of nonnegative integers by successive
subtractions.

The third part of HOARE [1969] is on "general reservations" about this paper. First sideeffects are excluded. Then, and most importantly, termination is not considered. Finally a
number of languages features (such as procedures and parallelism) are omitted.

Then HOARE [1969] discusses "proofs of program correctness". An axiomatic approach
is indispensable for achieving program reliability. The practicability of program proving is
advocated in view of the cost of programming errors and program testing. It is also useful
for program documentation and modification and to achieve portability (machine dependent
part being clearly identified by the use of implementation dependent axioms). The
difficulties such as unreliable specifications or proof complexity are also foreseen:
"program proving, certainly at present, will be difficult even for programmers of high
calibre; and may be applicable only to quite simple program designs".

Finally HOARE [1969] discusses "formal language definition". The axioms and rules of
inference can be understood as "the ultimate definitive specification of the meaning of the
language". The approach is simple. It can cope with the problem of machine dependence by
leaving certain aspects of the language undefined and serve as a guide for language design.

HOARE [1969] last words in the "acknowledgements" anticipated the enormous work on
Hoare logic: "The formal material presented here has only an expository status and
represents only a minute proportion of what remains to be done. It is hoped that many of
the fascinating problems involved will be taken up by others".

1 . 2 Further work by C. A. R. Hoare on Hoare

l o g i c

A number of these problems were treated by C. A. R. Hoare himself. A famous and
non trivial proof, that of the program "Find" ( HOARE [1961] ), was later given in  HOARE

[1971a]. This paper shows a significant move in the use of Hoare logic from a program
verification method (i.e. an a posteriori proof of a complete program) to a program design
method: "A systematic technique is described for constructing the program proof during the
process of coding it". The use of data abstractions to handle complex data structures was
introduced in HOARE [1972a] and exemplified in HOARE [1972b]. Other programming language
features were covered later: procedures and parameters (HOARE [1971b]), including recursion
(FOLEY & HOARE  [1971]), jumps and functions ( CLINT & HOARE [1972], A S H C R O F T ,
CLINT & HOARE [1976]), parallel programs ( HOARE [1972c], HOARE [1975]). Hoare logic had an
important influence over the design of modern programming languages, Pascal notably
(WIRTH  [1971],  HOARE & WIRTH [1973]). However progresses were sometimes slower than
anticipated by HOARE [1969]. In the case of parallelism for example, synchronization (HOARE
[1974]) and communication primitives ( HOARE [1978b],  HOARE [1985b]) had to be better
understood before introducing formal proof methods ( HOARE [1981],
ZHOU CHAO CHEN & HOARE  [1981]).

1 . 3 Further work by C. A. R. Hoare on methods of

reasoning about programs

With regard to program proofs,  HOARE [1969] suffers from a number of weaknesses,
some of which were corrected later ( HOARE & HE JIFENG [1986] [1987] , HOARE, HE
JIFENG & SANDERS  [1987], HOARE, HAYES, HE JIFENG, MORGAN, ROSCOE, SANDERS, SORENSEN &
SUFRIN [1987]). First of all, termination is not involved. For example "{true} while true do
skip {Q}" holds for all assertions Q. This can be considered as a regrettable omission (see
the "Envoi" to  HOARE & JONES [1989,  p. 391]) or as an historically fruitful simplification
(since, for example, total correctness was not that simple to understand in the presence of
nondeterminism,  HOARE [1978a] ). Later work by C. A. R. Hoare insists upon total
correctness ( HOARE [1981]). Second, the postcondition Q in Hoare correctness formulae
"{P} C {Q}" is a predicate of the final states alone. Adopting postconditions which are
predicates of initial and final states makes it easier to specify relations (as latter in HOARE &
HE, JIFENG [1986] [1987], HOARE, HE JIFENG & SANDERS [1987] , HOARE, HAYES, HE JIFENG, MORGAN,
ROSCOE, SANDERS, SORENSEN & SUFRIN [1987]). To achieve the same effect in Hoare logic, one
has to use so-called logical auxiliary variables such as x in "{X = x} C {X = x}" to
express that execution of C leaves the value of the programming variable X unchanged
(assuming that x does not appear in C). Third, Hoare correctness formulae "{P} C {Q}"
do not describe the course of computation of the program C, a flaw in the presence of
parallelism. For example "{X = x} C {X = x}" does not mean that X is not modified
during execution of C so that we do not know whether "{X = x}[C || X := 1]{X = 1}"
holds or not when command C is executed in parallel with assignment "X := 1". Sequences
of intermediate states (more precisely messages) were latter used in  HOARE [1984]. Fourth

Hoare logic imposes a standard form of reasoning about programs which, for example,
lacks the flexibility of choosing between positive or contrapositive arguments or of
choosing the most adequate form of induction (the only one allowed being structural
induction upon the syntactical structure of programs). Fifth, the logical treatment of Hoare
correctness formulae "{P} C {Q}", predicates P, Q and programs C are quite separated.
For example predicates can be conjuncted but not Hoare correctness formulae. The use of
names for designating objects obeys quite different conventions in predicates and
programs. Finally Hoare logic was a very important step in the understanding that
"computers are mathematical machines", "computer programs are mathematical
expressions", that "a programming language is a mathematical theory" and that
"programming is a mathematical activity" (HOARE [1985a]). However, the main emphasis of
HOARE [1969] is on the logical and formal aspects of this mathematical activity. It obliterates
the informal and often more elegant mathematical proofs which should also have had their
useful counterparts in programming. This point of view is similar to that of a mathematician
adhering to rigorous and well-understood proof methods without using exclusively formal
mathematical logic.

With regard to program semantics,  HOARE [1969]  was somewhat optimistic: the
axiomatic semantics can no longer be considered as universal but as one of the
complementary definitions of programming languages (HOARE & LAUER [1974]).

1 . 4 Surveys on Hoare logic

Over the past twenty years, Hoare logic has been considerably studied. Most results
have been reported in numerous surveys (LANGMAACK & OLDEROG [1980], APT [1981a], OLDEROG
[1983b], APT [1984], CLARKE [1984], BARRINGER [1985] , DE ROEVER  [1985a], APT [1988], HOOMAN
&  DE ROEVER [1989]) and books ( MANNA [1971], DE BAKKER [1980], LOECKX & SIEBER [1984] ), to
cite a few. This chapter is an elementary but rigorous introduction to Hoare logic. We just
assume some elementary familiarity with nai"ve logical and set theoretic notations and some
very rudimentary practice of the so-called intermediate assertions method of Floyd but
nevertheless do the necessary recalls. We hope that this survey will be useful to readers
willing to understand the abundant and often very technical literature on Hoare logic.
Numerous references are suggested for further study. We apologize to all those researchers
who have been misunderstood or not referenced.

1 . 5 Summary

In paragraph $ 2 we fix up the logical, set and order theoretic notations which are
used subsequently.

In paragraph $ 3 we define the syntax of while-programs, their operational semantics
(that is a set of finite or infinite sequences of configurations representing successive
observable states of an abstract machine executing the program for each possible input data)
and their relational semantics (directly defining the relationship between initial and final
configurations of terminating program executions).

In paragraph $ 4 we define the partial correctness " { p }C{ q }" of a command C
with respect to a precondition p and postcondition q to mean that any terminating execution
of C starting from an initial state s satisfying p must end in some final state s' satisfying q.

Paragraph $ 5 is a presentation of Floyd-Naur partial correctness proof method. After
giving a simple introductory example, we formally derive Floyd-Naur's method from the
operational semantics using an elementary stepwise induction principle and predicates
attached to program points to express invariant properties of programs. This systematic
construction of the verification conditions ensures that the method is semantically sound
(i.e. correct) and complete (i.e. always applicable). Then we explain a compositional
presentation of Floyd-Naur's method inspired by Hoare logic where proofs are given by
induction on the syntactical structure of programs. These two approaches are shown to be
equivalent in the strong sense that, up to a difference of presentation, they require to verify
exactly the same conditions. Few other partial correctness proof methods are shortly
reviewed and shown also to be variants of the basic Floyd-Naur's method.

In paragraph $ 6, we study total correctness (that is the conjunction of partial
correctness, absence of runtime errors, deadlock freedom and termination) and more
generally liveness proof methods. After giving a short mathematical recall on well founded
relations, well orderings and ordinals, we review Floyd's well-founded set method and
Burstall's intermittent assertion method.

Paragraph $ 7 is devoted to Hoare logic for while-programs.
In paragraph 7.1, we first start from a semantical point of view (mathematicians
would say a nai"ve set-theoretic approach) relative to a fixed interpretation specified by the
operational semantics. Hoare logic is therefore understood as a set of general theorems for
proving partial correctness by structural induction on the syntax of commands. This
approach is sound (correct) and complete (always usable) with respect to the operational
semantics. In practice, proofs can be presented informally using Owicki's proof outlines
(that is by attaching comments to program points) which is equivalent to Floyd's method.

In paragraph 7.2, we study Hoare logic from a syntactical point of view
(mathematicians would call it a logical approach) where predicates are required to be
machine representable and proofs to be machine checkable (but not necessarily machine
derivable). Therefore predicates P, Q and correctness formulae "{P} C {Q}" are now

considered as strings of symbols written according to a precise syntax with no fixed
meaning. Provability is formalized by rewriting rules for deriving valid theorems by
successive transformations of given axioms. More precisely, we first define the syntax of
first-order predicates P, Q (allowing only to quantify over elements, but not over subsets or
functions) and correctness formulae "{P} C {Q}". Then we introduce Hilbert-like
deductive systems (consisting of axioms such as "{P[X  \Xi  E]} X := E {P}"  and  rules
of inference such as "from {P} C 1 {Q} and {Q} C 2 {R} infer {P} C 1; C2 {Q}") and

define formal proofs (i.e. finite sequences of formulae, each of which is either an axiom or
else follows from earlier formulae by a rule of inference). This leads to Hoare's classical
proof system   and to Hoare's proof system   for proof outlines. Finally we define free
and bound variables in predicates so as to precisely specify the syntactical rules of
substitution P[X \Xi  E].

Paragraph $ 7.3 makes the link between the semantical point of view (i.e. the usual
mathematical reasoning about truth with respect to the interpretation of programs specified
by the operational semantics) and the syntactical point of view (where  provability is
understood as correctness formulae manipulations according to the formal rules of Hoare
deductive system  ). For that purpose we define the semantics of predicates P, Q and
correctness formulae "{P} C {Q}" in accordance with the relational semantics but
parametrized by the meaning of basic symbols +, *, <,... which can be left unspecified.

Paragraph $ 7.4 studies the link between truth (the semantical point of view) and
provability (the syntactical point of view). Kurt Go"del showed that truth and provability do
not necessarily coincide: provable implies true, refutable implies false but some formulae
may be undecidable that is neither provable nor refutable (using proofs that can be checked
mechanically) although they are either true or false. Therefore the question is whether
Hoare's formal proof system captures the true partial correctness formulae, only these
(soundness) and ideally all of these (completeness).

Soundness is proved in paragraph $ 7.4.1.
Completeness and incompleteness issues for Hoare logic are discussed in paragraph $
7.4.2. It is shown that the formalism of while-programs is more powerful than the
formalism of first-order predicates in the sense that, for example, the reflexive transitive
closure of a given basic relation is easily defined by a program whereas it is not (in general)
first-order definable (except when full arithmetic on natural numbers is available).
Incompleteness results for Hoare logic follows since intermediate assertions needed in the
correctness proofs of while loops cannot in general be expressed. The consequence is that
although the logical language might be simple enough so that all true facts about data
expressible in this language are machine derivable, some programs using the same basic
symbols might be either unspecifiable or unprovable within this restricted language. It
follows that the logical language must be enriched up to the point where arithmetic is
included. Hence the logical language can be used to describe its own deductive system but
then, as shown by Kurt Go"del, the deductive system is not powerful enough to prove all

true facts expressible in the logical language. It follows that we can only prove Cook's
relative completeness of Hoare logic assuming that the logical language is expressive
enough to specify the intermediate assertions and that all needed mathematical facts about
the data of the program are given. (The alternative which consists in using richer secondorder logical systems is not considered since proofs would then not even be machine
checkable). Expressiveness can be defined a` la Clarke (using Dijkstra's weakest liberal
preconditions) or a` la Cook (using strongest liberal postconditions), both notions of
expressiveness being equivalent. Examples of inexpressive (abacus arithmetic) and
expressive (Peano arithmetic) first-order languages are given. It is shown that
expressiveness is sufficient to obtain relative completeness but it is not necessary. Finally
we study Clarke's characterization problem which consists in characterizing which
programming languages have a sound and relatively complete Hoare logic. This is not the
case of Algol-like or Pascal-like languages (since soundness and relative completeness of
Hoare logic would imply the decidability of the halting problem for finite interpretations
where variables can only take a finite number of values). The intuitive reason is again that
first-order logical languages are less powerful than Algol or Pascal-like languages which
can manipulate a potentially infinite run-time stack.

In paragraph $ 8, we briefly consider complements on Hoare logic: data structures
($ 8.1), undefinedness due to runtime errors ($ 8.3), aliasing and side effects ($ 8.4),
block structured local variables ($ 8.5), goto statements ($ 8.6), functions and
expressions with side effects ($ 8.7), coroutines ($ 8.8) and a guide to the literature on
examples of program verification ($ 8.11) and other logics extending first-order logic with
programs ($ 8.12). Three topics are treated more extensively:

In paragraph $ 8.2 we consider procedures. First we define the syntax and relational
semantics of recursive parameterless procedures. Then we consider partial correctness
proofs based upon computation induction (a generalization of Scott induction) which leads
to Hoare's recursion rule. Since this only rule is not complete, we consider Park's fixpoint
induction, which, using auxiliary variables, can be indirectly transcribed into Hoare's rule
of adaptation. Then these rules are generalized for value-result parameters and numerous
examples of application are provided. References to the literature are given for variable
parameters and procedures as parameters.

Parallel programs are handled in paragraph $ 8.9. First we define the syntax and
operational semantics of parallel programs with shared variables and await commands. We
review a number of a` la Floyd proof methods for such parallel programs using the unifying
point of view of paragraph $ 5 where proof methods are shown to derive from a single
induction principle and only differ by the way of decomposing the global invariant on
memory states and control states (or auxiliary variables) into local invariants. In particular
we study in detail the Lamport and Owicki & Gries method as well as various strengthened
or weakened versions, its stepwise (a` la Floyd) and syntax-directed (a` la Hoare)

presentations. This proof method is formalized by Owicki & Gries logic and a
compositional version is given as introduced by Stirling following Jones. We end by a
guide to the literature on Hoare logics for communicating sequential processes.

In paragraph $ 8.10 we consider proof systems for total correctness. Such proof
systems cannot be of pure first-order logical character and must incorporate an external
well-founded relation. We first consider Harel proof rule for while-programs with finitely
bounded nondeterminism and its arithmetical completeness. Then we explain the use of
transfinite ordinals to deal with unbounded nondeterminism, a situation where a program
state may have infinitely many possible successor states. This leads to the total correctness
of fair parallel programs. Finally we introduce Dijkstra's weakest preconditions calculus for
unbounded nondeterminism.

Numerous references to the literature are given at paragraph $ 9.

1 . 6 Hints for reading this survey

This chapter can be read in a number of ways. In all cases, proofs should be omitted
on first reading. Here are two examples of non sequential readings:

* Readers purely interested in Hoare logic can read as follows: $ 2 (logical, set and
order theoretic notations), $ 3 (syntax and semantics of the programming language), $ 4
(partial correctness of a command), $ 5.1 (an example of partial correctness proof by
Floyd-Naur method), $ 5.3 (a Hoare style presentation of Floyd-Naur partial correctness
proof method by syntax-directed induction), $ 7.1.1 (general theorems for proof
construction), $ 7.2.1 (syntax of predicates and correctness formulae), $ 7.2.2 (deductive
systems and formal proofs), $ 7.2.3 (Hoare's proof system  ), $ 7.2.5 (syntactical rules
of substitution). Then the various complements on Hoare logic of paragraph $ 8 can be read
in any order or readers more interested in the soundness and completeness problems can go
on by $ 7.1.2 (semantical soundness and completeness), $ 7.3 (the semantics of Hoare
logic), $ 7.4 (the link between syntax and semantics : soundness and completeness issues
in Hoare logic).

* Readers interested in liveness proof methods but willing to ignore logic can read
successively $ 2 (logical, set and order theoretic notations), $ 3 (syntax and semantics of
the programming language), $ 4 (partial correctness of a command), $ 5 (Floyd-Naur
partial correctness proof method and some equivalent variants), $ 6 (liveness proof
methods), $ 8.9.1 (operational semantics of parallel programs with shared variables), $
8.9.2 (a` la Floyd proof methods for parallel programs with shared variables), $ 8.10.4
(Dijkstra's weakest preconditions calculus).

2 . L o g ic a l,   s et   a n d   o r d er   t h eo r et i c

n o t a t i o n s

For terms of logical character we use the following notations : tt denotes  truth, ff
falsity, ~ negation, \Pi  conjunction, \Sigma  inclusive disjunction , \Upsilon  logical implication , = (or
\Phi ) logical equivalence, \Delta  v. p universal quantification (p is true for any v), \Lambda  v. p existential
quantification (there are some v such that p is true), \Lambda ! v. p unique existential quantification
(there is a unique v such that p is true).  \Delta  v \Omega  E. p is an abbreviation for  \Delta  v .((v \Omega  E) \Upsilon  p)
and  \Lambda  v \Omega  E. p is  \Lambda  v. ((v \Omega  E)  \Pi  p).  \Delta  v1, ..., v n \Omega  E. p is an abbreviation for  \Delta  v1 \Omega  E.
... \Delta  vn \Omega  E. p. The same way  \Lambda  v1, ..., vn \Omega  E. p is an abbreviation for  \Lambda  v1 \Omega  E. ... \Lambda  vn \Omega 
E. p. We write (B ff X fi Y) to denote X when B is true and Y otherwise.

 (respectively  ,  +) is the set of integers (positive, strictly positive integers). As
usual in computer science, + is the addition, - the subtraction, * the product,  div the
division,  mod the modulus and ** the exponentiation of integers.  odd(x) (respectively
even(x)) is true if and only if x is an odd (respectively even) integer.

We accept the intuitive concept of a set as a collection of objects called elements of the
set. The notation  e \Omega  E means that e is an element of the set E and e  fl E = ~(e \Omega  E).
The void set is denoted o/. E  ffi E',  E ffl E' and E = E' respectively denote  proper
inclusion, inclusion and  equality of sets and (E  i E') = (E' ffi E),  (E j E') = (E' ffl E)
and (E  ` E') = ~(E = E'). We use the set theoretic operations  ' (union), ^ (intersection)
and - (difference). If a set D is fixed then for subsets E of D the complement ~E of E is (D
- E). If E is a set then P(E) (called the power set of E) denotes the set of all subsets of E. If
E 0, ..., E n-1 are sets, the  Cartesian product  E 0 x ... x En-1 is the set of n-tuples
< e 0 ,   ... ,   e n-1> , with e i \Omega    E i for i = 0, ..., n - 1. In symbols : E 0 x  ...   x  E n-1  =
{<e0, ..., en-1> : e0 \Omega  E0 \Pi  ...  \Pi  en-1 \Omega  En-1} where ":" reads "for those which satisfy".
The projection <e0, ..., en-1>i is the i-th component e i of the n-tuple  <e0, ..., en-1>. If n
> 1, the  elimination  <e0, ..., en-1>~i  is the (n - 1)-tuple  <e0, ... , ei-1, ei+1, ... , en 1>. If E 0 = ... = En-1 = E  then  En = E 0 x ...  x En-1. We define E 0 to be {o/} and
identify E1 with E. The cardinality of a set E is denoted | E | hence if E is finite, | E | is the

number of elements of E.

Given a positive integer n and a set E, we define an n-ary relation r on E as a subset
of En. n is called the arity of r and is denoted  #r. We say that r is  binary when #r = 2 and
often use the infix notation x r y for <x, y> \Omega  r. For example we write x <= y to mean that x
is less than or equal to y.

If r, r'  ffl   E   x  E are binary relations on E then r  *  r' = { < e, e' >   :

\Lambda  e" \Omega   E. <e, e"> \Omega   r  \Pi   <e", e'> \Omega   r'} is the  product of r and r', r- 1 = {<e', e> : <e ,
e'> \Omega  r} is the  inverse of r. If, moreover, p  ffl E is a subset of E then p  * r = {<e, e'> \Omega  r : e

\Omega  p} is the  left restriction of r to p and r  _ p = {<e, e'> \Omega  r : e' \Omega  p} is the  right restriction
of r to p. The equality relation on E also called the  diagonal of E2 is * = {<e, e> : e \Omega  E}.

The power of a binary relation r on a set E is defined by recurrence as r 0 = *, rn+1 =
rn * r for n >= 0. The (strict) transitive closure r+ of r is r+ = '{rn : n > 0} and the  reflexive

transitive closure r* of r is r* = r0 ' r+.

A partially ordered set  is a pair  <E, <=> where E is a nonvoid set and  <= is a binary
relation on E which is  reflexive (\Delta  a \Omega  E. a <= a), antisymmetric (\Delta  a, b \Omega  E.(a <= b \Pi  b <= a)
\Upsilon  (a = b)) and transitive ( \Delta  a, b, c \Omega  E.(a <= b \Pi  b <= c) \Upsilon  (a <= c)). Given P  ffl E, a \Omega  E is an
upper bound of P if \Delta  b \Omega  P. b <= a; a is the  least upper bound  of P (in symbols  lub P) if a
is an upper bound of P and if b is any upper bound of P then a <= b. If lub P exists, then it is
unique.  Lower bounds  and the  glb (greatest lower bound ) are defined dually, that is by
replacing <= by its inverse (also called  dual) >= defined by (a  >= b) = (b  <= a). A  complete
lattice is a partially ordered set  <E, <=> such that  lub P and  glb P exist for all P, P  ffl E. It
follows that E has a greatest element or supremum T = lub E = glb o/ and a least element or
infimum ss = glb E = lub o/. <P(E), ffl> is a complete lattice such that  lub = ', glb = ^, T = E
and ss = o/.

Given two sets A and B and a binary relation  ae on A ' B, we call  ae a function of A
into B (and write  ae : A ff B) if ( <a, b> \Omega  ae) \Upsilon  (a \Omega  A \Pi  b \Omega  B) \Pi  (\Delta  a \Omega  A. \Lambda ! b \Omega  B.<a, b>
\Omega  ae). A = dom ae is called the domain and B = rng ae is the range of ae. We use the functional
notation b = ae(a) instead of <a, b> \Omega  ae. The set of all functions of A into B will be denoted
BA or more frequently A ff B.

We specify a function ae : A ff B without giving it a name using the lambda notation
"oe x : A ff B. e" where the expression e is such that  \Delta  a \Omega  A. ae(a) = e(a). We write  oe x \Omega 
A. e (respectively oe x. e) when B (respectively A and B) can easily be inferred from e. If ae :
A ff B then  ae[a \Xi  b] is the function  ae' : A ' {a} ff B ' {b} such that  ae'(a) = b and  \Delta 
a' \Omega  A .((a'  ` a)  \Upsilon  (ae'(a') =  ae(a'))). A function  ae : {a1, ..., an} ff {b1, ..., b n} such that
ae(ai) = b i for i = 1, ..., n will be simply written [a 1 \Xi  b1, ..., a n \Xi  bn] so that [a 1 \Xi  b1,
..., a n \Xi  bn](ai) = b i.

A family o/ = <o/i : i \Omega  I> of elements of E is a function  ae from the index set I into the
set E, where o/i = ae(i). When I ffl  , o/ is called a sequence of elements of E. More precisely,
if I = o/ then  o/ is called an  empty sequence  and is written  AE. When I =  , it is an  infinite
sequence  and we write  o/ = o/0, ...,  o/i, ... . When n  \Omega    and I = {i : 0  <= i < n},  o/ is

called a finite sequence of length n and we write o/ = o/0, ..., o/n-1. We define seqn E to be the
set of sequences of elements of E of length n  >= 0, seq* E = 'n >= 0 seqn E, seqOE E to be the
set of infinite sequences of elements of E and  seq E =  seq* E ' seqOE E. Moreover, the
concatenation  o/e of e \Omega  E to the right of  o/ is defined by  AEe = e, o/e = o/0, ..., o/n-1, e if o/ is a

finite sequence of length n and o/e = o/ if o/ is an infinite sequence.

Let  <A,  <=> and  <B,  O/> be partially ordered sets.  ae : A  ff  B is  monotone (or
increasing) if \Delta  a, b \Omega  E. (a <= b) \Upsilon  (ae(a) O/ ae(b)). x \Omega  A is a fixpoint of ae : A ff A if ae(x) =
x. It is a  prefixpoint if x  <= ae(x) and a  postfixpoint if ae(x) <= x. Let  <L, <=> be a complete

lattice with infimum  ss and  ae : L ff  L be monotone. The set {x  \Omega  L : ae(x) = x}  of
fixpoints of ae is a (nonempty) complete lattice for  <= with infimum  lfp ae = glb{x \Omega  L : ae(x)
<= x} and supremum  gfp ae = lub{x \Omega  L : x <= ae(x)} ( TARSKI [1955]). An  increasing chain
is a sequence  o/ of elements of A such that  \Delta  i \Omega  dom o/ - {0}.  o/i-1 <= o/i. ae : L ff L is  uppercontinuous if lub{ae(o/i) : i  \Omega  dom o/} =  ae(lub{o/i : i \Omega  dom o/}) for any increasing chain  o/ of
L. If  ae is upper-continuous then lfp ae = lub{aen(ss) : n >= 0} where \Delta  x \Omega  L.  ae0(x) = x
and\Delta  n \Omega   . \Delta  x \Omega  L.  aen+1(x) =  ae(aen(x)), ( KLEENE [1952] ). In particular, if r is a binary
relation r on a set E then r * is uniquely determined by the facts that r * = * ' r * r* (or

r* = * ' r* * r) and if x  ffl E x E is such that x =  * ' r * x (or x  = * ' x  * r) then r * ffl x

that is r *  = lfp oe   x  :  E2 ff   E 2 .  *  '  r  *  x (or r *  = lfp oe  x  : E 2  ff  E 2 .  *  '  x  *  r). If
<L, <=, ss> is a complete lattice,  <P, O/> is a poset with infimum  ss', ! : L ff P is strict ( !(ss)
= ss'), upper-continuous and  ! * ae =  " * ! and moreover  ae : L  ff L and  " : P ff P are

monotone then  !(lfp ae) = lfp ".

3 . S y n t a x   a n d   s e m a n t i c s   o f   t h e

programming language

Since Hoare logic is closely bounded to a programming language, we introduce a
very simple Pascal-like (WIRTH [1971]) language. We first define its syntax that is the set
of well-formed programs. We use an  abstract syntax  describing the structure of
programs by trees (McCARTHY [1963]) and leave unspecified the concrete syntax (where
programs are linear strings of tokens to be parsed unambiguously ( AHO, SETHI &
ULLMAN [1986])). Then we define the  operational semantics of the language that is the
effect of executing syntactically correct programs. Traditionally, one imagines the
program running on an abstract machine with primitive instructions ( NEUHOLD [1971]).
Its execution steps can be idealized by mapping abstract syntactic constructs in the
program to a transition relation on configurations (KELLER [1976]). The language is nondeterministic since variables can be assigned random values so that a configuration may
have several possible successors. An execution of the program is a finite or infinite
sequence of configurations representing successive observable states of the machine
during execution. When observation of intermediate configurations is unnecessary, we
use a relational semantics directly defining the relationship between initial and final
configurations of program executions (HOARE & LAUER [1974]). This relational semantics
is latter used as a basis for justifying Hoare's logic ( HOARE [1969]). This  axiomatic
semantics associates an axiom or a rule of inference with each kind of basic or
structured statement of the language which states what we may assert after execution of
that statement in terms of what was true beforehand.

Such complementary definitions of programming language semantics
(HOARE & LAUER [1974],  DONAHUE [1976],  APT & PLOTKIN [1986]) are useful for
describing the semantics at various levels of details. This is a natural extension of
FLOYD [1967a] and HOARE [1969] primitive idea that "the specification of proof techniques
provides an adequate formal definition of a programming language" where "an" had to
be replaced by "one of" because not all methods have the same power of expression.

3 . 1 Syntax

Basic commands of our programming language are the null command "skip" and
the assignment "X := E" of the value of an expression E to a programming variable X or
the nondeterministic assignment "X := ?" of a random value to X. Commands C 1, C2
can be composed sequentially "(C1; C2)" and conditionally "(B ff C1 fi C2)" according

to the value of a Boolean expression B. A command C can also be iterated "(B * C)"
while B holds.

This abstract syntax is more formally defined below. For the time being, the sets
 of programming variables (ranged over by X),   of expressions (ranged over
by E) and   of Boolean expressions (ranged over by B) are assumed to be given.

 and   will be detailed later. The set   of commands (ranged over by C) is
the smallest set closed under the given formation rule (similar to a context-free grammar
expressed in Backus-Naur Form (BNF,  NAUR [1960])):

DEFINITION  Syntax of commands

X : Programming variables
E : Expressions
B : Boolean expressions
C : Commands (1)

C ::= skip | X := E | X := ? | (C1 ; C2) | (B ff C1 fi C2) | (B * C)

When necessary we omit parentheses and use a Pascal-like concrete syntax.
Example  A program to compute x ** y (2)

The program with concrete syntax :

Z := 1; (3)
while Y<>0 do

if odd(Y) then
  begin Y := Y - 1; Z := Z * X end

else

begin Y := Y div 2; X := X * X end;

has the following abstract syntax:

(Z := 1; (Y<>0 * (odd(Y) ff (Y := Y-1; Z := Z*X) fi (Y := Y div 2; X := X*X)))) (4)
\Delta 

The components of a command C are C itself and the commands appearing in C
together with their components. Two instances of the same command C' in a command
C should be considered as different components of C. Therefore we mark components
C' \Omega   C  of a command C by the Dewey number designating the root of the
subtree of C' in the syntactic tree of C. For example,  ((X := 1; X := 1); X := 1)
= {((X := 1; X := 1); X := 1) AE,  (X := 1; X := 1)0, X := 1 00,  X := 101,  X := 11} so
that  "X := 100" is the first, "X := 1 01" the second and "X := 1 1" the third instance of
assignment command "X := 1" in the sequence "((X := 1; X := 1); X := 1)". More
formally:

DEFINITION  Components of a command (5)

# C   = {C #} if C is skip, X := E or X := ? (.1)

# (C1; C 2) = {(C1; C 2)#} '  #0 C1  '  #1 C2 (.2)
# (B ff C1 fi C2) = {(B  ff C1 fi C2)#} '  #0 C1  '  #1 C2 (.3)

# (B * C)   = {(B * C) #} '  #0 C (.4)

so that :

C   = AE C . (.5)

C   = C ^ {(B*C') # : B \Omega    \Pi  C' \Omega   \Pi  # \Omega  seq* {0, 1} } (.6)
  = { C  : C  \Omega   } (.7)

= { C  : C  \Omega   } (.8)

Example  Components of program (4) (6)

C

1000 1001 1011
(Z := 1; (Y <> 0 * (odd(Y) (Y := Y-1; Z := Z*X) fi (Y := Y div 2; X := X*X)))) ff

C C C C

C C

C
C
C

1

10
100 101

1010

0
\Delta 

When there is no ambiguity, the exponent  # of a component C # will be omitted
and we identify   with  .

3 . 2 Operational semantics

A state (or valuation) is a function s with domain a given set   of variables
(including the set   of programming variables) and range a nonempty set  D of
objects (called data in computer science). s(v) is called the value of variable v in state s:

d : D Data (7)
s : S =   ff D States (8)

The operational semantics of a command C is a model of its execution. Such an
execution will be understood as a finite or infinite sequence " o/0, ...,  o/n, ..." of
configurations such that  <o/n, o/n+1> \Omega  op C  and  op C  is the operational transition
relation. The initial configuration  o/0 has the form  <s0, C> where s0 records the initial
values of variables. Configuration o/n has the form <sn, Cn> where sn records the current
values of variables after execution of n program steps and C n records the command
remaining to be executed. If execution terminates in configuration  o/l then o/l = sl where
sl records the final values of variables. In particular,  <<s, C>, <s', C'>> \Omega  op C  means
that one step of execution of C in state s can lead to state s' with C' being the remainder

of C to be executed. If  <<s, C>, s'> \Omega  op C  then execution of C in state s can lead to
state s' in one step and is then terminated. Hence we have:

o/ : \Gamma  = ( S x  ) ' S Configurations (9)
op :  ff P(\Gamma  x \Gamma ) Operational transition  (10)

relation

The definition of the semantics of expressions will be postponed. For the time
being, we will assume that the semantics of expressions is given: if E  \Omega    and s \Omega  S
then E(s) = I E (s) \Omega  D is the value of expression E in state s. The same way, if B  \Omega 

 then I B  also written B is the set of states s such that B holds in state s:

I : ff (S ff D), E = I E Semantics of expressions (11)
I : ff P(S), B = I B Semantics of Boolean

expressions (12)

The operational transition relation is defined by structural induction on the abstract
syntax of commands (in the style of  PLOTKIN [1981]  but using a direct recursive
definition) as follows :

DEFINITION  Operational transition relation  (13)
op skip   = { <<s, skip>, s > : s \Omega  S} (.1)
op X := E   = { <<s, X := E >, s[X  \Xi  E(s)]> : s  \Omega  S} (.2)
op X := ?   = { <<s, X := ? >, s[X  \Xi  d]> : s \Omega  S \Pi  d \Omega  D} (.3)
op (C1 ; C2)   = { <<s,(C1';C2)>,<s',(C1";C2)>>: <<s,C1'>,<s',C1">> \Omega  op C1 } (.4)

' {<<s, (C 1'; C 2)>,<s', C 2>> : <<s, C 1'>, s'> \Omega  op C1 }
' op C2
op (B ff C1 fi C2) = {<<s, (B ff C1 fi C2)>, <s, C1>> : s \Omega  B} ' op C1  (.5)

' {<<s, (B ff C1 fi C2)>, <s, C2>> : s fl B} ' op C2
op (B * C)   = { <<s, (B * C) >, <s, (C; (B * C)) >> : s \Omega  B} (.6)

' {<<s, (C'; (B * C)) >, <s', (C"; (B * C)) >> :

<<s, C' >, <s', C" >> \Omega  op C }
' {<<s, (C'; (B * C)) >, <s', (B * C) >> : <<s, C'>, s'> \Omega  op C }
' {<<s, (B * C) >, s> : s fl B}

Example  Operational semantics of program (4) (14)

Using the components (5) of program (4) given at example (2), we can define
labels:

(15)

L 1 = C AE = C, L 2 = C 1, L 3 = (C 10; C 1), L 4 = (C 100; C 1), L 5 = (C 1001; C1),
L6 = (C101; C1), L7 = (C1011; C1), including a final label L 8 represented by the
symbol "%"

 so as to name program points as follows:

(16)

L LLLLLL2 L1
(Z := 1; (Y <> 0 * (odd(Y) (Y := Y-1; Z := Z*X) fi (Y := Y div 2; X := X*X)))) ff

3 4 5 6 7 8

A label L can also be understood as designating the command remaining to be executed
when control is at that point L. According to definition (13), the operational transition
relation of program (4) is:

op C = {<<s, L 1>, <s[Z \Xi  1],  L2>> : s  \Omega  S} (17)

' {<<s, L2>, <s, L3>> : s(Y)  ` 0}
' {<<s, L3>, <s, L4>> : odd(s(Y))}
' {<<s, L4>, <s[Y \Xi  s(Y) - 1], L 5>> : s \Omega  S}
' {<<s, L5>, <s[Z \Xi  s(Z) * s(X)], L 2>> : s \Omega  S}
' {<<s, L3>, <s, L6>> : even(s(Y))}
' {<<s, L6>, <s[Y  \Xi  s(Y) div 2], L 7>> : s \Omega  S}
' {<<s, L7>, <s[X \Xi  s(X) * s(X)], L 2>> : s  \Omega  S}
' {<<s, L2>, s> : s(Y) = 0}

A possible execution sequence of this program is therefore as follows:
<[X \Xi  3, Y \Xi  2, Z \Xi  0], L1>, <[X \Xi  3, Y \Xi  2, Z \Xi  1], L2>, <[X \Xi  3, Y \Xi  2, Z \Xi  1],
L3>, <[X \Xi  3, Y  \Xi  2, Z  \Xi  1], L 6>, <[X \Xi  3, Y  \Xi  1, Z  \Xi  1], L 7>, <[X \Xi  9, Y  \Xi  1,
Z \Xi  1], L 2>, <[X \Xi  9, Y  \Xi  1, Z  \Xi  1], L 3>, <[X \Xi  9, Y  \Xi  1, Z  \Xi  1], L 4>, <[X \Xi  9,
Y \Xi  0, Z \Xi  1], L5>, <[X \Xi  9, Y \Xi  0, Z \Xi  9], L2>, <[X \Xi  9, Y \Xi  0, Z \Xi  9]>. \Delta 

3 . 3 Relational semantics

The relational semantics or interpretation I C  (also noted C) of a command C is a
relation between states such that  <s, s'> \Omega  C if and only if an execution of command C
started in initial state s may terminate into final state s' :

DEFINITION  HOARE & LAUER [1974]  Relational semantics (18)

I : ff P(S x S)
I C = C = {<s, s'> : <<s, C >, s'> \Omega  op C *}

The relational semantics can be characterized as follows :

THEOREM  HOARE & LAUER [1974], GREIF & MEYER [1981] (19)

skip  = {<s, s> : s \Omega  S} (.1)
X := E  = {<s, s[X \Xi  E(s)]> : s \Omega  S} (.2)
X := ?  = {<s, s[X \Xi  d]> : s \Omega  S \Pi  d \Omega  D} (.3)
(C1 ; C2) = C1 * C2 (.4)

(B ff C1 fi C2) = ( B * C1) ' (~B * C2) (.5)
(B * C)  = (B * C)* _~B (.6)

= lfp oe X. (* _~B) ' ((B * C) * X) (.7)

(B * C) is the unique r ffl S2 such that: (.8)

. r ffl (S x ~B) (.8.a)
. \Delta  q ffl S. (q * (B * C) ffl S x q) \Upsilon  (q * r ffl S x q) (.8.b)
. ((B * C) * r) ffl r (.8.c)

. (* _~B) ffl (* ^ r) (.8.d)

Example  Calculus of the relational semantics of a simple program (20)

Assume D =  , the relational semantics of C =  (Y <> 0 * Y := Y - 1)  is
C   =  {<s, s[Y \Xi   0]> : s(Y)  >= 0}.

To prove this, observe that by (19.7),  C = lfp F where F(X) = ( * _ ~Y<>0) '
((Y<>0 * Y := Y-1) * X) = { <s, s > : s(Y)  = 0} ' {<s, s[Y  \Xi   s(Y) - 1]> : s[Y] ` 0}  *

X. Define X i = Fi(o/). We have X 0 = o/, X 1 = {<s, s[Y  \Xi  0]> : s(Y) = 0}. Assume
by induction hypothesis that X i = { <s, s[Y  \Xi  0]> : 0  <= s(Y)  <= i - 1}. Then X i+1 =
F ( X i) = { < s ,   s >  : s(Y) = 0}  '   {< s, s[Y  \Xi   s(Y) - 1] >  : s[Y]  ` 0}  *

{ < s ,   s[ Y   \Xi    0 ]>  : 0  <= s(Y)  <= i - 1} = { < s, s >  :  s(Y) = 0}  '   {< s, s'>  :  \Lambda   s " .
s[Y] ` 0  \Pi   s" = s[Y  \Xi   s(Y) - 1]  \Pi   0  <= s"(Y)  <= i - 1  \Pi   s' = s"[Y  \Xi   0]}  =
{ < s ,   s >  : s(Y) = 0}  '   {< s ,   s [ Y   \Xi    s ( Y )   -   1 ] [ Y   \Xi    0 ] >  : s[Y]  ` 0  \Pi 
0   <=   s [ Y   \Xi   s(Y) - 1](Y)  <= i - 1} = { < s, s>  : s(Y) = 0}  '   {< s ,  s[Y  \Xi    0]>   :
1 <= s(Y) <= i} = { < s, s[Y \Xi   0]>  :  0 <= s(Y) <= i}. It follows that  C   = lfp F =
' i >= 0   X i   = { < s ,   s [ Y   \Xi    0 ] >   :   \Lambda    i   >=   0 .   0   <=   s ( Y )   <=   i }   =

{<s, s[Y \Xi   0]> : 0 <= s(Y)}.    \Delta 

Proof of theorem 10:

For short we write op instead of op C  when C is clear from the context.

* (19.1), (19.2) : skip and X := E are handled the same way as :

* (19.3) : X := ? = { <s, s' > :  <<s, X := ? >, s' > \Omega   op*} [by (18)] = { <s, s' > :
< < s ,   X   : =   ? > , s'>  \Omega   *  '  op  *  o p * } [since r *  =  *  '  r  *  r* ] = { < s, s' >  :  \Lambda   o/  \Omega   \Gamma .

<<s, X :=  ?>, o/> \Omega  op \Pi  <o/, s'> \Omega  op*} [since  <s, X := ? >  ` s'] = { <s, s' > : d  \Omega  D  \Pi 
<s[X \Xi  d],  s'> \Omega  op*} [by (13.3)] = { <s, s[X \Xi  d]> : s  \Omega  S \Pi  d \Omega  D} [since by (13),
<o/", o/'> \Omega  op implies  o/" fl S so that  <s", s'> \Omega  op* if and only if s" = s'  \Omega  S].

* ( 1 9 . 4 )   :   < s, s' >  \Omega   ( C 1 ; C 2 )  \Phi   < < s, (C 1 ; C 2 ) > , s' >  \Omega   o p *  [by (18)]  \Phi 
(\Lambda  s". <<s, C1>, s"> \Omega  op* \Pi  <<s", C 2>, s' > \Omega  op*) [by (13.4)]  \Phi  (\Lambda  s".  <s, s" > \Omega  C1
\Pi  <s", s'> \Omega  C2) [by (18)]  \Phi  <s, s'> \Omega  C1 * C2 [by definition of  *].

* (19.5) : (B  ff  C 1  fi  C2) = { < s, s' > :  \Lambda   o/  \Omega   \Gamma .<<s, (B  ff  C 1 fi  C2)>, o/>  \Omega   op \Pi 
< o/, s'>  \Omega   o p * } = { < s, s' >  : s  \Omega   B  \Pi   < < s, C 1 > , s' >  \Omega   o p * }  '   {< s, s' >  : s  fl  B  \Pi 
<<s, C2>, s'> \Omega  op*} = ( B * C1) ' (~B * C2) [by (13.5)].

* (19.6) : <s, s' > \Omega  (B * C)* _ ~B if and only if there are n  >= 1 and s 1, ..., s n \Omega  S
such that s = s 1 and for all i = 1, ..., n - 1, s i \Omega  B and  <si, si+1> \Omega  C and s n = s'  fl B
that is if and only if there is an execution sequence of the form " <s1, (B * C) >,
<s1, (C; (B  *  C))>, ...,  <s2, (B * C) >, <s2, (C; (B  *  C))>, ...,  <sn, (B * C) >, s n"
with s = s1, ..., sn-1 \Omega  B and sn = s' fl B hence if and only if  <s, s'> \Omega  (B * C).

* (19.7) : Let F =  oe X.  * ' (B * C) * X and G =  oe X. ( * _ ~B) ' (B * C) * X. We

have F 0(o/) _ ~B   =  G0(o/) = ( * _ ~B ). Assume by induction hypothesis that
F n(o/) _ ~B   =  Gn(o/) then F n+1(o/) _ ~B  = F(F n(o/)) _ ~B   =  (* _ ~B ) '  ((B * C ) *

Fn(o/)  _ ~B) = (* _ ~B) ' ((B * C) * Gn(o/)) = G n+1(o/). Since r * = lfp oe x. * ' r * x, it
follows that  (B * C) =  ( B  * C )* _  ~B   =  (lfp F) _ ~ B   =  (' n>=0 F n(o/)) _ ~B   =
'n>=0 (Fn(o/) _ ~B) = 'n>=0 Gn(o/) =  lfp G.

* (19.8) : We first show that  (B * C) satisfies conditions (a)-(d) :

- (a) (B * C) =  (B * C)* _ ~B ffl S x ~B.
- (b) If (q * r ffl S x q) then by induction on n  >= 0, (q  * rn ffl S x q). This is true
for n = 0 since q  * * =  * _ q  ffl S x q. Moreover q  * rn+1 = q  * (rn * r) = (q  * rn) * r ffl

(S x q)  * r =  S2 * (q * r)  ffl S2 * (S x q) =  S x q. Whence if (q  * (B * C) ffl S x q) then

(q * ( B  * C )n ffl  S  x q) so that q  * (B * C)  =  q * ( B  * C )* _ ~B  ffl  q * ( B  * C )*  =
q * (' n>=0  (B * C)n) =  ' n>=0 (q * (B * C)n) ffl ' n>=0 S x q =  S x q.

- (c) Since (B * C) = ( * _ ~B) ' ((B * C) * (B * C)), we have  (B * C) * (B * C)

ffl (B * C).

- (d) Since  (B * C) = ( * _ ~B ) '  ((B * C ) * (B * C)) we have ( * _ ~B ) ffl

(B * C). so that ( * _ ~B )  =  * ^  (* _ ~B ) ffl * ^  (B * C).

* Assuming that r satisfies conditions (19.8), we show that r = (B * C) :

- (* _ ~B )  =  * ^  r ffl r and  (B * C ) * r ffl r so that r is a postfixpoint of F =

oe X. (* _ ~B) ' ((B * C) * X) whence  (B * C) =  lfp F  ffl r.

- To show that r  ffl (B * C), we assume that  <s, s'> \Omega  r and prove that  <s, s'> \Omega 
(B * C)* _ ~B. Let q = {s 1 \Omega  S : <s, s1> \Omega  (B * C)*}. We have q  * (B * C) = {<s1, s2>
\Omega  (B * C) : <s, s1> \Omega  (B * C)*} ffl {<s3, s2> : <s, s2> \Omega  (B * C)+} ffl {<s3, s2> : <s, s2>
\Omega  (B * C)*} ffl S x q. By (19.8.b), (q  * r ffl S x q) so that s  \Omega  q and  <s, s'> \Omega  r imply s'
\Omega  q whence  <s, s'> \Omega  (B * C)*. Moreover  <s, s'> \Omega  r and (19.8.a) imply s'  \Omega  ~B, so
that  <s, s'> \Omega  (B * C)* _ ~B. \Delta 

Observe that the relational semantics (19) of our language   is not "equivalent"
to its operational semantics (13) because information about program termination is lost.

Example Programs with different operational semantics but identical relational  (21)

semantics

Assuming D =  , C = "Y := 0" and C' = "(Y := ?; (Y <> 0 * Y := Y - 1)) " have
the same relational semantics  C   = C' = {<s, s[Y  \Xi   0]> : s  \Omega   S}. We have  C   =
{<s, s[Y \Xi  0]> : s \Omega  S} by (19.2) and  C = Y := ?  * (Y <> 0 * Y := Y-1) [by 10.4] =

{< s, s[Y  \Xi   d]> : s  \Omega   S  \Pi  d  \Omega   D }  *  {<s, s[Y \Xi   0]>  : 0  <= s(Y)} [by (19.3) and

example (20)] = { <s, s[Y  \Xi   d][Y \Xi   0]  > : 0  <= d} = { < s, s[Y \Xi   0]> : s  \Omega   S} .
Hence no distinction is made between program C for which termination with Y = 0 is
guaranteed and program C' for which termination with Y = 0 is possible but not
guaranteed (with a usual Pascal-like implementation). \Delta 

It follows that if we choose (19) (or latter Hoare logic) as  the definition of the
semantics of   then a faithful implementation of   should ignores the possibility
of nontermination as a viable answer whenever termination is possible. This "angelic"
nondeterminism of FLOYD [1967b] could be implemented by parallelism or breadth search
to simultaneously examine all possible choices offered (we have to assume in a finite
number) by random assignments ( HAREL [1979]) The "demonic" nondeterminism of
DIJKSTRA [1975] [1976] is a radically different alternative where results are valid only if
the program examined always terminates. Again a strictly faithful implementation
should use depth-first backtracking to guarantee nontermination if this is possible for
one choice in random assignments. In practice, nondeterminism is implemented by
choosing arbitrarily or sometimes fairly one of the alternatives offered by (19). Then
angelic and demonic nondeterminism can be understood as describing the best and
worst possible situation ( HOARE [1978a], JACOBS & GRIES [1985]). In conclusion (19) is
an approximate version of (13) where "details" about termination are deliberately
ignored.

4 . P a r t i a l   c o r r e c t n e s s   o f   a

c o m m a n d

An assertion is a set of states. A  specification is a pair  <p, q> of assertions on
states (where p is called the  input specification  or precondition and q is the  output
specification or postcondition). A command C is said to be partially correct with respect
to a specification  <p, q > (written  { p }C{ q }) if any terminating execution of C
starting from an initial state s satisfying p must end in some final state s' satisfying q.
Stated in terms of the relational semantics (18), this means that (p * C) ffl (S x q):

DEFINITION  FLOYD [1967a] , NAUR [1966]  Partial correctness (22)

p, q :  = P(S) Assertions
<p, q> :  =   x  Specifications
{ p }C{ q } :  x   x   ff {tt, ff} Partial correctness
{ p }C{ q } = (p  * C) ffl (S x q)

Example  Partial correctness of program (4) (23)

Assume for program (4) that   is {X, Y, Z, x, y} and D is the set   of integers.
This program is partially correct with respect to the specification <p, q> such that:

p = {s \Omega  S : s(X) = s(x)  \Pi  s(Y) = s(y)  >= 0}
q = {s \Omega  S : s(Z) = s(x) ** s(y)}

Otherwise stated, if x and y  >= 0 respectively denote the initial values of programming
variables X, Y and if the corresponding execution terminates then the final value of Z is
equal to x ** y.  \Delta 

Observe that definition (22) of partial correctness is essentially of semantical
nature. It is relative to the operational semantics (13) and is defined in terms of nai"ve set
theory. No reference is made to a particular formal logical language for describing the
sets of states p and q (called "assertions" for convenience). Therefore we make no
difference between a predicate P (such as "Y  >= 0 \Pi  Z * (X ** Y) = x ** y") and the
assertion  P which its denote (that is "{s  \Omega  S : s(Y)  >= 0  \Pi  s(Z) * (s(X) ** s(Y)) =
s(x) ** s(y)}" in this example). The consequences of restricting assertions p, q to
those that can be formally described by first-order predicates will be studied later at
paragraph $ 7.2.

5 . F l o y d - N a u r   p a r t i a l   c o r r e c t n e s s

p r o o f   m e t h o d   a n d   s o m e
equivalent variants

The first method for proving partial correctness was proposed by  FLOYD [1967a]
and NAUR [1966]. After giving a simple introductory example, we formally derive FloydNaur's method from the operational semantics (13) using an elementary stepwise
induction principle and predicates attached to program points to express invariant
properties of programs. This systematic construction of the verification conditions
ensures that the method is semantically sound (i.e. correct) and complete (i.e. always
applicable). Then we introduce another presentation of Floyd-Naur's method inspired
by Hoare logic where proofs are given by induction on the syntactical structure of
programs. These two approaches are shown to be equivalent in the strong sense that, up
to a difference of presentation, they require to verify exactly the same conditions. Few
other partial correctness proof methods are shortly reviewed and shown also to be
variants of the basic Floyd-Naur's method.

5 . 1 An example of partial correctness proof by

Floyd-Naur method

Example  Partial correctness proof of program (4) (24)

* The informal partial correctness proof of program (4) by Floyd-Naur's method
first consists in discovering predicates  Pk(X, Y, Z, x, y)  associated with each label L k,

k = 1, ..., 8 which should relate the values  X, Y, Z of variables  X, Y, Z whenever
control is at Lk to the initial values x, y, z of these variables:

{L1} {  P1(X, Y, Z, x, y) = (X = x  \Pi  Y = y  >= 0) } (25)

Z := 1;
{L2} {  P2(X, Y, Z, x, y) = (Y  >= 0 \Pi  Z * (X ** Y) = x ** y) }  {-- loop invariant --}

while Y <> 0 do
{L3} {  P3(X, Y, Z, x, y) = (Y > 0  \Pi  Z * (X ** Y) = x ** y) }

if odd(Y) then
  begin

{L4} {  P4(X, Y, Z, x, y) = (Y > 0  \Pi  Z * (X ** Y) = x ** y) }

Y := Y - 1;
{L5} {  P5(X, Y, Z, x, y) = (Y  >= 0 \Pi  Z * (X ** (Y + 1)) = x ** y) }

  Z := Z *X;

end
else

begin

{L6} {  P6(X, Y, Z, x, y) = (even(Y)  \Pi  Y > 0 \Pi  Z * (X ** Y) = x ** y) }

  Y := Y div 2;
{L7} {  P7(X, Y, Z, x, y) = (Y  >= 0 \Pi  Z * (X ** (2 * Y)) = x ** y) }

  X := X * X;

end;

{L8} {  P8(X, Y, Z, x, y) = (Z = x ** y) }

* Observe that predicates P k associated with labels L k, k = 1, ..., 8 can be
understood as describing a set of states Pk. For example:

P2 = {s \Omega  S : s(Y) >= 0 \Pi  s(Z) * (s(X) ** s(Y)) = s(x) ** s(y)}

* Then it must be shown that these predicates satisfy verification conditions, which
can be stated informally as follows:

(AE) p \Upsilon  P1(X, Y, Z, x, y) where p is the input specification (26)
(i1) P1(X, Y, Z, x, y)  \Upsilon  P2(X, Y, 1, x, y)
(i2) [P2(X, Y, Z, x, y)  \Pi  Y ` 0] \Upsilon  P3(X, Y, Z, x, y)
(i3) [P2(X, Y, Z, x, y)  \Pi  Y = 0]  \Upsilon  P8(X, Y, Z, x, y)
(i4) [P3(X, Y, Z, x, y)  \Pi  odd(Y)] \Upsilon  P4(X, Y, Z, x, y)
(i5) [P3(X, Y, Z, x, y)  \Pi  even(Y)]  \Upsilon  P6(X, Y, Z, x, y)
(i6) P4(X, Y, Z, x, y)  \Upsilon  P5(X, Y - 1, Z, x, y)
(i7) P5(X, Y, Z, x, y)  \Upsilon  P2(X, Y, Z * X, x, y)
(i8) P6(X, Y, Z, x, y)  \Upsilon  P7(X, Y div 2, Z, x, y)
(i9) P7(X, Y, Z, x, y)  \Upsilon  P2(X * X, Y, Z, x, y)
(') P8(X, Y, Z, x, y)  \Upsilon  q  where q is the output specification

* These verification conditions imply that the predicates are local invariants, that is
Pk holds whenever control is at point L k, that is to say, according to (15), when
command Lk remains to be executed. In practice it is only necessary to discover loop
invariants since other local invariants can be derived from the loop invariants using
these verification conditions.

* It follows that the  local invariants  on states attached to program points are
equivalent to the following global invariant on configurations: (27)

 i = '

(=1

7  {<s, L

k> : s \Omega  Pk} ' P8

i is called invariant because it is always true during execution:

({<s, C> : s \Omega  p} * op C *) ffl i
It follows immediately that:

(p * C) ffl (S x q)
\Delta 

5 . 2 The stepwise Floyd-Naur partial correctness

proof method

A partial correctness proof can always be organized in the same way and reduced
to the discovery of local invariants which are then shown to satisfy elementary
verification conditions corresponding to elementary program steps. To show this, we
first introduce an induction principle expressing the essence of invariance proofs. Then
we specialize the induction principle for the operational semantics (13) of language

. This consists in representing the global invariant on configurations by local
invariants on states attached to program points. Once properties of programs have been
chosen to be expressed in this way, Floyd's verification conditions can be derived by
calculus from the operational semantics (13). This construction of the verification
conditions for local invariants a priori ensures semantical soundness and completeness
of the proof method.

5 . 2 . 1 Stepwise induction principle

Floyd-Naur's method is usually understood as stepwise induction (MANNA, NESS &
VUILLEMIN [1972]): to prove that some property i of a program is invariant during the
course of the computation, it is sufficient to check that i is true when starting the
computation and to show that if i is true at one step of the computation, it remains true
after the next step. This means that Floyd-Naur's method consists in applying the
following lemma to the operational semantics:

LEMMA  COUSOT [1981]  Stepwise induction principle (28)

if p, p', q \Omega  P(E) and r \Omega  P(E x E) then:
 [(p  * r* _ p') ffl (E x q)] \Phi  [\Lambda  i \Omega  P(E). (p ffl i) \Pi  (i * r ffl E x i) \Pi  (i ^ p' ffl q)]

Proof

* For \Upsilon , we observe that i = {e  \Omega  E : \Lambda  e' \Omega  p. <e', e> \Omega  r*} satisfies conditions p ffl i
and i  * r ffl E x i whereas p  * r* _ p' ffl E x q implies i  ^ p' ffl q.

* For ,, we observe that, by induction on n  >= 0, p  ffl i and i  * r ffl E x i imply that
p * rn ffl E x i whence p  * r* ffl E x i so that p  * r* _ p' ffl E x (i ^ p') ffl E x q. \Delta 

Floyd-Naur partial correctness proof method consists in discovering local
assertions on states attached to program points which must be shown to satisfy local
verification conditions. As shown by example (24), this can be understood as the

discovery of a global assertion i upon configurations which is shown to satisfy a global
verification condition gvc C [p,q](i) derived from lemma (28) :

THEOREM KELLER [1976], PNUELI [1977], COUSOT [1981]  Induction principle for (29)

Floyd-Naur's stepwise partial correctness proof method
{ p }C{ q } = [\Lambda  i \Omega  P(\Gamma ). gvc C [p, q](i)] (.1)

where gvc C [p, q](i) = ( \Delta  s \Omega  p.<s, C> \Omega  i) \Pi  (i * op C  ffl \Gamma  x i) \Pi  (i ^ S ffl q) (.2)

Proof

* { p }C{ q } = (p  * C ffl S x q) [by (22)] = (p  * {<s', s> : <<s', C>, s> \Omega  op C *} ffl
S x q) [by (18)] = ({ <s, C> : s \Omega  p} * op C * _ S ffl \Gamma  x q) = (\Lambda  i \Omega  P(\Gamma ). ({<s, C> : s \Omega  p} ffl
i)  \Pi  (i  * op C  ffl \Gamma  x i)  \Pi  (i  ^  S ffl q)) [by (28)] = ( \Lambda  i \Omega  P(\Gamma ).(\Delta   s \Omega   p.<s, C >  \Omega   i)  \Pi 
(i * o p C  ffl  \Gamma   x i)  \Pi   (i  ^  S  ffl  q)). \Delta 

 Any i satisfying the verification condition gvc C [p, q](i) is always true during
execution hence is a global invariant.. Such a global invariant always exists since there
is a strongest global invariant which implies all others and can be characterized as a
fixpoint:

THEOREM  PARK [1969], CLARKE [1979b], COUSOT [1981]  Fixpoint characterization (30)

of the strongest global invariant
The strongest global invariant

 I = {o/ : \Lambda  s \Omega  p. <<s, C>, o/> \Omega  op C *} (.1)
is such that:

I = lfp oe X : P(\Gamma ). {<s, C> : s \Omega  p} ' {o/ : \Lambda  o/'. <o/', o/> \Omega  X * op C } (.2)
if { p }C{ q } then gvc C [p, q](I) holds and \Delta  i. gvc C [p, q](i) \Upsilon  (I ffl i). (.3)

Proof

* <P(\Gamma ), ffl, o/> is a complete lattice.  ae = oe X :  P(\Gamma  x \Gamma ). * ' X  * op C  and  " =

oe X : P(\Gamma ). {<s, C> :  s  \Omega   p}  '   {o/ : \Lambda  o/'.  <o/',  o/>  \Omega  X  * op C } are monotone.  !  =
oe X : P(\Gamma  x \Gamma ).{o/ \Omega  P(\Gamma ) :  \Lambda  s \Omega  p.  <<s,C>, o/> \Omega  X} is strict, upper-continuous and
! * ae =  " * ! =  oe X. { o/ \Omega  P(\Gamma ) :  \Lambda  s \Omega  p.  <<s, C >, o/>  \Omega  * ' X  * op C }. Therefore

!(lfp ae) = !(op C *) = I =  lfp ".

* Obviously { <s, C > : s  \Omega   p} ffl  I. Moreover I  * op C  = { <o/, o/'> : \Lambda  s  \Omega  p.
<<s, C>, o/> \Omega  op C * \Pi  <o/, o/'> \Omega  op C } ffl {<o/, o/'> : \Lambda  s \Omega  p.  <<s, C >, o/'> \Omega  op C +} ffl \Gamma 
x I). Finally  { p }C{ q } \Upsilon  (p  * C ffl S x q) [by (22)]  \Upsilon  (p  * {<s', s > : <<s', C>, s> \Omega 
op C *} ffl S x q) [by (18)]  \Upsilon  (I _ S ffl \Gamma  x q) \Upsilon  (I  ^ S ffl q).

* if ( \Delta  s \Omega  p. <s, C > \Omega  i)  \Pi  (i  * op C  ffl \Gamma  x i) then  "(i)  ffl i so that I =  lfp "  =
^{X \Omega  P(\Gamma ) : "(X) ffl X}  [by  TARSKI [1955] ] ffl i. \Delta 

Example Application of induction principle (29) to the correctness proof of (31)

program (4)
Let us go on with example (24) and show that (26) is equivalent to (29):

* \Delta  s \Omega  p. <s, C1> \Omega  i is equivalent to p  ffl P1.

* i * op C  ffl \Gamma  x i is equivalent to the conjunction { <s, L k> : s  \Omega  Pk} * op C
ffl \Gamma  x i for k = 1, ..., 7 and  P8 * op C  ffl \Gamma  x i so that the proof can be done by cases
corresponding to each possible transition from configurations  <s, L k> for any s
satisfying predicate Pk attached to point Lk. Using in each case the definition of op C ,
we obtain Floyd's simpler verification conditions:

- For the assignments X k := Ek, k = 1, 4, 5, 6, 7 we have  <<s, Lk>, <s', Lk'>> \Omega 

o p C  if and only if k' = succ(k) where succ = [1  \Xi   2,  4 \Xi   5,  5 \Xi   2,  6 \Xi    7,
7 \Xi  2]. Therefore the corresponding verification conditions are of the following form
given by Floyd:  \Delta  s \Omega  Pk. s[Xk \Xi  Ek(s)]  \Omega  Psucc(k).

For example when k = 4, we have to prove that [s(Y) > 0  \Pi  s(Z) * (s(X) ** s(Y))
=  s(x) ** s(y)]  \Upsilon  [s[Y \Xi  s(Y) - 1]  \Omega  {s' : s'(Y)  >= 0  \Pi  s'(Z) * (s'(X) ** (s'(Y) + 1)) =
s'(x) ** s'(y)}]  or  equivalently  [s(Y) > 0  \Pi   s(Z) * (s(X) ** s(Y)) =  s(x) ** s(y)]
\Upsilon   [(s(Y) - 1) >= 0  \Pi   s(Z) * (s(X) ** ((s(Y) - 1) + 1)) =  s(x) ** s(y)]  which  is
obvious.

- For the test k = 3, we have  <<s, L3>, <s', L k'>> \Omega  op C  if and only if s' = s

and if s  \Omega   Y  < >  0  then k' = 4 else k' = 6, so that we have to prove that
(P3 ^  Y <> 0) ffl P4 \Pi  (P3 ^ Y = 0) ffl P6.

- The same way, for the while loop k = 2, we have to prove that (P2 ^ odd(Y)) ffl
P3 \Pi  (P2 ^ even(Y)) ffl P8.

* ((i ^ S) ffl q) = (P8 ffl q) is equivalent to  P8 = q. \Delta 

5 . 2 . 2 Representing a global invariant on configurations by

local invariants on states attached to program points

However, instead of using a single global invariant i on configurations as in (29),
Floyd and Naur proposed to use local invariants on states attached to program points
(originally, arcs of flowcharts). Such program points L  \Omega   C  for commands C
\Omega    can be understood as labels specifying where control can reside at before,
when or after executing a step within C. According to the operational semantics (13),

C  can be chosen as the set of control states C' of configurations  <s, C' > \Omega 
S x   encountered during execution of command C, together with a final label,
arbitrarily denoted "%", corresponding to configurations o/ \Omega  S for which execution of C
is terminated:

DEFINITION  Labels designating program control points (32)

C  =  C  '  C  '  C (.1)
C  = {C} (.2)
C  = o/  if C is skip, X := E or X := ? (.3)
(C1; C2)  = {(C' 1; C2) : C' 1 \Omega   C1 } '  C2  '  C2 (.4)
(B ff C1 fi C2)  =  C1  '  C1  '  C2  '  C2 (.5)
(B * C 1)  = {(C'; (B * C 1)) : C'  \Omega   C1  '  C1 } (.6)

C  = {  % }  (.7)

Example  Labels of program (4) (33)

The labels of program (4) have been defined at (15). We have  C10  = {C10}
and  C10  =  {C100, C1001, C101, C1011} whereas  CAE  = {L 1},  CAE  = {L 2, L3,
L4, L5, L6, L7} and  CAE  = {L 8}. \Delta 

 The local invariants are assertions on states attached to program points. More
formally they can be defined as a function 'inv', which maps labels of C to assertions:

DEFINITION  Local invariants (34)

inv : C  ff 

Example  Local invariants for program (4) (35)

Local invariants for program (4) have been given at (25): inv(L k) =  Pk,

k = 1, ...,  8. \Delta 

The local invariants inv(L), L \Omega   C  can be understood as describing the global
invariant o/(inv) \Omega  P(\Gamma ), which is the set of configurations such that when control is at L
the memory state belongs to inv(L). Reciprocally, a global invariant i  \Omega  P(\Gamma ) can be
decomposed into local invariants  !(i)(L), L  \Omega   C  , defined by the fact that when
control is at L the only possible memory states s are those for which the configuration
<s, L> belongs to i (or s belongs to i if L = %):

DEFINITION  COUSOT & COUSOT [1982]   Connection between local and global

invariants

Concretization function  (36)

o/ : ( C  ff  ) ff P(\Gamma ) (.1)
o/(inv) = {<s, L> : s \Omega  inv(L) \Pi  L \Omega   C  - { % }} ' inv( % ) (.2)
Abstraction function: (37)

! : P(\Gamma ) ff ( C  ff  ) (.1)

!(i)( L ) = {s : <s, L> \Omega  i} if L \Omega   C  '  C (.2)
!(i)( L ) = i ^ S if L  \Omega   C (.3)

Since ! is a bijection, the inverse of which is o/, the discovery of a global invariant
i \Omega  P(\Gamma ) satisfying verification condition gvc C [p, q](i) is equivalent to the discovery of
local invariants inv(L), L  \Omega    C  satisfying verification condition
gvc C [p, q]( o/(inv) ). This leads to the construction of the local verification
conditions by calculus ( COUSOT & COUSOT [1982] ). This equivalence is of theoretical
interest only since, from a practical point of view, each local invariant is simpler than
the global one and the task of checking gvc C [p, q](o/(inv)) can be decomposed into the
verification of more numerous but simpler conditions, one for each local invariant.

5 . 2 . 3 Construction of the verification conditions for local

invariants

To formally derive the local verification conditions from induction principle (29),
we first express the operational semantics (13) in an equivalent form using program
steps. From a syntactic point of view, the next elementary step  C [L] which will
be executed when control is at point L  \Omega   C  '  C  of command C  \Omega    is an
atomic command or a test defined by cases as follows (where n >= 0):

DEFINITION  Elementary steps within a command (38)

C [(...((C'; C 1); C 2) ...; C n)] = C'  if C' is skip, X := E or X := ? (.1)
C [(...(((B  ff C'  fi C"); C 1); C2)...; C n)] = B (.2)

C [(...(((B * C'); C 1); C 2)...; C n)] = B   (.3)

Example  Elementary steps of program (4) (39)

For program C defined by (4) with labels (15), we have  C   =
[L1 \Xi  Z := 1, L 2 \Xi  Y <> 0, L 3 \Xi  odd(Y), L 4 \Xi  Y := Y - 1 , L 5 \Xi  Z := Z * X ,
L6 \Xi  Y := T div 2 , L7 \Xi  X := X * X]. \Delta 

Again from a syntactic point of view, the next label  C [L] which will be
reached after execution of an elementary step when control is at point L  \Omega 

C  '  C  of command C  \Omega    can be defined by cases as follows (where n  >=
0 and (...(C 1; C2)...; Cn) is the final label  % for n = 0):

DEFINITION  Successors of a program control point (40)

C [(...((C'; C 1); C 2)...; C n)] =  (.1)

(...(C1; C 2)...; C n)   if C' is skip, X := E or X := ?
C [(...(((B  ff C'  fi C"); C 1); C2)...; C n) ] =  (.2)

[tt \Xi  (...((C';  C1); C 2)...; C n), ff  \Xi  (...((C";  C1); C 2)...; C n)]
C [(...(((B * C); C 1); C2)...; C n)] =  (.3)

[tt \Xi  (...(((C; (B * C)); C 1); C 2)...; C n), ff  \Xi  (...(C1; C 2)...; C n)]

Example  Successors of control points of program (4) (41)

For program C defined by (4) with labels (15), we have  C  = [L 1 \Xi  L2,
L 2  \Xi   [ t t  \Xi    L 3 , ff  \Xi    L 8 ], L 3  \Xi   [ t t  \Xi    L 4 , ff  \Xi    L 6 ], L 4  \Xi    L 5 , L 5  \Xi    L 2 ,
L 6 \Xi   L7, L 7 \Xi   L2]. \Delta 

Now from a semantical point of view, execution of an elementary step  C [L]
in memory state s can lead to any successor state s'  \Omega   C <s, L> as follows:

DEFINITION  Successor states (42)

C <s, L > = {s} if  C [L] is skip (.1)

C <s, L > = {s[X  \Xi  E(s)]} if  C [L] is X := E (.2)
C <s, L > = {s[X  \Xi  d] : d  \Omega  D} if  C [L] is X := ? (.3)

C <s, L > = {s} if  C [L] is B  (.4)

Again from a semantical point of view, the next label  C <s, L> which can
be reached after execution of an elementary step in configuration <s, L> of command C
\Omega    can be defined by cases as follows:

DEFINITION  Successor control point (43)

C <s, L > = { C [L]} if  C [L] is skip, X := E or X := ? (.1)
C <s, L > = { C [L](s  \Omega  B)} if  C [L] is B (.2)

The operational semantics (13)  can now be given an equivalent stepwise
presentation:

LEMMA  Stepwise presentation of the operational semantics (44)

op C  = {<<s, L>, final  <s', L'>>  : s \Omega  S   \Pi  L  \Omega   C  '  C  \Pi 

s'  \Omega   C <s, L > \Pi  L'  \Omega   C <s, L >  }
where final <s', %> = s' and otherwise final  o/ = o/.

We have seen that a partial correctness proof of  { p }C{ q } by Floyd-Naur's
method consists in discovering local invariants inv \Omega   C  ff   satisfying gvc C [p,
q]( o/(inv) ). This global verification condition is equivalent to a conjunction of simpler
local verification conditions as follows:

THEOREM  NAUR [1966], FLOYD [1967], MANNA [1969] [1971]   Floyd-Naur partial (45)

correctness proof method with stepwise verification conditions
A partial correctness proof of  { p }C{ q } by Floyd-Naur's method consists in
discovering local invariants inv  \Omega   C  ff  , which must be proved to satisfy the
following local verification conditions :

. p ffl inv(L) if L \Omega   C (.1)
. inv(L) ffl inv( C [L]) if L  \Omega   C '  C  \Pi   C [L] is skip (.2)
. inv(L) ffl {s \Omega  S : s[X \Xi  E(s)] \Omega  inv( C [L])} (.3)

if L  \Omega   C '  C  \Pi   C [L] is X := E
. {s[X \Xi  d] : s \Omega  inv(L) \Pi  d \Omega  D} ffl inv( C [L]) (.4)

if L  \Omega   C '  C  \Pi   C [L] is X := ?
. (inv(L) ^ B) ffl inv( C [L](tt)) if L  \Omega   C '  C  \Pi   C [L] is B (.5)
. (inv(L) ^ ~B) ffl inv( C [L](ff)) if L  \Omega   C '  C  \Pi   C [L] is B (.6)
. inv(L) ffl q if L \Omega   C (.7)

The verification condition (45.3) for assignment is backward. This name arises out of
the fact that the postcondition inv( C [L] is back-transformed into the assertion
{s \Omega   S  : s[X  \Xi   E (s)] \Omega  inv( C [L])} written in terms of the states before
assignment. Verification condition (45.4) for random assignment is forward.
Verification condition (45.3) can also be given an equivalent forward form
(KING [1969]) :

. {s[X \Xi  E(s)] : s \Omega  inv(L)} ffl inv( C [L])

if L \Omega   C ' C  \Pi   C [L] is X:=E (.8)

Proof

By (29), we have to show that  o/(inv(L)) satisfies gvc C [p, q](  o/(inv(L)) ). We
proceed by simplification of gvc C [p, q]( o/(inv(L)) ) which constructively leads to local
verification conditions (45):

* First, ( \Delta  s \Omega  p. <s, C > \Omega  o/(inv(L)))  \Phi  (\Delta  s \Omega  p. C  \Omega   C  '  C  \Pi  s \Omega  inv(C))
[by (36.2)]  \Phi  (p ffl inv(C)) [by (32.2)]  \Phi  (\Delta  L \Omega   C . p ffl inv(L)) [by (32.2)].

* Then, according to (36) and (44), the condition  o/(inv) * op C  ffl \Gamma  x o/(inv) can be
decomposed into a conjunction of simpler verification conditions, one for each program
step:
o/(inv) * op C  ffl \Gamma  x o/(inv)

\Phi  {<s, L > : s  \Omega  inv(L)  \Pi  L \Omega   C - { %}}  * op C  ffl \Gamma  x [{<s, L > : s  \Omega  inv(L)  \Pi 

L \Omega   C  - {%}}  '  inv(%)]
\Phi  { < < s, L > ,   f i n a l   < s', L' > >  : s  \Omega   i n v ( L )   \Pi  L  \Omega    C '  C   \Pi 

s' \Omega    C < s, L >  \Pi   L'  \Omega    C < s, L > }  ffl  \Gamma   x  [{< s, L >  : s  \Omega   inv(L)  \Pi 
L \Omega   C - {%}} ' inv(%)]

\Phi  \Delta   L  \Omega   C '  C . \Delta   s  \Omega   inv(L). {final  < s', L' >   : s' \Omega    C < s, L >  \Pi 

L' \Omega   C <s, L >}  ffl [{ <s", L" > : s"  \Omega   inv(L")  \Pi  L"  \Omega   C - { %}}  '
inv(%)]
We go on by cases, according to (38), (40) and (42):

- If  C [L] is X := E (skip and X: = ? are handled the same way), then we have
to check that :

{final <s', L' > : s' \Omega  {s[X \Xi   E (s)]}  \Pi  L'  \Omega  { C [L]} }  ffl [{ <s", L" > : s"  \Omega 
inv(L")  \Pi  L" \Omega   C - { %}}  ' inv(%)]
\Phi  final <s[X \Xi   E(s)],  C [L]> \Omega  [{ <s", L" > : s"  \Omega  inv(L")  \Pi  L"  \Omega   C {%}} ' inv(%)]
\Phi  s[X \Xi  E(s)]  \Omega  inv( C [L])
(and \Delta  s \Omega  inv(L). s[X  \Xi  E(s)] \Omega  inv( C [L]) is obviously equivalent to inv(L)  ffl
{s  \Omega   S  : s[X  \Xi   E (s)]  \Omega   inv( C [L])} and to {s[X  \Xi   E (s)] : s  \Omega   inv(L)} ffl
inv( C [L]))),

- If  C [L] is B then we have to check that :
{final <s', L' > : s' \Omega  {s}  \Pi  L'  \Omega  { C [L](s  \Omega  B)} } ffl [{<s", L" > : s"  \Omega  inv(L")
\Pi  L"  \Omega   C - { %}}  ' inv(%)]
\Phi  final <s,  C [L](s \Omega  B)> \Omega  [{<s", L" > : s"  \Omega  inv(L")  \Pi  L" \Omega   C - {%}} '

inv(%)]
\Phi  s \Omega  inv( C [L](s \Omega  B)).

* Finally, ( o/(inv) ^ S ffl q) \Phi  (inv(%) ffl q) [by (36.2)]  \Phi  (\Delta  L \Omega   C . inv(L)  ffl q)
[by (32.7)].  \Delta 

5 . 2 . 4 Semantical soundness and completeness of the

stepwise Floyd-Naur  partial correctnes s proof
method

A proof method is sound if it cannot lead to mistaken conclusions. It is complete
if it is always applicable to prove indubitable facts.

THEOREM DE BAKKER & MEERTENS [1975]  Soundness and semantical completeness (46)

of the stepwise Floyd-Naur method
The stepwise presentation of Floyd-Naur partial correctness proof method is
semantically sound and complete.

Proof

The method is sound since if inv satisfies (45) then, by construction of (45),
gvc C [p, q]( o/(inv) ) holds so that  { p }C{ q } derives from (29). It is semantically
complete since if  { p }C{ q } is true then by (29) we know that I  = {<s, C> : s \Omega  p} *
op C * satisfies gvc C [p,q](I) so that by construction, (45) holds for inv  = !(I). \Delta 

We insists upon  semantical soundness and completeness as in  DE BAKKER &
MEERTENS [1975] or MANNA & PNUELI [1970]  since (46) is relative to a given semantics of
programs (13) and to a representation of invariants by sets as opposed to the existence
of a formal calculus in a given language to prove partial correctness of programs
(GERGELY & SZO"TS [1978], SAIN [1985]).

5 . 3 T h e   c o m p o s i t i o n a l   F l o y d - N a u r   p a r t i a l

correctness proof method

HOARE [1969]  introduced the idea (often called  compositionality) that the
specification of a command should be verifiable in terms of the specifications of its
components. This means that partial correctness should be proved by induction on the
syntax of programs using their relational semantics (19) instead of an induction on the
number of transitions using their operational semantics (13). Following  OWICKI [1975],
we give a syntax-directed presentation of Floyd-Naur's method without appeal to a
formal logic. To do this we associate preconditions and postconditions with commands
and introduce structural verification conditions so that a proof of a composite command
is composed of the proofs of its constituent parts. Although this later turns out to be
redundant, we prove the semantical soundness and completeness of the method since
the underlying reasoning constitutes a simple introduction to relative completeness
proofs of Hoare logic.

5 . 3 . 1 Preconditions and postconditions of commands

A partial correctness proof of  { p }C{ q } by Floyd-Naur's method consists in
discovering a precondition pre(C') and a postcondition post(C') specifying the partial
correctness {pre(C') }C'{post(C')} of each component C' of command C. This
includes an invariant linv(C') for each loop C' within C. Formally "pre", "post" and
"linv" can be understood as functions which maps components of C to assertions :

DEFINITION Preconditions, postconditions and loop invariants attached to (47)

commands
pre, post :   C  ff   (.1)
linv :   C  ff   (.2)

Example  Preconditions, postconditions and loop invariants for program  (4) (48)

For program C defined by (4) with components defined by (5), we can choose :

pre(CAE) = pre(C0) = P1
post(C0) = pre(C1) = linv(C1) = post(C1001) = post(C100) = post(C1011) = post(C101) =

post(C10) = P2

pre(C10) = P3
pre(C100) = pre(C1000) = P4
post(C1000) = pre(C1001) = P5
pre(C101) = pre(C1010) = P6
post(C1010) = pre(C1011) = P7
post(C1) = post(CAE) = P8

\Delta 

5 . 3 . 2 Compositional verification conditions

Then these assertions should be proved to satisfy the following verification
conditions which are defined compositionally, that is by recursion on the syntax of
commands :

DEFINITION  OWICKI [1975]  Compositional Floyd-Naur partial correctness (49)

proof method
A partial correctness proof of  { p }C{ q } by Floyd-Naur's method consists in
discovering preconditions, postconditions and loop invariants (47) which must be
proved to satisfy the following compositional verification conditions :

. p ffl pre(C) \Pi  post(C) ffl q (.1)
For each component C'  \Omega   C  of C :
. pre(C') ffl post(C') if C' is skip (.2)
. pre(C') ffl {s \Omega  S : s[X  \Xi  E(s)] \Omega  post(C')} if C' is X := E (.3)
. {s[X \Xi  d] : s  \Omega  pre(C')  \Pi  d \Omega  D} ffl post(C') if C' is X := ? (.4)
. pre(C')  ffl pre(C1) \Pi  post(C1) ffl pre(C2) \Pi 

post(C2) ffl post(C') if C' is (C1; C2) (.5)
. (pre(C') ^ B) ffl pre(C1) \Pi  (pre(C')  ^ ~B) ffl pre(C2) \Pi 

post(C1) ffl post(C')  \Pi  post(C 2) ffl post(C') if C' is (B ff C1 fi C2) (.6)
. pre(C')  ffl linv(C') \Pi  (linv(C') ^ B) ffl pre(C1) \Pi 

post(C1) ffl linv(C') \Pi  (linv(C')  ^ ~B) ffl post(C') if C' is (B * C1) (.7)

Observe that these compositional verification conditions could also have been defined
by an attribute grammar (KNUTH [1968b]) using context-free grammar (1) with attributes
"pre", "post" and "linv" so that (49) expresses the relationships between these attributes
(see GERHART [1975]  and REPS & ALPERN [1984]).

Example  Compositional verification conditions for program (4) (50)

These verification conditions are given below for program C defined by (4).
Some of them, corresponding to assertions attached to the same label, are obviously
satisfied :

pre(CAE) ffl pre(C0) \Pi  post(C 0) ffl pre(C1) \Pi  post(C 1) ffl post(CAE) \Pi  pre(C100) ffl pre(C1000)
\Pi  post(C 1000) ffl pre(C1001) \Pi  post(C 1001) ffl post(C100) ffl post(C10) \Pi   pre(C101) ffl
pre(C1010) \Pi  post(C1010) ffl pre(C1011) \Pi  post(C1011) ffl post(C101) ffl post(C10) ffl linv(C1)

Moreover, (49.7) distinguishes the precondition of a loop (e.g.  pre(C1) = (X = x \Pi  Y =y >=
0 \Pi  Z = 1)) from its invariant (e.g.  linv(C1) = (Y >= 0 \Pi  Z * (X ** Y) = x ** y) ) :

pre(C1) ffl linv(C1)
The remaining verification conditions correspond to elementary steps of the program.
They are set theoretic interpretations of formulae (26) :

(AE) p ffl pre(CAE)
(i1) pre(C0) ffl {s \Omega  S : s[Z \Xi  1] \Omega  post(C0)}

(i2) (linv(C1) ^ {s \Omega  S : s(Y) ` 0}) ffl pre(C10)
(i3) (linv(C1) ^ ~{s \Omega  S : s(Y) ` 0}) ffl post(C1)
(i4) (pre(C10) ^ {s \Omega  S : odd(s(Y))}) ffl pre(C100)
(i5) (pre(C10) ^ ~{s \Omega  S : odd(s(Y))}) ffl pre(C101)
(i6) pre(C1000) ffl {s \Omega  S : s[Y \Xi  s(Y) - 1]  \Omega  post(C1000)}
(i7) pre(C1001) ffl {s \Omega  S : s[Z \Xi  s(Z) * s(X)]  \Omega  post(C1001)}
(i8) pre(C1010) ffl {s \Omega  S : s[Y \Xi  s(Y) div 2] \Omega  post(C1010)}
(i9) pre(C1011) ffl {s \Omega  S : s[X \Xi  s(X) * s(X)]  \Omega  post(C1011)}
(') post(CAE) ffl q
\Delta 

5 . 3 . 3 Semantical soundness and completeness of the

compositional Floyd-Naur partial correctness proof
method

The compositional presentation Floyd-Naur's proof method is semantically sound
and complete :

THEOREM  Soundness of the compositional Floyd-Naur proof method (51)

If verification conditions (49) are satisfied for all components C' of C then :

\Delta  C' \Omega   C . { pre(C')  }C'{  post(C')  } 
It follows from (49.1) that { p }C{ q } holds.

Proof

We prove that  \Delta  C'  \Omega   C . { pre(C') }C'{ post(C') } or equivalently by
(22) that  \Delta  C' \Omega   C . (pre(C') * C') ffl (S x post(C')) by structural induction on
C '   :

* I f   C '   i s   X  : =  E   t h e n   b y   ( 4 9 . 3 )   w e   h a v e
(pre(C') ffl {s \Omega  S : s[X \Xi   E (s)] \Omega  post(C')}) \Phi   ({s[X \Xi   E (s)] : s \Omega  pre(C')}    ffl
post(C')) \Phi  ((pre(C') * C') ffl (S x post(C'))) by (19.2). The cases skip and X := ?
are handled the same way.

* If C' is (C 1; C2) then (pre(C')  * C1; C2) = (pre(C')  * C1 * C2) [by (19.4)] =

(pre(C') * C1) * C2 ffl (S x post(C1)) * C2 [since (pre(C 1) * C1) ffl (S x post(C1)) by
induction hypothesis] =  S2 * (post(C1) * C2) ffl S2 * (pre(C2) * C2) [by (49.5)]  ffl
S2 * (S x post(C2)) [since (pre(C 2) * C2) ffl (S x post(C2)) by induction hypothesis] =
S x post(C2) ffl S x post(C') [by (49.5)]. The case C' = (B  ff C1 fi C2) is handled the
same way.

* If C' is (B * C 1) then (pre(C 1) * C 1) ffl (S x  post(C 1)) holds by induction
hypothesis which implies (linv(C')  * (B  * C 1)) ffl (S x  linv(C')) by (49.7). Also
p r e ( C ' )   ffl  linv(C') and (linv(C')  ^   ~B )  ffl  post(C') by (49.6) so that
(pre(C') * (B * C1)* _ ~B) ffl (S x post(C')) by (28) hence (pre(C')  * (B * C 1)) ffl (S

x post(C')) by (19.6).  \Delta 

THEOREM Semantical completeness of the compositional Floyd-Naur proof method (52)

If  { p }C{ q } holds then there are functions pre, post and linv verifying
conditions (49) for all components C' of C.

Proof

The proof is by structural induction on C :

* If C is X := E then  { p }X := E{ q  } \Upsilon   (p  * X := E)  ffl (S x  q) [by (22)]  \Upsilon 
{s[X \Xi  E(s)] :  s \Omega  p}  ffl q [by (19.2)]  \Upsilon  (49.1)  \Pi  (49.3) if we let pre(X := E) = p
and post(X := E) = q. The cases skip and X := ? are handled the same way.

* If C is (C 1; C2) then we let pre(C) = pre(C 1) = p,  post(C) = post(C2) = q
and post(C 1) = pre(C2) = {s : \Lambda  s' \Omega   p. <s', s> \Omega   C 1}. Then (49.1) and (49.5)
are satisfied. It remains to show that (49.2), ..., (49.7) hold for all components C' of
C. By induction hypothesis, we just have to show that  { pre(C1) }C1{ post(C1) }
and  { pre(C2) }C2{ post(C1) }.

We have  {   p  } C 1 {  {s  :  \Lambda   s' \Omega    p . < s', s>  \Omega   C 1 }   }  because (p  * C 1 ) =
{ < s ,  s '>  :   s \Omega   p  \Pi   < s ,  s '>  \Omega   C 1 }  ffl   {< s " ,   s ' >  : \Lambda    s   \Omega    p . < s ,  s '>  \Omega   C 1 } =  S   x
{s : \Lambda  s' \Omega   p.  < s', s>  \Omega   C 1}. Moreover  {   p } ( C 1; C 2){ q }  \Upsilon   (p  * ( C 1; C 2))  ffl
(S x q) [by (22)]  \Upsilon  ((p  * C1) * C2) ffl (S x q) [by (19.4)]  \Upsilon   (\Delta s', s, s"  \Omega  S. (s'  \Omega  p \Pi 

< s', s > \Omega   C 1 \Pi  < s, s">  \Omega   C 2) \Upsilon   (s"  \Omega   q))  \Upsilon   (({s :  \Lambda   s'  \Omega   p.  < s', s >  \Omega   C 1}  * C 2)  ffl
(S x q))  \Upsilon  { pre(C2) }C2{ post(C 1) } by (22).

* The proof is similar when C is (B ff C1 fi C2) choosing pre(c') = p, pre(C1) = p ^
B, pre(C 2) = p ^ ~B and post(C') = post(C 1) = post(C 2) = q.

* If C is (B * C 1) then  { p }(B * C 1){ q } \Upsilon  (p * (B * C 1)) ffl (S x q) [by (22)]  \Upsilon 
(p * (B * C1)* _ ~B) ffl (S x q) [by (19.6)]  \Upsilon  [\Lambda  i \Omega   . (p  ffl i) \Pi  (i * (B * C1) ffl S x i) \Pi 
(i ^ ~B  ffl q)] [by (28)]. We now define pre((B * C 1)) = p, linv((B * C 1)) = i,
post((B * C1)) = q, pre(C 1) = i ^ B and post(C 1) = i. It follows that (49.1) and (49.7)
hold for C. It remains to show that  { pre(C1) }C1{ post(C1) }. This immediately
follows from the definitions of pre(C 1) and post(C 1), (i ^ B) * C1 ffl S x i) and (22).  \Delta 

5 . 4 Equivalence of stepwise and compositional

Floyd-Naur partial correctness proofs

Examples (26) and (50) show that the compositional Floyd-Naur partial
correctness proof method introduces some trivially satisfied verification conditions
which do not appear in the stepwise version. Apart from this difference in the
presentation, the stepwise and compositional Floyd-Naur partial correctness proofs of
program (4) are equivalent. This property is general in the sense that a proof using one
presentation can always be derived from a proof using the other presentation. Since the
assertions are the same in both presentations, (30.3) and (36) imply that preconditions
and postconditions in the compositional presentation (hence later in Hoare logic) are
local invariants, a fact which is often taken for granted. By (46), this also implies that
the syntax-directed presentation is semantically sound and complete, a fact already
proved by (51) and (52).

5 . 4 . 1 The compositional presentation of a stepwise FloydNaur partial correctness proof

The precondition pre(C') of a component C' of a command C  \Omega    (and loop
invariant linv(C') when C' is a loop) can always be chosen as the local invariant inv(L)
attached to the label L =  C [C'] designating where control is just before executing
that component C'. The same way, the postcondition post(C') of a component C' of a
command C can always be chosen as the local invariant inv(L) attached to the label L =

C [C'] designating where control is just after executing that component C'.

DEFINITION  Program points before and after components of a command (53)

The label just before and after a component C' of a command C \Omega    :

C  \Omega   C  ff  C (.1)

C  \Omega   C  ff   C (.2)
is defined by structural induction on C :

.  C [C] = C (.3)
.  C [C] =  % (.4)
For each C'  \Omega   C  - {C} when C is (C 1;C2), (B ff C1 fi C2) or (B * C 1) :

* (C1; C2) [C'] = ( C1 [C']; C 2) if C'  \Omega   C1 (.5)
.  (C1; C 2) [C'] =  C2 [C'] if C'  \Omega   C2 (.6)
.  (C1; C2) [C'] = C 2 (.7)

if C'  \Omega   C1  \Pi   C1 [C'] =  %
.  (C1; C 2) [C'] = ( C1 [C']; C 2) (.8)

if C'  \Omega   C1  \Pi   C1 [C']  `  %
.  (C1; C 2) [C'] =  C2 [C'] if C'  \Omega   C2 (.9)

*  (B ff C1 fi  C2) [C'] =  C1 [C'] if C'  \Omega   C1 (.10)
.  (B ff C1 fi  C2) [C'] =  C2 [C'] if C'  \Omega   C2 (.11)
.  (B  ff C1 fi  C2) [C'] =  C1 [C'] if C'  \Omega   C1 (.12)
.  (B  ff C1 fi  C2) [C'] =  C2 [C'] if C'  \Omega   C2 (.13)
.  (B * C 1) [C'] = ( C1 [C']; (B * C 1)) if C'  \Omega   C1 (.14)

*  (B * C 1) [C'] = (B * C 1) (.15)

if C'  \Omega   C1  \Pi   C1 [C'] =  %
.  (B * C 1) [C'] = ( C1 [C']; (B * C 1)) (.16)

if C'  \Omega   C1  \Pi   C1 [C']  `  %

THEOREM  Compositional presentation of a stepwise proof (54)

If inv \Omega   C  ff   satisfies (45) then :

pre =  oe C'  \Omega   C . inv(  C [C'] ) (.1)
linv =  oe C'  \Omega   C . inv(  C [C'] ) (.2)
post =  oe C'  \Omega   C . inv(  C [C'] ) (.3)
satisfies (49).

5 . 4 . 2 The stepwise presentation of a compositional FloydNaur partial correctness proof

THEOREM  Stepwise presentation of a compositional proof (55)

If pre, post  \Omega    C  ff   and linv  \Omega   C  ff   satisfy (49) then
inv \Omega   C  ff   defined as follows by structural induction on C :

. inv( C ) = linv(C) if C \Omega   (.1)
. inv( C ) = pre(C) if C \Omega    -  (.2)
. inv( % ) = post(C) (.3)
. inv( (C'1; C2) ) = inv(C' 1) if C = (C 1; C2) \Pi  C'1 \Omega   C1 (.4)
. inv( (C'; (B * C 1)) ) = inv(C') if C = (B * C 1) \Pi  C' \Omega   C1  '  C1 (.5)
satisfies (45).

5 . 5 Variants of Floyd-Naur partial correctness

proof method

Lemma (29) hence Floyd-Naur's method has a great number of equivalent
variants, each one leading to a different partial correctness proof methodology
(COUSOT & COUSOT [1982] ). For example  MANNA [1971] uses an invariant i and an output
specification q which relate the possible configurations during execution to the initial
states of variables. Otherwise stated, one uses relations between the current and initial
values of variables instead of assertions upon their current values. More formally, we
have :

DEFINITION  MANNA [1971]  Relational partial correctness (56)

p :  = P(S) Input specifications (.1)
q :  = P(S x S) Output specifications (.2)
{ p }C/ q 0 :  x   x   ff {tt, ff} Relational partial (.3)
{ p }C/ q 0 = (p * C) ffl q correctness (.4)

Induction principle (29) can be rephrased as follows for relational partial correctness:
THEOREM MANNA [1971], COUSOT & COUSOT [1982]  Stepwise partial correctness (57)

relational proofs using invariants
{ p }C/ q 0 = [ \Lambda  i \Omega  P(S x \Gamma ).{<s, <s, C >> : s \Omega  p} ffl i  (.1)

\Pi  {<s, o/'> : \Lambda  o/.<s, o/> \Omega  i \Pi  <o/, o/'> \Omega  op C } ffl i (.2)
\Pi  i ^ S2 ffl q ] (.3)

It is also possible to prove relational partial correctness using an invariant i which relates
the possible configurations during execution to the final states of the variables :

THEOREM MORRIS & WEGBREIT [1977], COUSOT & COUSOT [1982]  Subgoal induction (58)

{ p }C/ q 0 = [ \Lambda  i \Omega  P(\Gamma  x S). {<s, s> : s \Omega  S} ffl i (.1)

\Pi  {<o/, s> : \Lambda  o/'. <o/, o/'> \Omega  op C \Pi  <o/', s> \Omega  i}  ffl i (.2)
\Pi  {<s, s'> : s \Omega  p \Pi  <<s, C>, s'> \Omega  i} ffl q ] (.3)

Assume that  <s, s' > \Omega  C  so that execution  o/0, ...,  o/n of command C started in
configuration  o/0 = <s, C> with s  \Omega  p, such that < o/k-1, o/k> \Omega  op C  for k = 1, ..., n does
terminate in state o/n = s'. Then <o/n, s'> \Omega  i by (58.1) and by downward induction on k =
n, n-1, ..., 0,  <o/k, s'> \Omega  i follows from (58.2). In particular  <o/0, s'> \Omega  i whence  <s, s'>
\Omega  q by (58.3). It follows that (p  * C) ffl q whence  { p }C/ q 0 holds. Semantical
completeness follows from that fact that i can always be chosen as op C *_S.

Semantical soundness and completeness imply that these partial correctness proof
methods are all equivalent. This is also true in the stronger sense that the necessary
invariants can be derived from one another:

THEOREM COUSOT & COUSOT [1982], DIJKSTRA [1982a]  Equivalence of stepwise (59)

induction and subgoal induction
 - if i satisfies (57) then i' = { <o/, s> : \Delta s'. <s', o/> \Omega  i \Upsilon  <s', s> \Omega  q} satisfies (58) (.1)
 - if i' satisfies (58) then i = { <s, o/> : \Delta s". <o/, s"> \Omega  i' \Upsilon  <s, s"> \Omega  q} satisfies (57) (.2)

Proof

* If i satisfies (57) then i  ^ S2 ffl q so that  \Delta  s \Omega  S. \Delta  s' \Omega  S. <s', s> \Omega  i \Upsilon  <s', s> \Omega  q
hence { <s, s>  : s  \Omega   S }  ffl  i' [by 33.1]. Moreover ( <o/, o/'> \Omega   op C \Pi  <o/', s > \Omega   i'  \Pi 
<s',o/> \Omega  i)  \Upsilon   (<s', o/'> \Omega  i \Pi  <o/', s > \Omega  i') [by (57)]  \Upsilon   (<s',  o/'> \Omega  i \Pi  (\Delta   s'.  <s',  o/'> \Omega  i \Upsilon 
< s', s >  \Omega   q)) [by (59.1)]  \Upsilon   < s', s >  \Omega   q so that { < o/, s >  :  \Lambda   o/  '.  < o/,  o/'>  \Omega   o p C \Pi 
< o/ ', s>  \Omega   i'}  ffl  i'. Finally, (s  \Omega   p  \Pi   < < s, C > , s' >  \Omega   i')  \Upsilon    (< s,  < s, C > >  \Omega   i  \Pi 
(\Delta s". <s", <s, C >> \Omega   i \Upsilon   <s", s' >  \Omega   q)) [by (57) and (59.1)]  \Upsilon   <s, s' >  \Omega   q hence
{<s, s'> : s  \Omega   p  \Pi  <<s, C>, s'> \Omega  i'}  ffl q.

* If i' satisfies (58) then { <s, s > : s  \Omega   S}  ffl i' so that i  ^  S2 = { <s, s' > :  \Delta   s".
<s', s">  \Omega   i'  \Upsilon   <s, s">  \Omega   q}  ffl  {<s, s' > :  <s', s' >  \Omega   i'  \Upsilon   <s, s' >  \Omega   q} = q. Moreover
(< s,  o/>  \Omega   i  \Pi   < o/,  o/'>  \Omega   o p C  \Pi  < o/', s" >  \Omega   i')  \Upsilon   (( \Delta   s'.  < o/, s' >  \Omega   i'  \Upsilon   < s, s' >  \Omega   q)  \Pi 
<o/, s">  \Omega   i') [by (59.2) and (58)]  \Upsilon   <s, s" >  \Omega   q so that { <s,  o/'> :  \Lambda   o/. <s,  o/>  \Omega   i \Pi 
<o/, o/'> \Omega  op C } ffl i. Finally,  \Delta  s \Omega  p.  \Delta  s".  <<s, C >, s" > \Omega  i' \Upsilon  <s, s"> \Omega  q [by (58)]
hence { <s, <s, C>> : s \Omega  p} ffl i [by (59.2)].  \Delta 

Replacing i by ~j in (58), we obtain an equivalent relational partial correctness proof
method proceeding by reductio ad absurdum:

THEOREM  COUSOT & COUSOT [1982]  Induction principle for contrapositive proofs (60)

{ p }C/ q 0 = [ \Lambda  j \Omega  P(\Gamma  x S). p  * ~q ffl {<s, s'> : <<s, C>, s'> \Omega  j} (.1)

\Pi  j ffl {<o/, s> : \Delta  o/'. <o/, o/'> \Omega  op C \Upsilon  <o/', s> \Omega  j} (.2)
\Pi  {<s, s> : s \Omega  S} ffl ~j ] (.3)

This consists in proving an invariance property by considering the situation where the
contrary property should be true and in establishing that this situation is impossible.
Assume that  <s, s'> \Omega  C  so that execution  o/0, ...,  o/n of command C started in
configuration  o/0 = <s, C> with s \Omega  p, such that < o/k-1, o/k> \Omega  op C  for k = 1, ..., n does
terminate in state  o/n = s'. Assume, by reductio ad absurdum, that  <s, s'> fl q. Then <o/0,
s'> \Omega  j by (60.1) and by induction on k = 1, ..., n,  <o/k, s'> \Omega  j follows from (60.2). In
particular  <o/n, s' > \Omega  j in contradiction with  <o/n, s'> fl j following from (60.3).
Semantical completeness follows from that fact that the contra-invariant j can always be
chosen as ~( op C *_S).

This may lead to simpler proofs when the "absurd" configuration is much simpler
than the "sensible" one (see VERJUS [1987] for an example).

6 . Liveness proof methods

Obviously termination is not implied in partial correctness since for example if
true =  I true  =  S then  true  *  skip = o/ so that  {p}true * skip{q } is true for all
p, q \Omega   . FLOYD [1967a] originally introduced total correctness as the conjunction of
partial correctness and termination. Hoare logic has also been extended to cope with
termination and more generally with  liveness properties of programs ( ALPERN &
SCHNEIDER [1985]).

We first introduce execution traces generated by the operational semantics (13) so
as to define total correctness and prove that it is the conjunction of partial correctness,
deadlock freeness and termination. After giving a short mathematical recall on well
founded relations, well orderings and ordinals, we introduce  FLOYD [1967a]'s wellfounded set method to prove termination of programs. We next consider an extension of
this method to prove liveness properties P  ~~C~~ff Q stipulating that starting from a

configuration of P, program C does eventually reaches a configuration of Q
(LAMPORT [1977]). Finally we study  BURSTALL [1974] intermittent assertion method for
proving total correctness ( MANNA & WALDINGER [1978], GRIES [1979] ) and generalize it to
arbitrary liveness properties. After proper generalization, Burstall's method includes
Floyd's method ( COUSOT & COUSOT [1987] ) and is more flexible since it allows the
combination of inductions on various underlying structures of the program (syntax,
computation, data, etc.).

6 . 1 Execution traces

We use execution traces to record the successive configurations that can be
encountered during a terminating or non-terminating execution of a program. Since
programs are nondeterministic, they can have many different possible executions so that
we have to use sets of finite or infinite traces. The theory of traces is surveyed by
MAZURKIEWICZ [1989].

Let be given a set   of programs, a set  S  of states so that the set of
configurations is  \Gamma    =   ( S  x  )  '   S  and an operational semantics
o p  \Omega     ff  P (\Gamma   x \Gamma  ) .

DEFINITION  Execution traces (61)

The set of finite complete execution traces of length n \Omega   + for command C \Omega  
starting in configuration o/ \Omega  \Gamma  is :

1n C o/  = {' \Omega  seqn \Gamma  : '0 = o/ \Pi  \Delta  i \Omega  {1, ..., n - 1}.  <'i-1 , 'i> \Omega  op C

\Pi  \Delta  o/ \Omega  \Gamma . <'n-1, o/> fl op C } (.1)
the set of infinite traces of execution of command C \Omega    starting in configuration o/ \Omega 
\Gamma  is :

1OE C o/ = {' \Omega  seqOE \Gamma  : '0 = o/ \Pi  \Delta  i \Omega   . <'i , 'i+1> \Omega  op C } (.2)
so that the set of  finite traces  of execution of command C  \Omega   starting in
configuration o/ \Omega  \Gamma  is :

1* C o/ = '{1n C o/ : n \Omega   } (.3)
and the set of  traces of execution of command C  \Omega   starting in configuration  o/ \Omega  \Gamma 
is :

1 C o/ = 1* C o/ ' 1OE C o/ (.4)
Given p \Omega   , we can define the traces of command C starting in a state of p  \Omega    as :

1n C (p) = '{1n C <s, C> : s \Omega  p} (.5)
1* C (p) = '{1* C <s, C> : s \Omega  p} (.6)
1OE C (p) = '{1OE C <s, C> : s \Omega  p} (.7)
1 C (p) = '{1 C <s, C> : s \Omega  p} (.8)

These sets of traces can also be given an equational definition, see DE BRUIN [1984].

6 . 2 Total correctness
  Let us recall (22) that a command C is said to be partially correct with respect to a
specification <p, q> (written {p}C{q}) if any terminating execution of C starting from
an initial state s satisfying p must end in some final state s' satisfying q. C is said to be
totally correct  with respect to  <p, q> (written  [p]C[q]) if any execution of C starting
from an initial state s satisfying p does terminate properly in a final state s' satisfying q.
Partial and total correctness can be defined in terms of execution traces as follows :

DEFINITIONS  FLOYD [1967a]  Partial and total correctness

[p]C[q] :  x   x   ff {tt, ff} Total correctness (62)
[p]C[q] =  \Delta  ' \Omega 1 C (p). \Lambda  n \Omega   +. ' \Omega  1n C (p) \Pi  'n-1 \Omega  q

{p}C{q} :  x   x   ff {tt, ff} Partial correctness (63)
{p}C{q} =  \Delta  ' \Omega  1 C (p). \Delta  i \Omega  dom '. ('i \Omega  S) \Upsilon  ('i \Omega  q) 

Total correctness as defined by (62) does not necessarily imply partial correctness (63)
because definition (62) does not imply that all states  'i \Omega  S belong to q. However this

follows from (13) because final states s  \Omega  S have no possible successor, an hypothesis
that we subsequently make upon the operational semantics:

HYPOTHESIS  Final states are blocking states (64)

\Delta  C \Omega   . \Delta  s \Omega  S. \Delta  o/ \Omega  \Gamma . <s, o/> fl op C
Observe that (64) \Upsilon  [(62) \Upsilon  (63)]. A command C is said to terminate for initial states s
\Omega  p if and only if no execution trace starting from configuration <s, C> can be infinite:

DEFINITION  Termination (65)

2[p]C :  x   ff {tt, ff}
2[p]C = \Delta  ' \Omega  1 C (p). \Lambda  n \Omega   +. ' \Omega  1n C (p)

Termination is  proper or clean for final states  'n-1 \Omega  S. Execution may also end with
other blocking states 'n-1 \Omega  \Gamma  - S. For example, a sequential program may be blocked by
a run-time error such as division by zero or a parallel program may be permanently
blocked because all processes are delayed at synchronization commands. Execution of a
command C starting with initial states s  \Omega  p can be blocked if and only if it can reach
some state some 'n-1 which is not final and has no possible successor :

DEFINITION  Blocked execution (66)

3{p}C :  x   ff {tt, ff}
  3{p}C = \Lambda  n \Omega   +. \Lambda  ' \Omega  1n C (p). 'n-1 fl S

When no execution of a command C starting with initial states s  \Omega  p can end in a
blocking configuration, we say that these executions are deadlock free :

DEFINITION  Deadlock freedom (67)

~ 3{p}C = \Delta  n \Omega   +. \Delta  ' \Omega  1n C (p). 'n-1 \Omega  S

Under hypothesis (64), total correctness is the conjunction of partial correctness,
deadlock freedom and termination :

THEOREM  Characterization of total correctness (68)

(64) implies [p]C[q] = {p}C{q} \Pi  ~ 3{p}C \Pi  2[p]C

According to (13), final states s  \Omega  S are the only possible states with no possible
successor, an hypothesis that is sometimes made made upon the operational semantics :

HYPOTHESIS  Final states are the only blocking states (69)

\Delta  C \Omega   . \Delta  o/ \Omega  \Gamma . (\Delta  o/' \Omega   \Gamma . <o/, o/'> fl op C ) \Upsilon  (o/ \Omega  S)

Under hypotheses (64) and (69), total correctness can be expressed as the conjunction
of partial correctness and termination :

THEOREM  FLOYD [1967a]  Characterization of total correctness  (70)

(64) and (69) imply [p]C[q] = {p}C{q} \Pi  [p]C[true]

Proof

(69) implies that  2[p]C = [p]C[true] = \Delta  ' \Omega  1 C (p).  \Lambda  i \Omega  dom '. 'i \Omega  S where
true = I true  = S and that 3{p}C = ff since no execution can be blocked. By (64) and
(69),  [p]C[q] = \Delta  ' \Omega  1 C (p). \Lambda  i \Omega  dom '. 'i \Omega  q = {p}C{q} \Pi  [p]C[true]. \Delta 

6 . 3 Well founded relations, well orderings and

ordinals

A relation -< on a class W is well-founded if and only if every subclass of W has a
minimal element that is  wf(W, -<) = [\Delta  E ffi W. (E ` o/ \Upsilon  \Lambda  y \Omega  E. (~ \Lambda  z \Omega  E. z -< y))] is
true. If wf(W, -<) then obviously there is no infinite decreasing sequence x0 >- x1 >- ...

where >- is the inverse of -<.

A relation  -< on a class W is a  strict partial ordering  if and only if it is antireflexive and transitive that is  spo(W, -<) = [(\Delta  x \Omega  W. ~(x -< x)) \Pi  (\Delta  x, y, z \Omega  W. (x -<
y \Pi  y -< z) \Upsilon  (x -< z))] is true. Observe that if <= is a partial ordering on W then x < y = (x
<= y \Pi  x ` y) is a strict partial ordering on W whereas if < is a strict partial ordering on W
then x <= y = (x < y \Sigma  x = y) is a partial ordering on W. A linear ordering on W is a strict
partial ordering such that any two different elements of W are comparable: lo(W, -<) =
spo(W, -<) \Pi  [\Delta  x, y \Omega  W. ((x  ` y)  \Upsilon  (x -< y \Sigma  y -< x))]. A relation  -< on a class W is
well-ordered if and only if it is a well-founded linear ordering on W:  wo(W,  -<) =
wf(W, -<) \Pi  lo(W, -<). A well-order is a pair <W, -<> such that wo(W, -<).

To study common properties of well-ordered relations independently of their
support class W, mathematicians have introduced a universal well-order called the class

 of ordinals ordered by <. We say that two well-orderings <W1, -<1> and <W2, -<2>
have the same order type if there exists a bijection 4 from W1 onto W2 such that x -<1 y
\Phi  4(x) -<2 4(y). An ordinal can be understood as the class of all well-orderings of the
same order type. Intuitively   is the transfinite sequence 0 < 1 < 2 < ... <  OE < OE+1 <
OE+2 < ... <  OE+OE = OE.2 < OE.2+1 <  OE.2+2 < ... <  OE.2+OE = OE.3 < ... <  OE.4 < ... <  OE.OE =
OE2 < OE2+1 < ... <  OE2.OE=OE3 <... <  OEOE = 2OE < ... <  OEOEOE = 3OE< ... <  AE0 = OEOEOE...}OE times
= OEOE < ... <  OEOEOE < ... and so on (although what is behind may seems inaccessible
indeed ineffable). An ordinal ! is a limit ordinal if it is neither 0 nor the successor of an
ordinal that is if 3 < ! then there is an ordinal o/ such that 3 < o/ < !. The first limit ordinal
OE is the order type of   well-ordered by <. If C  ffi   then  lub C is the least upper
bound of C [ \Delta  x \Omega  C. x <=  lub C  \Pi  \Delta  a \Omega   . (( \Delta  x \Omega  C. x  <= a)  \Upsilon  (lub C  <= a)] and
lub+ C is the least strict upper bound of C [ \Delta  x \Omega  C. x <  lub+ C \Pi  \Delta  a \Omega   . ((\Delta  x \Omega 

C. x < a)  \Upsilon  (lub+ C <= a)]. A more detailed and rigorous presentation of ordinals can
be found in SHOENFIELD [1977].

A well-founded relation  -< on a set W can be embedded into a well-ordered
relation on W (using  KNUTH [1968a] topological sorting algorithm when W is finite)
hence into an initial segment of the ordinals. Assuming wf(W, -<), we can do this by the
rank function  rk(W,  -<) (for short  rk-<) defined by  rk-<(x) =  lub+{rk-<(y) : y -< x}.

Minimal objects x of W (with no y -< x) will have rank 0. The objects x of W which are
not minimal but which are such that y -< x only for minimal objects y of W will have
rank 1, and so on. One can easily verify by induction on -< that  rk-<(x) is an ordinal.

Observe also that (x -< y)  \Upsilon    (rk-<(x) <  rk-<(y)). We call  rk-<(W) =  lub+{rk<(x) : x \Omega  W}  the  rank of (W,  -<).

6 . 4 Termination proofs by Floyd's well-founded

set method

To prove termination, we must discover a well-founded ( FLOYD [1967a] proposed
well-ordered) relation -< on a set W and a variant function f : \Gamma  ff W and show that its
value decreases after each program step:  \Delta  o/, o/' \Omega  op C . f(o/') -< f(o/).

Example  Proof of termination of program (4) (71)

Program (4) was proved to be partially correct at example (24) using local
invariants (25). To prove termination, let W be  x {L1, ..., L 8} and  -< be the wellfounded relation on W defined by  <Y, L> -< <Y', L'> if and only if (0  <= Y < Y')  \Sigma 
(Y = Y' \Pi  L << L') where < is the usual ordering on natural numbers 0 < 1 < 2 <... and
<< is defined by L 8 << L 6 << L 4 << L 3 << L 2 << L 7 << L 5 << L 1. Let f  : \Gamma  ff W be defined by
f(<Li, s>) = f i(s(Y)) for i = 1, ..., 7 and f(s) = f 8(s(Y)) where f i(y) = <y, L i>. The
proof that the value of f decreases after each program step amounts to the following
local arguments :

(i1) [P1(X,Y,Z,x,y) \Pi  Z' = 1 \Pi  P2(X,Y,Z',x,y)] \Upsilon  f2(Y) -< f1(Y) (since L2 << L1) (72)
(i2) [P2(X,Y,Z,x,y) \Pi  Y ` 0 \Pi  P3(X,Y,Z,x,y)] \Upsilon  f3(Y) -< f2(Y) (since L3 << L2)
(i3) [P2(X,Y,Z,x,y) \Pi  Y = 0 \Pi  P8(X,Y,Z,x,y)] \Upsilon  f8(Y) -< f2(Y) (since L 8 << L2)
(i4) [P3(X,Y,Z,x,y) \Pi  odd(Y) \Pi  P4(X,Y,Z,x,y)] \Upsilon  f4(Y) -< f3(Y) (since L4 << L3)
(i5) [P3(X,Y,Z,x,y) \Pi  even(Y) \Pi  P6(X,Y,Z,x,y)] \Upsilon  f6(Y) -< f3(Y) (since L 6 << L3)
(i6) [P4(X,Y,Z,x,y) \Pi  Y' = Y - 1  \Pi  P5(X,Y',Z,x,y)] \Upsilon  f5(Y') -< f4(Y) (since Y'<Y)
(i7) [P5(X,Y,Z,x,y) \Pi  Z' = Z * X  \Pi  P2(X,Y,Z',x,y)] \Upsilon  f2(Y) -< f5(Y) (since L 2 << L5)
(i8) [P6(X,Y,Z,x,y) \Pi  Y' = Y div 2  \Pi  P7(X,Y',Z,x,y)] \Upsilon  f7(Y') -< f6(Y) (since Y'<Y)
(i9) [P7(X,Y,Z,x,y) \Pi  X' = X * X  \Pi  P2(X',Y,Z,x,y)] \Upsilon  f2(Y) -< f7(Y) (since L 2 << L7)

In practice it is only necessary to prove that all program loops terminate since the
ordering << on labels directly follows from the syntactic structure of the program. \Delta 

Floyd's method for proving termination is sound because no infinite decreasing
sequence f( '0) >> f( '1) >> ... >> f( 'i) >> ... where  ' \Omega  1 OE C (p) is possible, so that
execution of the program must sooner or later terminate in a final state 'n-1 \Omega  S since by
definition of  1n C (p) we have  \Delta  o/ \Omega  \Gamma . <'n-1, o/> fl op C .

For completeness, observe that the distance n - i - 1 of the current state  'i to the
final state 'n-1 of any execution trace  ' of a terminating program is finite and that this
distance strictly decreases after each program step. Hence we can hope to be always
able to define the variant function f('i) as being this distance n - i - 1. When this is true,

we say after DIJKSTRA [1976] that the program strongly terminates. The nondeterminism
of a program C is  finite if and only if no configuration of C can have infinitely many
possible successors:  \Delta   o/  \Omega   \Gamma . \Lambda  n  \Omega   . | { o/' : <o/, o/'> \Omega  op C } |  <= n. It is  bounde d
when there is an upper bound on the number of possible successors:  \Lambda  n \Omega   . \Delta  o/ \Omega  \Gamma .
| {o/' : <o/, o/'> \Omega  op C } | <= n. When the nondeterminism of a program is finite then it
terminates if and only if it strongly terminates. Then termination can always be proved
with (W,  -<) chosen as ( , <). We say that the nondeterminism of a program is
enumerable if and only if any configuration of C has a enumerable set of possible
suc ce ssors:   \Delta    o/   \Omega    \Gamma  .   |   { o/ '   :   < o/ ,  o/ '>  \Omega   o p C } | <= |   | =  OE . When the
nondeterminism of a program is enumerable but not finite, there may be infinitely many
execution traces  ' starting with the same given initial state  '0 = <s, C> with no finite

upper bound on the length n of these traces  '. In this case, the program is said to
weakly terminate. This is the case of the following example:

(X <> 0 * (X < 0  ff (X := ? ; (X < 0  ff X := -X  fi skip)) fi X := X - 1))
where X takes its values in   (i.e. the set D of data at (7) is  ). Then f cannot be chosen
as being integer valued but we can always find a convenient well-founded range (W, -<)
for f.

6 . 5 Liveness

BURSTALL [1974] generalized Floyd's total correctness property into liveness. Given
a specification <P, Q> \Omega  P(\Gamma ) x P(\Gamma  x \Gamma ) a command C is said to be inevitably lead from P
to Q (written P ~~C~~ff Q, using a variant of LAMPORT [1977]'s notation) if any execution

of C starting from an initial configuration  o/ of P inevitably reaches a configuration  o/'
such that <o/, o/'> \Omega  Q. Liveness can be defined in terms of execution traces as follows :

D E F I N I T I O N     BURSTALL [1974] ,  LAMPORT [1977], ALPERN & SCHNEIDER
[1985]  Liveness (73)

P ~~C~~ff  Q :P(\Gamma ) x   x P(\Gamma  x \Gamma ) ff {tt, ff}

P ~~C~~ff Q  = \Delta  o/ \Omega  P. \Delta  ' \Omega  1 C o/. \Lambda  i \Omega  dom '. <o/, 'i> \Omega  Q

In particular  [p]C[q] = {<s, C> : s \Omega  p} ~~C~~ff {<o/, s'> : o/ \Omega  \Gamma  \Pi  s' \Omega  q}.

6 . 6 Generalization Floyd's total correctness

proof method to liveness

Floyd's total correctness proof method can be generalized to liveness properties
by the following induction principle:

THEOREM  PNUELI [1977] , COUSOT & COUSOT [1985]   Floyd's liveness proof method (74)

P ~~C~~ff Q = [ \Lambda  ! \Omega   . \Lambda  i \Omega  ! ff P(\Gamma  x \Gamma ).

(\Delta  o/ \Omega  P. \Lambda  3 < !. <o/, o/> \Omega  i(3)) (.1)
\Pi  (\Delta  3 < !. (3 > 0)

\Upsilon   (\Delta  o/, o/' \Omega   \Gamma . (<o/, o/'> \Omega  i(3))

\Upsilon  (\Lambda  o/" \Omega  \Gamma . <o/', o/"> \Omega  op C ))) (.2)
\Pi  (\Delta  3 < !. (3 > 0)

\Upsilon   (\Delta   o/, o/', o/" \Omega   \Gamma . ( <o/, o/'> \Omega  i(3) \Pi  <o/', o/"> \Omega  op C )

\Upsilon  (\Lambda  3' < 3. <o/, o/"> \Omega  i(3')))) (.3)
\Pi  (i(0) ffl Q)] (.4)

Proof

* We prove soundness (,) by reductio ad absurdum. Assume we have found ! \Omega 

 and i  \Omega   ! ff  P (\Gamma   x \Gamma ) satisfying verification conditions (74) and there is a
configuration  o/ \Omega  P and a trace  ' \Omega  1 C o/ such that  \Delta  j \Omega  dom '. <o/, 'j> fl Q. Then there
would be an infinite strictly decreasing sequence of ordinals 3 \Omega  seqOE   such that \Delta  j \Omega 

. j \Omega  dom ' \Pi  <o/, 'j> \Omega  i(3j), a contradiction since < is well-founded on  . We define
<3j : j \Omega   > inductively as follows: by (74.1),  \Lambda  30 < !. <o/, o/> \Omega  i(30) whence  <o/, '0> \Omega 
i(30) since  '0 = o/. If j \Omega  dom ' \Pi  <o/, 'j> \Omega  i(3j) then  3j ` 0 since otherwise by (74.4)  <o/,
'j> \Omega  i(3j) ffl Q so that by (74.2),  \Lambda  o/ \Omega  \Gamma . <'j, o/> \Omega  op C  so that by (61.1), j + 1  \Omega  dom
' . Moreover by (61.1),  < ' j,  ' j +1 >  \Omega   o p C  so that by (74.3),  \Lambda   3 j+1 <  3 j.
<o/, 'j+1> \Omega  i(3j+1) .

* To prove completeness ( \Upsilon ), assume  \Delta  o/ \Omega  P. \Delta  ' \Omega  1 C o/. \Lambda  i \Omega  dom '. <o/, 'i>
\Omega  Q. Define  <o/', o/'> << <o/, o/> = [ o/ \Omega  P \Pi  \Lambda  ' \Omega  1 C o/. \Lambda  i \Omega  dom '. (\Delta  j < i.  <'0, 'j> fl Q )
\Pi  (<'0, 'i> \Omega  Q)  \Pi  (o/' = o/ = '0) \Pi  \Lambda  k < i. (( o/ = 'k) \Pi  (o/' =  'k+1))].

We prove by reductio ad absurdum that << is well-founded on  \Gamma  x \Gamma . Assume
there is a infinite sequence <'0, '0> >> <'1, '1> >> .... If '0 = '0 then we have (\Delta  k \Omega   . 'k
= '0 \Pi  <'0, 'k> fl Q \Pi  <'k, 'k+1> \Omega  op C ) so that  ' \Omega  1 C '0 in contradiction with  \Lambda  i
\Omega  dom '. <o/, 'i> \Omega  Q. If '0 ` '0 then the same reasoning can be done by concatenation of
a finite prefix  <''0, ''0> >> ... >>  <''k, ''k> such that [ '1 \Omega  P \Pi  '' \Omega  1 C '1 \Pi  \Lambda  i \Omega  dom ''.
\Lambda  k < i. ( \Delta  j < i.  <''0, ''j> fl Q)  \Pi  (<''0, ''i> \Omega  Q)  \Pi  ('0 = '1 = ''0) \Pi  ('1 = ''k) \Pi  ('0 =
''k+1)] to the left of this sequence  <'0, '0> >> <'1, '1> >> ....

We choose  ! = rk<<(\Gamma  x \Gamma ) and i( 3) = { <o/, o/'> \Omega  P x \Gamma  : \Lambda  ' \Omega  1 C o/. \Lambda  i \Omega  dom '.
(\Delta  j < i.  <'0, 'j> fl Q)  \Pi  (<'0, 'i>  \Omega   Q)  \Pi  (o/ =  '0) \Pi  (\Lambda  k  <= i.  o/' =  'k) \Pi  (3 =  rk<<<o/,
o/'>)}. Obviously, ( \Delta  o/ \Omega  P. rk<<<o/, o/> < ! \Pi  <o/, o/> \Omega  i(rk<<<o/, o/>)) so that (74.1) holds.
If 0 <  3 < ! and <o/, o/'> \Omega  i(3) then rk<<<o/, o/'> is different from 0 so that  \Lambda  <o/, o/'> << <o/, o/'>

whence  <o/', o/'> \Omega  op C  and (74.2) is true. If 0 <  3 <!, <o/, o/'> \Omega  i(3) and  <o/', o/"> \Omega 
op C  then let  ' \Omega  1 C o/, i \Omega  dom ' and k  <= i be such that ( \Delta  j < i. <'0, 'j> fl Q) \Pi  (<'0,
'i> \Omega  Q) \Pi  (o/ = '0) \Pi  (o/' = 'k) \Pi  (3 = rk<<<o/, o/'>). Since  rk<<<o/, o/'> is different from 0,  \Lambda 

< o/ ,  o/ '>  <<  < o/ ,  o/ '>  whence [ o/  \Omega   P  \Pi    \Lambda   ''  \Omega   1 C o/ .  \Lambda   i'  \Omega   d o m   ' '. ( \Delta    j   <   i ' .
<''0, ''j> fl Q)  \Pi  (<''0, ''i'> \Omega  Q)  \Pi  (o/ =  o/ =  ''0) \Pi  \Lambda  k' < i'. (( o/' =  ''k') \Pi  (o/' =  ''k'+1))]
so that  <o/, o/'> fl Q, hence <'0, 'k> fl Q so that k < i. Moreover  \Lambda  '" \Omega  1 C o/ with '"0 =
'0 = o/, '"1 = '1, ...,  '"k = 'k = o/', '"k+1 = o/". Then [ \Lambda  i" \Omega  dom '". \Lambda  k" < i". ( \Delta  j < i".
<'"0, '"j> fl Q) \Pi  (<'"0, '"i"> \Omega  Q) \Pi  (o/ = o/ = '"0) \Pi  (o/' = '"k") \Pi  (o/" =  '"k"+1)] with k"
= k so that  <o/, o/"> << <o/, o/'>. It follows that  3' = rk<<<o/, o/"> < rk<<<o/, o/'> = 3 and  <o/, o/'>
\Omega  i(3") whence (74.3) holds. Finally, if  <o/, o/'> \Omega  i(0) and  <o/, o/'> fl Q then rk<<<o/, o/'> = 0

whence there is no  <o/, o/'> << <o/, o/'> so that [ \Delta  ' \Omega  1 C o/. \Delta  i \Omega  dom '. ( \Lambda  j < i.  <'0,
'j> \Omega  Q)  \Sigma  (<'0, 'i> fl Q)  \Sigma  \Delta  k < i. (( o/ ` 'k) \Sigma  \Delta  o/' \Omega  \Gamma . (o/' ` 'k+1))] so that for any  '
\Omega  1 C o/, choosing the least i  \Omega  dom '. <'0, 'i> \Omega  Q, we have i > 1 and  \Delta  k < i. (o/ ` 'k),
a contradiction for k=0.  \Delta 

6 . 7 Burstall total correctness proof method and

its generalization

FLOYD [1967a]'s total correctness proof method is by induction on the structure
of computations where computations are understood as empty or as a step followed by a
computation. A la Floyd proofs are elegant for programs which exactly have this linear
structure of computations. For example, this is the case for a program computing the
size of a list L ::= <> | <A; tl(L)> (where A is an atom and tl(L) is a list) since its
structure is of the form: size(L) = (L = <>  ff  0  fi 1 + size(tl(L)).  HOARE [1969]
remarked that programs (at least those written in structured languages with no gotos,
etc) often have a tree-like structure of computations similar to their syntactic structure,
so that correctness proofs are better handled by induction on this syntactic structure.

BURSTALL [1974] adopted the point of view that proofs are better handled by induction on
the data structures manipulated by the program since the structure of the computations is
often similar to that of these data structures. For example, an iterative program using a
stack for computing the size of a tree T ::= <> | <lf(T); A; rg(T)> (where lf(T) and
rg(T) are trees) would have a structure of computations of the form: size(T) = (L = <>
ff 0 fi 1 + size(lf(L) + size(rg(L)). The following induction principle generalizes these
points of view by considering that arbitrary structures of computations can be specified
by a well-founded relation (W,  -<) (or from a mathematical point of view ( , <))
which basis ultimately corresponds to elementary program steps:

THEOREM  COUSOT & COUSOT [1987]   Burstall's liveness proof method (75)

P ~~C~~ff Q = [ \Lambda  \Lambda  \Omega   . \Lambda  8 \Omega  \Lambda  ff P(\Gamma  x \Gamma ). \Lambda  ! \Omega   . \Lambda  i \Omega  \Lambda  ff ! ff P(\Gamma  x \Gamma ).

(\Lambda  9 \Omega  \Lambda . 89 ffl P * Q) (.1)
\Pi  (\Delta  oe \Omega  \Lambda .\Delta  o/ \Omega  \Gamma . \Lambda  3 < !. <o/, o/> \Omega  ioe(3)) (.2)
\Pi  (\Delta  oe \Omega  \Lambda . \Delta  3 < !. \Delta o/, o/' \Omega   \Gamma .

<o/, o/' > \Omega  ioe(3)) \Upsilon 

[ (\Lambda  o/" \Omega  \Gamma . <o/', o/"> \Omega  op C

\Pi  \Delta  o/" \Omega  \Gamma . (<o/', o/"> \Omega  op C  \Upsilon  \Lambda  3' < 3. <o/, o/"> \Omega  ioe(3'))) (.3)
\Sigma  (\Lambda  oe' <  oe. \Delta  o/" \Omega  \Gamma . <o/', o/"> \Omega  8oe' \Upsilon  \Lambda  3' <  3. <o/, o/"> \Omega  ioe(3'))) (.4)
\Sigma  (<o/, o/'> \Omega  8oe)])] (.5)

To prove P  ~~C ~~ff  Q, we must prove  \Gamma  ~~C ~~ff  8oe for  oe  \Omega   \Lambda  so that by (75.1)
\Gamma  ~~C~~ff 89 = P  ~~C~~ff Q holds. The liveness proofs  \Gamma  ~~C~~ff 8oe for  oe \Omega  \Lambda  can be

done using (75.2), (75.3) and (75.5) hence by Floyd's liveness proof method (74).
However, it is better to exhibit a proof showing up the recursive structure of the
computations. The basis corresponds to elementary program steps (75.3) whereas
induction is described by means of lemmata  8oe', oe' < oe which are first proved to be
correct ( \Gamma  ~~C~~ff 8oe') and can then be used in (75.4) for proving  8oe. More precisely,

<o/, o/'> \Omega   ioe(3) if and only if starting execution in configuration  o/ may lead to
configuration  o/' from which some final configuration  o/ such that  <o/, o/> \Omega   8oe will
inevitably be reached. To prove this we can either show by (75.3) that a single program
step inevitably lead to a state o/" with the same property (<o/, o/"> \Omega  ioe(3')) but closer to the
goal (since  3' < 3) or else use lemma  8oe' which, according to a previous proof ( oe' < oe),
states that zero or more program steps inevitably lead to a state  o/" with the same
property (<o/, o/"> \Omega  ioe(3')) but closer to the goal (since  3' < 3).

7 . Hoare logic

HOARE [1969] introduced the idea that partial correctness can be proved
compositionally, by induction on the syntax of programs. This idea turned out to be of
prime importance in other domains such as denotational semantics where "The values of
expressions are determined in such a way that the value of a whole expression depends
functionally on the values of its parts - the exact connection being found through the
clauses of the syntactical definition of the language" (SCOTT & STRACHEY [1972]) but had
to be slightly modified to take context-sensitive properties of languages into account.

HOARE [1969] also introduced the idea that such proofs can be formalized using a
formal logic. The first motivation is that "axioms may provide a simple solution to the
problem of leaving certain aspects of a language undefined". To illustrate this point of
view, Hoare gives the example of addition ( +) and multiplication ( *) of natural
numbers. These operations can be formalized by a few axioms of which   is a model.
They can also be given different consistent interpretations corresponding to various
possible implementations : and ; of + and * in a machine where only a finite subset {0,
..., maxint} of   is representable. These interpretations include modulo arithmetic
(x : y) = (x + y)  mod (maxint+1), firm boundary arithmetic (x  : y) = (x + y > maxint
ff maxint  fi x + y) and overflow arithmetic (x  : y) = (x + y > maxint  ff undefined  fi
x + y). More generally the idea would be that a program text may have different
interpretations (a computer scientist would say computer implementations) but its
correctness should be established once for all its possible interpretations (hence in a
machine-independent way). This leads to the second motivation of Hoare's axiomatic
semantics: "the specification of proof techniques provides an adequate formal definition
of a programming language". The idea first appeared in  FLOYD [1967a] and was
illustrated by HOARE & WIRTH [1973] on a part of Pascal. The trouble with this axiomatic
semantics is that non-standard hence computer unimplementable interpretations are not
ruled out (BERGSTRA & TUCKER [1984], WAND [1978]).

7 . 1 Hoare logic considered from a semantical

point of view

7 . 1 . 1 General theorems for proof construction

In paragraph $ 5.3, we have considered Hoare logic from a semantical point of
view that is to say with respect to the conventional operational semantics (13). In

summary this essentially consists in the natural extension of the relational semantics
(19) of commands from pairs of states to pairs of sets of states (22). This leads to the
proof of partial correctness by structural induction on commands using theorem (49)
which can also be rephrased as follows:

THEOREM  HOARE [1969], COOK [1978]  Semantic interpretation of Hoare logic (76)

{p}skip{p} = tt (.1)
{{s \Omega  S : s[X \Xi  E(s)] \Omega  p}}X := E{p } = tt (.2)
{p}X := ?{{s[X \Xi  d] : s \Omega  p \Pi  d \Omega  D}} = tt (.3)
{p}(C1; C2){q} = (\Lambda  i \Omega   .{p}C1{i} \Pi  {i}C2{q}) (.4)
{p}(B ff C1 fi C2){q} = ({p ^ B}C1{q} \Pi  {p ^ ~ B}C2{q}) (.5)
{p ^ B}C{p} \Upsilon  {p}(B * C){p  ^ ~B} (.6)
(\Lambda  p', q'  \Omega   . (p  ffl p') \Pi  {p'}C{q'} \Pi  (q ffl q')) =  {p}C{q} (.7)

In these theorems, the consequence rule (76.7) has been isolated whereas in (49) it is
distributed over all theorems (76.1) to (76.6). The idea is interesting because proofs
concerning properties of objects manipulated by the program (which turn out to be
always of the form p  ffl p') are isolated from proofs concerning the computation
(sequence of operations) performed by the program. In practice this separation leads to
excessively tedious proofs. The method proposed by  HOARE [1979] "of reducing the
tedium of formal proofs is to derive general rules for proof construction out of the
simple rules accepted as postulates". For example derived theorems such as
"(p ffl {s \Omega  S : s[X \Xi  E(s)]  \Omega  q})  \Upsilon  {p}X := E{q }" or "({s'[X  \Xi  E(s')] : s'  \Omega  p}  ffl
q) \Upsilon  {p}X := E{q}" are directly applicable hence often more useful than (76.2). Also
t h e   r e c i p r o c a l   o f   ( 7 6 . 6 )   i s   n o t   t r u e   ( f o r   e x a m p l e
" { X   =   0 }   w h i l e   t r u e   d o   X   : =   X   +   1   { X   =   0   \Pi   ~true}" holds but
"{X = 0 \Pi  true} X := X + 1 {X = 0}"  does  not).  Hence  (76.6)  does  not  make
completely clear the fact that a loop invariant has to be found (most often different from
the precondition and postcondition). In practice, we prefer a more explicit formulation :

COROLLARY  Partial correctness proof of while loops (77)

{p}(B * C){q } = (\Lambda  i \Omega   . (p ffl i) \Pi  {i ^ B}C{i} \Pi  ((i ^ ~B) ffl q))

Example  Partial correctness proof of assignments (78)

Let us derive "({s'[X  \Xi  E(s')] : s'  \Omega  p} ffl q) \Upsilon  {p}X := E{q }" from (76) :
(a) {s'[X \Xi  E(s')] : s'  \Omega  p}  ffl q by assumption,
(b) (s \Omega  p) \Upsilon  (s[X \Xi  E(s)] \Omega  q) by (a) and set theory,
(c) p ffl {s \Omega  S : s[X \Xi  E(s)] \Omega  q} by (b) and set theory,
(d) {{s \Omega  S : s[X \Xi  E(s)] \Omega  q}}X := E{q } by (76.2),
(e) q ffl q from set theory,
(f) {p}X := E{q} by (c), (d), (e) and (76.7).

\Delta 
Proof of theorems (76) and (77)

* {p}skip{p} = ((p  * skip) ffl (S x p)) [by (22)] = ((p  * {<s, s> : s \Omega  S}) ffl (S x p))
[by (19.1)] = tt.

* {{s  \Omega  S  : s[X  \Xi   E(s)]  \Omega   p}}X := E{p } = (({s  \Omega   S  : s[X  \Xi   E(s)]  \Omega   p}  *  {<s ,
s[X \Xi   E (s)]>  : s  \Omega   S })  ffl  (S  x p)) [by (22) and (19.2)] = ({ < s, s[X  \Xi   E (s)]>  :
s[X \Xi  E(s)]  \Omega  p}  ffl {<s, s' > : s'  \Omega  p}) = tt .

* {p}X := ?{{s[X \Xi  d] : s \Omega  p \Pi  d \Omega  D}} is tt since (p  * X := ?) = { <s, s[X \Xi  d]> :
s \Omega  p \Pi  d \Omega  D} by (22) and (19.3).

* {p}(C1; C2){q} = (( \Delta s, s', s".(s  \Omega  p \Pi  <s, s'> \Omega  C1 \Pi  <s', s" > \Omega  C2) \Upsilon  s" \Omega  q) by
(22) and (19.4). This implies that if we let i bet {s':  \Lambda  s \Omega  p. <s, s'> \Omega  C1} then (p  * C1)
ffl (S x i) and (i  * C2) ffl (S x q) whence  {p}C1{i} \Pi  {i}C2{q} holds. Reciprocally if i  \Omega 

 is such that  {p}C1{i} \Pi  {i}C2{q} then (p  * C1) ffl (S x i) and (i  * C2) ffl (S x q) by
(22) so that (p  * (C1; C2)) = (p  * (C1 * C2)) = ((p  * C1) * C2) ffl ((S x i) * C2) = (S2 * (i *
C2)) ffl (S2 * (S2 * q)) = ( S2 * q).

* { p }(B  ff  C 1  fi  C2) { q }  = [((p  ^  B )  * C 1  ' (p  ^  ~ B )  * C 2)  ffl   (S  x q)] =
({p ^ B}C1{q} \Pi  {p ^ ~ B }C2{q}) by (22) and (19.5).

* {p}(B * C){q } = ((p * (B * C)*_~B) ffl S x q) [by (22) and (19.6)] = ( \Lambda  i \Omega   . (p
ffl i)  \Pi  (i  * (B  * C ) ffl S x i)  \Pi  ((i  ^  ~B ) ffl  q)) [by (28)] = ( \Lambda  i \Omega   . (p  ffl  i)  \Pi  {i ^
B}C{i} \Pi  ((i ^ ~B) ffl q)) [by (22)]. It follows that  {p ^ B}C{p} \Upsilon  ((p  ffl p) \Pi  {p ^
B}C{p} \Pi  ((p  ^ ~B) ffl (p ^ ~B))) \Upsilon  {p}(B * C){p  ^ ~B}.

* ((p  ffl p')  \Pi  {p'}C{q'} \Pi  (q  ffl q'))  \Upsilon  ((p  ffl p')  \Pi  ((p'  * C) ffl (S x q'))  \Pi  (q  ffl q'))  \Upsilon 
((p * C) ffl (S x q))  \Upsilon  {p}C{q} by (22).  \Delta 

7 . 1 . 2 Semantical soundness and completeness

If we have proved {p}C{q} by application of theorems (76) to components C'  \Omega 

C  of C then we conclude, by structural induction, that  {p}C{q} holds. This is
called semantic soundness . If we can prove by (22) using (13) that  {p}C{q} holds,
then this can always be proved by application of theorems (76) to components C'  \Omega 

C  of C. This is called  semantic completeness . We use the epithet "semantic"
because soundness and completeness are relative to partial correctness as defined by
(22) with respect to operational semantics (13), that is are defined in terms of
mathematical structures without reference to a particular logical language for assertions.

THEOREM  Semantical soundness and completeness of Hoare logic

Hoare's partial correctness proof method (76) is semantically sound. (79)
Hoare's partial correctness proof method (76) is semantically complete. (80)

Proof

The proof that {p}C{q} is provable by (76) is by structural induction on C.

* If {p}skip{q} holds then p  ffl q by (22) and (19.1). Also p  ffl p. Therefore the
proof is "{p}skip{p} by (76.1) whence {p}skip{q} by p ffl p, p ffl q and (76.7)".

* If {p}X := E{q } holds then {s'[X  \Xi  E(s')] : s'  \Omega  p}  ffl q by (22) and (19.2).
Also p  ffl {s \Omega  S : s[X \Xi   E(s)]  \Omega  {s'[X \Xi   E(s')] : s'  \Omega  p}}. Therefore the proof is
simply as follows : " {{s \Omega   S  : s[X  \Xi   E (s)]  \Omega   {s'[X \Xi   E (s')] : s'  \Omega   p } }}  X :=
E  {{s'[X \Xi   E (s')] : s'  \Omega   p} } by (76.2) whence  {p }X := E{q } by p  ffl  {s  \Omega   S  :
s [ X  \Xi   E (s)]  \Omega   { s ' [ X  \Xi   E (s')] : s'  \Omega   p}}, {s'[X  \Xi   E (s')] : s' \Omega    p }   ffl  q  and
(76.7)".

* If {p}X := ?{q } holds then {s[X  \Xi  d] : s  \Omega  p \Pi  d \Omega  D} ffl q by (22) and (19.3).
Also p  ffl p. Therefore the proof is " {p}X := ?{{s[X  \Xi  d] : s \Omega  p \Pi  d \Omega  D}} by (76.3)
whence  {p}X := ?{q} by p  ffl p,  {s[X \Xi  d] : s  \Omega  p \Pi  d \Omega  D} ffl q and (76.7)".

* If {p}C1; C 2{q} holds then by (76.4) there is an assertion i  \Omega    such that
{p}C1{i} and  {i}C2{q} are true. Hence, by induction hypothesis,  {p}C1{i} and
{i}C2{q} are provable by induction on C 1 and C2 using theorems (76). Then applying
(76.4), we conclude  {p}C1; C2){q}.

* If  {p }(B  ff  C 1  fi  C2){q } holds then by (76.5)  {p  ^  B } C 1{q } and  {p  ^
~B}C2{q} are true whence provable by induction on C 1 and C2 using theorems (76).
Then we conclude by (76.5).

* If {p}(B * C){q} holds then by (77) there is an assertion i  \Omega    such that (p ffl i),
{i ^ B}C{i} and (i ^ ~B ffl q) are true. Hence, by induction hypothesis,  {i ^ B}C{i}
is provable by induction on C  using theorems (76). The proof goes on with " {i}(B *
C){i ^ ~B} by (76) whence  {p}(B * C){q } by (p ffl i), (i ^ ~B ffl q) and (76.7)".  \Delta 

7 . 1 . 3 Proof outlines

HOARE [1969] was aware that a formal proof is often tedious and claimed that "it
would be fairly easy to introduce notational conventions which would significantly
shorten it". From a practical point of view, it is indeed essential to be able to present
proofs by Hoare's method informally together with the program text.  OWICKI [1975]
showed that a proof a` la Hoare can be presented as a  proof outline  that is to say as a
proof a` la Floyd where local invariants are attached to program points:

DEFINITION  OWICKI [1975]  Proof outline (81)

The proof outline of a proof of {p}C{q} by theorems (76) is the triple pre, post
:  C  ff  , linv :  C  ff   such that pre(C) = p, post(C) = q and by
structural induction on C'  \Omega   C  :
- If C' is (B  ff  C1 fi  C2) then pre(C 1) = pre(C')  ^ B, pre(C 2) = pre(C')  ^ ~B,
post(C1) = post(C1) = post(C'),

- If C' is (C 1; C 2) then we can only prove  {pre(C')}(C1;  C2){post(C')} by
application of (76.4) once and of (76.7) n  >= 0 times. Therefore we have pre(C') = p 1 ffl
...  ffl pn, {pn}C1{i}, {i}C2{qn}, qn ffl ...  ffl q1 = post(C'). We let pre(C 1) = pre(C'),
post(C1) = pre(C2) = i and post(C 2) = post(C').
- If C' is (B * C 1) then we can only prove  {pre(C')}(B * C 1){post(C')} by
application of (76.6) once and of (76.7) n  >= 0 times. Therefore we have pre(C') = p 1 ffl
... ffl pn = i,  {i ^ B}C1{i}, i ^ ~B = qn ffl ...  ffl q1 = post(C'). We let pre(C 1) = i  ^ B,
post(C1) = i and linv(C') = i.

The following two theorems show that Hoare's method (76) is equivalent to the
syntax-directed presentation of Floyd's method of paragraph 5.3, hence by (55) is
semantically equivalent to Floyd's original stepwise proof method, otherwise stated that
assertions in a proof outline are local invariants:

THEOREM  A` la Floyd presentation of a proof by Hoare logic (82)

A proof outline of {p}C{q} by Hoare's method (76) satisfies (49)

Proof

(49.1) is true by definition (81). If C is skip then  {p}skip{q} can only be proved
using (76.1) and (76.7). It follows that p  ffl q so that (49.2) holds. The proof is similar
when C is X := E or X := ?. (49.5), (49.6) and (49.7) directly follow from definition
(81). \Delta 

THEOREM  A` la Hoare presentation of a proof by Floyd's method (83)
If assertions attached to program points are local invariants in the sense of Floyd (i.e.
satisfies (45) or equivalently by (54) and (55) satisfies (49)) then they can be used to
prove partial correctness by Hoare's method (76).

Proof

The proof is by structural induction on C. If C is X := E then the proof a` la Hoare
is "p  ffl   { s   \Omega   S   :   s [ X  \Xi   E ( s ) ]   \Omega   q} [which is true by (81) and (49.3)],
{{s \Omega  S : s[X \Xi  E(s)] \Omega  q}}X :=  E{q} [by (76.2) and q  ffl q] hence  {p}X := E{q }
[by (76.7)]". The proofs are similar when is is skip or X := ?.

If C is (C 1; C2) then by induction hypothesis there are proofs a` la Hoare of
{pre(C1)}C1{post(C1)} and {pre(C2)}C2{post(C2)}. It follows from (81) and (76.4)
that  p = pre(C1) and post(C 2) = q. If we let i be post(C 1) = pre(C 2) then we can use
(76.7) to prove  {p}C1{i} \Pi  {i}C2{q} and conclude  {p}C1{i} by (76.4). The proofs
are similar when C is (B  ff C1 fi C2) or (B * C 1). \Delta 

7 . 2 Hoare logic considered from a syntactical

point of view

The function of mathematical logic is to provide formal languages for describing
the structures with which the mathematician work, and to study the methods of proof
available to them.  HOARE [1969] introduced the same idea in computer science:
"Computer programming is an exact science in that all the properties of a program and
all the consequences of executing it in any given environment can, in principle, be
found out from the text of the program itself by means of purely deductive reasoning.
Deductive reasoning involves the application of valid rules of inference to sets of valid
axioms. It is therefore desirable and interesting to elucidate the axioms and rules of
inference which underlie our reasoning about computer programs".

Hoare logic   consists of first order predicates P, Q, ... for describing
assertions p, q, ... and correctness formulae {P}C{Q} for describing the partial
correctness {p}C{q} of commands C. First we define the set   of Hoare's formulae
that is the syntax of predicates P, Q, ... and correctness formulae {P}C{Q}. We also
fill in the details of the syntax (1) of expressions and Boolean expressions of the
programming language  . This syntactical definition is parametrized by a basis 1 =
< ,  ,  , #> (also called  signature, type,...) that is sets of constant, function and
relation symbols together with their arity.

Then we introduce Hoare's proof system to define inductively which formulae of
Hoare logic are  provable to be true. The proof system consists of sets of postulates
(axioms) and of syntactic rules of manipulation of formulae by rewritting (rules of
inference) to logically derive conclusions from hypotheses. These axioms and rules of
inference are defined finistically so that given formal proofs are checkable by
algorithmic means (although the proof itself may not be derivable by a machine). This
approach is "syntactical" in that the emphasis is upon a formal language for describing
assertions about the values taken by the program variables and upon a formal deductive
system for deriving proofs based upon combinatorial manipulations of formal
assertions. Proofs are parametrized by a set of axioms, which are supposed to describe
properties of the data manipulated by the programs into all the details of which we do
not want to enter. This is usual in logic where the primitive notions (0, +, ...) of
mathematical theories (such as groups,...) have no fixed meaning. This is also
consistent with  HOARE [1969] point of view that "Another of the great advantages of
using an axiomatic approach is that axioms offer a simple and flexible technique for
leaving certain aspects of a language undefined for example, range of integers, accuracy
of floating point, and choice of overflow technique. This is absolutely essential for
standardization purposes, since otherwise the language will be impossible to implement
efficiently on different hardware designs. Thus a programming language standard

should consist of a set of axioms of universal applicability, together with a choice of
supplementary axioms describing the range of choices facing an implementor".

Then we define which mathematical structures can be understood as being models
or interpretations of Hoare logic. Otherwise stated we define which formulae of Hoare
logic   are semantically true with respect to a relational semantics C of programs C \Omega 

. This programming language semantics (19) itself depends upon the semantics
(also called a  model or interpretation) <D, V> of the basis  1 = < ,  ,  , #> that
is upon the domain  D of data on which programs operate and the exact interpretation
V c  of the basic constants c  \Omega   , V f  of the functions f  \Omega    and  V r  of the
relations r \Omega    involved in the programs and predicates. By leaving this interpretation
as a parameter, we define a family of semantics of Hoare logic with respect to a family
of possible operational semantics of the programming language.

Kurt Go"del showed that  truth and provability do  not necessarily coincide:
provable implies true, refutable implies false but some formulae may be undecidable that
is neither provable nor refutable (using proofs that can be checked mechanically)
although they are either true or false ( SMORYNSKI [1977]). Therefore the question is
whether Hoare's formal proof system captures the true partial correctness formulae,
only these (soundness) and ideally all of these (completeness).

7 . 2 . 1 Syntax of predicates and correctness formulae

We have defined the syntax of the programming language   at (1). We now
define the syntax of the logical language   which will be used to specify the partial
correctness of programs. Hoare logic is a first-order language allowing only to quantify
over elements, but not over subsets or functions (for example "\Delta  A \Omega  P( ). (0 \Omega  A \Pi  \Delta  x
\Omega  A. (x + 1)  \Omega  A) \Upsilon  (A  =  )" or " \Delta  P. \Delta  x1. ... \Delta  xn. {P}skip{P}" are not a firstorder sentences).

In order to specify the syntax of predicates, we consider given disjoint sets of
symbols as follows :

DEFINITION  Symbols (84)

X, Y : Programming variables (.1)
x, y : Logical variables (.2)
v, u :  =   '  Variables (.3)
c : Constant symbols (.4)
f : Function symbols (.5)
r : Relation symbols (.6)

DEFINITION  Arity of function and relation symbols (85)

# :  '  ff  +

At any moment we shall use only a finite number of variables but assume that we are
given a countably infinite supply of these (in the examples we use capital letters for
programming variables and lower-case letters for logical variables). The basis  1  =
< ,  ,  , #> of the logical language   is assumed to be given but is otherwise
left unspecified.

The purpose of the syntactical definition of the logical language   is to define
algorithmically which finite strings of symbols (chosen in   '   '   '   ' {(,
), =, ~, \Pi , \Sigma , \Upsilon , \Lambda , \Delta , ., {, }, skip, :=, ?, ;, ff, fi, *}) belong to  , logicians would say
are well-formed formulae. The sets of  terms, atomic formulae, first-order predicates,
correctness formulae  and formulae  are the smallest sets closed under the following
formation rules (from which a recognizer is easy to derive, see for example AHO, SETHI &
ULLMAN [1986]):

DEFINITION  Syntax of formulae

T : Terms

T ::= v | c | f(T1, ..., T#f) (86)

A : Atomic formulae

A ::= (T1 = T2) | r(T1, ..., T#r) (87)

P, Q : Predicates

P ::= A | ~P | (P 1 \Pi  P2) | (P1 \Sigma  P2) | (P1 \Upsilon  P2) | \Lambda  x. P | \Delta  x. P  (88)

H : Hoare correctness formulae

H ::= {P}C{Q} (89)
F : Formulae of Hoare logic

F ::= P | H  (90)

(in (87), "=" is a relation of arity 2 in infix notation. The reason why we keep it separate
from the relations in   is that its intended interpretation is fixed as the diagonal
(equality) relation  * whereas the interpretations of members of   can be specified
arbitrarily).

In paragraph $ 3.1 the syntax of expressions was left unspecified. From now on,
we assume that   is included in the set of terms containing only programming
variables and that   is included in the set of  propositions (i.e. quantifier-free
predicates) containing only programming variables:

DEFINITION  Syntax of expressions

E : Expressions

E ::= X | c | f(E1, ..., E#f) (91)

B : Boolean expressions

B ::= (E1 = E2) | r(E1, ..., E#r) | ~B | (B1 \Pi  B2) | (B1 \Sigma  B2) | (B1 \Upsilon  B2) (92)

Definitions (84) to (90) are justified by the (restrictive) assumption that all we
shall ever have to say about programs is expressible by sentences of  . To formally
define the axiomatic semantics of the programming language  , it only remains to
define exactly all we can assert to be true about programs. This consists in partitioning

 into  tt (what is truth) and  ff (what is falsity). To do this logicians have

proposed to complementary approaches:
- The semantical point of view consists in defining an interpretation I :   ff {tt,ff}
with  tt = {F  \Omega   : I F = tt} and  ff = {F  \Omega   : I F  = ff}.

- The  syntactical point of view  consists in defining which sentences of   are
provable to be true (with given limited means, so that (hopefully) proofs can be checked
by mechanical computation).
We first start with provability.

7 . 2 . 2 Deductive systems and formal proofs

The basis of the inductive definition of formal proofs for a logical language   is
provided by a set of  axioms, that is a set of formulae of   the truth of which is
postulated. Since the set of axioms is usually not finite, we use a finite set AS = {ASi : i
\Omega  \Delta !} of axiom schemata ASi the instances of which are axioms. Otherwise stated an
axiom schema AS i is a syntactic rule specifying a set {A  \Omega    : IsAxiom(A, AS i)} of
axioms by their syntax. This set of axioms is said to be recursive because membership
is decidable that is there is a program IsAxiom(A, AS i) (called a recognizer) which

given any formula A \Omega    will always terminate and answer "tt" or "ff" whether or not
formula A belongs to the set of axioms generated by the axiom schema ASi.

The induction step in the definition of formal proofs is provided by a set of
inferences <F1, ..., Fn, F> \Omega   n+1 with n  >= 1, traditionally written under the form:

F 1,  ... ,  F n
--------------

F

which means that if formulae F 1, ..., Fn are provable then so is formula F. A further
requirement is again that this set of valid inferences should be recursive. Hence the set
of inferences is usually specified by a finite set IR = {IR i : i \Omega  \Delta 4} of syntactical rules

(called  rules of inference ) so that there is a program IsInference(F 1, ...,  Fn, F, IR i)
which given any formulae F1, ..., Fn, F of   will always terminate and answer "tt" or
"ff" whether or not the inference is correct according to one of the inferences generated
by the rule IRi.

The deductive system   is the (recursive) set of all axioms generated by the axiom
schemata and all inferences generated by the rules of inference:

DEFINITION  Deductive system (93)

AS = {ASi : i \Omega  \Delta !} Axiom schemata,
IsAxiom(A, ASi) Axiom recognizer
IR = {IRi : i \Omega  \Delta 4} Rules of inference
IsInference(F1, ...,  Fn, F, IR i) Inference recognizer

 =  '{{A \Omega    : IsAxiom(A, ASi)} : i \Omega  \Delta !} Deductive system

 ' '{{<F1, ..., F n, F> \Omega   n+1 : IsInference(F 1, ...,  Fn, F, IR i)} : i  \Omega  \Delta 4}

In order to be able to leave unspecified some aspects of the programming language 
(such as machine dependent features) we assume that we are given an additional set Th
ffi  of axioms, the truth of which is taken for granted. Th can be understood as a
specification of the data and operations on these data which are used by the programs of

. A proof of F from Th in the deductive system   is a finite sequence F 0,...,Fn of
formulae, with Fn= F, each of which is either a member of Th, an axiom of  , or else
follows from earlier Fi by one of the inferences of  :

DEFINITION  Formal proof (94)

Proof(F0, ..., F n, F, Th,  ) =
[Fn= F] \Pi  [\Delta  i \Omega  {0,...,n}. [(F i \Omega  Th) \Sigma  (\Lambda  k \Omega  \Delta !. IsAxiom(Fi, ASk)) \Sigma 
(\Lambda  k \Omega  \Delta 4. \Lambda  m >= 1. \Lambda  j1 < i, ...,  \Lambda  jm < i. IsInference(F j1, ..., Fjm, Fi, IRk))]]

Clearly from the above specification we can write a simple combinatorial program to
check proofs (provided Th is recursive). Using the more traditional notations of logic
(BARWISE [1977]) we say that F  is provable from  Th ffi   in  , and write |-  Th '   F, if

there is a proof of F from Th in  :

DEFINITION  Provability (95)
  |- Th '   F  if F \Omega  Th (.1)

|- Th '   F  if F \Omega   (.2)

     F1, ..., F m 
|- Th '   F  if |- Th '   F1, ..., |- Th '   Fm and ----------    \Omega     (.3)

F

7 . 2 . 3 Hoare's proof system 

In HOARE [1969]'s proof system   below, P[v \Xi  T] denotes the predicate obtained
by simultaneously substituting term T for all free occurrences of variable v in predicate
P. If the substitution would cause an identifier in T to become bound (e.g.  \Delta  x .(f(x) =
y)[y \Xi  x]) then a suitable replacement of bound identifiers in P must take place before
the substitution in order to avoid the conflict (e.g. \Delta  z. (f(z) = y)[y  \Xi  x] is \Delta  z. (f(z) =
x)). Substitution will be defined more rigorously later on by (118).

DEFINITION  Hoare proof system
 Axiom schemata of 

{P} skip {P} Skip axiom (96)
{P[X \Xi  E]} X := E {P} (Backward) assignment axiom (97)
{P} X := ? { \Lambda  X. P} Random assignment axiom (98)
(A schema of axioms of the form {P}skip{P} means that  \Delta  v1. ... \Delta  vn. {Q} skip {Q}
is true for all formulae Q  \Omega    with free variables v 1,...,vn. This is an approximation
of the second-order axiom  \Delta  P. \Delta  v1. ... \Delta  vn. {P} skip {P} where quantification over
predicates is mimicked by a recursive set of axioms corresponding to all possible
instances Q of quantified predicate P.)

Rules of inference of 

 {P1} C 1 {P2}, {P 2} C 2 {P3}
------------------------ Composition rule (99)
    {P1} (C 1; C 2) {P 3}

 {P \Pi  B} C 1 {Q}, {P  \Pi  ~B} C 2 {Q}
------------------------------ Conditional rule (100)

   {P}  (B  ff C1 fi C2) {Q}

      {P \Pi  B} C {P}
-------------------- While rule (101)
 {P}(B * C) {P  \Pi  ~B}

 P  \Upsilon   P', {P'} C {Q'}, Q'  \Upsilon    Q
-------------------------- Consequence rule (102)

      {P}  C  {Q}

The backward assignment axiom (97) which corresponds to (45.3) can be given an
equivalent forward form corresponding to (45.8) as proposed by FLOYD [1967a]:

{P} X := E {\Lambda  X'. P[X \Xi  X'] \Pi  X = E[X \Xi  X']}   (Forward) assignment axiom (103)
Example  Formal partial correctness proof of program (4) using  (104)
(a) {(Y-1)>=0 \Pi  Z*X*(X**(Y-1))=x**y} Y := Y-1 {Y>=0 \Pi  Z*X*(X**Y)=x**y} by (97)
(b) {Y>=0 \Pi  Z*X*(X**Y)=x**y} Z := Z*X {Y>=0 \Pi  Z*(X**Y)=x**y} by  (97)
(c) {(Y-1)>=0 \Pi  Z*X*(X**(Y-1))=x**y} (Y := Y-1; Z := Z*X) {Y>=0 \Pi  Z*(X**Y)=x**y} by a, b, (99)
(d) (Y>0 \Pi  Z*(X**Y)=x**y \Pi  odd(Y)) \Upsilon  ((Y-1)>=0 \Pi  Z*X*(X**(Y-1))=x**y) from Th
(e) (Y>=0 \Pi  Z*(X**Y)=x**y) \Upsilon  (Y>=0 \Pi  Z*(X**Y)=x**y) from Th
(f) {Y>0 \Pi  Z*(X**Y)=x**y \Pi  odd(Y)} (Y := Y-1; Z := Z*X) {Y>=0 \Pi  Z*(X**Y)=x**y} by d, c, e, (102)
(g) {(Y div 2)>=0 \Pi  Z*((X*X)**(Y div 2))=x**y} Y := Y  div 2 {Y>=0 \Pi  Z*((X*X)**Y)=x**y} by (97)
(h) {Y>=0 \Pi  Z*((X*X)**Y)=x**y} X := X*X {Y>=0 \Pi  Z*(X**Y)=x**y} by  (97)
(i) {(Y div 2)>=0 \Pi  Z*((X*X)**(Y div 2))=x**y} (Y := Y  div 2; X := X*X){Y>=0 \Pi  Z*(X**Y)=x**y}

by g, h, (99)
(j) (Y>0 \Pi  Z*(X**Y)=x**y \Pi  ~odd(Y)) \Upsilon  ((Y div 2)>=0 \Pi  Z*((X*X)**(Y div 2))=x**y) from Th
(k) {Y>0 \Pi  Z*(X**Y)=x**y \Pi  ~odd(Y)} (Y := Y div 2; X := X*X) {Y>=0 \Pi  Z*(X**Y)=x**y}

by j, i, e, (102)
(l) {Y>0 \Pi  Z*(X**Y)=x**y} (odd(Y) ff (Y := Y-1; Z := Z*X) fi (Y := Y div 2; X := X*X))

{Y>=0 \Pi  Z*(X**Y)=x**y} by f, k, (100)
(m) (Y>=0 \Pi  Z*(X**Y)=x**y \Pi  Y<>0) \Upsilon  (Y>0 \Pi  Z*(X**Y)=x**y) from Th
(n) {Y>=0 \Pi  Z*(X**Y)=x**y \Pi  Y<>0} (odd(Y) ff (Y := Y-1; Z := Z*X) fi

(Y := Y div 2; X := X*X)) {Y>=0 \Pi  Z*(X**Y)=x**y} by m, l, e, (102)
(o) {Y>=0 \Pi  Z*(X**Y)=x**y} (Y<>0 * (odd(Y) ff (Y := Y-1; Z := Z*X) fi

(Y := Y div 2; X := X*X))) {Y>=0 \Pi  Z*(X**Y)=x**y \Pi  ~(Y<>0)} by n, (101)
(p) {Y>=0 \Pi  1*(X**Y)=x**y} Z := 1 {Y >=0 \Pi  Z*(X**Y)=x**y} by (97)
(q) {Y>=0 \Pi  1*(X**Y)=x**y} (Z := 1; (Y<>0 * (odd(Y) ff (Y := Y-1; Z := Z*X) fi

(Y := Y  div 2; X := X*X)))) {Y >=0 \Pi  Z*(X**Y)=x**y \Pi  ~(Y<>0)} by p, o,  (99)
(r) (X=x \Pi  Y=y>=0) \Upsilon  (Y>=0 \Pi  1*(X**Y)=x**y) from Th
(s) (Y>=0 \Pi  Z*(X**Y)=x**y \Pi  ~(Y<>0)) \Upsilon  (Z=x**y) from Th
(t) {X=x \Pi  Y=y>=0} (Z := 1; (Y<>0 * (odd(Y) ff (Y: = Y-1; Z: = Z*X) fi

( Y := Y div 2; X := X*X)))) {Z=x**y} by r, q, s, (102)
\Delta 

7 . 2 . 4 Hoare's proof system   for proof outlines

If the deductive system   is useful for reasoning about Hoare logic, formal proofs
using this proof system are totally unworkable (as shown be the level of details needed

in example (104)). Proof outlines (81), as introduced by  OWICKI [1975]  and OWICKI &
GRIES [1976a], offer a much more useful linear notation for proofs in which the program
is given with assertions interleaved at cutpoints. A natural deduction programming logic
of proof outlines was first presented in  CONSTABLE & O'DONNELL [1978] . Hoare proof
outline system   below is due to  BERGTRA & KLOP [1984] , LIFSCHITZ[1984]. Further
developments can be found in SCHNEIDER & ANDREWS [1986].

DEFINITION  Hoare proof outline system

C ' : Asserted commands (105)

C ' ::= {P1} skip {P2}

| {P1} X := E  {P 2}
| {P1} X := ?  {P2}
| {P1} (C'1; {P2} C'2) {P 3}
| {P1} (B  ff {P2 }C'1 fi {P3} C'2) {P 4}
| {P1} (B *  {P  2} C' {P 3}) {P 4}
| {P}  C'
| C'{P}

{P}  skip {P} Skip axiom (106)
{P[X \Xi  E]}  X := E  {P} Assignment axiom (107)
{P}  X := ?  {\Lambda  X. P} Random assignment axiom (108)
{P1} C'1 {P2}, {P 2} C'2 {P3}
-------------------------- Composition rule (109)
 {P1}  (C'1; {P2}  C'2) {P3}

   {P  \Pi  B}  C'1 {Q}, {P  \Pi  ~B}  C'2 { Q }
------------------------------------ Conditional rule (110)
{P}  (B ff {P  \Pi  B} C'1 fi {P  \Pi  ~B} C'2) {Q}

       {P  \Pi   B}  C'  { P }
------------------------------ While rule (111)
{P}  (B * {P  \Pi  B}C'{P}) {P  \Pi  ~B}

(P  \Upsilon  P'), {P'}  C'  {Q}
------------------ Left consequence rule (112)
  {P}  {P'}  C'  { Q }

{P}  C' {Q'}, (Q'  \Upsilon  Q)
------------------ Right consequence rule (113)
  {P}  C'{Q'} {Q}

Example  Proof outline of program (4) using   (114)
{X = x \Pi  Y = y >=0} by (112), (113)

{Y >= 0 \Pi  1*(X**Y) = x**y} by  (107), (109)

(Z := 1;
{Y >= 0 \Pi  Z*(X**Y) = x**y} by  (111)

(Y <> 0 *

{Y >= 0 \Pi  Z*(X**Y) = x**y \Pi  Y <> 0} by  (112)

{Y > 0 \Pi  Z*(X**Y) = x**y} by  (110)

(odd(Y)  ff

{Y > 0 \Pi  Z*(X**Y) = x**y \Pi  odd(Y)} by  (112)

{(Y-1) >= 0 \Pi  Z*X*(X**(Y-1)) =x**y} by  (107), (109)

(Y := Y - 1;
{Y >= 0 \Pi  Z*X*(X**Y) = x**y} by  (107)

Z := Z * X)
fi

{Y > 0 \Pi  Z*(X**Y) = x**y \Pi  ~odd(Y)} by  (113)

{(Y div 2) >= 0 \Pi  Z*((X*X)**(Y div 2)) = x**y} by  (107), (109)

(Y := Y div 2;
{Y >= 0 \Pi  Z*((X*X)**Y) = x**y} by  (107)

X := X * X)
)
{Y >= 0 \Pi  Z*(X**Y) = x**y}

) )
{Y >= 0 \Pi  Z*(X**Y) = x**y \Pi  ~(Y <> 0)}
{Z = x**y}
\Delta 

7 . 2 . 5 Syntactical rules of substitution

Up to now, we have used informal definitions of variables, bound variables and
free variables occurring in a predicate or a command and of the substitution P[v  \Xi  T]
of a term T for a variable v in a predicate P. We now give the fully formal definitions.
This paragraph can be omitted and one can go on with paragraph $ 7.3.

7 . 2 . 5 . 1 Variables appearing in a term, predicate, command or

correctness formula

The set of variables appearing in a term, predicate, command or correctness
formula is defined by structural induction as follows :

DEFINITION  Variables appearing in a formula (115)

Var(c) =  o/ (.1)
Var(v) = {v} (.2)
Var(f(T1, ..., T#f)) = '{Var(Ti) : 1 <= i <= #f} (.3)

Var((T1 = T2)) = Var(T1) ' Var(T2) (.4)
Var(r(T1, ..., T#r)) = '{Var(Ti) : 1 <= i <= #r} (.5)

Var(~P) = Var(P) (.6)
Var((P1 \Pi  P2)) = Var((P1 \Sigma  P2)) = Var((P1 \Upsilon  P2)) = Var(P1) ' Var(P2) (.7)

  Var(\Lambda  v. P) = Var(\Delta  v. P)  = {v}  ' Var(P)  (.8)

Var(skip) =  o/ (.9)
Var(X := E) = {X} ' Var(E) (.10)
Var(X := ?) =  {X} (.11)
Var((C1; C2)) = Var(C1) ' Var(C2) (.12)
Var((B ff C1 fi C2)) = Var(B) ' Var(C1) ' Var(C2) (.13)
Var((B * C)) = Var(B) ' Var(C) (.14)
Var({P} C {Q}) = Var(P) ' Var(C) ' Var(Q) (.15)
Var(F1, ... , Fn) = Var(F1) ' ... ' Var(Fn) (.16)

7 . 2 . 5 . 2 Bound and free variables appearing in a term, predicate,

command or correctness formula

In the formula  \Lambda  x. ((+(x, y) = 0)), variable x is "bounded" up by  \Lambda  whereas y is
sort of floating "free". The notions of bound and free variable can be made more precise
as follows :

DEFINITION  Bound variables appearing in a formula (116)

Bound(A) = o/ (.1)
Bound(~P) =  Bound (P) (.2)
Bound((P1\Pi P2)) = Bound((P1\Sigma P2)) = Bound((P1\Upsilon P2)) = Bound(P1) ' Bound(P2)

(.3)
  Bound(\Lambda  v. P) =  Bound(\Delta  v. P) =  Bound(P) ' {v} (.4)

Bound(C) = o/ (.5)
Bound({P} C {Q}) =  Bound(P) ' Bound(Q) (.6)
Bound(F1, ... , F n) = Bound(F1) '...' Bound(Fn) (.7)

DEFINITION  Free variables appearing in a formula (117)

Free(A) = Var(A) (.1)
Free(~P) = Free(P) (.2)
Free((P1 \Pi  P2)) = Free((P1 \Sigma  P2)) = Free((P1 \Upsilon  P2)) = Free(P1) ' Free(P2) (.3)

  Free(\Lambda  v. P) = Free(\Delta  v. P)  = Free(P) - {v} (.4)

Free(C) = Var(C) (.5)
Free({P} C {Q}) = Free(P) ' Free(C) ' Free(Q) (.6)
Free(F1, ... , Fn) = Free(F1) ' ... ' Free(Fn) (.7)

7 . 2 . 5 . 3 Formal definition of substitution of a term for a variable in

a term or predicate

The substitution P[v \Xi  T] denotes the result of renaming bounded occurrences of
variables in P so that none of them is v or belongs to Var(T) and then replacing all free
occurrences of variable v by term T. Substitution can be formally defined as follows :

DEFINITION  Substitution of a term for a variable (118)

v'[v \Xi   T]   = T if v'=v (.1)

= v' if v' `v
c[v \Xi  T ] = c (.2)
f(T1,...,T#f)[v \Xi  T] = f(T 1[v \Xi  T],...,T#f[v \Xi  T]) (.3)
(T1 = T 2)[v \Xi  T] = (T 1[v \Xi  T] = T 2[v \Xi  T]) (.4)
r(T1,...,T#r)[v \Xi  T] = r(T 1[v \Xi  T],...,T#r[v \Xi  T]) (.5)
(~P)[v \Xi   T ] = ~(P[v  \Xi   T]) (.6)
(P1 \Pi  P2)[v \Xi  T ] = (P 1[v \Xi  T]  \Pi  P2[v \Xi  T]) (.7)
(P1 \Sigma  P2)[v \Xi  T ] = (P 1[v \Xi  T]  \Sigma  P2[v \Xi  T]) (.8)
(P1 \Upsilon  P2)[v \Xi  T] = (P 1[v \Xi  T]  \Upsilon  P2[v \Xi  T]) (.9)
(\Lambda v'.P)[v \Xi   T ]

= \Lambda v'.P if v' = v (.10)
= \Lambda v'.(P[v \Xi   T]) if v' ` v and v' fl Var(T)
= \Lambda w.(P[v' \Xi  w])[v \Xi  T] where w  fl {v}'Var(T)'Var(P),

if v' ` v and v' \Omega  Var(T)
(\Delta v'.P)[v \Xi   T ]

=  \Delta v'.P if v' = v (.11)
= \Delta v'.(P[v \Xi   T ]) if v' ` v and v' fl Var(T)
= \Delta w.(P[v' \Xi  w])[v \Xi  T] where w  fl {v}'Var(T)'Var(P),

if v' ` v and v' \Omega  Var(T)

7 . 3 The semantics of Hoare logic

We now define the semantics of Hoare logic that is an interpretation I :   ff {tt,
ff} defining the truth of predicates and correctness formulae with respect to a relational
semantics (19) of the programming language  . This programming language
semantics depends upon the semantics (also called a model or interpretation) <D, V> of
the basis 1. By leaving this interpretation unspecified, we define a family of semantics
of Hoare logic with respect to a family of possible relational semantics of the
programming language.

7 . 3 . 1 Semantics of predicates and correctness formulae

A model or interpretation <D, V> of the basis  1 =  < ,  ,  , #> specifies
the semantics of the common part of the programming and logical languages. It consists
of a nonempty set  D of data and a function  V with domain   '   '   which
define the intended meaning of constants, functions and relations :

DEFINITION  Interpretation of symbols (119)

V c  \Omega  D (.1)
V f  : D#f ff D (.2)
V r  ffl D#r (.3)

Let us also recall that we have defined states (or valuations) s assigning a value s(v) \Omega  D
to variables v  \Omega    (8) and s[v  \Xi  d] for the state s' which agrees with s except that
s'(v) = d :

DEFINITIONS  States and assignments

s : S =   ff D States (120)
s[v \Xi  d](u) = (v = u  ff d  fi s(u)) Assignment (121)

These states have been used to remember the values assigned to programming variables
during program execution. They will also be used to specify values for free variables in
first order predicates. Remarkably, programming and logical variables can be handled
the same way . This is not always possible for more complicated programming
languages.

We now define the semantics or interpretations  T = I T  of terms T,  P = I P  of
predicates P and  {P}C{Q} = I {P}C{P}  of correctness formulae {P}C{Q} with
respect to a given model <D, V> (and a given state for terms and predicates):

DEFINITION  Interpretation of terms (122)

I :   ff (S ff D),  T = I T
I v (s) = s(v) (.1)
I c (s) = V c (.2)
I f(T1, ..., T #f) (s) = V f (I T1 (s), ...,  I T#f (s)) (.3)

DEFINITION  Interpretation of predicates (123)

I :   ff   ,    P = I P
I (T1 = T2) = {s \Omega  S : <I (T1) (s), I T2 (s)> \Omega  *} (.1)
I r(T1, ..., T #r) = {s \Omega  S : <I T1 (s), ...,  I T#r (s)> \Omega  V r } (.2)
I ~ P = S - I P (.3)
I (P1 \Pi  P2) = I P1  ^ I P2 (.4)
I (P1 \Sigma  P2) = I P1  ' I P2 (.5)
I (P1 \Upsilon  P2) = (S - I P1 ) ' I P2 (.6)
I \Lambda  v. P = {s \Omega  S : ({s[v  \Xi  d] : d  \Omega  D} ^ I P ) ` o/} (.7)
I \Delta  v. P = {s \Omega  S : {s[v  \Xi  d] : d  \Omega  D} ffl I P } (.8)

DEFINITION  Interpretation of correctness formulae (124)

I :   ff {tt, ff}
I {P} C {Q} = {I P }C{I Q } where    {p}C{q} = (p * C) ffl (S x q)

Observe that the truth or falsity of a formula {P}C{Q} just depends upon the model
<D, V> since the semantics C of command C (19) itself depends only on the semantics
E of expressions E and B of Boolean expressions B which is the same as the semantics
of terms (with no logical variables) and predicates (with no quantifiers and logical
variables). An interpretation of {P}C{Q} different from (124) is investigated in
ANDRE'KA & NE'METI [1978] , GERGELY & U'RY [1978], ANDRE'KA, NE'METI & SAIN [1979] [1981]
[1983], CSIRMAZ [1981a] [1981b], HORTALA'-GONZA'LEZ & RODRI'GUEZ-ARTALEJO [1985] , NE'METI
[1980], RODRI'GUEZ-ARTALEJO [1985] using a nonstandard transfinite definition of execution
traces.

DEFINITION  Interpretation of formulae (125)

I :   ff {tt, ff}
I F  =  (F  \Omega    ff  I F =  S  fi I F )

(The function I is polymorphic, so that when P  \Omega   , and depending upon the context
,we have either  I P \Omega  P(S ) (by (123)) or  I P \Omega  {tt, ff} (by (125))).

Example  Proof of a formula with two different interpretations (126)

Let us consider the basis  <{0, 1}, {+}, o/, # >  with #(+) = 2. Then H =
{X = 1} (~(X = 0) * X := X + 1)  {X = 0}  is  a  formula  of  .

A first interpretation would be  < , V> with V 0 = 0 , V 1 = 1 , V + (x, y) = x

+  y. With this interpretation formula H is semantically true ( I H = tt) because

execution of program (~(X = 0) * X := X + 1) starting in a state s such that s(X) = 1
will never terminate.

A second interpretation would be  <{0 , 1 }, V> with  V 0 = 0 , V 1 = 1 ,

V + (x, y) = (x +  y) mod 2 . With this interpretation formula F is semantically true
because execution of program (~(X = 0) * X := X + 1) always terminates in a state s
such that s(X) = 0 .

Formula H can be proved to be formally correct from tautologies Th = {((true  \Pi 
~(X = 0))  \Upsilon  true), (true  \Upsilon  true), ((X = 1)  \Upsilon  true), ((true  \Pi  ~~(X = 0))  \Upsilon   (X = 0))}
where true denotes truth e.g.. (x = x), as follows:
(a) (true \Pi  ~(X = 0)) \Upsilon  true by Th
(b) {true} X := X + 1 {true} by  (97)
(c) (true) \Upsilon  (true) by Th
(d) {true \Pi  ~(X = 0)} X := X + 1 {true} by a, b, c, (102)
(e) {true} (~(X = 0) * X := X + 1) {true  \Pi  ~~(X = 0)} by d, (101)
(f) (X = 1) \Upsilon  true by Th
(g) (true \Pi  ~~(X = 0)) \Upsilon  (X = 0) by Th
(h) {X = 1} (~(X = 0) * X := X + 1) {X = 0} by f, e, g, (102)
\Delta 

7 . 3 . 2 Semantics of substitution

In (118) we have defined the substitution of a term for a variable in a predicate
which is used in the assignment axiom schema (97). To prove that this axiom schema is
sound we shall need a semantical characterization of substitution. This paragraph $
7.3.2 can be omitted on first reading.
  Informally substitution commutes with interpretation. More precisely, the
interpretation  T[v \Xi  T'](s) of term T where T' is substituted for v in state s is the
interpretation  T(s[v \Xi  T'(s)]) of term T in state s' =s [v  \Xi  T'(s)] which agrees with s
except that the value s'(v) of variable v is the interpretation of term T' in state s :

LEMMA  Semantics of substitution of a term for a variable in a term (127)

T[v \Xi  T'](s) =  T(s[v \Xi  T'(s)])

Proof

By structural induction on the syntax of terms :

* v [ v  \Xi   T ' ] (s) =  T ' (s) [by (118.1)] = s [ v  \Xi   T ' ( s ) ] (v) [by (121)] =
v(s[v \Xi  T'(s)]) [by (122.1)],

* when v'  ` v,  v'[v \Xi   T'](s) =  v'(s) [by (118.1)] = s(v') [by (122.1)] =
s[v \Xi  T'(s)](v') [by (121)] =  v'(s[v \Xi  T'(s)]) [by (122.1)],

* c[v \Xi   T'](s) =  c(s) [by (118.2)] =  V c  [by (122.2)] =  c(s[v \Xi   T'(s)]) [by
(122.2)],

* f(T1 ..., T#f)[v \Xi   T'](s) =  f(T1[v \Xi   T'], ..., T#f[v \Xi   T'])(s) [by (118.3)] =
V f (T1[v \Xi   T'](s), ..., T#f[v \Xi   T'](s)) [by (122.3)] =  V f (T1(s[v \Xi   T'(s)]), ...,
T#f(s[v \Xi  T'(s)])) [by induction hypothesis (127)] =  f(T1, ..., T#f)(s[v \Xi  T'(s)]) [by
(122.3)]. \Delta 

The same way, substitution of a term for a variable in a predicate can be
semantically characterized by the following :

LEMMA  Semantics of substitution of a term for a variable in a predicate (128)

P[v \Xi  T] = {s  \Omega  S : s[v  \Xi  T(s)]  \Omega  P}

Proof

The proof is (almost) by structural induction on the syntax of predicates. The only
difficulty is for ( \Delta  v'. P)[v  \Xi  T] when v'  ` v and v'  \Omega  Var(T) because  \Delta  w. P[v'  \Xi  w]
is not a syntactic component of  \Delta  v'. P. However they have the same shapes and more
variables of T appear in  \Delta  v'. P than in  \Delta  w. P[v'  \Xi  w]. Thus we define the height
#(P,T) of a predicate P with respect to a term T by structural induction as follows: #(A,
T) = 0;  #(~P, T) = 1 +  #(P, T);  #(P1 \Pi  P2, T) =  #(P1 \Sigma  P2, T) =  #(P1 \Upsilon   P2, T) =
1   +   m a x ( # ( P 1 , T),  # ( P 2 , T));  # (\Lambda   x. P, T) =  # (\Delta   x. P, T) = 1 +  # ( P ,   T )   +   |
Var(P) ^ Var(T) |. For a given term T, the proof is by induction on the height  #(P, T)
of P. This is long but not difficult. Therefore we only treat few typical cases :

* ( T 1   =   T 2 ) [ v  \Xi   T ]   =  ( T 1 [ v  \Xi   T] = T 2 [ v  \Xi   T ] )   [by (118.4)] = {s  \Omega   S   :
T 1 [ v  \Xi   T ] (s) =  T 2 [ v  \Xi   T ] (s)} [by (123.1) and definition of  * ] = {s  \Omega   S  :
T1(s[v \Xi  T(s)]) =  T2(s[v \Xi  T(s)])} [by (127)] = {s  \Omega  S : s[v  \Xi  T(s)]  \Omega  (T1 = T 2)}
[since ( T1(s[v \Xi  T(s)]) = T2(s[v \Xi  T(s)])) \Phi  (s[v \Xi  T(s)] \Omega  {s' :  T1(s') =  T2(s')})  \Phi 
(s[v \Xi  T(s)] \Omega  (T1 = T2)), by (123.1)].

* (P1 \Upsilon  P2)[v \Xi  T] = (P1[v \Xi  T] \Upsilon  P2[v \Xi  T])  [by (118.9)] = ( S - P1[v \Xi  T])
' P2[v \Xi  T] [by (123.6)] = ( S - {s  \Omega  S : s[v \Xi  T(s)] \Omega  P1}) ' {s \Omega  S : s[v \Xi  T(s)] \Omega 
P 2} [by induction hypothesis (128)] = {s  \Omega   S  : s[v  \Xi   T (s)] fl  P 1}  '  {s  \Omega   S  :
s[v \Xi   T(s)] \Omega  P2} = {s  \Omega   S  : s [v \Xi   T(s)] \Omega  (S - P1) ' P2} = {s  \Omega  S : s[v \Xi   T(s)]
\Omega  (P1 \Upsilon  P2)} [by (123.6)].

* (\Delta  v.  P)[v \Xi  T] = \Delta  v. P [by (118.11)] = {s  \Omega  S : \Delta  d \Omega  D. s[v  \Xi  d]  \Omega  P} [by
(123.8)] = {s  \Omega  S : \Delta  d  \Omega  D. (s[v  \Xi  T(s)])[v \Xi  d]  \Omega  P} [since (s[v  \Xi  d'])[v \Xi  d] =
s[v \Xi  d] by (121)] = {s  \Omega  S : s[v \Xi  T(s)] \Omega  \Delta  v. P} [by (123.8)].

* if v'  ` v and v'  fl Var(T) then  (\Delta  v'. P)[v  \Xi  T] = \Delta  v'. (P[v  \Xi  T])  [by (118.11)]
= {s  \Omega  S : {s[v'  \Xi  d] : d  \Omega  D} ffl P[v \Xi  T]} [by (123.8)] = {s  \Omega  S : {s[v'  \Xi  d] : d  \Omega 
D} ffl {s \Omega  S : s[v \Xi  T(s)] \Omega  P}} [by induction hypothesis (128) since  #(P, T) < #(\Delta  v'.
P, T)] = {s  \Omega   S  :  \Delta   d \Omega   D . ((s[v'  \Xi   d ] ) [ v  \Xi   T (s)]  \Omega   P )} = {s  \Omega   S  :  \Delta   d  \Omega   D .
((s[v \Xi   T (s)])[v' \Xi   d]  \Omega   P )} [by (121) since v  ` v'] = {s  \Omega   S  : s[v  \Xi   T (s)]  \Omega 
(\Delta  v'. P)}  [by (123.8)].

* if v'  ` v and v'  \Omega  Var(T) then  (\Delta  v'. P)[v  \Xi  T] = \Delta  w. (P[v'  \Xi  w])[v \Xi  T] [by
(118.11)] = {s  \Omega  S : s[v  \Xi  T(s)] \Omega  \Delta  w. (P[v'  \Xi  w])}  [by induction hypothesis (128)
since v'  \Omega  Var(T) and w  fl {v}  ' Var(T) ' Var(P) imply  Var((\Delta  v'. P))  ^Var(T) =
( V a r ( \Delta   w .   ( P [ v '   \Xi    w ] ) )   ^  V a r ( T ) )   '  { v ' }   w h e n c e
# (\Delta   w.  (P[v' \Xi   w ] ), T )   <  # (\Delta   v'. P, T)  since  # ( P[v' \Xi   w ],   T )  =   # (P, T)  and
v'  fl  V a r ( \Delta    w .   ( P [ v '  \Xi   w ] ) )   ^  V a r (T)] = {s  \Omega   S  : s[v  \Xi   T (s)]  \Omega   {s'  \Omega   S   :
{ s ' [ w  \Xi   d] : d  \Omega    D }  ffl  P [ v '  \Xi   w ] }} [by (123.8)] = {s  \Omega   S  :  \Delta    d   \Omega    D .
s[v \Xi   T (s)][w \Xi   d]  \Omega   P[v' \Xi   w ] }} = {s  \Omega   S  :  \Delta   d \Omega   D . s[v \Xi   T (s)][w \Xi   d]  \Omega 
{s' \Omega  S : s'[v'  \Xi  w(s')]  \Omega  P}} [by induction hypothesis (128) since  #(P[v' \Xi  w], T)
<  # ((\Delta   v'. P), T)  because  w fl  V a r (P) '  V a r (T) whence  # (P[v' \Xi   w], T)  <=
#(P, T)] = {s  \Omega  S :  \Delta  d \Omega  D . s[v  \Xi   T (s)][w \Xi   d][v' \Xi   w (s[v \Xi   T (s)][w \Xi   d])]  \Omega 
P} = {s  \Omega  S : \Delta  d \Omega  D. s[v  \Xi  T(s)][w \Xi  d][v' \Xi   d]  \Omega  P} [by (122.1) and (121)] =
{s \Omega  S : \Delta  d \Omega  D. s[v  \Xi  T(s)][v' \Xi  d][w \Xi  d] \Omega  P} [by (121) since w  ` v'] = {s  \Omega  S :
\Delta  d \Omega  D. s[v  \Xi  T(s)][v' \Xi  d] \Omega  P} [since w  fl Var(P)] = {s  \Omega  S : s[v  \Xi  T(s)]  \Omega  (\Delta  v'.
P)} [by (123.8)].  \Delta 

7 . 4 The link between syntax and semantics:

soundness and completeness issues in Hoare
l o g i c

In paragraph $ 7.2 we have defined the language   of Hoare logic and then the
provability |- Th '   F of formulae F. In paragraph $ 7.3 we have defined the semantics

of   that is the truth  F of formulae F. We now investigate the relationship between
these two definitions that is the soundness of provability (is a provable formula always
true ?) and the completeness of provability (is a true formula always provable ?). The
deductive system   is sound for   (provided all theorems in Th are true). The
question of completeness is more subtle because this depends upon which class of
interpretations I (induced by the semantics (D, V) of the basis 1) is considered. Hence
we can only prove  relative completeness , a notion first delineated by  WAND [1978] and
COOK [1978].

7 . 4 . 1 Soundness of Hoare logic

Hoare deductive system   is sound: if we have proved {P} C {Q} from Th using
 then C is partially correct with respect to specification <P, Q> (assuming that all T in
Th are true) :

THEOREM  COOK [1978]  Soundness of Hoare logic (129)
(\Delta  T \Omega  Th. I T  = tt) \Pi  (|- Th '   {P} C {Q}) \Upsilon  {P} C {Q}

The proof of (129) shows that Hoare's formal proof system   simply consists in
applying theorem (76) within the framework of the restricted logical language  . This
proof can be done by a theorem prover (SOKOLOWSKI [1987]).

Proof

Assuming  \Delta  T \Omega  Th. I T = tt and given a proof H 0, ..., H n of {P} C {Q}, we
prove by induction that for all i = 0, ..., n we have I Hi  = tt, so that in particular {P} C
{Q} is true:

* If Hi \Omega  Th, then by hypothesis  I Hi  = tt.

* If Hi is an axiom of  , then three cases have to be considered for any given P  \Omega 

  :
- For a skip axiom (96), {P} skip {P} obviously holds by (76.1),
- For a backward assignment axiom (97), we have  {P[X \Xi  E]} X := E  {P} =
{{s \Omega  S : s[v \Xi  E(s)] \Omega  P}} X := E {P} [by (128)] which is true by (76.2),
- For a forward assignment axiom (103), we have  \Lambda  X'. P[X  \Xi   X']  \Pi  X =
E[X \Xi   X'] = {s  \Omega  S : ({s[X'  \Xi   d] : d  \Omega  D} ^ P[X  \Xi   X']  \Pi  X = E[X  \Xi   X'])  ` o/}
[by (124) and (123.7)] = {s  \Omega  S : ({s[X'  \Xi  d] : d \Omega  D} ^ {s \Omega  S : s[X \Xi  s(X')] \Omega  P} ^
{s \Omega  S : s(X) =  E(s[X \Xi  s(X')])}) ` o/} [by (123.4), (128), (122.1) and (127)] = {s  \Omega 
S : \Lambda  d \Omega  D. s[X  \Xi  d]  \Omega  P \Pi  s(X) =  E(s[X \Xi  d])}  whence  {P} X := E { \Lambda  X'. P[X  \Xi 
X']  \Pi  X = E[X  \Xi  X']}  = P * X :=  E ffl S  x {s  \Omega  S : \Lambda  d \Omega  D. s[X  \Xi  d]  \Omega  P \Pi  s(X) =
E(s[X \Xi  d])} [by (124)] = {s[X  \Xi  E(s)] : s  \Omega  P} ffl {s  \Omega  S : \Lambda  d \Omega  D. s[X  \Xi  d] \Omega  P \Pi 
s(X) =  E(s[X \Xi  d])} [by (19.2)] =  \Delta  s \Omega  P. \Lambda  d \Omega  D. s[X \Xi  E(s)][X  \Xi  d] \Omega  P \Pi  s[X \Xi 
E(s)](X) =  E(s[X \Xi  E(s)][X \Xi  d]) = \Delta  s \Omega  P. \Lambda  d \Omega  D. s[X \Xi  d] \Omega  P \Pi  E(s) = E(s[X \Xi 
d]) which is obviously true by choosing d =  E(s).
- For a random assignment axiom we have  {P} X := ? { \Lambda  X. P} = {P} X := ? {{s
\Omega  S :  ({s[X \Xi  d] : d  \Omega  D} ^ P) ` o/}} [by (123.7)] =  {P} X := ? {{ s  \Omega  S : \Lambda  d \Omega  D.
s[X \Xi  d] \Omega  P}} = {P} X := ? { {s'[X \Xi  d'] : d'  \Omega  D \Pi  s' \Omega  P}} [by (121) since we let
s' = s[X  \Xi  d] and d' = s(X)] = tt by (76.3).

* If Hi follows from an inference of  , then four cases have to be considered for any
given P 1, P 2, P, P', Q, Q'  \Omega    :

- For a composition inference, assuming  {P1} C1 {P2} and  {P2} C2 {P3} by
induction hypothesis, we have {P1} (C1; C2) {P3} = tt by (76.4),
- For a conditional inference (100), assuming  {P  \Pi  B} C 1 {Q} and {P  \Pi  ~B}
C2 {Q} by induction hypothesis, we have  {P ^ B} C1 {Q} and  {P ^ ~B} C2 {Q} by
(123.4) and (123.3) hence  {P} (B  ff C1 fi C2) {Q} is true by (76.5),
- For a while inference (101), assuming  {P \Pi  B} C {P}, we have  {P ^ B} C {P}
[by (123.4)] =  {P} (B * C) {P  ^  ~B} [by (76.6)] =  {P} (B * C) {P  \Pi  ~B} [by
(123.4) and (123.3)],
- For a consequence inference (102), assuming  (P \Upsilon  P'), {P'} C {Q'} and  (Q' \Upsilon 
Q) by induction hypothesis, we have  P ffl P' and Q' ffl Q by (123.6) hence  {P} C {Q}
by (76.7).  \Delta 

7 . 4 . 2 Relative completeness of Hoare logic
7 . 4 . 2 . 1 Completeness and incompleteness issues for Hoare logic

Having defined the syntax   of programs [see (1) with later complements (91)
for expressions and (92) for Boolean expressions] parametrized by a basis  < ,  ,

, #> [(84), (85)], we have approached Hoare's ideas on formal definition of the
partial correctness of programs C \Omega   , in essentially two different ways:

* From the semantic point of view of computer scientists (corresponding to the
point of view of mathematicians using normal everyday set-theoretic apparatus where
objects are thought of in terms of their representation, e.g. sets are understood as
collections of objects), we have assumed that the semantics  <D, V> of the basis  < ,

,  , #> is given (119), from which we have defined the sets  S of states (120), the
set   = P(S) of assertions as well as the semantics  E = I E  (122) of expressions E
(91) and the semantics  B = I B  (123) of Boolean expressions B (92), whence the
operational semantics op C  (13) and then the relational semantics C = I C  [(18), (19)]
of programs C  \Omega   . This leads to the introduction of Hoare's partial correctness
specifications:

(I) =   x   x  Hoare's specifications (130)
where the dependence upon the interpretation  I is a shorthand for denoting the
dependence upon the basis  < ,  ,  , #> and its semantics  <D, V>. A triple  <p,
C, q> of  (I) should be understood as specifying that p is the input specification and q
is the output specification of program C where p and q are sets of states specifying all
possible combinations of values of the variables. The set  (I) has been partitioned

(22) into a subset representing truth (i.e. which programs C are partially correct with
respect to which specifications):

(I)tt  = {<p, C, q > : p  \Omega    \Pi  Hoare's valid specifications (131)

  C  \Omega    \Pi  q \Omega    \Pi  {p}C{q} = tt}

and a subset representing falsity (i.e. all we know not to be true about the partial
correctness of programs):

(I)ff  = {<p, C, q > : p  \Omega    \Pi  Hoare's invalid specifications (132)

  C  \Omega    \Pi  q \Omega    \Pi  {p}C{q} = ff}

Within this framework, we have explained several (equivalent) partial correctness proof
methods by decomposition of the proof that  {p}C{q} = tt into simpler elementary
proofs based upon stepwise induction (45) as in  NAUR [1966]  and  FLOYD [1967a]  or
compositional induction (49), (76) as in  HOARE [1969] . In this development, we have
already used a formalized language with only vestigial traces of English but without any
linguistic constrain (for example we felt free to use second order sentences [such as
(19.8.b)] and regretfully to misuse English).

* Then we have defined the language   (89) of Hoare logic for describing partial
correctness (or incorrectness) of programs. By choosing the particular language  ,
we deliberately limit our means of expression. Hence we have adopted the syntactical
point of view of computer scientists (corresponding to the use of specific formal
languages by logicians, where objects are thought of in terms of their denotation and
rewriting manipulations). In this framework, we can define veracity (and falsehood) of
partial correctness formulae by two disjoint sublanguages  tt ffi    (a n d

ff ffi  ). We have used two methods to specify the sublanguage  tt, one
defining "truth" and the other "provability":

* Following the semantic development of logic (also called "model theory",
BARWISE [1977] , part A ), we have defined the semantics of  , by means of an
interpretation I :   ff {tt,ff} (124) which depends upon the semantics  <D, V> of the
basis < ,  ,  , #> and induces a partition of the language   into true and false
sentences:

tt(I) = {H  \Omega    : I H  = tt} Hoare's valid correctness formulae (133)
ff(I) = {H  \Omega    : I H  = ff} Hoare's invalid correctness formulae (134)

(Instead of writing I H  = tt, logicians would use the satisfaction relation I |= H. The set

tt(I) = {H \Omega    : I |= H} of sentences of   true in I would be called the  theory of
I).

* Following the calculative development of logic (also called "proof theory",
BARWISE [1977], part D), we have presented Hoare deductive system   (93) with its axiom
schemata (96) to (98) and rules of inference (99) to (102) and defined provability |- Th '

H of formulae H \Omega    with respect to a set Th of postulates (95). Therefore we get the

subset of provable sentences (contrary to the ordinary situation in logic we do not define
refutable sentences since we are not interested in partial incorrectness ~H):

  pr(Th) = {H  \Omega    : |- Th '  H}. Hoare's provable (135)

correctness formulae

In order to compare syntactical objects Th,  pr(Th),  tt(I) with semantic
objects I,  tt(I), we can map Hoare correctness formulae H = {P}C{Q} into triples
o/(H) =  <I P , C,  I Q > of  (I) and assume that the interpretation  I is a model of
postulates Th, that is to say that every formula t of Th is true for interpretation  I:
\Delta  t \Omega  Th. I t  = tt. Then we get the following picture of inclusion between these sets
(up to the correspondence o/):

pr(Th)

tt(   )Iff(   )I

tt(   )Iff(   )I

The fact that  pr(Th)  ffi  tt(I) follows from the soundness theorem (129): every
provable formula is true. The inclusions  o/( tt(I)) ffi  (I)tt and o/( ff(I)) ffi  (I)ff
follow from the interpretation (123) of correctness formulae. Now, the incompleteness /
completeness question is whether these inclusions are strict or not :

- \Delta I.  (I)tt ffl o/( tt(I)): is every true fact about the partial correctness of
programs expressible in the restricted language  ?
- \Delta I.  tt(I) ffl  pr(Th): is every true formula of   provable by the deductive

system   ?

In both cases, and without additional hypotheses, the answer is no. Intuitively, the
origin of these incompleteness problems is that there exist programs C  \Omega  
constructing (input-output) relationships between objects of the domain D of data that,
under the assumption that the programming language   and first-order formulae 
must have the same signature  < ,  ,  , #>, cannot be described by a predicate
of  . To illustrate the phenomenon, we shall use "abacus arithmetic" a limited version

of usual arithmetic with 0, Su(x) = x + 1 and Pr(x) = x - 1 (with 0 - 1 = 0) as only nonlogical symbols.

7 . 4 . 2 . 2 Abacus arithmetic

The purpose of this section is to show that addition cannot be defined by some
formula of abacus arithmetic  A defined by (88) where the basis is A =  < ,  ,

, #> with  = {0},   = {Pr, Su},  = o/, #(Pr) = #(Su) = 1 for interpretation
IA defined by (123) where  D  =  , V 0 = 0, V Pr (x) = (x = 0  ff  0  fi x - 1) and

V Su (x) = x  + 1. It is also shown that the  theory of abacus arithmetic (i.e.

A,tt(IA) = {P  \Omega   A : I P  = tt}) is  decidable, that is there is an algorithm which
given P \Omega   A will always terminate and answer "tt" if P  \Omega   A,tt(IA) and answer "ff"

if this is not true. We will also show that abacus arithmetic has nonstandard
interpretations.

7 . 4 . 2 . 2 . 1 Inexpressibility of addition in abacus arithmetic
LEMMA  ENDERTON [1972]  Disjunctive normal form (135)
A quantifier-free predicate P and its negation ~P have equivalent  disjunctive normal
forms  P\Sigma \Pi  and P \Sigma \Pi  (i.e.  I P = I P\Sigma \Pi   and  I ~P = I P\Sigma \Pi   for all interpretations  I)
such that:

P\Sigma \Pi  = \Sigma  i = 1, m  \Pi  j = 1, n Pij P\Sigma \Pi  = \Sigma  k = 1, p \Pi  l = 1, q Pkl
where the Pij and Pkl are atomic formulae or negations of atomic formulae.

Proof

By induction on the syntactical structure of P:

* If P is an atomic formula A then P \Sigma  \Pi   =  \Sigma   i =1, 1  \Pi   j = 1, 1  A and P \Sigma  \Pi    =\Sigma 

 k   =   1 ,   1   \Pi   l  =  1, 1  ~ A .

* If P is ~Q then P \Sigma \Pi  = Q\Sigma \Pi  and P\Sigma \Pi  = Q\Sigma \Pi .

* If P is (Q  \Sigma  R) then P \Sigma \Pi  = (Q \Sigma \Pi  \Sigma  R\Sigma \Pi ) whereas P \Sigma \Pi  = ~(Q  \Sigma  R) = ~Q  \Pi  ~R =
(Q\Sigma \Pi  \Pi  R\Sigma \Pi ) = (( \Sigma  i = 1, m  \Pi j = 1, n  Qij) \Pi  (\Sigma  k = 1, p  \Pi  l = 1, q  Rkl)) = \Sigma  r = 1, n*m  Tr with
T i+m*(k-1) = ( \Pi   j = 1,  n Q ij \Pi  \Pi   l = 1, q  R kl ) for i = 1, ..., m; k = 1, ..., p; using a
generalized form of the distributive law ((P 1 \Sigma  P2) \Pi  (P 3 \Sigma  P4)) = (P 1 \Pi  P3) \Sigma 
(P1 \Pi  P4)  \Sigma  (P 2 \Pi  P3)  \Sigma  (P 2 \Pi  P4) .

* If P is (Q  \Pi  R) = ~(~Q  \Sigma  ~R) or P is (Q  \Upsilon   R) = (~Q  \Sigma  R) then we use the
previous transformations. \Delta 

LEMMA  ENDERTON [1972]  Quantifier elimination (136)

Any predicate P  \Omega   A is equivalent to a quantifier-free predicate P'  \Omega   A (i.e.
IA P = IA P' ), (with no occurrence of the Pr symbol or of terms with two
occurrences of the same variable).

Proof

Generalizing the simultaneous substitution P[x  \Xi  t] of term t for all occurrences
of variable x in predicate P, we can define on the model of (118) the substitution
P[F' \Xi   F] of F for all occurrences of F' in predicate P. We obtain P' from P by
repeated application of the following transformations (such that IA P  = IA P' ):

* We first eliminate the function symbols Pr from P by repeated transformation of P
into P[Pr(t)  \Xi  x]  \Pi  (((t = 0)  \Pi  (x = 0))  \Sigma  (~(t = 0)  \Pi  (Su(x) = t))), where x  \Omega   -
Var(P) is a fresh variable, as long as some term Pr(t) appears within P.

* Then we eliminate universal quantifiers \Delta  v. Q from P by repeated transformation
of P into P[ \Delta  v. Q  \Xi  ~(\Lambda  v. ~Q)].

* Finally we eliminate all subformulae  \Lambda  x. Q from P, starting from the innermost
ones (so that Q can be assumed to be quantifier-free), by repeated application of
transformations, in the following order:
- Q is replaced by its disjunctive normal form  \Lambda  x. Q = \Lambda  x. Q\Sigma \Pi  = \Lambda  x. \Sigma  i = 1, m \Pi  j

= 1, n  Qij = \Sigma  i = 1, m (\Lambda  x. \Pi  j = 1, n  Qij) so that in the following we can assume that Q
is of the form  \Pi  j = 1, n Qij with all Q ij being an atomic formula A or the negation ~A

of an atomic formula A.
- \Lambda  x. \Pi  i = 1, n  Qi where x does not appear in Q k, 1  <= k  <= n is replaced by Q k \Pi 
(\Lambda  x. \Pi  i = 1, k-1 Qi \Pi  \Pi  j = k+1, n  Qj). Afterwards, using the commutativity of =, all Q i
can be assumed to be of the form A or ~A where A is (Su p(x) = Suq(t)) and t is 0, x or
another variable y ` x, with Su0(t) = t and Sup+1(t) = Sup(Su(t)).
- All (Sup(x) = Suq(x)) are replaced by (0 = 0) if p = q and by ~(0 = 0) when p  `
q whence no term has two occurrences of the same variable so that in the following we
can assume that t is 0 or y`x:
. If all Q i are of the form ~(Su p(x) = Suq(t)) then  \Lambda  x.\Pi  i = 1, n  Qi assert than we
can find a value of x different from a finite number of given values, so that \Lambda  x. \Pi  i = 1, n
Qi is replaced by (0 = 0) representing truth.
. Else some Q k is of the form (Su p(x) = t') where t' is a term of the form Su r(t)
with t = 0 or t = y  ` x so that intuitively x can be chosen as (t' - p)  >= 0. Therefore Q k
is replaced by (0 = 0) if p = 0 else by (~(t' = 0)  \Pi  ~(Su(t') = 0)  \Pi  ...  \Pi  ~(Sup-1(t') =
0)) expressing that x >= 0 whereas all terms Qi, i = 1, ..., n, i ` k, which are of the form
~(Suq(x) = ti) or (Su q(x) = ti), are respectively replaced by ~(Su q(t') = Su q(ti)) or
(Suq(t') = Suq(ti)) since intuitively x = (t' - p) >= 0 and q + x = ti is equivalent to x = (t' -
p) >= 0 and q + t ' = ti + p. Since formula Q no longer contains variable x,  \Lambda  x. Q can be
replaced by Q.  \Delta 

LEMMA  ENDERTON [1972]  Definability in abacus arithmetic  (137)
A subset E  ffl   is definable by P  \Omega   A (i.e.  \Lambda  v \Omega   . E = {s(v)  \Omega    : s \Omega  I? P }) if

and only if it is finite or cofinite (i.e.   - E is finite).

Proof

Let P' be the quantifier-free predicate equivalent to P (136) in disjunctive normal
form (135). If v  fl Var(P') then  \Delta  d  \Omega   . (s  \Omega  IA P' \Phi   s[v \Xi   d]  \Omega  IA P' ) whence
E = {s(v) \Omega    : s \Omega  I? P' } =   is cofinite so that in the following we can assume that
v occurs free in P'. Then we proceed by induction on the syntactical structure of P'. If
P' is an atomic formula A ij, it is of the form (Su p(v) = Suq(t)) where t is 0 or x  ` v

whence if t is 0 then E = o/ (when q < p) or E = {q - p} (when q  >= p) is finite else t is x
` v and E = {v  \Omega    : \Lambda  x  >= 0. p + v = q + x} = (p  >= q ff   fi {v  \Omega    : v  >= (q - p)})
is cofinite. If P' is a negated atomic formula Aij then E is the complement of a finite or
cofinite set hence is cofinite or finite. If P' is a conjunction  \Pi  j = 1, n Aij of atomic
formulae Aij or negations Aij of atomic formulae then E is the intersection of finite or
cofinite sets. If all are cofinite then it is cofinite else it is finite. Finally if P' is a
disjunction  \Sigma  i = 1, m \Pi  j = 1, n Aij then E is the union of finite or cofinite sets hence

is finite or cofinite. \Delta 

LEMMA  ENDERTON [1972]  Inexpressibility of addition in abacus arithmetic (138)
There is no formula P \Omega   A equivalent to A + B = C (more precisely such that IA P  =

{s \Omega  S : (s(A) + s(B)) = s(C)}).

Proof

Otherwise the set of even naturals would be definable by  \Lambda  k. (k + k = v) (more
precisely  \Lambda  k.  P[A \Xi  k][B \Xi  k][C \Xi  v]), in contradiction with (137).  \Delta 

Let  A be the language (1) with the abacus arithmetic A as basis. Every partial
recursive function (such as addition of natural numbers) can be computed by some
command C  \Omega   A (LAMBEK [1961];  BOOLOS & JEFFREY [1974], $ 7 ). However (138)
shows that addition is not expressible in  A. It follows that  A has a greater
expressive power than  A. This is the source of incompleteness problems with Hoare
logic.

7 . 4 . 2 . 2 . 2 Decidability of abacus arithmetic
LEMMA  ENDERTON [1972]   Decidability of abacus arithmetic (139)

The theory  A,tt(IA) = {P \Omega   A : I P  = tt} of abacus arithmetic is decidable.

Proof

The algorithm used in the proof of (136) transforms a predicate P  \Omega   A into an
equivalent quantifier free formula P' with variables v1,...,vn. Using the same algorithm
we can transform ~ \Lambda  v1. ...  \Lambda   vn. ~P' into a quantifier free formula P" with no
variables and atoms of the form (Sum(0) = Sun(0)) which are replaced by tt if m=n and
ff otherwise. Then the answer is obtained using truth tables (tt  \Pi  tt = tt, tt  \Pi  ff = ff,
...). \Delta 

The decidability of  A,tt(IA) shows that the fact that true correctness formulae in

tt(I) are not provable by   does not necessarily come from unprovability problems
in  A that would be inherited in Hoare logic through the consequence rule.

7 . 4 . 2 . 2 . 3 Nonstandard interpretations of abacus arithmetic
LEMMA  BERGSTRA & KLOP [1984]   Nonstandard interpretations of abacus arithmetic (140)
The nonstandard interpretation  I'A defined by  D' = ({0}  x  ) ' ( + x  ), V' 0  = <0,
0>, V' Pr (<i, x >) = (i = 0  ff  (x = 0  ff  <0, 0 > fi <0, x - 1 >)  fi  <i, x - 1>) and
V' Su (x) = (x = 0  ff <0, x - 1> fi <i, x - 1>) has the same theory than the standard
interpretation IA i.e. is such that  A,tt(I'A) =  A,tt(IA).

This is an extension of the standard naturals:  <0, 0> ? 0 < <0, 1> ? Su(0) < <0, 2> ? Su2(0)
< ... < ... <  <i, -2>  < <i, -1>  < <i, 0>  < <i, 1>  < <i, 2>  < ... < ... <  <j, -1>  < <j, 0>  < <j, 1>  <
... by strictly greater nonstandard infinite numbers, organized by groups isomorphic to
integers (however  I'A  is not a model of Peano arithmetic with addition and

multiplication, because there are too few nonstandard numbers, see  BOOLOS & JEFFREY
[1974], $ 17) .

Proof of lemma  ( 1 4 0 )

If P'  \Omega    A  and s  \Omega   IA P'  then obviously s' =  oe  v.  < 0, s(v) >  \Omega   I'A P' .
Reciprocally, if s'  \Omega  I'A P'  then we define s  \Omega  IA P  where P is equivalent to P',
quantifier free and in disjunctive normal form  \Sigma  i = 1, m \Pi  j = 1, n Pij, with the P ij being
atoms Q of the form (Su m(0) = Sun(0)), (Su m(x) = Sun(0)), (Su m(x) = Sun(y)) with
x ` y or their negation ~Q [(136), (137)]. One of the\Pi 

 j = 1, n Pij hence all Pij must be true in s' and we define s so that all P ij are also true in
s. If x \Omega  VP = ' j = 1, n Var(Pij) and s'(x) =  <k, p> then s(x) = (k = 0  ff p fi (k * OE) + p
- u) else s(x) = 0 where  OE is a natural greater than [max({p : x  \Omega  VP \Pi  s'(x) = <k, p>}) -
u] + ( + 1, u = min({0}  ' {p : x \Omega  VP \Pi  s'(x) = <k, p>}) and ( \Omega   + is greater than all n
such that the term Su n(0) appears in P. Hence the idea is to map finitely many finite
segments of groups of nonstandard numbers into disjoint segments of   in the same
order beyond the finite initial segment used to do the standard computations. Then, the
truth of (Su m(0) = Sun(0)) or its negation does not depend upon the states s' and s. If
(Sum(x) = Sun(0)) is true in s' then s'(x) =  <0, n - m > and n - m  >= 0 so that it is also

true for s(x) = n - m. If ~(Su m(x) = Sun(0)) is true in s' then s'(x) =  <k, p > with
either k = 0 and m+p ` n in which case it is also true for s(x) = p or else k > 0 in which
case m + s(x) = m + (k *  OE) + p -  u > ( > n. If (Su m(x) = Sun(y)) is true in s' then
s'(x) = <k, p> and s'(y) =  <k, q> with m + p = n + q. Therefore it is also true of s(x) =
(k *  OE) + p -  u and s(y) = (k *  OE) + q -  u. If ~(Su m(x) = Sun(y)) is true in s' then
s'(x) = <k, p> and s'(y) =  <l, q> with k = l and m + p  ` n + q in which case m + s(x) =
m + (k * OE) + p - u ` n + (k * OE) + q - u = n + s(y) or else k  ` l in which case m + s(x)
= n + s(y) would imply  OE <= (k - l) *  OE = m - n + (q -  u) - (p -  u) <= ( + (q -  u) < OE. \Delta 

No predicate of the standard theory  A,tt(IA) can distinguish nonstandard numbers
from the standard naturals. Whence partial correctness is unchanged when considering
nonstandard interpretations. For example {0 = 0} C {X = 0} with C = (~(X = 0) *
X := Pr(X)) is true in the above nonstandard interpretation since C is either started
with some value <0, p> of X and terminates with the value  <0, 0> or else X is initially <i,
p> and takes successive values  <i, p> , <i, p - 1>, <i,  p - 2>, ... so that execution of C
never terminates (HITCHCOCK & PARK [1973]).

7 . 4 . 2 . 3 Incompleteness results for Hoare logic

We are now in a position to explain the incompleteness results for Hoare logic:
(a) the partial correctness of a program C may not be expressible by a formula
{P}C{Q} and, for the same reason that in certain cases first-order languages may be too
weak assertion languages, (b) some true formula {P}C{Q} may not be provable by the
deductive system   ' Th where Th is the set of all theorems P true in a given
interpretation and (c) Th may not be axiomatizable by a deduction system   so that
incompleteness problems for the first-order language   show in Hoare logic through
the consequence rule.

7 . 4 . 2 . 3 . 1 Unspecifiable partially correct programs

A program based upon abacus arithmetic  A which computes addition by
successive increments is not provable in   since, by (138), addition is not expressible
in  A. It follows that Hoare logic is incomplete since partial correctness of this

program is not expressible in  :

THEOREM  BERGSTRA & TUCKER [1982a]  Unspecifiable partially correct programs (141)

\Lambda  I. o/( tt(I)) `  (I)tt

Proof

Choosing the basis A =  <{0}, {Su, Pr}, o/, #> of abacus arithmetic  IA, p = {s \Omega  S
: s(X) = s(x)  \Pi  s(Y) = s(y)}, C = (~(X = 0) * (Y := Su(Y); X := Pr(X))) , q = {s  \Omega  S :
s(Y) = s(y) + s(x)} we have  {p}C{q} hence <p, C, q> \Omega   (I)tt whereas by (138) there
is no Q \Omega   ? such that IA Q  = q whence no {P}C{Q}  \Omega    such that o/({P}C{Q}) =
<p, C, q >. \Delta 

The proof shows that "{X = x  \Pi  Y = y} (~(X = 0) * (Y := Su(Y); X := Pr(X))) {Y = y
+ x}" is not expressible in abacus arithmetic  A because addition cannot be

represented in this logic (138). Hence we can enrich the basis A with the addition
symbol + to get Presburger arithmetic PB =  <{0}, {Su, Pr, +}, o/, # >. But then "{X =
x \Pi  Y = y} (Z := 0; (~(X = 0) * (Z := Z + Y; X := Pr(X)))  {Z = x * y}" is not
expressible because multiplication cannot be represented in this logic (ENDERTON [1972],
Co.32G). Presburger arithmetic has no sound and complete Hoare logic for its while
programs ( COOK [1978] , BERGSTRA & TUCKER [1982a] ) although one can be found when
restricting the class of considered programs as in CHERNIAVSKY & KAMIN [1977]. Hence we
must enrich the basis PB with the multiplication symbol * to get Peano arithmetic PE
<{0}, {Su, Pr, +, *}, o/, # > (but then by Go"del second incompleteness theorem, Peano
arithmetic IPE is no longer first-order axiomatizable). This situation was first described

by  HAREL [1979] , MEYER & HALPERN [1980] [1981] , BERGSTRA, TIURYN & TUCKER [1982],
MEYER [1986]: the partial correctness of a program C on basis \Sigma  can always be expressed
by a Hoare correctness formula {P}C{Q}with predicates P, Q  fl  \Sigma  using extra

symbols of Peano arithmetic PA = (0, Su, +, *, <) which may not appear in the
programs and whose semantics may not be expressed in the language  \Sigma . This is

because the functions and relations computed by the program are recursive, hence can
be coded in arithmetic ( JOHNSTONE [1987] , $ 4). But then incompleteness problems in

PA appear through the consequence rule.

A similar incompleteness argument can be given using the fact that the transitive

closure of a first-order expressible binary relation is computable by a while-program but
may not be first-order expressible ( GUREVICH [1984] , GAIFMAN & VARDI [1985] ). Then
transitive closures ( IMMERMAN [1983] ) or least fixpoints ( AHO &  ULLMAN [1979]) can be
turned into logical operators. Such extented first-order logics are studied in  GUREVICH
[1985] [1988] .

7 . 4 . 2 . 3 . 2 Unprovable partially correct programs

It may also happen that partial correctness can be specified by a Hoare formula
{P}C{Q}, but may not be provable using Hoare's deductive system   [(96) to (102)],
even with the help of all true postulates Th = {P  \Omega    : I P  = tt} in the intended
interpretation I. As independently shown by  GERGELY & SZO"TS [1978]  and WAND [1978] it
might be the case that given P, Q  \Omega    the proof of  { P }(B * C){ Q } by (77) would

involve an intermediate assertion i which cannot be expressed by a first order predicate:
~(\Lambda  P \Omega   . I P = i). In this case, the proof using (77) cannot be carried out into 
because intermediate (and in particular loop) invariants are not first-order definable.
Otherwise stated, intermediate states in the computation of a program (e.g. with
recursive procedures) are often much more complicated than the initial and final states
involved in paragraph $ 7.4.2.3.1. Hence the language of first order logic   may be
too weak to describe precisely enough the set of intermediate states that is computed by
a program.

7 . 4 . 2 . 3 . 2 . 1 Incompleteness of Hoare logic for an interpretation

with decidable first-order theory

A program based upon abacus arithmetic  A which involves a loop invariant I
expressing addition is not provable in   since, by (138), the invariant I is not
expressible in  A (BERGSTRA & KLOP [1984]). It follows that Hoare logic is incomplete:

THEOREM  GERGELY & SZO"TS [1978] , WAND [1978]  Local incompleteness of Hoare 

logic (142)
\Lambda  I.  tt(I) `  pr(Th)

Proof  (BERGSTRA & KLOP [1984] )

Let us choose abacus arithmetic  A and define P = ((X = x)  \Pi  (Y = 0)), C =

(~(X = 0) * C'), C' =  (X := Pr(X); Y := Su(Y)) and Q = (Y= x).

The proof of {P}C{Q} in   must involve the while inference rule (101) with
some I such that {I  ^ ~(X = 0)}C'{I} and the consequence rule (102) so that (P  \Upsilon  I)
and ((I \Pi  (X = 0)) \Upsilon  Q). Then by the soundness theorem (129), we have  { I ^ ~(X=0)
}C'{ I }, whence { <s, s[X  \Xi   s(X) - 0 1] [Y  \Xi   s(Y) +  1] : s  \Omega   I \Pi  (s(X)  ` 0)  }  ffl

{<s', s">: s" \Omega  I} by (19.2), (19.4) and (22) so that by (123) we must have:

(a) {s \Omega  S : (s(X) = s(x)) \Pi  (s(Y) = 0)} ffl I,
(b) {s[X \Xi  s(X) -  1][Y \Xi  s(Y) +  1] : s  \Omega  I \Pi  (s(X)  ` 0)}  ffl I,
(c) {s \Omega  I : s(X) = 0} ffl {s \Omega  S : s(Y) = s(x)}.
Let us show that i = {s \Omega  S : s(x) = s(Y) + s(X)} is the unique solution  I of (a), (b), (c).
If s[X  \Xi  0][Y \Xi  s(X) + s(Y)]  \Omega  I then 0  <= s(X) + s(Y) and by (c) we have s(X) +
s(Y) = s(x) whence s  \Omega  i. By (b), we have s[X  \Xi  n + 1][Y  \Xi  s(X) + s(Y) - (n + 1)]  \Omega 
I \Pi  n + 1  <= s(X) + s(Y) which implies s[X  \Xi  n][Y \Xi  s(X) + s(Y) - n]  \Omega  I \Pi  n <= s(X) +
s(Y). It follows, by induction, that m  >= n  >= 0 implies (s[X  \Xi  m][Y \Xi  s(X) + s(Y) -
m] \Omega  I \Pi  m <= s(X) + s(Y))  \Upsilon  (s[X \Xi  n][Y \Xi  s(X) + s(Y) - n]  \Omega  I \Pi  n <= s(X) + s(Y))  \Upsilon 
(s \Omega  i). When m = s(X) + s(Y)  >= n = s(X)  >= 0, we get (s[X  \Xi  s(X) + s(Y)][Y  \Xi  0] \Omega 
I) \Upsilon  (s \Omega  I) \Upsilon  (s \Omega  i). Moreover, if s  \Omega  i then s' = s[X  \Xi  s(X) + s(Y)][Y  \Xi  0] is such
that s'(X) = s(Y) + s(X) = s(x) = s'(x) and s'(Y) = 0 so that by (a) we have s  \Omega  I
whence (s \Omega  i) \Upsilon  (s \Omega  I) \Upsilon  (s \Omega  i). In conclusion i = I.

Now, by (138), an invariant I expressing that x = (Y + X) is not expressible in
A . \Delta 

The same way, ABRAMOV [1981] proves that an invariant I expressing that A + B > C is
needed to prove {A > C} (A > 0 * (A := Pr(A); B := Su(B))) {B > C}  whereas
ABRAMOV [1984] shows that I is not expressible in the first-order logic with basis  <{0},
{Su, Pr}, {>}, # >.

7 . 4 . 2 . 3 . 2 . 2 Incompleteness of Hoare logic for an interpretation

with undecidable first-order theory

As above ($ 7.4.2.3.1) we could enrich the predicate language   with Peano
arithmetic which would allow more expressive predicates. However, as shown by COOK
[1978], $ 6, p. 85, this would not solve the completeness problem: if the halting problem
is undecidable for interpretation  I of   then the set of valid Hoare formulae of the
form {true}C{false}, C \Omega    is not recursively enumerable hence cannot be included
in the recursively enumerable set of provable Hoare formulae in any formal deductive
system Th  '  . The rest of this paragraph is devoted to a detailed explanation of the
argument. (Another proof of the local incompleteness of Hoare logic (142) not referring
to an interpretation  I whose first-order theory Th is decidable is given by  LEIVANT &
FERNANDO [1987], Theorem 1).

7 . 4 . 2 . 3 . 2 . 2 . 1 The set of provable Hoare formulae is recursively

enumerable

A set E is recursive if there is a terminating algorithm to check that x \Omega  E. A set E
is recursively enumerable if there is an algorithm that list elements of E in some order
(of course since E may be infinite, the list may never be completed but any particular
element of E will appear in the list after some finite length of time).

LEMMA  Recursive enumerability of provable Hoare formulae (143)

 is recursively enumerable. If Th is recursive then  pr(Th) is recursively

enumerable.

Proof

Computer scientists know that symbols, finite lists of symbols, finite lists of lists
of symbols, etc... can be coded in a machine into an integer in binary representation and
that from this representation it is possible to recover the original object.  Go"del
numbering is a similar idea. An odd code  _'i* is associated with the n basic logical or

programming symbols  'i :  _=* = 3,  _\Upsilon * = 5,  _\Pi * = 7,  _\Sigma * = 9,  _~* = 10,  _\Delta * = 13,
_\Lambda * = 15,  _skip* = 17,  _:=* = 19,  _fi* = 21,  ...,  _'n* = 2 n + 1 and with the constant
symbols  _ci* = 2 (n + 1) + 10 i + 1 where   ffl {c i : i \Omega   }, function symbols  _fi*
= 2 (n+1) + 10 i + 3 where   ffl {fi : i \Omega   }, relation symbols  _ri* = 2(n+1)  +  10 i
+ 5 where   ffl {ri : i \Omega   }, programming variables  _Xi*  = 2 (n+1) + 10 i + 7 where

 ffl {Xi : i \Omega   } and logical variables  _xi* = 2 (n+1) + 10 i + 9 where   ffl {xi : i
\Omega   } are assumed to be enumerable. Then a command or a predicate that is a finite
string  '1,...,'n of symbols will be coded as  _'1,...,'n* = 2 _'1* 3 _'2* 5 _'3* ... pn _'n*
where pn is the nth prime number. Then a Hoare correctness formula {P}C{Q} will be
coded as  _{P}C{Q}* = 2  _P* 3 _C* 5 _Q*. Then a proof in   that is a finite string
F1,...,Fm of predicates or Hoare correctness formulae will be coded as  _F1,...,Fm* = 2
_F1* 3 _F2* ... pm _Fm* where pm is the mth prime number.

Now given an integer n it is possible to decode it. If n is odd then n is the code  _'* of a
symbol  ' =  _n*-1. Else it is even and can be decomposed into its prime factors n
=2n1 3n2 ... pmnm. The decomposition is unique. If each n i is odd then n is the code
of a finite string of symbols _n*-1 = _n1*-1 _n2*-1 ... _nm*-1. A syntactical recognizer will
tell if the string is a syntactically correct command or predicate. Else each n i can be
decomposed into its prime factors. If m = 3, _n1*-1 is a predicate P, _n2*-1 is a command
C and _n3*-1 is a predicate Q then n is the code of Hoare formula {P}C{Q}. Else it can
be checked if each n i is the code of a predicate or a Hoare formula F i = _ni*-1 so that n
is the code of a proof _n*-1 = _n1*-1 _n2*-1 ... _nm*-1. Else n is not the Go"del number of a
proof, Hoare formula, predicate, command or symbol. Observe that the numbering is
injective: different objects have different Go"del numbers, coding and decoding is
recursive that is can be done by a terminating algorithm as informally described above
and the set of codes is recursive that is given any natural number n the algorithm
described above always terminates with the object _n*-1 coded by n or else answer that n
is not a Go"del number.

To do the recursive enumeration of  , we just have to enumerate the natural
numbers, for each one we check if it is the Go"del number of a Hoare formula and then
output the corresponding formula. Since all Hoare correctness formulae H have a code
_H* \Omega    and no two different formulae can have the same code, no formula H can be
omitted in the enumeration.

To do the recursive enumeration of  pr(Th), we just have to enumerate the
natural numbers, for each one we check that it is the Go"del number of a proof F 1 ... Fn
and then test the validity of the proof using a recognizer to check that Fi is an instance of
an axiom scheme or combinatorially check that Fi follows from previous Fj by a rule of
inference of   or we algorithmically check that Fi \Omega  Th (the algorithm exists since Th is
assumed to be recursive). If the proof is correct we output the formula Fn. \Delta 

7 . 4 . 2 . 3 . 2 . 2 . 2 The non-halting problem is not semi-decidable for

Peano arithmetic

 A problem P depending upon data d  \Omega  D with a logical answer "yes" or "no" is
decidable (or solvable) (written  Decidable(P)) if and only if there exists an algorithm
(Decision(P) : D ff {tt, ff}) which when given the data d always terminates with output
"tt" or "ff" corresponding to the respective answer "yes" or "no" to the problem. A
problem is undecidable (or unsolvable) when no such algorithm exists.
A problem P depending upon data d  \Omega  D with a logical answer "yes" or "no" is  semidecidable (written  Semi-decidable(P)) if and only if there exists an algorithm ( SemiDecision(P) : D ff {tt, ff}) which when given the data d always delivers an answer "tt"
in a finite amount of time when this the answer to the problem is "yes" but may answer
"ff" or may be blocked or else may not terminate when the problem for d has answer
"no".
The halting problem is the problem of deciding whether execution of a command C  \Omega 

 started in a given initial state s \Omega  S terminates or not (for the interpretation I where
the basis <{0, 1}, {+, *}, o/, #> has its natural arithmetical interpretation on  ).

LEMMA  CHURCH [1936], TURING [1936] [1937]   Undecidability of halting problem (144)
The halting problem is semi-decidable but undecidable. The non-halting problem is not
semi-decidable.

Sketch of proof

* Following  HOARE & ALLISON [1972] , we now briefly sketch a coarse proof for a
subset of Pascal. A Pascal program is a finite sequence of characters. It can be
represented in Pascal as a text file of arbitrary length. Obviously, we can write a Pascal
function  I of type " function I(var F, D : text) : Boolean" such that if F is the text
of a Pascal function of type " function F(var D : text) : Boolean"  and D is the text of
the data of F then I(F, D) is the result F(D) of executing function  F with data D.  I is
simply a Pascal interpreter written in Pascal but specialized in execution of Boolean
functions F with text parameter D.

* The semi-decision algorithm for the halting problem simply consists in executing
F with data D using interpreter I and answering "tt" upon termination:

function SemiDecisionOfHaltingProblem(var F, D : text);

var R : Boolean;
begin R := I(F, D); write('tt'); SemiDecisionOfHaltingProblem := True; end;

* To show that the halting problem is undecidable, we prove by reductio ad
absurdum that we cannot write a termination prover in Pascal that is a function of type
"function T(var F, D : text) : Boolean" such that for all texts F of Pascal functions

F of type "function F(var D : text) : Boolean" and all data D of type "text", execution
of T with data F and D would always terminate and yield a result  T(F, D) which is
"True" if and only if execution of  I(F, D) i.e. of  F with data D does terminate.
Assuming the existence of such a  T, we let TC be the text : " function C(var F : text)
: Boolean;  begin if T(F, F)  then C :=  not I(F, F)  else  C := True  end;". Observe
that T(F, F) terminates and either  T(F, F) = True and "C :=  not I(F, F)"  terminates
or T(F, F) = False and "C := True" terminates so that  T(TC, TC) is "True". Then
I(TC, TC)  =  if T (TC, TC) then not I(TC, TC) else True  =  not I(TC, TC),  a
contradiction. In conclusion there is no algorithm by means of which we can test an
arbitrary program to determine whether or not it always terminates for given data.

* The argument can be rephrased for   using a coding of text files into natural
numbers (or for Turing machines, see rigorous details in  BOOLOS & JEFFREY [1974], $ 3 &
4; DAVIS [1977]; ENDERTON [1972], $ 3.5; KLEENE [1967], $ 43 or ROGERS [1967], $ 1.9).

* The negation ~P of a problem P depending upon data d \Omega  D with a logical answer
"yes" or "no" is the problem of answering the opposite of P : ~P(d) = (P(d) = "yes"  ff
"no" fi "yes"). We have  Decidable(P) \Phi  [Semi-decidable(P) \Pi  Semi-decidable(~P)].  \Upsilon  is
obvious. For  , define  Decision(P)(d) by executing alternatively one step of  SemiDecision(P)(d) and one step of ~Semi-Decision(~P)(d) as long as both are not terminated
and by terminating Decision(P)(d) as soon as one of these fairly interleaved executions
of Semi-Decision(P)(d) and ~ Semi-Decision(~P)(d) is terminated.

*  If the non-halting problem were semi-decidable then [ Semi-decidable(halting)  \Pi 
Semi-decidable(~halting)] would imply  Decidable(halting), in contradiction with the
undecidability of the halting problem. \Delta 

7 . 4 . 2 . 3 . 2 . 2 . 3 The set of valid Hoare formulae for Peano arithmetic

is not recursively enumerable

LEMMA   Valid Hoare formulae for Peano arithmetic (145)
The set  tt(I) is not recursively enumerable for Peano arithmetic (i.e.the interpretation

I where the basis  <{0, 1}, {+, *}, o/, # > has its natural arithmetical interpretation on

).

Proof

Assume the contrary. Let s \Omega  S be a state, Var(C) = X1, ..., Xn be the variables of
C with initial values x 1 = s(X1), ..., xn = s(Xn) \Omega    and P = (X 1 = Sux1(0)) \Pi  ... \Pi  (Xn
= Su xn(0)) where Su 0(0) = 0 and Su n+1(0) = (Su n(0) + 1). Execution of C in state s
never terminates if and only if  I {P}C{false}  = tt (where false is (0 = 1)). Hence
execution of C in state s never terminate if and only if the formula {P}C{false} is to be
found in the recursive enumeration of  tt(I). It would follow that the non-halting

problem would be semi decidable, in contradiction with (144). \Delta 

7 . 4 . 2 . 3 . 2 . 2 . 4 Incompleteness of Hoare logic for Peano arithmetic
THEOREM COOK [1978], APT [1981a]  Incompleteness of Hoare logic for  (146)

Peano arithmetic
 \Lambda  I.  tt(I) `  pr(Th)   (where Th is recursive and  I is Peano arithmetic).

Proof

pr(Th) is recursively enumerable by (143) but  tt(I) is not by (145) hence
these sets are different. \Delta 

We say that the interpretation domain D is Herbrand definable when all elements of D
can be represented by a term:

DEFINITION  Herbrand definability (147)

D is Herbrand definable if and only if (\Delta  d \Omega  D. \Lambda  T \Omega   . I T  = d)

Theorem (146) is also a consequence of (145) combined with the following
observation:

THEOREM  BERGSTRA & TUCKER [1982a], BERGSTRA & TIURYN [1983] (148)
if Th is recursive, |  D | = |  |, D is Herbrand definable and  tt(I) =  pr(Th) then

tt(I) is recursive.

Proof

If C  \Omega    then for all n  \Omega    there is C n \Omega    -   running at most n
(assignments or test) steps of C. Indeed, since |  D | = |  |,  C  is finite and  D is
Herbrand definable there is a bijection  # between  C  and some finite subset
#( C ) = {L 0, ..., L k} of   with L 0 =  #( % ) and L 1 =  #(C). Let Xc  \Omega    -
Var(C) be a fresh variable used as program counter. C n is ( ... ((Xc := L1 ; I1) ; I2) ;
... In) where each I i = (~(Xc = L 0) ff (S1 ; (S2; ... (Sk-1 ; Sk)...)) fi skip) executes one
step of C (unless C is terminated). Each Sj executes the elementary step of C labeled Lj
and updates the program counter Xc therefore S j is (Xc = L j ff  ( C [Lj] ;
Xc := #( C [Lj]) fi skip) when  C [L] is skip, X := E or X := ? and S j is (Xc
= L j ff  (B ff  Xc :=  #( C [Lj](tt)) fi Xc :=  #( C [Lj](ff)))  fi skip) when

C [Lj] is B.

Moreover there is a formula Rn of   such that {P}Cn{Q} holds if and only if Rn
is true. Rn is (P \Upsilon  wlp(Cn, Q)) where by induction on the syntax of C n \Omega    -  ,
wlp(skip, Q) = Q,  wlp(X := E, Q) = Q[X  \Xi  E], wlp(X := ?, Q) =  \Delta X.Q, wlp((B ff C1

fi  C2), Q) = ((B  \Pi  wlp(C1, Q))  \Sigma  (~B  \Pi  wlp(C2, Q))),  wlp((C1; C 2), Q) =  wlp(C1,
wlp(C2, Q)). wlp(C, Q) is explained in more details below, see (151).

If  tt(I) =  pr(Th) and Th is recursive then  tt(I) is recursively enumerable
by (143). Moreover  ff(I) is also recursively enumerable because {P}C{Q}
\Omega   ff(I) if and only if R n fl Th so that  ff(I) can be recursively enumerated by
generating all formulae {P}C{Q} according to the syntax (89) and recursively testing
for each of them that R n fl Th. We conclude that  tt(I) is recursive by running
alternatively one step of the algorithms to recursively enumerate  tt(I) and  ff(I). \Delta 

BERGSTRA, CHMIELINSKA & TIURYN [1982a]  have shown that the converse of (148) is not
true : we may have  tt(I) `  pr(Th) with  tt(I) recursive.

7 . 4 . 2 . 3 . 3 Unprovable valid predicates, mechanical proofs

Following COOK [1978], definition (95) of provability |-  Th '   H includes the use
of a given set Th of postulates which are indispensable in the consequence rule (102).
Hence Hoare's system   can be thought of as being equipped with an infallible oracle
answering questions on the validity of first-order predicates. In this way, the reasoning
about the programs is separated from the reasoning about the underlying language 
of invariants.

However FLOYD [1967a] propounded the definition of this set Th of postulates as
the set of provable theorems in a formal deductive system. This approach was further
advocated by  HOARE [1969] (with the additional idea that "a programming language
standard should consist of a set of axioms of universal applicability, together with a
choice from a set of supplementary axioms describing the range of choices facing an
implementor"). More precisely, we should define Th = {P  \Omega    : |- A '  P} by

means of a set of non-logical axioms A and a deductive system  . The set A of nonlogical axioms should be recursive (that is P \Omega  A should be decidable by a machine) and
consistent (A should have at least one  model i.e. an interpretation  I such that \Delta  t \Omega  A.
I t = tt). The existence of this deductive system A  '   depends upon the class of
interpretations I which is considered.

More precisely, by Go"del's 1930 completeness theorem (see  BARWISE [1977],

$ A.1.4; BOOLOS & JEFFREY [1974], $ 12;  ENDERTON [1972], $ 2.5;  JOHNSTONE [1987], $ 3  or
KLEENE [1967], $ 52 ), there is a deductive system   such that all (and only) formulae
which holds for all interpretations I which are models of some given set of axioms A ffi

 are provable in A '  :

{P \Omega    : |- A '  P} = {P  \Omega    :\Delta I. (\Delta  t \Omega  A. I t = tt)  \Upsilon  I P  = tt}. (149)

However when considering  some given intended interpretation  I (for example Peano
arithmetic) or a restricted non-empty class  ( of such interpretations  I satisfying all
axioms of A, it may happen, by Go"del's 1931 second incompleteness theorem (see
SMORYNSKI [1977], $ A.1.4;  BOOLOS & JEFFREY [1974], $ 16; ENDERTON [1972], $ 3.5;
JOHNSTONE [1987], $ 9  or KLEENE [1967], $ 44 ) that some P 1 \Omega    is true for  I (or all I in ()

but is neither provable nor refutable in A  '  . By Go"del's 1930 completeness theorem
(149), this simply means that P 1 may be true for some interpretations  I' and false for
other interpretations I". Hence we should add P 1 or ~P1 to A in order to eliminate the
unintended interpretations  I' or I". But then there is some P 2 \Omega    which is true for  I
(or all  I in () but is neither provable nor refutable in A  ' {P1} '  , and so on. This
means that unintended interpretations cannot be eliminated when using only first-order
concepts (ANDRE'KA, NE'METI & SAIN [1979], BERGSTRA & TUCKER [1982a], CARTWRIGHT [1983],
NE'METI [1980]).

Since program hand-proving is tedious, long and sometimes difficult, the
1970s have lived in hopes of mechanical verification of program correctness ( KING
[1969], IGARASHI, LONDON & LUCKHAM [1975], BOYER & MOORE [1979] ). This approach has
theoretical limits determined by uncomputability problems: no computer (even ideal
ones without size and time limits) can be used to  automatically prove program partial
correctness. This follows from Go"del's 1931 second incompleteness theorem which
implies that the theory {P  \Omega    : \Delta  I \Omega   (. I P  = tt} of a class  ( of interpretations
satisfying the axioms A (i.e. \Delta  I \Omega  (. (\Delta  t \Omega  A. I t = tt)) and including Peano arithmetic
is not recursive (because otherwise proofs would simply consists in using the
terminating algorithm to check that  \Delta I \Omega  (. I P  = tt). Hence the discovery of the
invariant needed in the while rule (101) can be partly automated ( WEGBREIT [1974] ,
GERMAN & WEGBREIT [1975] , KATZ & MANNA [1976]) but ultimately requires human
interventions. The same way, the use of the consequence rule (102) may also call for
such error-prone human interactions. But then, lonesome individuals have to carefully
manage large amounts of detailed information produced by machines. This has practical
limits discussed in  DE MILLO, LIPTON & PERLIS [1979] . Experience with a proof editor for
interactive proof checking is discussed in  REPS & ALPERN [1984]  and that with more
ambitious verification environments in  BOYER & MOORE [1988] , CONSTABLE, JOHNSON &
EICHENLAUB [1982] and GOOD [1984].

7 . 4 . 2 . 4 Cook's relative completeness of Hoare logic

COOK [1978] circumscribed these incompleteness problems by assuming that the
set Th of mathematical theorems P \Upsilon  P' which have to be used in the consequence rule
(102) is given. This corresponds to the common mathematical practice to accept certain
notions and structures as basic and work axiomatically from there on, even if we are
aware that these notions cannot be completely axiomatized in the restricted language of

first order logic. COOK [1978] also assumed that the intermediate invariants needed in the
composition rule (99) and while rule (101) can be expressed in the first order language

. This is called relative completeness, which consists in proving that:

Expressive( ,  , op, I) \Upsilon   (150)

\Delta  C \Omega   .\Delta  P, Q \Omega   . ( { P }C{ Q } \Upsilon  |- Th '  { P }C{ Q } )

where Th = {P  \Omega    : I P  = tt} and  Expressive( ,  , op, I) is a sufficient (and
preferably necessary) condition implying that intermediate invariants can be expressed
in  . This implies that true Hoare formulae are provable:  Expressive( ,  , op, I)
\Upsilon   tt(I) ffl  pr(Th).

7 . 4 . 2 . 4 . 1 Expressiveness a` la Clarke

Such a condition Rc( ,  , op, I) was first proposed in  COOK [1978], an
equivalent one was later proposed by  CLARKE [1977]. Observe that in the proof of
{ P } ( C 1; C 2){Q} we can choose an intermediate invariant I such that  I =
{s \Omega  S : \Delta  s' \Omega  S. ( <s, s' > \Omega   C 1) \Upsilon   (s'  \Omega   I Q )} whereas in the proof of {P}(B *
C){Q} we can choose an intermediate invariant I such that  I = {s  \Omega  S : \Delta  s' \Omega  S. (<s,
s'> \Omega  B * C) \Upsilon  (s' \Omega  I Q )}. This leads to the following:

DEFINITON  DIJKSTRA [1976]  Weakest liberal precondition (151)

wlp : P(S x S) x P(S) ff P(S)
wlp(r, q) = {s  \Omega  S : \Delta  s' \Omega  S. (<s, s' > \Omega  r) \Upsilon  (s'  \Omega  q)}

so that execution of a command C starting from a state s  \Omega  wlp(C, Q) cannot reach a
final state not satisfying the predicate Q. The qualifier "liberal" means that nontermination is left as an alternative. The epithet "weakest" means "making less possible
restrictions on initial states". More precisely,  P ffl wlp(C, Q) is another notation for { P
}C{ Q } :

LEMMA  DIJKSTRA [1976]  Definition of partial correctness using wlp (152)

{ p }C{ q } \Phi  p ffl wlp(C, q)

Proof
{ p }C{ q } \Phi  (p  * C ffl S x q)  \Phi  [\Delta  s, s'  \Omega  S. (s  \Omega  p) \Upsilon  ((<s, s' > \Omega  C) \Upsilon  (s'  \Omega  q))]  \Phi 
(p ffl wlp(C, Q)). \Delta 

Termination is obviously not implied since, for example, if execution of command C
never terminates then  C = o/ whence  wlp(C, q) = S. DIJKSTRA [1976]'s most commonly

used predicate transformer wp guarantees termination (see paragraph $ 8.4.8). Possible
alternative definitions of wlp are studied in MORRIS [1987a].

DEFINITION  CLARKE [1977]  Expressiveness a` la Clarke (153)
The interpretation I is said to be expressive for the languages   and   if and only if
\Delta  C \Omega   .\Delta  Q \Omega   . \Lambda  P \Omega   . P = wlp(C, Q).

The expressiveness requirement rules out the interpretations that have been used in
paragraph $ 7.4.2.3 to prove incompleteness results for Hoare logic. For example:

THEOREM  Non-expressiveness of abacus arithmetic (154)

The standard interpretation  IA of abacus arithmetic  A is not expressive for

A

Sketch of proof

This follows from the local incompleteness (142) and relative completeness (156)
below. One can also prove that for Q = (Z = 0) and C = (~(X = 0  \Pi  Y = 0) * ((X = 0 ff
Y := Y - 1  fi X := X - 1); Z := Z - 1)) we have  wlp(C, Q) = {s  \Omega  S : Z(s) = X(s) +
Y(s)} but, by (138), no formula of  A is equivalent to (Z = X + Y).  \Delta 

THEOREM  COOK [1978]  Expressiveness of Peano arithmetic (155)
The standard interpretation  IPE of Peano arithmetic PE  <{0}, {Su, Pr, +, *}, {<}, # >
on the domain   of natural numbers is expressive for  PE and  PE.

Proof

We have, by induction on  , wlp(skip, Q) = Q, wlp(X := E, Q) = Q[X \Xi  E],
wlp(X := ?, Q )  = \Delta   X. Q,  wlp((C1; C2), Q )  =  wlp(C1, wlp(C 2, Q )),  wlp((B  ff
C1 fi C2), Q) = (B \Pi  wlp(C1, Q))  \Sigma  (~B  \Pi  wlp(C2, Q)). For a while-loop (B * C), the
idea is to code the values <s(X1), ..., s(Xk)> of free variables X1, ..., Xk of B and C in
state s by their Go"del number and then to code the terminating execution traces <s0, ...,
sn> of the loop by their Go"del number. To do this we observe that for all n  \Omega    the
coding of any finite sequence of naturals  <a0, ..., an> \Omega  seq*   into a natural c and the
later decoding of c into the a i is representable in Peano arithmetic by a predicate
* \Omega   PE with  Var(*) = {c, n, i, a} such that:  \Delta  n \Omega   . \Delta  c \Omega   . \Lambda  <a0, ..., a n> \Omega 

n+1. \Delta  i <= n.  * \Phi   (a = a i) and  \Delta  n \Omega   . \Delta  <a0, ..., a n> \Omega   n+1. \Lambda  c  \Omega   . \Delta  i  <= n.  *

\Phi  (a = a i) (see  ENDERTON [1972], p. 247-248 ). Now  wlp((B * C),  Q) = {s  \Omega  S : \Lambda  s' \Omega  S.
\Lambda  n \Omega   . (<s, s' > \Omega  (B * C)n) \Pi  (s'  \Omega  ~B  \Pi  Q)} [by (151) and (19.6)] =  P \Pi  ~B  \Pi   Q
where we have to find P such that  P  = {s  \Omega   S  :  \Lambda   s' \Omega   S .  \Lambda   n \Omega    .  < s, s' >  \Omega 
( B  * C ) n } = {s  \Omega   S  :  \Lambda    n   \Omega    .  \Lambda   < s 0 ,   ... ,   s n >  \Omega   S n + 1 . ( \Delta   i <  n.  (si \Omega   B )  \Pi 
(<si, si+1> \Omega  C)) \Pi  (sn = s)}. Assume that X is the only free variable in B and C,
(generalization consists in coding the values of the variables into an integer), we have P

= {s  \Omega   S  :  \Lambda    n   \Omega    .  \Lambda    < d 0 , ..., d n >  \Omega    n + 1 . ( \Delta    i  <  n.  (s[X \Xi   d i]  \Omega   B )  \Pi 
(<s[X \Xi  di], s[X  \Xi  di+1]> \Omega  C)) \Pi   (s(X) = dn)}. Let x  \Omega    be a fresh variable not
appearing in B or C. Let, by induction hypothesis, R  \Omega   PE be such that  R = wlp(C,
(X = x)). We have  P =  {s \Omega  S :  \Lambda  n \Omega   . \Lambda  <d0, ..., d n> \Omega   n+1. ( \Delta  i < n.  s  \Omega  ( B
\Pi   R)[X \Xi   di][x \Xi   di+1]) ^  (X = dn))} =  (\Lambda  n.  \Lambda  c.  (\Delta  i < n.  \Lambda  X.  \Lambda  x.  *[a \Xi   X]  \Pi 
*[i \Xi   i+1][a \Xi   x]  \Pi  B  \Pi  R)  \Pi   *[i \Xi   n][a \Xi   X]). \Delta 

BERGSTRA & TUCKER [1982c]  have shown that the expressiveness concept is
awkward to apply for two-sorted data types. For example two independent copies of 
can form a two-sorted sorted interpretation that is not expressive.

7 . 4 . 2 . 4 . 2 Relative completeness of Hoare logic

By the expressiveness requirement on interpretations I, one can always express in
 intermediate invariants which are sufficient to prove the partial correctness of
commands C using Hoare logic:

THEOREM  COOK [1978]  Relative completeness of Hoare logic (156)

(I is expressive for   and   \Pi  Th = {P  \Omega   : I P = tt})  \Upsilon 

\Delta  C \Omega   .\Delta  P, Q \Omega   . ( { P }C{ Q } \Upsilon  |- Th '  { P }C{ Q } )

Proof

By structural induction on formulas {P}C{Q}.

* If  { P }skip{ Q } then P \Upsilon  P is true [by (123.6) and (125)] and so is  P \Upsilon  Q [by
(22), (19.1), (123.6) and (125)]. It follows by hypothesis that (P  \Upsilon  P) and (P  \Upsilon  Q)
belong to Th. Therefore the proof of {P}skip{Q} consists in applying the skip axiom
(96) and the consequence rule (102).

* If  { P }X := E{  Q } then  P ffl {s  \Omega  S : s[X  \Xi  E(s)]  \Omega  Q} [by (22) and (19.2)]
whence  P \Upsilon  Q[X \Xi  E] is true [by (123.6) and (128)]. Therefore the proof of {P}X
:= E{Q} consists in applying the assignment axiom (97) and the consequence rule
(102).

* If  { P }X := ?{  Q } then {s[X  \Xi  d] : s  \Omega  P \Pi  d \Omega  D} ffl Q [by (22) and (19.3)]
whence {s :  \Lambda  d  \Omega  D . s[X  \Xi   d]  \Omega  P}  ffl Q  so that  \Lambda  X. P  \Upsilon   Q  is true [by (123.7),
(123.6), and (125)]. The proof of {P}X := ?{Q} consists in applying the random
assignment axiom (98) and the consequence rule (102).

* If  { P }(C1; C2){ Q } then  P ffl wlp((C1; C2), Q) [by (152)] =  wlp(C1, wlp(C2,
Q)) [by (151) and (19.4)]. By expressiveness let I and J be such that  I = wlp(C2, Q)
and J = wlp(C1, I). By induction hypothesis we can prove {I}C 1{J} and {J}C 2{Q}.
Moreover  P \Upsilon  I and  Q \Upsilon  Q are true so that the proof of {P}(C 1; C2){Q} ends by
application of the composition rule (99) and the consequence rule (102).

* If  { P }(B  ff   C1  fi  C2){ Q  } then we can prove {P  \Pi  B } C 1{Q} and {P  \Pi 
~B}C2{Q} [by (22), (19.5), (123.4), (123.3) and induction hypothesis] and conclude
by the conditional rule (100).

* If  { P }(B * C){ Q } then let I, J be such that  I = wlp((B * C), Q) and J = wlp(C,
I). We have  wlp(C, I) ffl wlp(C, I) whence  { wlp(C, I) }C{ I } [by (152)] so that by
induction hypothesis we can prove that {J}C{I}. Moreover  I = {s \Omega  S : \Delta  s' \Omega  S. (<s,
s'> \Omega  (B * C))  \Upsilon  (s'  \Omega  Q)} [by (151)] = {s  \Omega  S : \Delta  s' \Omega  S. (<s, s' > \Omega  (* _ ~B) ' (B *
C ) * (B * C))  \Upsilon   (s'  \Omega  Q )} [by (19.7)] = {s  \Omega  S : (s  \Omega   ~B ) \Upsilon   (s  \Omega  Q )}  ' {s  \Omega  S :

\Delta  s' \Omega  S. (<s, s' > \Omega  (B * C) * (B * C))  \Upsilon  (s'  \Omega  Q)} = ( ~B \Upsilon  Q) ' {s  \Omega  B : \Delta  s' \Omega  S.
(<s, s'> \Omega  C * (B * C))  \Upsilon  (s'  \Omega  Q)} [by (123)] = ( ~B  \Upsilon  Q)  ' (B ^ wlp(C * (B * C),

Q)) [by (151)] = ( ~B \Upsilon  Q) ' (B ^ wlp(C, wlp((B * C),  Q))) [by (151)]. It follows
that ( I \Pi  B)  \Upsilon   J. Whence applying the consequence rule (102) with (I  \Pi  B)  \Upsilon   J,
{J}C{I} and I  \Upsilon  I, we can prove {I  \Pi  B}C{I} so that {I}(B * C){I  \Pi  ~B} derives
from  the while rule (101). Finally  P \Upsilon  I [by (152)] and (I \Pi  ~B) \Upsilon  Q so that the proof
of {P}(B * C){Q} ends by application of the consequence rule (102).

* Observe that in the above proof Cook's expressiveness is only used to guarantee
weak expressiveness  (RODRI'GUEZ-ARTALEJO [1985]) that is that loop invariants for the
while rule (101) and intermediate invariants for the composition rule (99) can be
expressed in  . \Delta 

7 . 4 . 2 . 4 . 3 Expressiveness a` la Cook and its equivalence with

Clarke's notion of expressiveness

The original notion of  expressiveness is due to COOK [1978] and was expressed in
term of the predicate transformer slp(C, P) (that is the set of all final states C can reach
when started in a state satisfying P, such that  { p }C{ slp(C, p) } and { p }C{ q } \Upsilon 
slp(C, p) ffl q):

DEFINITION  DIJKSTRA [1976]  Strongest liberal postcondition   (157)

slp(r, p) = {s  \Omega  S : \Lambda  s' \Omega  p. <s', s> \Omega  r}

DEFINITION  COOK [1978]  Expressiveness a` la Cook (158)
The interpretation I is said to be expressive for the languages   and   if and only if
\Delta  C \Omega   .\Delta  P \Omega   . \Lambda  J \Omega   . J = slp(C, P).

However expressiveness a` la Cook is equivalent to expressiveness a` la Clarke. To show
this we first observe that:

LEMMA  Relationships between wlp and slp (159)
{ p }C{ q } \Phi  (slp(C, p) ffl q),  wlp(r, q) = ~ slp(r-1, ~q) and  slp(r, p) = ~ wlp(r-1,
~ p )

Proof

{ p }C{ q } \Phi  (p * C ffl S x q) \Phi  [\Delta  s, s'  \Omega  S. ((s  \Omega  p) \Pi  (<s, s'> \Omega  C)) \Upsilon  (s' \Omega  q)]
\Phi  (slp(C, p) ffl q). Moreover ~ slp(r-1, ~q) = {s  \Omega  S : ~( \Lambda  s'. s'  \Omega  ~q  \Pi  <s', s > \Omega  r-1)}
[by (157)] = {s  \Omega  S :  \Delta  s'.  s' \Omega  q \Sigma  <s, s' > fl r} =  wlp(r, q) [by (151)]. Finally  slp(r,
p) = ~~ slp(r-1-1, ~~p) = ~ wlp(r-1, ~p). \Delta 

LEMMA  Semantic inversion (160)
if {X1,...,Xn} = Free(C), {x 1,...,xn} ^ Free(C) = o/, {x 1,...,xn} ^ Free(P) = o/,  Q =
s l p ( C ,  X 1  = x 1   \Pi   ...  \Pi   X n  = x n ) and Q' = ( \Lambda   X 1 . ...  \Lambda   X n . Q  \Pi 
P)[x1 \Xi  X1] ... [xn \Xi  Xn] then  Q' = slp(C-1, P).

Proof

* We let n = 1 for simplicity. We first prove that ( \Lambda   s". s"(X) = s"(x)  \Pi 
< s " ,   s [ X  \Xi   d ] [ x   \Xi    s ( X ) ] >   \Omega   C )  \Phi    (< s, s[X   \Xi    d ] >   \Omega   C ). If  < s " ,
s[X \Xi   d][x \Xi   s(X)]> \Omega  C then s"(y) = s[X  \Xi   d][x \Xi   s(X)](y) for y  fl Free(C) =
{X} since execution of C does not modify the value of variables not appearing in C. It
follows that s"(y) = s(y) for y  fl {x, X} and s"(x) = s(X). Whence s"(X) = s"(x)
implies s"(X) = s(X) that is s"(y) = s(y) for y  `x and s"(x) = s(X) so that s" =
s[x \Xi   s(X)]. It follows that  <s[x \Xi   s(X)], s[X  \Xi   d][x \Xi  s(X)]> \Omega  C whence that
<s[x \Xi   d'], s[X \Xi   d][x \Xi   d']> \Omega  C for all d'  \Omega  D  since x  fl Free(C). For d' = s(x)
we conclude that  <s, s[X \Xi   d]>  \Omega   C . Reciprocally if  < s, s[X  \Xi   d]> \Omega   C  then
<s[x \Xi  d'], s[X \Xi  d][x \Xi  d']> \Omega  C for all d'  \Omega  D since x  fl Free(C) so that for d' =
s(X) we get  < s [ x   \Xi    s ( X ) ] ,   s [ X   \Xi    d ] [ x   \Xi    s ( X ) ] >   \Omega   C  that is
<s", s[X \Xi  d][x \Xi  s(X)]> \Omega  C with s" = s[x  \Xi  s(X)] so that s"(X) = s"(x) since x  `
X .

*  I (\Lambda  X.  Q  \Pi  P)[x \Xi   X]  = {s  \Omega  S : s[x  \Xi  s(X)]  \Omega  I (\Lambda  X.  Q  \Pi  P) } [by (128),
and (122.1)] = {s  \Omega  S :  \Lambda  d  \Omega  D . s[x  \Xi   s(X)][X \Xi   d]  \Omega   Q  ^  P} [by (123.7) and
(123.4)] = {s  \Omega  S : \Lambda  d \Omega  D. s[x  \Xi  s(X)][X \Xi  d] \Omega  slp(C, X=x) ^ P} [by hypothesis
of (160)] = {s  \Omega  S : \Lambda  d \Omega  D. \Lambda  s". s"(X) = s"(x)  \Pi  <s", s[X \Xi  d][x \Xi  s(X)]> \Omega  C \Pi 
s[x \Xi   s(X)][X \Xi   d]  \Omega   P } [by (157), (123.1), (122.1) and (121) since X `x] =
{s \Omega   S  :  \Lambda  d \Omega   D . <s, s[X  \Xi   d]> \Omega   C  \Pi  s[x  \Xi   s(X)][X \Xi   d] \Omega   P } [by the above
argument] = {s  \Omega  S :  \Lambda  d \Omega  D . <s, s[X  \Xi   d]> \Omega   C  \Pi  s[X  \Xi   d]  \Omega   P} [since X  ` x
implies s[x  \Xi   s ( X ) ] [ X  \Xi   d] = s[X  \Xi   d ][x  \Xi   s(X)] and x  fl  F r e e (P) so that
s[X \Xi   d][x \Xi   d']  \Omega  P  implies s[X  \Xi   d]  \Omega  P ] = {s  \Omega   S :  \Lambda  s'.<s, s' > \Omega   C  \Pi  s'  \Omega   P }
[since  <s, s'> \Omega  C implies s(y) = s'(y) when y  fl Free(C) so that s'=s[X  \Xi  d] where
d = s'(X)]  =  slp(C-1, P) [by (157)].  \Delta 

THEOREM CLARKE [1977], JOSKO [1983], OLDEROG [1980] [1983]  Equivalent (161)

definitions of expressiveness
Expressiveness a` la Cook is equivalent to expressiveness a` la Clarke: (153) \Phi  (158)

Proof

If  \Delta   C  \Omega   .  \Delta   P  \Omega   .  \Lambda   J  \Omega   .  J =  slp(C ,  P ) then if {X 1, ..., X n} =
Free(C), {x 1, ..., xn} ^ Free(C) = o/, {x 1, ..., x n} ^ Free(P) = o/,  Q = slp(C, X1 = x1
\Pi  ...  \Pi  X n = x n) and Q' = ( \Delta   X 1. ...  \Delta   X n. Q  \Upsilon   ~P)[x1 \Xi   X 1] ... [xn \Xi   X n] then
~Q' = ~slp(C-1, ~P) [by (160)] =  wlp(C, P) [by (159)] is expressible in  .

The same way if  R = wlp(C, X1 = x1 \Pi  ... \Pi  Xn = x n) and R' = ( \Lambda  X1. ...  \Lambda  Xn.
Q  \Pi  ~P)[x1 \Xi   X 1] ... [xn \Xi   X n] then  ~R' = slp(C, P). \Delta 

7 . 4 . 2 . 4 . 4 Relative completeness of Hoare logic for arithmetical

while-programs and nonstandard interpretations

 Hoare logic is relatively complete for while-programs applied to arithmetic:
THEOREM COOK [1978]   Relative completeness of Hoare logic for arithmetical  (162)

while-programs
 ' Th( ) is relatively complete for the standard interpretation IPE of Peano arithmetic

PE <{0}, {Su, Pr, +, *}, {<}, # > on the domain   of natural numbers where Th( ) =
{P \Omega   PE : IPE P  = tt} is the number theory.

Proof

By (155), IPE is expressive for  PE and  PE so that, by relative completeness
(156), the Hoare logic   ' Th( ) is relatively complete for  IPE. \Delta 

HAREL [1979] has pointed out that any interpretation  I can be expanded to an
interpretation with a complete Hoare logic by expanding it to an arithmetical universe
(but this expansion may increase the degree of undecidability of the theory of  I). A
simpler expansion when  tt(I) is recursive is proposed in  BERGSTRA, CHMIELINSKA &

TIURYN [1982b].

In (162) the facts about arithmetic one needs in a program correctness proof are
given by the oracle Th( ). BERGSTRA & TUCKER [1983]  use instead Peano's first-order
axiomatization of arithmetic ( KLEENE [1967], $ 38;  JOHNSTONE [1987], $ 3 ). Second-order
Peano arithmetic PE 2 over the basis  = {0, 1},  = {+, *} and  = o/ can be
formalized by the following axioms (axioms (163.4) to (163.7) are not strictly
necessary since addition and multiplication can be defined the same way as (x  <= y) is
defined by (\Lambda  z. (x + z) = y)):

DEFINITION   Second-order Peano arithmetic PE 2 (163)

\Delta  x. ~(x + 1 = 0) (.1)
\Delta  x. \Delta  y. (x + 1 = y + 1)  \Upsilon  (x=y) (.2)
\Delta  x. ~(x = 0)  \Upsilon  \Lambda  y. (x = y + 1) (.3)
\Delta  x. (x + 0 = x) (.4)
\Delta  x. \Delta  y. (x + (y + 1)) = ((x + y) + 1) (.5)
\Delta  x. (x * 0 = 0) (.6)
\Delta  x. \Delta  y. (x * (y + 1)) = ((x * y) + y) (.7)
\Delta  P. (P[x  \Xi  0] \Pi  \Delta  x. P  \Upsilon  P[x \Xi  x + 1])  \Upsilon  \Delta  x. P (.8 2)

The last axiom (163.8 2) states that if a property P is true for 0 and is true for the
successor x + 1 of x whenever it is true for x then it is true for all x. Since P ranges
over all subsets of  , the second-order axiom (163.82) describes properties of | P( ) | =
B1 subsets of  . To stay in the realm of first-order logic, one can define first-order

Peano arithmetic PE 1 which consists of axioms (163.1) to (163.7) plus the axiom
scheme:

DEFINITION   First-order Peano arithmetic PE1 (164)

\Delta  x. ~(x + 1 = 0) (.1)
... ...
\Delta  x. \Delta  y. (x * (y + 1)) = ((x * y) + y) (.7)
For all P  \Omega   PE.

(P[x \Xi  0]  \Pi  \Delta  x. P  \Upsilon  P[x \Xi  x'])  \Upsilon  \Delta  x. P (.8 1)

There are |   | = B0 = OE predicates P (the proof uses an enumeration of  PE by Go"del
numbers, see (143)) and |   | ` | P( ) | whence (164.8 1) describes less subsets of 
than (163.82) (The proof that |   | ` | P( ) | is by reductio ad absurdum using Cantor
diagonal argument : if |  P( ) | = |  | then  P( ) is of the form {s j : j \Omega   } where s j
ffi   for all j  \Omega   . Then the set {i  \Omega    : i fl si} would be some element s k of  P( )
whence a contradiction since either k  \Omega  sk and sk = {i \Omega    : i fl si} implies k  fl sk or k
fl sk and sk = {i \Omega    : i fl si} implies k  \Omega  sk). Since PE 1 imposes less constrains on its
interpretations than PE2, PE1 can have nonstandard interpretations that are disallowed
by PE2. Such nonstandard models of PL 1 (SKOLEM [1934]; BOOLOS & JEFFREY [1974], $ 17
or KLEENE [1967], $ 53 ) consists of the naturals followed by infinitely many blocks
isomorphic to   : 0, 1, 2, ... ...... ...-2' -1' 0' 1' 2... ...... ...-2" -1" 0" 1" 2"...
......, without least nor greatest block and between any two blocks lies a third. It
follows that   is not first-order axiomatizable (although it is by PE2 since   is the only
model of PE2) in the sense that there are true facts that can be proved by PE2 but not by
PE1 (using again the diagonalization argument : if Pi is the predicate with Free(Pi) = x
and Go"del number i then Q such that  \Delta  i \Omega   . Q[x  \Xi  i] = ~P i[x \Xi  i] is not one of

them). So why not use second-order logics ? Essentially because PE 1 deals with finite
sets of integers (as in pure arithmetic) whereas PE 2 deals with infinite sets of integers
(as in mathematical analysis) and P( ) is much more complicated to understand than 
(COHEN [1966] proved that there are infinitely many different ways to conceive of  P( )
from the same  ). The same way Hoare logic deals with finite sets of variables and
terminating programs i.e. finite execution traces and  BERGSTRA & TUCKER [1983]  have
shown that Hoare logic for while-programs is essentially first order : the strongest
postcondition calculus can be represented in Peano arithmetic PE 1 (because slp(C, P)
can be expressed by a predicate SLP(C, P) of PE1, see the proof of (155) and (159)) so
that Hoare logic over PE1 is equivalent to PE1 itself (because {P}C{Q} is equivalent to
(SLP(C, P) \Upsilon  Q) by (159)). The comparison of Hoare-style reasoning about programs
to reasoning about programs with first order rendering of predicate transformers is
pursued by LEIVANT [1985].

7 . 4 . 2 . 4 . 5 On  the unnecessity of expressiveness

 Expressiveness is sufficient to obtain relative completeness but it is not
necessary: BERGSTRA & TUCKER [1981] have shown that Hoare logic can be complete for
an inexpressive interpretations  I whose first-order theory has some expressive model
(i.e. interpretation  I' with the same first-order theory {P  \Omega    : I' P  = tt}). This point
is illustrated by the following:

THEOREM  BERGSTRA & TUCKER [1982a]   Unnecessity of expressiveness (165)
Hoare logic   ' Th( ) is relatively complete for any model I of Peano arithmetic (such
that  \Delta  P  \Omega  Th( ).  I P  = tt where Th( ) = {P  \Omega   PE :  IPE P  = tt}) but  I is not
expressive for  PE and  PE when I is not the standard model IPE of arithmetic.

Proof

By (162), the Hoare logic   ' Th( ) is relatively complete for IPE. Since any P \Omega 

PE is true for the standard interpretation  IPE if and only if it is true for the
nonstandard interpretation I,   ' Th( ) is also relatively complete for I.

Let C = (X := Y; ((~(X=0) * X := Pr(X)); X := Y)). Execution of C for the
nonstandard interpretation  I terminates only if the initial value of Y is standard. It
follows that  slp(C, S) = {s  \Omega  S : (s(X) = s(Y))  \Pi  s(Y)  \Omega   }. Now  I is not expressive
for  PE and  PE since otherwise there is a P  \Omega   PE such that  P = slp(C, S), so

that \Lambda  Y. P is true of X only if X is a standard natural number in contradiction with the
fact that no predicate of  PE can be used to distinguish among standard and

nonstandard numbers.  \Delta 

BERGSTRA & TIURYN [1983]  have identified and studied two necessary (but not
sufficient) conditions that an interpretation I must satisfy if a sound Hoare logic is to be
complete for this given  I: first they prove that the first order theory of  I must be  PCcompact that is each asserted program which is true in all models of the theory is true in
all models of a finite subset of the theory (if Th = {P  \Omega    : I P  = tt} then  \Delta  H  \Omega 

tt Th . \Lambda  Th' ffl Th. (| Th' |  \Omega   ) \Pi  (H  \Omega   tt Th' ) where  tt T  = {H  \Omega   : \Delta 
I'. (\Delta  P \Omega  T. I' P  = tt ) \Upsilon  (I' H  = tt)}). Secondly they prove that the partial correctness
theory  tt(I) must be decidable relative to its first order theory Th (as shown in

(148)).

From a practical point of view, the incompleteness results about Hoare logic are
not restrictive for hand-made proofs, just as Go"del's incompleteness theorems do not
prevent mathematicians to make proofs. Only the semantic counterpart of Hoare logic
matters and it is complete in the sense of (80). As far as expressiveness is concerned,
the limited power of first order logic can always be overcome using infinitary logics
since (76) is expressible in LOE1OE (which allows infinite formulae \Pi  \Phi  and \Sigma  \Phi  when \Phi  is

a countable set of formulae,  BARWISE [1977]) as noticed in  ENGELER [1968]  [1975] and
BACK [1980] [1981] (but then the finitary nature of proofs in ordinary first order logic LOEOE
is lost). Also the use of a given theory Th corresponds to the common mathematical
practice to accept certain notions and structures as basic and work axiomatically from
there on. However when considering more complicated programming language
features, Hoare logic turns out to be incomplete for intrinsic reasons.

7 . 4 . 2 . 5 Clarke's characterization problem

CLARKE [1977] has shown that some programming languages have no sound and
relatively complete Hoare logic. The formal argument is first that if a programming
language possesses a relatively complete and sound Hoare logic then the halting
problem for finite interpretations must be decidable and second that Algol-like
(NAUR [1960]) or Pascal-like ( WIRTH [1971]) languages have an undecidable halting
problem for finite interpretations with |  D | >= 2. The intuitive reason is that names in
predicates P, Q  \Omega    and in commands C  \Omega    are used in a similar way: all
considered objects, at a given instant of time, are given different names. Hence
variables of P, Q and C can be interpreted in exactly the same way by means of states
(120). But when considering Algol or Pascal-like languages, the naming conventions in
P, Q and C are totally different. For example objects deeply buried in the run-time stack
cannot be accessed by their name although they can be modified using procedure calls !
Such Algol or Pascal-like languages are more precisely characterized by the following:

DEFINITION  CLARKE [1977]  Clarke's languages (166)
A Clarke's language   is a programming language allowing procedures (with a finite
number of local variables and parameters taking a finite number of values, without
sharing via aliases) and the following features :

- procedures as parameters of procedure calls (without self-application); (i)
- recursion; (ii)
- static scoping; (iii)
- use of global variables in procedure bodies; (iv)
- nested internal procedures as parameters of procedure calls. (v)
A Clarke language  j is obtained by disallowing feature (j).

The non-existence of Hoare logic for Clarke's languages (and other variants of (166),
see  CLARKE [1977] and  LEIVANT & FERNANDO [1987] ) introduces the  characterization
problem (CLARKE [1984]): what criteria guarantee that a programming language has a
sound and relatively complete Hoare logic ? First we prove the non-existence of Hoare
logics for Clarke languages and next review the literature on the characterization
problem.

7 . 4 . 2 . 5 . 1 Languages with a relatively complete and sound Hoare

logic have a decidable halting problem for finite
interpretations

LEMMA CLARKE [1977]  Decidability of the halting problem ... (167)

If   has a sound and relatively complete Hoare logic then the halting problem
must be decidable for all interpretations I on a finite Herbrand definable domain D.

Proof

Let be given some particular finite interpretation I. There is a decision procedure
to verify that P  \Omega  Th that is  I P = tt: we just have to check using truth tables that P
holds for the finitely many possible combinations of values of the free variables of P.
Moreover since  D is finite and Herbrand definable,   is expressive with respect to

 and  I: any subset of  D  can be represented as a finite disjunction of terms
representing its elements. Then by the soundness theorem (129) and relative
completeness (156) we have  { true }C{ false } \Phi   |-  Th '   { true }C{ false }

where true = (x = x) and false = ~(x = x). Since Th is recursive, it follows from (143)
that  p r (Th) is recursively enumerable whence so is {C : |-
 Th '  { true }C{ false } } = {C :  { true }C{ false } } so that the non-halting

problem is semi-decidable. We conclude that the halting problem cannot be undecidable
(see (144)).  \Delta 

7 . 4 . 2 . 5 . 2 The halting problem for finite interpretations is

undecidable for Clarke's languages

The halting problem is decidable for while-programs on finite interpretations (we
may test for termination (at least theoretically) by watching the execution trace of the
program to see if a state is repeated, JONES & MUCHNICK [1977]). For recursion one might
expect that the program could be viewed as a type of push-down automaton for which
the halting problem is also decidable ( COOK [1971], JONES & MUCHNICK [1978] ). However
this is not true for Clarke's languages:

LEMMA CLARKE [1977], JONES & MUCHNICK [1978]   Undecidability of the halting (168)

problem...
Clarke's languages have an undecidable halting problem for finite interpretations
with | D | >= 2.

Proof

The proofs of  JONES & MUCHNICK [1978]  (modified in  CLARKE [1977]) consist in
showing that such languages can be used to simulate a queue machine which have an
undecidable halting problem. Clarke's languages can also simulate the more wellknown Turing machines ( TURING [1936] [1937]; ENDERTON [1977], $ 2;  BOOLOS &
JEFFREY [1974], $ 5;  KLEENE [1967],  $ 41; ROGERS [1977], $ 1.5 ). Since, by  Church thesis
(i.e. formally unprovable mathematical assertion), all functions intuitively computable
algorithmically are computable by Turing machines ( CHURCH [1936], KLEENE [1936],
TURING [1937]), it follows that all computable functions are programmable in Clarke's
languages with | D | \Omega   + - {1}. Hence by (144), the halting problem is undecidable. A
similar result was previously obtained by LANGMAACK [1973], where it is shown that the
pure procedure mechanism of Algol 60 can simulate any Turing machine.

A Turing machine has a finite number of internal states Q 0, ..., Qn. It can read
and write symbols chosen in a finite alphabet S 0, ..., Sm (containing the blank symbol
'_') on a potentially infinite tape marked off into squares by means of a head which can
also be moved left or right, one square at a time:

1 1 _ _ _ _ _ _ ________
Write S Move rightMove left j

Si

Qi
0

An instruction has the form M(Q i, Si, Sj, D, Q j) where D is 'Left' or 'Right'. This
instruction is executable if the machine is in the configuration  <Qi, Si>, that is its
internal state is Qi and the symbol scanned under the head is Si. Its execution consists in

overwriting the square under the head by symbol Sj, in moving the head on the tape one
square of the present square in the direction indicated by D and in changing the internal
state into Qj. A program consists of a finite number of instructions M(Qi, Si, Sj, D, Qj),

i = 1, ..., l. Its execution consists in repeatedly executing any one of the executable
instructions. The execution of the program halts when no instruction is executable.
Initially the tape contains finitely many non-blank symbols.

By induction on the number of steps, it follows that only finitely many squares
can be non-blank at any time during execution of the program. Therefore Turing
machines can be built-up recursively from a finite number of simpler identical machines
M1, ..., Mn consisting only of two squares:

No_Head

Qi

_ _ 0 1 : Square[Right] Square[Left] : 
Present_State :  

M 
M 

1 
M 

S i M 0 

1 
2 
3  Head_Position :  On_Subtape 

Right

Machine M 0 is empty. The internal state of each machine consists of two squares
(Square[Left] and Square[Right]), an indication of whether the head of the Turing
machine is on its left square (Head_Position = Left), on its right square (Head_Position
= Right), on a square of one of the machines M i-1, ..., M 1 (Head_Position =
On_Subtape), or on a square of one of the machines M i+1, ..., M n (Head_Position =
No_Head) and an indication of whether machine M i-1 is empty (Is_Empty_Subtape =
true). We could have used only Boolean variables as done by the Pascal compiler. To
execute the program of the Turing machine, machine Mn has access to the current state
Qi stored in the global variable Present_State (using feature (166.iv) of Clarke's
languages). Execution of an instruction of the Turing machine (such as
M(Qi, Si, Sj, Left, Qj) when the head is on the left square of machine M n (i.e.
Head_Position = Left) containing Si (i.e. Square[Left] = Si)) may require to extend the
tape by one square on side D (D = Left in the example). In this case, machine M n
(currently simulated by procedure Turing_Machine) assigns S j to its D square,
No_Head to its Head_Position, Q j to Present_State and creates a new machine M n+1
(by a recursive call to procedure Turing_Machine, using feature (166.ii) of Clarke's
languages). This machine M n+1 has two blank squares and Head_Position =
Initial_Head_Position = D. M n+1 is now in charge of executing the program of the
Turing machine. To do this, machine M n+1 can ask the cooperation of machine M n
(hence recursively of machines M n-1, ..., M1) using functions and procedures local to
Mn and passed to procedure Turing_Machine upon creation of machine M n+1 (using

features (166.i) and (166.v) of Clarke's languages). These functions and procedures
can be used by M n+1 to read (Scanned_Symbol_On_Ends_Of_Subtape) or write
( W r i t e _ O n _ E n d s _ O f _ S u b t a p e )   t h e   s q u a r e s   o f   M n  and to read
( H e a d _ P o s i t i o n _ O n _ E n d s _ O f _ S u b t a p e )   o r   w r i t e
(Set_Head_Position_On_Ends_Of_Subtape) the Head_Position of M n. Procedure
M_On_Subtape can be used by machine M n+1 to execute an instruction
M(Qi, Si, Sj, D, Qj) of the Turing machine when M n+1 knows that the head of the
Turing machine is not on the squares of machine M n  (by calling
Head_Position_On_Ends_Of_Subtape) so that after execution of this instruction, the
head will remain on the subtape represented by machines M n, ..., M1. It follows that
in order to simulate the Turing machine, machine M n+1 has just to take care of head
moves from its squares to those of machine M n. For example when the head is on the
left square of machine Mn+1 (i.e. Head_Position = Left) and reads Si (i.e. Square[Left]
= Si), execution of M(Q i, Si, Sj, Right, Qj) consists in writing Sj in this left square,
in changing Head_Position to On_Subtape, in changing the Head_Position of machine
Mn to Left (by calling Set_Head_Position_On_Ends_Of_Subtape(Left)) and in going to
the next state by assignment of Qj to Present_State. When n = 0, the Head_Position of
machine M 1 is simply changed to Right. Details are given in the following Pascal
program (using the static scope execution rule (166.iii) which states that procedure calls
are interpreted in the environment of the procedure's declaration rather than in the
environment of the procedure call, thus allowing to access values normally buried
deeply in the run-time stack):

program Simulate_Turing_Machine;

const Blank = '_';
type

State_Type = 0..107; Symbol_Type = char;
Head_Position_Type = (Left, Right, On_Subtape, No_Head); Side_Type = Left..Right;

function Opposite(D : Side_Type) : Side_Type;

{ Opposite(Left) = Right and Opposite(Right) = Left. }
begin {Opposite}

case D of

Left  : Opposite := Right;
Right : Opposite := Left;
end;
end; {Opposite}

var Present_State : State_Type;

{ Present state of the Turing machine. }
Stopped : Boolean;

{ True only if the Turing machine must halt (initially false). }
Configuration_Found : Boolean;

{ To check no invalid configuration (Present_State, scanned symbol) is found. }

procedure Turing_Machine

(Initial_Head_Position : Side_Type;
 Is_Empty_Subtape : Boolean;
 function Scanned_Symbol_On_Ends_Of_Subtape(D: Side_Type) : Symbol_Type;
 procedure Write_On_Ends_Of_Subtape(D: Side_Type; WS : Symbol_Type);
 function Head_Position_On_Ends_Of_Subtape : Head_Position_Type;
 procedure Set_Head_Position_On_Ends_Of_Subtape(P : Head_Position_Type);
 procedure Dump_Subtape;

procedure M_On_Subtape(Q : State_Type; S : Symbol_Type;

WS : Symbol_Type; D : Side_Type; NQ : State_Type));
var

Square : array [Side_Type] of Symbol_Type;
Head_Position : Head_Position_Type;

{ An infinite tape is represented by its  finite  non-blank  part  as  a  quadruple }
{ <Square[Left],  subtape,  Square[Right],  Head_Position>, where the Head_Position }
{ equals D if the head is on 'Square[D]' where D is 'Left' or 'Right', 'On_Subtape' }
{ if the head is on the subtape or else 'No_Head' when the  head  is  outside  that }
{ part  of  the  whole tape. Is_Empty_Subtape is true if and only if the subtape is }
{ empty. When the subtape is not empty, it can  be  manipulated  by  functions  and }
{ procedures, similar to the ones explained below for manipulating the tape.        }
{ A call of 'Turing_Machine' extends the subtape by two additional blank squares on }
{ its  left  and  right  ends.  The  head  of  the  machine  is set on one of these }
{ additional squares as specified by Initial_Head_Position. }

function  Scanned_Symbol_On_Ends_Of_Tape(D: Side_Type) : Symbol_Type;

{ Returns the symbol written on the square on the D end of the tape. }
begin Scanned_Symbol_On_Ends_Of_Tape := Square[D]; end;

procedure Write_On_Ends_Of_Tape(D : Side_Type; WS : Symbol_Type);

{ Writes WS on the square on the D end of the tape. }
begin Square[D] := WS; end;

function Head_Position_On_Ends_Of_Tape : Head_Position_Type;

{ To  check  if  the  head of the machine is on the left end of the tape (when the }
{ returned value is Left) or on its right end (when the returned value  is  Right) }
{ or  whether  it  is on the subtape delimited by the extreme squares (On_Subtape) }
{ or if it is outside the part of the ideal  infinite  tape  represented  by  that }
{ finite tape (No_Head). }
begin Head_Position_On_Ends_Of_Tape := Head_Position; end;

procedure Set_Head_Position_On_Ends_Of_Tape(P : Head_Position_Type);
     { Sets the Head_Position of the tape to P. }

begin Head_Position := P; end;

procedure Dump_Tape;

{ Dump all tape marking the scanned symbol under the head between square brackets. }
procedure Dump_Square(D : Side_Type);
begin {Dump_Square}

if (Head_Position = D) then write('[') else write(' ');
if (Square[D] = Blank) then write('_') else write(Square[D]);
if (Head_Position = D) then write(']');
end; {Dump_Square}
begin Dump_Square(Left); Dump_Subtape; Dump_Square(Right); end;

procedure M(Q : State_Type; S, WS : Symbol_Type; D : Side_Type; NQ : State_Type);

{ Whenever  the  machine  (which  is  not  stopped) comes to state Q (that is the }
  { instruction labeled Q) while scanning under  the  head  a  square  where  S  is }

{ written, (set Configuration_Found to true), overwrite this square with WS, move }
{ the  head  in the direction indicated by D one square of the present square and }
{ proceed to instruction labeled NQ. }
begin {M}

if (not Configuration_Found) and (not Stopped) and (Present_State = Q) then
begin

if (Head_Position = Opposite(D)) then begin

if (Square[Opposite(D)] = S) then begin

Configuration_Found := true;
Square[Opposite(D)] := WS;
if Is_Empty_Subtape then Head_Position := D
else begin { Move the head on the Opposite(D) end of the subtape }

Set_Head_Position_On_Ends_Of_Subtape(Opposite(D));
Head_Position := On_Subtape;
end;
Present_State := NQ; { Go to next state. }
end;

end else if (Head_Position = D) then begin

if (Square[D] = S) then begin

Configuration_Found := true;
Square[D] := WS; Head_Position := No_Head;
{ From now on, the continuation of the simulation of the Turing machine }
{ is  delegated  to  the  next call of procedure 'Turing_Machine' which }
{ extends the non empty tape by two new blank squares on its  ends  and }
{ moves the head of the machine on the square on the D end. }
Present_State := NQ; { Go to next state. }
Turing_Machine(D, false, Scanned_Symbol_On_Ends_Of_Tape,

Write_On_Ends_Of_Tape, Head_Position_On_Ends_Of_Tape,
Set_Head_Position_On_Ends_Of_Tape, Dump_Tape, M);
end;
end else if (Head_Position = On_Subtape) then begin

if (Head_Position_On_Ends_Of_Subtape = D)

and (Scanned_Symbol_On_Ends_Of_Subtape(D) = S) then begin
Configuration_Found := true;
Write_On_Ends_Of_Subtape(D, WS);
{ The head leaves the subtape for the D square }
Set_Head_Position_On_Ends_Of_Subtape(No_Head); Head_Position := D;
Present_State := NQ; { Go to next state. }
end else { The move of the head on the subtape will remain on that subtape. }

M_On_Subtape(Q, S, WS, D, NQ);
end;
end;
end; {M}

begin {Turing_Machine}

Head_Position := Initial_Head_Position;
Square[Left] := Blank; Square[Right] := Blank;
while (not Stopped) do begin

{ Execute one instruction of the Turing machine }
Configuration_Found := false;
{**********************************************************************************}
    { Turing machine of ENDERTON [1977], p. 532. (This  machine  computes  x + y.  The }

{ arguments  x and y of the + function are respectively represented by a string of }
{ 1's of length x and y. The arguments are separated by a single blank.  The  tape }
{ is  otherwise blank. The head is initially on the leftmost non-blank symbol. The }
    { result is a string of 1's of length x + y. }
    { M(State, Scan,  Write, Move,    Next state) }

M(0,     '1',   '1',   Right,   0);  { Pass over x. }
M(0,     Blank, '1',   Right,   1);  { Fill blank square between x and y }
M(1,     '1',   '1',   Right,   1);  { Pass over y. }
M(1,     Blank, Blank, Left,    2);  { Move to end of y; }
M(2,     '1',   Blank, Left,    3);  { Erase a 1 at end of y. }
M(3,     '1',   '1',   Left,    3);  { Back up to leftmost 1 of x + y. }
M(3,     Blank, Blank, Right,   4);  { Halt. }
{**********************************************************************************}
{ Initialize the tape to compute x + y with x = 2 and y = 3 }

M(100,   Blank, '1',   Left,  101); { Write y }
M(101,   Blank, '1',   Left,  102);
M(102,   Blank, '1',   Left,  103);
M(103,   Blank, Blank, Left,  104); { Write blank between x and y }
M(104,   Blank, '1',   Left,  105); { Write x }
M(105,   Blank, '1',   Left,  106);
M(106,   Blank, Blank, Right, 107); { Move head on leftmost 1 of x }
{**********************************************************************************}
if Present_State = 107 then begin

{ Dump the initial tape and start the computation. }
Dump_Tape; writeln; Present_State := 0;
end else if (not Configuration_Found) then begin

{ Dump the final tape and halt the computation. }
Dump_Tape; writeln; Configuration_Found := true; Stopped := true;
end;
end;
end; {Turing_Machine}

function  Scanned_Symbol_On_Ends_Of_Empty_Tape(D: Side_Type) : Symbol_Type;

begin Scanned_Symbol_On_Ends_Of_Empty_Tape := '?'; end;
procedure Write_On_Ends_Of_Empty_Tape(D: Side_Type; WS : Symbol_Type); begin end;
function  Head_Position_On_Ends_Of_Empty_Tape : Head_Position_Type;

begin Head_Position_On_Ends_Of_Empty_Tape := No_Head; end;
procedure Set_Head_Position_On_Empty_Tape(P : Head_Position_Type); begin end;

procedure Move_On_Empty_Tape(D : Side_Type; Q : State_Type); begin end;
procedure Dump_Empty_Tape; begin end;
procedure M_On_Empty_Tape(Q : State_Type; S : Symbol_Type;

WS : Symbol_Type; D : Side_Type; NQ : State_Type); begin end;

begin {Simulate_Turing_Machine}

Present_State := 100; Stopped := false;
Turing_Machine(Right, true, Scanned_Symbol_On_Ends_Of_Empty_Tape,

Write_On_Ends_Of_Empty_Tape, Head_Position_On_Ends_Of_Empty_Tape,
Set_Head_Position_On_Empty_Tape, Dump_Empty_Tape, M_On_Empty_Tape);
end. {Simulate_Turing_Machine}

Execution of the above program leads to the following initial and final
configurations of the Turing machine :

 _[1] 1 _ 1 1 1 _ _ _ _ _
 _[1] 1 1 1 1 _ _ _ _ _ _

The program can be easily modified to simulate any Turing machine. \Delta 

7 . 4 . 2 . 5 . 3 Languages with no sound and relatively complete Hoare

logi c

THEOREM  CLARKE [1977]  Non-existence of Hoare logics for Clarke's languages (169)
The Hoare logic for Clarke's languages (166) is not relatively complete in the class of
all expressive interpretations.

Proof

By (167) and (168) there exists no sound and relatively complete Hoare logic for
languages with features (166) since for finite domains D,   can be enriched by finitely
many constant symbols denoting elements of D so that D is Herbrand definable. \Delta 

First order logic is not expressive for Clarke's languages because their control
structure is very complex. LEIVANT & FERNANDO [1987] gives another proof of (169) using
lambda calculus (for a variant of Clarke's languages). They also exhibit a programming
language whose control structure is trivial (the language consists of the single program
C = (y := x; (~(y = 0) * y := x + y)) where (0, +) is a torsion-free Abelian group) and
yet for which no relatively complete logic exists in the sense of Cook. The idea is that
the notion of torsion-free group ( \Delta  n >= 1. \Delta  x. (x ` 0) \Upsilon  (n.x ` 0), where 1.x is x and
(n + 1).x abbreviates (n.x) + x) is not finitely axiomatizable in first-order logic
(BARWISE [1977], proposition 2.2 ) but is completely captured by {x  ` 0}C{false}. In this
case the poverty of the language is precisely what permits certain interpretations to be
expressive, interpretations which would not be expressive had the program constructs
been used more freely. This result is not in contradiction with relative completeness
(156) which holds for   as defined by (1) and (13).

LIPTON [1977] proved a form of converse of (167) further extended by  CLARKE,
GERMAN & HALPERN [1984] , GERMAN & HALPERN [1983]  and URZYCZYN [1983] who showed
that for a deterministic acceptable programming language   (see the long definition
in CLARKE, GERMAN & HALPERN [1983]  or GRABOWSKI [1984] and know that almost all
Algol-like programming languages are acceptable ( CRASEMANN & LANGMAACK [1983] ))
with recursion, the relative completeness of Hoare logic in the class of expressive and
Herbrand definable interpretations is equivalent to the condition that the halting problem
for the programming language must be decidable for finite interpretations.
GRABOWSKI [1984] proved that the requirement of Herbrand definability can be dropped
in the case of partial correctness. The result of  CLARKE, GERMAN & HALPERN [1983]  also
holds for total correctness but GRABOWSKI [1985] and GRABOWSKI & HUNGAR [1988] proved
that it cannot essentially be strengthened.

7 . 4 . 2 . 6 Nonstandard semantics and logical completeness

 Observe that soundness (129) is proved for all interpretations  I satisfying all
theorems of Th whereas completeness in the sense of Cook (156) is relative to a given
expressive interpretation I satisfying all theorems of Th and not for all interpretations I'
with theory Th (i.e. such that Th is exactly the subset of formulae of   which are true
for  I'). However this second understanding fits better with  FLOYD [1967a] and
HOARE [1969] original idea that Hoare logic defines the semantics of the program where
Th is the specification of the operations invoked in the program. It follows that the lack
of a general completeness theorem for a sound Hoare logic implies that the operational
semantics of the programming language is not the semantics about which the logic is
reasoning. This remark has motivated two rather different perspectives on completeness
theorems: the first (ANDRE'KA & NE'METI [1978], GERGELY & U'RY [1978], ANDRE'KA, NE'METI &
SA IN [ 19 79]  [19 81]  [19 82 ] ,  B E R G S T R A   &   T U C K E R  [ 1 9 8 4 ] ,  B I R O'   [ 1 9 8 1 ] ,
CSIRMAZ [1980] [1981a] [1981b],  GERGELY & U'RY [1980], HORTALA'-GONZA'LEZ &  RODRI'GUEZARTALEJO [1985], MAKOWSKY & SAIN [1986], NE'METI [1980] , SAIN [1985]) consists in
considering "explicit time semantics" or "nonstandard semantics" which can permit of
transfinite execution traces, the second ( BERGSTRA & TUCKER [1982b] [1982c] ) consists in
considering another notion of completeness called  logical completeness  such that

 ' Th is logically complete if and only if any partial correctness formula H which is
valid in  all models of the specification Th is provable in   ' Th (that is  ^I  tt(I) ffl

pr(Th)) whereas CSIRMAZ & HART [1986] only consider finite models. Another variant
of the notion of incompleteness is studied in RODRI'GUEZ-ARTALEJO [1985]. Soundness and

completeness of Hoare logic is studied from an algebraic point of view in
WECHLER [1983].

8 . Complements on Hoare logic
8 . 1 Data structures

The assignment axiom (97) is correct for simple variables but cannot be used to
handle all data structures. For example, considering one dimensional arrays, we could
deduce {1 = 1} T[T[2]] := 1 {T[T[2]] = 1} from the assignment axiom (97) and
since  (T[1] = 2 \Pi  T[2] = 2)  \Upsilon   (1 = 1) we deduce that {T[1] = 2  \Pi  T[2] = 2}
T[T[2]] := 1 {T[T[2]] = 1} from the consequence rule (102) but this is not correct.

* One way to handle arrays correctly is to understand the value of an array T as a
function T :  dom T ff rng T where dom T is the domain of its indexes and  rng T is the
domain of its elements and to consider assignments to an element as a modification of
the whole array ( McCARTHY [1962], HOARE & WIRTH [1973] , MANNA & WALDINGER [1981] ).
For example from {T = t  \Pi  (t(2) = 2)  \Upsilon  (t(1) = 1)} we derive that after assignment
T[T[2]] := 1 we have {T = t[t(2)  \Xi  1] \Pi  (t(2) = 2)  \Upsilon  (t(1) = 1)} so that T[T[2]] =
t[t(2) \Xi  1](t[t(2) \Xi  1](2)) = t[t(2)  \Xi  1](t(2) = 2  ff 1 fi t(2)) = ( t(2) = 2  ff t(1) fi 1 ) =
1.

* Another way to handle arrays correctly is to understand them as a collection of
simple variables with possible aliases ( IGARASHI, LONDON & LUCKHAM [1975] ). For
example,  DE BAKKER [1980] suggests the following assignment axiom for subscripted
variables:

{P[T[E1] \Xi  E2]} T[E1] := E 2 {P} (170)
with a refinement of substitution such that:

T[I][T[E1] \Xi  E2] = ( I = E 1 ff E2 fi T[I] ) (171)
and the case when an arbitrary expression stands for I is not handled. By way of
example, we prove that {(T[2] = 2) \Upsilon  (T[1] = 1)} T[T[2]] := 1 {T[T[2]] = 1}. We have
(T[T[2]] = 1)[T[T[2]] \Xi   1] = ( \Lambda  I. T[I]  =  1  \Pi  T[2] = I)[T[T[2]]  \Xi   1] = ( \Lambda  I. (I =
T[2] ff 1 fi T[I]) = 1  \Pi  (2 = T[2]  ff 1 fi T[2]) = I). Now the last formula is implied by
(T[2] = 2) \Upsilon  (T[1] = 1) since if T[2] = 2 holds then we choose I = 1 else we choose
I = T[2]. Thus by the consequence rule and axiom (170) we get the desired result.

* Axioms of assignment applicable to multi-dimensional arrays or pointers to linked
data structures are given in  DEMBINSKI & SCHWARTZ [1976], CARTWRIGHT & OPPEN [1978]
[1981], GRIES [1978], GRIES & LEVIN [1980], JANSSEN & VAN EMDE BOAS [1977] , KOWALTOWSKI

[1977], LUCKHAM & SUZUKI [1979] , MANNA & WALDINGER [1981] , MORRIS [1982], PRATT [1976] ,
SCHWARTZ & BERRY  [1979]. One can also consult  HOARE [1972a] , COOK & OPPEN [1975] ,
OPPEN & COOK [1975], JONES [1980] , TIURYN [1985]  and HOARE, HE JIFENG & SANDERS [1987]
on proving partial correctness properties of programs with user-defined data types and
BURSTALL [1972] and NELSON [1983] for the special case of linear lists.

8 . 2 Procedures

First we define the syntax and relational semantics of recursive parameterless
procedures. Then we consider partial correctness proofs based upon computation
induction (a generalization of Scott induction) which leads to Hoare's recursion rule.
Since this only rule is not complete, we consider Park's fixpoint induction, which,
using auxiliary variables, can be indirectly transcribed into Hoare's rule of adaptation.
Then these rules are generalized for value-result parameters and numerous examples of
application are provided. References to the literature are given for variable parameters
and procedures as parameters.

8 . 2 . 1 Recursive parameterless procedures
8 . 2 . 1 . 1 Syntax and relational semantics of a parameterless

procedural language

Let us now consider programs of the form Pn :: C 1; C 2 which consists of a
command C2 calling a single recursive parameterless procedure Pn with body C1:

DEFINITION  Syntax of a parameterless procedural language (172)

Pn : Procedure names (.1)
Pg : Programs (.2)

Pg ::= Pn :: C1; C2

C : Commands (.3)

C ::= skip | X := E | X := ? | (C1; C2) | (B ff C1 fi C2) | (B * C) | Pn

The relational semantics  Pn of a procedure Pn is a relationship between states
such that a call of Pn in state s leads (if execution of such a call does terminate) to a state
s' such that  <s, s'> \Omega  Pn. When a command C contains a procedure call, the relational
semantics of the command can only be defined if the semantics of the procedure is
known. Therefore, we define the semantics  C of a command C as a function of the
semantics r of the procedure Pn :: C 1. The definition of C(r) = I C (r) is similar to (19)

but for the fact that we must define the effect of a procedure call Pn (r) = r and that of a

program: Pn :: C1; C2 = C2(r) where r is the semantics of procedure Pn. If procedure
Pn is not recursive then its semantics is simply r =  C1(o/). If it is recursive, the
definition is circular since r =  C1(r). To avoid paradoxes, we must prove that this
equation has a solution and, if it is not unique, we must specify which one is to be
considered. To do this, observe that P(S x S) is a complete lattice for the partial ordering
ffl where o/ is the infimum,  S x S is the supremum,  ' is the least upper bound and  ^ is
the greatest lower bound. Also  C1 is monotone i.e. r  ffl r' \Upsilon  C1(r) ffl C1(r'). Therefore,

according to TARSKI [1955], C1 has a least fixed point lfp C1 = ^{r \Omega  P(S x S) : C1(r) ffl r}
such that lfp C1 = C1(lfp C1) and r = C1(r) \Upsilon  (lfp C1 ffl r). The semantics of procedure
Pn is chosen to be lfp C1 (this could be justified with respect to an operational semantics
as in (19), see DE BAKKER [1980, Ch. 5]). We get the following:

DEFINITION after SCOTT & DE BAKKER [1969], DE BAKKER & DE ROEVER [1972]

Relational semantics (173)
I :   ff (P(S x S) ff P(S x S)) (.1)

skip (r) = {<s, s> : s \Omega  S} (.2)
X := E (r) = {<s, s[X  \Xi  E(s)]> : s  \Omega  S} (.3)
X := ? (r)  = {<s, s[X  \Xi  d]> : s  \Omega  S \Pi  d \Omega  D} (.4)
(C1 ; C2) (r) = C1 (r) * C2 (r) (.5)

(B ff C1 fi C2) (r) = (B * C1 (r)) ' (~B * C2 (r)) (.6)
(B * C) (r) = (B * C (r))* _~B (.7)
Pn (r) = r (.8)
Pn :: C1; C2 = C2(lfp C1) (.9)

(For a version of (173) taking termination into account see HITCHCOCK & PARK [1973], DE
BAKKER [1976] , DE ROEVER [1976] , PLOTKIN [1976] , MAJSTER-CEDERBAUM [1980]  and APT &
PLOTKIN [1986] . A semantics of nondeterministic recursive programs based upon
fixpoints of relations on functions rather than fixpoints of functions on relations is
presented in PLAISTED [1986]).

The recursive programming language (172) with its relational semantics (173) is
strictly more powerful than the iterative language (1) with its semantics (19) since, for
example, when the data space D is finite,the first may use an unbounded storage space
while the second may not (the comparison is pursued in  KFOURY [1983] , KFOURY &
URZYCZYN [1985]).

8 . 2 . 1 . 2 The recursion rule based upon computational induction

 To prove partial correctness, we need, in addition of theorems (76), means of
proving  {p}lfp C1{q} or, more generally, of proving a property P( lfp F) of the least
fixpoint lfp F of a monotone function F on a complete lattice L:

LEMMA   SCOTT & DE BAKKER [1969]   Computation induction  (174)

If <L, <=, ', ^, ss> is a complete lattice and F : L ff L is monotone then
[P(ss) \Pi  \Delta  X. P(X) \Upsilon  P(F(X))  \Pi  \Delta  !. \Delta  X \Omega  ! ff L. (\Delta  3 < !. P(X3)) \Upsilon  P(' 3 < ! X3)] \Upsilon 

P(lfp F)

Proof

lfp F is one of the elements of the transfinite sequence X 0 = ss, ..., X! = F(X!-1)
for successor ordinals  !, ..., X ! = '  3 < ! X3 for limit ordinals  ! (see  COUSOT &
COUSOT [1979] ). Therefore we can prove that P( lfp F) holds by proving  \Delta  !. P(X !),
which, by transfinite induction, is implied by P( ss) \Pi   \Delta  X. P(X)  \Upsilon   P(F(X))  \Pi 
\Delta   ! . \Delta   X.  (\Delta   3  < ! .  P (X3 ))  \Upsilon   P( '  3  <  !  X 3). \Delta 

Computation induction is a generalization of Scott induction (also called computational
induction in MANNA, NESS & VUILLEMIN [1972] ) which corresponds to the particular case
when F is upper-continuous (if so  lfp F = XOE where OE = |   |) and P is admissible (i.e.
\Delta   X  \Omega     ff  L. ( \Delta   n \Omega    . P(Xn))  \Upsilon   P( '  n \Omega      Xn)) so that (P( ss) \Pi   \Delta  X. P(X)  \Upsilon 

P(F(X)) \Upsilon  P(lfp F). When specialized to the partial correctness proof {p}lfp C1{q} of a
recursive procedure Pn :: C 1, computation induction leads to the following theorem
where the assumption  \Delta   < s, s' >  \Omega   r.  \Delta   v fl  V a r ( C 1). s(v) = s'(v)  \Pi   \Delta   d  \Omega   D .
<s[v \Xi  d], s'[v  \Xi  d]> \Omega  r states that the relational semantics r of C 1 is without sideeffects, more precisely that the variables v not appearing in C1 cannot be modified and
can have any value during execution of C1:

THEOREM  Partial correctness proof of procedures by computation induction (175)
[\Delta  r \Omega  P(S2).

(\Delta  <s, s' > \Omega  r.  \Delta  v fl Var(C1). s(v) = s'(v)  \Pi  \Delta  d  \Omega  D. <s[v \Xi  d], s'[v  \Xi  d]> \Omega  r)
\Upsilon  ({p}r{q} \Upsilon  {p}C1(r){q})]
\Upsilon  {p}lfp C1{q}

Proof

We prove ( \Delta  <s, s'> \Omega  lfp C1. \Delta  v fl Var(C1). s(v) = s'(v)  \Pi  \Delta  d \Omega  D. <s[v \Xi  d],
s'[v \Xi  d]> \Omega  lfp C1 \Pi  {p}lfp C1{q}) by computation induction (174). ( \Delta  <s, s'> \Omega  o/.
\Delta   v  fl  Var(C1). s(v) = s'(v)  \Pi   \Delta  d  \Omega  D . <s[v \Xi   d], s'[v  \Xi   d]> \Omega   o/  \Pi   {p}o/{q}) is
obviously true. If, by induction hypothesis,  \Delta  <s, s'> \Omega  r. \Delta  v fl Var(C1). s(v) = s'(v)  \Pi 
\Delta  d  \Omega  D . <s[v \Xi   d], s'[v  \Xi   d]> \Omega  r \Pi  {p}r{q} is true then  {p}C1(r){q} holds by

hypothesis of theorem (175) and we can prove  \Delta  <s, s'> \Omega  C1(r).  \Delta  v fl Var(C1).
s(v) = s'(v)  \Pi  \Delta  d \Omega  D. <s[v \Xi  d], s'[v  \Xi  d]> \Omega  C1(r) by structural induction on the
syntax (172) of C 1. Finally  \Delta  !. \Delta  r. ( \Delta  3 < !. (\Delta  <s, s'> \Omega  r3. \Delta  v fl Var(C1). s(v) =
s'(v)  \Pi   \Delta  d \Omega   D .  < s[v \Xi   d], s'[v  \Xi   d]>  \Omega   r3)  \Pi   (p  *  r3 ffl  S  x q))  \Upsilon   (( \Delta   < s, s'>  \Omega 
'  3  <  !  r3 .  \Delta   v  fl  V a r ( C 1 ). s(v) = s'(v)  \Pi   \Delta  d  \Omega   D .  < s [ v  \Xi   d], s'[v  \Xi   d ] >  \Omega 
'  3 < ! r3 )  \Pi  p * ('  3 < ! r3) ffl S x q) is again obviously true.  \Delta 

Computation induction (175) can be directly translated into Hoare logic by the
following recursion rule due to HOARE [1971b]:

 Recursive procedure Pn :: C1; :

{P}Pn{Q} |- {P}C1{Q}
-------------------- Recursion rule (176)

     {P}Pn{Q}

This rule of inference in the sense of PRAWITZ [1965] means that "H1, ..., Hn" is a formal
proof of Hn using:

F H   | -   H ' G
' H ------ I

J       H " K

if and only if "H, H1, ..., Hn" is a formal proof of Hn (as defined at (94)) using:

F H ' G
' H -- I

J  H " K

Formally, the metarule (176) can be avoided by transforming the proof system into in
ordinary one as shown by APT [1981a] and AMERICA & DE BOER [1989].

Example  Partial correctness proof by computation induction (177)

The following program terminates with X = n and Y = n ! when initially X = n >=
0:

procedure F; (178)
begin

if X = 0 then Y := 1
         else begin X := X - 1; F; X := X + 1; Y := Y * X; end;
end;
F;

Partial correctness "{ true } F { Y = X ! }" can be proved using (176) as follows:

(a) { true } F {Y = X ! } by induction hypothesis
(b) { true \Pi  X = 0 } Y := 1 { Y = X ! } by (97), (102)
(c) { Y * X = X ! } Y := Y * X { Y = X !} by (97)
(d) { Y * (X + 1) = (X + 1) !} X := X + 1 { Y * X = X } by (97)
(e) { Y = X ! } \Upsilon  { Y * (X + 1) = (X + 1) ! } from Th
(f)  { Y = X ! } X := X + 1 { Y * X = X ! } by e, d,  (102)
(g) { true } X =: X - 1 { true } by (97)
(h) { true \Pi  ~(X = 0) } (((X := X - 1; F); X := X + 1); Y := Y * X) { Y * X = X ! } by g, a, f, c,

(99), (102)
(i) { true } (X = 0 ff Y := 1 fi (((X := X - 1; F); X := X - 1); Y := Y * X)) { Y = X ! } by b, h, (100)
(j) { true } F { Y = X ! } by a, i, (175)

Observe that the recursion rule (176) is powerful enough to prove "{true} F {Y =
X !}" because this conclusion (j) can be used as induction hypothesis (a). This is not
the case for proving "{X = n} F {X = n  \Pi  Y = n !}" since then we need induction
hypothesis  "{X = n - 1} F {X = n - 1 \Pi   Y = (n - 1) !}"  which  cannot  be
directly derived from the conclusion "{X = n} F {X = n  \Pi  Y = n !}" using the
consequence rule (102). Hence a proof method using (175) or (176) only is not
relatively complete (APT [1981a]). \Delta 

Various proof rules, known as  copy-rule induction , have been proposed by
GORELICK [1975], CLARKE [1977], LANGMAACK & OLDEROG [1980] , APT [1981], OLDEROG [1981]
[1983b] [1983c], TRAKHTENBROT, HALPERN & MEYER [1983] to extend the recursion rule (176)
for higher-order procedure calls.

8 . 2 . 1 . 3 The rule of adaptation based upon fixpoint induction

 Since the proof method (175) based upon computation induction is not
complete, we come back to the problem of proving a property P( lfp F) of the least
fixpoint lfp F of a monotone function F on a complete lattice L. When P(lfp F) is of the
form X ^ lfp F <= Y, which is the case for {p}lfp C1{q}, we can use fixpoint induction:

LEMMA  PARK [1969]   Fixpoint induction (179)

If <L, <=, ', ^, ss> is a complete lattice and F : L ff L is monotone then

(X ^ lfp F <= Y) \Phi  (\Lambda  Z \Omega  L. F(Z) <= Z \Pi  X ^ Z <= Y)
Proof

For \Upsilon  we can choose Z =  lfp F so that F(Z) = Z and for  , we have (F(Z) <= Z) \Upsilon 
(lfp F <= Z) since lfp F = ^{Z : F(Z) <= Z} by TARSKI [1955] whence X  ^ lfp F <= X ^ Z <=
Y. \Delta 
When specialized to the partial correctness proof {p}lfp C1{q} of a recursive procedure
Pn :: C1, fixpoint induction (179) leads to the following proof method (a version of

which is used in  COURCELLE [1985]  to establish the partial correctness of clausal
programs):

THEOREM after PARK [1969], MANNA & PNUELI [1970]  Partial correctness proof of  (180)

* ( \Lambda  r \Omega  P(S2). C1(r) ffl r \Pi  {p}r{q} ) \Upsilon  {p}lfp C1{q} (.1)

* {p}lfp C1{q} \Upsilon  ( \Lambda  r \Omega  P(S2). C1(r) ffl r \Pi  {p}r{q} (.2)

\Pi  \Delta  <s, s'> \Omega  r. \Delta  v fl Var(C). ( s(v) = s'(v))
\Pi  \Delta  d  \Omega  D. <s[v \Xi  d], s'[v  \Xi  d]> \Omega  r )
Proof

We have ( \Lambda  r \Omega  P(S2). C1(r) ffl r \Pi  {p}r{q}) \Upsilon  (\Lambda  r \Omega  P(S2). C1(r) ffl r \Pi  (p x S) ^
r ffl S x q)   \Upsilon  ((p x S) ^  (lfp C 1) ffl S x q)  \Upsilon   (p  * (lfp C 1) ffl S x q)  \Upsilon  {p}lfp C 1{q}.
Reciprocally, if  {p}lfp C1{q} then obviously  C1(r)  ffl r \Pi  {p}r{q} for r =  lfp C1.
Moreover we can prove P( lfp C1) where P(r) = ( \Delta  <s, s'> \Omega  r. \Delta  v fl Var(C). (s(v) =
s'(v))  \Pi  \Delta  d \Omega  D. <s[v \Xi  d], s'[v  \Xi  d]> \Omega  r) by computation induction (174). P(o/) is
obvious. Assuming P(r) we can prove P(C1(r)) by structural induction on the syntax of
C 1. For example P( Pn(r)) = P(r) [by (173.8)] = tt [by induction hypothesis].
P(X := E(r)) =  \Delta  v fl {X}  ' Var(E). s(v) = s[X  \Xi   E(s)](v)  \Pi  \Delta  d  \Omega  D . <s[v \Xi  d],
s[X \Xi  E(s)][v \Xi  d]> \Omega  {<s", s"[X  \Xi  E(s")]> : s"  \Omega  S} is true, etc. Finally  \Delta  !. \Delta  r.
(\Delta  3 < !. P(r3))  \Upsilon   P( '  3 < ! r3) is obvious.  \Delta 

Example  Partial correctness proof by fixpoint induction I (181)

Partial correctness "{ X = n } F { X = n  \Pi  Y = n ! }" of program (178) can be
proved using (180.1) as follows:

(a) r = {<s, s'> : s'(X) = s(X)  \Pi  s'(Y) = s(X) !}  by definition of r
(b) Y := 1(r)  ffl {<s, s[Y  \Xi  0 !]> : s  \Omega  S} by (173.2)
(c) X = 0 * Y := 1(r) ffl r by b, a
(d) X := X - 1(r)  ffl {<s, s[X  \Xi  s(X) - 1]> : s  \Omega  S} by (173.2)
(e) F(r) ffl {<s, s'> : s'(X) = s(X)  \Pi  s'(Y) = s(X) !} by (173.8), a
(f) (X := X - 1; F)(r)  ffl {<s, s'> : s'(X) = s(X) - 1  \Pi  s'(Y) = (s(X) - 1) !}  by d, e, (173.5)
(g) X := X + 1(r)  ffl {<s, s[X  \Xi  s(X) + 1]> : s  \Omega  S} by (173.2)
(h) ((X := X - 1; F); X := X + 1)(r)  ffl {<s, s'> : s'(X) = s(X)  \Pi  s'(Y) = (s(X) - 1) !} by f, g, (173.5)
(i) Y := Y * X(r)  ffl {<s, s[Y \Xi  s(Y) * s(X)]> : s  \Omega  S} by (173.2)
(i) ~X = 0 * (((X := X - 1; F); X := X + 1); Y := Y * X)(r)  ffl r by f, g, (173.5)
(j) (X = 0 ff Y := 1 fi (((X := X - 1; F); X := X - 1); Y := Y * X))(r)  ffl r by c, j, (173.6)
(k) {{s : s(X) = n}} r {{s : s(X) = n \Pi  s(Y) = n !}} by a, (22)
(l) {{s : s(X) = n}} lfp (X = 0 ff Y := 1 fi (((X := X - 1; F);

 X := X - 1); Y := Y * X))  {{s : s(X) = n \Pi  s(Y) = n !}} by j, k, (178.1)
\Delta 

Theorem (180) is not directly expressible in Hoare logic since ( \Lambda  r \Omega  P(S2). C1(r)
ffl r \Pi  {p}r{q}) does not use only formulae of the style p'  ffl q' and  {p'}C'{q'}. To
enforce this, we can let  S'=S2, p' = { <s, s> : s \Omega  S}, C1' = {<<s, s>, <s, s'>> : <s, s'>
\Omega  C1(r)} and q' = r so that  {p'}C1'{q'} = {{<s, s'> : s  \Omega  S}} {<<s, s>, <s, s'>> : <s,
s'> \Omega  C1(r)}  {r} = (p'  * C1' ffl S x q') = ({ <<s, s>, <s,s>> :  <s, s > \Omega  C1(r)}  ffl {<<s0,
s1>, <s, s>> : <s, s> \Omega  r} ) =  C1(r) ffl r. In this translation the relationship r between
states before and after the procedure call is expressed using predicates upon states but
the state space S has been changed into  S2 (as in MANNA & PNUELI [1974] ). To remain in
the spirit of traditional Hoare logic, it is better to use logical auxiliary variables not
appearing in the program to memorize the value of the programming variables before the
procedure call (the importance of these auxiliary variables in correctness proofs for
recursive procedures was first realized by GREIBACH [1975] and GORELICK [1975] and later
thoroughly investigated in GALLIER [1978] [1981], APT, BERGSTRA & MEERTENS [1979], APT &
MEERTENS [1980], MEYER & HALPERN [1980]  and APT [1981b]). Let X* = <X1, ..., Xn> be the
vector of variables  Var(C1) = {X 1, ..., X n} appearing in command C 1. Following
OLDEROG [1983] , we write X * || x* whenever X * = <X1, ..., Xn>, x* = <x1, ..., xm>, m =
n and {X1, ..., Xn} ^ {x1, ..., xm} = o/ and extend all notations to vectors, for example
X* = x* means X 1 = x1 \Pi  ... \Pi  Xn = xm, s(X*) stands for  <s(X1),...,s(Xn)>, s[X* \Xi  x*]
means s[X 1 \Xi  x1]...[Xn \Xi  xn], {X *} is {X 1, ..., X n}, etc. Then we let p' = {s  \Omega  S :
s(X*) = s(x *)} and q' = {s'  \Omega  S : <s'[X* \Xi  s'(x*)], s'> \Omega  r} where {X *} =  Var(C1) and
X* || x* so that {p'}C1(r){q'} implies C1(r) ffl r. More precisely, we have:

DEFINITION  OLDEROG [1983]   Non-interference (182)
X* || x * = [X * = <X1, ..., X n> \Pi  x* = <x1, ..., x m> \Pi  m = n  \Pi  {X1, ..., X n} ^ {x1, ...,
xm} = o/]

THEOREM  Partial correctness proof of procedures by fixpoint induction II (183)

\Lambda  r \Omega  P(S2). C1(r) ffl r \Pi  {p}r{q}
\Phi 

\Lambda  p', q'  \Omega  P(S).

\Delta  r' \Omega  P(S2).

(\Delta  <s, s'> \Omega  r'. \Delta  v fl Var(C1). s(v) = s'(v)

\Pi  \Delta  d  \Omega  D. <s[v \Xi  d], s'[v  \Xi  d]> \Omega  r')
\Upsilon  ({p'}r'{q'} \Upsilon  {p'}C1(r'){q'})

\Pi  \Delta  s \Omega  p. \Delta  d* \Omega  D*. (\Delta  s' \Omega  S. s'[X * \Xi  s(X*)] \Omega  p'

\Upsilon  s'[X * \Xi  d*] \Omega  q')  \Upsilon  s[X * \Xi  d*] \Omega  q
where {X*} = Var(C1) and X* || x*

(The assertion  \Delta  <s, s' > \Omega  r'.  \Delta  v fl Var(C1). s(v) = s'(v)  \Pi  \Delta  d  \Omega  D . <s[v \Xi   d] ,
s'[v \Xi  d]> \Omega  r' states that the relational semantics r' of C 1 is without side-effects, more
precisely that the variables v not appearing in C1 cannot be modified and can have any

value during execution of C 1. We have  {p'}lfp C1{q'} so that in the assertion  \Delta  s \Omega  p.
\Delta  d* \Omega  D *. ( \Delta  s'  \Omega  S. s'[X * \Xi   s(X*)]  \Omega  p'  \Upsilon   s'[X * \Xi   d*] \Omega  q')  \Upsilon   s[X * \Xi   d*] \Omega  q, the
assumption  \Delta  s' \Omega  S. s'[X * \Xi  s(X*)] \Omega  p' \Upsilon  s'[X * \Xi  d*] \Omega  q' states that  <p', q' > is the
specification of the procedure Pn :: C1. More precisely any terminating execution of C1

started with initial values s(X *) of the variables X * satisfying p' must terminate with
final values d * of X * satisfying q' and this whatever the possible values s'(v) of the
variables v fl {X*} not appearing in the procedure body C1 may be. The assertion states

that any execution of Pn started with values s(X*) of X* and terminated with values d* of
X* satisfying specification  <p', q' > must satisfy the postcondition q whenever the
precondition p is satisfied.)

Proof

* For  \Upsilon , assume  C1(r)  ffl r \Pi  {p}r{q} then  {p}lfp C1{q} by (180.1) so that by
(180.2) we can assume that  \Delta  <s, s' > \Omega  r.  \Delta  v  fl Var(C1). s(v) = s'(v)  \Pi  \Delta  d  \Omega  D .
< s[v  \Xi   d], s'[v  \Xi   d ]>  \Omega   r . Let p' = {s  \Omega   S  : s(X *) = s(x *)} and q' = {s'  \Omega   S  :
<s'[X* \Xi  s'(x*)], s'> \Omega  r} where {X *} = Var(C1) and X * || x*.
(A) Assuming  \Delta  <s, s' >  \Omega  r'.  \Delta  v  fl Var(C1). s(v) = s'(v)  \Pi  \Delta  d  \Omega  D . <s[v \Xi   d],
s'[v \Xi  d]> \Omega  r' and  {p'}r'{q'} we first prove  {p'}C1(r'){q'}.
(a) We first prove that p'  * C1(r') ffl p' * C1(r). We have (p'  * r' ffl S x q') [by (124)]
so that  \Delta   s, s' \Omega   S . (s(X *) = s(x *)  \Pi   < s, s' >  \Omega   r')  \Upsilon   < s'[ X* \Xi   s'(x*)], s' >  \Omega   r. It
follows that if  <s, s'> \Omega  r'  then  <s[x* \Xi   s(X*)], s'[x * \Xi   s(X*)]> \Omega   r' [since  \Delta  v  fl
Var(C1).  \Delta  d  \Omega  D . <s[v \Xi   d], s'[v \Xi   d]> \Omega  r'] in which case s[x * \Xi   s(X*)](X) =

s[x* \Xi   s(X*)](x*) = s(X *) [by (121) since X * || x* hence {X *} ^ {x*} = o/] so that
<s'[x* \Xi  s(X*)][X* \Xi  s'[x* \Xi  s(X*)](x*)], s'[x * \Xi  s(X*)]> \Omega  r and, after simplification
by (121),  <s'[x* \Xi  s(X*)][X* \Xi  s(X*)], s'[x * \Xi  s(X*)]> \Omega  r hence  <s, s'> \Omega  r since  \Delta 
v fl Var(C1). s'(v) = s(v). We conclude r'  ffl r, whence by monotony  C1(r') ffl C1(r) so
that p' * C1(r') ffl p' * C1(r).
(b) Then we prove that  {p'}C1(r){q'} is true. We have  {p'}r{q'} = (p'  * r ffl S x q')
[by (124)] = ( \Delta  <s, s'> \Omega  r : s(X *) = s(x *) \Upsilon  <s'[X* \Xi  s'(x*)], s' > \Omega  r ) which is true
since  <s, s'> \Omega   r implies  <s'[X* \Xi   s(X*)], s'> \Omega   r [since  \Delta  v  fl Var(C1).  \Delta  d  \Omega  D .

<s[v \Xi   d], s'[v  \Xi   d]> \Omega  r] hence  <s'[X* \Xi   s(x*)], s'> \Omega  r [since s(X *) = s(x *)]. We
conclude {p'}C1(r){q'} since C1(r) ffl r.
(c)  Finally,  {p'}C1(r'){q'} is true since (b) implies (p'  * C1(r) ffl S x q'), whence
p' * C1(r') ffl S x q' by (a) so that  {p'}C1(r'){q'} holds.
(B) Let be given s  \Omega   p and d * \Omega  D *. Assuming ( \Delta  s'  \Omega  S. s'[X * \Xi   s(X*)]  \Omega  p'  \Upsilon 
s'[X* \Xi   d *]  \Omega   q') we prove that s[X * \Xi   d *]  \Omega   q. We have s[x * \Xi   s ( X *)](X*) =
s[x* \Xi   s(X*)](x*) = s(X *) [by (121) since X * || x * hence {X *} ^ {x*} = o/] so that
s[X* \Xi   s(X*)]  \Omega  p'. By definition of p', this implies s(X *) = s[X * \Xi   s(X*)](X*) =
s[X* \Xi  s(X*)](x*) = s(x *) [since X * || x*]. Also s[X * \Xi  s(X*)] \Omega  p' implies s[X * \Xi  d*] \Omega 
q' that is  <s[X* \Xi  d*][X* \Xi  s[X* \Xi  d*](x*)], s[X * \Xi  d*]> \Omega  r , after simplification by

(121),  <s[X* \Xi  s(x*)], s[X * \Xi  d*]> \Omega  r hence  <s[X* \Xi  s(X*)], s[X * \Xi  d*]> \Omega  r [since
s(X*) = s(x *)] so that  <s, s[X* \Xi  d*]> \Omega  r [by (121)] whence s[X * \Xi  d*] \Omega  q [since s  \Omega 
p and {p}r{q}].

* For  ,, we have  \Delta  r' \Omega  P(S2). (\Delta  <s, s'> \Omega  r'. \Delta  v fl Var(C1). s(v) = s'(v)  \Pi  \Delta  d \Omega 
D . <s[v \Xi   d], s'[v  \Xi   d]> \Omega  r')  \Upsilon   ({p'}r'{q'} \Upsilon   {p'}C 1(r'){q'}) and (175) which
imply  {p'}lfp C1{q'}. Hence by (180.2) there exists r  \Omega  P(S2) such that  C1(r)  ffl
r \Pi  {p'}r{q'} \Pi   \Delta  <s, s'>  \Omega  r.  \Delta  x  fl Var(C1). (s(x) = s'(x))  \Pi  \Delta  d  \Omega  D . <s[x \Xi   d],
s'[x \Xi   d]> \Omega  r.

To prove  { p } r{ q }  we assume that s  \Omega   p and  < s, s'>  \Omega   r so that s' =
s[X * \Xi   s'(X*)] since  \Delta   x fl  {X *}. s(x) = s'(x). Also  \Delta   s"  \Omega   S . <s"[X* \Xi   s(X *)],
s"[X* \Xi   s'(X*)]> \Omega  r since  \Delta  x fl {X*}.  \Delta  d  \Omega  D . <s[x \Xi   d], s'[x  \Xi   d]> \Omega  r. Hence
{p'}r{q'} implies  \Delta  s" \Omega  S. s"[X * \Xi  s(X*)] \Omega  p' \Upsilon  s"[X * \Xi  s'(X*)] \Omega  q'. Then ( \Delta  s" \Omega 
S. s"[X * \Xi  s(X*)] \Omega  p' \Upsilon  s"[X * \Xi  d*] \Omega  q') where d * = s'(X *) implies s[X * \Xi  d*] \Omega  q
that is s[X * \Xi  s'(X*)] \Omega  q so that, in conclusion, s'  \Omega  q. \Delta 

Theorem (183) can be directly transcribed in Hoare logic using the recursion rule (176)
and the following rule of adaptation (due to MORRIS [19??] and OLDEROG [1983a]) :

                          { P ' } C { Q ' }
------------------------------------------ Rule of adaptation (184)
{\Delta  x*. (\Delta  y*. (P'  \Upsilon  Q'[X * \Xi  x*])) \Upsilon  Q[X * \Xi  x*]}C{Q}

where X* || x* with {x*} ^ Free(P', Q', C, Q) = o/ and {y *} = Free(P', Q') - {X*}
A first version of this rule of adaptation was introduced by  HOARE [1971b] and slightly
extended in IGARASHI, LONDON & LUCKHAM [1975] and ERNST [1977]. OLDEROG [1983a] proved
that it is sound and relatively complete and can replace the substitution rule I, the
invariance rule and the elimination rule of APT [1981a]. Although HOARE [1971b] adaptation
rule is relatively complete,  MORRIS [19??]  and OLDEROG [1983a]  showed that it does not
give the best possible precondition and proposed a strengthened version. An incorrect
version of HOARE [1971b] adaptation rule was designed by  GUTTAG, HORNING & LONDON
[1978] and LONDON, GUTTAG, HORNING & LAMPSON [1978] . The error was pointed out by
GRIES & LEVIN [1980]  who proposed a sound version. A slightly simpler version of the
adaptation rule of  GRIES & LEVIN [1980]  and GRIES [1981] was proposed by  MARTIN [1983]
but BIJLSMA, WILTINK & MATTHEWS [1986] showed that both versions were equivalent. As
pointed out by OLDEROG [1983a] and BIJLSMA, WILTINK & MATTHEWS [1989] the rule of GRIES
& LEVIN [1980]  and  GRIES [1981]  is relatively incomplete. They proposed a sound and
relatively complete variant. Another sound and relatively complete variant was designed
by CARTWRIGHT & OPPEN [1978] [1981].

Example  APT [1981a]  Recursive factorial procedure with global variables (185)

To prove partial correctness { X = n } F { X = n  \Pi  Y = n ! } of procedure F of
program (178) we use the recursion rule (176). Assuming {X = n}  F {X = n  \Pi  Y =
n !} by induction hypothesis, we must prove {X = n - 1} F {X = n - 1  \Pi  Y = (n - 1) !}
for the recursive call of F. By the rule of adaptation (184) where X * = <X, Y>, x* = <x,
y> and y * = <n> we derive { \Delta  x. \Delta  y. (\Delta  n. ((X = n)  \Upsilon  (x = n  \Pi  y = n !)))  \Upsilon  (x = n -
1 \Pi  y = (n - 1) !)} F {X = n - 1  \Pi  Y = (n - 1) !}. By the consequence rule (102), it
remains to show that (X = n - 1)  \Upsilon   (\Delta  x. \Delta   y. (\Delta   n'. ((X = n')  \Upsilon   (x = n'  \Pi  y =
n' !)))  \Upsilon  (x = n - 1  \Pi  y = (n - 1) !)) which is obviously true. Then the correctness
proof of procedure F can be completed as shown by the following proof outline:

procedure F;
begin

{ X = n }
if X = 0 then

{ X = n \Pi  X = 0 } Y := 1 { X = n \Pi  Y = n ! }
else begin

{ X = n \Pi  X ` 0 } X := X - 1; { X = n - 1 }
F;
{ X = n - 1 \Pi  Y = (n - 1) ! } X := X + 1; { X = n \Pi  Y = (n - 1) ! } Y := Y * X;
end;
{ X = n  \Pi  Y = n ! }
end;
{ X = y } F; { X = y \Pi  Y = y ! }

For the main call of procedure F it remains to prove {X = y} F {X = y  \Pi  Y = y !}.
Since (X = y)  \Upsilon   (\Delta  x'.  \Delta  y'. ( \Delta  n. ((X = n)  \Upsilon   (x' = n  \Pi  y' = n !))  \Upsilon   (x' = y  \Pi  y' =
y !))) this follows from the rule of adaptation (184) and the consequence rule (102). \Delta 

Example  DE BAKKER & MEERTENS [1975]   Showing the usefulness of auxiliary
variables (186)

The following program computes 2n0 - 1 when n0 is positive:

procedure F;
begin

{ N = n  >= 0 \Pi  S = s }
if N > 0  then begin

{ N = n > 0 \Pi  S = s } N := N - 1; { N = n - 1 >= 0 \Pi  S = s }
F;
{ N = n - 1 >= 0 \Pi  S = s + 2n-1 - 1 } S := S + 1;  { N = n - 1 >= 0 \Pi  S = s + 2n-1 }
F;
{ N = n - 1 >= 0 \Pi  S = s + 2n-1 + 2n-1 - 1 } N := N + 1;
end;
{ N = n  >= 0 \Pi  S = s + 2 n - 1 }
end;
{ N = n0 >= 0 \Pi  S = 0 } F; { N = n0 >= 0 \Pi  S = 2n0 - 1 }

Its partial correctness proof uses the recursion rule (176) with induction hypothesis { N
= n >= 0 \Pi  S = s } F { N = n  >= 0 \Pi  S = s + 2n - 1 } and the adaptation and consequence
rules (184) and (102) for the main and recursive calls of F. For example {  N = n - 1 >= 0
\Pi  S = s } F { N = n - 1 >= 0 \Pi  S = s + 2n-1 - 1 } follows from the fact that (N = n - 1  >= 0 \Pi 
S = s) implies ( \Delta  n', s'.  (\Delta  n, s. ((N =  n >= 0  \Pi  S =  s) \Upsilon   (n' =  n >= 0  \Pi  s' =  s + 2 n -
1)) \Upsilon  (n' = n - 1 >= 0 \Pi  s' = s + 2n-1 - 1)). The proof proposed in  DE BAKKER & MEERTENS
[1975] uses an infinite pattern of assertions (one for each program step) because in their
proof system one cannot use logical variables (n, s) to relate the different values of the
program variables (N, S) at different stages of the computation so that the invariants
have to express the current value of N and S in terms of their initial values n0 and 0 and

of a representation of the computation history (see  GREIBACH [1975] , GORELICK [1975] ,
GALLIER [1978] [1981] , APT, BERGSTRA & MEERTENS [1979] , APT & MEERTENS [1980] , MEYER &
HALPERN [1980]  and APT [1981b]). \Delta 

8 . 2 . 1 . 4 Hoare-like deductive systems with context-dependent

conditions

Hoare deduction systems are Hilbert-style systems (BARWISE [1977]) where proofs
proceed by induction on the syntax of programs. Since this syntax is specified by a
context-free grammar (AHO, SETHI & ULLMAN [1986]) it is not directly possible to express
context-dependent conditions such as the correspondence between procedure calls and
procedure declarations (specifying which procedure bodies are associated with
procedure names). In fact a formula {P}C{Q} is often relative to declarations of named
objects which can be represented by a mapping from names to objects called an
environment (SCOTT & STRACHEY [1972] ). Therefore, most often, Hoare correctness
formulae have the form  <L | {P}C{Q}> where  L is an environment associating
procedure bodies to procedure names (and informally all procedure names occurring
free in C are declared in  L and there are no procedure names occurring free in  L). The
axioms and rules of inference propagate the environment from declarations to calls:

(187)
 < [Pn  \Xi  C1] | {P} C 2 {Q}  >
------------------------- Rule of programs (.1)
      {P}  Pn  ::  C1; C 2 {Q}

 < L | {P} skip {P}  > Skip axiom (.2)

8 . 2 . 2 Value-result parameters

Let us now consider a procedural language with value-result parameters:
DEFINITION  Syntax of a procedural language with value and/or result parameters (188)

Pn : Procedure names (.1)
Pg : Programs (.2)

Pg ::= Pn (? U ?! V ! W) :: C1; C2

C : Commands (.3)

C ::= skip | X := E | X := ? | (C1; C2) | (B ff C1 fi C2) | (B * C) | Pn (E, Y, Z)

U, V and W are distinct formal parameters of recursive procedure Pn with body C 1
which are respectively passed by value, value-result and result. Therefore a call Pn (E,
Y, Z) with expression E and variables Y and Z as actual parameters consists in creating
new local variables named U, V, W within C1, respectively initialized to the value of E,
to the value of Y and to any value d of  D, then in executing the body C 1 of procedure
Pn (which may modify the values of local variables U, V, W as well as that of the
global variables (but not the ones named U, V and W which are hidden by their local
homonyms)), and finally in assigning the values of the local variables V and W to the
variables Y and Z (in that order) and then in destroying the local variables named U, V,
W:

DEFINITION after DE ROEVER [1974] Relational semantics of value and/or  (189)

result parameter passing

* Pn (E, Y, Z) (r) = (.1)

{<s, s'[U  \Xi  s(U)][V \Xi  s(V)][W \Xi  s(W)][Y \Xi  s'(V)][Z \Xi  s'(W)]> :

\Lambda  d \Omega  D. <s[U \Xi  E(s)][V \Xi  s(Y)][W \Xi  d], s' > \Omega  r}

* Pn (? U ?! V ! W) :: C1; C2 = C2(lfp C1) (.2)

Example (190)

Let Pg be Pn(? A ?! B ! C) :: C1; C2 where C1 is (B := A + B; (A := A + 1; (C :=
A + 1; D := C + 1))) and C 2 is (C3; Pn(A + 1, B, B)) with C 3 = (A := 1; (B := 10; C :=
100)). By (173.3) and (173.5), the relational semantics of the procedure body is  lfp C1
=  C 1(o/) = { <s, s[A \Xi   s(A) + 1] [B  \Xi   s(A) + s(B)]  [C \Xi   s(A) + 2]  [D \Xi   s(A) +
3]> : s \Omega  S}. By (189.1), the semantics of the procedure call is  Pn(A + 1, B, B) = { <s,
s'[A \Xi  s(A)] [B  \Xi  s(B)] [C  \Xi  s(C)] [B  \Xi  s'(B)] [B  \Xi  s'(C)]> : \Lambda  d \Omega  D. <s[A \Xi  A
+ 1(s)] [B  \Xi  s(B)] [C  \Xi  d], s' > \Omega  lfp C1} = { <s, s[D \Xi  s(A) + 4] [B  \Xi  s(A) + 3] > :
s \Omega  S}. The semantics of the initialization part of the program body is C3(lfp C1) = {<s,
s[A \Xi  1] [B  \Xi  10] [C  \Xi  100]> : s \Omega  S} so that the semantics of the program is  Pg =
C2(lfp C1) = { <s, s[A  \Xi  1] [B  \Xi  10] [C  \Xi  100] [D  \Xi  (s[A\Xi 1][B\Xi 10][C\Xi 100](A))

+ 4] [B  \Xi  (s[A\Xi 1][B\Xi 10][C\Xi 100](A)) + 3] > : s  \Omega  S} = { <s, s[A \Xi  1] [C  \Xi  100]
[D \Xi   5] [B  \Xi   4]  > : s  \Omega   S}. \Delta 

This semantics leads to an analog of theorem (183) and then to the following proof rules
(the rule of adaptation (192) could be simplified as in  GRIES & LEVIN [1980], GRIES [1981],
MARTIN [1983] and BIJLSMA, MATTHEWS & WILTINK [1989]  by restricting the use of global
variables, the assignments to value parameters, or assuming that value-result and result
parameters are different):

Recursive procedure Pn (? U ?! V ! W) :: C1;

Recursion rule (191)

{P}  Pn (? U ?! V ! W)  {Q} |- {P} C1{Q}
------------------------------------

     {P}  Pn (? U ?! V ! W)  {Q}

Rule of adaptation (192)

                                            {P'}  Pn(?U, ?!V, !W)  { Q ' }
------------------------------------------------------------------
{\Delta  x*. \Delta  "u, "v, "w, "y, "z, v', w'.

((U = "u) \Pi  (V = "v) \Pi  (W = "w) \Pi  (Y = "y) \Pi  (Z = "z) \Pi 
 (\Delta   m *.

((\Lambda `w.P'[U \Xi  E[U\Xi "u][V\Xi "v][W\Xi "w][Y\Xi "y][Z\Xi "z]][V\Xi "v][W\Xi `w][Y\Xi "y]
[Z\Xi "z])

\Upsilon  (\Lambda  u', y', z'. Q'[X * \Xi  x*] [U  \Xi  u'] [V  \Xi  v'] [W  \Xi  w'] [Y  \Xi  y'] [Z  \Xi  z']))))
 \Upsilon  Q [Z  \Xi  w'] [Y  \Xi  v'] [X * \Xi  x*] [U  \Xi  "u] [V  \Xi  "v] [W  \Xi  "w]}
Pn(E, Y, Z)
{Q}

where:

* X* = Var(E, C1) - {U, V, W, Y, Z} is the value of the global variables (not

used as actual result parameters and not homonyms of actual or formal parameters)
before the call Pn(E, Y, Z),

* x* such that X * || x* are fresh variables denoting the value of X * after the call
Pn(E, Y, Z),

* "u, "v, "w, "y, "z are fresh variables denoting the values of the global
variables U, V, W, Y, Z before the call,

* `w is a fresh variable denoting the undetermined initial value d  \Omega  D of formal
result parameter W when execution of the procedure body C1 is started,

* u', v', w' are the values of the local formal parameters U, V and W after
execution of the procedure body C1 and before the result parameters passing,

* {m*} = Free(P', Q') - ({X*} ' {U, V, W, Y, Z}) is the set of logical variables
used for the specification of the procedure body C1,

* y', z' are the values of the global variables Y and Z after execution of the
procedure body C1 and before the result parameters passing,

* all fresh variables are distinct and do not appear in  Free(P', Q', C 1, E, Q)  '
{U, V, W, Y, Z}.

When reading rule (192) it must be remembered that substitution is left to right that is to
say P[X  \Xi  x][Y \Xi  y] is P'[Y  \Xi  y] where P' = P[X  \Xi  x].

Example  MARTIN [1983] Once used to show the inconsistency of erroneous
procedure- (193)

call proof rules
Assuming {P'} Pn(! Z) {Q'} where P' = true and Q' = ((Z = 1)  \Sigma  (Z = 2)), we
can determine P such that {P} Pn(C) {Q} holds with Q = (C = 2) by the rule of
adaptation (192):
P = \Delta  "w, "z, w'.  ((Z = "w)  \Pi  (C = "z)  \Pi  ((\Lambda  `w. P'[Z  \Xi  `w][C \Xi  "z])

\Upsilon  (\Lambda  z'. Q'[Z  \Xi  w'][C \Xi  z']))) \Upsilon  Q [C  \Xi  w'] [Z  \Xi  "w]
= \Delta  "w, "z, w'.  ((Z = "w) \Pi  (C = "z) \Pi  ((w' = 1) \Sigma  (w' = 2))) \Upsilon  (w' = 2)
= false.
MARTIN [1983]  gives David Gries the credit for this example who used it to
demonstrate the inconsistency of a number of procedure-call proof rules. The Euclid
proof rule (LONDON, GUTTAG, HORNING, LAMPSON, MITCHELL & POPEK [1978]) , for example,
gives P = true . \Delta 

Example  BIJLSMA, MATTHEWS & WILTINK [1989]   On the use of auxiliary variables (194)

Consider a procedure Pn(? X ! Z) for rounding the real number X to a nearby
integer Z (such as Z := floor(X), Z := ceil(X) or Z = round(X)). The specification
{P'}Pn{Q'} where P' = (m  <= X <= m + 1) and Q' = (Z = m  \Sigma  Z = m + 1) states that Z
is obtained from X by rounding any non-integer X either up or down to an integer, and
by using X itself if X happens to be an integer. Now consider the call Pn(A, C) with
postcondition Q = (C = 0). The corresponding precondition P such that {P} Pn(A, C)
{Q} holds is given by the rule of adaptation (192) as:
P = \Delta  "u, "w, "z, w'.

((X = "u) \Pi  (Z = "w) \Pi  (C = "z) \Pi 
 (\Delta   m .

((\Lambda  `w. P'[X  \Xi  A[X\Xi "u][Z\Xi "w][C\Xi "z]] [Z  \Xi  `w] [C  \Xi  "z])

\Upsilon  (\Lambda  u', y', z'. Q'[X  \Xi  u'] [Z  \Xi  w'] [C  \Xi  z']))))
  \Upsilon  Q [C  \Xi  w'] [X  \Xi  "u] [Z  \Xi  "w]
= \Delta  w'. ( \Delta  m. ((m <= A <= m + 1)  \Upsilon  (w' = m  \Sigma  w' = m + 1)))  \Upsilon  (w' = 0)
= (A = 0).

This example was designed by  BIJLSMA, MATTHEWS & WILTINK [1989]  to show that
the precondition in  GRIES [1981, Th. 12.4.1 p. 161]' s rule (which would be P = false) is not
the weakest that can be inferred solely from the procedure's specification in cases when
the specification involves auxiliary logical variables. \Delta 

Example  Homonym formal and actual parameters (195)

Let Pg be Pn(? A ?! B ! C) :: C1; C2 where C1 is (B := A + B; (A := A + 1; (C :=
A + 1; D := C + 1))) and C 2 is (C3; Pn(A + 1,B, B)) with C 3 = (A := 1; (B := 10; C :=
100)). By (97), the composition rule (99) and the consequence rule (102) we have {A =
a \Pi  B = b  \Pi  C = c} C 1 {A = a  +  1  \Pi  B = a + b  \Pi  C = a + 2  \Pi  D = a + 3} whence by

the recursion rule (191 we derive {P'} Pn (? A ?! B ! C)  {Q'} where P' is (A = a  \Pi  B =
b \Pi  C = c) and Q' is (A = a + 1  \Pi  B = a + b  \Pi  C = a + 2  \Pi  D = a + 3). Let Q be (A =
a \Pi  B = b \Pi  C = c \Pi  D = d). In order to derive the corresponding precondition P by the
rule of adaptation (192) we observe that X * = <D>, m* = <a, b, c> whence:
P = (\Delta  x. \Delta  "u, "v, "w, "y, "z, v', w'.

((A = "u) \Pi  (B = "v) \Pi  (C = "w) \Pi  (B = "y) \Pi  (B = "z) \Pi 

(\Delta  a, b, c.
  ((\Lambda `w. P'[A \Xi  E[A\Xi "u][B\Xi "v][C\Xi "w][B\Xi "y][B\Xi "z]] [B\Xi "v] [C\Xi `w]
[B\Xi "y] [B\Xi "z])

\Upsilon    (\Lambda  u', y', z'. Q'[D  \Xi   x] [A  \Xi   u'] [B  \Xi   v'] [C  \Xi   w'] [B  \Xi   y ' ]
[B \Xi  z']))))

  \Upsilon  Q[B  \Xi  w'] [B  \Xi  v'] [D  \Xi  x] [A  \Xi  "u] [B  \Xi  "v] [C  \Xi  "w])
= (A = a = b - 3 = d - 4  \Pi  C = c). \Delta 

Example  Recursive factorial procedure with value-result parameters  (196)

Let us prove the partial correctness of the following program:

procedure F(?X !Y);
begin

{X = n}
if X = 0 then

{X = n \Pi  X = 0} Y := 1
else begin

{X = n \Pi  X ` 0} F(X - 1, Y); {X = n \Pi  Y = (n - 1) !} Y := Y * X;
end;
{X = n \Pi  Y = n !}
end;
{X = y} F(X, Y); {X = y \Pi  Y = y !}

To prove partial correctness {X = n} F(?X !Y) {X = n  \Pi  Y = n !} of procedure F, we
use the recursion rule (191). Assuming {X = n} F(?X !Y) {X = n  \Pi  Y = n !} by
induction hypothesis, we must prove {X = n  \Pi  X ` 0} F(X -1, Y) {X = n  \Pi  Y = (n -
1) !} for the recursive call of F. By the consequence rule (102) and rule of adaptation

(192) (where {x *} = {x *} = o/ and m * = n, P' = (X = n), Q' = (X = n  \Pi  Y = n !) and Q
= (X = n \Pi  Y = (n - 1) !)) we must show that:

(X = n \Pi  X ` 0) \Upsilon 

(\Delta  "u, "w, "z, w'.

((X = "u) \Pi  (Y = "w) \Pi  (Y = "z)
\Pi  (\Delta  n.((\Lambda `w. P'[X  \Xi  (X - 1)[X \Xi "u][Y\Xi "w][Y\Xi "z]] [Y  \Xi  `w] [Y  \Xi  "z])

\Upsilon  (\Lambda  u', z'. Q'[X  \Xi  u'] [Y  \Xi  w'] [Y  \Xi  z']))))
\Upsilon  Q [Y  \Xi  w'] [X  \Xi  "u] [Y  \Xi  "w])

that is, after simplification:

(X = n  \Pi  X ` 0) \Upsilon  (\Delta  "u, w'.  ((X = "u)  \Pi  (w' = ("u - 1) !) ) \Upsilon  ("u = n  \Pi  w' = (n
- 1) !) )

which is obvious. Then the correctness proof of procedure F can be completed as
shown by the above proof outline. Knowing that {X = n} F(?X !Y) {X = n  \Pi  Y = n !}
holds, it remains to prove {X = y} F(X, Y) {X = y  \Pi  Y = y !} for the main call of
procedure F. This follows from the rule of adaptation (192) and the consequence rule
(102) since: (X = y)  \Upsilon   (\Delta  "u, "w, w'.  ((X = "u)  \Pi  (Y = "w)  \Pi  (\Delta  n.  ((\Lambda  `w. (X =
n)[X \Xi  "u] [Y  \Xi  `w])  \Upsilon  (\Lambda  u'. (X = n  \Pi  Y = n !)[X  \Xi  u'] [Y  \Xi  w'])))) \Upsilon  (X = y  \Pi 
Y = y !)[Y  \Xi   w'] [X  \Xi   "u]). \Delta 

8 . 2 . 3 Complements on variable parameters and procedures

as parameters

APT [1981a] 's survey on Hoare logic is mainly concerned with procedures and
parameter mechanisms. More recent progresses concerning procedures have been
reported in  LANGMAACK & OLDEROG [1980]  and  OLDEROG [1983b] . In particular, for a
treatment of variable (reference, address,...) parameters, see  HOARE [1971b], IGARASHI,
LONDON & LUCKHAM [1975], ERNST [1977], SCHWARTZ [1979], DE BAKKER [1980, Ch. 9], GRIES &
LEVIN [1980], APT [1981a], CARTWRIGHT & OPPEN [1981] , GERMAN, CLARKE & HALPERN [1983]
[1988], SIEBER [1985] . A separation of procedural abstraction and parameter passing is
attempted is MORGAN [1988a] [1988b] [1988c].

CLARKE [1977]'s theorem (169) put a borderline to sound and relatively complete
Hoare logics for programs with procedures.  CLARKE [1977]  also claimed that the
languages  j (as defined at (166)) do have a sound and relatively complete Hoare logic.

For j  ` 4 these claims were either proved in  CLARKE [1977]  or later established by
OLDEROG [1981]. These languages  j, j ` 4 are easy to axiomatize since for each program,

there is a bound on the number of procedure environments (associating a procedure
body to a procedure name) that can be reached during execution (OLDEROG [1983b], DAMM
& JOSKO [1983a] [1983b]). It follows that each of the procedure environments can be treated
as a separate case. The case of  4 was more difficult because it can give rise to nonhomomorphic chains of procedure environments that grow arbitrarily long.

Example  (197)

In the following program written in an  4 subset of Pascal:

program L4;

procedure P (X : integer; procedure Q (Z : integer));

procedure L (X : integer); begin Q(X - 1) end;
begin if X > 0 then P(X - 1, L) else Q(X) end;

procedure M;

var N : integer;
procedure R (X : integer); begin writeln(X) end;
begin write('N = '); readln(N); P(N, R) end; {M}
begin M end.

the main procedure M reads the value n of N and calls P(n, L n) with L n = R which
recursively calls P(n - 1, L n-1), P(n - 2, L n-2), ... , P(1, L 1) and P(0, L 0) thus creating
a chain of procedures  procedure Li-1 (X : integer);  begin Li(X - 1) end; for i = n, ...
, 1 so that execution of P(0, L 0) consists in calling L 0(0) whence L 1(-1), L2(-2), ...,
Ln-1(-n+1) and Ln(-n) that is R(-n) which finally prints -n. \Delta 

The first axiomatizations of Clarke's language  4 proposed by  DAMM & JOSKO [1983a]
[1983b] and  OLDEROG [1983c] [1984]  used higher-order assertion languages to make
assertions about this unbounded number of procedure environments (i.e. statetransformation): all concepts for Hoare logic are lifted from programs transforming
states into states to programs transforming state-transformations into statetransformations. This leads to completeness not relative to the first-order theory of the
interpretation. GERMAN, CLARKE & HALPERN [1983] [1988] were the first to propose a relative
completeness proof for  4 with a first-order oracle and  COOK [1978] 's notion of

expressiveness. However the logic extends Hoare logic by allowing quantifiers over
first-order variables ( \Delta  x. H, ...) and other logical connectives (H  \Upsilon  H',...) to be
used on the level of Hoare formulae (an idea going back to HAREL, PNUELI & STAVI [1977]
and also exploited in SIEBER [1985]) but is relatively complete only for Hoare correctness
formulae H. Later  GOERDT [1987]  proposed an indirect axiomatic proof method for  4

which involves the translation of programs into finitely typed lambda calculus and then
the application of  GOERDT [1985] 's Hoare calculus. This calculus was shown to be
relatively complete (without Herbrand definability hypothesis) in GOERDT [1988]. Hoare
logic has also been extended to other types of languages with higher-order concepts in
ERNST, NAVLAKHA & OGDEN [1982], DE BAKKER, KLOP & MEYER [1982] , GOERDT [1985]  [1987]
[1988], HALPERN [1984], LANGMAACK [1983], TRAKHTENBROT, HALPERN & MEYER [1983].

8 . 3 Undefinedness

When the evaluation of expressions in the programming language can lead to
runtime errors, it is necessary to deal with partially defined functions. Hoare logic can
be easily extended when the domain of these partial functions is given (for example in
the case of array bounds outside their declared range) or can be easily defined (for
example in the case of dangling pointers) as shown by  SITES [1974], GERMAN [1978] [1981],
COLEMAN & HUGUES [1979]  and BLIKLE [1981]. Contrary to mathematicians, the computer
scientists are often confronted with the problem of proving properties about partial
objects without the knowledge of their domain. In this case first-order 2-valued logic is
inadequate as pointed out by  ASHCROFT, CLINT & HOARE [1972]  who have discovered an
error in  CLINT & HOARE [1976]  for nonterminating functions. Alternatives consist in
introducing an extra element ss in the data domain which is forced by the axiomatization
to behave like a "divergent" or "undefined" data object ( CARTWRIGHT [1984]) or, more
generally, in using a 3-valued logic covering undefinedness in program proofs as
proposed by BARRINGER, CHENG & JONES [1984] and studied by HOOGEWIJS [1987].

8 . 4 Aliasing and side effects

The assignment axiom (97) is incorrect when aliases (i.e. distinct identifiers
designating a shared storage location) are allowed. One can hide the source of the
problem by prohibiting interference between identifiers in the programming language or
in the proofs ( REYNOLDS [1978] [1989] , MASON [1987]) or consider augmented versions of
Hoare logic which allows aliasing between variables ( JANSSEN & VAN EMDE BOAS [1977],
SCHWARTZ [1979] , CARTWRIGHT & OPPEN [1981] , OLDEROG [1981] , LANGMAACK [1983] ,
TRAKHTENBROT, HALPERN & MEYER [1983]).

The same way the assignment axiom (97), conditional (100) and while (101)
rules are incorrect when the evaluation of expressions can have side effects ( MANNA &
WALDINGER [1981]). A number of Hoare-like axiomatizations of expression languages or
expressions with side effects ( CUNNINGHAM & GILFORD  [1976], KOWALTOWSKI [1977] ,
PRITCHARD [1977], SCHWARTZ & BERRY [1979] ) modify Hoare logic by introducing Hoare
correctness formulae for expressions {P}E{Q} where the precondition P and
postcondition Q explicitly depend upon a distinguished variable standing for the value
returned by the evaluation of expression E. Alternatives consist in considering other
logics explicitly referring to the state of computation ( MANNA & WALDINGER [1981] ), in
using the value of a programming language expression as the underlying primitive
(BOEHM [1985]) or in transforming the program into procedural form (LANGMAACK [1983]).

8 . 5 Block structured local variables

If we extend the syntax (188) of programs with block structured local variables:

C ::= (var X; C1) (198)
local variable X is like a bound variable of a predicate (see (116)) so that blocks satisfy
the property that systematic replacement of the local variable X by some fresh variable Y
preserves the meaning of the block: ( var X; C) is equivalent to ( var Y;  C[X \Xi  Y]),
provided that Y does not occur free in C. For example, according to this  static scope
rule, ( var X; (( var X; C 1); C 2)) is equivalent to ( var Y; (( var Z; C 1[X \Xi   Z]);
C2[X \Xi  Y])) where Y, Z  fl Var(C1, C2 ). This leads to the following rule ( HOARE
[1971b], HOARE & WIRTH [1973], APT [1978] [1981a], DE BAKKER [1980]) where the renaming of
X for Y is performed to distinguish between the occurrences of local X in C and
possible free occurrences of nonlocal X in P or Q:

  {P}  C[X \Xi  Y] {Q}
------------------ Block rule (199)
 {P} (var X; C) {Q}

where Y fl Free(P, Q) \Pi  ((X = Y) \Sigma  Y fl Free(C))
Observe that rule (199) may necessitate alteration C[X \Xi  Y] of the program text C, but
this can be avoided (nai"ve solutions such as  DONAHUE [1976] fail to treat the scope rule
properly, see the discussion in  APT [1981a] and FOKKINGA [1978], OLDEROG [1981] [1983] for
further details). Observe also that rule (199) does not take the possibility of running out
of new storage locations into account ( TRAKHTENBROT, HALPERN & MEYER [1983] ). There
are other difficulties in defining the semantics of such blocks (see DE BAKKER [1980, Ch. 6],
TRAKHTENBROT, HALPERN & MEYER [1983], HALPERN, MEYER & TRAKHTENBROT [1984], MEYER &
SIEBER [1988],  for a discussion). For example, mapping local variables into a global
memory using a stack discipline is an overspecification, since then, for example, {true}
((var X; X := 0); (var Z; Y := Z)) {Y = 0} would hold because X and Z are allocated
at the same address. Although this phenomenon can be observed in a number of
implementations, it is contrary to the specification of block-structured languages where
the initial value of local variables is usually  undetermined so that, as shown by (199),
(var X; C) should be equivalent to ( var X; (X := ?; C)), see  WIRTH [1971] for example.
However this introduces unbounded nondeterminism (see the corresponding difficulties
in paragraph $ 8.10.2). Another way to cope with uninitialized local variables ( APT
[1981a]) is to use an extra uninitialized value  OE \Omega  D so that ( var X; C) is equivalent to

(var X; (X :=  OE; C)). But then one can prove {true} (( var X; Y := X); ( var X; Z :=
X)) {X = Y}, which is not true for most implementations where the initial value of local
variables is undetermined.  DE BAKKER  [1980]  introduces simple conditions which
guarantee that variables are initialized before being used. One can also force initialization
upon declaration with the syntax ( var X := E; C1) (and use union types to allow for

an initial value denoting logical uninitialization).

8 . 6 Goto statements

Goto's with static labels cause no problem with FLOYD [1967a] partial correctness
proof method. For each label L, if {P i, i = 1, ..., n} is the set of preconditions of the

statements  goto L in the program (including the postcondition of the command
sequentially preceding the command labeled L) and Q is the precondition of the
command labeled L then  FLOYD [1967a] 's verification condition is simply ( ' i = 1,...,n
Pi) \Upsilon  Q. The difficulty with Hoare logic is to express this verification conditional
compositionally, by induction on the syntax of programs ( O'DONNELL [1982]). Various
solutions have been proposed by  CLINT & HOARE [1972] , DONAHUE [1976] , WANG [1976] ,
KOWALTOWSKI [1977], DE BRUIN [1981] and LIFSCHITZ [1984]. An inconsistency problem with
CLINT & HOARE [1972], DONAHUE [1976]  and KOWALTOWSKI [1977]  goto rules is noticed by
ARBIB & ALAGIC [1979]  and  O'DONNELL [1982] . Scope problems with jumps out of
procedures have not been explicitly dealt with (but for weakened forms of procedure
escapes, FOKKINGA [1978]).

Techniques similar to that used in theorem (169) have also been used to obtain
incompleteness results for programming languages that include label variables with
retention (CLARKE [1977] [1984]).

8 . 7 Functions and expressions (with side effects)

Functions are not called in order to change states - the realm that assertions can
capture - but to return a value. In the proof rule proposed by  CLINT & HOARE [1972]  the
result returned by the function call f(x) (in the computer science sense) is simply
denoted in predicates by f(x) (in the mathematical sense). This excludes side-effects in
functions (since for example the mathematical identity f(x) + f(x) = 2 f(x) is not valid
whenever a call to f increments x by 1). Moreover ASHCROFT, CLINT & HOARE [1976] have
noticed that the proof rule of  CLINT & HOARE [1972]  yields an inconsistency whenever a
defined function fails to halt for some possible argument, even if the value of the
function is never computed for that argument, as explained in  O'DONNELL [1982]. Hence

the problem is again the one of undefinedness which is not correctly handle in
predicates (see 8.3).

The problem can be circumvented by reduction of expression evaluation to
execution of a sequence of assignment statements (DE BAKKER, KLOP & MEYER [1982]), by
transforming the programmer-declared functions into procedures (LANGMAACK [1983]) or
by modifying Hoare logic by explicitly introducing one or more symbols to denote
expression values ( CUNNINGHAM & GILFORD [1976], KOWALTOWSKI [1977] , PRITCHARD
[1977], SCHWARTZ [1979]) or by introducing primitive notations to make explicit assertions
about the value of programming language expressions ( BOEHM [1982]  [1985],
SOKOLOWSKI [1984]).

8 . 8 Coroutines

CLINT [1973]  extended Hoare logic to simple coroutines (block-body and single
coroutine combination) based upon the semi-coroutine concept of Simula 67 ( WANG &
DAHL [1971] , DAHL & NYGAARD [1966] ). It was further extended by  PRITCHARD [1976]  to
multiple coroutines and DAHL [1975] to multiple dynamic instances of coroutines. They
use auxiliary variables to accumulate the computation and communication history in
stacks of arbitrary size.  CLARKE [1980]  showed that, in the case of simple static
coroutines, history variables are useless and that auxiliary variables of bounded size
simulating program counters are enough.  CLINT [1981]  argues that history variables,
although not necessary, can help for clarity and ease of verification.

Techniques similar to that used in theorem (169) have also been used to obtain
incompleteness results for programming languages that include coroutines with local
recursive procedures that can access global variables (CLARKE [1977] [1984]).

8 . 9 Parallel programs

The controversy on the usefulness of program verification ( DE MILLO, LIPTON &
PERLIS [1979] ) can hardly be extended to parallel programs because their correctness
which is often very intricate cannot be checked by non-reproducible and non-exhaustive
tests. Therefore, clear programming notations as well as correctness proofs are
indispensable, at least when designing the underlying basic algorithms (DIJKSTRA [1968],
ANDREWS [1981]).

The evolution of Hoare logic for parallel programs is tightly coupled with the
slow emergence of clear notations for expressing process synchronization and
communication to which C. A. R. Hoare largely contributed, from shared variables
(HOARE [1972c] [1975] , OWICKI & GRIES [1976a]  [1976b], ...), then monitors ( HOARE [1974],

HOWARD [1976], OWICKI [1978]) and finally synchronous message passing ( HOARE [1978b],
APT, FRANCEZ & DE ROEVER [1980], LEVIN & GRIES [1981], ...).

We discuss proof methods for parallel programs with shared variables and briefly
survey the case of synchronous message passing in  HOARE [1978b] 's communicating
sequential processes CSP.

8 . 9 . 1 Operational semantics of parallel programs with

shared variables

We consider (a simplified version of) parallel programs with shared global
variables as introduced by OWICKI [1975] and OWICKI & GRIES [1976a]:

DEFINITION  Syntax of parallel programs (200)

. Pp : Parallel programs (.1)

Pp ::=  [C1 || C 2 || ... || C n]    n >= 2

. C : Sequential commands (.2)

C ::= skip | X := E | X := ? | (C1; C2) | (B ff C1 fi C2)

| (B * C) | (B ? C) | %

Execution of a program "[C 1 || C2 || ... || Cn]" consists in executing processes C 1, C2,
... and C n in parallel. These commands act upon implicitly declared shared global
variables. Evaluation of a Boolean expression B, execution of assignments "X := E" or
"X := ?" and execution of await commands "(B ? C)" are  atomic or indivisible actions
that is no concurrent action can modify the value of the variables involved in this action.
When a process attempts to execute an await command "(B ? C)", it is delayed until the
condition B is true. Then the command C is executed as an indivisible action.
Evaluation of B is part of the indivisible action so that another process may not change
variables so as to make B false after B has been evaluated but before C begins
execution. Upon termination of "(B ? C)", parallel processing continues. If two or more
processes are waiting for the same condition B, any one of them may be allowed to
proceed when B becomes true, while the others continue waiting. The order in which
processes are scheduled is indifferent, for example a weak fairness hypothesis would be
that no nonterminated hence permanently enabled process C k can be indefinitely

delayed. A  strong fairness hypothesis  would be that no process can be indefinitely
delayed if condition B can be infinitely often evaluated to true while that process is
waiting, see  LAMPORT [1980a] , LEHMANN, PNUELI & STAVI [1981] , MANNA & PNUELI [1984]
[1989] and  FRANCEZ [1986]  for more details on fairness hypotheses.  OWICKI [1975]  and
OWICKI & GRIES [1976a] assume that await commands (B ? C) cannot be imbricated since
this could lead to deadlocks (as in "[(true ? (false ? skip)) || skip]"). Execution of a

program "[C 1 || C2 || ... || C n]" is terminated when all processes have finished their
execution. We use the empty command "%" to denote termination.

To define the operational semantics of parallel programs, we introduce control
states:

DEFINITION  Labels designating control states  (201)
if Pp is [C1 || C2 || ... || Cn] then

. (B ? C) ={(B ? C)},  (B ? C)  = o/,  (B ? C)  = { %} (.1)
. Pp = C1  x ...  x  Cn (.2)

. = '{ Pp  : Pp  \Omega   } (.3)
. Pp = C1  x ...  x  Cn (.4)
. Pp = C1  x ...  x  Cn (.5)
. Pp = Pp  -  Pp  -  Pp (.6)

The  operational semantics  of parallel programs is defined by interleaved
executions of atomic actions as defined by (13.1) to (13.6):

DEFINITION  Operational semantics of parallel programs (compositional
presentation) (202)

d : D Data (.1)
s : S =   ff D States (.2)
o/ : \Gamma  = ( S x  ) ' (S x  ) Configurations (.3)
op : (  '  ) ff P(\Gamma  x \Gamma ) Operational transition (.4)
 relation

op (B ? C)   = {<<s, (B ? C) >, s'> : s \Omega  B \Pi  <<s, C>, s'> \Omega  op C *} (.5)

o p %   =  o/ (.6)
op  [C1 || C2 || ... || Cn]   = (.7)

{<<s, L >, <s', L[k  \Xi  L'k]>> : k  \Omega  {1, ..., n}  \Pi  <<s, L k>, <s', L' k>> \Omega  op Ck }
'  {<<s, L >, <s', L[k  \Xi  %] >> : k  \Omega  {1, ..., n}  \Pi  <<s, L k>, s'> \Omega  op Ck }

Example  (203)

We illustrate the proof methods for parallel programs on the following program
Pp which increments the value of variable X by a + b where a and b are given integer
constants:

(204)
[  X := X + a  ||  X := X + b  ]

L L L L11 12 21 22

Its operational semantics is (for simplicity we write  < s,  < L 1, ... , L n> >  as
< s, L1, ... , Ln> ) :

op Pp   = {<<s, L 11, L21>, <s[X \Xi  s(X) + a], L 12, L21>> : s  \Omega  S}

' {<<s, L11, L22>, <s[X \Xi  s(X) + a], L 12, L22>> : s \Omega  S}
' {<<s, L11, L21>, <s[X \Xi  s(X) + b], L 11, L22>> : s  \Omega  S}
' {<<s, L12, L21>, <s[X \Xi  s(X) + b], L 12, L22>> : s  \Omega  S}

where L11 = (X := X + 1) 0, L12 = %0, L21 = (X := X + 1) 1 and L22 = %1. This program
is partially correct {p}Pp{q} for the specification p = {s \Omega  S : s(X) = s(x)} and q = {s \Omega 
S : s(X) = s(x) + a + b}.  \Delta 

By (44), the operational semantics (202)  can also be given a stepwise
presentation:

LEMMA  Operational semantics of parallel programs (stepwise presentation) (205)

op  [C1 || C2 || ... || Cn]   (.1)

= {<<s, L>, <s', L[k \Xi   L'k]>> : k  \Omega   {1, ... ,  n}

\Pi   Lk fl  C k \Pi   s' \Omega   C k <s, Lk>  \Pi  L' k \Omega   C k <s, L k>}
where:

C [(...(((B ? C'); C 1); C2)...; C n)] = (B ? C') (.2)

C [(...(((B ? C'); C 1); C2)...; Cn)] = (...(C 1; C2)...; Cn) (.3)
if  C [L] is (B ? C') then

C <s, L > = {s' : s  \Omega  B \Pi  <<s, C' >, s'> \Omega  op C * } (.4)

C <s, L> = { C [L] : s  \Omega  B} (.5)

Example  Stepwise operational semantics of parallel program (204) (206)

The stepwise presentation of the operational semantics of program Pp = [C1 || C2]
defined at (204) is specified by C 1 = (X := X + a), C 2 = (X := X + b),  C1  = {L11},

C1  = o/,  C 1  = {L 12},  C 1  = {L 11, L 12},  C 2  = {L 21},  C 2  = o/,

C2  = {L 22},  C2  = {L 21, L 22},  C1 <s, L 11> = {s[X  \Xi  s(X) + a]},

C1 <s, L 11> = {L 12},  C2 <s, L 21> = {s[X  \Xi  s(X) + b]},  C2 <s,
L21> = {L 22}. \Delta 

8 . 9 . 2 A`  la  Floyd proof methods for parallel programs with

shared variables

We follow the style of presentation of COUSOT & COUSOT [1984]. From a semantical
point of view, proofs of partial correctness  {p}Pp{q} consist in discovering local
invariants I \Omega   Pp  attached to control points and in proving that they satisfy local
verification conditions lvc Pp [p, q](I) expressing that local invariants must remain true
after execution of atomic actions. Hence without complementary hypotheses on the set

Pp  of local invariants we can assume that a` la Floyd proof methods are of the
form:

[\Lambda  I \Omega   Pp . lvc Pp [p, q](I)] (207)
Up to a connection (!, o/) between local invariants I \Omega   Pp  and global invariants i \Omega 
P(\Gamma ):

!  \Omega   P (\Gamma ) ff   P p ) (208)
o/ \Omega   Pp  ff  P(\Gamma ) (209)

this proof method (207) consists in applying induction principle (29.1):

[\Lambda  i \Omega  P(\Gamma ). gvc Pp [p, q](i)] (210)
This induction principle involves a single global invariant i on configurations (i.e.
memory and control states), which is true for initial configurations satisfying p, remains
true after each program step and implies q for final configurations:

(211)
gvc Pp [p, q](i) = ( \Delta  s \Omega  p. <s, C 1, ... C n> \Omega  i) (.1)

\Pi  (i * op Pp  ffl \Gamma  x i) (.2)
\Pi  (\Delta  <s, %, ... ,  %> \Omega  i. s \Omega  q) (.3)

By theorem (29.1) this induction principle (210) is semantically sound ( ,) and
complete (\Upsilon ):

{p}Pp{q} \Phi  [\Lambda  i \Omega  P(\Gamma ). gvc Pp [p, q](i)] (212)
The connection i = o/(I) and I = !(i) between (207) and (210) expresses the fact that the
global invariant i can be decomposed into local assertions I attached to control points. It
induces a logical connection between local and global verification conditions:

\Delta  I \Omega   Pp . lvc Pp [p, q](I)  \Upsilon  gvc Pp [p, q](o/(I)) (213)
\Delta  i \Omega  P(\Gamma ). gvc Pp [p, q](i) \Upsilon  lvc Pp [p, q](!(i)) (214)

which, together with (212), ensures the soundness and semantical completeness of
(207):

{p}Pp{q} \Phi  [\Lambda  I \Omega   Pp . lvc Pp [p, q](I)] (215)

This approach can also be used to formally construct proof methods by symbolic
calculus: Given gvc Pp [p, q],  ! and  o/ , we can let lvc Pp [p, q](I) be gvc Pp [p,
q](o/(I)) (so that the proof method is sound by construction) and check semantical
completeness by showing that gvc Pp [p, q](i)  \Upsilon  lvc Pp [p, q]( !(i)) = gvc Pp [p,
q](o/(!(i))) (which is often obvious because gvc Pp [p, q] is monotone and ( !, o/) is a
Galois connection so that  \Delta  i \Omega  P(\Gamma ). i \Upsilon  o/(!(i))). For example, this point of view was
applied to the design of a partial correctness proof method for CSP programs in COUSOT
& COUSOT [1980].

We now explain a few partial correctness proof methods for parallel programs
following these guidelines.

8 . 9 . 2 . 1 Using a single global invariant

ASHCROFT [1975] partial correctness proof method and the one of  KELLER [1976]
(illustrated by BABICH [1979]) consist in directly applying induction principle (29.1).

Example (216)

To prove partial correctness of (204) by (210), we can use the global invariant:

i   = {<s, L11, L21> : s(X) = s(x)}  ' {<s, L12, L21> : s(X) = s(x) + a}

' {<s, L11, L22> : s(X) = s(x) + b}  ' {<s, L12, L22>: s(X) = s(x) + a + b}

and show that (for all s, s'  \Omega  S):

(s \Omega  p) \Upsilon  (<s, L11, L21> \Omega  i)
(<s, L 11, L21> \Omega  i \Pi  s' = s[X  \Xi  s(X) + a])  \Upsilon  (<s', L 12, L21> \Omega  i)
(<s, L 11, L22> \Omega  i \Pi  s' = s[X  \Xi  s(X) + a])  \Upsilon  (<s', L 12, L22> \Omega  i)
(<s, L 11, L21> \Omega  i \Pi  s' = s[X  \Xi  s(X) + b])  \Upsilon  (<s', L 11, L22> \Omega  i)
(<s, L 12, L21> \Omega  i \Pi  s' = s[X  \Xi  s(X) + b])  \Upsilon  (<s', L 12, L22> \Omega  i)
(<s, L12, L22> \Omega  i) \Upsilon  (s \Omega  q)

\Delta 

In Ashcroft-Keller method,  AK Pp  is chosen as  P(\Gamma ) and ( !AK, o/AK) is
identity. According to the operational semantics (205), the verification condition (i  *
op Pp  ffl \Gamma  x i) of (210) can be decomposed into simpler verification conditions
corresponding to each atomic action. Otherwise stated (i  * op Pp  ffl \Gamma  x i) is equivalent
to:

\Delta  s, s'  \Omega  S. \Delta  L \Omega   Pp . \Delta  k \Omega  {1, ..., n}. (217)

(<s, L > \Omega  i \Pi  Lk fl  Ck \Pi  s'  \Omega   Ck <s, L k> \Pi  L' k \Omega   Ck <s, L k>) 

\Upsilon  (<s', L[k  \Xi  L'k]> \Omega  i)

This means that starting with i true of  <s, L 1, ... , L k , ... , L n> and executing any
atomic action labeled L k of any process C k of the program (that is a transition in that
process C k from configuration  <s, Lk> to configuration  <s', L'k>) leaves i true of  <s',
L1 , ... , L' k , ... , L n>.

The difficulty with this method is that for large programs the single global
invariant i tends to be unmanageable without being decomposed into simpler assertions.

8 . 9 . 2 . 2 Using an invariant on memory states for each control state

 Early attempts towards the decomposition of the global invariant such as
ASHCROFT & MANNA [1970]  and LEVITT [1972] involves the transformation of the parallel
program into an equivalent nondeterministic one upon which Floyd-Naur's partial
correctness proof method is applied. As observed by KELLER [1976], this simply consists
in using induction principle (210) with local invariants on memory states attached to
each control state.

Example  Partial correctness proof of program (204) by  Ashcroft & Manna's
method (218)

To prove partial correctness of parallel program (204) by (220), we can use the
local invariants:

I<L11, L21> = {s \Omega  S : s(X) = s(x)} 
I<L11, L22> = {s  \Omega  S : s(X) = s(x) + b}
I<L12, L21> = {s \Omega  S : s(X) = s(x) + a}
I<L12, L22> = {s \Omega  S : s(X) = s(x) + a + b}

and show that (for all s, s'  \Omega  S):
Initialization (220.1):

(s \Omega  p) \Upsilon  (s \Omega  I[L11 || L21])

Induction (220.2):

(s \Omega  I<L11, L21> \Pi  s' = s[X  \Xi  s(X) + a])  \Upsilon  (s' \Omega  I<L12, L21>)
(s \Omega  I<L11, L22> \Pi  s' = s[X  \Xi  s(X) + a])  \Upsilon  (s' \Omega  I<L12, L22>)
(s \Omega  I<L11, L21> \Pi  s' = s[X  \Xi  s(X) + b])  \Upsilon  (s' \Omega  I<L11, L22>)
(s \Omega  I<L12, L21> \Pi  s' = s[X  \Xi  s(X) + b])  \Upsilon  (s' \Omega  I<L12, L22>)
Finalization (220.3):

(s \Omega  I<L12, L22>) \Upsilon  (s \Omega  q)
\Delta 

To formally construct this proof method, we exactly follow the development of
paragraph $ 5.2 for Floyd-Naur method.Then we introduce local invariants and their
connection with the global invariant of (210):

DEFINITION  Connection between local and global invariants (219)

. AM Pp  =  Pp  ff    (where   =  P(S)) (.1)
. !AM : P(\Gamma ) ff  Pp , !(i)(L) = {s :  <s, L> \Omega  i} (.2)
. o/AM :  Pp  ff P(\Gamma ), o/(I) = {<s, L> : L \Omega   Pp  \Pi  s \Omega  I(L)} (.3)

Then we derive local verification conditions by lvc Pp [p, q](I) = gvc Pp [p,
q](o/AM(I)):

THEOREM  Ashcroft & Manna partial correctness proof method for parallel
programs (220)

Pp is [C 1 || C2 || ... || C n] then  {p}Pp{q} holds if and only if their exists I  \Omega 

Pp  ff 
such that:

. if L \Omega   Pp  then p ffl I(L) (.1)
. if s, s'  \Omega  S, L  \Omega   Pp  '  Pp , k \Omega  {1, ... , n} then (.2)
(s  \Omega  I(L)  \Pi  Lk fl  Ck \Pi  s'  \Omega   Ck <s, L k> \Pi  L' k \Omega   Ck <s, L k>)

\Upsilon  (s' \Omega  I(L[k \Xi  L'k])
. if L \Omega   Pp  then I(L) ffl q (.3)

Otherwise stated, starting with I <L1, ... , L k , ... , L n> true of s and executing any
atomic action labeled L k of any process C k of the program (that is a transition in that
process Ck from configuration  <s, Lk> to configuration  <s', L'k>) leaves I<L1, ... , L'k
, ... , Ln> true of s'. These local verification conditions can further be detailed as in (45)
for each possible kind of atomic action. In particular when the atomic action

C k [Lk] labeled L k is an await command (B ? C) with next label L' k  =
Ck [Lk] we must prove  { I(L) ^ B }C{ I(L[k \Xi  L'k] } to which Floyd's stepwise
partial correctness proof method (45) is directly applicable.

As shown by the success of Floyd-Naur partial correctness proof method, this
approach is very well suited for sequential programs. However for parallel programs Pp
= [C 1 || C2 || ... || C n] the number of local invariants I(L), L  \Omega   Pp  grows as the
number |  Pp  | of control states that is as the product |  C1  | . |  C2  | . ...
. |  Cn  | of the sizes of the processes C 1, C2, ..., C n. Apart for trivial programs
this exponential explosion is rapidly unmanageable.

8 . 9 . 2 . 3 Using an invariant on memory states for each program point

In paragraph $ 8.9.2.2, Floyd-Naur partial correctness proof method was
generalized to parallel programs by using an invariant on memory states for each control
state <L1, ... , Ln> \Omega   Pp . For sequential programs this is equivalent to the use of

an invariant I(L)  \Omega    on memory states for each program point L  \Omega   Pp . These
two points of view do not coincide for parallel programs. So, as first suggested by
ASHCROFT [1975], Floyd-Naur proof method can also be generalized to parallel programs
using an invariant on memory states for each program point. This consists in defining:

DEFINITION  Connection between local and global invariants (221)

.  [C1 || ... || C n]   = '{ Ck  : k \Omega  {1,..., n}} (.1)
. A Pp  =  Pp  ff  (.2)
. !A : P(\Gamma ) ff  A Pp , !A(i)(l) = {s :  \Lambda  L \Omega   Pp . <s, L[k  \Xi  l]> \Omega  i} (.3)
. o/A :  A Pp  ff P(\Gamma ), o/A(I) = {<s, L> : \Delta  j \Omega  {1, ... , n}. s  \Omega  I(Lj)} (.4)

The induction step ( o/A(I)  * op Pp  ffl \Gamma  x o/A(I)) of the corresponding verification
conditions lvc Pp [p, q](I) = gvc Pp [p, q]( o/A(I)) can be, following  OWICKI & GRIES
[1976a], decomposed into a sequential proof and a proof of interference freedom (also
called "monotony condition" in  LAMPORT [1977]). Sequential correctness asserts that
executing any atomic action labeled L of any process C k of the program (that is a
transition of that process C k from configuration  <s, L > to configuration  <s', L'>)
starting with I(L) true of s makes I(L') true of s'. Interference freedom asserts that for
every label L" in a different process C m, starting with both I(L") and I(L) true of s

leaves I(L") true of s':

THEOREM  Incomplete partial correctness proof method for parallel programs (222)

if Pp is [C 1 || C2 || ... || Cn] then {p}Pp{q} holds if their exists I  \Omega   Pp  ff

such that :

. if k \Omega  {1, ..., n} and L  \Omega   Ck  then p  ffl I(L) (.1)
. if s, s'  \Omega  S, k \Omega  {1, ..., n}, L  \Omega   Ck  '  Ck  and L'  \Omega   Ck  then  (.2)

[s \Omega  I(L)  \Pi  s' \Omega   Ck <s, L > \Pi  L' \Omega   Ck <s, L >] \Upsilon  s' \Omega  I(L')
. if s, s'  \Omega  S, k \Omega  {1, ..., n}, L  \Omega   Ck  '  Ck , m  \Omega  {1, ..., n} - {k} (.3)

and L" \Omega   Cm  then

[s \Omega  I(L")  \Pi  s \Omega  I(L) \Pi  s' \Omega   Ck <s, L>] \Upsilon  s' \Omega  I(L")
. ^ {I(L) :  \Lambda  k \Omega  {1, ..., n}. L  \Omega   Ck  } ffl q (.4)
but the reciprocal is not true.

Example Partial correctness proof of program (204) with method (222) (223)

Assuming a = 1 and b = 2, we can apply (222) to prove partial correctness of
parallel program (204). The verification conditions are the following:

Initialization (222.1):

p ffl I(L11)
p ffl I(L21)
Sequential correctness (222.2):

s \Omega  I(L11) \Upsilon  s[X \Xi  s(X) + 1]  \Omega  I(L12)
s \Omega  I(L21) \Upsilon  s[X \Xi  s(X) + 2]  \Omega  I(L22)
Interference freedom (222.3):

 s \Omega  I(L11) \Pi  s \Omega  I(L21) \Upsilon  s[X \Xi  s(X) + 2]  \Omega  I(L11)
 s \Omega  I(L12) \Pi  s \Omega  I(L21) \Upsilon  s[X \Xi  s(X) + 2]  \Omega  I(L12)
 s \Omega  I(L21) \Pi  s \Omega  I(L11) \Upsilon  s[X \Xi  s(X) + 1]  \Omega  I(L21)
 s \Omega  I(L22) \Pi  s \Omega  I(L11) \Upsilon  s[X \Xi  s(X) + 1]  \Omega  I(L22)
Finalization (222.4):

I(L12) ^ I(L22) ffl q

They are satisfied by the following local invariants:
I[L11] = {s \Omega  S : s(X) = s(x)  \Sigma  s(X) = s(x) + 2}
I[L12] = {s \Omega  S : s(X) = s(x) + 1  \Sigma  s(X) = s(x) + 3}
I[L21] = {s \Omega  S : s(X) = s(x)  \Sigma  s(X) = s(x) + 1}
I[L22] = {s \Omega  S : s(X) = s(x) + 2  \Sigma  s(X) = s(x) + 3}

\Delta 

Using example (204) with a = b = 1, KELLER [1976] and OWICKI & GRIES [1976a] have
shown that the corresponding proof method is semantically incomplete:

Counterexample  Incompleteness of method (222) for program (204) (224)

Formally, the strongest global invariant  i satisfying the global verification
condition gvc P p [p, q] in (210) for program (204) is given by its fixpoint
characterization (30) as:

i = lfp oe X : P(\Gamma ). {<s, C 1, ..., C n> : s  \Omega  p}  ' {o/ : \Lambda  o/'. <o/', o/> \Omega  X  * op Pp }
= {<s, L11, L21> : s(X) = s(x)}  ' {<s, L12, L21> : s(X) = s(x) + a}

' {<s, L11, L22> : s(X) = s(x) + b}  ' {<s, L12, L22>: s(X) = s(x) + a + b}

It follows by monotony, that the strongest local invariants are I = !A(i), that is to say:
I(L11) = {s \Omega  S : s(X) = s(x)  \Sigma  s(X) = s(x) + b}
I(L12) = {s \Omega  S : s(X) = s(x) + a  \Sigma  s(X) = s(x) + a + b}
I(L21) = {s  \Omega  S : s(X) = s(x)  \Sigma  s(X) = s(x) + a}
I(L22) = {s \Omega  S : s(X) = s(x) + b  \Sigma  s(X) = s(x) + a + b}

When a = b = 1 they are too weak to satisfy the interference freedom (222.3) and
finalization (222.4) verification conditions as given at example (223) \Delta 

8 . 9 . 2 . 4 Using an invariant on memory and control states for each

program point

The use of an invariant on memory states for each program point is incomplete
because the relationship between memory and control states is lost. This can be avoided
by using local invariants I(Lk) attached to program points Lk of each process Ck of the

program Pp specifying the relationship between the memory state and the control state
of other processes:

L Pp  = {I  \Omega   Pp  ff P(S x  Pp n-1) :

\Delta  k \Omega  {1, ..., n}.  \Delta  Lk \Omega   Ck . (225)

I(Lk) ffl S x  C1  x  ... x  Ck-1  x  Ck+1  x  ... x  Cn }

This way of expressing the global invariant can be extirpated from NEWTON [1975] and is
clear in LAMPORT [1977]. These local invariants can be written as a proof outline (as in
7.2.4 and OWICKI & GRIES [1976a]) in which a predicate P k representing I(Lk) is attached
to control point L k. Control predicates a` la  LAMPORT [1977]  [1980b] and  LAMPORT &
SCHNEIDER [1984] can be used in Pk to explicitly mention the control state. Such control
predicates can, to some extent, be defined in a language independent way ( COUSOT &
COUSOT [1989]).

Example  Proof outline for parallel program (204)

To prove partial correctness of parallel program (204), we can use the following
local invariants:

I[L11] = {<s, L21> : s(X) = s(x)}  ' {<s, L22> : s(X) = s(x) + b} (226)
I[L12] = {<s, L21> : s(X) = s(x) + a}  ' {<s, L22> : s(X) = s(x) + a + b}
I[L21] = {<s, L11> : s(X) = s(x)}  ' {<s, L12> : s(X) = s(x) + a}

which can be specified by the following proof outline:
{ X = x } (227)
[ L11 : { (at(L 21) \Pi  X = x)  \Sigma  (at(L22) \Pi  X = x + b) }

X := X + a
  L12 : { (at(L21) \Pi  X = x + a)  \Sigma  (at(L22) \Pi  X = x + a + b) }
||  L21 : { (at(L 11) \Pi  X = x) \Sigma  (at(L12) \Pi  X = x + a) }

X := X + b
  L22 : { (at(L 11) \Pi  X = x + b)  \Sigma  (at(L12) \Pi  X = x + a + b) }  ]

{ X = x + a + b }
\Delta 

This decomposition of the global invariant can be specified by the following
connection between local and global invariants (recall that  <L1, ... , Ln>~k is <L1, ... ,
Lk-1, Lk+1, ... , L n>):

DEFINITION  Connection between local and global invariants (228)

. !L : P(\Gamma ) ff  L Pp ,  !L (i)(Lk) = { <s, L ~k> : <s, L > \Omega  i} (.1)
. o/L :  L Pp  ff P(\Gamma ),  o/L(I) = { <s, L> : \Lambda  k \Omega  {1, ..., n}.  <s, L~k> \Omega  I(Lk)} (.2)

Observe that ( !L, o/L) is a bijection between  P(\Gamma ) and  L Pp  which ensures the
soundness and semantical completeness of the derived proof method.

8 . 9 . 2 . 4 . 1 The strengthened Lamport and Owicki & Gries method

The local verification conditions lvc Pp [p, q](I) = gvc Pp [p, q]( o/L(I))
corresponding to the above definition of  L Pp  and (!L, o/L) were first designed by
COUSOT & COUSOT [1984] and later by LAMPORT [1988]. The proof method is similar to that
of LAMPORT [1977]  and OWICKI & GRIES [1976a]  but for the fact that it is strengthened by
allowing the use of the proof outline of the processes not involved in the sequential or
interference freedom proof.

Example (229)

To prove partial correctness of parallel program (204), we can use local invariants
(226) and check the following local verification conditions (for all s, s'  \Omega  S, c1 \Omega  {L11,
L12} and c 2 \Omega  {L21, L22}):

* Initialization (230.1):

The initialization step ( \Delta  s \Omega  p. <s, C1, ... C n> \Omega  o/A(I)) states that the input
specification p implies the invariants attached to entry points of the processes C 1, ... ,
Cn of Pp:

. s \Omega  p \Upsilon  <s, L21> \Omega  I(L11)
. s \Omega  p \Upsilon  <s, L11> \Omega  I(L21)

* Induction step:

The induction step ( o/A(I) * op Pp  ffl \Gamma  x o/A(I)) must be checked for all atomic
actions of all processes Ck of program Pp that is all transitions of that process C k from

configuration  <s, L> to configuration  <s', L' >. The proof can be decomposed into
sequential and interference freedom proofs:
- Sequential correctness (230.2):

To prove sequential correctness we must show that starting with I(L) true of
<s, L1, ..., L k-1, L k+1, ..., L n> (as well as the assertions I(L j) attached to control
points L j of the other processes C j, j \Omega  {1, ..., n} - {k} true of  <s, L1, ..., L j-1, Lj+1,
..., Lk-1, L, Lk+1, ..., L n>) makes I(L') true of  <s', L1, ..., L k-1, Lk+1, ..., L n>:

. [<s, c2> \Omega  I(L11) \Pi  <s, L11> \Omega  I(L21) ' I(L22)] \Upsilon  <s[X \Xi  s(X) + a], c2> \Omega  I(L12)
. [<s, c1> \Omega  I(L21) \Pi  <s, L21> \Omega  I(L11) ' I(L12)] \Upsilon  <s[X \Xi  s(X) + b], c 1> \Omega  I(L22)
- Interference freedom (230.3):

 Interference freedom asserts that for every label L" in a different process Cm,
m \Omega  {1, ... , n} - {k} starting with both I(L) true of  <s, L1, ..., L m-1, L", L m+1, ...,
Lk-1, Lk+1, ... , L n> and I(L") true of  <s, L1, ... , L m-1, Lm+1, ... , L k-1, L, L k+1, ... ,
Ln> (as well as the assertions I(L j) attached to control points L j of the other processes
Cj, j \Omega  {1, ..., n} - {k, m} true of  <s, L 1, ... , L m-1, L", L m+1, ... , L j-1, L j+1, ... ,
Lk-1, L, L k+1, ... , L n>) leaves I(L") true of  <s', L 1, ... , L m-1, L m+1, ... , L k-1, L',
Lk+1, ... , L n> :

. [<s, c2> \Omega  I(L11) \Pi  <s, L11> \Omega  I(L21)] \Upsilon  <s[X \Xi  s(X) + b], c 2> \Omega  I(L11)
. [<s, c2> \Omega  I(L12) \Pi  <s, L12> \Omega  I(L21)] \Upsilon  <s[X \Xi  s(X) + b], c 2> \Omega  I(L12)
. [<s, c1> \Omega  I(L21) \Pi  <s, L21> \Omega  I(L11)] \Upsilon  <s[X \Xi  s(X) + a], c 1> \Omega  I(L21)
. [<s, c1> \Omega  I(L22) \Pi  <s, L22> \Omega  I(L11)] \Upsilon  <s[X \Xi  s(X) + a], c 1> \Omega  I(L22)

* Finalization (230.4):

The final step of the partial correctness proof ( \Delta  <s, [% || ... ||  %]> \Omega  o/A(I). s \Omega 

q) shows that final states satisfy the output specification q:

. [<s, L22> \Omega  I(L12) \Pi  <s, L12> \Omega  I(L22)] \Upsilon  s \Omega  q

\Delta 

This proof method [ \Lambda  I \Omega   L Pp . lvcSLO Pp [p, q](I)] directly follows from the
choice lvc SLO Pp [p, q](I) = gvc Pp [p, q]( o/L(I)) with gvc Pp [p, q](i) defined by
(211) and operational semantics (205):

THEOREM COUSOT & COUSOT [1984] Strengthened Lamport and Owicki &  (230)

 Gries method
{p}Pp{q} where Pp is [C 1 || C2 || ... || C n] is true if and only if their exists
I \Omega   L Pp such that for all k  \Omega  {1, ..., n}, m  \Omega  {1, ..., n} -{k} and s, s'  \Omega  S :

. if l \Omega   Ck  and L \Omega   Pp  then s  \Omega  p \Upsilon  <s, L~k> \Omega  I(l) (.1)
. if l \Omega   Ck  '  Ck  and L  \Omega   Pp  then (.2)

[<s, L~k> \Omega  I(l) \Pi  \Delta  j \Omega  {1,...,n} - {k}.  <s, L[k  \Xi  l]~j \Omega  I(Lj) \Pi 

s' \Omega   C k <s, l>  \Pi  l' \Omega   C k <s, l >] \Upsilon   <s', L ~k> \Omega  I(l')
. if l \Omega   Ck  '  Ck , l" \Omega   Cm  and L  \Omega   Pp  then (.3)

[<s, L[k \Xi  l]~m> \Omega  I(l")  \Pi  <s, L[m \Xi  l"]~k> \Omega  I(l) \Pi  \Delta  j \Omega  {1, ... , n} - {k, m}. 
<s, L[m \Xi   l"][k \Xi   l]~m> \Omega  I(Lj) \Pi  s' \Omega   C k <s, l > \Pi  l' \Omega   C k <s, l >]

\Upsilon  <s', L[k \Xi  l']~m> \Omega  I(l")
. if L \Omega   Pp  then [ \Delta  k \Omega  {1, ..., n}.  <s, L~k> \Omega  I(Lk)] \Upsilon  s \Omega  q (.4)

8 . 9 . 2 . 4 . 2 Newton's method

NEWTON [1975] proof method (although designed for a quite different definition of
concurrent programs) is a weakened version of  LAMPORT [1977]  and  OWICKI & GRIES
[1976a] method in the sense that interference freeness simply consists in proving that any
local invariant I(L"), L"  \Omega   Cm  remains true after execution of any atomic action
labeled L of any other process C k, k  ` m (and this without assuming that I(L) holds
before executing this atomic action).

Example  Partial correctness proof of program (204) by Newton's method (231)

To prove partial correctness of parallel program (204), we can use local invariants
(226) and check the following local verification conditions (for all s, s'  \Omega  S, c1 \Omega  {L11,
L12} and c 2 \Omega  {L21, L22}):

* Initialization (232.1):

. s \Omega  p \Upsilon  <s, L21> \Omega  I(L11)
. s \Omega  p \Upsilon  <s, L11> \Omega  I(L21)

* Sequential correctness (232.2):

. <s, c2> \Omega  I(L11) \Upsilon  <s[X \Xi  s(X) + a], c 2> \Omega  I(L12)
. <s, c1> \Omega  I(L21) \Upsilon  <s[X \Xi  s(X) + b], c 1> \Omega  I(L22)

* Interference freedom (232.3):

. <s, c2> \Omega  I(L11) \Upsilon  <s[X \Xi  s(X) + b], c 2> \Omega  I(L11)
. <s, c2> \Omega  I(L12) \Upsilon  <s[X \Xi  s(X) + b], c 2> \Omega  I(L12)
. <s, c1> \Omega  I(L21) \Upsilon  <s[X \Xi  s(X) + a], c 1> \Omega  I(L21)
. <s, c1> \Omega  I(L22) \Upsilon  <s[X \Xi  s(X) + a], c 1> \Omega  I(L22)

* Finalization (232.4):

. [<s, L22> \Omega  I(L12) \Pi  <s, L12> \Omega  I(L22)] \Upsilon  s \Omega  q

\Delta 
This proof method [ \Lambda   I  \Omega   L P p .  l v c N P p [p, q](I)] is sound (since
lvcN Pp [p, q](I)  \Upsilon  lvc SLO Pp [p, q](I)  \Upsilon  gvc Pp [p, q](o/(I)))  \Upsilon  {p}Pp{q}) and
relatively complete (since any I  \Omega   L Pp  satisfying lvc SLO Pp [p, q](I) can be
strengthened into I' such that I'(l) = { <s, L~k> \Omega  I(l)  : \Delta   j \Omega  {1,...,n} - {k}.  <s ,
L[k \Xi  l]~j \Omega  I(Lj)} satisfying lvc N Pp [p, q](I')):

THEOREM  Newton's method (232)

{p}Pp{q} where Pp is [C 1 || C 2 || ... || C n] is true if and only if their exists
I \Omega   L Pp such that for all k  \Omega  {1, ... , n}, m  \Omega  {1, ... , n} -{k} and s, s'  \Omega  S:

. if l \Omega   Ck  and L \Omega   Pp  then s  \Omega  p \Upsilon  <s, L~k> \Omega  I(l) (.1)
. if l \Omega   Ck  '  Ck  and L  \Omega   Pp  then (.2)

[<s, L~k> \Omega  I(l)  \Pi  s' \Omega   Ck <s, l > \Pi  l' \Omega   Ck <s, l >]

\Upsilon  <s', L~k> \Omega  I(l')
. if l \Omega   Ck  '  Ck , l" \Omega   Cm  and L  \Omega   Pp  then (.3)

[<s, L[k \Xi  l]~m> \Omega  I(l")  \Pi  s'  \Omega   Ck <s, l > \Pi  l' \Omega   Ck <s, l >]

\Upsilon  <s', L[k \Xi  l']~m > \Omega  I(L")
. if L \Omega   Pp  then [ \Delta  k \Omega  {1, ... , n}.  <s, L~k> \Omega  I(Lk)] \Upsilon  s \Omega  q (.4)

Observe that interference freedom (232.3) disappears when considering monoprocess
programs (n = 1, in which case (232) exactly amounts to Floyd's stepwise partial
correctness proof method (45)) or multiprocess programs with assertions about parts of
the store such that only operations acting upon separate parts may be performed
concurrently (as in HOARE [1975] or MAZURKIEWICZ [1977] for example).

Although partial correctness proof methods (230) and (232) are both
semantically complete, it may be the case that some assertions I  \Omega   L Pp  satisfy

(230) but cannot be proved to be invariant using (232) without being strengthened.

Example  Weak invariants for program (204) (233)

Parallel program (204) with a = 1 and b = 2 is partially correct with respect to
specification  <p, q> such that p = {s  \Omega  S : s(X) = 0} and q = {s  \Omega  S : s(X) = 3}. This
can be proved using the verification conditions of (230) given at example (229) and the
following invariants:

I[L11] = {<s, L2> : even(s(X))}
I[L12] = {<s, L2> : s(X) = 1  \Sigma  s(X) = 3}
I[L21] = {<s, L1> : s(X) = 0  \Sigma  s(X) = 1}
I[L22] = {<s, L1> : s(X) = 2  \Sigma  s(X) = 3}

These invariants are too weak to satisfy the verification conditions of (232) given at
example (231).  \Delta 

8 . 9 . 2 . 4 . 3 The lattice of proof methods including Lamport's method

Any proof method [ \Lambda  I \Omega   L Pp . lvc Pp [p, q](I)] with local verification
conditions lvc Pp [p, q] such that lvc N Pp [p, q](I) \Upsilon  lvc Pp [p, q](I) \Upsilon  lvcSLO Pp [p,
q](I) for all  I \Omega   L Pp  is sound (since lvc Pp [p, q](I)  \Upsilon  lvc SLO Pp [p, q](I)  \Upsilon 
gvc Pp [p, q](o/(I)) \Upsilon  {p}Pp{q}) and semantically complete (since  {p}Pp{q} \Upsilon  [\Lambda  i \Omega 
P (\Gamma ). gvc Pp [p, q](i)]  \Upsilon   [\Lambda  i \Omega   P (\Gamma ). lvc N P p [p, q](!(i))]  \Upsilon   [\Lambda  I  \Omega   L Pp .
lvcN Pp [p, q](I)]  \Upsilon  [\Lambda  I \Omega   L Pp . lvcN Pp [p, q](I)]). This is the case of  LAMPORT
[1977]'s method which is OWICKI & GRIES [1976a] method with control predicates instead of
auxiliary variables:

THEOREM  LAMPORT [1977]  Lamport's partial correctness proof method (234)

{p}Pp{q} where Pp is [C 1 || C 2 || ... || C n] is true if and only if their exists
I \Omega   L Pp such that for all k  \Omega  {1, ..., n}, m  \Omega  {1, ..., n} -{k} and s, s'  \Omega  S :

. if l \Omega   Ck  and L \Omega   Pp  then s  \Omega  p \Upsilon  <s, L~k> \Omega  I(l) (.1)
. if l \Omega   Ck  '  Ck  and L  \Omega   Pp  then (.2)

[<s, L~k> \Omega  I(l)  \Pi  s' \Omega   Ck <s, l > \Pi  l' \Omega   Ck <s, l >]

\Upsilon  <s', L~k> \Omega  I(l')
. if l \Omega   Ck  '  Ck , l" \Omega   Cm  and L  \Omega   Pp  then (.3)

[<s, L[k \Xi  l]~m> \Omega  I(l")  \Pi  <s, L[m \Xi  l"]~k> \Omega  I(l)  \Pi  s' \Omega   Ck <s, l > \Pi  
l' \Omega   Ck <s, l >] \Upsilon  <s', L[k  \Xi  l']~m> \Omega  I(L")
. if L \Omega   Pp  then [ \Delta  k \Omega  {1, ..., n}.  <s, L~k> \Omega  I(Lk)] \Upsilon  s \Omega  q (.4)

Example  Partial correctness proof of program (204) by Lamport's method (235)

To prove partial correctness of parallel program (204), we can use local invariants
(226) and check the following local verification conditions (for all s, s'  \Omega  S, c1 \Omega  {L11,
L12} and c 2 \Omega  {L21, L22}):

* Initialization (234.1):

. s \Omega  p \Upsilon  <s, L21> \Omega  I(L11)
. s \Omega  p \Upsilon  <s, L11> \Omega  I(L21)

*  Sequential correctness (234.2):

. <s, c2> \Omega  I(L11) \Upsilon  <s[X \Xi  s(X) + a], c 2> \Omega  I(L12)
. <s, c1> \Omega  I(L21) \Upsilon  <s[X \Xi  s(X) + b], c 1> \Omega  I(L22)

* Interference freedom (234.3):

. [<s, c2> \Omega  I(L11) \Pi  <s, L11> \Omega  I(L21)] \Upsilon  <s[X \Xi  s(X) + b], c 2> \Omega  I(L11)
. [<s, c2> \Omega  I(L12) \Pi  <s, L12> \Omega  I(L21)] \Upsilon  <s[X \Xi  s(X) + b], c 2> \Omega  I(L12)

. [<s, c1> \Omega  I(L21) \Pi  <s, L21> \Omega  I(L11)] \Upsilon  <s[X \Xi  s(X) + a], c 1> \Omega  I(L21)
. [<s, c1> \Omega  I(L22) \Pi  <s, L22> \Omega  I(L11)] \Upsilon  <s[X \Xi  s(X) + a], c 1> \Omega  I(L22)

* Finalization (234.4):

. [<s, L22> \Omega  I(L12) \Pi  <s, L12> \Omega  I(L22)] \Upsilon  s \Omega  q
\Delta 

8 . 9 . 2 . 5 Using an invariant on memory states with auxiliary

variables for each program point

8 . 9 . 2 . 5 . 1 A  stepwise presentation of Owicki & Gries method

OWICKI & GRIES [1976a] partial correctness proof method is based upon (234) using
auxiliary variables (also called "dummy", "phantom", "ghost", "history", "mythical" or
"thought" variables) for completeness.

DEFINITION  OWICKI & GRIES [1976a]   Auxiliary variables (236)

A V  ffl   is a set of  auxiliary variables  for a parallel program Pp and a
specification  <p, q> if and only if  AV is finite, any X  \Omega  AV appears in Pp only in
assignments of the form X := E and q does not depend upon some X  \Omega  AV (i. e. \Delta  X
\Omega  A V . \Delta  s  \Omega  S. \Delta  d \Omega  D . s[X  \Xi   d]  \Omega  q).

We say that Pp is obtained from Pp' by elimination of auxiliary variables AV if Pp
can be obtained from Pp' by deleting all assignments to the variables of  AV and
subsequently replacing some of the components of the form "(true ? Y := E)" by
"Y := E".

We say that p is obtained from p' \Omega    by elimination of auxiliary variables AV =
{X1, ... , X n} if and only if  \Lambda  d1 \Omega  D. ...  \Lambda  dn \Omega  D. p = {s[X 1 \Xi  d1] ... [X n \Xi  dn] :

s \Omega    p ' } .

Auxiliary variables can be used to record the history of execution or indicate which part
of a program is currently executing. Owicki & Gries proof method is sound since the
elimination of auxiliary variables does not change the result of program execution. It is
semantically complete since auxiliary variables can simulate program counters:

THEOREM   OWICKI & GRIES [1976a]   Owicki & Gries partial correctness proof
method (237)

{p}Pp{q} if and only if there exists Pp' and p' such that  {p'}Pp'{q} holds by
(234) and there exists a set AV of auxiliary variables for Pp' and <p, q> such that p and
Pp are respectively obtained from p' and Pp' by elimination of auxiliary variables AV.

Example  Auxiliary variables for the partial correctness proof of program (204) (238)

The partial correctness of parallel program (204) can be proved using Owicki &
Gries method (237) as shown by the following proof outline:

{ X = x \Pi  L1 = 1 \Pi  L2 = 1 }
[ { (L1 = 1  \Pi  L2 = 1 \Pi  X = x) \Sigma  (L1 = 1  \Pi  L2 = 2 \Pi  X = x + b) }

(true ? (X := X + a; L1 := 2))
  { (L1 = 2  \Pi  L2 = 1 \Pi  X = x + a)  \Sigma  (L1 = 2 \Pi  L2 = 2 \Pi  X = x + a + b) }
||  { (L2 = 1  \Pi  L1 = 1 \Pi  X = x) \Sigma  (L2 = 1 \Pi  L1 = 2 \Pi  X = x + a) }

(true ? (X := X + b; L2 =: 2))
  { (L2 = 2  \Pi  L1 = 1 \Pi  X = x + b)  \Sigma  (L2 = 2  \Pi  L1 = 2 \Pi  X = x + a + b) }  ]
{ X = x + a + b }
\Delta 

8 . 9 . 2 . 5 . 2 On  the use of auxiliary variables

As shown by example (238) auxiliary variables can simulate program counters
and this ensures the equivalence of Owicki & Gries method (237) with Lamport method
(234), hence its semantical completeness. LAMPORT [1988] claims that "although dummy
variables can represent the control state, the implicit nature of this representation limits
their utility". However, contrary to program counters the values of auxiliary variables
are not bounded whence they can also be used to record computation histories
(SOUNDARARAJAN [1984a]). This introduces additional power. For example, as shown by
APT [1981b], the invariants can be restricted to recursive predicates. The argumentation
of LAMPORT [1988] in favor of control predicates goes on with the claim that "the use of
explicit control predicates allows a strengthening of the ordinary Owicki-Gries method
that makes it easier to write annotations". Observe however that (237) corresponds to
(234) and that a strengthened version corresponding to (230) can be used instead. Then
example (233) shows that the usefulness of this strengthened version does not depend
upon the use of control predicates or auxiliary variables. To designate which part of a
program is currently executing, program counters have the default that their value
changes after each execution of an atomic action within that part. For example
introducing extra "skip" commands in a program would change nothing when using
auxiliary variables but would introduce additional interference freeness checks upon
control predicates. In return, changes to auxiliary variables are not concomitant with an
atomic action. Critical sections (such as "(true ? (X := X + a; L1 := 2))" at example
(238)) can be used for assignments but not for Boolean tests.  DE ROEVER  [1985a]
suggestion of using a dynamic extension of  APT, FRANCEZ & DE ROEVER [1980]
"indivisibility" brackets can be very useful in that respect. Finally the use of program
counters would be advantageous for program verifiers since "intelligence" would be
needed to introduce auxiliary variables.

8 . 9 . 2 . 5 . 3 A  syntax-directed presentation of Owicki & Gries method

Following the guidelines of paragraphs $ 5.2, $ 5.3 and $ 5.4, we can give a
syntax-directed presentation of (237) due to  OWICKI [1975] (see also APT [1981b]), where
definition of program components is extended by  # [C'1 || ... || C' n]   =
'{ #i C'i  : i = 1, ... , n} and  # (B ? C)  = {(B ? C) #} '  #0 C  :

THEOREM  OWICKI [1975] Syntax-directed presentation of Owicki & Gries proof  (239)

method
{p}Pp{q} if and only if there exists Pp'  \Omega   , a set  AV of auxiliary variables for
Pp' and  <p, q> and preconditions pre  \Omega   Pp'  ff   and postconditions post  \Omega 

Pp'  ff   such that :
. Pp is obtained from Pp' by elimination of auxiliary variables AV (.1)
. p ffl pre(Pp')  \Pi  post(Pp')  ffl q (.2)
Each component C \Omega   Pp'  of program Pp' is sequentially correct:
. if C is skip then pre(C)  ffl post(C) (.3)
. if C is X := E then pre(C)  ffl {s \Omega  S : s[X \Xi  E(s)] \Omega  post(C)} (.4)
. if C is X := ? then {s[X  \Xi  d] : s \Omega  pre(C) \Pi  d \Omega  D} ffl post(C) (.5)
.  if C is (C1; C2) then (.6)

pre(C) ffl pre(C1) \Pi  post(C1) ffl pre(C2) \Pi  post(C2) ffl post(C)
. if C is (B ff C1 fi C2) then (.7)

(pre(C) ^ B) ffl pre(C1) \Pi  (pre(C) ^ ~B) ffl pre(C2) \Pi  post(C1) ffl post(C) \Pi 
post(C2) ffl post(C)
.  if C is (B * C1) then (.8)

pre(C) ffl post(C1) \Pi  (post(C1) ^ B) ffl pre(C1) \Pi  (post(C1) ^ ~B) ffl post(C)
.  if C is (B ? C1) then (.9)

(pre(C) ^ B) ffl pre(C1) \Pi  post(C1) ffl post(C)
No await "(B ? C)" or assignment "X := E" or "X := ?" command C' j \Omega   Cj  of a
process C j of program Pp of the form "[C 1 || ... || C n]" interfere with the proof of
components C' i \Omega   Ci  of other processes C i, i ` j :
.  {pre(C'i) ^ pre(C'j)}C'j{pre(C'i)} (.10)
.  {post(C'i) ^ pre(C'j)}C'j{post(C'i)} (.11)

8 . 9 . 3 Hoare logics for parallel programs with shared

variables

8 . 9 . 3 . 1 Owicki & Gries logic

After HOARE [1972c] [1975], OWICKI [1975] and OWICKI & GRIES [1976a] [1976b] where the
first to extend  HOARE [1969] to parallel programs with shared variables (see also the
summary given by  DIJKSTRA [1982d] ). The difficulty is that although (239) is syntaxdirected, it is not compositional in the sense of  DE ROEVER [1985a] , that is "the
specification of a program should be verifiable in terms of the specification of its
syntactic subprograms". More precisely, the sequential proof which is context-free can
be expressed in Hoare's style but interference freeness which is context-sensitive
cannot. Therefore OWICKI & GRIES [1976a] method   is usually informally presented in
HOARE [1969]'s style adding to   the following :

Rules of inference of 

 {P} Pp' {Q} Auxiliary variables (240)
------------ elimination rule
  {P} Pp {Q}

provided Pp is obtained from Pp' by elimination of auxiliary variables AV and Q
contains no variable of AV.

    {P}  Pp {Q}  Substitution rule (241)
---------- --------
 {P[X \Xi  T]} Pp {Q}

provided X fl Free(Pp, Q).

 {P \Pi  B} C {Q} Await rule (242)
--------------
 {P} (B ? C) {Q}

      {P1} C 1 {Q 1} ... {P n} C n {Q n}  Parallelism rule (243)
--------------------------------------
 {P1 \Pi  ... \Pi  Pn} [C1 || ... || C n] {Q1 \Pi  ... \Pi  Qn}

provided {Pi} Ci {Qi}, i = 1, ... , n are interference free.
Rule (240) allows for the elimination of auxiliary variables in the program whereas
substitution rule (241) allows for the elimination of auxiliary variables in the
precondition. Rule (243), where interference freedom is defined as in (239.10-11), is
not compositional because interference freeness of the whole is directly reduced to that
of its atomic parts and not to that of its constituent parts. This  OWICKI & GRIES [1976a]
proof system can be extended to proof outline as in  SCHNEIDER & ANDREWS [1986] . The
relative completeness of   is proved in APT [1981b].

8 . 9 . 3 . 2 Stirling compositional logic

LAMPORT [1980b]  proposed a first compositional version of Hoare logic, named
GHL. It essentially consists in axiomatizing (217), with the disadvantages of global
invariants. However GHL can be developed  in a programming language independent
way (LAMPORT & SCHNEIDER [1984] , COUSOT & COUSOT  [1989]). Another language
independent step toward compositionality was GERTH [1983].

A most important observation is that a compositional specification of a command
should include a first part called  assumption or rely-condition describing the desired
behavior of the command and a second part called commitment or guarantee-condition
describing the behavior of the environment ( LAMPORT [1983]). A first step was taken by
FRANCEZ & PNUELI [1978] who introduced statements of the form <N> C <"> meaning that
in any execution such that the environment behaves according to assumption  N, it is
guaranteed that command C behaves according to ". However this significantly departs
from Hoare triples since  N and " are temporal formulae ( PNUELI [1985]). A further step
was taken by JONES [1983a] [1983b] who introduced specifications of commands under the
form (Rc, Gc): {P} C {Q} where P is a precondition and Q is a postcondition over
states whereas the assumption A and the guarantee-condition G are sets of pairs of
states characterizing the interference of command C with other processes. More
precisely, Rc defines the relationship which can be assumed to exist between the free
variables of command C in states changed by other processes. Gc is a commitment
which must be respected by all state transformations of C thus constraining the
interference which may be cause by command C.

An HOARE [1969] and OWICKI & GRIES [1976a] -like axiomatization   of JONES [1983a]
[1983b] was proposed by  STIRLING [1986] [1988] . In order to remain in the realm of first
order logic where states but not pairs of states are considered, each P  \Omega    is
determines a set of changes which are invariant with respect to it:

 :   ff P(S x S) (244)

P  = {<s, s'> : s  \Omega  P \Upsilon  s' \Omega  P}

The interpretation is that if <s, s'>  fl  P  then the transition from state s to state s'
interferes with the truth of P. This is extended to families R of formulae in  :

 : P( ) ff P(S x S) (245)

R  = ^{ P  : P  \Omega  R}

STIRLING [1988] then defines an invariant implication between sets of formulae :

(R ?> G) \Phi  ( R  ffl  G ) (246)

which is characterized by the following lemma :

(247)
(R ?> G) \Phi  (\Delta  Q \Omega  G. \Delta  R' ffl R. (\Delta  P \Omega  R'. P \Upsilon  Q) \Sigma  (\Delta  P \Omega  R - R'. ~P  \Upsilon  ~Q)) (.1)
R ?> R (.2)
(R ?> G \Pi  G ?> H) \Upsilon  (R ?> H) (.3)
(G ffl R) \Upsilon  (R ?> G) (.4)
(P \Phi  Q \Pi  R ?> G) \Upsilon  (R ' {P} ?> G ' {Q}) (.5)
R ' {P, Q} ?> R ' {P \Pi  Q} (.6)

STIRLING [1988]'s proof system   consists of   for proving properties of sub-commands
within await commands plus the following axiom schemata and rules of inference :

(R, G) : {P} skip {P}    Skip axiom (248)
 R ?> {P}, P  \Upsilon  Q[X  \Xi  E],  \Delta  I \Omega  G. (P  \Pi  I) \Upsilon  I[X  \Xi  E]
-----------------------------------------------   Assignment rule (249)
          (R,  G)  :  {P}  X  :=  E  {Q}

 (R, G) : {P 1} C1 {P2}, (R, G) : {P 2} C2 {P3}
----------------------------------------   Composition rule (250)

     (R,  G)  :  {P1} (C 1; C 2) {P 3}

 R ?> {P}, (R, G) : {P  \Pi  B} C 1 {Q}, (R, G) : {P  \Pi  ~B} C 2 {Q}
----------------------------------------------------   Conditional rule (251)

         (R,  G)  :  {P}  (B  ff   C1  fi  C2) {Q}

 R ?> {P}, (R, G) : {P  \Pi  B} C {P}
----------------------------   While rule (252)
  (R, G) : {P}(B * C) {P  \Pi  ~B}

 R  ?> R', P  \Upsilon  P', (R', G') : {P'} C {Q'}, Q'  \Upsilon  Q, G'  ?>  G
------------------------------------------------   Consequence rule (253)

               (R,  G)  :  {P}  C  {Q}

 R ?> {P}, {P  \Pi  B} C {Q},  \Delta  I \Omega  G. {P  \Pi  B \Pi  I} C {I} (254)

--------------------------------------------   Await rule
           (R,  G)  :  {P}  (B  ?  C)  {Q}

 R1 ?> {Q1}, (R1, R2 ' G) : {P 1} C1 {Q1}, (255)
 (R2, R1 ' G) : {P 2} C2 {Q2}, R2 ?> {Q2}
--------------------------------------   Parallelism rule
 (R1 ' R2, G) : {P 1 \Pi  P2} [C1 || C2] {Q1 \Pi  Q2}

 (R, G) : {P} Pp {Q} (256)
------------------   Derelativisation rule
    {P}  Pp  {Q}

 {P} Pp' {Q} (257)
------------   Auxiliary variables elimination  rule
 {P} Pp {Q}

provided Pp is obtained from Pp' by elimination of auxiliary  variables AV and Q
contains no variable of AV.

     {P}  Pp {Q}  (258)
---------- -------   Substitution rule
 {P[X \Xi  T]} Pp {Q}

provided X fl Free(Pp, Q).

Example (259)

The partial correctness proof (238) of parallel program (204) as given by the
following proof outline :

{P} {I 0} [{I11}(true ? (X := X + a; L1 := 2)){I 12} || {I 21}(true ? (X := X + b; L2 =:
2)){I22}] {Q}

where :

P = (X = x)
I0 = (X = x \Pi  L1 = 1 \Pi  L2 = 1)
I11 = ((L1 = 1  \Pi  L2 = 1 \Pi  X = x) \Sigma  (L1 = 1 \Pi  L2 = 2 \Pi  X = x + b))
I12 = ((L1 = 2 \Pi  L2 = 1 \Pi  X = x + a)  \Sigma  (L1 = 2 \Pi  L2 = 2 \Pi  X = x + a + b))
I21 = ((L2 = 1 \Pi  L1 = 1 \Pi  X = x) \Sigma  (L2 = 1 \Pi  L1 = 2 \Pi  X = x + a))
I22 = ((L2 = 2 \Pi  L1 = 1 \Pi  X = x + b)  \Sigma  (L2 = 2 \Pi  L1 = 2 \Pi  X = x + a + b))

Q = (X = x + a + b)
can be formalized by   as follows :
(a) {I11, I12} ?> {I11} by (247.4)
(b) {I11 \Pi  true} (X := X + a; L1 := 2) {I12} by   ' Th
(c) {I11 \Pi  true \Pi  I21} (X := X + a; L1 := 2) {I 21} by   ' Th
(d) {I11 \Pi  true \Pi  I22} (X := X + a; L1 := 2) {I 22} by   ' Th
(e) ({I11, I12}, {I21, I22}) : {I11} (true ? (X := X + a; L1 := 2)) {I 12} by a, b, c, d, (254)
(f) {I21, I22} ?> {I21} by (247.4)
(g) {I21 \Pi  true} (X := X + b; L2 := 2) {I 22} by   ' Th
(h) {I21 \Pi  true \Pi  I11} (X := X + b; L2 := 2) {I 11} by   ' Th
(i) {I21 \Pi  true \Pi  I12} (X := X + b; L2 := 2) {I 12} by   ' Th
(j) ({I21, I22}, {I11, I12}) : {I11} (true ? (X := X + b; L2 := 2)) {I 22} by f, g, h, i, (254)
(k) {I11, I12} ?> I12 by (247.4)
(l) {I21, I22} ?> I22 by (247.4)
(m) ({I11, I12, I21, I22}, o/) : by k, e, j, l, (255)

{I11 \Pi  I21}[(true ? (X := X + a; L1 := 2)) || (true ? (X := X + b; L2 := 2))]{I 12 \Pi  I22}
(n) {I11 \Pi  I21}[(true ? (X := X + a; L1 := 2)) || (true ? (X := X + b; L2 := 2))]{I 12 \Pi  I22} by m, (256)
(o) {I0} [(true ? (X := X + a; L1 := 2)) || (true ? (X := X + b; L2 := 2))] {Q} by Th, n, (102)
(p) {I0} [X := X + a || X := X + b] {Q} by o, (257)
(q) {I0[L1\Xi 1, L2\Xi 1]} [X := X + a || X := X + b] {Q} by p, (258)

(r) {P} [X := X + a || X := X + b] {Q} by Th, q, (102)
\Delta 

Additional techniques for proving partial or total correctness of parallel programs with
shared variables are extensively discussed in number of surveys such as  APT [1984] ,
BARRINGER [1985], DE ROEVER [1985a] and SCHNEIDER & ANDREWS [1986].

8 . 9 . 4 Hoare logics for communicating sequential processes

HOARE [1978b] [1985b]  introduced CSP (Communicating Sequential Processes), a
language for parallel programs with communication via synchronous unbuffered
message-passing. A program has the form "[Pl1 :: C1 || Pl2 :: C2 || ... || Pln :: Cn]" where
process labels Pl 1, ... ,  Pln respectively designate parallel processes C 1, ... ,  Cn.
Shared variables are disallowed.

Communication between processes Pl i and Plj (i ` j) is possible if process Pl i is
to execute a send primitive "Pl j ! E" and process Pl j is to execute a receive primitive
"Pli ? X". The first process ready to communicate has to wait as long as the other one
is not ready to execute the matching primitive. Their execution is synchronized and

results in the assignment of the value of expression E (depending upon the values of the
local variables of Pl i) to the variable X (which is local to Pl j). For example "{X = a}
[Pl1 :: Pl2 ! X || Pl2 :: (Pl1 ? Y; Pl3 ! Y) || Pl3 :: Pl2 ? Z] {Z = a}" is true.

Nondeterminism is introduced via the alternation command "(B 1; G1 ff C1 fi B2;
G2 ff C2 fi ... fi Bn; Gn ff Cn)" where the guards "B k; Gk", k = 1, ... , n consist of a
Boolean expression B k followed by a send "Pl j ! E" or "skip" command G k. Its
execution consists in selecting and executing an arbitrary successful guard B k; Gk
(where Bk evaluates to true and process Pl j is ready to communicate if G k is "Plj ! E")
and then the corresponding alternative C k. Their is no fairness hypothesis upon the
choice between successful guards. For the repetition command "*(B 1; G1 ff C1 fi B2;
G2 ff C2 fi ... fi Bn; Gn ff Cn)", this is repeated until all guards "B k; Gk" fail, that is Bk
evaluates to false or process Pl j has terminated if G k is "Pl j ! E". This is called the
distributed termination convention (APT & FRANCEZ [1984]).

COUSOT & COUSOT [1980]  extended Floyd's proof method to CSP using control
predicates. LEVIN & GRIES [1981]  extended OWICKI & GRIES [1976a] 's axiomatic method to
CSP using global shared auxiliary variables (to simulate control states). In sequential
proofs, communication is simply ignored thus any assertion may be placed after a
communication command :

{P} Pli ! E {Q} Send rule (260)
{P} Plj ? X {Q} Receive rule (261)

A satisfaction proof (also called cooperation proof) is then provided for any pair of
communication commands which validates these assumptions :

(P \Pi  P')  \Upsilon  (Q  \Pi  Q')[X  \Xi  E] Satisfaction proof (262)
when

[ ... || Pli :: ... {P} Plj ! E {Q} ... || ... || Plj :: ... {P'} Pli ? X {Q'} ... ||... ]

Not all matching pairs of communication commands can rendezvous, so that satisfaction
proofs for dynamically unmatching pairs can be avoided by a simple static analysis of
programs (APT [1983b], TAYLOR [1983]). The use of shared auxiliary variables necessitates
interference freedom proofs, but many trivial ones can be omitted (MURTAGH [1987]). APT,
FRANCEZ & DE ROEVER [1980] succeeded in restricting the use of auxiliary variables so that
the assertions used in the proof of Pli do not contain free variables subject to changes in
Plj, j ` i. The soundness and relative completeness of their proof system was shown
by  APT [1983a] . A simplified and more comprehensive presentation is given by
APT [1985a]. A restricted and modified version was later introduced by  APT [1985b]  to
prove the correctness of distributed termination algorithms a` la  FRANCEZ [1980]. JOSEPH,
MOITRA & SOUNDARARAJAN [1987]  have extended their proof rules for fault tolerant
programs written in a version of CSP and executing on a distributed system whose

nodes may fail. However, in these approaches, one cannot deal with the individual
processes of a program in isolation from the other processes. The special case of a
single process interacting with its environment is considered in GERGELY & U'RY [1982].

To deal with the individual processes of a program in isolation,  LAMPORT &
SCHNEIDER [1984] reinterpret Hoare's triple {P}C{Q} so that P = Q is a global invariant
during execution of C whereas  BROOKES [1984]  [1986] introduces a new class of
assertions for expressing sets of execution traces. In order to remain faithful to Hoare
interpretation of P as a precondition,  SOUNDARARAJAN [1983] [1984b]  and subsequently
ZWIERS, DE BRUIN & DE ROEVER [1983], ZWIERS, DE ROEVER & VAN EMDE BOAS [1985] allowed to
reason about hidden variables that correspond to the sequences of messages sent and
received by each process up to some moment during the execution of that process, an
idea going back for example to  DAHL [1975] for coroutines,  MISRA & CHANDY [1981]  for
networks of processes,  ZHOU CHAO CHEN & HOARE [1981] , HOARE [1981]  [1984], HEHNER &
HOARE [1983], FRANCEZ, LEHMANN & PNUELI [1984] , OLDEROG & HOARE [1986], FAUCONNIER
[1987] for CSP.

Similar proof rules have been developed for the ADA(TM) rendezvous by  GERTH
[1982] and GERTH & DE ROEVER [1984], for BRINCH HANSEN [1978]'s distributed processes
by SOBEL & SOUNDARARAJAN [1985]  and DE ROEVER [1985b], for a version of  MILNER [1980]
calculus of communicating systems by  PONSE [1989] , and for more abstract
communication mechanisms named "scripts" by  TAUBENFELD & FRANCEZ [1984]  and
FRANCEZ, HAILPERN & TAUBENFELD [1985] . Hoare logic was also extended for proving
partial correctness of parallel logic programming languages (MURAKAMI [1988]).

The dynamic creation and destruction of processes is considered in  ZWIERS, DE
BRUIN & DE ROEVER [1983], DE BOER [1987], FIX & FRANCEZ [1988].

Additional techniques for proving partial or total correctness of communicating
sequential processes with nested parallelism, hiding of communication channels,
buffered message passing etc... are extensively discussed in number of surveys such as
APT [1985], BARRINGER [1985], HOOMAN & DE ROEVER [1986] , ZWIERS [1989].

8 . 1 0 Total correctness

Hoare logic was originally designed for proving partial correctness but has been
extended to cope with termination ( MANNA & PNUELI [1974],  WANG [1976] , SOKOLOWSKI
[1976], HAREL [1979]) including in the case of recursive procedures (PNUELI & STAVI [1977],
SOKOLOWSKI [1977], GRIES & LEVIN [1980] , APT [1981a] , MEYER & MITCHELL [1982] [1983],
MARTIN [1983] , PANDYA & JOSEPH [1986] , BIJLSMA, WILTINK & MATTHEWS [1986] [1989],
AMERICA & DE BOER [1989]).

8 . 1 0 . 1 Finitely bounded nondeterminism and arithmetical

completeness

In the case of deterministic while-programs (  without random assignment "X
:= ?") we can use  HAREL [1979, p. 38]'s rule where P(n) stands for P[x  \Xi  n] and "x" is an
integer valued logical variable not in Var(B, C) :

P(n + 1) \Upsilon  B, [P(n + 1)] C [P(n)], P(0)  \Upsilon  ~B
------------------------------------ While rule (263)

     [\Lambda  n. P(n)] (B * C) [P(0)]

Soundness follows from the fact that nontermination would leads to an infinite strictly
decreasing sequence of integers values n, n-1, ... for the logical variable x. Semantic
completeness follows from the remark that if execution of the while loop does terminate
then after each iteration in the loop body the number of remaining iterations must strictly
decrease and so, can always be chosen as the value of the logical variable x.

Observe that we now go beyond first-order logic and consider  -logic (also called
OE-logic, BARWISE [1977]) that is a two-sorted language with a fixed structure  . Since 
is infinite,  -logic is stronger than first-order logic. This is because there cannot be a
sound and relatively complete deductive system based on a first-order oracle for total
correctness ( HITCHCOCK & PARK [1973] ). For the oracle to be a realistic analog of an
axiomatic proof system, it should be a uniform recursive enumeration procedure P of
the theory Th = {  \Omega    : I P  = tt} i.e. the procedure should operate exactly in the
same way over interpretations I with their theories Th equal to one another and should
be totally sound i.e., as in (129), sound over all interpretations with theory Th. The
argument given in APT [1981a] is that if such a procedure P exists, we could prove using
P and the relatively complete deductive system that "[true] C [true]" where C is "(X :=
0;  (~(X = Y) * X := X + 1))"  holds  for  the  standard  interpretation  IPE of arithmetic

which is expressive by (165). Since P is uniform and totally sound, C should be
guaranteed to terminate for all initial values of X and Y but this is not true for the
nonstandard interpretations of arithmetic when the initial value of X is a standard natural
number and that of Y is a nonstandard one. Moreover it is shown in  GRABOWSKI [1985]
that there cannot be a deductive system that is sound and relatively complete for total
correctness even if, for acceptable languages (e.g. Pascal-like languages ( CLARKE,
GERMAN & HALPERN [1983] )), the deductive system is required to be sound only for
expressive interpretations.

It remains to look for classes of interpretations for which total correctness is
relatively complete. The idea of HAREL [1979], called arithmetical completeness, consists
in extending the interpretation to an arithmetic universe by augmenting it, if necessary,
with the natural numbers and additional apparatus for encoding finite sequences into one

natural. More precisely, following GRABOWSKI [1985] where the set of natural numbers is
not primitive but first-order definable in the interpretations involved, an interpretation I
is k-weakly arithmetic if and only if I is expressive and there exist first-order formulae
N (x),  E (x, y),  Z(x),  Add(x, y, z),  Mult(x, y, z) with at most k quantifiers and
respectively n, 2n, 2n, N, 3n, 3n free variables for some n such that  E defines an
equivalence relation on  In and formulae  N, E, Z, Add and  Mult define on the set
{x : I N(x) } the model M such that the quotient model M /  E is isomorphic to the
standard model  IPE of Peano arithmetic PE with equality  <{0}, {Su, +, *}, o/, # >.

GRABOWSKI [1985] states that for every acceptable programming language with recursion
and for every k \Omega   , Hoare logic for total correctness is relatively complete for k-weakly
arithmetic interpretations (but not for P-weakly arithmetic interpretations). GRABOWSKI
[1988] proceeds along with the comparison of arithmetical versus relative completeness.

In conclusion, the proof systems for total correctness cannot be of pure first-order
logical character but must incorporate the standard model for Peano arithmetic or an
external well-founded relation.

8 . 1 0 . 2 Unbounded nondeterminism

The while rule (263) implies the  strong termination  of while loops that is
(DIJKSTRA [1982b]), for each initial state s there is an integer n(s) such that the loop (B *
C) is guaranteed to terminate in at most n(s) iterations. As first observed by BACK [1981],
no such bound can exist for the program given by DIJKSTRA [1976, p. 77] :

(X <> 0 * (X < 0 ff (X := ?; (X < 0 ff X := -X fi skip)) fi X := X - 1))
in which case termination that is not strong is called  weak termination. In the case of
finitely bounded nondeterminism ( \Delta  o/ \Omega  \Gamma . | { o/' \Omega  \Gamma  : <o/, o/'> \Omega  op C } |  \Omega   ) weak
termination implies strong termination. However when termination is weak but not
strong one can use the following rule due to  APT & PLOTKIN [1986]  and directly deriving
from Floyd's liveness proof method (74), where P( !) stands for P[x  \Xi  !] and "x" is
an ordinal valued logical variable not in Var(B, C) :

(P(!) \Pi  ! > 0) \Upsilon  B, [P(!) \Pi  ! > 0] C [\Lambda  3 < !. P(3)], P(0) \Upsilon  ~B
-------------------------------------------------- While rule (264)

           [\Lambda   ! . P( ! )] (B * C) [P(0)]

The use of ordinal-valued loop counters (as in induction principle (74)) was first
advocated in  BOOM [1982]  but in fact was already proposed by  FLOYD[1967a] and
incorporated in Hoare logic by  MANNA & PNUELI [1974]  under the form of well-founded
sets. APT & PLOTKIN [1986]  have shown that, in the case of countable nondeterminism
(such that | D | = OE), rule (264) is sound and complete (provided the assertion language

 contains the sort of countable ordinals including the constant 0 and the order relation
< upon ordinals), and that all recursive ordinals are needed.

8 . 1 0 . 3 Total correctness of fair parallel programs
8 . 1 0 . 3 . 1 Fairness hypotheses and unbounded nondeterminism

Total correctness of parallel programs usually depends upon  weak  or strong
fairness hypotheses (LAMPORT [1980a], LEHMANN, PNUELI & STAVI [1981] , MANNA & PNUELI
[1984] [1989]  and  FRANCEZ [1986] ) that is assumptions that an action which can be
respectively permanently or infinitely often executed will eventually be executed. For
example  termination  of  "X := true; [ (X * skip) || X := false ]"  is  not  guaranteed
without weak fairness hypothesis since the first process "(X * skip)" can loop for ever
if the second process "X := false" is never activated. Under the weak fairness
hypothesis that no non-terminated process can be eternally delayed, the command "X :=
false" must eventually be executed so that the while command "(X * skip)" terminates
and so does the parallel command "[ (X * skip) || X := false ]".

We write P  ~wf~[C1 || ... || Cn]~~ff Q to state that execution of parallel program

[C1 || ... || Cn] under weakly fair execution hypothesis inevitably lead from P to Q.
This fairness hypothesis is more precisely that on all infinite execution traces  ' no
process k is permanently enabled and never activated :

DEFINITION  Weak fairness hypothesis (265)

\Delta  o/ \Omega  \Gamma .  \Delta  ' \Omega  1OE [C1 || ... || Cn] o/. \Delta  k \Omega  {1, ..., n}.

~(\Delta  i >= 0. \Lambda  o/' \Omega  \Gamma . <'i, o/'> \Omega  op Ck  \Pi  <'i, 'i+1> fl op Ck )

 Observe that in practice the waiting delay is always bounded (say by the lifetime
of the computer) but this bound is unknown. Hence fairness hypotheses with
unbounded waiting delays should be understood as theoretical simplifications of actual
scheduling policies. However this introduces unbounded nondeterminism since for
example "N" can have any finite final value in the program "(X := false; N := 0);
[ (X * N := N + 1) || X := false ]" which terminates under the weak fairness
hypothesis.

8 . 1 0 . 3 . 2 Failure of Floyd liveness proof method

Floyd's liveness proof method (74) is not (directly) applicable to prove
P ~wf~[C1 || ... || Cn]~~ff Q since under fairness hypotheses there may be no variant

function decreasing after each program step.

Counterexample  Failure of Floyd liveness proof method for weak fairness (266)
  Floyd liveness proof method (74) is not applicable to prove termination of "[ (X *
skip) || X := false ]" executed under the weak fairness hypothesis (265). For
simplification, the operational semantics of this program C can be defined by \Gamma  = {a, b}
and op C  = op C1  ' op C2  with op C1  = {<a, a>} and op C2  = {<a, b>} where a is

the configuration in which either "(X  * skip)" or "X := false" is executable and b is
the final configuration. Let P = {a} and Q = { <a, b>}. Applying (74), we would have
some !, i and an infinite chain of ordinals  3k, k >= 0 such that  <a, a> \Omega  i(30) and 0 <  30

< ! [by (74.1) and (74.4) since  <a, a> fl Q] so that assuming by induction hypothesis
that <a, a> \Omega  i(3k) and 3k > 0 their exists some  3k+1 such that <a, a> \Omega  i(3k+1) [by (74.3)
since  <a, a> \Omega  op C ] and 0 <  3k+1 < 3k [by (74.4) since  <a, a> fl Q], a contradiction
since 3k, k  >= 0 is strictly decreasing.  \Delta 

The difficulty is due to the fact that some program steps (such as an iteration in the
loop "(X * skip)" of example (266)) do not directly contribute to termination.
However such inoperative steps contribute indirectly to termination in that their
execution brings the scheduler nearer the choice of an operative step. This can be taken
into account by coding the scheduler into the program ( APT & OLDEROG [1982] ) or by
requiring the variant function to decrease only for such operative steps (LEHMANN, PNUELI
& STAVI [1981]).

8 . 1 0 . 3 . 3 The transformational approach

 Floyd's total correctness proof method can be generalized to liveness properties
of weakly fair parallel programs [C 1 || ... || Cn] by application of induction principle

(74) to a semantics including a scheduler which ensures that execution of the program is
weakly fair :

DEFINITION APT & OLDEROG [1983] Operational semantics of weakly fair (267)

parallel programs

\Gamma ' = ({1, ..., n}  ff  ) x \Gamma 
op'  [C1 || ... || Cn]   = { <<p,  o/>, <p',  o/'>> :  \Lambda  k \Omega  {1, ..., n}.

<o/, o/'> \Omega  op Ck \Pi  ([pk > 0  \Pi  p' < k= p]  \Sigma  [B(p,  o/) \Pi  p' > 0])}
where

(p' <k= p) = [(p' k < pk) \Pi  (\Delta  j \Omega  {1, ..., k - 1, k + 1, ..., n}. p' j = pj)]
B(p, o/) =  [\Delta  k \Omega  {1, ..., n}. ( \Delta  o/' \Omega  \Gamma . <o/, o/'> fl op Ck  \Sigma  pk = 0)]
p' > 0 = [ \Delta  k \Omega  {1, ..., n}. p k > 0]

Execution is organized into rounds within which each process C k will be blocked or
else will be executed at least one and at most p k steps so that p 1, ..., p n can be
interpreted as priorities respectively assigned to processes C 1, ..., Cn. An execution
step within a round consists in executing a step of some process C k with a non-zero
priority pk > 0. After this step the priority p' k of that process C k has strictly decreased
while the priority p'j of other processes Cj is left unchanged (whence p' < k= p). A new
round begins when all processes are either blocked or have a zero priority (whence B(p,
o/) holds), all priorities being strictly positive at the beginning of the next round (whence
p' > 0). This is weakly but not strongly fair since a process which is almost always
enabled but infinitely often disabled may never be activated.

Applying Floyd's liveness induction principle (74) to this transformed semantics,
we get the following induction principle :

THEOREMAPT & OLDEROG [1983] Liveness proof method for weakly fair (268)

parallel programs
P ~wf~[C1 || ... || Cn]~~ff Q =

[\Lambda  ! \Omega   . \Lambda  i \Omega  ! ff ({1, ..., n}  ff  ) ff P(\Gamma  x \Gamma ).

(\Delta  o/ \Omega  P. \Delta  p \Omega  (1, ..., n}  ff  ). \Lambda  3 < !. <o/, o/> \Omega  i(3)(p)) (.1)
\Pi  (\Delta  o/, o/' \Omega  \Gamma . \Delta  p, p'  \Omega  ({1, ..., n}  ff  ). \Delta  3 < !. <o/, o/'> \Omega  i (3)(p) \Upsilon 

<o/, o/'> \Omega  Q (.2)
\Sigma 

[ (\Lambda  o/" \Omega  \Gamma . \Lambda  k \Omega  {1, ..., n}.  <o/', o/"> \Omega  op Ck \Pi  (pk > 0  \Sigma  B(p,  o/'))) (.3)

\Pi  (\Delta  k \Omega  {1, ..., n} . \Delta  o/" \Omega  \Gamma .

[<o/', o/"> \Omega  op Ck  \Pi  ([p k > 0  \Pi  p' < k= p]  \Sigma  [B(p,  o/') \Pi  p' > 0])]

\Upsilon  [\Lambda  3' < 3. <o/, o/"> \Omega  i(3')(p')])])] (.4)

Example  Termination of a weakly fair parallel program by (268) (269)

Applying (268) to prove termination of "[ (X * skip) || X := false ]" with
simplified operational semantics defined by  \Gamma  = {a, b},  op C1  = {<a, a>}, op C2  =

{<a, b>} and specification P = {a} and Q = { <a, b>} we can choose ! = OE and i = oe 3. oe
p.(3 = 0  ff {<a, b>} fi (3 = p1 ff {<a, a>}  fi o/)).  \Delta 

Including the scheduling policy into the induction principle by counting the number of
step executed within each round is often very clumsy.

8 . 1 0 . 3 . 4 The intermittent well-foundedness approach

 Floyd's total correctness proof method can also be generalized to liveness
properties of weakly fair parallel programs by the following more elegant induction
principle. The variant function  3 is not asssumed to decrease after each computation

step. When no such progress is possible, it is sufficient that there exists one
permanently enabled process Ck which decreases the rank 3 when activated. By fairness

hypothesis, this is inevitable. Hence, one has only to record the identity k of the next
process Ck which will make progress to the computation :

THEOREM LEHMANN, PNUELI & STAVI [1981]  Liveness proof method for weakly (270)

fair parallel programs
P ~wf~[C1 || ... || Cn]~~ff Q =

[\Lambda  ! \Omega   . \Lambda  i \Omega  ! ff {1, ..., n}  ff P(\Gamma  x \Gamma ).

(\Delta  o/ \Omega  P. \Lambda  3 < !. \Lambda  k \Omega  {1, ..., n} . <o/, o/> \Omega  i(3)(k)) (.1)
\Pi  (\Delta  o/, o/' \Omega  \Gamma . \Delta  3 < !. \Delta  k \Omega  {1, ..., n} . <o/, o/'> \Omega  i (3)(k) \Upsilon 

<o/, o/'> \Omega  Q (.2)
\Sigma 

[ (\Lambda  o/" \Omega  \Gamma . <o/', o/"> \Omega  op Ck ) (.3)

\Pi  (\Delta  j \Omega  {1, ..., n} . \Delta  o/" \Omega  \Gamma . <o/', o/"> \Omega  op Cj  \Upsilon 

[ (\Lambda  3' <  3. \Lambda  k' \Omega  {1, ..., n} . <o/, o/"> \Omega  i(3')(k')) (.4)
\Sigma  (j ` k \Pi  <o/, o/"> \Omega  i(3, k))])])] (.5)

Starting from any initial state  o/ \Omega  P (270.1), each program step makes progress toward
the goal Q (270.2) since all non-final intermediate states  o/' (which satisfy invariant
i(3)(k) where  3 bounds the number of remaining operative steps and k is an enabled
operative process) are not blocking states (270.3) whence must have a successor state o/"
either by an operative step (270.4) in which case  o/" is closer to the goal or by an
inoperative step (270.5) in which case process Ck remains permanently enabled, which,

by the weak fairness hypothesis guarantees a future progress by (270.4).

Example  Termination of a weakly fair parallel program by (270) (271)

Applying (270) to prove termination of "[ (X * skip) || X := false ]" with
simplified operational semantics defined by  \Gamma  = {a, b},  op C1  = {<a, a>}, op C2  =

{<a, b>} and specification P = {a} and Q = { <a, b>} we can choose  ! = 2, i(0) =  oe
k.{<a, b>}, i(1)(1) = o/ and i(1)(2) = { <a, a>}. \Delta 

(265) and (270) approaches can be generalized to arbitrary semantics (COUSOT
[1985]) and formalized in the temporal logic framework ( MANNA & PNUELI [1984]  [1989]).
LEHMANN, PNUELI & STAVI [1981]  and MANNA & PNUELI [1989]  consider generalizations of
(270) for strong fairness.  GRU"MBERG & FRANCEZ [1982]  and GRU"MBERG, FRANCEZ & KATZ
[1983]  respectively consider weak and strong equifairness where a permanently or
infinitely often enabled process is infinitely often activated with the further requirement
for the scheduler to give an equally fair chance to each process in a group of jointly
enabled processes.  FRANCE & KOZEN [1984]  present a unifying generalization of these

fairness and equifairness notions by parametrization of the enabling and activating
conditions.

APT [1984]  [1988] and  FRANCEZ [1986]  are surveys of fair parallel program
correctness proof methods with numerous references to the literature.

8 . 1 0 . 4 Dijkstra's weakest preconditions calculus

DIJKSTRA [1975] [1976] introduced the calculus of weakest preconditions as a
generalization of Hoare logic to total correctness as first considered in FLOYD [1967a] :

DEFINITION  DIJKSTRA [1975]   Weakest precondition (272)

wp :   x   ff 
wp(C, q) =  ' {p \Omega    : [p]C[q]}

The operational interpretation of the assertion wp(C, q) is that any valid implementation
of C, when started in any state satisfying wp(C, q), should lead to a finite computation
that ends in a state of q and that it is the weakest such assertion :

THEOREM  DIJKSTRA [1976]  Characterization of weakest preconditions (273)

\Delta  C \Omega   . \Delta  p, q  \Omega   .

[wp(C, q) ]C[q] \Pi  (.1)
[p]C[q] \Upsilon  p ffl wp(C, q) (.2)

DIJKSTRA [1975] axiomatized wp as follows (the continuity condition (*W3.5) was later
introduced in  DIJKSTRA [1976, P. 72]  to take into account strong termination of finitely
bounded nondeterminism (DIJKSTRA [1982b] [1982c])) :

THEOREM  DIJKSTRA [1975] [1976]   Healthiness  criteria (274)

\Delta  C \Omega   . \Delta  p, q  \Omega   .

wp(C, o/) = o/ (.1)
(p ffl q) \Upsilon  (wp(C, p) ffl wp(C, q)) (.2)
wp(C, p ^ q) = wp(C, p) ^ wp(C, q) (.3)
wp(C, p ' q) = wp(C, p) ' wp(C, q) (.4)
\Delta  s \Omega  S. | {s' :  <s, s'> \Omega  C} | \Omega    \Upsilon  (.5)

\Delta  p \Omega    ff  . (\Delta  i \Omega   . pi ffl pi+1) \Upsilon  (wp(C, ' i \Omega    pi) = ' i \Omega    wp(C, pi))

Continuity of  wp (274.5) is obviously violated for unbounded nondeterminism since
for example if we let  D =   and pi = {s \Omega  S : s(X) <= i} for i  >= 0 then \Delta  i \Omega   . pi ffl pi+1
but S = wp(X := ?,  S) = wp(X := ?,  ' i \Omega    pi) ` ' i \Omega    wp(X := ?, p i) = ' i \Omega    o/ =

o/, a contradiction when | D | >= 1.

Dijkstra's weakest preconditions form a calculus for the derivation of programs that
"turned program development into a calculational activity (and the idea of program
correctness into a calculational notion)" ( DIJKSTRA [1984] ). This point of view was
extensively developed in  DIJKSTRA [1976]  and GRIES [1981]. The basis of this calculus is
the following theorem (case (275.7) corresponding to bounded nondeterminism) :

THEOREM DIJKSTRA [1975] (YEH [1976], HOARE [1978a], CLARKE [1979], (275)

HEHNER [1979], APT & PLOTKIN [1986])

wp(skip, q) = q (.1)
wp(X := E, q) = {s  \Omega  S : s[X  \Xi  E(s)]  \Omega  q} (.2)
wp(X := ?, q) = {s  \Omega  S : \Delta  d \Omega  D. s[X \Xi  d] \Omega  q} (.3)
wp((C1; C2), q) = wp(C1, wp(C2, q))  (.4)
wp((B ff C1 fi C2), q) = ( B ^ wp(C1, q)) ' (~B ^ wp(C2, q))  (.5)
wp((B * C), q) =  lfp oe X. (B ^ wp(C, X))  ' (~B ^ q) (.6)
\Delta  s \Omega  S. | {s' :  <s, s'> \Omega  C} | \Omega    \Upsilon  (.7)

\Delta  p \Omega    ff  .

(p0 = ~B ^ q) \Pi  (\Delta  i \Omega   . pi+1 = (B ^ wp(C, pi)) ' p0)
\Upsilon  (wp((B * C), q) =  ' i \Omega    pi)

The derivation of weakest preconditions can be impractical for loops. Therefore (275.6)
and (276.7) are advantageously replaced by the following theorem (using an invariant p
and a variant function t as in FLOYD [1967a]) :

THEOREM  DIJKSTRA [1976] (BACK [1981], DIJKSTRA & GASTEREN [1986] ) (276)

(\Lambda  D. \Lambda  W ffl D. \Lambda  t \Omega  DS.

wf(W, -<)
\Pi  ((p ^ B) ffl {s \Omega  S : t(s) \Omega  W}
\Pi  (\Delta  x \Omega  D. (p ^ B ^ {s \Omega  S : t(s) = x}) ffl wp(C, p ^ {s \Omega  S : t(s) -< x})))
\Upsilon  (p ffl wp((B * C), ~ B ^ q))

Dijkstra's weakest precondition calculus has been formalized in a number of ways such
as for example using the infinitary logics LOE1OE (for finitely bounded nondeterminism) or
LOE1OE1 (for unbounded nondeterminism,  BACK [1980]  [1981]), linear algebra ( MAIN &
BENSON [1983] ), category theory ( WAGNER [1986]), etc. It can be extended to more
language features ( DE ROEVER [1976] , MILNE [1978], HEHNER [1979] , VAN LAMSWEERDE &
SINTZOFF [1979], GRIES & LEVIN [1980],  DE BAKKER [1980, Ch. 7] , FLON & SUZUKI [1981] , GRIES
[1981],  PARK [1981],  MARTIN [1983] , ELRAD & FRANCEZ [1984] , BIJLSMA, WILTINK &
MATTHEWS [1986] [1989], BROY & NELSON [1989] , HESSELINK [1989] ), thus loosing part of
their original simplicity when considering complicated languages. Various
generalizations have been introduced by  BACK [1980] , HOARE & HE JIFENG [1986] [1987] ,

HOARE, HAYES, HE JIFENG, MORGAN, ROSCOE, SANDERS, SORENSEN & SUFRIN [1987] , JACOBS &
GRIES [1985], LAMPORT [1987], NELSON [1987], BACK & VON WRIGHT [1989].

8 . 1 1 Examples of program verification

Classical examples of program verification using Floyd-Naur's proof method,
Hoare logic or Dijkstra's weakest preconditions calculus are given in  LONDON [1970a]
[1970b], HOARE [1971a] [1972b] , FOLEY & HOARE [1971],  MANNA [1974], DIJKSTRA [1975] , GRIES
[1977], WAND [1980], LOECKX & SIEBER [1984], FOKKINGA [1987], GRIBOMONT [1989].

8 . 1 2 Other logics extending first-order logic with

programs

Hoare's idea of extending first-order logic with programs or for program proofs
has also been exploited in a number of formal systems such as the algorithmic logics of
ENGELER [1967] [1968] [1975]  and SALWICKI [1970], RASIOWA [1979], the computational logic
of BOYER & MOORE [1979] [1988], the dynamic logic of PRATT [1976], HAREL [1979] [1980], the
first order programming logic of CARTWRIGHT [1983] [1984], the predicative semantics of
HEHNER [1984a] [1984b] , HOARE [1984], HEHNER, GUPTA & MALTON [1986],  the programming
logic of CONSTABLE [1977] [1983] , CONSTABLE & O'DONNELL [1978], CONSTABLE, JOHNSON &
EICHENLAUB [1982],  the  situational calculus  of  MANNA & WALDINGER [1981] , the
programming calculus of MORRIS [1987b], the specification logic of REYNOLDS [1982] (see
also  TENNENT [1985] ), the  weakest preconditions calculus  of  DIJKSTRA [1976]  (see
paragraph $ 8.10.3), the  weakest prespecification  of HOARE & HE JIFENG [1986] [1987] ,
HOARE, HE & SANDERS [1987], HOARE, HAYES, HE JIFENG, MORGAN, ROSCOE, SANDERS, SORENSEN
& SUFRIN [1987]  (see also chapter 11 on "Logics of Programs" by  KOZEN & TIURYN  and
chapter 16 on "Temporal and Modal Logic" by  EMERSON in the second volume of this
handbook).

9 . R e f e r e n c e s
ABRAMOV, S. V.

[1981] Remark on the method of intermediate assertions,  Soviet Math. Dokl.  24 (1), 91-93.
[1984] The nature of the incompleteness of the Hoare system,  Soviet Math. Dokl.  29 (1), 83-84.
AHO, A. V., SETHI, R. & ULLMAN, J. D.

[1986] Compilers; principles, techniques and tools, Addison-Wesley, Reading, 796 p.
AHO, A. V. & ULLMAN, J. D.

[1979] Universality of data retrieval languages,  Conference record of the sixth ACM SIGACT-SIGPLAN

symposium on Principles Of Programming Languages , 110-117.
ALPERN, B. & SCHNEIDER, F. B.

[1985] Defining liveness,  Information Processing Letters  21, North-Holland, Amsterdam, 181-185.
AMERICA, P. & DE BOER F. S.

[1989] Proving total correctness of recursive procedures, Research report CS-R8904, Centrum voor

Wiskunde en Informatica, Amsterdam, 33 p.
ANDRE'KA, H. & NE'METI, I.

[1978] Completeness of Floyd logic,  Bull. Section of Logic, Wroclaw  7, 115-121.
ANDRE'KA, H., NE'METI, I. & SAIN, I.

[1979] Completeness problems in verification of programs and program schemes, In  Mathematical

Foundations of Computer Science 1979 , J. Becva'r (Ed.),  Lecture Notes in Computer Science  74,
Springer-Verlag, Berlin - New York, 208-218.
[1981] A  characterization of Floyd-provable programs,  Lecture Notes in Computer Science  118,

Springer-Verlag, Berlin - New York, 162-171.
[1982] A  complete logic for reasoning about programs via non-standard model theory, Parts I-II,

Theoretical Computer Science  17, North-Holland, Amsterdam, 193-212 and 259-278.
ANDREWS, G. R.

[1981] Parallel programs : proofs, principles, and practice,  Communications of the Association for

Computing Machinery  24 (3), 140-146.
APT, K. R.

[1978] A  sound and complete Hoare-like system for a fragment of Pascal, Research Report IW 96/78,

Afdeling informatica, Mathematish Centrum, Amsterdam, 59 p.
[1981a] Ten years of Hoare's logic: a survey - part I,  ACM Transactions On Programming Languages And

Systems 3 (4), 431-483.
[1981b] Recursive assertions and parallel programs,  Acta informatica  15, 219-232.
[1983a] Formal  justification of a proof system for communicating sequential processes,  Journal of the

Association for Computing Machinery  30 (1), 197-216.
[1983b]  A static analysis of CSP programs, In  Logics of programs,  Ed. Clarke & D. Kozen (Eds.),

Lecture Notes in Computer Science  164, Springer-Verlag, Berlin - New York, 1-17.
[1984] Ten years of Hoare's logic: a survey - part II: nondeterminism,  Theoretical Computer Science

28, North-Holland, Amsterdam, 83-109.
[1985a] Proving correctness of CSP programs, a tutorial,  Control Flow and Dataflow : Concepts of

Distributed programming , M. Broy (Ed.), Springer-Verlag, Berlin - New York, 441-474.
[1985b] Correctness proofs of distributed termination algorithms, In  Logics and Models of Concurrent

Systems, K. R. Apt (Ed.), NATO ASI Series, Vol. F13, Springer-Verlag, Berlin - New York,
147-167.
[1988] Proving correctness of concurrent programs : a quick introduction, In  Trends in Theoretical

Computer Science , E. Bo"rger (Ed.), Computer Science Press, Rockville, 305-345.
APT, K. R., BERGSTRA, J. A. & MEERTENS, L. G. L. T.

[1979] Recursive assertions are not enough - or are they ?,  Theoretical Computer Science  8, NorthHolland, Amsterdam, 73-87.
APT, K. R. & DE BAKKER, J. W.

[1977] Semantics and proof theory of PASCAL procedures, In  Fourth International Colloquium on

Automata, Languages and Programming , A. Salomaa & M. Steinby (Eds.),  Lecture Notes in
Computer Science  52, Springer-Verlag, Berlin - New York, 30-44.

APT, K. R. & FRANCEZ, N.

[1984] Modeling the distributed termination convention of CSP,  ACM Transactions On Programming

Languages And Systems  6 (3), 370-379.
APT, K. R., FRANCEZ, N. & DE ROEVER, W. P.

[1980] A  proof system for communicating sequential processes,  ACM Transactions On Programming

Languages And Systems  2 (3), 359-385.
APT, K. R. & MEERTENS, L. G. L. T.

[1980] Completeness with finite systems of intermediate assertions for recursive program schemes,

SIAM Journal on Computing  9 (4), 665-671.
APT, K. R. & OLDEROG, E.-R.

[1983] Proof rules and transformations dealing with fairness,  Science of Computer Programming  3,

North-Holland, Amsterdam, 65-100.
APT, K. R. & PLOTKIN, G. D.

[1986] Countable nondeterminism and random assignment,  Journal of the Association for Computing

Machinery 33 (4), 724-767.
ARBIB, M. A. & ALAGIC, S.

[1979] Proof rules for gotos,  Acta Informatica  11, 139-148.
ASHCROFT, E. A.

[1975] Proving assertions about parallel programs,  Journal of Computer and System Sciences  10 (1),

110-135.
ASHCROFT, E. A., CLINT, M. & HOARE, C. A. R.

[1976] Remarks on " Pro gram proving : jumps and functions by M. Clint and C.A.R. Hoare",  Acta

informatica 6, 317-318.
ASHCROFT, E. A. & MANNA, Z.

[1970] Formalization of properties of parallel programs,  Machine Intelligence  6, Edinburgh University

Press, 17-41.
BABICH, A. F.

[1979] Proving the total correctness of parallel programs,  IEEE Transactions on Software Engineering ,

SE-5 (6),  558-574.
BACK, R. J. R.

[1980] Correctness preserving program refinements : proof theory and applications,  Mathematical

Centre Tracts  131, Mathematisch centrum, Amsterdam.
[1981] Proving total correctness of nondeterministic programs in infinitary logic,  Acta Informatica

15, 233-249.
BACK, R. J. R. & VON WRIGHT, J.

[1989] A lattice-theoretical basis for a specification language, In  Mathematics of Program

Construction, J. L. A. van de Snepscheut (Ed.),  Lecture Notes in Computer Science  375,
Springer-Verlag, Berlin - New York, 139-156.
BARRINGER, H.

[1985] A  survey of verification techniques for parallel programs,  Lecture Notes in Computer Science

191, Springer-Verlag, Berlin - New York.
BARRINGER, H., CHENG, J. H. & JONES, C. B.

[1984] A logic covering undefinedness in program proofs,  Acta Informatica  21, 251-269.
BARWISE, J.

[1977] An  introduction to first-order logic, In  Handbook of Mathematical Logic , J. Barwise (Ed.),

North-Holland, Amsterdam (1978), 5-46.
BERGSTRA, J. A., CHMIELINSKA, A. & TIURYN, J.

[1982a] Another incompleteness result for Hoare's logic,  Information and control  52, 159-171.
[1982b] Hoare's logic is incomplete when it does not have to be,  Lecture Notes in Computer Science

131, Springer-Verlag, Berlin - New York, 9-23.
BERGSTRA, J. A. & KLOP, J. W.

[1984] Proving program inclusion using Hoare's logic,  Theoretical Computer Science  30, NorthHolland, Amsterdam, 1-48.
BERGSTRA, J. A. & TIURYN, J.

[1983] PC-compactness, a necessary condition for the existence of sound and complete logics for

partial correctness, In  Logics of programs,  Ed. Clarke & D. Kozen (Eds.),  Lecture Notes in
Computer Science  164, Springer-Verlag, Berlin - New York, 45-56.
BERGSTRA, J. A., TIURYN, J. & TUCKER, J. V.

[1982] Floyd's principle, correctness theories and program equivalence,  Theoretical Computer Science

17, North-Holland, Amsterdam, 113-149.

BERGSTRA, J. A. & TUCKER, J. V.

[1981] Algebraically specified programming systems and Hoare's logic,  Lecture Notes in Computer

Science 115, Springer-Verlag, Berlin - New York, 348-362.
[1982a] Some natural structures which fail to possess a sound and decidable Hoare-like logic for their

while-programs,  Theoretical Computer Science  17, North-Holland, Amsterdam, 303-315.
[1982b] Expressiveness and the completeness of Hoare's Logic,  Journal of Computer and System

Sciences 25, 3, 267-284.
[1982c] Two theorems about the completeness of Hoare's logic,  Information Processing Letters  15 (4),

143-149.
[1983] Hoare's logic and Peano's arithmetic,  Theoretical Computer Science  22, North-Holland,

Amsterdam, 265-284.
[1984] The axiomatic semantics of programs based on Hoare's logic,  Acta Informatica  21, 293-320.
BIJLSMA, A., WILTINK, J.G. & MATTHEWS, P. A.

[1986] Equivalence of the Gries and Martin proof rules for procedure calls,  Acta Informatica  23, 357-

360.
[1989] A sharp proof rule for procedures in wp semantics,  Acta Informatica  26, 409-419.
BIRO', B.

[1981] On  the completeness of program verification methods,  Bull. Section of Logic , Wroclaw 10 (2),

***-***.
BLIKLE, A.

[1981] The clean termination of iterative programs,  Acta Informatica  16, 199-217.
BOEHM, H.-J.

[1982] A  logic for expressions with side effects,  Conference record of the ninth ACM SIGACTSIGPLAN symposium on Principles Of Programming Languages , 268-280.
[1985] Side effects and aliasing can have simple axiomatic descriptions,  ACM Transactions On

Programming Languages And Systems  7 (4), 637-655.
BOOLOS, G. S. & JEFFREY, R. C.

[1974] Computability and logic, Cambridge University Press, Cambridge, 1974, 1980.
BOOM, H. J.

[1982] A weaker  precondition for loops,  ACM Transactions On Programming Languages And Systems

4 (4), 668-677.
BOYER, R. S. & MOORE, J. S.

[1979] A computational logic, Academic Press, New York.
[1988] A computational logic handbook, Academic Press, New York.
BRINCH HANSEN, P.

[1978] Distributed processes : a concurrent programming concept,  Communications of the Association

for Computing Machinery  21 (11), 934-941.
BROOKES, S. D.

[1984] On  the axiomatic treatment of concurrency, In  Seminar on Concurrency , S. D. Brookes, A. W.

Roscoe & G. Winskel (Eds.),  Lecture Notes in Computer Science  197, Springer-Verlag, Berlin -
New York, 1-34.
[1986] A semantically based proof system for partial correctness and deadlock in CSP, In  Proceedings

symposium on Logic In Computer Science 1986 , IEEE Computer Society Press, 58-65.
BROY, M. & NELSON, G.

[1989] Can fair choice be added to Dijkstra's calculus ?, Research Report MIP - 8902, Fakulta"t fu"r

Mathematik und Informatik, Universita"t Passau, West Germany.
BURSTALL, R. M.

[1972] Some techniques for proving correctness of programs which alter data structures,  Machine

Intelligence 7, Edinburgh University Press, 23-50.
[1974] Program proving as hand simulation with a little induction, In  Information Processing 74 ,

North-Holland, Amsterdam, 308-312.
CARTWRIGHT, R.

[1983] Non-standard fixed points in first-order logic, In  Logics of programs,  Ed. Clarke & D. Kozen

(Eds.), Lecture Notes in Computer Science  164, Springer-Verlag, Berlin - New York, 129-146.
[1984] Recursive programs as definitions in first order logic,  SIAM Journal on Computing  13 (2), 374-

408.
CARTWRIGHT, R. & OPPEN, D. C.

[1978] Unrestricted procedure calls in Hoare's logic,  Conference record of the fifth ACM SIGACTSIGPLAN symposium on Principles Of Programming Languages , 131-140.
[1981] The logic of aliasing,  Acta Informatica  15, 365-384.

CHERNIAVSKY, J. & KAMIN, S.

[1977] A  complete and consistent Hoare axiomatics for a simple programming language,  Conference

record of the fourth ACM SIGACT-SIGPLAN symposium on Principles Of Programming
Languages, 131-140 and  Journal of the Association for Computing Machinery  26, (1979), 119-
128.
CHURCH, A.

[1936] An  unsolvable problem of elementary number theory,  Amer. Jour. Math. , 58, 345-363.
CLARKE, E. M. Jr

[1977] Programming language constructs for which it is impossible to obtain good Hoare axiom

systems,  Conference record of the fourth ACM SIGACT-SIGPLAN symposium on Principles Of
Programming Languages , 10-20 and  Journal of the Association for Computing Machinery  26
(1), (1979) 129-147.
[1979] Program invariants as fixedpoints,  Computing 21, 273-294.
[1980] Proving correctness of coroutines without history variables,  Acta Informatica  13, 169-188.
[1984] The characterization problem for Hoare logic,  Phil. Trans. R. Soc. Lond.  A 312, 423-440.
CLARKE, E. M. Jr, GERMAN, S. M. & HALPERN, J. Y.

[1983] Effective axiomatizations of Hoare Logics,  Journal of the Association for Computing

Machinery 30 (3), 612-636.
CLINT, M.

[1973] Program proving : coroutines,  Acta Informatica , 2, 50-63.
[1981] On the use of history variables,  Acta Informatica  16, 15-30.
CLINT, M. & HOARE, C. A. R.

[1972] Program proving : jumps and functions,  Acta Informatica  1, 214-224.
COHEN, P. J.

[1966]  Set theory and the continuum hypothesis, W. A. Benjamin Inc., New York.
COLEMAN, D. & HUGUES, J. W.

[1979] The clean termination of Pascal programs,  Acta Informatica  11, 195-210.
CONSTABLE, R. L.

[1977] On  the theory of programming logic,  Conference record of the ninth annual ACM Symposium

on Theory Of Computing , 269-285.
[1983] Mathematics as programming, In  Logics of programs,  Ed. Clarke & D. Kozen (Eds.),  Lecture

Notes in Computer Science  164, Springer-Verlag, Berlin - New York, 116-128.
CONSTABLE, R. L., JOHNSON, S. & EICHENLAUB, C.

[1982] Introduction to the PL/CV2 programming logic,  Lecture Notes in Computer Science  135,

Springer-Verlag, Berlin - New York.
CONSTABLE, R. L. & O'DONNELL, M. J.

[1978] A  programming logic, Winthrop, Cambridge Massachusetts.
COOK, S. A.

[1971] A  characterization of pushdown machines in terms of time-bounded computers,  Journal of the

Association for Computing Machinery  18 (1), 4-18.
[1978] Soundness and completeness of an axiom system for program verification,  SIAM Journal on

Computing 7 (1), 70-90.
COOK, S. A. & OPPEN, D. C.

[1975] An  assertion language for data structures,  Conference record of the second ACM SIGACTSIGPLAN symposium on Principles Of Programming Languages , 160-166.
COURCELLE, B.

[1985] Proofs of partial correctness for iterative and recursive computations, In  Logic colloquium `85 ,

The Paris Logic Group (Ed.), North-Holland, Amsterdam, 1987, 89-110.
COUSOT, P.

[1981] Semantic foundations of program analysis, In  Program flow analysis: theory and practice , S.S.

Muchnick & N. D. Jones (Eds.), Prentice-Hall, Englewood Cliffs, 303-342.
COUSOT, P. & COUSOT, R.

[1979] A  constructive version of Tarski's fixpoint theorems,  Pacific Journal of Mathematics  82 (1),

43-57.
[1980] Semantic analysis of communicating sequential processes, In  Seventh International Colloquium

on Automata, Languages and Programming , J. W. De Bakker & J. van Leeuwen (Eds.),  Lecture
Notes in Computer Science  85, Springer-Verlag, Berlin - New York, 119-133.
[1982] Induction principles for proving invariance properties of programs, In  Tools & notions for

program construction, D. Ne'el (Ed.), Cambridge University Press, 75-119.

[1984] Invariance proof methods and analysis techniques for parallel programs, In  Automatic program

construction techniques , A. W. Biermann, G. Guiho & Y. Kodratoff (Eds.), Macmillan, New
York, 243-272.
[1985] "A la  Floyd" induction principles for proving inevitability properties of programs, In  Algebraic

methods in semantics , M. Nivat & J. Reynolds (Eds.), Cambridge University Press, Cambridge,
277-312.
[1987] Sometime = always + recursion = always, on the equivalence of the intermittent and invariant

assertions methods for proving inevitability properties of programs,  Acta informatica  24, 1-
31.
[1989] A  language independent proof of the soundness and completeness of Generalized Hoare Logic,

Information and Computation  80 (2), 165-191.
CRASEMANN, Ch. & LANGMAACK, H.

[1983] Characterization of acceptable by Algol-like programming languages, In  Logics of programs,

Ed. Clarke & D. Kozen (Eds.),  Lecture Notes in Computer Science  164, Springer-Verlag, Berlin -
New York, 129-146.
CSIRMAZ, L.

[1980] Structure of program runs of nonstandard time,  Acta Cybernet.  4, 325-331.
[1981a] On  the completeness of proving partial correctness,  Acta Cybernet.  5, 181-190.
[1981b] Programs and program verifications in a general setting,  Theoretical Computer Science  16,

North-Holland, Amsterdam, 199-210.
CSIRMAZ, L. & HART, B.

[1986] Program correctness on finite fields, In  Proceedings symposium on Logic In Computer Science

1986, IEEE Computer Society Press, 4-10.
CUNNINGHAM, R. J., & GILFORD, M. E. J.

[1976] A  note on the semantic definition of side effects,  Information Processing Letters  4 (5), NorthHolland, Amsterdam, 118-120.
DAHL, O.-J.

[1975] An  approach to correctness proofs of semi-coroutines, In  Proceedings of the symposium and

summer school on mathematical foundations of computer science , A. Blikle (Ed.),  Lecture Notes
in Computer Science  28, Springer-Verlag, Berlin - New York, 157-174.
DAHL, O.-J. & NYGAARD, K.

[1966] SIMULA - An ALGOL-based simulation language,  Communications of the Association for

Computing Machinery  9, 671-678.
DAMM, W. & JOSKO, B.

[1983a] A sound and relatively * complete axiomatization of Clarke's language L 4, In  Logics of

programs, Ed. Clarke & D. Kozen (Eds.),  Lecture Notes in Computer Science  164, SpringerVerlag, Berlin - New York, 161-175.

[1983b] A sound and relatively * complete Hoare-logic for a language with higher type procedures,  Acta

Informatica 20, 59-101.
DAVIS, M.

[1977] Unsolvable problems, In  Handbook of Mathematical Logic , J. Barwise (Ed.), North-Holland,

Amsterdam (1978), 567-594.
DE BAKKER, J. W.

[1976] Semantics and termination of nondeterministic recursive programs, In  Third International

Colloquium on Automata, Languages and Programming , S. Michaelson & R. Milner (Eds.),
University Press, Edinburgh, 436-477.
[1980] Mathematical theory of program correctness, Prentice-Hall, Englewood Cliffs.
DE BAKKER, J. W. & DE ROEVER, W. P.

[1972] A  calculus for recursive program schemes, In  First International Colloquium on Automata,

Languages and Programming , M. Nivat (Ed.), North-Holland, Amsterdam, 167-196.
DE BAKKER, J. W., KLOP, J. W. & MEYER J.-J. Ch.

[1982] Correctness of programs with function procedures, In  Logics of Programs , D. Kozen (Ed.),

Lecture Notes in Computer Science  131, Springer-Verlag, Berlin - New York, 94-112.
DE BAKKER, J. W. & MEERTENS, L. G. L. T.

[1975] On  the completeness of the inductive assertion method,  Journal of Computer and System

Sciences 11 (3), 323-357.

DE BOER, F. S.

[1987] A  proof rule for process-creation, In  Formal description of programming concepts III

(Proceedings of the IFIP TC2 WG2.2 Working Conference on Formal Description of
Programming Concepts, Ebberup, Denmark, 25-28 August 1986), M. Wirsing (Ed.), NorthHolland, Amsterdam, ***-***.
DE BRUIN, A.

[1981] Goto statements : semantics and deduction system,  Acta informatica  15, 385-424.
[1984] On the existence of Cook semantics,  SIAM Journal on Computing  13 (1), 1-13.
DEMBINSKI, P. & SCHWARTZ, R. L.

[1976] The pointer type in programming languages : a new approach, In  Programmation, proceedings

of the second international symposium on programming , B. Robinet (Ed.), Dunod, Paris, 89-
105.
DE MILLO, R. A., LIPTON, R. J. & PERLIS, A. J.

[1979] Social processes and proofs of theorems and programs,  Communications of the Association for

Computing Machinery  22 (5), 271-280.
DE ROEVER, W. P.

[1974] Recursion and parameter mechanisms : an axiomatic approach, In  Second International

Colloquium on Automata, Languages and Programming , J. Loeckx (Ed.),  Lecture Notes in
Computer Science  14, Springer-Verlag, Berlin - New York, 34-65.
[1976] Dijkstra's predicate transformer, non-determinism, recursion and termination, In  Mathematical

Foundations of Computer Science , Lecture Notes in Computer Science  45, Springer-Verlag,
Berlin - New York, 472-481.
[1985a] The quest for compositionality, a survey of assertion-based proof systems for concurrent

programs, Part 1 : concurrency based on shared variables, In  Formal Models in Programming , E.
J. Neuhold & C. Chroust (Eds.), Elsevier Science Publishers B. V. (North-Holland, Amsterdam),
(C) IFIP, 181-205.
[1985b] The cooperation test : a syntax-directed verification method, In  Logics and Models of

Concurrent Systems, K. R. Apt (Ed.), NATO ASI Series, Vol. F13, Springer-Verlag, Berlin - New
York, 213-257.
DIJKSTRA, E. W.

[1968] A constructive approach to the problem of program correctness,  BIT 8, 174-186.
[1975] Guarded commands, nondeterminacy and formal derivation of programs,  Communications of the

Association for Computing Machinery  18 (8), 453-457.
[1976] A discipline of programming, Prentice-Hall, Englewood Cliffs.
[1982a] On  subgoal induction, In  Selected writings on computing: a personal perspective , SpringerVerlag, Berlin - New York, 223-224.
[1982b] On weak and strong termination, In  Selected writings on computing: a personal perspective , 

Springer-Verlag, Berlin - New
York, 355-357.
[1982c] The equivalence of bounded nondeterminacy and continuity, In  Selected writings on computing:

a personal perspective , Springer-Verlag, Berlin - New York, 358-359.
[1982d] A personal summary of the Gries-Owicki theory, In  Selected writings on computing: a personal

perspective, Springer-Verlag, Berlin - New York, 188-199.
[1984] Invariance and non-determinacy,  Phil. Trans. R. Soc. London , A 312, 491-499.
DIJKSTRA, E. W. & GASTEREN, A. J. M.

[1986] A  simple fixpoint argument without the restriction to continuity,  Acta Informatica  23, 1-7.
DONAHUE, J. E.

[1976] Complementary definitions of programming language semantics,  Lecture Notes in Computer

Science 42, Springer-Verlag, Berlin - New York.
ELRAD, T. & FRANCEZ, N.

[1984] A  weakest precondition semantics for communicating processes,  Theoretical Computer Science

29, North-Holland, Amsterdam, 231-250.
ENDERTON, H. B.

[1972] A mathematical introduction to logic, Academic Press, New York.
[1977] Elements of recursion theory, In  Handbook of Mathematical Logic , J. Barwise (Ed.), NorthHolland, Amsterdam (1978), 527-566.

ENGELER, E.

[1967] Algorithmic properties of structures,  Math. Systems Theory  1, 183-195.
[1968] Remarks on the theory of geometrical constructions, In  The syntax and semantics of infinitary

languages, J. Barwise (Ed.),  Lecture Notes in Mathematics  72, Springer-Verlag, Berlin - New
York, 64-76.
[1975] Algorithmic logic, In  Foundations of computer science , J. W. De Bakker (Ed.),  Mathematical

Center Tracts  63, Mathematisch centrum, Amsterdam, 57-85.
ERNST, G. W.

[1977] Rules of inference for procedure calls,  Acta Informatica  8, 145-152.
ERNST, G. W., NAVLAKHA, J. K. & OGDEN, W. F.

[1982] Verification of programs with procedure-type parameters,  Acta Informatica  18, 149-169.
FAUCONNIER, H.

[1987] Se'mantique asynchrone et comportements infinis en CSP,  Theoretical Computer Science  54,

North-Holland, Amsterdam, 277-298.
FIX, L. & FRANCEZ, N.

[1988] Proof rules for dynamic process creation and destruction, manuscript, 37 p.
FLON, L. & SUZUKI, N.

[1981] The total correctness of parallel programs,  SIAM Journal on Computing  10 (2), 227-246.
FLOYD, R. W.

[1967a] Assigning meanings to programs, In  Proc. Symp. in Applied Mathematics , J. T. Schwartz (Ed.),

19, 19-32.
[1967b] Nondeterministic algorithms,  Journal of the Association for Computing Machinery , 14 (4),

636-644.
FOKKINGA, M. M.

[1978]   Axiomatization of declarations and the formal treatment of an escape construct, In  Formal

Descriptions of Programming Concepts , E. J. Neuhold (Ed.), North-Holland, Amsterdam, 221-
235.
[1987] A  correctness proof of sorting by means of formal procedures,  Science of Computer

Programming 9, North-Holland, Amsterdam, 263-269.
FOLEY, M. & HOARE, C. A. R.

[1971] Proof of a recursive program : QUICKSORT,  Computer Journal 14 (4), 391-395.
FRANCEZ, N.

[1980] Distributed termination,  ACM Transactions On Programming Languages And Systems  2 (1), 42-

55.
[1986] Fairness, Springer-Verlag, Berlin - New York.
FRANCEZ, N., HAILPERN, B. & TAUBENFELD, G.

[1985] Script : a communication abstraction mechanism and its verification, In  Logics and Models of

Concurrent Systems , K. R. Apt (Ed.), NATO ASI Series, Vol. F13, Springer-Verlag, Berlin -
New York, 169-212.
FRANCEZ, N. & KOZEN, D.

[1984] Generalized fair termination,  Conference record of the eleventh ACM SIGACT-SIGPLAN

symposium on Principles Of Programming Languages , 46-53.
FRANCEZ, N., LEHMANN, D. & PNUELI, A.

[1984] A  linear history semantics for languages for distributed programming,  Theoretical Computer

Science 32, North-Holland, Amsterdam, 25-46.
FRANCEZ, N. & PNUELI, A.

[1978] A proof method for cyclic programs,  Acta Informatica  9, 133-157.
GAIFMAN, H. & VARDI, M. Y.

[1985] A simple proof  that connectivity of finite graphs is not first-order definable,  Bulletin of

European Association for Theoretical Computer Science , June 1985, 43-45.
GALLIER, J. H.

[1978] Semantics and correctness of nondeterministic flowchart programs with recursive procedures, In

Fifth International Colloquium on Automata, Languages and Programming , G. Ausiello & C.
Bo"hm (Eds.),  Lecture Notes in Computer Science  62, Springer-Verlag, Berlin - New York, 252-
267.
[1981] Nondeterministic flowchart programs with recursive procedures : semantics and correctness,

Theoretical Computer Science  13, North-Holland, Amsterdam, Part I : 193-223, Part II : 239-
270.
GERGELY, T. & SZO"TS, M.

[1978] On the incompleteness of proving partial correctness,  Acta cybernetica  4 (1), Szeged., 45-57.

GERGELY, T. & U'RY, L.

[1978] Time models for programming logics, In  Mathematical Logic in Computer Science,

(Salgo'tarja'n, Hungary, 1978), B. Do"mo"lki & T. Gergely (Eds.), 359-427,  Colloquia
Mathematica Societatis Ja'nos Bolyai  26, North-Holland, Amsterdam, 1981.
[1980] Specification of program behavior through explicit time considerations, In  Information

Processing 80 , S. H. Lavington (Ed.), North-Holland, Amsterdam,107-111.
[1982] A  theory of interactive programming,  Acta Informatica  17, 1-20.
GERHART, S. L.

[1975] Correctness-preserving program transformations,  Conference record of the second ACM

SIGACT-SIGPLAN symposium on Principles Of Programming Languages , 54-66.
GERMAN, S. M.

[1978] Automatic proofs of the absence of common runtime errors,  Conference record of the fifth ACM

SIGACT-SIGPLAN symposium on Principles Of Programming Languages , 105-118.
[1981] Verifying the absence of common runtime errors in computer programs, Report No. STAN-CS81-866, Department of computer science, Stanford university.
GERMAN, S. M., CLARKE, E. M. Jr & HALPERN, J. Y.

[1983] Reasoning about procedures as parameters, In  Logics of programs,  Ed. Clarke & D. Kozen (Eds.),

Lecture Notes in Computer Science  164, Springer-Verlag, Berlin - New York, 206-220.
[1986] True relative completeness of an axiom system for the language L4 (abridged), In  Proceedings

symposium on Logic In Computer Science 1986 , IEEE Computer Society Press, 11-25.
[1988] Reasoning about procedures as parameters in the language L4, IBM research report, RJ 6387.
GERMAN, S. M. & HALPERN, J. Y.

[1983] On the power of the hypothesis of expressiveness, IBM research report, RJ 4079.
GERMAN, S. M. & WEGBREIT, B.

[1975] A synthesizer of inductive assertions,  IEEE Transactions on Software Engineering , SE-1 (1), 68-

75.
GERTH, R.

[1982] A  sound and complete Hoare axiomatization of the ADA-rendezvous, In  Ninth International

Colloquium on Automata Languages and Programming , M. Nielsen & E. M. Schmidt (Eds.),
Lecture Notes in Computer Science  140, Springer-Verlag, Berlin - New York, 252-264.
[1983] Transition logic : how to reason about temporal properties in a compositional way, In

Proceedings of the sixteenth annual ACM Symposium on Theory Of Computing , 39-50.
GERTH, R. & DE ROEVER, W. P.

[1984] A  proof system for concurrent ADA programs,  Science of Computer Programming  4, NorthHolland, Amsterdam, 159-204.
GOERDT, A.

[1985] A Hoare calculus for functions defined by recursion on higher types, In  Logics of Programs , R.

Parikh (Ed.),  Lecture Notes in Computer Science  193, Springer-Verlag, Berlin - New York, 106-
117.
[1987] Hoare logic for lambda-terms as basis of Hoare logic for imperative languages, In  Proceedings

symposium on Logic In Computer Science 1987 , IEEE Computer Society Press, 293-299.
[1988] Hoare calculi for higher-type control structures and their completeness in the sense of Cook, In

Proceedings of the thirteenth symposium on the  Mathematical Foundations of Computer Science
1988, M. P. Chytil, L. Jane & V. Koubek (Eds.),  Lecture Notes in Computer Science  324,
Springer-Verlag, Berlin - New York, 329-338.
GOLDSTINE, H. H. & VON NEUMANN, J.

[1947] Planning and coding of problems for an electronic computing instrument, Report for U.S. Ord.

Dept., In  Collected Works of J. von Neumann , A. Taub (Ed.), Vol. 5, (1965), Pergamon, New
York, 80-151.
GOOD, D. I.

[1984] Mechanical proofs about computer programs,  Phil. Trans. R. Soc. London , A 312, 389-409.
GORELICK, G.A.

[1975] A  complete axiomatic system for proving assertions about recursive and non-recursive

procedures, Technical report 75, Department of Computer Science, University of Toronto.
GRABOWSKI, M.

[1984] On the relative completeness of Hoare logics,  Conference record of the eleventh ACM SIGACTSIGPLAN symposium on Principles Of Programming Languages , 258-261 and  Information and
control 66, (1986) 29-44.
[1985] On the relative incompleteness of logics for total correctness, In  Logics of Programs , R. Parikh

(Ed.), Lecture Notes in Computer Science  193, Springer-Verlag, Berlin - New York, 118-127.

[1988] Arithmetical completeness versus relative completeness,  Studia Logica  XLVII (3), Ossolineum

and Kluwer Academic Publishers, Wroclaw, Poland, 213-220.
GRABOWSKI, M. & HUNGAR, *.

[1988] On  the existence of effective Hoare logics, In  Proceedings symposium on Logic In Computer

Science 1988 , IEEE Computer Society Press, ***-***.
GREIBACH, S. A.

[1975] Theory of program structures : schemes, semantics, verification,  Lecture Notes in Computer

Science 36 Springer-Verlag, Berlin - New York.
GREIF, I. & MEYER, A. R.

[1981] Specifying the semantics of  while programs: a tutorial and critique of a paper by Hoare and

Lauer, ACM Transactions On Programming Languages And Systems  3 (4), 484-507.
GRIBOMONT, P. E.

[1989] Stepwise refinement and concurrency : a small exercise, In  Mathematics of Program

Construction, J. L. A. van de Snepscheut (Ed.),  Lecture Notes in Computer Science  375,
Springer-Verlag, Berlin - New York, 219-238.
GRIES, D.

[1977] An exe rcise in proving parallel programs correct,  Communications of the Association for

Computing Machinery  20 (12), 921-930.
[1978] The multiple assignment statement,  IEEE Transactions on Software Engineering , SE-4 (2), 89-

93.
[1979] Is `sometime' ever better than `always'?,  ACM Transactions On Programming Languages And

Systems 1, 258-265.
[1981] The science of programming, Springer-Verlag, Berlin - New York.
GRIES, D. & LEVIN, G. M.

[1980] Assignment and procedure call proof rules,  ACM Transactions On Programming Languages And

Systems 2 (4), 564-579.
GRU"MBERG, O., FRANCEZ, N.

[1982] A complete proof rule for (weak) equifairness, IBM research report, T. J. Watson Research Center

RC-9634.
GRU"MBERG, O., FRANCEZ, N. & KATZ, S.

[1983] A  complete proof rule for strong equifair termination, In  Logics of programs,  Ed. Clarke & D.

Kozen (Eds.),  Lecture Notes in Computer Science  164, Springer-Verlag, Berlin - New York,
257-278.
GUREVICH, Y.

[1984] Toward logic tailored for computational complexity, In  Computation and Proof Theory , M. M.

Richter, E. Bo"rger, W. Oberschelp, B. Schinzel & W. Thomas (Eds.),  Lecture Notes In
Mathematics 1104, Springer-Verlag, Berlin - New York, 175-216.
[1988] Logic and the challenge of computer science, In  Trends in Theoretical Computer Science , E.

Bo"rger (Ed.), Computer Science Press, Rockville, 1-58.
GUTTAG, J. V., HORNING, J. J. & LONDON, R. L.

[1978] A  proof rule for Euclid procedures, In  Formal Description of Programming Concepts , E. J.

Neuhold (Ed.), North-Holland, Amsterdam, 211-220.
HALPERN, J. Y.

[1984] A  good Hoare axiom system for an ALGOL-like language,  Conference record of the eleventh

ACM SIGACT-SIGPLAN symposium on Principles Of Programming Languages , 262-271.
HALPERN, J. Y., MEYER, A. R. & TRAKHTENBROT, B. A.

[1984] The semantics of local storage, or what makes the free-list free?,  Conference record of the

eleventh ACM SIGACT-SIGPLAN symposium on Principles Of Programming Languages , 245-
254.
HAREL, D.

[1979] First-order dynamic logic,  Lecture Notes in Computer Science  68, Springer-Verlag, Berlin - New

York.
[1980] Proving the correctness of regular deterministic programs : a unifying survey using dynamic

logic, Theoretical Computer Science  12, North-Holland, Amsterdam, 61-81.
HAREL, D., PNUELI, A. & STAVI, J.

[1977] A complete axiomatic system for proving deductions about recursive programs,  Proceedings of

the ninth annual ACM Symposium on Theory Of Computing , 249-260.
HEHNER, E. C. R.

[1979] Do considered od : a contribution to the programming calculus,  Acta Informatica  11, 287-304.
[1984a] The logic of programming, Prentice Hall, Englewood Cliffs.

[1984b] Predicative programming,  Communications of the Association for Computing Machinery  27,

134-151.
HEHNER, E. C. R., GUPTA, L. E. & MALTON, A. J.

[1986] Predicative methodology,  Acta Informatica  23, 487-505.
HEHNER, E. C. R. & HOARE, C. A. R.

[1983] A more complete model of communicating processes,  Theoretical Computer Science  26, NorthHolland, Amsterdam, 105-120.
HESSELINK, W. H.

[1989] Predicate-transformer semantics of general recursion,  Acta Informatica  26, 309-332.
HITCHCOCK, P. & PARK, D. M. R.

[1973] Induction rules and proofs of program termination, In  First International Colloquium on

Automata, Languages and Programming , M. Nivat (Ed.), North-Holland, Amsterdam, 225-251.
HOARE, C. A. R.

[1961] Algorithm 63, Partition; Algorithm 64, Quicksort; Algorithm 65, Find,  Communications of the

Association for Computing Machinery  4 (7), 321-322.
HOARE, C. A. R.

[1969] An  axiomatic basis for computer programming,  Communications of the Association for

Computing Machinery  12, 10, 576-580, 583 (Ch. 4 of HOARE & JONES [1989, pp. 45-58]).
[1971a] Proof of a program : Find,  Communications of the Association for Computing Machinery  14

(1), 39-45 (Ch. 5 of HOARE & JONES [1989, pp. 59-74]).
[1971b] Procedures and parameters : an axiomatic approach, In  Symposium on Semantics of Algorithmic

Languages,  E. Engeler (Ed.),  Lecture Notes in Mathematics  188, Springer-Verlag, Berlin - New
York, 102-116 (Ch. 6 of HOARE & JONES [1989, pp. 75-88]).
[1972a] Proof of correctness of data representations,  Acta Informatica  1, 271-281 (Ch. 8 of HOARE &

JONES [1989, pp. 103-116]).
[1972b] Proof of a structured program: `the sieve of Eratosthenes',  Computer Journal  15 (4), 321-325

(Ch. 9 of HOARE & JONES [1989, pp. 117-132]).
[1972c] Towards a theory of parallel programming, In  Operating System Techniques , C. A. R. Hoare &

R. H. Perrott, Academic Press, New York, 61-71.
[1974] Monitors : an operating system structuring concept,  Communications of the Association for

Computing Machinery  17 (10), 549-557 (Ch. 12 of HOARE & JONES [1989, pp. 171-191]).
[1975] Parallel programming : an axiomatic approach,  Computer Languages  1 (2), Pergamon Press,

151-160 (Ch. 15 of HOARE & JONES [1989, pp. 245-258]).
[1978a] Some properties of predicate transformers,  Journal of the Association for Computing Machinery

25 (3), 461-480.
[1978b] Communicating sequential processes,  Communications of the Association for Computing

Machinery 21 (8), 666-677 (Ch. 16 of HOARE & JONES [1989, pp. 259-288]).
[1981] A calculus of total correctness for communicating processes,  Science of Computer Programming

1 (1-2), North-Holland, Amsterdam, 49-72.
[1984] Programs as predicates,  Phil. Trans. R. Soc. Lond.  A  312, 475-489, Also in  Mathematical

Logic and Programming Languages , C. A. R. Hoare & J. C. Shepherdson (Eds.), Prentice Hall,
New York, 141-154, 1985 (Ch. 20 of HOARE & JONES [1989, pp. 333-349]).
[1985a] The mathematics of programming, In  Proceedings of the fifth conference on Foundations of

Software Technology and Theoretical Computer Science , S. N. Maheshwari (Ed.), Lecture Notes
in Computer Science  206, Springer-Verlag, Berlin - New York, 1-18 (Ch. 21 of HOARE &
JONES [1989, pp. 351-370]).
[1985b] Communicating sequential processes, Prentice Hall, New York, 256 p.
HOARE, C. A. R. & ALLISON, D. C. S.

[1972] Incomputability,  Computing Surveys  4 (3), 169-178.
HOARE, C. A. R., HAYES, I. J., HE JIFENG, MORGAN, C. C., ROSCOE, A. W., SANDERS, J. W.,

SORENSEN, I. H. & SUFRIN, B. A.
[1987] Laws of  programming,  Communications of the Association for Computing Machinery  30 (8),

672-686.
HOARE, C. A. R. & HE JIFENG

[1986] The weakest prespecification,  Fundamenta Informaticae  9, Part I : 51-84, Part II : 217-252.
[1987] The weakest prespecification,  Information Processing letters  24 (2), North-Holland,

Amsterdam, 127-132.
HOARE, C. A. R., HE JIFENG & SANDERS, J. W.

[1987] Prespecification in data refinement,  Information processing letters  25 (2), North-Holland,

Amsterdam, 71-76.

HOARE, C. A. R. & JONES, C. B. (Ed.)

[1989] Essays in computing science, Prentice Hall, New York, 412 p.
HOARE, C. A. R. & LAUER, P.

[1974] Consistent and complementary formal theories of the semantics of programming languages,

Acta Informatica  3, 135-155.
HOARE, C. A. R. & WIRTH, N.

[1973] An  axiomatic definition of the programming language PASCAL,  Acta Informatica  2, 335-355

(Ch. 11 of HOARE & JONES [1989, pp. 153-169]).
HOOMAN, J. & DE ROEVER, W.-P.

[1989] The quest goes on : a survey of proof systems for partial correctness of CSP, In  Current Trends in

Concurrency, Overviews and Tutorials , J. W. de Bakker, W.-P. de Roever & G. Rozenberg (Eds.),
Lecture Notes in Computer Science  224, Springer-Verlag, Berlin - New York, 343-395.
HOOGEWIJS, A.

[1987] Partial-predicate logic in computer science,  Acta Informatica  24, 381-393.

HORTALA'-GONZA'LEZ, Ma T. & RODRI'GUEZ-ARTALEJO, M.

[1985] Hoare's logic for nondeterministic regular programs : a nonstandard completeness theorem,

Lecture Notes in Computer Science  194, Springer-Verlag, Berlin - New York, 270-280.
HOWARD, J. H.

[1976] Proving monitors,  Communications of the Association for Computing Machinery  19 (5), 273-

279.
IGARASHI, S., LONDON, R. L. & LUCKHAM, D. C.

[1975] Automatic program verification I : a logical basis and its implementation,  Acta Informatica  4,

145-182.
IMMERMAN, N.

[1983] Languages which capture complexity classes,  Proceedings of the fifteenth annual ACM

Symposium on Theory Of Computing , 347-354.
JACOBS, D. & GRIES, D.

[1985] General correctness: a unification of partial and total correctness,  Acta informatica  22, 67-83.
JANSSEN, T. M. V. & VAN EMDE BOAS, P.

[1977] On  the proper treatment of referencing, dereferencing and assignment,  Lecture Notes in

Computer Science  52, Springer-Verlag, Berlin - New York, 282-300.
JOHNSTONE, P. T.

[1987] Notes on logic and set theory, Cambridge University Press, Cambridge.
JONES, C. B.

[1980] Software development: a rigorous approach, Prentice-Hall, Englewood Cliffs.
[1983a] Specification and design of (parallel programs), In  Information Processing 83 , R. E. A. Mason

(Ed.), Elsevier Science Publishers B. V. (North-Holland, Amsterdam), (C) IFIP, 321-332.
[1983b] Tentative steps toward a development method for interfering programs,  ACM Transactions On

Programming Languages And Systems  5 (4), 596-619
JONES, N. D. & MUCHNICK, S. S.

[1977] Even simple programs are hard to analyze,  Journal of the Association for Computing Machinery

24 (2), 338-350.
[1978] Complexity of finite memory programs with recursion,  Journal of the Association for

Computing Machinery  25, 312-321.
JOSEPH, M., MOITRA, A. & SOUNDARARAJAN, N.

[1987] Proof rules for fault tolerant distributed programs,  Science of Computer Programming  8, NorthHolland, Amsterdam, 43-67.
JOSKO, B.

[1983] A  note on expressivity definitions in Hoare logic, Schriften zur Informatik und angewandten

Mathematik, Rheinisch Westfalische TH Aachen, Bericht 80.
KATZ, S. & MANNA, Z.

[1976] Logical analysis of programs,  Communications of the Association for Computing Machinery

19, 188-206.
KELLER, R. M.

[1976] Formal verification of parallel programs,  Communications of the Association for Computing

Machinery 19 (7), 371-384.
KFOURY, A. J.

[1983] Definability by programs in first-order structures,  Theoretical Computer Science  25, NorthHolland, Amsterdam, 1-66.

KFOURY, A. J. & URZYCZYN, P.

[1985] Necessary and sufficient conditions for the universality of programming formalisms,  Acta

Informatica 22, 347-377.
KING, J.

[1969] A program verifier, Ph. D. Thesis, Carnegie-Mellon U.
KLEENE, S. C.

[1936] oe-definability and recursiveness,  Duke Math. Jour.  2, 340-353.
[1952] Introduction to metamathematics, D. Van Nostrand Inc., Princeton.
[1967] Mathematical logic, Wiley & sons, New York.
KNUTH, D. E.

[1968a] The art of computer programming, Vol. 1, Fundamental algorithms, Addison-Wesley, Reading.
[1968b] Semantics of context-free languages,  Math. Systems Theory  2 (2), 127-145, correction :  Math.

Systems Theory  5, (1971), 95-96.
KOWALTOWSKI, T.

[1977] Axiomatic approach to side effects and general jumps,  Acta Informatica  7, 357-360.
LAMBEK, J.

[1961] How to program an infinite abacus,  Canadian Math. Bulletin  4, 295-302.
LAMPORT, L.

[1977] Proving the correctness of multiprocess programs,  IEEE Transactions on Software Engineering ,

SE-3 (2), 125-143.
[1980a] "Sometime" is sometimes "not never",  Conference record of the seventh ACM SIGACTSIGPLAN symposium on Principles Of Programming Languages , 174-185.
[1980b] The "Hoare logic" of concurrent programs,  Acta Informatica  14, 31-37.
[1983] Specifying concurrent program modules,  ACM Transactions On Programming Languages And

Systems 5 (2), 190-222.
[1987] win and sin: predicate transformers for concurrency, DIGITAL SRC Research Report 17, System

Research Center, Palo Alto.
[1988] Control predicates are better than dummy variables for reasoning about program control,  ACM

Transactions On Programming Languages And Systems  10 (2), 267-281.
LAMPORT, L. & SCHNEIDER, F. B.

[1984] The "Hoare logic" of CSP, and that all,  ACM Transactions On Programming Languages And

Systems 6 (2), 281-296.
LANGMAACK, H.

[1973] On  procedures as open subroutines, part I,  Acta Informatica  2 (1973), 311-333; part II,  Acta

Informatica  3 (1974), 227-241.
[1983] Aspects of programs with finite modes, In  Foundations of Computation Theory , M. Karpinski

(Ed.), Lecture Notes in Computer Science  158, Springer-Verlag, Berlin - New York, 241-254.
LANGMAACK, H. & OLDEROG, E.R.

[1980] Present day Hoare-like systems for programming languages with procedures : power, limits and

most likely extensions, In  Seventh International Colloquium on Automata Languages and
Programming, J. W. De Bakker & J. van Leeuwen (Eds.),  Lecture Notes in Computer Science  85,
Springer-Verlag, Berlin - New York, 363-373.
LEHMANN, D., PNUELI, A. & STAVI, J.

[1981] Impartiality, justice and fairness : the ethics of concurrent termination, In  Eighth International

Colloquium on Automata Languages and Programming , S. Even & O. Kariv (Eds.),  Lecture Notes
in Computer Science  115, Springer-Verlag, Berlin - New York, 264-277.
LEIVANT, D.

[1985] Partial-correctness theories as first-order theories, In  Logics of Programs , R. Parikh (Ed.),

Lecture Notes in Computer Science  193, Springer-Verlag, Berlin - New York, 190-195.
LEIVANT, D. & FERNANDO, T.

[1987] Skinny and fleshy failures of relative completeness,  Conference record of the fourteenth ACM

SIGACT-SIGPLAN symposium on Principles Of Programming Languages , 246-252.
LEVIN, G. M. & GRIES, D.

[1981] A proof technique for communicating sequential processes,  Acta Informatica  15, 281-302.
LEVITT, K. N.

[1972] The application of program-proving techniques to the verification of synchronization

processes, In  1972 AFIPS Fall Joint Computer Conference , AFIPS Conference Proceedings  41,
AFIPS Press, Montvale, 33-47.

LIFSCHITZ, V.

[1984]  On verification of programs with goto statements,  Information Processing Letters  18, NorthHolland, Amsterdam, 221-225.
LIPTON, R. J.

[1977] A  necessary and sufficient condition for the existence of Hoare Logics, In  eighteenth annual

IEEE-ACM symposium on Foundations Of Computer Science , 1-6.
LOECKX, J. & SIEBER, K.

[1984] The Foundations of program verification, Teubner - John Wiley & Sons, New York (1987).
LONDON, R. L.

[1970a] Proof of algorithms : a new kind of certification (certification of algorithm 245, TREESORT 3),

Communications of the Association for Computing Machinery  13 (6), 371-373.
[1970b] Proving programs correct: some techniques and examples,  BIT 10, 168-182.
LONDON, R. L., GUTTAG, J. V., HORNING, J. J., LAMPSON, B. W., MITCHELL, J. G. & POPEK, G. J.

[1978] Proof rules for the programming language Euclid,  Acta Informatica  10, 1-26.
LUCKHAM, D. C. & SUZUKI, N.

[1979] Verification of arrays, record and pointer operations in Pascal,  ACM Transactions On

Programming Languages And Systems  1, 226-244.
MAIN, M. G. & BENSON, D. B.

[1983] Functional behavior of nondeterministic programs, In  Foundations of Computation Theory , M.

Karpinski (Ed.),  Lecture Notes in Computer Science  158, Springer-Verlag, Berlin - New York,
290-301.
MAJSTER-CEDERBAUM, M. E.

[1980] A  simple relation between relational and predicate transformer semantics for nondeterministic

programs,  Information Processing letters  11 (4, 5), North-Holland, Amsterdam, 190-192.
MAKOWSKY, J. A. & SAIN, I.

[1986] On  the equivalence of weak second order and nonstandard time semantics for various program

verification systems, In  Proceedings symposium on Logic In Computer Science 1986 , IEEE
Computer Society Press, 293-300.
MANNA, Z.

[1969] The correctness of programs,  Journal of Computer and System Sciences  3 (2), 119-127.
[1971] Mathematical theory of partial correctness,  Journal of Computer and System Sciences  5 (3),

239-253.
[1974] Mathematical theory of computation, McGraw-Hill, New York.
MANNA, Z., NESS, S. & VUILLEMIN, J.

[1972] Inductive methods for proving properties of programs,  SIGPLAN Notices  7 (1), 27-50.
MANNA, Z. & PNUELI, A.

[1970] Formalization of properties of functional programs,  Journal of the Association for Computing

Machinery 17 (3), 555-569.
[1974] Axiomatic approach to total correctness of programs,  Acta Informatica  3, 243-264.
[1984] Adequate proof principles for invariance and liveness properties of concurrent programs,

Science of Computer Programming  4, North-Holland, Amsterdam, 257-289.
[1989] Completing the temporal picture, In  Sixteenth International Colloquium on Automata,

Languages and Programming , G. Ausiello, M. Dezani-Ciancaglini & S. Ronchi Della Rocca
(Eds.), Lecture Notes in Computer Science  372, Springer-Verlag, Berlin - New York, 534-558.
MANNA, Z. & WALDINGER, R.

[1978] Is  `sometime' sometimes better than `always'?,  Communications of the Association for

Computing Machinery  21 (2), 159-172.
[1981] Problematic features of programming languages : a situational-calculus approach,  Acta

Informatica  16, 371-426.
MASON, I. A.

[1987] Hoare's logic in the LF, LFCS report series ECS-LFCS-87-32, Laboratory for Foundations of

Computer Science, University of Edinburgh.
MARTIN, A. J.

[1983] A  general proof rule for procedures in predicate transformer semantics,  Acta Informatica  20,

301-313.
MAZURKIEWICZ, A.

[1977] Invariants of concurrent programs, In  International Conference On Information Processing,

IFIP-INFOPOL-76, J. Madey (Ed.), North-Holland, Amsterdam, 353-372.

[1989] Basic notions of trace theory, In  Linear Time, Branching Time and Partial Orders in Logics and

Models for Concurrency , J. W. de Bakker, W.-P. de Roever & G. Rozenberg (Eds.),  Lecture Notes
in Computer Science  354, Springer-Verlag, Berlin - New York, 285-363.
McCARTHY, J.

[1962] Towards a mathematical science of computation, In  Information Processing , Proceedings of IFIP

congress, C. M. Popplewell (Ed.), North-Holland, Amsterdam, 21-28.
[1963]  A basis for a mathematical theory of computation, In  Computer programming and formal

systems, P. Braffort & D. Hirschberg (Eds.), North-Holland, Amsterdam, 33-69.
MEYER, A. R.

[1986] Floyd-Hoare logic defines semantics: Preliminary version, In  Proceedings symposium on Logic

In Computer Science 1986 , IEEE Computer Society Press, 44-48.
MEYER, A. R. & HALPERN, J. Y.

[1980] Axiomatic definitions of programming languages, a theoretical assessment,  Conference record

of the seventh ACM SIGACT-SIGPLAN symposium on Principles Of Programming Languages ,
203-212 and  Journal of the Association for Computing Machinery  29, (1982), 555-576.
[1981] Axiomatic definitions of programming languages, II,  Conference record of the eighth ACM

SIGACT-SIGPLAN symposium on Principles Of Programming Languages , 139-148.
MEYER, A. R. & MITCHELL, J. C.

[1982] Axiomatic definability and completeness for recursive programs,  Conference record of the ninth

ACM SIGACT-SIGPLAN symposium on Principles Of Programming Languages , 337-346.
[1983] Termination assertions for recursive programs : completeness and axiomatic definability,

Information and Control  56, 112-138.
MEYER, A.R. & SIEBER, K.

[1988] Towards fully abstract semantics for local variables : preliminary report,  Proceedings of the

fifteenth annual ACM SIGACT-SIGPLAN symposium on Principles Of Programming Languages ,
191-203.
MILNE, R.

[1978] Transforming predicate transformers, In  Formal descriptions of programming concepts , E. J.

Neuhold (Ed.), North-Holland, Amsterdam, 31-65.
MILNER, R.

[1980] A calculus of communicating systems,  Lecture Notes in Computer Science  92, Springer-Verlag,

Berlin - New York.
MISRA, J. & CHANDY, K. M.

[1981] Proofs of networks of processes, ,  IEEE Transactions on Software Engineering , SE-7 (4), 417-

426.
MORGAN, C. C.

[1988a] The  specification statement,  ACM Transactions On Programming Languages And Systems  10

(3), 403-419.
[1988b] Procedures, parameters, and abstraction: separate concerns,  Science of Computer Programming

11, North-Holland, Amsterdam, 17-27.
[1988c] Data refinement by miracles,  Information Processing Letters  26, 243-246.
MORRIS, F. L. & JONES, C. B.

[1984] An early program proof by Alan Turing,  Annals of the History of Computing  6 (2), 139-143.
MORRIS, J. H.

[19??] Comments on "procedures and parameters", (undated and unpublished manuscript).
MORRIS, J. H. & WEGBREIT, B.

[1977] Subgoal induction,  Communications of the Association for Computing Machinery  20 (4), 209-

222.
MORRIS, J. M.

[1982] A  general axiom of assignment, In  Theoretical foundations of programming methodology , M.

Broy & G. Schmidt (Eds.), D. Reidel, 25-34.
[1987a] Varieties of weakest liberal preconditions,  Information Processing Letters  25, 207-210.
[1987b] A theoretical basis for stepwise refinement and the programming calculus,  Science of Computer

Programming 9, North-Holland, Amsterdam, 287-306.
MURAKAMI, M.

[1988] Proving partial correctness of guarded Horn clauses programs, In  Logic Programming '87,

Proceedings of the sixth Conference,  K. Furukawa, H. Tanaka & T. Fujisaki (Eds.),  Lecture Notes
in Computer Science  315, Springer-Verlag, Berlin - New York, 215-235.

MURTAGH, T. P.

[1987] Redundant proofs of non-interference in Levin-Gries CSP program proofs,  Acta Informatica  24,

145-156.
NAUR, P.

[1966] Proof of algorithms by general snapshots,  BIT 6, 310-316.
NAUR, P. (Ed.)

[1960] Report on the algorithmic language Algol 60,  Communications of the Association for

Computing Machinery  3, 5, 299-314; Revised report on the algorithmic language Algol 60,
Communications of the Association for Computing Machinery  6 (1), 1-17;
NELSON, G.

[1983] Verifying reachability invariants of linked structures,  Conference record of the tenth annual

ACM SIGACT-SIGPLAN symposium on Principles Of Programming Languages , 38-47.
[1987] A  generalization of Dijkstra's calculus, DIGITAL SRC Research Report 16, System Research

Center, Palo Alto.
NE'METI, I.

[1980] Nonstandard runs of Floyd-provable programs, In  Logics of programs and their applications , A.

Salwicki (Ed.),  Lecture Notes in Computer Science  148, Springer-Verlag, Berlin - New York,
186-204.
NEUHOLD, E. J.

[1971] The formal description of programming languages,  IBM System Journal  2, 86-112.
NEWTON, G.

[1975] Proving properties of interacting processes,  Acta Informatica  4, 117-126.
O'DONNELL, M. J.

[1982] A  critique of the foundations of Hoare style programming logic,  Communications of the

Association for Computing Machinery  25, 12, 927-935.
OLDEROG, E.-R.

[1980] General equivalence of expressivity definitions using strongest postconditions, resp. weakest

preconditions, Institut fu"r Informatik und praktische Mathematik, Kiel Universita"t, Bericht
8007.
[1981] Sound and complete Hoare-like calculi based on copy rules,  Acta Informatica  16, 161-197.
[1983a] On  the notion of expressiveness and the rule of adaptation,  Theoretical Computer Science  24,

North-Holland, Amsterdam, 337-347.
[1983b] Hoare's logic for programs with procedures - what has been achieved?, In  Logics of programs,

Ed. Clarke & D. Kozen (Eds.),  Lecture Notes in Computer Science  164, Springer-Verlag, Berlin -
New York, 385-395.
[1983c] A characterization of Hoare's logic for programs with Pascal-like procedures,  Proceedings of the

fifteenth annual ACM Symposium on Theory Of Computing , 320-329.
[1984] Correctness of programs with PASCAL-like procedures without global variables,  Theoretical

Computer Science  30, North-Holland, Amsterdam, 49-90.
OLDEROG, E.-R. & HOARE C. A. R.

[1986] Specification-oriented semantcs for communicating processes,  Acta Informatica   23, 9-66.
OPPEN, D. C. & COOK, S. A.

[1975] Proving assertions about programs that manipulate data structures,  Proceedings of the seventh

annual ACM Symposium on Theory Of Computing , 107-116.
OWICKI, S. S.

[1975] Axiomatic proof techniques for parallel programs, Ph.D. Thesis, TR 75-251, Comp. Sci.,

Cornell U., U.S.A.
[1978] Verifying concurrent programs with shared data classes, In  Formal Description of Programming

Concepts, E. J. Neuhold (Ed.), North-Holland, Amsterdam, 279-299.
OWICKI, S. S. & GRIES, D.

[1976a] An axiomatic proof techniques for parallel programs I,  Acta Informatica  6, 319-340.
[1976b] Verifying properties of parallel programs : an axiomatic approach,  Communications of the

Association for Computing Machinery  19 (5), 279-285.
PANDYA, P. & JOSEPH, M.

[1986] A  structure-directed total correctness proof rule for recursive procedure calls,  The Computer

Journal 29 (6), 531-537.
PARK, D. M. R.

[1969] Fixpoint induction and proofs of program properties,  Machine intelligence  5, B. Meltzer & D.

Michie (Eds.), Edinburgh University Press, 59-78.

[1981] A predicate transformer for weak fair iteration, In  Proceedings of the sixth IBM symposium on 

Mathematical Foundations of
Computer Science, Logical Aspects of Programs , Corporate & Scientific Program, IBM Japan,
259-275.
PLAISTED, D. A.

[1986] The denotational semantics of nondeterministic recursive programs using coherent relations, In

Proceedings symposium on Logic In Computer Science 1986 , IEEE Computer Society Press,
163-174.
PLOTKIN, G. D.

[1976] A  powerdomain construction,  SIAM Journal on Computing  5, 452-487.
[1981] A  structural approach to operational semantics, Research report DAIMI FN-19, Computer

Science Department, Aarhus University, Denmark.
PNUELI, A.

[1977] The temporal logic of programs,  eighteenth annual IEEE-ACM symposium on Foundations Of

Computer Science , 46-57.
[1985] In transition from global to modular temporal reasoning about programs, In  Logics and Models

of Concurrent Systems , K. R. Apt (Ed.), NATO ASI Series, Vol. F13, Springer-Verlag, Berlin -
New York, 123-144.
PONSE, A.

[1989] Process expressions and Hoare's logic, Research report CS-R8905, Centrum voor Wiskunde en

Informatica, Amsterdam, 19 p.
PRATT, V. R.

[1976] Semantical considerations on Floyd-Hoare logic,  seventeenth annual IEEE-ACM symposium on

Foundations Of Computer Science , 109-121.
[1979] Process logic : preliminary report,  Conference record of the sixth ACM SIGACT-SIGPLAN

symposium on Principles Of Programming Languages , 93-100.
PRAWITZ, D.

[1965] Natural deduction, a proof-theoretic study, Almqvist & Wiksell, Stockholm.
PRESBURGER, M.

[1929] U"ber die Vollsta"ndigkeit eines gewissen Systems der Arithmetik ganzer Zahlen, in welchem die

Addition als einzige Operation hervortritt,  Comptes rendus du premier congre`s des
mathe'maticiens slaves , Warszawa, 92-101.
PRITCHARD, P.

[1976] A proof  rule for multiple coroutine systems,  Information processing letters  4 (6), 141-143.
[1977] Program proving - expressions languages, In  Information Processing 77 , North-Holland,

Amsterdam, 727-731.
RABIN, M. O.

[1977] Decidable theories, In  Handbook of mathematical logic , J. Barwise (Ed.), North-Holland,

Amsterdam (1978), 595-629.
RASIOWA, H.

[1979] Algorithmic logic and its extensions, a survey, In  Fifth Scandinavian Logic Symposium ,

Aalborg University Press, 163-174.
REPS, T. & ALPERN, B.

[1984] Interactive proof checking,  Conference record of the eleventh ACM SIGACT-SIGPLAN

symposium on Principles Of Programming Languages , 36-45.
REYNOLDS, J. C.

[1978] Syntactic control of interference,  Conference record of the fifth ACM SIGACT-SIGPLAN

symposium on Principles Of Programming Languages , 39-46.
[1981] The craft of programming, Prentice-Hall, Englewood Cliffs.
[1982] Idealized Algol and its specification logic, In  Tools & notions for program construction,  D.

Ne'el (Ed.), Cambridge University Press, 121-162.
[1989] Syntactic control of interference, Part 2, In  Sixteenth International Colloquium on Automata,

Languages and Programming , G. Ausiello, M. Dezani-Ciancaglini & S. Ronchi Della Rocca
(Eds.), Lecture Notes in Computer Science  372, Springer-Verlag, Berlin - New York, 704-722.
RODRI'GUEZ-ARTALEJO, M.

[1985] Some questions about expressiveness and relative completeness in Hoare's logic,  Theoretical

Computer Science  39, North-Holland, Amsterdam, 189-206.
ROGERS, H. Jr.

[1967] Theory of recursive functions and effective computability, McGraw-Hill, New York.

SAIN, I.

[1985] A  simple proof for the completeness of Floyd's method,  Theoretical Computer Science  35,

North-Holland, Amsterdam , 345-348.
SALWICKI, A.

[1970] Formalized algorithmic languages,  Bulletin de l'Acade'mie Polonaise des Sciences , Se'rie des

Sciences Mathe'matiques, Astronomiques et Physiques  18 (5), 227-232.
SCHNEIDER, F. B. & ANDREWS, G. R.

[1986] Concepts for concurrent programming, In  Current Trends in Concurrency, Overviews and

Tutorials, J. W. De Bakker, W. P. De Roever & G. Rozenberg (Eds.),  Lecture Notes in Computer
Science 224, Springer-Verlag, Berlin - New York, 670-716.
SCHWARTZ, R. L.

[1979] An axiomatic treatment of ALGOL 68 routines, In  Sixth International Colloquium on Automata,

Languages and Programming , H. A. Maurer (Ed.),  Lecture Notes in Computer Science  71,
Springer-Verlag, Berlin - New York, 530-545.
SCHWARTZ, R. L. & BERRY, D. M.

[1979] A semantic view of ALGOL 68,  Computer Languages 4, 1-15.
SCOTT, D. S. & DE BAKKER, J. W.

[1969] A  theory of programs, Seminar on programming, IBM research center, Vienna (unpublished

manuscript).
SCOTT, D. S. & STRACHEY, C.

[1972] Toward a mathematical semantics for computer languages, In  Computers and automata , J. Fox

(Ed.), Wiley, New York, 19-46.
SHOENFIELD, J. R.

[1977] Axioms of set theory, In  Handbook of Mathematical logic , J. Barwise (Ed.), North Holland ,

Amsterdam (1978), 321-344.
SIEBER, K.

[1985] A partial correctness logic for procedures, In  Logics of Programs , R. Parikh (Ed.),  Lecture Notes

in Computer Science  193, Springer-Verlag, Berlin - New York, 320-342.
SITES, R. L.

[1974] Proving that computer programs terminate cleanly, Research report STAN-CS-74-418,

Computer Science Department, Stanford University, 139 p.
SKOLEM, T.

[1934] U"ber die Nicht-charakterisierbarkeit der Zahlenreihe mittels endlich oder abza"hlbar unendlich

vieler Aussagen mit ausschliesslich Zahlenvariablen,  Fund. math.  23, 150-161.
SMORYNSKI, C.

[1977] The incom pleteness theorems, In  Handbook of Mathematical Logic , J. Barwise (Ed.), NorthHolland, Amsterdam (1978), 821-865.
SOBEL, A. E. K. & SOUNDARARAJAN, N.

[1985] A proof system for distributed processes, In  Logics of Programs , R. Parikh (Ed.),  Lecture Notes

in Computer Science  193, Springer-Verlag, Berlin - New York, 343-358.
SOKOLOWSKI, S.

[1976] Axioms for total correctness,  Acta Informatica  9, 61-71.
[1977] Total correctness for procedures,  Proceedings of the sixth symposium on the Mathematical

Foundations of Computer Science , J. Gruska (Ed.),  Lecture Notes in Computer Science  53,
Springer-Verlag, Berlin - New York, 475-483.
[1984] Partial correctness: the term-wise approach,  Science of Computer Programming  4, NorthHolland, Amsterdam, 141-157.
[1987] Soundness of Hoare's logic: an automated proof using LCF,  ACM Transactions On Programming

Languages And Systems  9 (1), 100-120.
SOUNDARARAJAN, N.

[1983] Correctness proofs of CSP programs,  Theoretical Computer Science  24, North-Holland,

Amsterdam, 131-141.
[1984a] A  proof technique for parallel programs,  Theoretical Computer Science  31, North-Holland,

Amsterdam, 13-29.
[1984b] Axiomatic semantics of communicating sequential processes,  ACM Transactions On

Programming Languages And Systems  6 (4), 647-662.
STIRLING, C.

[1986] A compositional reformulation of Owicki-Gries's partial correctness logic for a concurrent while

language, In  Thirteenth International Colloquium on Automata, Languages and Programming , L.

Kott (Ed.),  Lecture Notes in Computer Science  226, Springer-Verlag, Berlin - New York, 408-
415.
[1988] A g eneralization of Owicki-Gries's Hoare logic for a concurrent while language,  Theoretical

Computer Science  58, North-Holland, Amsterdam, 347-359.
TARSKI, A.

[1955] A lattice theoretic fixpoint theorem and its applications,  Pacific Journal of Mathematics  25 (2),

285-309.
TAUBENFELD, G. & FRANCEZ, N.

[1984] Proof rules for communication abstraction, In  Foundations of Software Technology and

Theoretical Computer Science , M. Joseph & R. Shyamasundar (Eds.),  Lecture Notes in Computer
Science 181, Springer-Verlag, Berlin - New York, 444-465.
TAYLOR, R. N.

[1983] A general-purpose algor ithm for analyzing concurrent programs,  Communications of the

Association for Computing Machinery  26 (5), 362-376.
TENNENT, R. D.

[1976] The denotational semantics of programming languages,  Communications of the Association for

Computing Machinery  19 (8), 437-453.
[1985] Semantical analysis of specification logic, In  Logics of Programs , R. Parikh (Ed.),  Lecture

Notes in Computer Science  193, Springer-Verlag, Berlin - New York, 373-386.
TIURYN, J.

[1985] A  simple programming language with data types: semantics and verification, In  Logics of

Programs, R. Parikh (Ed.),  Lecture Notes in Computer Science  193, Springer-Verlag, Berlin -
New York, 387-405.
TRAKHTENBROT, B. A., HALPERN, J. Y. & MEYER, A. R.

[1983] From denotational to operational and axiomatic semantics for Algol-like languages: an

overview, In  Logics of programs , Ed. Clarke & D. Kozen (Eds.),  Lecture Notes in Computer
Science 164, Springer-Verlag, Berlin - New York, 474-500.
TURING, A. M.

[1936] On computable numbers, with an application to the Entscheidungsproblem,  Proc. London Math.

Soc., Ser. 2,  42, 230-265. A correction, ibid.  43 (1937), 544-546.
[1937] Computability and  oe-definability,  Journal of Symbolic Logic  2, 153-163.
[1949] On checking a  large routine, In  Report of a conference on high-speed automatic calculating

machines, University Mathematics Laboratory, Cambridge, 67-69.
URZYCZYN, P.

[1983] A  necessary and sufficient condition in order that a Herbrand interpretation be expressive

relative to recursive programs,  Information and Control  56, 212-219.
VAN LAMSWEERDE, A. & SINTZOFF, M.

[1979] Formal derivation od strongly correct concurrent programs,  Acta Informatica  12, 1-31.
VERJUS, J.-P.

[1987] On the proof of a distributed algorithm,  Information Processing Letters  25 (3), 145-147.
WAGNER, E. G.

[1986] A  categorical view of weakest liberal preconditions,  Lecture Notes in Computer Science  240,

Springer-Verlag, Berlin - New York, 198-205.
WAND, M.

[1978] A  new incompleteness result for Hoare's system,  Journal of the Association for Computing

Machinery 25 (1), 168-175.
[1980] Induction, recursion and programming, North-Holland, Amsterdam.
WANG, A.

[1976] An  axiomatic basis for proving total correctness of goto-programs,  BIT 16, 88-102.
WANG, A. & DAHL, O.-J.

[1971] Coroutine sequencing in a block structured environment,  BIT 11, 425-449.
WECHLER, A.

[1983] Hoare algebras versus dynamic algebras, In  Algebra, Combinatorics and Logic in Computer

Science, Vol. I, II (Gyo"r, Hungary, 1983), 835-847,  Colloquia Mathematica Societatis Ja'nos
Bolyai 42, North-Holland, Amsterdam, 1986.
WEGBREIT, B.

[1974] The syn thesis of loop predicates,  Communications of the Association for Computing

Machinery 17, 102-112.
WIRTH, N.

[1971] The programming language PASCAL,  Acta informatica 1 (1), 35-63.

YEH, R. T.

[1976] Verification of nondeterministic programs, Technical report TR-56 (revised 1977), Department

of Computer sciences, University of Texas, Austin.
ZHOU CHAO CHEN & HOARE, C. A. R.

[1981] Partial correctness of communicating sequential processes, In  Proceedings of the Second

International Conference on Distributed Computing Systems,  IEEE Computer Society Press, 1-
12.
ZWIERS, J.

[1989] Compositionality, concurrency and partial correctness, proof theories for networks of processes

and their relationship,  Lecture Notes in Computer Science  321, Springer-Verlag, Berlin - New
York, 272 p.
ZWIERS, J., DE BRUIN, A. & DE ROEVER, W. P.

[1983] A proof system for partial correctness of dynamic networks of processes, In  Logics of programs,

Ed. Clarke & D. Kozen (Eds.),  Lecture Notes in Computer Science  164, Springer-Verlag, Berlin -
New York, 513-527.
ZWIERS, J., DE ROEVER, W. P. & VAN EMDE BOAS, P.

[1985] Compositionality and concurrent networks: soundness and completeness of a proof system, In

twelfth International Colloquium on Automata Languages and Programming , W. Brauer (Ed.),
Lecture Notes in Computer Science  194, Springer-Verlag, Berlin - New York, 509-519.

185 P. COUSOT