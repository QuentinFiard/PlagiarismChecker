

Winnowing: Local Algorithms for Document Fingerprinting

Saul SchleimerMSCS
University of Illinois, Chicago

saul@math.uic.edu

Daniel S. WilkersonComputer Science Division

UC Berkeley
dsw@cs.berkeley.edu

Alex AikenComputer Science Division

UC Berkeley
aiken@cs.berkeley.edu

ABSTRACT
Digital content is for copying: quotation, revision, plagiarism, andfile sharing all create copies. Document fingerprinting is concerned

with accurately identifying copying, including small partial copies,within large sets of documents.

We introduce the class of local document fingerprinting algo-rithms, which seems to capture an essential property of any fingerprinting technique guaranteed to detect copies. We prove a novellower bound on the performance of any local algorithm. We also
develop winnowing, an efficient local fingerprinting algorithm, andshow that winnowing's performance is within 33% of the lower
bound. Finally, we also give experimental results on Web data, andreport experience with M

OSS, a widely-used plagiarism detectionservice.

1. INTRODUCTION

Digital documents are easily copied. A bit less obvious, perhaps,is the wide variety of different reasons for which digital documents

are either completely or partially duplicated. People quote fromeach other's email and news postings in their replies. Collaborators
create multiple versions of documents, each of which is closelyrelated to its immediate predecessor. Important Web sites are mirrored. More than a few students plagiarize their homework fromthe Web. Many authors of conference papers engage in a similar
but socially more acceptable form of text reuse in preparing journalversions of their work. Many businesses, notably in the software
and entertainment industries, are based on charging for each digitalcopy sold.

Comparing whole document checksums is simple and sufficesfor reliably detecting exact copies; however, detecting partial copies
is subtler. Because of its many potential applications, this secondproblem has received considerable attention.

Most previous techniques for detecting partial copies, which wediscuss in more detail in Section 2, make use of the following idea.
A k-gram is a contiguous substring of length k. Divide a docu-ment into

k-grams, where k is a parameter chosen by the user. Forexample, Figure 1(c) contains all the 5-grams of the string of characters in Figure 1(b). Note that there are almost as many k-grams

Permission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specificpermission and/or a fee.
SIGMOD 2003, June 9-12, 2003, San Diego, CA.Copyright 2003 ACM 1-58113-634-X/03/06 ...$5.00.

A do run run run, a do run run(a) Some text from [7].
adorunrunrunadorunrun(b) The text with irrelevant features removed.
adoru dorun orunr runru unrun nrunr runru
unrun nruna runad unado nador adoru dorun
orunr runru unrun(c) The sequence of 5-grams derived from the text.

77 72 42 17 98 50 17 98 8 88 67 39 77 72 42
17 98(d) A hypothetical sequence of hashes of the 5-grams.

72 8 88 72(e) The sequence of hashes selected using 0 mod 4.

Figure 1: Fingerprinting some sample text.

as there are characters in the document, as every position in thedocument (except for the last

k L/ 1 positions) marks the begin-ning of a
k-gram. Now hash each k-gram and select some subsetof these hashes to be the document's fingerprints. In all practical

approaches, the set of fingerprints is a small subset of the set of all
k-gram hashes. A fingerprint also contains positional information,which we do not show, describing the document and the location

within that document that the fingerprint came from. If the hashfunction is chosen so that the probability of collisions is very small,
then whenever two documents share one or more fingerprints, it isextremely likely that they share a

k-gram as well.For efficiency, only a subset of the hashes should retained as

the document's fingerprints. One popular approach is to choose allhashes that are 0 mod p, for some fixed

p. This approach is easy toimplement and retains only
1/p of all hashes as fingerprints (Sec-tion 2). Meaningful measures of document similarity can also be

derived from the number of fingerprints shared between documents[5].

A disadvantage of this method is that it gives no guarantee thatmatches between documents are detected: a

k-gram shared be-tween documents is detected only if its hash is 0 mod p. Consider

the sequence of hashes generated by hashing all k-grams of a filein order. Call the distance between consecutive selected fingerprints the gap between them. If fingerprints are selected 0 mod p,the maximum gap between two fingerprints is unbounded and any

matches inside a gap are not detected.1 In experiments with select-ing fingerprints 0 mod p on HTML data taken from the Web, we
found that gaps between fingerprints chosen 0 mod p can be quitelarge. In fact, there are passages much longer than the size of the
average Web page in which no hashes are 0 mod p for reasonablevalues of

p (Section 5).In this paper we give the first efficient algorithm for selecting the

fingerprints from a sequence of hashes that guarantees that at leastpart of any sufficiently long match is detected. Define a window of
size w to be w consecutive hashes of k-grams in a document (w isa parameter set by the user). By selecting at least one fingerprint
from every window our algorithm limits the maximum gap betweenfingerprints. In fact, our algorithm is guaranteed to detect at least
one k-gram in any shared substring of length at least w + k L/ 1.Central to our construction is the idea of a local algorithm (Section 4), which we believe captures the essential properties of anydocument fingerprinting algorithm which guarantees that matches
are detected. An algorithm is local if, for every window of w con-secutive hashes

hi, . . . , hi+wL/1, the algorithm decides to selectone of these hashes as a fingerprint and this choice depends only

on the window's contents hi, . . . , hi+wL/1. Note that some other ofthe hashes in this window may also be selected due the action of the
algorithm in one of the (potentially many) overlapping windows.With respect to a given input distribution, let the density of a
fingerprinting algorithm be the expected proportion of hashes itselects as fingerprints. In Section 3 we introduce a particular local algorithm, the winnowing algorithm. We analyze its perfor-mance on random (independent uniformly distributed) data. We
show that, with a given window size w, the density is asymptoti-cally

2/(w + 1). We also prove a lower bound of 1.5/(w + 1) onthe density of local algorithms. Thus the winnowing algorithm is

within 33% of optimal.We also report on experience with two implementations of winnowing. The first is an purely experimental framework for compar-ing actual performance with the theoretical predictions (Section 5).
Our analysis of all fingerprinting algorithms, including that of win-nowing, is based on the assumption that the sequence of hashes
generated by hashing k-grams is independent and uniformly ran-dom. On such random data the performance of our system matches
theoretical predictions perfectly. We have found, however, thatthere are situations where real data does not generate sufficiently
random sequences of hashes. In particular, there are clusters oflow-entropy strings on the Web, such as

000000000000000000000000 . . .
or more complex patterns such as

abbaabbaabbaabbaabbaabba . . .
The important characteristic of such strings is that they have fewdistinct

k-grams, and therefore few distinct hashes of k-grams. Inthe case of a long string of a single character, there is only a single k-gram (and hash). The straightforward winnowing algorithmselects far more fingerprints than predicted on such strings, but a
simple modification of the algorithm reduces the density of fin-gerprints2. Note that other fingerprint-selection approaches are exposed to the same problem. For example, choosing hashes that are0 mod p means that on the string "11111111 . . . " either all or none
of the hashes of the string are selected as fingerprints.We also report on experience with M

OSS, a widely-used ser-vice for detecting plagiarism, primarily in programming assign1See Section 3.1 for a discussion of the expected gap size.
2See Definition 3 "Robust Winnowing" in Section 5.1

ments (Section 5.2).3 Document fingerprinting has worked ex-tremely well in this setting. Because the basic idea of hashing

k-grams makes minimal assumptions about the form of the input, it

is easy to incorporate fingerprinting for a new data format withoutdisturbing the underlying hashing engine. False positives appear to
be non-existent, and the infrequent reports of false negatives (in-stances of shared substrings missed by the system) have always
been tracked back either to implementation bugs or user error.

2. BACKGROUND AND RELATED WORK

Not all copy-detection algorithms are based on selecting finger-prints of

k-grams. To give some basis for discussing differenttechniques, we list several criteria that a copy-detection algorithm

should satisfy. These requirements are based on our own experi-ence (mostly in the context of plagiarism detection), but echo many
of the goals outlined in papers on related techniques.
2.1 Desirable properties

We believe a copy-detection algorithm should have three proper-ties:

1. Whitespace insensitivity In matching text files, matches shouldbe unaffected by such things as extra whitespace, capitalization, punctuation, etc. In other domains the notion of whatstrings should be equal is different--for example, in matching software text it is desirable to make matching insensitiveto variable names.

2. Noise suppression Discovering short matches, such as thefact that the word the appears in two different documents,

is uninteresting. Any match must be large enough to implythat the material has been copied and is not simply a common word or idiom of the language in which documents arewritten.

3. Position independence Coarse-grained permutation of the con-tents of a document (e.g., scrambling the order of paragraphs)

should not affect the set of discovered matches. Adding to adocument should not affect the set of matches in the original
portion of the new document. Removing part of a documentshould not affect the set of matches in the portion that remains.
All schemes that we know of handle property (1) in essentiallythe same way. A first pass over the data transforms it to eliminate
undesirable differences between documents. For example, whites-pace and punctuation are removed, all letters are converted to lower
case, or all variable names are replaced by the identifier "V". Theexact details vary from one type of document to the next, but the
essential feature is that semantic information about the documenttype is used to eliminate unimportant differences between documents.Schemes based on fingerprinting

k-grams satisfy requirement (2)by choosing
k to be sufficiently long that common idioms of thelanguage have length shorter than

k. An important assumption ofthis class of algorithms is that there is some threshold

k such thatmatches with length shorter than
k are almost always uninterestingand matches with length longer than

k are almost always interest-ing. In our experience, there is such a sharp threshold (Section 5).

The most interesting requirement is (3). Before describing meth-ods for addressing requirement (3), we first give a short history of
the use of hashes of k-grams in copy detection.

3http://wwww.cs.berkeley.edu/~aiken/moss.html

2.2 Karp-Rabin String Matching

Karp and Rabin's algorithm for fast substring matching is appar-ently the earliest version of fingerprinting based on

k-grams [10].Their problem, which was motivated by string matching problems

in genetics, is to find occurrences of a particular string s of length
k within a much longer string. The idea is to compare hashes ofall

k-grams in the long string with a hash of s. However, hashingstrings of length

k is expensive for large k, so Karp and Rabin pro-pose a "rolling" hash function that allows the hash for the

i + 1st
k-gram to be computed quickly from the hash of the ith k-gram.Treat a

k-gram c1 . . . ck as a k-digit number in some base b. Thehash
H(c1 . . . ck) of c1 . . . ck is this number:

c1 * bkL/1 + c2 * bkL/2 * . . . + ckL/1 * b + ck
To compute the hash of the k-gram c2 . . . ck+1, we need only sub-tract out the high-order digit, multiply by

b, and add in the new loworder digit. Thus we have the identity:

H(c2 . . . ck+1) = (H(c1 . . . ck) L/ c1 * bkL/1) * b + ck+1
Since bkL/1 is a constant, this allows each subsequent hash to becomputed from the previous one with only two additions and two

multiplications. Further, this identity holds when addition and mul-tiplication are modulo some value (e.g., the size of the largest representable integer), so this method works well with standard machinearithmetic.

As an aside, this rolling hash function has a weakness. Becausethe values of the

ci are relatively small integers, doing the additionlast means that the last character only affects a few of the low-order

bits of the hash. A better hash function would have each character
ci potentially affect all of the hash's bits. As noted in [5], it is easyto fix this by multiplying the entire hash of the first

k-gram by anadditional
b and then switching the order of the multiply and add inthe incremental step:

H\Delta (c2 . . . ck+1) = ((H\Delta (c1 . . . ck) L/ c1 * bk) + ck+1) * b
2.3 All-to-all matching

The first scheme to apply fingerprinting to collections of doc-uments was developed by Manber, who apparently independently

discovered Karp-Rabin string matching and applied it to detectingsimilar files in file systems [12]. Rather than having a single candidate string to search for, in this problem we wish to compare allpairs of

k-grams in the collection of documents.The all-to-all nature of this comparison is a key difficulty in document fingerprinting. To illustrate, consider the problem of all-to-all matching on ASCII text. Since there is a

k-gram for every byteof an ASCII file, and at least 4-byte hashes are needed for most interesting data sets, a naive scheme that selected all hashed k-gramswould create an index much larger than the original documents.
This is impractical for large document sets, and the obvious nextstep is to select some subset of the hashes to represent each document. But which hashes should be selected as fingerprints?

A simple but incorrect strategy is to select every ith hash of adocument, but this is not robust against reordering, insertions and

deletions (requirement (3) above). In fact, prepending one charac-ter to a file shifts the positions of all

k-grams by one, which meansthe modified file shares none of its fingerprints with the original.

Thus, any effective algorithm for choosing the fingerprints to rep-resent a document cannot rely on the position of the fingerprints
within the document.The scheme Manber chose is to select all hashes that are 0 mod
p. In this way fingerprints are chosen independent of their position,

and if two documents share a hash that is 0 mod p it is selected inboth documents. Manber found this technique worked well.

In [8], Heintze proposed choosing the n smallest hashes of all
k-grams of a document as the fingerprints of that document. Byfixing the number of hashes per document, the system would be

more scalable as large documents have the same number of finger-prints as small documents. This idea was later used to show that
it was possible to cluster documents on the Web by similarity [6].The price for a fixed-size fingerprint set is that only near-copies
of entire documents could be detected. Documents of vastly dif-ferent size could not be meaningfully compared; for example, the
fingerprints of a paragraph would probably contain no fingerprintsof the book that the paragraph came from. Choosing hashes 0 mod
p, on the other hand, generates variable size sets of fingerprints fordocuments but guarantees that all representative fingerprints for a
paragraph would also be selected for the book. Broder [5] classi-fies these two different approaches to fingerprinting as being able
to detect only resemblance between documents or also being ableto detect containment between documents.

2.4 Other techniques

Instead of using k-grams, the strings to fingerprint can be chosenby looking for sentences or paragraphs, or by choosing fixed-length

strings that begin with "anchor" words [4, 12]. Early versions ofour system also used structure gleaned from the document to select substrings to fingerprint. The difficulty with such schemes, inour experience, is that the implementation becomes rather specific
to a particular type of data. If the focus is on English text, forexample, choosing sentences as the unit to hash builds in text semantics that makes it rather more difficult to later use the system tofingerprint, say, C programs, which have nothing resembling English sentences. In addition, even on text data the assumption thatone can always find reasonable sentences is questionable: the input
may be a document with a large table, a phone book, or Joyce'sFinnegans Wake [9]. In our experience, using

k-grams as the unitof hashing is much more robust than relying on common-case assumptions about the frequency of specific structure in the input.There are approaches to copy detection not based on fingerprinting. For example, in SCAM, a well-known copy-detection system,one of the ideas that is explored is that two documents are similar
if the distance between feature vectors representing the two docu-ments is small. The features are words, and the notion of distance is
a variation on standard information-retrieval measures of similarity[14].

Baker considers the problem of finding near-duplication in soft-ware and develops the notion of parameterized matches, or

p-matches.Consider two strings, some letters of which are designated as parameters. The strings match if there is a renaming of parametersthat makes the two strings equal. For example, if we take the parameters to be variable names, then two sections of program textcould be considered equal if there was a renaming of variables that
mapped one program into the other. Baker gives an algorithm forcomputing

p-matches and reports on experience with an implemen-tation in [2] and in a subsequent paper considers how to integrate

these ideas with matching on k-grams [3].There is an important distinction to be made between copy-detection
for discrete data and for continuous data. For discrete data, such astext files and program source, after a simple suppression of the uninteresting pieces of documents, exact matching on substrings ofthe remainder is a useful notion. For continuous data, such as audio, video, and images, there have been a number of commercialcopy-detection systems built but relatively little has been published
in the open literature (an exception is [13]). The problems here are

more difficult, because very similar copies of images, for exam-ple, may have completely different bit representations, requiring
a much more sophisticated first step to extract features of interestbefore the matching can be done.

Further afield from copy-detection, but still related, is DigitalRights Management (DRM). DRM systems seek to solve the problem of the use of intellectual property by preventing or controllingcopying of documents. DRM schemes are encryption-based: the
valuable content is protected by encrypting it and can only be usedby those who have been granted access in the form of the decryption key. However, regardless of the copy-prevention technologychosen, users must ultimately have access to the unencrypted data
somehow--otherwise they cannot use it--and as discussed in Sec-tion 1, it seems to be nearly a natural law that digital content is
copied. We find ourselves in agreement with [4]: for at least someforms of digital media copy-prevention systems will have trouble
ultimately succeeding. We suspect that in many environments thebest one can hope for is efficient copy detection.

3. WINNOWING

In this section we describe and analyze the winnowing algorithmfor selecting fingerprints from hashes of

k-grams. We give an upperbound on the performance of winnowing, expressed as a trade-off

between the number of fingerprints that must be selected and theshortest match that we are guaranteed to detect.

Given a set of documents, we want the find substring matchesbetween them that satisfy two properties:

1. If there is a substring match at least as long as the guaranteethreshold,

t, then this match is detected, and

2. We do not detect any matches shorter than the noise thresh-old,

k.

The constants t and k <= t are chosen by the user. We avoidmatching strings below the noise threshold by considering only

hashes of k-grams. The larger k is, the more confident we canbe that matches between documents are not coincidental. On the
other hand, larger values of k also limit the sensitivity to reorder-ing of document contents, as we cannot detect the relocation of any
substring of length less than k. Thus, it is important to choose kto be the minimum value that eliminates coincidental matches (see
Section 5).Figures 2(a)-(d) are reproduced from Figure 1 for convenience
and show a sequence of hashes of 5-grams derived from some sam-ple text.

Given a sequence of hashes h1 . . . hn, if n > t L/ k, then atleast one of the

hi must be chosen to guarantee detection of allmatches of length at least

t. This suggests the following simpleapproach. Let the window size be

w = t L/ k + 1. Consider thesequence of hashes
h1h2 . . . hn that represents a document. Eachposition
1 <= i <= n L/ w + 1 in this sequence defines a window ofhashes
hi . . . hi+wL/1. To maintain the guarantee it is necessary toselect one hash value from every window to be a fingerprint of the

document. (This is also sufficient, see Lemma 1.) We have foundthe following strategy works well in practice.

DEFINITION 1 (WINNOWING). In each window select the min-imum hash value. If there is more than one hash with the minimum value, select the rightmost occurrence. Now save all selectedhashes as the fingerprints of the document.

Figure 2(e) gives the windows of length four for the sequence ofhashes in Figure 2(d). Each hash that is selected is shown in boldface (but only once, in the window that first selects that hash). The

A do run run run, a do run run(a) Some text.
adorunrunrunadorunrun(b) The text with irrelevant features removed.
adoru dorun orunr runru unrun nrunr runru
unrun nruna runad unado nador adoru dorun
orunr runru unrun(c) The sequence of 5-grams derived from the text.

77 74 42 17 98 50 17 98 8 88 67 39 77 74 42
17 98(d) A hypothetical sequence of hashes of the 5-grams.

(77, 74, 42, 17) (74, 42, 17, 98)
(42, 17, 98, 50) (17, 98, 50, 17)
(98, 50, 17, 98) (50, 17, 98, 8)
(17, 98, 8, 88) (98, 8, 88, 67)
( 8, 88, 67, 39) (88, 67, 39, 77)
(67, 39, 77, 74) (39, 77, 74, 42)
(77, 74, 42, 17) (74, 42, 17, 98)(e) Windows of hashes of length 4.

17 17 8 39 17(f) Fingerprints selected by winnowing.
[17,3] [17,6] [8,8] [39,11] [17,15](g) Fingerprints paired with 0-base positional information.

Figure 2: Winnowing sample text.
intuition behind choosing the minimum hash is that the minimumhash in one window is very likely to remain the minimum hash
in adjacent windows, since the odds are that the minimum of wrandom numbers is smaller than one additional random number.
Thus, many overlapping windows select the same hash, and thenumber of fingerprints selected is far smaller than the number of
windows while still maintaining the guarantee. Figure 2(f) showsthe set of fingerprints selected by winnowing in the example.

In many applications it is useful to record not only the finger-prints of a document, but also the position of the fingerprints in the
document. For example, we need positional information to showthe matching substrings in a user interface. An efficient implementation of winnowing also needs to retain the position of the mostrecently selected fingerprint. Figure 2(f) shows the set of [fingerprint, position] pairs for this example (the first position is num-bered 0). To avoid the notational complexity of indexing all hashes
with their position in the global sequence of hashes of k-grams ofa document, we suppress most explicit references to the position of
k-grams in documents in our presentation.
3.1 Expected Density

Recall that the density of a fingerprinting algorithm is the ex-pected fraction of fingerprints selected from among all the hash

values computed, given random input (Section 1). We now analyzethe density of winnowing, which gives the trade-off between the
guarantee threshold and the number of fingerprints required.Consider the function

C that maps the position of each selectedfingerprint to the position of the first (leftmost) window that selected it in the sequence of all windows for a document. We saywe are charging the cost of saving the fingerprint to the indicated
window. The charge function is monotonic increasing -- that is, if
p and q are the positions of two selected fingerprints and p < q,then

C(p) < C(q).To prove this, assume fingerprints are selected at distinct positions p and q where p < q but C(p) > C(q). Then both positions
p and q are in both windows. Let hp be the hash at position p and let
hq be the hash at position q. There are two possibilities: If hp = hqthen, as

p < q, the window C(p) was not charged for p nor for q,as
C(q) < C(p). If hp \Lambda = hq then one of C(p) or C(q) was notcharged. These both contradict the hypothesis. We conclude that

the charge function is monotonic increasing.To proceed further recall that the sequence of hashes we are winnowing is random. We assume that the space of hash values is verylarge so that we can safely ignore the possibility that there is a tie
for the minimum value for any small window size. We examine thesoundness of this assumption in Section 5.

Consider an indicator random variable Xi that is one iff the ithwindow

Wi is charged. Consider the adjacent window to the left
WiL/1. The two intervals overlap except at the leftmost and right-most positions. Their union is an interval of length

w+1. Considerthe position
p containing the smallest hash in that union interval.Any window that includes

p selects hp as a fingerprint. There arethree cases:

1. If p = i L/ 1, the leftmost position in the union, then WiL/1selects it. Since

p \Lambda \Xi  Wi, we know Wi must select a hashin another position,

q. This hash is charged to Wi since Wiselected it,
WiL/1 did not select it, and the charge function ismonotonic increasing. Thus in this case,

Xi = 1.

2. If p = i + w L/ 1, the rightmost position in the union interval,then

Wi selects it. Wi must be charged for it, as Wi is alsothe very leftmost interval to contain

p. Again, Xi = 1.

3. If p is in any other position in the union interval, both WiL/1and

Wi select it. No matter who is charged for it, it won't be
Wi, since WiL/1 is further left and also selected it. Thus inthis case,

Xi = 0.

The first two cases happen with probability 1/(w + 1), and sothe expected value of

Xi is 2/(w + 1). Recall that the sum ofthe expected values is the expected value of the sum, even if the

random variables are not independent. The total expected numberof intervals charged, and therefore the total number of fingerprints
selected, is just this value times the document length. Thus thedensity is

d = 2w + 1 .
3.1.1 Comparison to 0 mod p at same density

Here we compare the 0 mod p algorithm and winnowing at thesame density. That is, we take

p = 1/d = (w + 1)/2. For a stringof length
t = w + k L/ 1 consider the event that the 0 mod p al-gorithm fails to select any fingerprint at all within it. (Recall that

winnowing would never fail to do so.) We now compute the prob-ability of this event for one given string. Please note that for two
overlapping such strings these events are not independent. Thusthe probability we compute is not a good estimate for the fraction
of all such substrings of a text that do not have a fingerprint selectedusing the 0 mod p algorithm.

Again we assume independent uniformly distributed hash values.Also we assume large

w; in our experiments w = 100 (see Sec-tion 5.1). Thus, the probability that the guarantee fails in a given

sequence of text of length t, i.e. that no hash in a given sequenceof

w hashes is 0 mod p, is

(1 L/ d)w = \Delta 1 L/ 2w + 1 \Theta 

w \Pi 

e L/

2w
w+1 = eL/2+ 2w+1 \Sigma  13.5%.

3.1.2 Comparison to 0 mod p with guarantee

One may be tempted to try modifying the 0 mod p algorithmto give a guarantee. There is one straightforward solution that we

know of: In the event that a gap longer than the guarantee thresholdthreatens to open up, select all hashes as fingerprints until the next
hash that is 0 mod p.Let the Safe 0 mod p algorithm be as follows. Partition hashes
into:

A* Good if a hash is 0 mod p.A* Bad if it and the

w L/ 1 hashes to its left are not Good, andA* Ugly otherwise [11].

Select all non-Ugly hashes as fingerprints. As we will see in Sec-tion 4 this algorithm is local and is therefore correct. Note that
we have chosen the parameters so that the guarantee t is the sameas that of winnowing. All that remains is to compute the optimal
expected density.Fix a document and consider a position

i. Let Gi and Bi de-note the events that the hash at
i is good or bad respectively. (Ournotation for an event also denotes the appropriate indicator random variable (1 = true and 0 = false) depending on context.) Let
P = 1/p. (Note that to compete with winnowing we would need Pto be rather small:

P <= 2/(w + 1); however even a slightly larger
P will allow for the 1 + x \Pi  ex approximation we use below.) Wehave

Pr[Gi] = P
and (except for the very first w L/ 1 hashes) for small P

Pr[Bi] = (1 L/ P )w \Pi  eL/wP .
Again, the expected value of a sum is the sum of the expectedvalues. Ignoring the error introduced by the first

w L/ 1 hashes, wehave that the expected value of the non-ugliness of a position is

Ex \Lambda \Xi  Gi + Bi\Pi  = \Xi  Ex [Gi] + \Xi  Ex [Bi]

= NP + N(1 L/ P )w\Pi 

N(P + eL/wP ).

The next step is to minimize the density. Let f(P ) = P +eL/wP .Setting

f\Delta (P0) = 0 and solving we have ewP0 = w, or

P0 = ln ww .
We check that f\Delta \Delta (P ) = w2eL/wP > 0 so we have found the globalminimum. If we use this optimal value,

P0, the Safe 0 mod p algo-rithm has density at least

f (P0) = ln ww + eL/

w ln ww = 1 + ln w

w ,
which is considerably more than that of winnowing: 2/(w + 1).

3.2 Queries

This section is primarily about how to choose hashes well, but wedigress a bit here to discuss how hashes can be used once selected.

In a typical application, one first builds a database of fingerprintsand then queries the fingerprints of individual documents against
this database (see Section 5). Winnowing gives us some flexibility

to treat the two fingerprinting times (database-build time and querytime) differently.

Consider a database of fingerprints (obtained from k-grams) gen-erated by winnowing documents with window size

w. Now, querydocuments can be fingerprinted using a different window size. Let

Fw be the set of fingerprints chosen for a document by winnowingwith window size

w. The advantage of winnowing query docu-ments with a window size

w\Delta  >= w is that Fw\Delta  \Phi  Fw, which meansfewer memory or disk accesses to look up fingerprints. This may be

useful if, for example, the system is heavily loaded and we wish toreduce the work per query, or if we are just interested in obtaining
a faster but coarser estimate of the matching in a document.We can extend this idea one step further. Fingerprint a query document with the same window w used to generate the database, andthen sort all of the selected fingerprints in ascending order. Next
look up some number of the fingerprints in the database, startingwith the smallest. If we stop after a few, fixed number of hashes,
we have realized Broder's and Heintze's approach for testing doc-ument resemblance [8, 5]. If we use all of the hashes as fingerprints, we realize the standard notion of testing for document con-tainment. There is also a spectrum where we stop anywhere in
between these two extremes. Broder's paper on resemblance andcontainment gives distinct algorithms to compute these two properties [5]; winnowing naturally realizes both.

4. LOCAL ALGORITHMS

In this section we consider whether there are fingerprinting al-gorithms that perform better than winnowing. We introduce the

notion of local fingerprinting algorithms. We prove a lower boundfor the density of a local algorithm given uniform identically distributed random input. This lower bound does not meet the upperbound for winnowing. We suspect the lower bound can be improved.Winnowing selects the minimum value in a window of hashes,
but it is clearly just one of a family of algorithms that choose ele-ments from a local window. Not every method for selecting hashes
from a local window maintains the guarantee, however. Assume,for example, that the window size is

50 and our approach is to select every 50th hash as the set of fingerprints. While this methoddoes select a hash from every window, it depends on the global position of the hash in a document, and, as discussed in Section 2,any such approach fails in the presence of insertions or deletions.
The key property of winnowing is that the choice of hash dependsonly on the contents of the window--it does not depend on any
external information about the position of the window in the fileor its relationship to other windows. This motivates the following
definition.

DEFINITION 2 (LOCAL ALGORITHMS). Let S be a selectionfunction taking a

w-tuple of hashes and returning an integer be-tween zero and
w L/ 1, inclusive. A fingerprinting algorithm is localwith selection function

S, if, for every window hi, . . . , hi+wL/1, thehash at position
i+S(hi, . . . , hi+wL/1) is selected as a fingerprint.

It can be beneficial to weaken locality slightly to provide flexi-bility in choosing among equal fingerprints--see Section 5.1. We
now show that any local algorithm is correct, in the sense that itmeets the guarantee threshold

t.

LEMMA 1 (CORRECTNESS OF LOCAL ALGORITHMS).Any matching pair of substrings of length at least

t is found by anylocal algorithm.

PROOF. The sequence of hashes of k-grams representing theeach substring spans at least one window,

W , of length w. Be-cause the selection function is only a function of the contents of

W , the same fingerprint is selected from W in both copies.

We now consider whether there is any local algorithm that isbetter than winnowing. We do not have a matching lower bound
for winnowing, but we can show the following:

THEOREM 1 (LOWER BOUND). Any local algorithm with noisethreshold

k and guarantee t = w + k L/ 1 has density

d >= 1.5w + 1 .

Note that winnowing algorithm, with a density of 2/(w + 1), iswithin

33% of this lower bound.

PROOF. Assume the hashes are independent and uniformly dis-tributed. Consider the behavior of the algorithm on every

w + 1stwindow. Such windows are separated by a single position that is

not part of either window. Because the windows are disjoint, theirhashes and selected fingerprints are independent of each other, and
each window selects a separate fingerprint.

Now consider all of the windows between the ith and (i+w+1)stwindows

Wi and Wi+w+1; these are the w windows that overlapthe disjoint windows at either end. Let

Z be the random variablesuch that
Z = 0 iff among these windows, no additional fingerprintis selected, and

Z = 1 otherwise. We compute a lower-bound onthe expected value of

Z.Let
X and Y denote the random variables S(Wi) and S(Wi+w+1)respectively. Again, because the windows do not overlap,

X and Yare independent.

Now, if Y >= X then Z = 1, because the algorithm is required toselect at least one additional fingerprint from a window in between
Wi and Wi+w+1. Otherwise Z >= 0. Since X and Y are identicallydistributed we have Pr

[Y > X] = Pr[X > Y ]. Let \Theta  denote thisquantity. Let
\Delta  = Pr[Y = X]. We have 1 = 2\Theta  + \Delta . Thus
\Theta  + \Delta  = (1 + \Delta )/2 > 1/2 and

Ex[Z] >= Pr[Y >= X] = \Theta  + \Delta  > 1/2.
We thus see that in every sequence of w+1 windows, in additionto the fingerprint selected in the first window we expect to select an

additional distinct fingerprint at least half the time for one of thesubsequent windows. The density of selected points is therefore

d >= 1.5w + 1 .

OBSERVATION 1. This result can be improved slightly: As a bitof notation let

xi = Pr[X = i] for i = 0, 1, . . . , w L/ 1. Of course\Sigma 
xi = 1. Then \Delta  = Pr[Y = X] = \Sigma  x2i . Apply the Cauchy-Schwartz inequality to show

\Delta  >= 1/w. The proof above then givesdensity

d >= 1.5 +

1
2w

w + 1 .

Our lower bound proof relies only on information derived fromtwo windows that are separated sufficiently to be disjoint. We conjecture therefore that 2/(w + 1) is a lower bound on the density ofany local fingerprinting algorithm.

total bytes 7, 182, 692, 852text bytes

1, 940, 576, 448hashes computed
1, 940, 576, 399
winnowing fingerprints 38, 530, 846measured density

0.019855expected density
0.019802
fingerprints for 0 mod 50 38, 761, 128measured density

0.019974expected density
0.020000longest run with no fingerprint

29983

Figure 3: Results on 500,000 HTML pages

5. EXPERIMENTS

In this section we report on our experience with two differentimplementations of winnowing. In Section 5.1 we report on a series of experiments on text data taken from the World Wide Web,and in Section 5.2 we give a more qualitative report on experience
over several years with a widely-used plagiarism detection service,M

OSS.

5.1 Experiments with Web Data

Because of its size and the degree of copying, the World-WideWeb provides a readily accessible and interesting data set for document fingerprinting algorithms. For these experiments, we used500,000 pages downloaded from the Stanford WebBase [1]. We
use the rolling hash function described in Section 2. Because fin-gerprinting a half-million Web pages generates nearly two billion
hashes and 32-bits can represent only about four billion distincthash values, we use 64-bit hashes to avoid accidental collisions.
As an aside, we have found using a rolling (or incremental) hashfunction is important for performance with realistic

k-gram sizes(say
k = 50) when using 64-bit arithmetic. Recomputing a 64-bithash from scratch for each

k-gram reduces the throughput of thefingerprinting algorithm by more than a factor of four.

In our first experiment we simply fingerprinted 8MB of ran-domly generated text. This experiment serves solely to check that
our hash function is reasonably good, so that we can trust the num-ber of matches found in experiments on real data. Strings of

50characters were hashed and the winnowing window was set at
100.Winnowing selected
0.019902 of the hashes computed, which veryclosely matches the expected density of

2/(100 + 1) = 0.019802.Selecting hashes equal to 0 mod 50 results in a measured density of

0.020005, which is also very close to the predicted value of 0.02.We also observed a uniform distribution of hash values; taken all
together, the hash function implementation appears to be sufficientfor our fingerprinting algorithm.

Our second experiment calculated the hashes for 500,000 HTMLdocuments and measured various statistics. We again measured the
density and compared it with the expected density for both winnow-ing and selecting fingerprints equal to 0 mod p. Again the winnowing window size is 100 and the noise threshold is 50. The resultsare shown in Figure 3.

There were interesting things to note in the data. Both algorithmscome close to the expected density in each case. However, the gross
averages cover up some local aberrations. For example, there is arun of over 29,900 non-whitespace, non-tag characters that has no
hash that is 0 mod 50. It is easily checked that the odds of thishappening on uniformly random inputs are extremely small. (The
chances that a string of 29,900 characters has no hash of a substringthat is 0 mod 50 is

(1 L/ 1/50)29,851, which is less than 10L/260.

Even in a terabyte, or 240 bytes, of data, the chances that everysubstring of length 29,900 has no

k-gram hash that is 0 mod 50,is less than
10L/220.) Clearly the data on the Web is not uniformlyrandom.

As discussed briefly in Section 1, there are long passages on theWeb of repetitive, low-entropy strings. For example, in one experiment we did (not reported here) we stumbled across a collection ofpages that appear to be raw data taken from sensors in a research
experiment. This data consists mostly of strings of 0's with the oc-casional odd character thrown in. Both winnowing as defined so far
and selecting hashes equal to 0 mod p perform poorly on such data.For the latter, if a long string has few

k-grams, then it is very likelythat none of them is 0 mod p, and no fingerprints are selected for

the region at all. This is what leads to the large gaps in fingerprintsfor this strategy on real data.

Winnowing, however, has a different problem. In low-entropystrings there are many equal hash values, and thus many ties for the
minimum hash in a given window. To be truly local and indepen-dent of global position, it is necessary to take, say, the rightmost
such hash in the winnowing window. But in the extreme case, saya long string of 0's with only one

k-gram, nearly every single hashis selected, because there is only a single

k-gram filling the en-tire winnowing window and at each step of the algorithm we must

choose the rightmost copy--which is a new copy in every window.There is, however, an easy fix for this problem. We refine winnowing as follows:

DEFINITION 3 (ROBUST WINNOWING). In each window se-lect the minimum hash value. If possible break ties by selecting

the same hash as the window one position to the left. If not, se-lect the rightmost minimal hash. Save all selected hashes as the
fingerprints of the document.

Robust winnowing attempts to break ties by preferring a hashthat has already been chosen by a previous window. This is no

longer a local algorithm, but one easily observes that for any twomatching substrings of length

t = w +k L/ 1 we guarantee to selectthe same hash value and so the match is still found; we simply

no longer guarantee that these fingerprints are in the same relativeposition in the substrings. However, the two fingerprints are close,
within distance w L/ 1. This technique reduces the density on astring such as "0000 . . . " from asymptotically

1 to just 1/w, onefingerprint selected per window-length. We reran the experiment

in Figure 3 and found that the density of winnowing dropped from
0.019855 to 0.019829. One can imagine non-text document setswhere the difference could be greater.

One may wonder why we bother worrying about low-entropystrings at all, as they are in a technical sense inherently uninteresting. But just because data is low-entropy does not mean that peopleare not interested in it--take the example of the sensor data given
above. Such strings do exist and people may want to fingerprint alarge corpus of low entropy data so that copies of it can be tracked
just as they may want to fingerprint any other sort of document.Our final experiment examines in more detail the structure of
copying in 20,000 Web pages taken from our corpus of 500,000pages. Interestingly, even though the theoretical predictions based
upon an assumption that the input is uniformly random work verywell, the distribution of real data is hardly uniform. We need two
definitions:

A* Let the frequency of a k-gram (or its hash) be the number oftimes it occurs.

A* Sort the frequencies in monotonically decreasing order. Therank of a

k-gram (or its hash) is the position of its frequency

10^0
10^1
10^2
10^3
10^4
10^5

10^0 10^1 10^2 10^3 10^4 10^5 10^6 10^7 10^8
Frequency

Rank

slope = -0.68
slope = -0.70

The y-intercept logarithmic difference = 1.07,which is a factor of 10^1.07 = 11.74 allall cleanedwinnowed

winnowed cleaned

Figure 4: Log-log plot of Frequency by Rank for all hashes (upper line) and for fingerprints (lower line) on 20,000 Web pages.

on this list, starting with 1 for the most frequently occurring
k-gram.

Plotting the resulting (rank, frequency) pairs on a log-log scaleone obtains a line of slope about L/

0.7, demonstrating a power lawrelationship between frequency and rank

f \Psi  rL/0.7.
Two such plots of (rank, frequency) pairs are shown in Figure 4.The upper curve is all

k-gram hashes computed from the entire setof 20,000 Web pages, while the lower curve is only those hashes

selected as fingerprints by winnowing. (This is the reason we havelimited the data set in this experiment to 20,000 pages. Even on
20,000 Web pages saving the hash of every k-gram requires quitea bit of storage.) Our data contains an aberrant "plateau", perhaps
because of one document with a long repeating pattern of text, orone file that occurs many times in our sample. The peaked-looking
curves labeled "all" show all of the data while the curves labeled"cleaned" have the aberrant plateau removed.

Since frequencies are integers, they form plateaus where a setof hashes all have the same frequency. For example, for the upper
curve there are over 20 million fingerprints with one occurrence,which ties them all for the last rank. While more obvious in the
lower right, these plateaus occur throughout the data. Additionally,due to the logarithmic scale, almost all points are in the lower right.
Thus, if one were to actually plot all points, the line fit would bequite poor, as it would just go through the center of the last two
plateaus (we tried this). It would also overwhelm the plotting pro-gram with points that do nothing other than thicken the horizontal
lines drawn between points.We plot only the left and right endpoints of each plateau (a plateau
of length one is plotted twice so the weights are not biased duringline fit). The lines are fitted to the cleaned plateau endpoints. The

lines fit quite well, giving a slope of about L/0.7, which correspondsto the exponent of the power law. Zipf seems to have first noticed
the power law phenomenon, stating what has become known as"Zipf's Law" [16]: the frequencies of English words are proportional to the inverse of their rank when listed in decreasing order.That is, frequency and rank of English words exhibit a power law
relationship with exponent L/1.In this experiment, 82% of the fingerprints selected by winnowing were chosen only once; 14% were selected twice; and only 2%occurred three times. At the other extreme, one

k-gram appears3,270 times across all the documents. The distribution of frequencies for the set of all hashes is nearly identical to the distributionfor winnowed fingerprints; again the number of

k-grams that occuronce is 82%, while 14% occur twice and 2% occur three times.

We have looked at some of the most common strings and foundthat they are what one might expect: strings taken from menus (e.g.,
"English Spanish German French . . . "), common legal boilerplate(e.g., disclaimers), and finally repetitive strings (for some reason
the string "documentwritedocumentwritedocumentwrite" was verycommon in our sample). We suspect that the repetitive strings,
in particular, are responsible for the most common k-grams. Be-cause such strings have relatively few

k-grams, they dramaticallyincrease the frequency of a few
k-grams in the overall statistics.

5.2 Plagiarism Detection

One of the authors has run MOSS, a widely-used plagiarism de-tection service, over the Internet since 1997. M

OSS, which standsfor Measure Of Software Similarity, accepts batches of documents

and returns a set of HTML pages showing where significant sec-tions of a pair of documents are very similar. M

OSS is primarilyused for detecting plagiarism in programming assignments in computer science and other engineering courses, though several text

formats are supported as well. The service currently uses robustwinnowing, which is more efficient and scalable (in the sense that
it selects fewer fingerprints for the same quality of results) than pre-vious algorithms we have tried. There are a few issues involved in
making such a system work well in practice.For this application, positional information (document and line
number) is stored with each selected fingerprint. The first stepbuilds an index mapping fingerprints to locations for all documents,
much like the inverted index built by search engines mapping wordsto positions in documents. In the second step, each document is fingerprinted a second time and the selected fingerprints are looked upin the index; this gives the list of all matching fingerprints for each
document.Now the list of matching fingerprints for a document

d may con-tain fingerprints from many different documents
d1, d2, . . .. Inthe next step, the list of matching fingerprints for each document

d is sorted by document and the matches for each pair of docu-ments

(d, d1), (d, d2), . . . is formed. Matches between documentsare rank-ordered by size (number of fingerprints) and the largest

matches are reported to the user. Note that up until this last step,no explicit consideration of pairs of documents is required. This is
very important, as we could not hope to carry out copy detectionby comparing each pair of documents in a large corpus. By postponing the quadratic computation to the last step, we can optimizeit by never materializing the matching for a pair of documents if it
falls below some user-specified threshold.There are a number of issues in a full copy-detection system beyond how fingerprints are selected. To give the reader some senseof how winnowing fits into a complete system, we briefly discuss
the most important problems.M

OSS has several thousand users who wish to do copy detec-tion for many different kinds of data. As mentioned in Section 1,

we use the following architecture. For each document format, afront-end specific to that format eliminates features that should not
distinguish documents (e.g., we eliminate white space in text). Asoutput each front-end produces a string of a standard form, which
is the input to the fingerprinting engine. The fingerprinting codeitself knows nothing about the different kinds of documents. This
architecture has proven essential to maintaining support for a widevariety of document formats. While this benefit may seem obvious,
we report it because it is very tempting to put some document se-mantics in the fingerprinting routines, but we have always found it
to be better to keep the document-specific processing separate.Efficiency is an important consideration for fingerprinting. In
Figure 5 we give code for an efficient implementation of the mainwinnowing loop. This implementation takes advantage of the fact
that by far the most common case is that the minimum value fromthe preceding window is still within the current window; in this
case checking to see if there is a new minimum requires only asingle comparison. The only instance in which it is necessary to recompute the minimum by traversing the entire window is the casewhere the minimum hash of the preceding window is just outside
of the current window; note that the loop that does the scan of thearray works from right-to-left to ensure that the rightmost minimal
hash is selected. Thus, the choice of which of several equal hashesto select is not completely arbitrary. Note the

record functionmust compute the global position using the relative position,

min.Saving this position, together with the selected hash, creates a fingerprint. This loop implements winnowing--it always selects therightmost minimal hash in a window. To implement robust winnowing the <= comparison on line marked (*) should be replacedby

<.As a minor aside, because winnowing selects the minimum hash

void winnow(int w /*window size*/) {

// circular buffer implementing window of size w
hash_t h[w];
for (int i=0; i<w; ++i) h[i] = INT_MAX;
int r = 0; // window right end
int min = 0; // index of minimum hash
// At the end of each iteration, min holds the
// position of the rightmost minimal hash in the
// current window. record(x) is called only the
// first time an instance of x is selected as the
// rightmost minimal hash of a window.
while (true) {

r = (r + 1) % w; // shift the window by one
h[r] = next_hash(); // and add one new hash
if (min == r) {

// The previous minimum is no longer in this
// window. Scan h leftward starting from r
// for the rightmost minimal hash. Note min
// starts with the index of the rightmost
// hash.
for(int i=(r-1)%w; i!=r; i=(i-1+w)%w)

if (h[i] < h[min]) min = i;
record(h[min], global_pos(min, r, w));
} else {

// Otherwise, the previous minimum is still in
// this window. Compare against the new value
// and update min if necessary.
if (h[r] <= h[min]) { // (*)

min = r;
record(h[min], global_pos(min, r, w));
}
}
}
}

Figure 5: Code for winnowing.

in each window, the distribution of hashes selected is skewed. If auniform distribution is desired, the selected hashes can be hashed a
second time (not shown in Figure 5).A very significant issue in a practical copy-detection system is
the ability to ignore boiler-plate. For example, standard copyrightnotices, disclaimers, and other legalese would all come under the
heading of material that we would not be interested in for manyapplications. In the case of plagiarism detection, boilerplate is usually material supplied by a course instructor that is expected to bepart of the final solution--i.e., it is sanctioned copying. Excluding
boilerplate is easily done by fingerprinting the boilerplate with aspecial document ID that indicates any match with that fingerprint
should be discarded.Presentation of the results is another important issue for users.
Statistics such as reporting the percentage of overlap between twodocuments are useful, but not nearly as useful as actually showing
the matches marked-up in the original text. MOSS uses the finger-prints to determine where the longest matching sequences are; in
particular, if a1 in document 1 matches a2 in document 2, and b1in document 1 matches

b2 in document 2, and furthermore a1 and
b1 are consecutive in document 1 and a2 and b2 are consecutive indocument 2, then we have discovered a longer match across documents consisting of a followed by b. While this merging of matchesis easy to implement,

k-grams are naturally coarse and some of thematch is usually lost at the beginning and the end of the match. It

is possible that once a pair of similar documents are detected usingfingerprinting that it would be better to use a suffix-tree algorithm
[15] to find maximal matches in just that pair of documents.In Section 2 we mentioned that there appears to be a sharp threshold between what people consider coincidental similarity (meaningreuse of idioms, common words, etc.) and copying. We have no
formal experiments on this topic, but we have informally exper-imented with M

OSS by simply examining the results of tests onsample data. Regardless of input data type, the result is always

the same: There is some value of k (dependent on the documenttype) for which the reported matches are likely to be the result of
copying, and for a slightly smaller value of k significant numbersof obvious false positives appear in the results. Along the same
lines, early versions of MOSS incorporated a technique similar toBaker's parameterized matches (Section 2). However, we found
that replacing all of the parameters with a single constant and in-creasing

k by 1 worked just as well. This appears to be a generaltrick: sophisticated efforts to exploit document semantics can often be closely approximated by very simple exploits of documentsemantics together with a small increase in

k.We can report that after years of service, M

OSS performs itsfunction very well. False positives (hash collisions) have never

been reported, and all the false negatives we have seen were quicklytraced back to the source, which was either an implementation bug
or a user misunderstanding. Furthermore, users report that copydetection does dramatically reduce the instances of plagiarism in
their classes.

6. CONCLUSIONS

We have presented winnowing, a local document fingerprintingalgorithm that is both efficient and guarantees that matches of a

certain length are detected. We have also presented a non-triviallower bound on the complexity of any local document fingerprinting algorithm. Finally, we have discussed a series of experimentsthat show the effectiveness of winnowing on real data, and we have
reported on our experience with the use of winnowing in practice.

7. ACKNOWLEDGMENTS

The authors wish to thank Joel Auslander, Steve Fink, and PaulTucker for many useful discussions and for helping make this work

possible.

8. REFERENCES

[1] Arvind Arasu, Junghoo Cho, Hector Garcia-Molina, AndreasPaepcke, and Sriram Raghavan. Searching the web. ACM

Transactions on Internet Technology (TOIT), 1(1):2-43,2001.
[2] Brenda S. Baker. On finding duplication and near-duplicationin large software systems. In L. Wills, P. Newcomb, and

E. Chikofsky, editors, Second Working Conference onReverse Engineering, pages 86-95, Los Alamitos,
California, 1995. IEEE Computer Society Press.
[3] Brenda S. Baker and Udi Manber. Deducing similarities injava sources from bytecodes. In Proc. of Usenix Annual

Technical Conf., pages 179-190, 1998.
[4] Sergey Brin, James Davis, and H'ector Garc'ia-Molina. Copydetection mechanisms for digital documents. In Proceedings

of the ACM SIGMOD Conference, pages 398-409, 1995.
[5] Andrei Broder. On the resemblance and containment ofdocuments. In SEQS: Sequences '91, 1998.

[6] Andrei Broder, Steve Glassman, Mark Manasse, andGeoffrey Zweig. Syntactic clustering of the web. In

Proceedings of the Sixth International World Wide WebConference, pages 391-404, April 1997.
[7] The Crystals. Da do run run, 1963.

[8] Nevin Heintze. Scalable document fingerprinting. In 1996USENIX Workshop on Electronic Commerce, November

1996.
[9] James Joyce. Finnegans wake [1st trade ed.]. Faber andFaber (London), 1939.

[10] Richard M. Karp and Michael O. Rabin. Pattern-matchingalgorithms. IBM Journal of Research and Development,

31(2):249-260, 1987.
[11] Sergio Leone, Clint Eastwood, Eli Wallach, and Lee VanCleef. The Good, the Bad and the Ugly / Il Buono, Il Brutto,

Il Cattivo (The Man with No Name). Produzioni EuropeeAssociate (Italy) Production, Distributed by United Artists
(USA), 1966.
[12] Udi Manber. Finding similar files in a large file system. InProceedings of the USENIX Winter 1994 Technical

Conference, pages 1-10, San Fransisco, CA, USA, 17-211994.
[13] Peter Mork, Beitao Li, Edward Chang, Junghoo Cho, ChenLi, and James Wang. Indexing tamper resistant features for

image copy detection, 1999. URL:citeseer.nj.nec.com/mork99indexing.html.
[14] Narayanan Shivakumar and H'ector Garc'ia-Molina. SCAM:A copy detection mechanism for digital documents. In

Proceedings of the Second Annual Conference on the Theoryand Practice of Digital Libraries, 1995.
[15] Esko Ukkonen. On-line construction of suffix trees.Algorithmica, 14:249-260, 1995.
[16] George K. Zipf. The Psychobiology of Language. HoughtonMifltm Co., 1935.