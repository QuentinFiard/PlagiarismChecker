

E f f i c i e n t   F i x P o i n t   C o m p u t a t i o n  

B.  Vergauwen,  J.  Wauman,  J.  Lewi 
Department  of Computer  Science,  K.U.Leuven, Celestijnenlaan  200A,  B-3001  Leuven,  Belgium 

Abstract.  Most  of the  algorithms  presented  for  computing  fixpoints  have  been developed  for very specific  application  areas,  e.g.  dataflow  analysis,  abstract  in- 
terpretation,  denotational  semantics,  system  verification,  to  mention  but  a  few. Surprisingly  little  attention  has  been  devoted  to the  construction  of general pur- 

pose,  application  independent  fixpoint  algorithms  (one  notable  exception  being [17]).  The  goal  of this  paper  is  to  put  known  ideas  and  algorithms  into  a  more 
general  and  abstract  setting.  More precisely  we discuss  a  variety of efficient  gen- eral purpose  algorithms for computing  (part  of) the least  solution of a  monotone 

system  of  equations  over  a  complete  partial  order  structure.  The  advantage  of having  general  purpose  fixpoint  algorithms  is  twofold.  Firstly,  once  proven  cor- 

rect, they  can  be instantiated  to  a wide variety of application  domains.  Secondly, they separate  the essentials of fixpoint computation  from concrete application  do- 
main  aspects.  We consider algorithms  based on  (chaotic)  fixpoint  approximation, as well as  algorithms  based  on fixpoint  induction  techniques.  The  algorithms  are 

constructed  in  a stepwise  fashion:  First  a  basic  schema,  capturing  the  essence  of the  algorithm,  is  discussed,  which  is  then  subsequently  refined  using  a  number 
of optimisation  steps.  As  a  sample  application,  we  sketch  how  an  algorithm  for computing  the  prebisimulation  preorder  is obtained,  matching  the  complexity  of 

the  so far  best  known  'ad  hoc'  constructed  algorithm  for this  preorder. 

1  Monotone  Equation  Systems 
Fix  a  set  l; of values and  an  order relation  E  on r  such  that  (12, _)  forms a complete par- 
tial  order  (cpo)  structure  of finite height  with  bottom  element  _[_. A  monotone  equation system  E 

over the  cpo  (V,E)  is  of  the  form 

x l =   fl 

Xn  ~  fn 
where  each  left hand  side  x~ is  a  variable  (an  unknown),  and  each  right  hand  side  fi  is  a 
function  of the  form  f,  : O  --+ V,  where  O  =  IX  ~  V] with  X  =  {xl,...,  xn}.  Left  hand 

side  variables  are  assumed  to  be  distinct.  Furthermore,  right  hand  side  functions  fi  are 

required  to  be  monotone.  Recall that  a  function  f  : O  --+ V is monotone  iff f(O) E  f(O') 
whenever  0  E_  ~',  where  the  order  relation  on  O  is  just  the  pointwise  extension  of  E, 
i.e.,  8  E  0'  iff  O(x)  E  O'(x)  for  every  x  E  X.  An  element  8  E  O  is  a  solution  of  E  iff O(x)  =  f,(O) 

for  every  x  E  X,  where  f~  denotes  the  right  hand  side  function  of  the 
equation  having  variable  x  as  its  left  hand  side.  As  right  hand  sides  are  monotone,  it 
immediately  follows from  basic  fixpoint  theory  that  E  has  a  unique  least  solution  w.r.t. 

E,  noted  [E].  It  is  this  least  solution  [E]  that  we  are  interested  in  computing. 

2  Global  FixPoint  Approximation  (Basic Schema) 
The  least  solution  [E]  of  E  can  be  computed  using  chaotic  [6]  fixpoint  approximation: 

315 
val  :=  Sx  E X._L do 

not  (Vx  E  X :   val(x)  =  fx(val)  ) 
x  :=  choose(X  ) 

od --  val 

-- [El 

_...-+ 

Data-structure  val  : X  --+ 12  encodes the current value of variables. [El  is approximated 
from below (i.e.  val  E  [El)  by repeatedly selecting a variable, evaluating the associated 

right hand side function, and updating the value of the selected variable accordingly. To 

abstract  away the details of right hand side evaluation, an evaluation function EvalRhs 
is  assumed such that EvalRhs(x,8)  returns f~(O)  (for x  E X  and 0 E O).  The updating 

process terminates when. all variables are  'stable'. 

The  above  schema  captures  the  essentials of  fixpoint  approximation. It  is  easily  seen 

to  be  partially  correct.  Assuming that  the  choose-operation  is  fair  (i.e.  variables that 

are  unstable are eventually selected for  evaluation), termination is also guaranteed. To 
obtain  an efficient practical algorithm, the  choose-operation  should  be  implemented in 

a  way  that  minimizes the  number of  iteration  (evaluation) steps  needed  for  reaching 
the  least  solution |El.  The  key idea  of our  variable selection strategy is  the  following: 
Assume  that  a  variable  x  has  been  selected  for  evaluation. If  it  now  turns  out  that f=(val) 

evaluates to  val(x),  then x  was certainly a  'bad' choice:  Time has  been wasted 
computing f~ (val)  without  there being any progress made.  Hence in order to  minimize 

the  number of  evaluations needed  for  reaching  [El,  care  should  be  taken  in  order  to 

avoid selecting variables that  are currently stable, ttow do we know, without computing f~(val), 

whether or not x is stable? To find out, an additional workset  ws  C_  X  is used 
for  keeping  track  of  variables  that  are  potentially unstable. In  other  words,  variables 

outside  ws  are  known  to  be  stable.  Formally  the  meaning of  ws  is  captured  by  the 
following assertion 

which  will be  kept  invariant (@~ stands for  X  \  ws).  Assuming that  I ~   is  kept  invari- 

ant,  then  obviously only variables belonging to  ws  should be selected for  re-evaluation. 

Furthermore,  as  a  byproduct,  a  simple  stability  detection  mechanism is  obtained  by 
checking emptyness of  ws:  If  ws  =  ~  then  it  immediately follows  from  I, os  that  val  is 

stable. The  whole point now is,  of course, to keep I~s  invariant, while at  the same time 

keeping the  size  Iwsl  as  small  as  possible! The  challenging point  here  is  to  restore  I~, 
when components of val  change value as a result of re-evaluating right hand sides.  More 
exactly, in order  to restore I,,s  upon modifying val(x),  a  code fragment Restore(x,  ws) 

has  to  be constructed satisfying the following Hoare-like specification: 

{  ~ ,  ^  fi9 c  ws  ^  v az (x) #fx( ,aZ )  } 
ws  :=  ws \  {x}  val(x)  :=  f=(val)  Restore(x,ws) 

{x~,} 

(It is of course understood that  Restore(x,  ws)  may only modify variable ws).  Assuming 
a  correct implementation for  Restore(x,  ws),  we obtain: 

316 
Global fixpoint  approximation  :  basic  schema,  parameterised  by Restore(x, ws) 

val  :=  ~x  E X.J_  w s : = X  
while  ws #  0  --~ x  :=  ehoo~e(ws)  ~os :=  ~s   \  {x} 

~o~  :=  ~ l( x)  ~Z(x)  :=  F .~ al~ (x,   ~al) if  ~ot~ #  ,,al(x)  - - ,   Restore(x,  ws)  n 
od 
--  val  "= [El 

Plugging  in  any  correct implementation for  Restore(x,  ws)  yields a  correct schema for 
computing  [El.  Furthermore the  number  of evaluations is in  the  worst case O(IXI~.H), 

where H is the height  of the cpo  (V, if_). It  now  remains to implement Restore(x, ws). 

3  I m p l e m e n t i n g   R e s t o r e ( x ,   w s )  
A  (naive)  straightforward implementation for  Restore(x,  ws)  is  to  simply  re-initialize ws.  I.e.  take Restore(x,  ws)  =_  ws  := X.  Hence whenever a  variable changes value,  all 

variables are  scheduled for  re-evaluation, thereby  trivially restoring I~,,.  Re-initialising ws 

is  a  simple  but  rather  crude  way  to  restore  I~,.  Below  we  discuss  two  more  so- 
phisticated  implementations.  The  key  idea  is  to  keep  track  of  dependencies  between 
variables and  right  hand  sides.  Whenever  a  variable then  changes value, this  additional 
dependency information is consulted to  find out  which  variables are  affected and  hence might become unstable as a  result of the change. Those  variables are then  scheduled for 

re-evaluation  by  including  them  in  ws.  Both  schema's  differ in  the  granularity  of  the dependency information that  is  carried  around. 

3.1  Chasing  Static  (Syntactic)  D e p e n d e n c i e s  
The  first  schema for  Restore(x,  ws)  uses  syntactic dependency information.  Static  de- 

pendencies  are  well-known,  and  their  use  has  been  discussed  by  several researchers in various  apphcation  domains,  e.g.  by  [12] in  the  context  of dataflow analysis, and  more 

recently, by  [7] in  the  context  of abstract interpretation, and  by  [1,5,22]  in  the  context 
of model-checking. They  are also  at  the  heart  of O'Keefe's general algorithm [11]. 
The  key idea  underpinning  static dependencies is  a  follows:  For  a  function  f  : O  ~  P 
define Arg(f)  as  the  set  of argument  variables of f,  i.e.  only  variables from hrg(f)  are 

needed  in  order  to  evaluate  f.  Formally Arg(f)  is defined  as  the  least  set  A  C_ X  such that  the  following holdsl: 

VO,O' E O :   OIA =O'la  ~  f(O)=  f(O t) 
Note  that  in  most  practical applications lrg(f=),  or  at  least  a  small superset of it,  can easily be computed by scanning the defining expression of f=  and  recording all variables 

occurring in it.  Clearly upon  modifying val(x),  only variables y that  depend upon  x  (i.e. 

x  E Jlrg(fv) )  are affected and  hence may  become unstable.  Hence: 

Restore(x,  ws)  =  ws  := wsO  {y  E X  I x  E hrg(fy)} 
0[a denotes the restriction of 0 to A. I.e., 0In is the mapping m  : A --+ V such that re(x) =  O(x) for every x  E A. 

317 
To efficiently execute the above updating-statement for ws,  an additional data-structure 

Dep  : X  --+ 2 x  is used, such that Dep(x)  =  {y  E  XI z   E  hrg(fy)}. Note that, as Dep does 
not depend upon val,  it can be computed prior to fixpoint iteration. Putting everything 
together yields: 

Global fixpoint  approximation  guided  by  syntactic  dependencies 

val  :=  +Xx E X._L  ws  := X 
Dep  :=  )~x E X . { y  E  X  l~  e  Arg(fy)} while  ws  #  ~  ----+ 

x  :=  choos~(ws)  ~os :=  ~s   \  {x} vo~  :=  val(x)  val (x) :=   F valP~s(x,  val) 
if  "otd #  ~al(z)  -+  ~ s   :=  ws u  D~p(x) fl 
od 
--  val  =  [El 

As  a  variable  x  is  only  re-evaluated when  one  of  the  variables from  Arg(f=)  changes 
value, it  is clear that  x  is evaluated at  most  1 +  H.IArg(fx)l times, where tt is the height of the cpo  (V, E_). Hence the number of evaluations for  x  only depends on its right hand 

side function, and is independent of the  rest  of the  equation system. Summing up over 

all  variables,  the  total  number of  evaluations is,  in  the  worst  case,  O(H.IEI),  where IEI  =  IXl  + 

~ =+x  tArg(f~)l. 

3.2  Chasing  D y n a m i c   (Se mantic )  D e p e n d e n c i e s 
In  the  syntactic approach  a  right  hand  side  function f  is  re-evaluated anew  whenever 
one of  its  argument variables changes  value.  The  set  hrg(f),  however, is  static  in  the 
sense  that  it  does  not  take  into  account  the  specific  context  val  in  which  f  is  to  be 

evaluated.  By  properly  taking  this  context  into  account,  some  of  these  reevaluations 
may  be  further  avoided.  Let's  illustrate  this  by  means  of  an  example:  Consider  the 

boolean function f  -  xl A  x2 A x3.  Clearly, hrg(f)  =  {xt,x2, x3}.  Now  assume that  f  is 
evaluated in  a  context where  val(xl)  =  fa l se .  Hence f(val)  =  fa lse .  Furthermore, as 
long as val(xl)  stays f a l s e ,  so does f(val).  Hence there is no need to re-evaluate f  when 

x2 or  x3  changes value. Only when the  value of xl  is modified should f  be  reevaluated. 
In order to keep track of semantic dependency information, an additional data-structure 

arg  : X  ---+ 2 x  is  used.  Informally arg(x)  records the  set  of variables that  x  currently 
depends on (for x  E @-g). Formally the meaning of arg is given by the following assertion 

(Id~p)  Vx e  ~:   Suff(y,,  ~ l,   arg(x)) 
where predicate Surf(f, 0, A)  holds iff the  values of variables from A  suffice to  evaluate 
f  in context 0. I.e.: 

Suff(f,0,A)  ~  V0' E O:   OIA =  O'IA  =~  f(O)  =  f(O') 
To  compute  semantic dependency information  (and  hence  to  keep  assertion  Id~p in- 

variant),  we  slightly  extend function EvalRhs(x,0 )  such  that  the  additional set  A  of 
variables  that  were  needed  (used)  in  order  to  evaluate  f=(val)  is  also  returned.  The 
refined specification for Evalahs  now reads as follows: 

318 
func  EvalRhs(  x, *  )  return  ( V, A ) 
-- postcondition: v  =  f=(8),  A  C_ Arg(f=),  and  Su f f ( f ~, O , A ) 

Assuming invariance of Idr  Restore(x,  ws)  can  be implemented as follows: 

Restore(z,  ws)  -  ws  :-= wsU  {y  E  ws l   x  E  arg(y)} 
To efficiently execute the above updating-statement for ws,  an  additional data-structure 

dep  : X  --+ 2 X  is used, such that  dep(x)  =  {y  e  -@'~I x  E  arg(y)}.  Note  that,  unlike Dep, 
variables arg  and  dep  are  dynamic in  that  they  depend  upon  the  current  context  val. Putting  everything together yields: 

Global fixpoint  approximation  guided  by  semantic  dependencies 

val  :=  )~x E  X._L  ws  :=  X 
arg  :=  )~x E  X.O  dep  :=  ~ x E X . 0  while  ws  #  $ 

x  :=  choose(~s)  ~s :=  ~ s  \  {x} 
,o.  :=  ,al(x)  (,al(~),  arg(x))  :=  F val m~s (~, ,al)  for  y  E  arg(x)  do  dep(y)  :=  dep(y)  O {x}  od  

if road #  val(x)  ---+ for  y E  dep(x)  do 

for  z  E  arg(y)  do  dep(z)  :=  dep(z)  \  {y}  od ~s   :=  ~s   u  {y}  arg(y)  :=  0 
od 
fl od 

--  val  =  [El 

As  arg(x )  C  Arg(f=),  the  number  of  evaluations  using  semantic  dependencies  is  al- 

ways less (or equal)  than when  syntactic dependencies are used.  (There is, however, also slightly more overhead in  the  semantic case). 

The  use  of semantic dependencies for  guiding  fixpoint  iteration is  rather  novel.  It  has 
been discussed in  a  general setting by the authors in  [26],  and, independently, by [10,16] 
in  the context of abstract interpretation for Prolog. Semantic dependencies gain interest 

as  right  hand  sides become  'sparse'. Sparseness is  a  rather  qualitative notion.  A  func- 
tion  f  is  said  to  be  sparse if,  for  most  environments  O, only  a  'small' fragment  of 0  is needed  in  order  to  evaluate  f(O),  although  the  actual  fragment  needed  might  greatly 

depend  on  0.  Boolean  equation  systems e.g. usually exhibit  a  high degree of sparseness (true  V  _ =  true,  and  false  A  _ =  false). 

4  L o c a l   F i x P o i n t   A p p r o x i m a t i o n  
The  algorithms discussed so far  are global,  in that  they compute the complete least solu- tion  [E].  In practice, however, one is often only interested in  the value of one  particular 

variable, say  5.  This is e.g. the case in model-checking [1,2,5,8,15,20,23,25]  where one is 
only interested in whether  the  initial  system state satisfies a  given property. Of course, 

one  can  always first  compute  [El,  and  then  pick  the  component  [E](~)  of interest.  It 
seems, however, overwhelming to  compute the complete least solution [El  just in order 

319 
to  decide the value [E](5)  of interest. This observation is central to the development of local 

algorithms. As  opposed  to  global algorithms, local  algorithms aim  at  computing 
the desired component [E](5)  by investigating only a  'necessary' fragment of the equa- 
tion system E.  In this section we discuss how to derive local algorithms from the global 

algorithms discussed in sections 2 and 3. 
The  basic local fixpoint approximation schema for computing [E] (5) is depicted below. 
The  essential (only)  difference with  the  basic  global schema of section 2  is  the  stabil- 
ity  detection mechanism: Instead  of  a  priori stabilizing  all  variables as  is  done in  the 

global schema, the local schema tries to construct only a  partial  fixpoint. I.e., it  tries to 
construct a  subset  S  of variables (called the  search  space)  that  is stable and complete 

(closed, self-contained) in the sense that  values of variables from S  do not depend upon 
values of variables outside S.  More formally, the  goal is  to  construct an S  _D {5}  such 
that  the  following holds: 

Stable(S,  val)  and  Complete(S,  val ) 
where  Stable( S, val )  - V x   fi9 S:  val(x)  =  f=( val ),  and Complete(  S, val )  -= Vx  fi9 S :  

Suff(f=, val, S). 

Two  basic  actions are  needed in  order  to  construct such  a  stable  and complete search 
space: 

-  Update  :  Variables  of  S  that  are  in  ws  are  potentially unstable,  ttence  they  are 

reevaluated and updated, just  as in the  global schema. 
-  Expand : If a  variable of S  is stable but currently depends upon variables outside S, 

then the search space is expanded as  a  step  towards completeness. 

The  local schema terminates when the search  space S  is both  stable and complete. We 

then have  that  pal  agrees with  IS 1 on a.  Hence, as  ~  fi9 S,  [El(5 ) =  pal(5). 

Local  fixpoint  approximation  :  basic  schema,  parameterised  by  Restore(x, ws) 

val  :=  )~x E X . Z   ws  := X  3 : = { 2 }  
loop if  S N w s #   O  ---+ 

-- Update 

z : = c h o o s e ( S n w s )   w s : = w s \   {x} void 

:=  pal(x)  pal(x)  := EvalRhs(x, pal) 
if  Void #  pal(x)  ~  Restore(x, ~s)  fi 
{x  Z S N ~ I   not  Suf~(f~,val,  S)}  #  0 

-- Expand 

x  := choose(  {x  fi9 S  n-@--gl not  Suff(f~, pal, S)} ) 

y  :=  ehoose(Arg(fx) \ S) S:=SU{y} 

~ otherwise  ---+  exit  loop fi 
e ndlo op  
--  ~  fi9 S  and  palls  =  [E]ls 

Code fragment Restore(x,  ws)  serves the same purpose as in the global schema and can 
be implemented as discussed in section 3. Implementing rule Expand is also easy: In case 

320 
semantic  dependencies  are  used,  the  information  in  arg  can  be  used  to  keep  track  of {x  E  S  n~-~Inot  Suf:~(f~,val,S)}. 

For  y  (the  variable to  be  added  to  S),  one  of the 
variables from  arg(x) \ $  is then  chosen. In case of syntactic dependencies, the extended 
evaluation function of section 3.2 can be exploited in following way: Whenever a variable 

of  S  is  re-evaluated,  we  use  this  extended  evaluation function  to  find  out  whether  its new value currently depends on variables outside S. In this way we can easily keep track 

of  (a small superset of)  {x  E S  n  ~-~ I not  Suff(f=, val, S)}. 

Note  that  a  practical implementation will probably give rule Update  priority over rule 
Expand.  I.e.,  in  case the  guards of Update  and  Expand are  both  enabled,  rule Update is 

selected. In  this way search space expansion only takes place when  all variables of S  are 
stable,  thereby  keeping the  size of  the  search  space  as small  as  possible.  For the  same reason, additional search space reduction techniques can  be integrated within  the above 

local schema. Variables that  were added to S  at some point, may later on turn  out not to 

be needed any longer, at least not as far as computing [E](~) is concerned. Such 'useless' 
variables may however cause a further (cumulative) useless expansion of the search space. The  goal of  (occasionally) running  a  reduction  algorithm is precisely to  remove useless 

variables  from  S.  Several  reduction  heuristics  can  be  thought  of.  To  mention  one:  In case  semantic dependencies  are used,  then  a  simple,  yet  effective, reduction  technique, 

exploiting the  semantic dependency information  contained in  arg,  would  be  to  remove 

all variables from  S  that  are no longer  (directly or indirectly) 'reachable' from  ~. The idea underlying the local approximation schema, i.e. the construction of a stable and 

complete search space, is  closely related to  the  "minimal function  graphs"  approach in 

refined denotational semantics [9]. It is also at the heart of the model-checking algorithms discussed in  e.g.  [1,23]. 

5  L o c a l   F i x P o i n t   I n d u c t i o n  
In  this  section  we  present  a  local  function  t.ocComp  for  computing  [E](x)  based  on 
fixpoint induction.  An  implicit use  of fixpoint induction techniques can  be found e.g. in 

[3,14,15,20,25],  where  algorithms,  under  the  form  of tableau  systems,  are  discussed for 
model-checking the  modal mu-calculus [13]. These tableau systems, however, essentially 

deal with  boolean equations. The  algorithm discussed in  this section is based  on  a more 
explicit use  of fixpoint induction.  Furthermore it  is not restricted to  boolean variables. Informally, the  key idea is the  following: In order  to compute  [El(x),  first  make a  safe 

assumption  about  [E](x).  I.e.  take  a  value  v  E  [E](x)  and  assume  that  [E](x)  equals 
v.  Then  check  whether  the  latter  assumption  is  correct.  This  is  done  by  evaluating 

f =( [E ] )  under  the  assumption that  [El(x )  equals  v.  If, under  the latter  assumption, it 
indeed  turns  out  that  f=( [E] )  evaluates to  v,  then  the  assumption was indeed correct, 

and  hence  [E](x)  =  v.  In  order  to  compute  jr=(|E])  under  the  assumption  that  v equals [E](x),  many  different components of [E]  may be needed.  For [El(x)  there is no 

problem: According to  the  assumption,  v can be  used for  [E~(x).  If, however,  [E](x')  is 
needed for some  x'  #  x,  then  first compute  [E](x')  under  the  assumption  that  v equals 

[E](x).  This  can  be  done  by  a  recursive  application  of  the  above  schema.  I.e.  take  a 
safe  assumption v'  for  [E](x'),  and  then  try to  prove that  this  assumption is correct by 

proving  that  f~,([E ])  evaluates to  v'.  The  difference with  computing  f ~ ( [ E ] )   now  is that  fz,( [E] ) is computed under  two assumptions: an  assumption that  v  equals [El(x), 

and  an  assumption that  v' equals [E](x').  If, in order to compute fx, ( [E] ), yet another 
component  [E](x")  is  needed  (x  #  x"  #  x'),  then  this  component  is  again  computed 

by  recursively applying the  above schema, etc.  Termination is guaranteed as  the  list  of 
assumptions grows with  each recursive invocation. 

321 
To formally capture the  above fixpoint induction technique, define  -,z  as the  smallest 

binary relation on  [X  -r  V]  generated  by  the  following rules: 2 

(R4 

(R~) 
-q~/~l  "*  c ,A  ..,.,  (~,lv) 

(R1)  ,4  "-* 
(R2)  .~  ~  c  ~  ~,  (~/~) 

,4  ~  (x/,4(x))  x  fi9 dora(,4) 
x  r  dom(A),  v  E  [E](x),  and  v  --- f~(0[C])  for  all 0  fi9 O 

Informally, ,4  -,z  C may  be  read  as follows:  Under the  assumption that  ,4  equals  | E l  
on  dora(A),  it  follows that  C equals  [E]  on  dora(C).  Rules  (R1), (R2), (R3)  are straight- 
forward.  Rule  (R4)  captures  the  induction  principle:  In  order  to  derive  (x/v)  from  `4 

(with  x  ~  dora(`4)  and  v  U_ [E](x)),  a  mapping  C has  to  be  derived from  the  extended 
assumption list  `4[x/v]  such  that  the  right  hand  side  f~  associated  with  x  evaluates to 
v  when  only information from  C is used. 

T h eo r e m  Let  A  ",~  C.  Then  ,4  =  [E]ldo,,(.a) implies C =  [E]ldom(O. 
C o ro l l ar y  Let  r  ,,z  ( x/ v) .   Then  [E](x)  =  v .  

Function  LocComp(x,`4),  based  upon  the  above  rules  for  --~  ,  is  listed  below.  It 
computes  [E](x)  under  the  assumptions `4.  Hence  according  to  the  above  corollary,  a 

toplevel call  LocCorap( x, c )  returns  [El(x).  Variable v is  the current safe  approximation 
for  [E](x)  (initially  v  =  3_). The  goal  of  the  inner  while-loop  is  to  check  whether  v 

equals  [El(x)  by  evaluating  fx(  [El )  under  the  extended  assumption list  A[x/v].  This 
evaluation is done in  a  lazy  (demand driven)  manner: Only components of [E]  that  are 

really  needed in  order  to  evaluate  fx( [E] )  are  computed,  one  at  a  time.  To  implement 
lazy  evaluation,  the  evaluation  function  EvalRhs  is  slightly  extended  in  the  following 

way:  Its  second  parameter  is  allowed  to  be  a  partial  mapping  p  :  X  -r  V,  providing 

only  values for  variables belonging to  its  domain  dom(p).  If the  values of variables from dora(p) 

suffice to  evaluate  fx,  then  EvalRhs( x, p)  returns  this  desired  value,  as  before. 
Otherwise  the  symbol  ?  is  returned,  indicating that  more  argument  values  are  needed 
in  order  to  evaluate  f~.  The  latter  can,  of course,  only occur  in case  hrg(f, )  q:  dora(p). 

The  result of the lazy evaluation of fx([E])  is stored in  v . . . .   If v ~   =  v,  then it  follows 
from  rule  (R4)  that,  under  the  assumption .4,  the  assumption  v  =  [E](x)  was  correct, 

aud  hence the  value v  is returned. If,  on the  other hand,  fz ( [E] )  evaluates to  v ~   r  v, 
then  it  must  be  the  case  that  v  E  v n ~   E_ [E](x).  (This  is  easily proved  using a  simple 
inductive  argument,  exploiting  the  monotonicity  of  Ets  right  hand  sides).  Hence  v  is 

updated  by v n ~ ,   and  the  whole procedure is repeated  using the  new value of v  as  a  safe 

approximation for  [El(x). 

2 The  symbol -~  denotes partial  mappings. The  domMn of ,4 : X  -~ 12, i.e. the  set  of variables x  E X  for which .A(x)  is defined, is denoted  by dom(.A),  e  denotes  the  'empty' mapping, i.e. 

dom(~)  =  0. A[x/v]  denotes .4 'updated' by blading x to v, i.e., dora(A[x/v])  =  dom(,4)U{x}, .A[x/v](x)  =  v,  and  .A[x/v](x')  =  ~4(x')  for x'  :/: x.  We also  use  (x/v)  as  an abbreviation for 

e[x/v]. Finally, O[C] denotes 0 'updated' by C, i.e., O[C] is the mapping 0' such that O'(x)  =  C(x) 
if ~  e  dora(c),  and  0'(~)  =  0(~)  otherwise. 

322 
Local fixpoint  induction  :  basic  schema 
func  LocComp( x, -4 ) return  ( v ) is 
-- postcondition:  -4  -,~  (x/v) 

if  x  e  dora(`4)  ---,  ~  :=  -4(x) x  ~[ dom(`4)  --+ 

v: =   l 
loop 

p := e  vne~ :---- EvalRhs(f~,p) while  v,~w  =  ?  ---* 

--  `4[x/~]  ~  p x'  :=  choose(Arg(fx)  \  dora(p)  ) 

r  :=  LocCo~p(x',,4[x/v]  ) 
od i f v = v ~   ~  ex it  l o o p t l  
--  v  r- v . ~   _E [Z](x) 

V  :=  Ynew e n d l o o p  

--  .4[x/v]  ...*  p,  with  v  U  [E](x),z  r  dom(.4),f~(a[p])=  v --  Hence,  using  rule  (Ra),  ,4  -,z  (x/v) 
fl 

V0 EO 

Termination  of LocComp is  guaranteed  as  recursive invocations have  more  assumptions 

(i.e., dora(-4)  C  dom(A[x/v])).  Function LocComp, as it stands, is rather inefficient: The 
number  of evaluations is  in  the  worst  case  exponential  in  ]X].  Hence its  running  time 
in  practice may  be catastrophic. It,  however, only requires a  strict  minimum of storage 

space: Esentially, the current list of assumption .4 has to be stored, and for each variable 
of  dora(A)  storage space for  the  local variable  p  is  needed  (usually dora(p)  <<  X).  In the local fixpoint approximation schema of section 4, on the other hand,  storage space is 

needed to  store the  complete  search  space.  LocComp only stores information concerning 
the  current  search path  (i.e., for  variables of dora(-4)). 

6  O p t i m i s i n g   f u n c t i o n   LocComp 
In  this section we discuss two optimisations that,  when  combined, remove the exponen- 
tial characteristic of LocComp. The  result is an optimised function  LocComp that  runs in 

time polynomial  (quadratic) in  IX I. 

Exploiting  Lower  Bounds  In function  LocComp(x,`4)  variable v is initialised to 

_I_. This is of course a safe approximation  for [E|(x).  However,  any value that is below 
[E](x)  can  be used  to initialise v. Initialising v to a safe but  somewhat  bigger  value 
than J_ reduces the number  of iteration steps needed for reaching local stabifity. Hence 
introduce an additional  global data-structure  s  : X  -+ r  for keeping track of safe ini- 

tialisation values, i.e. assertion s  E  [E] is maintained  as an invariant. Hence  s  can 
be used, instead of _L, as an initialisation value for v. Keeping  s  C  [E]  invariant is 
easy: Initially /: =  Ax  fi X._L, thereby trivially satisfying s  E  [E]. Upon  return from 

LocComp( x, ,4 ) variable s  is updated by executing the assignment s  := v, where v 
is the value returned by LocComp( x, -4 ). It can easily be proved, exploiting monotocity, 

that v is still safe. Furthermore,  the value of v upon  return is always equal or greater 

than its initialisation value. Hence entries of s  can only increase. 

323 
Reusing I n f o r m a t i o n   Consider again ghe while loop of LocComp. Its goal is to evaluate 

fx  by  computing components (using a  recursive call of  LocComp), one  at  a  time,  until 
enough has been computed for evaluating fx.  The source of inefficiency here is that there 
is not the slightest re-use of information among the computation of different components. 

To remedy this, the specification of LocComp is extended as follows: 

fune  LocComp( x,A, C )  r e t u r n   ( v, A C  ) 
-- precondition: ,4  -,-*  C 
--postcondition:A  -,z  (x/v)  and  A  -,z  AC 

Hence there is an extra parameter C : X  ~+ V, holding additional information (under the 
assumptions ~4). Parameter C can be exploited to speed  up  LocComp by  'pruning' com- 
putations at the level of variables whose fixpoint value is known (under the assumptions 
A).  Furthermore, an  additional component AC : X  -~  V is returned, holding the  newly 
gathered information during execution of LocComp. This information can  then  be  used 

by subsequent calls to  LocComp (having the same assumption list). The computation of 

z3C is based on the following two  additional rules: 

A  -,-*  C~ 
(R6)  A  .,~  c  *  dom(,A) a[~/~]---,  c  x 

Integrating the  above two optimisations yields the following schema: 

Local fizpoint  induction  :  optimised  schema 
globvar  s  :=  ;~x E X.L 
rune  LocComp( x, ,4, C )  r e t u r n   ( v, AC ) is 

if  x  e  do.~(A)  --.  ~  :=  A( x)   z~c  :=~ 

Q x  e  dora(C)  --~   ,  :=  C(x)  ae   :--_, 

otherwise  ----+ 

v  :=  z:(z)  AC:=e 
loop 

p : =   e  Vnew :=  EvalRhs(x,p) 
while  vne~  =  ?  -----+ x'  :=  choose(Arg(f~)  \  dora(p) ) 

(  v t , ~C )  :=  LocColap(  x t, ,A[x/v],  C[AC]  ) 
a c  :=  a c[6 cl 
.  :=  d~'/v']  v.o~  :=  Zv ala h~( ~ , ,)  od 

if vn~  =  v  ---+  exit  loop  fl 

V  :=  ~)new  AC  :=  s endloop 

s  := v  aC  :=  aC[x/v] 

The exponential behaviour has been removed as a result of the above two optimisations. 

More  precisely,  it  can  be  proved  that  for  the  above  optimised  function LocComp the 

324 
number of  evaluations 3 is  in  the  worst  case  O(H.[XI 2 ).  Intuitively this  follows  from 

the  fact  that  E-entries can  only increase, and  computations are  pruned at  the  level of 
variables that are in dora(C) (variables are only removed from dora(C) when assumptions 

of .4 turn out  to be invalid). 
The  price  to  pay  for  this  reduction  in  time  complexity is  an  additional global  data- 
structure  s  and  local structures C/AC.  Note  however that  ClAd  were  merely used  to 
clearly illustrate how information is passed  and reused between different LocComp calls. 

In  a  practical implementation, the latter variables can be  replaced by one single global 
variable gC : X  -r  V,  and a global stack gstck  of variables. 4 Hence the increase in space 

complexity is rather small, and is usually not a problem. However,  for applications where 
storage  space  is  at  premium  (e.g.  model-checking) the  following intermediate method 

offers a good compromise between time and space complexity: Use the optimised schema, 

and,  whenever storage space  is  needed  (for  .4, p)  but  none is  available, then just  clear 
some of the  space  occupied by E, gO, gstek.  (This does  not affect the  correctness of the 

algorithm). If the  erased  information is  needed later on,  it  will simply be  recomputed. 
In this manner the given bounded storage space is optimally exploited, and the running 
time is close to  optimal (w.r.t.  the  bounded memory). 

7  S o m e   F u r t h e r   O p t i m i s a t i o n s  
Several further optimisations are possible. We briefly sketch two of them. 

Collecting  definitive  values  [18]  :  Upon  return  from  LocComp( x, `4, C)  we  have  that 
.4  -.-+  AC.  Hence  if  the  assumptions M  are  correct,  i.e.  if  .4  =  [E]l~om(~),  then 

also AC  =  [E]l~om(~c).  It  is,  however, at  that  point not known whether or  not the  as- 
sumptions .4  are correct. If,  however, in order to  compute AC, none of the  assumptions 

of  .4  have  actually  been  used  (consulted), either  directly or  indirectly via  g,  then  it 
really  does  not  matter  whether  or  not  .4  is  correct.  For  one might  as  well  have  taken [E]ldom(~) 

instead of .4,  and  the  same value for  AC would have been obtained. Hence, 
if  A  has  not  been  used  for  computing  AC,  AC  agrees  with  [E]  on  dora(At).  In  that 
case variables from dom(AC)  have reached their final fixpoint value. An additional data- 

structure :D : X  -r  V  can be  used  to  collect variables that  are  known to  have  reached 
their final fixpoint value, i.e.,/)  =  [E][dom(7) ) is maintained as an invariant./) can then 

be  used  to  prune  computations in  a  way similar to  parameter C.  Hence it  remains to 
detect,  without  too  much overhead,  whether  or  not  AC  actually depends on  `4.  Note 

that AC might have used .4 directly, but also indirectly via C, which itself might depend 
upon .4.  (cfr. precondition of LocComp). A straightforward approach would be to simply 

keep track of the set  used(x)  C dom(Jl) O dora(C) of variables that  have been consulted 
during execution of LocCorap(z, A, t:).  Computing used(x)  is straightforward (note that used(x) 

can be reset to ~ when AC is reinitialised to c). If used(x)  is empty upon return 
from  LocComp(x,`4, C),  then  AC  does  not  (in)directly depend on `4,  and hence can be 

collected by  :D.  The  above sketched  approach  roughly corresponds to  the  collection of 
definitive tuples as discussed in [18]. A  disadvantage of this approach is that  it  requires 

set-manipulations. Below  we  sketch another approach,  requiring less overhead (both in 

3 Only calls to EvalRhs that  return a value different from ? are counted. 4 gC is such that  gC =  C upon entry of LocOomp, and gC =  C[AC] upon exit.  The stack  gstck 

is  used  to  implement the  assignment  AC := e (following the  assignment  v  := vnew), i.e. to restore  gC to  its  entry-value  C. More precisely, upon entry of LocComp variable  x is pushed 

onto gstck  (in  case  x  r  dom(A),x  *  dom(C)); Restoring  gC to  its  entry  value is  done by popping  all variables  on top  of x  and then removing these  variables from dom(gC). 

325 
time  and  space).  The  idea  is  to  use  the  algorithm from  [21] for  computing maximal 
strongly connected components (mscc) of directed graphs. Integrating [21] into LocCorap 
is  not too  difficult, as  both  are  based on depth-first search.  By  slightly modifying [21], 

the  'roots' of the mscc's of the  'dynamic dependency relation' can be detected. If,  upon 
return from  LocComp(x,A,C), it  turns out  that  x  is  the  'root' of such  a  mscc, then  zlC 
does  not  depend  on  A  and  hence  can  be  collected.  The  main cost  for  detecting such 
roots,  is  an  additional global  variable  lowlink  :  X  --+  Nat.  No  used-sets  are  needed. 
Informally, the  reason that  we can do without the  used(z)  sets, is that  we only have to 
check for  emptyness of  used(x).  By  exploiting depth-first characteristics, this  test  can 

be implemented using lowlink,  the  set  used(x)  itself is not  needed to  check emptyness 
of used(x).  (This is, btw, the reason why the algorithm in [21] is linear in the size of the 
graph,  instead of quadratic). 

Chasing  dependencies  : When v is updated by v :=  v=~,  variable AC is reset to  ~. This 
is  done in  order  to  restore invariance of the  assertion A[x/v]  --~  AC,  which  risks to 

be  violated as  a  result of updating v.  If,  however, the  old  value of v has  not been used 
in  order  to  compute  AC,  then  AC  need  not  be  reinitiaiised but  can  stay  as  is.  If  the 
old  value of  v  has  been used,  then  AC  is  re-initialised. Alternatively, one could  try  to 
find out  (using e.g.  the transitive closure of a  semantic dependency graph  as in  [16]) by 

which fragment of  AC the old v-value has been used.  Only that fragment then has to be 

reinitialised. Space  limitations prevent us  from  discussing these  optimisations in  more 
detail  (see  [24] for  more details).  Just  note that  such  optimisations do  not  touch  upon 
the  worst  case  behaviour of the  algorithm: The  number of  evaluations remains, in  the 

worst case,  quadratic in  IX].  They may however decrease the  running time in practice. 

8  A p p li c a ti o n   :  P r e o r d e r   C h e ck i n g  
In this section we discuss one sample application for the generic algorithms discussed in 

previous sections. The application domain is the automatic verification of finite transition 
systems (finite-state machines). Verification is the process of comparing a  transition sys- 

tem  with  a  given system specification. One promising approach is  equivalence/preorder checking. 

(Another complementary approach  is  model-checking). In  this  approach  the 
system specification is also given under the form of second transition system. Verification 

then boils down to comparing two transition systems w.r.t,  a given equivalence/preorder. 
As  an example of a  preorder relation we  will consider the  prebisimulation  preovder  [4], 

noted by  4. 

Fix a finite transition system (7 ~, Act, --+), where 79 is the finite set of processes (states), Act 

is the finite set  of actions, and --+C_ P  *  ,Act *  79 is the transition relation. We shall 
write p  -% q instead of  (p, a, q)  E--+. Informally, p  -% q means that,  when is state  p,  the 
system may move to state  q by performing an action a. 

Definition  [4].  Let  Tc_  P  be  a  set  of  divergent  states.  Define  ~  (relative  to  T)  as 
the  largest  relation  R  C_ P  x  79 satisfying the  following condition: Whenever p R q  and a E Act 

then 

(i)  Vp'E79:  p ~ p '  

(ii)  If p  gT  then: 

(a)  q CT, and (b)  Vq' ~  P: 

implies  ( 3 q ' E P :   q - ~ q ' a n d p ' R q ' )  

q ~.  q'  implies  (  3p' E P:  p-% p'  and g  Rq'  ) 

326 
Many  other interesting behavioural relations can  be obtained as special cases of ~  (see 
[4],  e.g.  by  taking  t =   0  strong  bisimuhtion  equivalence  [19] is  obtained.)  The  pre- 
order  6  is defined as  the largest relation satisfying a  number  of conditions. These con- 

ditions  can  be  rephrased  in  terms  of  a  monotone  boolean  equation  system.  I.e.,  take V  =  {false,  true}, 

and  let  ~  be  defined by true  r-  false. 5  For the set  of left  hand  side variables take: 

X - ~ { X p , q l p E V ,   q E 9   }  U  {y p , ql p E g ,   q E g }  

u  {za,p,,q  I a  E  Act, p'  E  9, q E IP  such  that  p  ~  p' for  some p  E 9} 

For each  variable x  E  X  there is  one equation  in  E  having  x  as its  left  hand  side. The 
equations  of E  are of the  following general form: 

Xp,q=yp,q  A  [ p e t   ::~  ( q r   Ayq,p)] 

Yp,o =  A~,f  i p_~p,  z~,p,,q Za,pS,q ~  Vqs ] q.~q,  Xp~,q, 

The  following relationship holds between  g  and  the  equation  system E  defined above: 

Vp, q  E  9:  p  ~  q  r162  [El(zp,q)  =  true 
Hence checking whether  p  ~  q  is reduced  to  computing  the  least  component  [E](xv,q). 
Furthermore a simple calculation shows that  IEI  =  o (   191.1  ~  I). Hence using the global 
algorithms  of section  3,  [El  can  be  computed  using  at  most  O(IEI)  =  o (   191.1 -~  I) right hand side evaluations. The best algorithm [4] for computing g  known today, runs in 

time O( I --+ I 2 ).  Note  however that  the  latter complexity result includes the  evaluation 
time of right hand sides. If we would take this time into account, our result O( 191.1  --,  I ) 

would also become O( I --+ 12 ). Hence for this particular case, our generic global algorithm 
matches the so far best known  'ad hoc' constructed global algorithm. What we also have 

are local algorithms for checking 4,  running in  time O( I --+ 12 ). 

9  C o n c l u s i o n s  
In  his paper we  have discussed the  systematic stepwise construction of efficient generic global and local fixpoint algorithms based on fixpoint approximation and fixpoint induc- 

tion  techniques.  The  proposed  algorithms can  be  instantiated for  a  concrete  equation system  by  providing  a  concrete  implementation  for  the  evaluation  function  ~.valRhs. 

Most  of the  concepts used  (e.g.  semantic dependencies, local iteration strategies, etc.) 
are not  new in  se, but  have  already been discussed by various researchers in  various ap- 
phcation  areas.  However, putting these ideas into the more abstract and  general setting 

of  monotone  equation  systems  is  new.  The  only  other  generic fixpoint  algorithm  that 
we are  aware of is  [17],  where  a  (more) general top-down fixpoint algorithm is proposed which only assumes that  the transformation is given by an effective procedure satisfying 

some  weak  (monotonicity) properties. The  algorithm in  [17] seems to  be similar to  our 
local fixpoint  induction  algorithm  combined  with  semantic  dependencies for  removing 

redundant  computations. 

For the  global/local algorithms based  on  fixpoint  approximation, the  number  of evalu- 
ations for  z  only depends upon  the right  hand  side of x,  and is independent of the rest of the  program. This  property does not  hold  for the  local algorithms based  on fixpoint 

s  We take true  F" false  as ~  is defined as fi9 largest fixpoint, whereas we compute least fixpoints. 

327 
induction, where a  variable x  is evaluated at  most Ii.X times. 

The  fixpoint approximation schema's are  well-suited to  be  implemented using parallel 

architectures. Imagine for example n  processors, all selecting and evaluating variables in 
parallel. Because of the syntactic/semantic dependencies, the processing of a  variable x 
only  affects the  immediate 'neighbourhood' of x.  Hence little interference between the 

processors is to be expected. Such a  parallel implementation might potentially substan- 
tially  reduce the  running in  practice.  The  fixpoint induction algorithms, on  the  other 
hand,  cannot  benefit from  parallel  execution, as  they  are  based  on  depth-first  search 
which is inherently sequential. 

Sparseness seems to be  a necessary condition for successfully applying local algorithms. 
For if the  equations system E  is not sufficiently sparse,  then a  local  algorithm will end 
up  computing (almost)  the  complete least  solution [El,  which  is  precisely what  a  lo- 
cal  algorithm attempts  to  avoid.  Often  a  sufficiently high  degree of sparseness can  be 
obtained by first  transforming E  using some  form of  'quotienting' [1,22] operation. To 
conclude, let  us illustrate this  important point  by  means of a  simple example  (rooted 
in  model-checking). Consider a  transition system (P,.Act,---+).  Take V =  2 p,  and for  E 

take ordinary set-inclusion C.  Let  E  be  the following equation system: 

E  -  {  x  =  ;re(al,~)  U  ;re(as,  ~') 
where  al  and  a2  are  two  actions from  .Act,  and pred  is  the  predecessor function, i.e., pred( a, Q ) 

denotes the set of states from which a state of Q can be reached by performing 
an a-action, i.e.,  pred( a, Q )  =  {p E P[p  -5, p'  for some p'  E  Q }.  (Note: [E](x)  is the set 
of states from which there is a path consisting of a finite number of al-action eventually 
followed  by  a  a2-action.)  Suppose  that  we  are  given  a  state  ~  (e.g.  the  intial system 

state),  and  we  want  to  check  whether  ~  E  [El(z).  Instead  of  computing [El(z)  and 
then  checking whether ~  is  in  [E](x),  we  first  construct a  'quotient' boolean equation 
system E'  as follows:  Left  hand side variables of E ~ are of the  form xp  with p  E :P. The 

equations of E I are of the form 

E'  =  {  xp  =  (  Vp,~.  I:,,~,,'  ~''  )  v  (  V,,,e~  I,,~,,'  t,-ue  ) 

(There  is  one  such equation for  every p  E  7)).  E ~ is equivalent to  E  in  the  sense that 
p  e  [E](x)  iff  [E'](xv)  =  true.  (Note  that  this  time  we  take  false  F-  true).  As  E' 

is  sparse,  it  makes sense  to  apply  local  algorithms in  order  to  compute  [E'](xp).  The 

above sketched  technique can  be  generalised to  e.g.  equation systems used  in  abstract 
interpretation. There it is often the case that  t2 is of the form  [D -~  R]  (in our example V  =  [P  --~  {false, true}]). 

And  one  is  often  interested,  not  in  [E](~),  but  in  [E](~) 

applied  to  some  d  E  D.  This  situation occurs  e.g.  when  taking into  account  the  way 
predicates are called (call patterns) in the top-down semantics. Instead of first computing 

[E](~)  and  then  computing ([E](~))(~),  an  equivalent 'quotient' equation system  E ~ 
can  first  be  constructed (at  least  conceptually). As  E'  often  exhibits  a  high  degree  of 

sparseness, a local algorithm seems the  proper choice for computing [E'](~d). 

R e f e r e n c e s  

1.  Andersen, H.  R.:  Model  Checking  and  Boolean  Graphs,  ESOP'92,  LNCS  582,  1992 
2.  Clarke, E.M.,  Emerson, E.A.,  Sistla, A.P.:  Automatic  verification  of finite-state  con- current  systems  using  temporal  logic  specifications, 

ACM  Transactions on  Progr. 
Languages and Systems, Vol.8,  No.  2,  pp.  244-263,  April 1986 

328 
3.  Cleaveland,  R.:  Tableau-Based  Model  Checking in  the  Propositional  Mu-Calculus, Acta Informatica, 1990 
4.  Cleaveland, R.,  Steffen, B.:  Computing  Behavioural  Relations,  Logically, ICALP  91, pp.  127-138,  LNCS  510 

5.  Cleaveland, R., Steffen, B.: A Linear- Time Model Checking Algorithm for the Alternation- Free Modal  Mu-Calculus,  CAV'91,  LNCS  575,  1991 

6.  Cousot,  P.,  Cousot,  R.:  Abstract  interpretation:  a  unified  lattice model for  static analysis  of programs  by construction  or  approximation  of fixpoints,  POPL,  1977. 

7.  Debray,  S.  K.:  Static  inference  of  modes  and  data  dependencies  in  logic programs, TOPLAS,  11  (3),  1989. 
8.  Emerson, E.A.,  Lei, C.-L.:  Efficient  model  checking in fragments  of the propositional #-calculus,  LICS, 267-278,  1986 

9.  Jones,  N.D.,  Mycroft, A.:  Data flow  analysis  of applicative  programs  using  minimal function  graphs, POPL,  1986. 
10.  Jorgensen, N.:  Chaotic fixpoint  iteration  guided by dynamic dependency, in Workshop on  Static Analysis  (WSA'93),  LNCS. 
11.  O'Keefe, R.A.:  Finite fixed-point  problems,  ICTL  1987. 12.  Kildall,  G.A.:  A  unified  approach to  global program  optimization,  POPL  1973. 

13.  Kozen, D.:  Results  on  the propositional  #-calculus,  TCS,  27,  1983. 14.  Larsen, K.G.:  Proof systems  for  Hennessy-Milner  logic with  recursion,  CAAP,  1988, 

see also TCS,  72,  1990 15.  Larsen,  K.G.:  Efficient  local correctness  checking, CAV'92,  Forthcoming 
16.  Le Charlier,  B.,  Van Hentenryck, P.:  Experimental  Evaluation  of a Generic  Abstract Interpretation  Algorithm for  PROLOG,  TOPLAS,  Vol. 16,  nr.  1,  January  1994. 
17.  Le Charlier,  B.,  Van Hentenryck, P.:  A  universal  top-down fixpoint  algorithm, Tech- nical  Report  92-22,  Institute  of Computer Science,  University of Namur,  Belgium, 

April  1992. 18.  Le Charlier,  B.,  Degimbe,  O.,  Michel,  L.,  Van  Hentenryck,  P.:  Optimization  Tech- 

niques for General Purpose  Fixpoint Algorithms:  Practical Efficiency  for the Abstract Interpretation  of Prolog, in Workshop on Static  Analysis (WSA'93),  LNCS. 
19.  Milner,  R.:  Communication  and  Concurrency,  Prentice-Hall International,  1989. 20.  Stifling,  C.,  Walker, D.:  Local model  checking in  the  modal  ran-calculus, TCS,  Octo- 

ber  1991, see also LNCS  351,369-383,  CAAP  1989 21.  Tarjan,  R.E.:  Depth  first  search and  linear  graph algorithms,  SIAM  J.  Comput.,  1 

(2),  1972. 22.  Vergauwen,  B.,  Lewi,  J.:  A  linear  algorithm  for  solving  fixed  points  equations on 

transition  systems,  CAAP'92,  LNCS  581,322-341 23.  Vergauwen, B.,  Lewi,  J.:  Efficient  Local Correctness  Checking for  Single  and Alter- 
nating Blocks,  ICALP'94,  LNCS 820. 24.  Vergauwen,  B.:  Verification  of  Temporal  Properties  of  Concurrent  Systems,  Ph.D. 
thesis, in  preparation. 25.  Winskel,  G.:  A  note  on  model  checking  the  modal  v-calculus,  ICALP,  LNCS  372, 

1989, see also TCS  83,  1991 26.  Wauman,  J.:  Ontwerp  en  implementatie  van  lokale verificatie-algoritmen  voor  re- 

actieve  systemen  met  behulp  van  temporele  logic,  Ms.  thesis,  Dept.  of  Computer Science,  K.U.Leuven,  1992-1993. 