

Program Derivation With Verified

Transformations -- A Case Study

J. P. KELLER

Kepler, Paris

AND
R. PAIGE
Courant Institute

Abstract
A program development methodology based on verified program transformations is described and illustrated through derivations of a high level bisimulation algorithm and an improved minimum-state DFA algorithm. Certain doubts that were raised about the correctness
of an initial paper-and-pencil derivation of the DFA minimization algorithm were laid to rest by
machine-checked formal proofs of the most difficult derivational steps. Although the protracted
labor involved in designing and checking these proofs was almost overwhelming, the expense
was somewhat offset by a successful reuse of major portions of these proofs. In particular, the
DFA minimization algorithm is obtained by specializing and then extending the last step in
the derivation of the high level bisimulation algorithm.

Our experience suggests that a major focus of future research should be aimed towards
improving the technology of machine checkable proofs -- their construction, presentation,
and reuse. This paper demonstrates the importance of such a technology to the verification of
programs and program transformations. We believe that the utility of transformational systems
to program development will ultimately rest on a practical program correctness technology.

c
fl

1995 John Wiley & Sons, Inc.

1. Introduction
This paper describes a transformational programming methodology that includes a fully operational set-theoretic proof checker developed by the first author. The methodology is explained through illustrative examples of two moderately difficult program derivations. One of these derives a high level form
of an algorithm to compute the bisimulation equivalence relation (see [51] and
[60]) (which is used to model CCS program equivalence, [50], and also hyperset
equivalence, [54]). The other derives an algorithm to minimize the number of
states in a deterministic finite state automaton (DFA); see [52], [53], and [2].
Based on our experience with these derivations, we believe that mechanical verification of program transformations is the most important missing ingredient to
the successful use of transformational programming as part of a viable program
development technology.

1.1. Origins in SETL
The philosophical underpinnings of our work draw on two separate but related
research directions pursued by Jack Schwartz and his collaborators within the

Communications on Pure and Applied Mathematics, Vol. XLVIII, 1-62 (1995)

cfl

1995 John Wiley & Sons, Inc. CCC 0010-3640/95/??0001-62

2 J. P. KELLER AND R. PAIGE
SETL project, which began almost thirty years ago. One direction had to do with
the mechanization of a fully formal programming methodology powerful enough
to derive the kind of combinatorial algorithms informally developed by Dijkstra
in the second part of Discipline of Programming; see [27]. The other direction
had to do with the extension of global program analysis techniques found in
Cocke and Schwartz (see [18]) to support the compilation and optimization of a
very high level programming language based on finite set theory.

The SETL language was central to both directions as a vehicle for demonstrating that set theory, which was accepted by the mathematics community as
the lingua franca of mathematical abstraction, could support a fundamentally
new, more productive and reliable software development technology. Schwartz
proposed SETL (see [70]) as a language that would cost more to implement but,
he believed, would make programming easier by providing a fixed but powerful
collection of set-theoretic primitives that could model data structures and specify
algorithms clearly and succinctly with a minimal amount of language extension.
Development of the first validated Ada compiler (see [1]) implemented in SETL
supported this view.

In order to place our own work in perspective, it is worth mentioning a few
specific research contributions that came out of the SETL project. Schwartz was
one of the earliest proponents of a comprehensive fully formal computer-aided
system supporting program development by verified source program transformations based on a computational set theory. The programming language was
SETL. The formal language and verification technology was based on Hoare logic
(see [43]) with formulas consisting of SETL code fragments annotated with settheoretic assertions; see [73]. Convenient algebraic rules for combining correct
program fragments into larger correct fragments were a key feature. Davis and
Schwartz developed a formal way to enable a mechanical verification system to
extend itself with new proof methods without violating soundness and without
changing the set of provable statements; see [24]. Edith Schonberg later extended
Davis and Schwartz's verification framework to support the computer-assisted
verification and application of program transformations; see [25]. She presented
one of the most difficult transformational proofs in the literature -- a formal
derivation of Tarjan's graph reducibility algorithm (see [84]) -- as a convincing
case study. This early work inspired Cantone, Ferro, and Omodeo's work on
fast decision procedures for fragments of set theory; see [13]. Early research on
the generic program transformation method of set-theoretic finite differencing
(an abstract mechanizable form of top-down stepwise refinement that was implemented by a generalized form of strength reduction, [18] and [17]) is found in
[59], [58], and [75].

Implementation of an optimizing compiler was, of course, the greatest challenge of the SETL project, which spanned a period from the early 1970's through
the 1980's. The two major goals for compiling SETL programs into efficient code
were (1) to automatically select conventional storage structures for sets and maps
and (2) to implement mathematical copy-value semantics without having to perform expensive copy operations. To achieve these goals, Schwartz and others in

PROGRAM DERIVATION 3
the SETL project extended the methods of global program analysis discussed
in his earlier work with Cocke (see [18]) to handle three new analysis problems:
type inference, [86], analysis for set-theoretic inclusion and membership relations, [71], and analysis for multiple pointers to the same aggregate data, [72]
and [69].

Much of the work on value flow and destructive update analysis is just now
beginning to be digested by the functional programming and rapid prototyping communities. Tenenbaum's pioneering work in SETL type inference was
especially successful, and has had wide influence. In particular, it had an impact on the European SED Esprit project directed by the first author. Within
this project a programming environment for SETL, [30], was constructed within
MENTOR, [31], using TYPOL, [26] and [45]; it implemented SETL type inference, [32], to facilitate SETL-to-Ada translation, [29].

Because none of the proposals for mechanizing a set-theoretic programming
and verification methodology was actually implemented, the ideas were never put
to a test. Although several unoptimized forms of SETL have been implemented
(e.g., SETL2, [80], and CANTOR, [47], which was used to develop the NAP
proof checker discussed in this paper), the optimizer was never fully operational
(see [36], [76], and [82]), and some of the most basic theoretical ideas for SETL
analysis and optimization were also never fully tested. In addition to a wealth
of research results, the SETL project has left many interesting problems open
for future investigation.

1.2. Transformational Programming
A program transformation is a mapping on well formed programs that preserves semantics. Transformational programming is a program development
methodology whose origins can be traced back to Dijkstra's top-down stepwise
refinement; see [28]. The basic idea is to map an initial formal problem specifi-
cation into an implementation by successive applications of meaning-preserving
transformations. A set of proven lemmas that each applied transformation preserves program semantics comprises a formal derivation that the final program
implements the specification. The use of Hoare logic (see [43]) to prove that the
specification satisfies input/output conditions may be added to the derivation
to prove more convincingly that the implementation is "correct." When each
abstract program transformed in this way can be expressed in a single language
from the highest level specification down to the lowest level implementation, then
Bauer called that language "wide spectrum"; see [3]. SETL was in this category.
These rudimentary ideas evolved into a powerful programming methodology by
Dijkstra, [27], and Gries, [40].

The top-down stepwise refinement approach promised to provide a more disciplined and goal-directed approach to program design in which derivational steps
could be guided by complexity considerations to produce efficient programs. A
top-down program derivation was a byproduct of construction that lent credibility to the proven code. It also offered to document deep semantic information
about that code, which could facilitate redesign and maintenance. Key questions

4 J. P. KELLER AND R. PAIGE
remained about how hard it would be to design and prove transformations, and
to recognize when they could be usefully applied.

However, any manual activity is prone to human factor difficulties. Key
weaknesses of any purely programming methodology include the inconvenience
of having to conform to a rigid discipline, the potential for human error in paperand-pencil calculation, and the tedious, sometimes repetitive, low level, manual
steps that are required. The point of a transformational methodology is to
provide enough mechanization to make formal program development useful.

Two research directions have been taken towards the development of a useful
semi-automatic transformational methodology. In one direction, exemplified by
Burstall and Darlington (see [10]), the goal was to have a small, but complete (in
the sense that any two equivalent programs can be transformed into each other)
collection of program transformations. However, the difficulty of understanding
how to verify Burstall and Darlington's Fold/Unfold transformations impeded
mechanization. The total correctness of these transformations in the context of
higher order functional languages was open for thirty years until Sands finally
came up with a solution; see [68].

In the other approach, the goal was to have a large collection of easily mechanizable transformations specified as conditional rewriting rules; i.e., rewriting
rules guaranteed to preserve correctness if conditions (limited to syntactically
checkable program properties that implied semantic correctness) could be automatically verified. Cheatham and Wegbreit (see [15]) proposed this approach for
use in the construction of experimental prototype optimizing compilers. Standish et al. later proposed a catalog of transformations of this kind; see [81].
Transformational systems developed with this approach made use of large transformational libraries of relatively low level transformations whose enabling predicates were proved by ad hoc simplifiers that relied mainly on local contextual
information. These systems could group and sequence transformations to form
simple automatic translators.

Transformational systems today, such as Kids, [79], and APTS, [57], are metasystems in the sense of accepting formal syntax specifications. They also support
global program analysis using attribute grammars or more sophisticated logicbased methods, and they incorporate conditional rewriting rules with conditions
that are proved true based on the results of such global analysis. These systems
facilitate the construction of powerful prototype systems for symbolic program
analysis, manipulation, and translation.

However, these systems do not ensure soundness. Lacking from them (and
sorely missed) is the mechanical support for verifying the correctness of transformations. The need to create confidence in the products of transformational
systems rests upon preventing them from being used to derive invalid results.
The need for machine checked verification of transformations has been expressed
by Pepper in [62]. It is one of the most important and difficult goals of the Korso project (see [63]) and its KIV verification environment; see [64]. Currently it
is also recognized to be highly labor intensive, and perhaps impractical for the
most difficult theorems; see [66].

PROGRAM DERIVATION 5
1.3. Motivation
Our choice of derivation came from two surprising experiences. We derived
an improvement in Hopcroft's algorithm to compute a minimum state DFA (see
[44]), which runs in asymptotically better space and matching worst case time.
The most difficult step in the derivation involved application of a transformation
whose correctness depended on a simply stated set-theoretic identity. A seemingly correct proof of this identity was worked out in mathematical style with
paper and pencil. However, when the identity was then assigned to an honors
graduate class in compilers at New York University, no student could prove it,
and several students believed it was wrong. We found no error in the proof, but
our confidence was shaken. Consequently, a formal mechanical proof was attempted using the NAP set-theoretic proof checker. After a great deal of work,
there was an impasse, and a gap seemed to exist in the proof. We worked some
more, filled the gap, and eventually obtained a mechanically checked proof.

The second surprise was that the Paige/Tarjan strong bisimulation algorithm
was published with an error in the preprocessing phase; see [60]. The algorithm
had been designed informally and without concern for an implementation, as
is the style within the algorithms community. The error was minor and easily
fixable, but we found out about it from scientists (outside the algorithms community) who wanted to implement it, and had difficulty finding a fix. We believe
that this particular error would be almost impossible with a paper-and-pencil
transformational derivation. Yet the error was introduced in the same portion
of code obtained from application of a variant of the critical transformation that
we proved for the DFA minimization derivation.

2. Mathematical Preliminaries
Our derivations will fit in the framework of Bloom and Paige (see [7]), where
a paper-and-pencil transformational proof was given for a new improved ready
simulation algorithm. That derivation made essential use of a SETL-like programming language, three basic transformations to map a fixed point specification into RAM code, and Hoare logic combined with set theory to verify transformations. However, the derivations to be presented here are somewhat more
complex. More importantly, they make essential use of a new mechanical proof
checking capability found in NAP (a set-theoretic proof checker designed and
implemented by the first author) for verifying the most difficult transformations
in our derivations.

2.1. Notation
The SETL-like notation (see [70], [74], and [80]) to be used throughout this paper is a concise variant of standard mathematical notation augmented with conventional dictions as may be found in an imperative programming language like
Algol. Besides the usual elementary datatypes, SETL includes built-in datatypes
for finite tuples (i.e., ordered collections of data) and finite sets. The elements
of tuples and sets may be any datatype to an arbitrary finite depth of nesting.

6 J. P. KELLER AND R. PAIGE
Examples are the empty tuple (written [ ]), the empty set (written fg), and the
3-tuple

\Theta 

fg; [true; [1]; 2]; "abs"

\Lambda 

(whose first component is fg).

Set comprehension is, perhaps, the most useful expression in SETL. In its
most general form it is written in the following way,

fe(x

1

; . . .; x

k

) : x

1

2 S

1

; x

2

2 S

2

(x

1

); . . . ; x

k

2 S

k

(x

1

; . . .; x

k\Gamma 1

)jK(x

1

; . . . ; x

k

)g

which denotes the set of element values e(x

1

; . . . ; x

k

) evaluated over all k-tuples

[x

1

; . . . ; x

k

] belonging to the search space satisfying the condition,

x

1

2 S

1

^ x

2

2 S

2

(x

1

) ^ . . . ^ x

k

2 S

k

(x

1

; . . .; x

k\Gamma 1

)

and also satisfying the Boolean-valued qualifier K(x

1

; . . . ; x

k

). Each variable x

i

,

i = 1; . . . ; k, that occurs in the iterator subpart (the text between symbols `:'
and `j', which defines the search space) is bound to the set-valued expression
S

i

(x

1

; . . . ; x

i\Gamma 1

). No applied occurrence of a bound variable of a set former can

appear before its bound occurrence within the iterator, and all bound variables
of a set former must be distinct.

When the qualifier subpart K(x

1

; . . . ; x

k

) is the constant true, it may be

elided. As a notational convenience, the frequently used set former

fx : x 2 SjK(x)g
may be abbreviated

fx 2 SjK(x)g

The bound variable convention of SETL set formers simplifies identification of
bound and free variables. It also allows us to avoid writing existential quantifiers
that would be required in conventional notation. For example, the union of two
sets S and T may be expressed

fy : x 2 fS; T g; y 2 xg
rather than

fy : 9x 2 fS; T gjy 2 xg

SETL maps are finite binary relations treated as multi-valued functions that
are defined as sets of ordered pairs. Let R and R

0

be maps, and let S be a set.

Then:

[=S =

[

x2S

x

domain R = fx : [x; y] 2 Rg

range R = fy : [x; y] 2 Rg

R

\Gamma 1

= f[y; x] : [x; y] 2 Rg

Rfxg = fy : [u; y] 2 Rju = xg

R[S] = fy : [x; y] 2 Rjx 2 Sg =

[

x2S

Rfxg

PROGRAM DERIVATION 7
The preceding notation illustrates how expression [x; y] can be used in a natural
way within iterator [x; y] 2 R to bind variables x and y to the first and second
components of pairs that belong to set R.

If S is a set, then S with x and S less x are convenient abbreviations for
S [ fxg and S \Gamma  fxg, respectively. The powerset of S is denoted by pow(S). The
cardinality of S is denoted by #S. An arbitrary element can be selected from a
set S by the expression 3 S.

SETL is an imperative language with assignment a := b under copy/value semantics. If S is a set and op is any binary operator, then SETL allows assignment
S op := x to abbreviate S := S op x.

Image sets of maps can be updated by performing assignments of the following
two kinds:

Rfxg with:= y --element addition
Rfxg less:= y --element deletion

or, equivalently, by writing,
R with:= [x; y]
R less:= [x; y]

When R is a many-to-one relation, we use the conventional notation for function application R(a) to denote the unique b such that [a; b] 2 R. R(a) is
undefined otherwise. In assignments,

R(a) := b
means
R :=

\Phi 

[a

0

; b

0

] 2 Rja

0

6= a

\Psi 

with[a; b]

which preserves the single-valued nature of R. SETL constant om denotes a
special undefined value, and can be used to remove an element from the domain
of a map, as in the example

R(a) := om

Finally, it is useful to follow the SETL (see [80] and [74]) convention of allowing Boolean-valued bounded existential quantifiers 9x 2 SjK to have a side
effect assignment. Whenever the quantifier evaluates to true, then variable x is
assigned an arbitrary value that belongs to S and to the truth set of qualifier K.
This convention allows us to use such existential quantifiers as the condition of
a while-loop whose body can refer to x in order to retrieve the truth set value
newly chosen before each while-loop iteration.

SETL contains a special looping construct,

for x 2 S loop

!a sequence of statements goes here?
end loop

8 J. P. KELLER AND R. PAIGE
which searches through a set S without repetition, and assigns successive values
of S to x, which can then be used within the body of the loop.

SETL's built-in set datatype supports two distinctive and powerful features
-- nondeterminism and associative access -- which makes SETL programming
easier than many other languages, even when SETL is restricted to its most primitive set operations. Both the for-loop and arbitrary selection support nondeterministic programming, which relieves the programmer from having to choose an
implementation for a particular search strategy. SETL's membership test x 2 S
and map application f(x) are primitive associative access operations that test
whether a value belongs to a set. In contrast to conventional languages, which
encourage the storage and retrieval of data indirectly through location expressions (pointers or array indexes), SETL encourages the direct use of essential
data to access and navigate through other related data. Navigational paths are
consequently short; programs are concrete and readable. A single SETL primitive for element deletion might require a dozen statements in C.

2.2. Derivational Tools
Before proceeding with the derivation, we first need to describe three generic
transformational tools that can be used to map an abstract problem specification
into efficient RAM code.

Dominated Convergence
A partially ordered set (X;

^

\Gamma 

) is a set X with a reflexive, anti-symmetric,

transitive relation

^

\Gamma 

. It has the descending chain condition if every totally

ordered subset of X that includes a maximum element is finite. Given a function
f : X ! X, we define f

0

(x) = x and f

n+1

(x) = f(f

n

(x)), and use the term

gfp x:f(x) to denote the greatest fixed point of f. Function f is said to be
deflationary if f(a)

^

\Gamma 

a for every a 2 X. f is monotone if f(a)

^

\Gamma 

f(b) for every

a; b 2 X such that a

^

\Gamma 

b.

We will use the fixed point theorem and dominated convergence argument
just below as tools for proving correctness of a formal specification, and for
taking the initial step in the algorithm derivation.

DEFINITION 2.1. A semilattice is a structure (L; u; 1), where L is a set, u is
a binary meet (i.e., greatest lower bound) operation, and 1 is the greatest element
in the partial order induced by u: x

^

\Gamma 

y if and only if x = x u y.

THEOREM 2.2. (Tarski -- [85]) Given a semilattice (L; u; 1) which has
the descending chain condition, and given a monotone function f : L ! L, then

(max Y jY

^

\Gamma 

f(Y )) = gfp Y:f(Y ) = f

i

(1)

for some finite i.

PROGRAM DERIVATION 9
THEOREM 2.3. (Dominated Convergence -- [12]) Under the same conditions as in Theorem 2.2, and given any sequence s

i

such that

1. s

0

= 1

2. s

i+1

= s

i

if s

i

= f(s

i

)

3. s

i

? s

i+1

*

\Gamma 

f(s

i

) otherwise.

then s

i

converges to s

k

= gfp Y:f(Y ) for some finite k.

The idea of dominated convergence is to compute a sequence [s

i

: i = 0; 1; . . .]

that converges to the same fixed point as the Tarski sequence

\Theta 

f

i

(1) : i = 0; 1; . . .

\Lambda 

,

and satisfies s

i

*

\Gamma 

f

i

(1) for i = 0; 1; . . .. We seek a dominated sequence that

takes more steps to reach the fixed point but at less overall cost than the Tarski
sequence. Dominated convergence can be used to compute fixed points over
a variety of lattice-theoretically defined datatypes. In [7] the ready simulation
problem was solved by using dominated convergence to compute the fixed point
of a set-valued function defined on a powerset lattice.

If S is a finite nonempty set, then P is a partition of S if and only if P is a
set of nonempty mutually disjoint subsets of S whose union is all of S. The two
problems considered in this paper can be specified as fixed points of functions
defined on partitions. Elements of a partition are sometimes called blocks. Let
P(S) denote the set of all partitions over S. If Q; P 2 P(S), then we say that Q is
a refinement of P , denoted by Q

^

\Gamma 

P , if and only if 8q 2 Qj(9p 2 P jq ` p). The

partially ordered set (P(S);

^

\Gamma 

) has maximum element fSg and minimum element

ffxg : x 2 Sg, and, being finite, it must have the descending chain condition.

Consider the following basic operation on partitions:

DEFINITION 2.4. If P is a partition of a finite set S, and a ` S (i.e.,
a 2 pow(S)): Then

P split a

def

= fs 2 pow(S)js 6= fg ^ (9p 2 P js 2 fp " a; p \Gamma  ag)g

Basic properties of split, which follow directly from the definition, are summarized below.

PROPOSITION 2.5. Let P and Q be two partitions of finite set S, and let
a; b ` S.

1. P split a

^

\Gamma 

P (split is deflationary in its first argument)

2. if P

^

\Gamma 

Q, then P split a

^

\Gamma 

Q split a (split is monotone in its first argument)

3. P split a = P if and only if 9A ` P ja = [=A (splitting a partition has no

effect if and only if one splits by any union of elements of P)

4. P split a = P if and only if 8p 2 P jp ` a . p " a = fg

10 J. P. KELLER AND R. PAIGE

5. a = [=fx 2 P split ajx ` ag(Set a is exactly covered by a subset ofP split a)
6. If p 2 P , a ` p, a 6= p, and a 6= fg, then P split a = (P \Gamma  fpg) [ fa; p \Gamma  ag

(Splitting P by a set contained within some block p 2 P is a local change
in which p is replaced by a and p \Gamma  a.)

7. If A ` P , A 6= fg, a = [=A, b ae a, and b 6= a, then P split b = (P \Gamma  A) [

A split b (Splitting by a small set affects only the blocks that intersect that
set.)

8. (P split a) split b = (P split b) split a (The result of splitting with respect to

more than one set is the same regardless of the splitting order.)

Based on the last property of Proposition 2.5, we can form a collective split
operation that splits a partition relative to a collection of sets. It is also useful
to treat such collections formally, as was done for partitions. If A and B are sets
of subsets of a finite set S, then B v A if and only if 8a 2 A; 9D ` Bj

S

=D = a.

The space (pow(pow(S)); v) forms a partially ordered set with maximum element
fg and minimum element ffxg : x 2 Sg.

DEFINITION 2.6. Let P be a partition over a finite set S, let A be a collection
of subsets of S, and let a be any subset of S. Then we have the following recursive
function definition.

P split% fg = P
P split% A with a = (P split a) split% A

Properties of split% that follow directly from its definition are summarized
below.

PROPOSITION 2.7. Let Q and P be partitions over a finite nonempty set S,
and let A, B, and R be subsets of pow(S).

1. If Q

^

\Gamma 

P and A v B, then Q split% A

^

\Gamma 

P split% B. (split% is monotone

in both arguments.)

2. P split% B

^

\Gamma 

P . (split% is deflationary in its first argument.)

3. P split% A = P if and only if 8p 2 P; 8a 2 Ajp ` a . p " a = fg (A is a

right identity)

4. P split% (A [ B) = (P split% A) split% B
5. P = fSg split% P
6. P split% P = P (idempotence).
7. P split% Q = Q split% P (commutativity).
8. (P split% Q) split% R = P split% (Q split% R) (associativity).

PROGRAM DERIVATION 11
9. If a ` S, P split a = P , R partitions a, and b 2 R, then P split% (R less b)

= P split% R (A crucial property used in the partitioning algorithms of
Cardon and Crochemore; see [14]):

10. The meet operation on partitions is defined by QuP

def

= Q split% P .

A partition P is said to be stable with respect to set a if and only if the
conditions of Proposition 2.5 (4) hold. P is stable relative to a collection A of
sets if the conditions of Proposition 2.7 (3) hold. If S is a finite nonempty set,
then (L = P(S); split%; fSg) forms a semilattice by Proposition 2.7 (10). Since
L has a descending chain condition, we can exploit the Dominated Convergence
Theorem to compute fixed points of monotone functions defined on L.

COROLLARY 2.8. Let S be a finite set with L = P(S), and let f : L ! L be
a monotone, computable function. Then gfp Y.f(Y) can be computed in O(#S)
iterations by the following code:

-- maintain invariant f(Y )

^

\Gamma 

Y

Y := fSg;
while 9 x 2 (f(Y ) \Gamma  Y ) loop

-- execution frequency O(#S)
Y := Y split x
end loop

Proof: The preceding code assigns successive values s

i

to Y , where s

0

=

fSg, s

i+1

= s

i

if s

i

= f(s

i

), and s

i+1

2 fs

i

split x : x 2 f(s

i

) \Gamma  s

i

g otherwise. It

is shown by induction on i that s

i+1

! s

i

whenever s

i

6= f(s

i

), and f

i+1

(fSg)

^

\Gamma 

f(s

i

)

^

\Gamma 

s

i+1

^

\Gamma 

s

i

^

\Gamma 

fSg for all i.

(Basis) If s

0

= fSg is a fixed point of f, then all of the inequalities become

equalities, and the claim holds. Otherwise, fSg ? f(fSg), since fSg is the
maximum partition. Hence, there must be some x 2 (f(fSg) \Gamma  fSg). Let
s

1

= fSg split x. By Proposition 2.5 (3), fSg split x ! fSg and f(fSg) split x =

f(fSg). By monotonicity of split, f(fSg) split x

^

\Gamma 

fSg split x, and the claim

holds.

(Induction) Assume the claim holds for any i ? 0. Then f

i+1

(fSg)

^

\Gamma 

s

i+1

^

\Gamma 

s

i

, and f(s

i

)

^

\Gamma 

s

i+1

. By monotonicity of f, we have f

i+2

(fSg)

^

\Gamma 

f(s

i+1

)

^

\Gamma 

f(s

i

), which implies that f

i+2

(fSg)

^

\Gamma 

f(s

i+1

)

^

\Gamma 

s

i+1

. If s

i+1

is a fixed point,

then f(s

i+1

) = s

i+2

= s

i+1

, and the claim holds. Otherwise, f(s

i+1

) ! s

i+1

,

and there must be some x 2 f(s

i+1

) \Gamma  s

i+1

. Let s

i+2

= s

i+1

split x. By Proposition 2.5 (3), s

i+1

split x ! s

i

, and f(s

i+1

) split x = f(s

i+1

). By monotonicity

of split, f(s

i+1

) split x

^

\Gamma 

s

i+2

= s

i+1

split x. The result follows.

12 J. P. KELLER AND R. PAIGE

Finite Differencing
Whereas dominated convergence computes fixed points according to a rudimentary strategy, the finite differencing transformation (see [58]) maintains and
exploits program invariants that are used to implement this strategy efficiently.
The idea underlying this transformation has been called "strengthening invariants" by Dijkstra in [27] and Gries in [40]. Related techniques go back to the
compiler optimizations for strength reduction, [18], [17], iterator inversion, [33],
and set-theoretic strength reduction, [35], [59].

The essential idea is to avoid costly repeated calculations f(x

1

; . . . ; x

k

) occurring in a program region R by maintaining equality invariants of the form

(2:1) C = f(x

1

; . . . ; x

k

)

where C; x

1

; . . . ; x

k

are distinct variables. The finite difference transformation

inserts code on entry to R that establishes invariant (2.1). Within R just before
each assignment x

i

:= g(x

i

) to a variable on which f depends spoils invariant

(2.1), code is inserted to update variable C in order to re-establish invariant (2.1)
immediately after the assignment to x

i

. All calculations of f(x

1

; . . . ; x

k

) that

are made redundant by these steps are replaced by the variable C. Finite differencing improves performance when the cumulative cost of repeatedly computing
f(x

1

; . . .; x

k

) in the unoptimized program exceeds the cost of establishing and

maintaining invariant (2.1) in the optimized program.

Finite differencing rules can be formalized using Hoare Logic.

DEFINITION 2.9. Let S be a command in an imperative programming language, and let ' and '

0

be assertions in a formal language, where the free variables of ' and '

0

are also program variables. The Hoare formula f'gS

\Phi 

'

0

\Psi 

is

said to be satisfied if and only if every terminating execution of S that satisfies
' in its initial state satisfies '

0

in its final state.

The following lemma, which goes back to the general strength reduction
framework of Cocke and Schwartz in [18], and was used throughout the derivation
in [7], provides a basic schema for designing correct code to maintain equality
invariants of the form (2.1). This code is made more efficient by straightforward
simplification.

LEMMA 2.10. (Finite Difference Schema) Let f, g, and h be computable functions, let C; x

1

; x

2

; . . . ; x

k

be distinct program variables, and let '

represent formula

(2:2) f(x

1

; . . . ; x

i\Gamma 1

; g(x

i

); x

i+1

; . . . ; x

k

) = h(f(x

1

; . . . ; x

k

); x

1

; . . . ; x

k

)

PROGRAM DERIVATION 13
which we call a differential identity for f. Then the following Hoare formula is
satisfied:

f' ^ C = f(x

1

; . . . ; x

k

)g

C := h(C; x

1

; . . . ; x

k

)

x

i

:= g(x

i

)

fC = f(x

1

; . . .; x

k

)g

Real-Time Simulation
Finite differencing supports efficient navigation using low level SETL primitives. However, we still need an efficient method of implementing these primitives
-- especially associative access operations such as membership testing and map
application. For this purpose we use a data structure design tool that generalizes
Schwartz's method of basings; see [71].

Basings was one of the most ingenious ideas to come out of the SETL project.
It was a way to aggregate data (related by storage or retrieval operations) around
finite sets called "bases" that were used like a "bulletin board." For example,
suppose that all the elements of two sets A and B were stored in the same base.
Suppose also that both A and B were implemented as linked lists of pointers to
their element values stored in the base. Then we could compute the union of A
and B, and store the result in a third set C in linear time, regardless of the sizes
of the elements. Just traverse one of the lists, say A, and mark each member of
the base that belongs to A. At the same time, C is constructed as a linked list
of pointers to marked elements of the base. Then traverse the other list B, and
extend C only when we see an unmarked element of the base.

The SETL implementation fell short of this sort of efficiency in favor of hashed
data structures that could support a fast general purpose read method (to detect
and eliminate duplicate elements of deeply nested sets), and could respond well
to unpredictable worst case situations. In SETL, sets A, B, and the base would
be implemented as linked hash tables. A new hash table for C would also be
constructed if it had to be searched (as for printing). Otherwise, C could be
represented by marked elements in the base. As in our implementation, A and
B would be searched efficiently using their linked list implementations. Although
the "bulletin board" approach was clearly present in Schwartz's technique, the
persistence of hashing in the best data structures available within the runtime
system made the performance of SETL programs unpredictable.

In [55], the bulletin board idea underlying Basings was formalized and extended into a generic technique for arrays and linked lists. Sufficient conditions
were given for a method that could simulate SETL-like associative access x 2 S
on a RAM in real-time; i.e., access could be performed in unit worst case time
and space on a RAM independent of the size of the data. Unfortunately, no
efficient read method was given. That came later in [56].

The real-time simulation method just mentioned only works on a highly restrictive class of SETL2 programs that read all of their input at the beginning

14 J. P. KELLER AND R. PAIGE
of execution. A simplified form of the method is sketched here. More details are
found in [11].

Assume that each distinct program variable v is restricted to values that
belong to a set denoted by some type o/ throughout the execution of the program.
Such a program invariant is specified by a type assignment v : o/ . Consider the
following simple type system, where each type o/ represents a set val(o/ ). Type
INT stands for the set of integers. If o/ is a type, then type set(o/ ) represents
the set of finite subsets of val(o/ ). If o/

1

and o/

2

are types, then smap(o/

1

; o/

2

) and

mmap(o/

1

; o/

2

) represent the set of finite partial single-valued (resp. multi-valued)

maps from domain val(o/

1

) to range val(o/

2

). Type variable B is called a base

type, and is uniquely associated with a subtype constraint of the form B ! o/
(where o/ is any type except for a base type, although o/ may contain occurrences
of base types), which indicates that val(B) is a subset of val(o/ ). Every base
type B must be associated with a single subtype constraint of the form B ! o/

1

.

The subtype constraints for a given program are restricted so that the graph

f[B

1

; B

2

] : B

1

! o/

2

is a subtype constraint and base type B

2

occurs in o/

2

g

is acyclic.

Besides representing specific sets, each type is also used to model specific
data structures needed to implement set and map operations efficiently. For
this purpose, we augment the type system slightly by allowing a base type B to
appear in two forms, called weak and strong, and denoted by B and B \Gamma  strong
respectively. Three kinds of data structures are considered -- unbased, weakly
based, and strongly based. An unbased set is a set whose element type is not
a base type. It is implemented as a doubly linked list of element values. An
unbased map is a map whose domain element type is not a base type. Its
domain is implemented as a linked list. Each image set is stored separately, and
is accessible from its corresponding domain value by pointer.

Each base type B with subtype constraint B ! o/

1

is implemented as a set

R

B

of records, whose key field stores all the values in val(B); i.e., the key field

of each record stores a distinct value of type o/

1

that belongs to val(B). Weakly

based sets, denoted by set(B), are implemented by doubly linked lists of pointers
to records in R

B

whose key fields store the elements of these sets (see Figure 1).

Weakly based maps, denoted by smap(B; o/

1

) and mmap(B; o/

1

), have similar

linked list implementations for their domains. Strongly based sets are denoted
by set(B \Gamma  strong); strongly based maps are denoted by smap(B \Gamma  strong; o/

1

)

and mmap(B \Gamma strong; o/

1

). The records of R

B

have a distinct field for each set or

map that is strongly based on B. If S is a strongly based set, its corresponding
field is used to store a doubly linked list that connects the set of records in
R

B

whose key fields store elements of S. The domains of strongly based maps

are implemented in a similar way (see Figure 2). This representation supports
unit-time element deletion and linear time search.

PROGRAM DERIVATION 15

.

..

.

..

..

.

..

.

..

..

.

..

..

..
.

.
.

.

.
.
.

.
.

.

.
.
.

.
.
.
.

.
.
.

.

.
.
.

.
.

.
.
.

..
.

.
.
.

.
..

.
.
..

.
.

..
.
..

.
..

..
.
..

..
..

..
..
...

...
...

....
....
..........

....
..........
.....
....

...
..

...
..
..

..
..

..
..
..

.
..

.
..
.

..
.

.
..
.

.
.

.
..
.

.
.

.
.
.

.
.

.
.
..

.
.

.
.
.

.
.

.
.
.

.
.

.

.
.
.

.
.

.
.

.
.
.

.
..

..
..
..........
..
..

.
..

.
.
.

.
.

.

.
.
.

.

.
.

.
.

.
.

.
.

.
.
.

.
.
.

.
.
.

.

.

.

.

.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

..

.

...

..........

..

..

.

.

..

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.
.

.

.
.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

..
.
.
.

.
.
.

.
.
.

.
.
.

.
.

.
.
.

.
.
.

.

..............
.....
....
...

...
..

..
...
.

..
..

.
..
.

..
.

..
.
.

.
..

.
.
.

.
.

.
.
..

.
.

.
.

.
.

.
.
.

.
.

.
.
.

.
.
.
.

.

.
.
.

.

.
.
.

.
.
.

.
.

.

.
.

.
.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.
.

.
.

.
.
.

.
.

.

.
.

.

.
.

.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.

.
.

.
.

.

.
.

.

.
.

.

.
.

.
.

.

.
.

.
.

.
.

.

.
.

.
.
.

.
.
.

.

.
.

.
.

.

.
.

.

.
.

.
.

.

.
..

.
.

.

.

.

.

.

.

.

.

.

.
.

.

.

.

.

.
.

.

.

.
.
.

.

.
.
.

.
.
.

.
.
.

.
.
.

.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.

.
.
.

.
.
.

.
.

.

.
.

.
.

.
.
.

.
.

.
.
.

.
.

.
.

.
.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.
.

.
.

.
.
.

.
.

.
.
.

.
.

.

.
.
.

.
.

.

.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.

.
.
.

.
.

.
.

.
.

.
.
.

.
.

.
.
.

.
.

.

.
.
.

.
.
.

..
.

.
.
.

.
.

.
.
.

..
.

.
.
.

..
.

.
..
.

.
..

.
.
..

.
..

.
..
..

.
..

..
..
..

..
..

..
..
...

...
..

....
...
.....

.....
...........................
......

....
....
...

...
...

..
..
...

..
..

..
..
.

..
..

.
..
..

.
..

.
.
..

.
..

.
.
.

..
.

.
.
..

.
.

.
.
.

..
.

.
.
.

.
.

.
.
.

.
.

.
.
.

.
.

.
.
.

.

.
.
.

.
.

.
.
.

.
.
.
.

.
.

.

.

..

..

.

..

.

..

..

.

..

..

.

...

.
.

.
.

.

.
.
.

.
.

.

.
.
.

.
.
.

.
.

.

..
.
.
.

.
.

.

.
.
.

.
.
.

.
.

.

.
.
.

.
.

...

.

..

..

.

..

..

.

..

..

.

..

.

.

.
.
.
.

.
.

..
.
.

.
.

.
.
..

.
.

.
..
.

..
.

..
.
..

.
..

..
.
..

..
...

..
...
..

....
...

......
.....................
.....

....
...

...
..
...

..
..

..
..
..

.
..

.
..
.

..
.

..
.
.

.
..

.
.
.

..
.

.
.
.

.
.

.
.
.

.

.

.

..

..

.

..

.

..

..

.

..

..

.

...
.
.

.
.

.

.
.
.

.
.

.

.
.
.

.
.
.

.
.

.

..
.
.
.

.
.

.

.
.
.

.
.
.

.
.

.

.
.
.

.
.

...

.

..

..

.

..

..

.

..

..

.

..

.

.

.
.
.
.

.
.

..
.
.

.
.

.
.
..

.
.

..
.
.

..
.

..
.
..

.
..

..
..
.

...
..

..
...
...

...
....

.....
....................
......

...
....

...
..
...

..
..

..
..
.

..
..

.
..
.

..
.

.
..
.

.
..

.
.
.

.
.

..
.
.

.
.

.
.
.

.

.

.

..

..

.

..

.

..

..

.

..

..

.

...

.
.

.
.

.

.
.
.

.
.

.

.
.
.

.
.
.

.
.

.

..
.
.
.

.
.

.

.
.
.

.
.
.

.
.

.

.
.
.

.
.

...

.

..

..

.

..

..

.

..

..

.

..

.

.

.
.
.
.

.
.

..
.
.

.
.

.
.
..

.
.

.
..
.

..
.

..
.
..

.
..

..
.
..

..
...

..
...
..

....
...

......
.....................
.....

....
...

...
..
...

..
..

..
..
..

.
..

.
..
.

..
.

..
.
.

.
..

.
.
.

..
.

.
.
.

.
.

.
.
.

.

.

..

.

..

..

.

..

.

..

..

.

..

..

..
.

.
.

.

.
.
.

.
.

.

.
.
.

.
.
.
.

.
.
.

.

.
.
.

.
.

.
.
.

..
.

.
.
.

.
..

.
.
..

.
.

..
.
..

.
..

..
.
..

..
..

..
..
...

...
...

....
....
..........

....
..........
.....
....

...
..

...
..
..

..
..

..
..
..

.
..

.
..
.

..
.

.
..
.

.
.

.
..
.

.
.

.
.
.

.
.

.
.
. .

.
.

.
.
.

.
.

.
.
.

.
.

.
.

.

.
.
.

.
.
.
.

.
.

.
.
..

.
..

...
.........
...

..
.

.
..
.

.
.

.
.
.

.

.
.
.

.
.
.

.
.

.
.

.
.
.

.
.

.

.
.

.

.
.

.

.

.

.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

..

.

..

...

.........

...

..

.

.

..

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.

.

.

.
.

.

.

.
.
.

.
.

.

.
.

.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.
..

.
..

..
..........
..

..
.

..
.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.
.

.
.

.
.

.

.
.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

..

.

..

...

......

...

..

..

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.

..
.

.

.
.

.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.
..

.
..

..
..........
..

..
.

..
.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.
.

.
.

.

.
.

.
.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

..

.

..

...

......

...

..

..

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.

..
.

.

.
.

.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.
..

.
..

..
..........
..

..
.

..
.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.
.

.
.

.
.
.

.
.

.

.
.

.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

..

.

..

...

......

...

..

..

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.

.

.

.

.

.

.

.

.

.

. .

.

.

.
.

.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.
..

.
..

..
.........
...

..
.

.
..
.

.
.

.

.
.
.

.

.
.
.

.
.

.
.

.

.
.

.

.
.

.
.

.
.
.

.

.
.

.

.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

..

..

....

....

...

..

..

.

..

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.
.

.
.
.

.
.

.
.
.

.
.
.

.
.

.
.
.

.
.

.

.
.
.

.
.

.
.

.
.
.

.
..

..
..
..........
..
..

.
..

.
.
.

.
.

.

.
.
.

.

.
.

.
.

.
.

.
.

.
.
.

.
.
.

.
.
.

.

.

.

.

.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

..

.

...

..........

..

..

.

.

..

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.
.

.

.
.
.

.

.
.

.
.
.

.
.

.

.
..

.
.
.

.
.
.

.
.

..

.
.

.
.

.
.

.
..

.
.
.

.
.

.
.

..
.
.

.
.

.
.

.

..
.

.
.

.
.

.
..

.
.

.
.

.
.

..
.

.
.

.
.

.
..

.
.

.

.
.

.
..

.
.

.
.
.

..
.

.
.

.

.
..

.
.

.
.

.
..

.
.

.
.

.
..

.
.

.
.

..

.
.

.

.
..
.

.
.

.

..
.

.

.
.

..

.
.

.
.

..
.

.
.
..
.
.

.
..
.

.
.
..
.
.

.
..
.

.
.
.
.
.
.

..
.

.
.
.

.
.
.
..
.

.
.
.
.

.

.

..
.
.

.
.
.

.

.
..

.
.

.
.
.

.

.

..

.

.

.

.

.
..

.

.
.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

.

..

.

.

.

.

.

.

..

.

.

.

.

.

.

.

..

.

.

..

.

.

.

.

...

.

.

.

..

.

.

..

..

.

.

...

.

..

..

.

..

.

..

..

.

.

...

..

.

...

.

..

..

..

..

.

..

...

.

..

.

...

..

.

...

..

.

..

..

.

............

..........

.
.
.

..
.

.
.
.

.
.

.
.
.

.
.

.
.
..

.

Last
First

set elements

Weakly based set of type set(B)

B
WEAKLY BASED DATA STRUCTURES

o/ is any type except for a base type, and B ! o/

o/
Figure 1. Weakly based data structures.
For each base type B, val(B) is defined to be the smallest set that satisfies
the program type assignments (restricted to input variables and constants) and
the program subtype constraints. In [11] it is shown how to calculate val(B)
for any base type B as a finite set of values obtained from the input and from
program constants.

The preceding types and corresponding data structures can be used to analyze the asymptotic time and space bounds utilized by SETL programs formally. Consider the type assignment v : B, y : C, g : smap(B \Gamma  strong; o/

0

), f :

mmap(B \Gamma  strong; C), T : set(C \Gamma  strong), R : set(C), and S : set(B \Gamma  strong),
with subtype constraints B ! o/ and C ! o/

0

. Then operations v 2 S, S less := v,

S with := v, g(v) := y can all be done in unit time.

Assignment T := f[S] can be performed in time O(#S+#f[x; y] 2 fjx 2 Sg),
as can be seen from the following implementation:

16 J. P. KELLER AND R. PAIGE

.
.

.
.

.

.
.

.
.

.
.

.
.

.
.

.
.
.

.
.

.
..

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.

.

.

.

.
.

.

.

.

.
.

.

.

.

.

.
.

.

.

.

.

.

.

.

.

.

.

.
.

.
.

.
.

.
.
.

.
.
.

.
.

.

.
.

.

.
.

.

.
.

.
.

.
.
.

.
.
.

.

.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.

.
.
.

.
.
.

.
.
.

.
.

.
.
.

.
.

.
.

.

.
.

.
.

.
.

.
.

.
.

.
.
.

.
.
.

.
.

.
.

.
.

.
.
.

.
.

.
.
.

.
.

.
.

.

.
.
.
.

.
.

.
.
.

.
.

.
.

.
.
.

.
.

.
.

.
.
.

.
.

..
.
..

...
......
...

..
..
.

.
.

.
.
.

.
.

.
.

.
.

.

.
.

.
.

.
.

.
.

.
.
.

.
.

.

.
.
.

.

.
.

.
.

.

.

.
.

.

.

.

.

.

.

.

.

.

.

..

.

..

..

..........

..

..

.

..

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.
.

.
.

.

.

.
.

.
.

.
.

.
.
.

.
.

.
.

.
.

.
.
.

.
.

.
..

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.

.

.

.

.
.

.

.

.

.
.

.

.

.

.

.
.

.

.

.

.

.

.

.

.

.

.

.
.

.
.

.
.

.
.
.

.
.
.

.
.

.

.
.

.

.
.

.

.
.

.
.

.
.
.

.
.
.

.

.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.

.
.
.

.
.
.

.
.
.

.
.

.
.
.

.
.

.
.

.

.
.

.
.

.
.

.
.

.
.

.
.
.

.
.
.

.
.

.
.

.
.

.
.
.

.
.

.
.
.

.
.

.
.

.

.
.
.
.

.
.

.
.
.

.
.

.
.

.
.

.
.
.

.
.
.
.

.
.

.
.
.

..
.

..
..
...........
..
..

.
..

.
.
.

.
.

.
.
.

.

.
.
.

.
.

.
.

.
.

.
.

.
.
.

.
.

.
.
.

.
.
.

.
.
.

.

.

.

.

.
.

.

.

.

.

.

.

.

.

.

.

.

..

.

.

..

..

...

.......

...

..

..

.

.

..

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.
.

.

.
.

.
.

.
.
.
.

.
.

.
.
.

.
.

.
.

.
.
.

.
.

.
.

.
.
.

.
.

..
.
..

...
......
...

..
..
.

.
.

.
.
.

.
.

.
.

.
.

.

.
.

.
.

.
.

.
.

.
.
.

.
.

.

.
.
.

.

.
.

.
.

.

.

.
.

.

.

.

.

.

.

.

.

.

.

..

.

..

..

..........

..

..

.

..

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.
.

.
.

.

.

.
.
.

.
.

.
.

.
.

.
.

.
.
.

.
.

.
.

.
.

.
..

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.

.

.

.

.

.

.

.

.

.

.

.
.

.

.

.

.

.
.

.
.

.

.
.

.
.

.
.

.
.
.

.
.

.
.
.

.
.
.

.
.

.

.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.

.

.
.

.

.
.

.

.
.

.

.
.

.

.
.

.

.
.

.
.

.
.
.

.
.
.

.
.

.
.

.
.
.

.
.

.
.

.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

.
.

.
.
.

.
.

.
.
.

.
.

.
.
.

.
.

.
.
.

.
.

.
.

.
.

.
.
.

.
.

.
.

.
.
.

..
.

..
..
..........
..
..

.
.

..
.
.

.
.

.

.
.
.

.
.

.

.
.

.
.

.
.
.

.
.

.
.
.

.
.
.

.
.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

..

.

..

.....

..

....

..

..

.

..

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.

.

.

.
.

.

.

.

.

.
.
.

.
.

.
.

.
.

.
.

.
.
.

.
.

.
.

.
.

.
..

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.

.

.

.

.

.

.

.

.

.

.

.
.

.

.

.

.

.
.

.
.

.

.
.

.
.

.
.

.
.
.

.
.

.
.
.

.
.
.

.
.

.

.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.

.

.
.

.

.
.

.

.
.

.

.
.

.

.
.

.

.
.

.
.

.
.
.

.
.
.

.
.

.
.

.
.
.

.
.

.
.

.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

.
.

.
.
.

.
.

.
.
.

.
.

.
.
.

.
.
.

.
.

.
.
.

.
.

.
.

.

.
.
.

.
.
.
.

.
.

.
.
..

.
..

...
.........
...

..
.

.
..
.

.
.

.
.
.

.

.
.
.

.
.
.

.
.

.
.

.
.
.

.
.

.

.
.

.

.
.

.

.
.

.

.

.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

..

.

..

...

.........

...

..

.

.

..

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.
.

.
.

.

.
.
.

.
.

.
.
.

.
.

.
.

.
.

.
.
.

.
.

.
.

.
.
.

..
.

..
..
..........
..
..

.
.

..
.
.

.
.

.

.
.
.

.
.

.

.
.

.
.

.
.
.

.
.

.
.
.

.
.
.

.
.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

..

.

..

.....

..

....

..

..

.

..

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.

.

.

.
.

.

.

.

.

.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.

.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.

.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.

.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.

....

....................................

.........

......

....

...

....

..

..

..

..

..

.

..

.

..

.

.

.

..

..

.

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

.

..

.

.

.

.

.
.

.

..

.

.

.

.

.

.

.

.

..
.

.

.
.

..

.
.

.

.
..

.

.

.

.

.

..
.

.
.

.
.

..

.
.

.
.

.

.

..
.

.
.

.
.

.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

..
.
.
.

.
.

.
.

.

.
.

.
.

.
.

.

.
.

.
.

.

.

.

.

.

.

.

.

.

..

..

.

.

.

...

.

.

.

.

.

..

..

.

.

.

..

.

..

.

..

.

.

.

.

...

.

.

.

..

.

.

...

.

.

..

.

.

..

..

.

.

.

...

.

.

...

.

..

.

...

..

.

...

..

..

...

...

..

..

....

..

...

...

...

...

...

...

...

.

.

..

.

.

.

..

.

.

..

.

.

.

..

.

.

...

...........
..........

.

..

.

..

..

.

...

.

..

..

.

...

.

..

..

.

..

...

.

..

.

..

..

..

.

..

.

...

.

..

..

.

...

.

..

.

..

..

..

.

..

.

...

.

..

.

...

.

..

.

..

..

..

.

.

...

.

..

.

.

...

.

..

.

...

.

..

.

..

..

.

.

...

.

.

.

...

.

.

...

.

.

..

..

.

.

..

.

.

.

...

.

.

.

..

.

.

...

.

.

.

.

.

.

...

.

.

.

.

.

.

..

.

.

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

.

..

.

.

.

.

.

.

..

.

.

.

.

.

.

.

..

.

.

.

.

.

.

.

..

.

.

.

.

.

.

.

.

..

.

.

.

.

.

.

.

.

.

..

.

.

.

.

..

.

.

.

.

..

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

.

..

.

.

.

.

.

.

..

.

.

.

.

.

.

..

.

.

.

.

.

.

.

..

.

.

.

.

.

.

.

..

.

.

.

.

.

.

.

..

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.
.

.

.

.

.

.

.

.
.

.
.

.
.

..
.

.
.

.
.
.

.
.
.
.

.
.

.
.

.
.

.
.
.

.
.

................

..............

.............

............

..........

.........

........

........

.......

......

.....

....

.....

.....

..

....

..

..

....

..

..

...

.

..

...

..

.

...

..

.

..

..

..

..

.

...

.

..

.

.

...

.

..

.

..

.

..

..

.

..

.

..

..

.

..

.

.

.

..

..

.

..

.

..

..

.

.

..

..

.

.

..

.

.

..

..

.

.

..

.

..

..

.

.

.

...

.

.

.

.

...

.

.

.

.

..

..

.

.

..

..

.

..

.

.

.

..

..

.

.

.

.

...

.

.

..

.

.

..

..

.

.

.

..

..

.

.

..

.

.

.

.

.

...

...

..

...

...

...

...

..
.

.
.
.
.

.
.

.
.

.
.

.
.
.

.
.

.
.

.
.

.

.
.
.

.

.
.

.
.
..

.
.
.

.
.

.

.
..

.
.

.
.

.
.

..
.

.
.

.
.

.
..

.
.

.
.

.
.
..

.
.

.
.

.
.
..

.
.

.
.
.

.
..

.
.

.

.
.

.
..

.

.
.
.

.
.

..

.
.

.
.

.
..

.
.

.

.
.

..
.
.

.
.

.
..

.
.

.
.
.

..
.

.
.
.

.
..

.
.
.

.
.

..
.

.

.
.
.

..
.
.
.

.
.
..

.
.

.
.

.
..

.

.
.
.

.
.

.
.
.

.
..

.
.

.
.

.

.
.
.

.

.
..
.

.
.

.

.
.
.

.
.
..

.
.

.
.

.
.

.
.

.
..

.
.

.
.

.
.
.

.
.
..
.

.
.
.
.

.
.
.
..

.
.
.
.

.

.
.
.

..
.

.
.

.

.
.
.

..
.

.
.

.
.

.

.
..
.

.
.
.

.

.
.
..

.
.

.
.
.

.
.

..
.
.

.
.
.
.

..
.

.
.
.

.
.

..
.
.

.
.

.
.
..

.
.

.
.
..

..
.

.
.
..

.
..

.
.
.

.
...

.
.
.

.
...

.
.
..

.
...

.
..
.

...
.

..
...
..

.
..

...
...
...

..
..

...
....
...

...
....

....
....
.....

.....
......

.......
........
............

..............................................
...........
.........
......

......
.....

.....
.....
...

.....
....

...
...
...

....
..

....
..
...

...
...

..
..
...

..
...

..
..
..

..
..

...
.
..

...
.

..
..
..

.
...

.
..
.

...
.

.
..
.

.
..

..
.
.

.
..

.
.
.

..
.

.
.
.

.
.

.
.
..

.
.

.
.
.

.
.

.
..
.

.
.

.
.
.

.
.
..
.

.

.
.
.

.
.
..

.
.

.
.

.

.
.

..
.

.
.

.
.

.
.

..
.

.
.

.
.
.

.
..

.
.
.

.
.
.

.
..
.
.
.
..

.
.

.
..
.
.
.

..
.
.
.

..

.

.
.

..

.
.

.

.
..

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

.

..

.

.

.

.

.

.

.

..

.

.

.

.

.

.

.

.

..

.

.

.

.

.

.

.

.

.

..

.

.

.

.

.

.

.

.

.

.

..

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

..

.

.

.

.

.

.

..

..

.

..

..

..

.

..

..

..

.

..

..

.
.

.
.

.
.

.

.
.
.

.
.

.
.

.
.

.

.
.
.

.

...
...
....

...
....

...
...
....

...
....

...
....
...

...
....

...
....
...

....
..

...
....
..

....
...

...
...
....

...
..

....
...
....

..
....

...
....
...

...
...

....
...
....

...
....

..
...
....

...
....

...
....
...

...
...

....
..
....

..
....

..
....
..

....
..

....
....
..

....
...

...
...
....

...
...

...
....
..

....
....

...
...
...

....
...

....
....
..

....
...

....
..
...

....
..

...
...
...

..
....

...
...
...

....
..

...
...
...

...
..

....
..
...

....
..

...
...
...

..
....

...
..
....

...
..

....
..
...

...
..

...
...
...

..
...

...
..
....

..
...

...
...
..

....
..

....
..
..

...
...

..
...
..

....
..

..
...
...

..
...

..
....
..

..
....

.
..
...

..
...

..
..
...

..
...

..
..
...

..
...

..
..
...

..
...

..
..
....

..
..

..
...
.

..
...

.
...
..

..
..

.
...
..

..
..

..
...
..

.
...

..
..
..

...
..

...
..
...

..
...

..
...
...

...
..

..
...
..

...
..

..
...
..

...
.

..
...
..

..
..

..
..
..

..
..

...
.
..

..
..

..
..
..

.
...

.
.
..

.
.

..
..
.

.
.

..
.
.

..
.

.
.
.

.
.

..

.
.
.

.
.

..

.
.

.
.
..

.
.
.

.
..

.

.

.

.

.

.

.

.

..

..

...

..

....

...

.....

....

....

.

....

....

....

....

....

...

.
..
.

..
..

.
..
.

..
.

..
..
.

..
..
..

...
..

..
..
...

..
..

...
..
..

.
...

..
..
...

.
..

...
..
..

.
...

..
..
...

..
..

...
..
..

...
..

..
...
..

..
..

..
..
..

..
..

...
.
..

...
..

.
...
..

...
..

..
..
..

..
...

..
...
..

.
...

..
.
...

..
..

..
.
...

..
..

..
...
.

..
..

.
...
..

..
.

..
...
..

.
..

...
..
.

..
.

...
..
.

..
...

.
..
..

..
..

.
..
..

..
.

..
...
.

..
.

...
..
.

...
.

.
..
..

..
.

.
...
.

..
..

.
..
.

...
.

..
..
.

..
.

.
..
.

..
..

.
.
..

.
.

...
.
.

..
.

..
..
..

.
..

.
...
.

...
.

..
...
.

..
.

...
..
.

...
.

..
..
..

.
...

.
.
..

..
..

..
.
...

.
..

...
.
..

.
...

.
.
..

.
...

.
..
.

..
..

.
.
...

.
.

..
.
..

.
.

.
.
..

.
.

.
.
..

.
.

.
.
..

.
.

.
.
..

.

.
.
.

.
.

.

.
.
..

.
.

.
.

.
.
.

.
.
..

.
.

.
.

.
.

.
.

..
.

.
.

.
.
.

.
.

..
.
.
.
..

.
.
.
..
.

.
.
.

.

..

.

.

.

.

.

..

.

.

.

.

.

.

..

.

.

.

.

.

.

.

..

.

..

.

.

.

.

...

.

..

.

..

..

..

.

..

...

.

..

..

...

...

....

....................

........

.......

.....

.

..

.

..

.

..

..

.

..

.

..

..

.

....
...

....
....

...
....
..

.
.
.

.
.

.
.
..

.
.

.
.
.

.
..

.

.
.

.
.

.

..
.
.

.
.

.

.
..
.

.
.
.
.

.
..
.

.
.

.
.

.
..

.

.
.
.

.

.
..
.

.
.

.
.
..

.
.

.
.

.

..
.

.
.

.

.
..
.

.
.

.

.
..
.

.
.
.

.
..

.

.
.
.

.

..
.
.

.

.
.
..

.
.

.
.

.
.

.

.
.
.

..
.
.

.
.

.
.

.
.

.
.

..
.

.
.
.

.
.
.
.

.
.
..
.

.

.
.
.

.

.
.
.

..

.
.
.

.
.

.
.

.
.

..

.
.
.

.

.
.
.

.
..

.
.

.

.
.
.

.
.

..
.

.
.
.

.
.

.

.
..
.

.
.

.
.

.
.
..

.

.
.
.

.
.

.
..
.

.
.
.
.

.
.

..
.
.

.
.

.
.

..
.

.
.
.

.
.

..
.
.

.
.

.
.
..

.
.

.
.
.

.
..

.
.
..

.
.

..
.
.

.
.

.
...
.

.
.

.
..
..

.
.

.
...
.

..
.

..
..
.

..
..

..
..
...

.
..

..
...
..

..
...

..
...
...

...
....

...
......
......

.......
.........................................
............

..........
.......
......

......
......

.....
.....
....

....
.....

...
.....
....

...
....

....
..
....

...
....

..
....
..

....
..

....
..
...

..
...

..
...
.

..
...

..
..
..

...
.

..
..
.

...
..

.
..
.

...
.

.
..
.

...
.

.
.
..

.
..

.
.
.

.
...

.
.
.

.
.

..
.
.

.
.
..
.

.

.
.
.

.
.
.

..
.

.
.
.

.
.
..

.

.

.

.
.

..

.

..

.

..

..

.

..

.

...

..

...

...

...

..

...

...

..

....

...

....

..

...

....

...

....

...

....

..

......

......

.......

....

..
.

.
..

.
..
.

.
..

.
..
.

.
..

.

STRONGLY BASED DATA STRUCTURES

domain F
0/1

B

S

Prev

b. Strongly based single-valued map F: smap(B-strong,o/ )

y: o/

1

F(y): o/

B

Next

a. Strongly based set S: set(B-strong)

y: o/

1

0/1

Last

First

o/

1

is any type except for a base type, o/ is any type, and B ! o/

1

c. Strongly based multi-valued map F: mmap(B-strong, o/ ) is

the same as in (b), except that each image Ffyg is of type set(o/ )

Next
First
Last

Prev

Figure 2. Strongly based data structures.

PROGRAM DERIVATION 17
T := fg
for x 2 S loop

-- execution frequency #S
for y 2 ffxg loop

-- execution frequency #f[x; y] 2 fjx 2 Sg
if y 62 T then

T with:= y
end if
end loop
end loop

where iteration through a set takes linear time in the cardinality of the set, and
each of the other operations takes unit time. Given type assignment T

0

: set(C),

we can split set T into two sets T \Gamma  R and T " R in time O(#R) by destructive
modification to T ,

T := T \Gamma  R
T

0

:= T " R

using the following lower level implementation:
T

0

:= fg

for y 2 R loop

-- execution frequency #R
if y 2 T then

T less:= y
T

0

with:= y

end if
end loop

All of the time bounds just given hold for any types o/ and o/

0

other than

base types. In contrast, under the most optimistic assumptions for a hash table
implementation, the expected cost of computing a single primitive associative
access, say f(x), would grow linearly in the size of x for the case where x 2
domain f.

Our approach to algorithm derivation depends strongly on guidance from
complexity considerations. Hence, before discussing the derivation we need to
carry out a thorough analysis of the essential datatypes and operations used in
the problem domain. The real-time simulation technique makes such an analysis
possible with minimal involvement in low level implementation detail.

The main datatype for our problem domain is the partition semilattice together with the split operation. For an efficient implementation we need to
augment our type system with two new features. Whenever the element types
of the domain and range of a single valued map are both base types, say B
and C, then there is a convenient data structure to implement type smap(B \Gamma 
strong; C \Gamma  strong), which is strongly based in its range as well as its domain.
Since the preimages of such a map are pair-wise disjoint, each preimage can be

18 J. P. KELLER AND R. PAIGE
separately linked, and all of the preimages can be stored in a single column. The
range can be stored like a strongly based set, where each range element points
to some one arbitrary witness in its preimage. If map f is of this type, and we
also have x : B and y : C, then single-valued map operations f(x) := om (which
removes x from a unique preimage set) and f(x) := y (which adds x to preimage set f

\Gamma 1

fyg) can be performed in unit time. Preimage set f

\Gamma 1

fyg can also

be accessed directly from range element y in unit time; it can be subsequently
searched in linear time with respect to its cardinality.

We will also want to have operation new(B) to create a new value of type B
dynamically. Although we will not need it in this paper, delete(x) is an obviously
useful thing to have for explicit garbage collection of unused base elements.

These new features allow us to represent partitions that can be updated easily
in situ. Partitions that are represented as a set of sets are harder to modify in
this way, and require closer scrutiny to ensure that they remain partitions after
they are modified.

Suppose that partition P over set S is input as a set of sets, where the
elements of S are represented by integers. More formally, suppose we have type
assignment P : set(set(node)), where elements of each block of P are of type
node ! IN T . Using the multiset set discrimination procedure found in [56]
(based on [60]), an input method can remove duplicate blocks and ensure that
the remaining blocks of P are mutually disjoint in linear time with respect to
the length of the input string.

Consider how to convert P into the alternative representation T 1 : smap
(node\Gamma strong; thin\Gamma strong), where thin ! IN T is a base type used to "name"
blocks (preimage sets) in T 1, and where

(2:3) P =

\Phi 

T 1

\Gamma 1

fqg : q 2 range T 1

\Psi 

Let smap pname = f[p; new(thin)] : p 2 P g "name" each block of P uniquely.
Then the conversion can be done in time O(#S) by implementing assignment

T 1 :=

\Phi 

[x; pname(p)] : p 2 P; x 2 p

\Psi 

using the following lower level code (based on the operational semantics of SETL
set formers)

for p 2 P loop

id := new(thin) -- id = pname(p)
for x 2 p loop

-- execution frequency #S
-- place all elements of block p in the same preimage of
T 1 T 1(x) := id
end loop
end loop

We can also split T 1 relative to any set splitter ` S, in a way corresponding
to P split splitter, so that relationship (2.3) is preserved. Define two smaps

PROGRAM DERIVATION 19
T 1

0

and T 1

00

to be equivalent, written T 1

0

j T 1

00

, if their respective sets of

preimages form the same partition. Let smap pname = f[p; new(thin)] : p 2
P split splitterg "name" each block of P split splitter uniquely. Let twin :
smap(thin \Gamma  strong; thin) map "names" of blocks in T 1 that intersect with
splitter : set(node) into new "names"; i.e.,

twin =

\Phi 

[n; new(thin)] : n 2 T 1[splitter]

\Psi 

Then the following equivalences,

\Phi 

[x; pname(p)] : p 2 P split splitter; x 2 p

\Psi 

j

\Phi 

[x; twin(n)] : n 2 T 1[splitter]; x 2 splitter " T 1

\Gamma 1

fng

\Psi 

[

f[x; y] 2 T 1jx 62 splitterg
j

\Phi 

[x; twin(T 1(x))] : x 2 splitter

\Psi 

[ f[x; y] 2 T 1jx 62 splitterg

ensure that T 1 can be split by splitter using assignments,

twin :=

\Phi 

[n; new(thin)] : n 2 T 1[splitter]

\Psi 

T 1 := T 1 \Gamma  f[x; y] 2 T 1jx 2 splitterg [

\Phi 

[x; twin(T 1(x))] : x 2 splitter

\Psi 

so that invariant (2.3) is preserved. The procedure just below is a lower level
implementation, which takes time and space O(#splitter):

-- call by reference
procedure split(T 1, splitter)
twin := fg
for x 2 splitter loop

-- execution frequency #splitter
qold := T 1(x)
qnew := twin(qold)
if qnew = om then

-- generate a new base element that `names' the new block
qnew := new(thin)
twin(qold) := qnew
end if
-- indexed assignment to T 1 is broken into two operations
T 1(x) := om --remove x from block T 1

\Gamma 1

fqoldg

-- we could execute delete(qold) if T 1

\Gamma 1

fqoldg = fg

T 1(x) := qnew --add x to block T 1

\Gamma 1

fqnewg

end loop
end

The data structure for node can be represented by either an array or by an
unconnected set of records. However, since base thin is dynamic, we prefer an
implementation by separate records -- not by an array.

Suppose that we repeatedly call the preceding split procedure to execute
T 1 split f

\Gamma 1

[q], where f

\Gamma 1

is drawn from a family of inverse maps F

0

. Suppose

also that we have the type assignment

20 J. P. KELLER AND R. PAIGE
F

0

: set(mmap(node \Gamma  strong; node))

where an array is used to implement node, and each strongly based map f

\Gamma 1

2 F

0

is implemented as an array-based adjacency list (either as a column of the base
data structure or a detached vector). Then each preimage set calculation f

\Gamma 1

[q]

to form a splitter is achieved in time O(#q + #

\Phi 

[y; x] 2 f

\Gamma 1

jy 2 q

\Psi 

), and split

is performed in time O(#f

\Gamma 1

[q]), as reported above. These data structures are

similar to those used in Hopcroft's DFA minimization algorithm; see [44].

3. Derivation
Having gained a basic understanding of partitions and their costs, we can
begin the derivation. First, we will derive a high level, generalized form of the
bisimulation algorithm of Paige and Tarjan; see [60]. This is sufficient to correct
a minor error found in that paper. Next, we will transform the bisimulation
algorithm into an improved form of Hopcroft's algorithm to minimize a deterministic finite state automaton; see [44]. The derivations will be short, will reveal
the essential algorithmic decisions, and will facilitate a complexity argument. In
this section the most difficult transformational step, the one that has been proofchecked mechanically, will be accepted on faith. The next section will describe
the formal proof in detail.

Previously Fernandez in [34] and Westin in [89] gave partial derivations of a
generalized form of the fast bisimulation algorithm found in [60]. Fernandez's
algorithm was not quite the same, and Westin's algorithm did not achieve the
best complexity, because he was unable to fix the error that appeared in [60].
In [39] Gries gave a full derivation of Hopcroft's algorithm, but that derivation was lengthy, and did not capture the essential algorithmic decisions that
Hopcroft made. In [12] Cai and Paige later presented the initial portion of a
shortened derivation similar to the one given here, and they speculated that the
full derivation would yield an algorithm that improves Hopcroft's algorithm in
space utilization. In [87] Watson also derived the Hopcroft algorithm in a more
intuitive way than did Gries. However, his derivation was not intuitive enough
to support an easy and formal algorithmic analysis of the resulting algorithm.

3.1. Specification
Like Smolka and Kanellakis (see [46]) we will model the bisimulation problem
mathematically as the relational coarsest partition problem. Given a partition
P of a finite set S, and a nonempty finite family F of binary relations over S,
compute the maximum refinement Q of P such that Q is stable relative to every
preimage f

\Gamma 1

[q] for every f 2 F and every q 2 Q; i.e.,

(3:1) max Q

^

\Gamma 

P j(8q 2 Q; 8f 2 F; 8r 2 Qjq ` f

\Gamma 1

[r] . q " f

\Gamma 1

[r] = fg)

After rewriting specification (3.1) equivalently as

max Q

^

\Gamma 

P j(8q 2 Q; 8b 2

\Phi 

f

\Gamma 1

[r] : f 2 F; r 2 Q

\Psi 

jq ` b . q " b = fg)

PROGRAM DERIVATION 21
we can invoke Proposition 2.7 (3) in order to place the specification directly into
the more convenient form,

(3:2) max Q

^

\Gamma 

P jQ = Q split%

\Phi 

f

\Gamma 1

[r] : f 2 F; r 2 Q

\Psi 

)

Syntactic analysis of specification (3.2) tells us that subexpression

\Phi 

f

\Gamma 1

[r] : f 2 F; r 2 Q

\Psi 

is monotone with respect to Q; i.e., if Q

1

^

\Gamma 

Q

2

on partition space (P(S); u),

then

\Phi 

f

\Gamma 1

[r] : f 2 F; r 2 Q

1

\Psi 

v

\Phi 

f

\Gamma 1

[r] : f 2 F; r 2 Q

2

\Psi 

on the partially ordered set (pow(S); v). Since split% is monotone in both
arguments (cf. Proposition 2.7), and since monotonicity is closed under composition, we know that expression Q split%

\Phi 

f

\Gamma 1

[r] : f 2 F; r 2 Q

\Psi 

is monotone in

Q. Since split% is deflationary in its first argument (cf. Proposition 2.7), then
we can use a well-known lattice-theoretic identity (cf. [21] or Theorem 5 in [12])
to rewrite specification (3.2) as,

gfp Q:P uQ split%

\Phi 

f

\Gamma 1

[r] : f 2 F; r 2 Q

\Psi 

which can be transformed into the following more convenient form,
(3:3) gfp Q:Q split% (P [

\Phi 

f

\Gamma 1

[r] : f 2 F; r 2 Q

\Psi 

)

using the definition of meet and Proposition 2.7.

Since split% and union are monotone in both arguments, and monotonicity
is closed under composition, we know that function

F (Q)

def

= Q split% (P [

\Phi 

f

\Gamma 1

[r] : f 2 F; r 2 Q

\Psi 

)

is monotone on the partition semilattice (P(S); u; fSg). Hence, specification
(3.3) is well defined, and is computed by F

j

(fSg) for some j

^

\Gamma 

#S, according to

Theorem 2.2. In [46] Kanellakis and Smolka used a similar approach to obtain
an algorithm that ran in time O(mn), where n = #S and m =

P

f2F

#f.

3.2. Dominated Convergence
The sequence of iterates (that results from Theorem 2.2) computed by Kanellakis and Smolka's algorithm converges rapidly, but each successive iterate involves potentially costly redundant computations. The main point of dominated
convergence is to avoid this redundancy by converging less rapidly towards the
fixed point, but proceeding with much more efficient small steps. Instead of
computing the expensive and potentially redundant F (Q) operation each step of
the Tarski sequence, dominated convergence will form a sequence of small steps
with inexpensive split operations. Such an approach leads to the efficient algorithm found in [60]. Immediate application of Corollary 2.8 allows us to compute
specification (3.3) with the code shown in Figure 3.

22 J. P. KELLER AND R. PAIGE

-- maintain invariant F (Q)

^

\Gamma 

Q

Q := fSg
while 9 q 2 (F (Q) \Gamma  Q) loop

-- execution frequency O(n)
Q := Q split q
end loop

Figure 3. First approximation to a relational coarsest partition algorithm.
The code found in Figure 3 computes the fixed point through a sequence of
inexpensive split operations, but the costly calculation of F (Q) is merely shifted
to the top of the while-loop. We will avoid this calculation by using the finite
differencing transformation to maintain and exploit the invariant

(3:4) T = F (Q)
However, in order to apply finite differencing conveniently, a small detail needs
to be added to our code. Since the code found in Figure 3 maintains the invariant
F (Q)

^

\Gamma 

Q, we know that each time block q is chosen from F (Q) at the top of

the while-loop, there must be a unique block b 2 Q such that q ae b ^ q 6= b. We
need to compute b explicitly in the program. Since the choice of q is arbitrary,
we can first choose any b 2 Q \Gamma  F (Q), and then base our choice of q on any block
within F (Q) that is also contained in b. The result is shown in Figure 4.

-- maintain invariant F (Q)

^

\Gamma 

Q

Q := fSg
while 9 b 2 (Q \Gamma  F (Q)), 9 q 2 F (Q)jq ae b loop

-- execution frequency O(n)
Q := Q split q
end loop

Figure 4. Algorithm just before finite differencing.
Invariant (3.4) can be established by executing the assignment,
(3:5) T := F (fSg)
on entry to the loop. After being spoiled by the assignment,

Q := Q split q

PROGRAM DERIVATION 23
the invariant can be reestablished by updating T in the following way,
(3:6) T := T split% (

\Phi 

f

\Gamma 1

[q] : f 2 F

\Psi 

[

\Phi 

f

\Gamma 1

[b \Gamma  q] : f 2 F

\Psi 

)

just after Q is updated. Code (3.6), which updates T , is derived from the Finite
Difference Schema of Lemma 2.10 using the differential identity

(3:7) F (Q split q) = F (Q) split% (

\Phi 

f

\Gamma 1

[q] : f 2 F

\Psi 

[

\Phi 

f

\Gamma 1

[b \Gamma  q] : f 2 F

\Psi 

)

Consequently, we can replace both occurrences of F (Q) within the while-loop by
occurrences of T .

It was a variant of differential identity (3.7) that caused trouble for students
in the honors section of a graduate course in compilers, which is what motivated
the current work. We accept the identity at this point. In the next section the
identity is proved formally, and its passage through the NAP proof checker is
described.

The decision to eliminate the repeated computation of F (Q) has now been
made. The cost of the whole algorithm is now dominated by the cumulative
cost of splitting partition T . Recall from the preceding section that with an
appropriate implementation the cost of computing T split f

\Gamma 1

[q] is dominated

by the cost of computing f

\Gamma 1

[q], which is

(3:8) O(#q + #f[x; y] 2 fjy 2 qg)
It is reasonable to reduce this cost by choosing each block q (used in splitting)
wisely. One heuristic is to try to lower the sum of the cardinalities #q over every
block q that is chosen by the algorithm. A way to do this was discovered by
Hopcroft (see [44]) in the design of his algorithm to compute the minimum state
DFA, and adapted by Paige and Tarjan to their relational coarsest partition
algorithm; see [60].

Known as Hopcroft's "process the smaller half strategy," the idea is deceptively simple. From the definition of partition refinement, we know that whenever block q is chosen from "thin" partition T = F (Q), the block b 2 Q (which
contains q) from the "fat" partition must also contain another "thin" block q

0

.

Paige and Tarjan's form of Hopcroft's idea was to examine two arbitrary "thin"
blocks belonging to b, and to choose one that contains no more than half the
number of elements in b. These modifications are incorporated into the program
found in Figure 5.

The significance of Hopcroft's strategy is clear. The sequence q

1

; q

2

; . . . of

"thin" splitter blocks chosen by the program in Figure 5 satisfies the condition
that for every i = 1; . . . and every j ? i, either q

i

" q

j

= fg or q

j

ae q

i

^ #q

j

^

\Gamma 

#q

i

=2. Hence, partial order (fq

i

: i = 1; . . .g; `) has the structure of a forest in

which the set of all blocks at equal depth in the forest partition a subset of S.
Let Q

d

be the set of splitters occurring in the forest at depth d. Let n = #S, let

k = #F, and let m be the sum of the total number of pairs in each relation f
over all relations contained in F. Then the total cost of splitting by operations
of the form T split f

\Gamma 1

[q] for all splitters q 2 Q

d

at depth d is,

24 J. P. KELLER AND R. PAIGE

-- maintain invariant T

^

\Gamma 

Q

Q := fSg
T := F (fSg)
while 9 b 2 (Q \Gamma  T ), 9 q 2 T jq ae b ^ #q

^

\Gamma 

#b=2 loop

Q := Q split q
T := T split% (

\Phi 

f

\Gamma 1

[q] : f 2 F

\Psi 

[

\Phi 

f

\Gamma 1

[b \Gamma  q] : f 2 F

\Psi 

)

end loop

Figure 5. Algorithm after finite differencing.

O

0
@

X

f2F

X
q2Q

d

#q + #f[x; y] 2 fjy 2 qg

1
A

= O

0
@

X

f2F

0
@

X

q2Q

d

#q

1
A

+ #f[x; y] 2 fjy 2 [

q2Q

d

qg

1
A

= O

0
@

X

f2F

#S + #f[x; y] 2 fjy 2 Sg

1
A

= O(kn + m)
Also, the forest depth cannot exceed log n, since the geometric progression
n=2

i

for i = 1 . . . collapses to 1 in log n steps. Therefore, the cumulative cost

of splitting T by operations of the form T split f

\Gamma 1

[q] is O((kn + m) log n).

Unfortunately, this strategy alone gives no such bound on operations of the form
T split f

\Gamma 1

[b \Gamma  q], which also appears in the code that updates T . Nevertheless,

another strategy found in [60] is used to compute both splitting operations in
time (3.8).

Several observations can be made about the program in Figure 5. The code
(3.5) used to establish invariant T can be improved by straightforward simplifi-
cation. Using Proposition 2.7 (4) and (5), we obtain

F (fSg) = fSg split% (P [

\Phi 

f

\Gamma 1

[r] : f 2 F; r 2 fSg

\Psi 

)

= fSg split% (P [

\Phi 

f

\Gamma 1

[S] : f 2 F

\Psi 

)

= (fSg split% P ) split%

\Phi 

f

\Gamma 1

[S] : f 2 F

\Psi 

= P split%

\Phi 

f

\Gamma 1

[S] : f 2 F

\Psi 

which is computable in O(kn + m) time by previous analysis. Later, we will
show how to reduce this cost to O(n + m).

PROGRAM DERIVATION 25
The error published in [60] occurred in the preprocessing steps to establish
the initial double partition T and Q. Finite differencing forces the preprocessing
and maintenance of T to serve the unified goal of establishing and maintaining
a single invariant, which makes errors of that kind less likely. It is fair to say,
though, that such errors are regarded as minor in the algorithms community,
and are routinely corrected. The benefits of our transformational approach may
pertain more to program development than to algorithm design. Yet in the next
subsection, we see how straightforward application of our transformations leads
to an improvement to Hopcroft's DFA minimization algorithm.

3.3. DFA Minimization
In [2] the problem of minimizing the number of states in a deterministic finite
state automaton is formulated exactly like specification (3.1), except that the
family F of relations is restricted to single-valued functions each totally defined
on S. Since the many function coarsest partition problem is just a special case
of the many relation coarsest partition problem, the program shown in Figure 5
will solve both problems.

The restriction to total functions leads to a startling simplification in the
preprocessing operations that establish invariant (3.4). Because each function
f 2 F is totally defined on S, we know that f

\Gamma 1

[S] = S; i.e., the preimage of

a function's range is its whole domain. Consequently, the right-hand side of the
preprocessing code,

(3:9) T := P split%

\Phi 

f

\Gamma 1

[S] : f 2 F

\Psi 

can be simplified, first to P split% fSg, and then directly to P by Proposition 2.7.
What this means is that all we have to do is read the input, store it in variable
T (i.e., make T an alias of P ), and the invariant T = F (fSg) is established for
free.

The restriction to total functions is not exploited in the remainder of the
derivation. If partial functions would be allowed, then the preprocessing code
(3.9), which takes O(kn + m) to compute, will establish the initial double partition for partial functions, as it did for relations. The remainder of the derivation
exploits single-valuedness of relations, and is correct for partial as well as total
functions.

We have already discussed one aspect of Hopcroft's "process the smaller half"
strategy. This strategy has a second aspect that works for functions (but not
relations), and that was exploited in Hopcroft's DFA minimization algorithm;
see [44]. Hopcroft's idea is based on the simplest form of Proposition 2.7 (9).
That is, if R is a partition of S, A ` S such that R split A = R, and B ` A such
that B 6= A, then

R split% fB; A \Gamma  Bg = R split% B = R split% (A \Gamma  B)
By the way that F (Q) is defined, we know that for any b 2 Q and any f 2 F,
we have F (Q) split% f

\Gamma 1

[b] = F (Q). Since preimages must be disjoint for any

26 J. P. KELLER AND R. PAIGE
two distinct elements in the range of a function (which is not true for binary
relations), we also have,

f

\Gamma 1

[b \Gamma  q] " f

\Gamma 1

[q] = fg

for any two sets q and b. Hence, the differential identity (3.7) can be simplified
to

F (Q split q) = F (Q) split%

\Phi 

f

\Gamma 1

[q] : f 2 F

\Psi 

so that the update code (3.6) can be simplified to

T = T split%

\Phi 

f

\Gamma 1

[q] : f 2 F

\Psi 

With this simplification, the earlier analysis of the cumulative time used in splitting operations on T is O((kn + m)log n).

The remainder of the derivation has to do with data structuring. The program shown in Figure 6 consolidates the preceding simplifications, and also implements the split% operations in terms of the more primitive split operations
with which it is defined. This is achieved with a SETL for-loop implementation
by application of what E. Schonberg (cited as Deak, [25]) calls a compilation
transformation.

The space improvement to Hopcroft's algorithm is achieved as follows. Represent the "thin" partition T as the preimages of a single-valued map T 1 (as
described in the last section); i.e., so that identity

T =

\Phi 

T 1

\Gamma 1

fqg : q 2 range T 1

\Psi 

holds. Represent "fat" partition Q indirectly by the preimages of a single-valued
map Q1 defined on the range of T 1; i.e., so that identity

Q =

\Phi 

T 1

\Gamma 1

[Q1

\Gamma 1

fbg] : b 2 range Q1

\Psi 

holds. If we also replace Q \Gamma  T , which appears at the top of the while-loop,
by the set

\Phi 

b 2 range Q1j#Q1

\Gamma 1

fbg ? 1

\Psi 

of "fat" blocks that contain more than

one "thin" block, we obtain the essence of the double partition implementation
used in [60]. The code that results from this shift in representation is shown in
Figure 7 and Figure 8.

The final algorithm is obtained by inputting the set F

0

of inverse maps (instead of F), and by maintaining and exploiting two more invariants throughout
the code in Figure 7 and Figure 8. The first is a map

count(q) = #T 1

\Gamma 1

fqg

of the sizes of each "thin" block q; the second is the set

ready =

\Phi 

b 2 range Q1j#Q1

\Gamma 1

fbg ? 1

\Psi 

The type assignments for the two new variables are

PROGRAM DERIVATION 27
-- maintain invariant T

^

\Gamma 

Q

Q := fSg
-- input partition, and establish invariant T = F (fSg)
while 9 b 2 (Q \Gamma  T ), 9 q 2 T jq ae b ^ #q

^

\Gamma 

#b=2 loop

Q := Q split q
for f 2 F loop

T := T split f

\Gamma 1

[q]

end loop
end loop

Figure 6. The result of "processing the smaller half."
count : smap(thin \Gamma  strong; IN T )
ready : set(fat \Gamma  strong)

Just before a value x is removed from a thin block by the modification T 1(x)
:= om, count is decremented by executing count(T 1(x)) \Gamma := 1. Just before a
thin block is augmented by the assignment T 1(x) := q, count is incremented
by executing count(q) +:= 1. Each time a call q := new(thin) creates a new
thin block q, count is initialized by executing count(q) := 0. Variable ready
is initialized to the empty set. After that, it must be maintained relative to
two operations. Just before a thin block is removed from a fat block by the
assignment Q1(q) := om, we perform the conditional update,

if #Q1

\Gamma 1

fQ1(q)g = 2 then

ready less := Q1(q)
end if

Just before a thin block is added to a fat block by the assignment Q1(q) := b,
we perform the conditional update,

if #Q1

\Gamma 1

fbg = 1 then

ready with := b
end if

The space improvement arises from keeping the size of the two auxiliary sets
ready and splitter low, essentially O(n). Hopcroft's workset was of size `(m) in
the worst case.

Although the algorithm just developed utilizes space efficiently for certain
auxiliary data structures, it also suffers from two sources of inefficiency. First,
the array representation of f

\Gamma 1

2 F

0

may have empty components. When #F

0

is large and partial functions are allowed, then the space needed to store all these
sparse maps may be excessive.

28 J. P. KELLER AND R. PAIGE

The second inefficiency is in the repeated computation of argument f

\Gamma 1

[q],

which is passed to the split routine at the bottom of Figure 7. This computation
involves iteration through q even though range f may intersect with only a small
subset of q. This problem contributes O(kn log n) steps to the running time.

We end this section with a solution to both problems, and an illustration of
how well our type system adds computational transparency to a set-theoretic
language. Instead of using a collection F

0

of arrays to store the inverses of

functions that belong to F, we will use a partial function

G : smap(node \Gamma  strong; mmap(symb; node))
defined by the rule,

G(x) =

\Phi 

[f; y] : f 2 F; y 2 f

\Gamma 1

fxg

\Psi 

for every x 2 S. Base type symb represents an alphabet with one symbol for each
distinct function that belongs to F. Only list structures are used to implement
G, and our algorithm will make no use of arrays. The space needed to store G
and base symb is O(m) and O(k) respectively.

Based on the definition of G, we know that the following two identities,

f

\Gamma 1

fxg = G(x)ff g

f

\Gamma 1

[q] = fy : x 2 q; y 2 G(x)ffgg

hold for any "thin" block q and any function f 2 F. Hence, for fixed q we
can compute auxiliary map bucket : mmap(symb \Gamma  strong; set(node)) by the
assignment,

bucket :=

\Phi 

[f; G(x)ff g] : x 2 q; f 2 domain G(x)

\Psi 

in order to encode preimage sets f

\Gamma 1

[q] for all f 2 F; i.e.,

1. domain bucket = ff 2 Fjrange f " q 6= fgg,
2. bucketffg =

\Phi 

f

\Gamma 1

fxg : x 2 q

\Psi 

for every f 2 domain bucket, and, hence,

3. f

\Gamma 1

[q] = fy : T 2 bucketff g; y 2 T g for every f 2 F.

The preceding analysis allows us to replace the iterated calls to split at the
bottom of Figure 7 by the equivalent, but more efficient, code shown in Figure 9.

The cost of computing bucket is O(#q +

P

x2q

#ff 2 Fjx 2 range f g. The

cost of the for-loop through domain bucket is dominated by the cumulative cost of
computing f

\Gamma 1

[q] for every f 2 domain bucket. This cost is

P

f2F

#f

\Gamma 1

[q]. Consequently, the total running time of the algorithm is O((m + n)log n). The space
needed to store the new auxiliary data bucket is O(

P

x2q

#ff 2 Fjx 2 range fg),

which starts out being as much as O(m=2) in the worst case, and gradually diminishes as "thin" blocks q get progressively smaller.

As a final clarification to the algorithmic analysis, we note that the preprocessing operations will partition the set fx 2 Sj(8f 2 Fjx =2 domain f)g into

PROGRAM DERIVATION 29
P : set(set(node)) -- the read method ensures that P is a partition
T 1 : smap(node \Gamma  strong; thin \Gamma  strong)
Q1 : smap(thin \Gamma  strong; fat \Gamma  strong)
F

0

: set(mmap(node \Gamma  strong; node)) -- set of inverse maps

-- input conversion
b := new(fat)
for p 2 P loop

q := new(thin)
Q1(q) := b -- corresponds to Q := fSg
for x 2 p loop

-- execution frequency n
T 1(x) := q -- corresponds to T := P
end loop
end loop
-- repeated partition refinement
while 9b 2

\Phi 

b 2 range Q1j#Q1

\Gamma 1

fbg ? 1

\Psi 

loop

q1 := 3 Q1

\Gamma 1

fbg

q2 := 3 (Q1

\Gamma 1

fbg less q1)

if #q1

^

\Gamma 

#q2 then

q := q1
else

q := q2
end if
Q1(q) := om -- remove q from b
Q1(q) := new(fat) -- make q a new `fat' block
for f

\Gamma 1

2 F

0

loop

split(T 1; f

\Gamma 1

[q])

end loop
end loop

Figure 7. Double partition algorithm.

30 J. P. KELLER AND R. PAIGE

-- call by reference
procedure split(rw T 1 : smap(node \Gamma  strong; thin \Gamma  strong); splitter : set(node))
twin : smap(thin \Gamma  strong; thin)
twin := fg
for x 2 splitter loop

-- execution frequency #splitter
qold := T 1(x)
qnew := twin(qold)
if qnew = om then
-- generate a new base element that `names' the new block

qnew := new(thin)
Q1(qnew) := Q1(qold)
twin(qold) := qnew
end if
if # T 1

\Gamma 1

fT 1(x)g = 1 then

Q1(T 1(x)) := om
end if
-- indexed assignment to T 1 is broken into two operations

T 1(x) := om --remove x from block T 1

\Gamma 1

fqoldg

-- we could execute delete(qold) if T 1

\Gamma 1

fqoldg = fg

T 1(x) := qnew --add x to block T 1

\Gamma 1

fqnewg

end loop
end split

Figure 8. Double partition algorithm.
separate blocks P

0

that will never be split again. The worst case time to do this is

O(m + n). Subsequently, each block in P

0

will be used at most once for splitting

other blocks. The total cost of such splitting is also O(m + n). As was observed
in [60], the number n

0

of elements in S outside of P

0

is at most m. Hence, if we

prune blocks that belong to P

0

from the forest (used in earlier analysis of the

"process the smaller half" strategy), then a more accurate time bound for our
algorithm is O(n + m + m log n). The read method for constructing the initial
data structures is left as an exercise.

Cardon and Crochemore used another strategy based on lexicographical sorting and on exploiting Proposition 2.7 (9) to obtain a DFA minimization algorithm
with similar asymptotic time and space bounds; see [14]. However, a finer parameterization would reveal that our algorithm has a theoretical and computational
advantage in space and time.

PROGRAM DERIVATION 31
-- low level code to compute bucket
bucket := fg
for x 2 q loop

-- execution frequency #q
for f 2 domain G(x) loop

-- execution frequency

P

f2F

#(q " range f)

bucketffg with:= G(x)ff g
end loop
end loop
-- iterated call to split
for f 2 domain bucket loop

-- execution frequency #ff 2 Fjrange f " q 6= fgg
splitter := fg
for T 2 bucketff g loop

-- execution frequency

P

f2F

#(q " range f)

for x 2 T loop

--execution frequency

P

f2F

#f

\Gamma 1

[q]

if x =2 splitter then

splitter with:= x
end if
end loop
end loop
split(T 1; splitter)
bucket(f) := om -- initialize bucket
end loop

Figure 9. Splitting with sparse data structures.
4. Mechanical Verification of a Finite Difference Transformation

In this section we go through the main steps of how the differential identity
(3.7) that supported the finite difference transformation in the last section was
formally proved within the NAP proof checking system. This requires a brief
discussion of the lattice properties of the set P(S) of all partitions over S. It
includes a systematic exposition of definitions, corollaries and simple lemmas,
which reproduces exactly the structure of the proof accepted by the NAP system.

Unlike a mathematician, who is likely to be satisfied with a less formal proof or
even a proof sketch, the NAP proof checker does not allow unstated hypotheses,
or proof gaps. All the elementary theorems in number theory, and naive set
theory needed in the proofs have to be formally expressed if not proved. We
will state precisely all the definitions we had to introduce, and all the derived

32 J. P. KELLER AND R. PAIGE
corollaries and lemmas. The background theory out of which arithmetics and
naive set theory are obtained is Hereditarily Finite Set Theory, which is assumed
to be the semantic foundation of SETL and its Cantor dialect. The NAP proof
checker uses the Cantor runtime system to directly evaluate the validity of simple
Boolean identities involving either constants or terms built out of constants (e.g.,
#f1; 3g = 2).

It is useful to restate some of the basic definitions and elementary properties
that were introduced earlier.

DEFINITION 4.1. If S is a finite set, then P is a partition of S if and only
if

1. P ` pow(S) \Gamma  ffgg (P is a collection of nonempty subsets of S.)
2. [=P = S (S is the union of all the sets in P .)
3. 8p; q 2 P jp 6= q ! p " q = fg (The elements of P are pair-wise disjoint.)

DEFINITION 4.2. The set P(S) of all partitions over S may be partially ordered as follows: If Q; P 2 P(S), then we say that Q is a refinement of P ,
denoted by Q

^

\Gamma 

P , if and only if 8q 2 Qj(9p 2 P jq ` p). The partially ordered

set (P(S);

^

\Gamma 

) has maximum element fSg and minimum element ffxg : x 2 Sg.

DEFINITION 4.3. If P is a partition of a finite set S, and a ` S (i.e.,
a 2 pow(S)). Then

P split a

def

= fs 2 pow(S)js 6= fg ^ (9p 2 P js 2 fp " a; p \Gamma  ag)g

PROPOSITION 4.4. Let P and Q be two partitions of finite set S, and let
a; b ` S.

1. P split a

^

\Gamma 

P (split is deflationary in its first argument)

2. If P

^

\Gamma 

Q, then P split a

^

\Gamma 

Q split a (split is monotone in its first argument)

3. P split a = P if and only if 9A ` P ja = [=A (splitting a partition has no

effect if and only if one splits by any union of elements of P)

4. P split a = P if and only if 8p 2 P jp ` a . p " a = fg
5. a = [=fx 2 P split ajx ` ag(Set a is exactly covered by a subset of P split a)
6. If p 2 P , a ` p, a 6= p, and a 6= fg, then P split a = (P \Gamma  p) [ fa; p \Gamma  ag

(Splitting P by a set contained within some block p 2 P is a local change
in which p is replaced by a and p \Gamma  a.)

7. If A ` P , A 6= fg, a = [=A, b ae a, and b 6= a, then P split b = (P \Gamma  A) [

A split b (Splitting by a small set affects only the blocks that intersect that
set.)

PROGRAM DERIVATION 33
8. (P split a) split b = (P split b) split a (The result of splitting with respect to

more than one set is the same regardless of the splitting order.)

A useful, simple consequence of Proposition 4.4 (6) is: if P is a partition of
S, a 2 P , b ae a, and b =2 P , then

P split b = P split (a \Gamma  b)
Throughout this section every function f : S ! S and relation E ` S \Theta  S
has S as its range and domain.

The first major step in the proof of differential identity (3.7) is to provide
a detailed account of the relationship between the split operation and the meet
operation over the P(S) semilattice.

DEFINITION 4.5. (Compound Operations) Let A, B be sets, and let F
be a binary operation A \Theta  B ! A. Let a 2 A, and let t be an ordered subset of
B. Then a F % t is defined recursively as:

1. a F % [] = a
2. a F % u with b = (a F % u) F b

We will formally define compound operations over unordered collections in
the following way. If for every two arbitrary enumerations t; t

0

of the same subset

of B, we can prove

a F % t = a F % t

0

then we define the following equivalence relation over ordered subsets of B:

t j t

0

if and only if fx : x 2 tg = fx : x 2 t

0

g

The equivalence classes of this relation correspond to subsets of B. In this
way we extend the definition of F % to unordered sets, and formally justify
Definition 2.6 of the compound operation split% applied to sets. The following
corollary characterizes Definition 4.5 of the compound operation formed from
the split operation introduced in Definition 4.3.

COROLLARY 4.6. (The Compound Split Operation on Ordered Sets)
Let P be a partition of S, and let t be an ordered collection of subsets of S. Then
s 2 P split% t if and only if

1. s 6= fg
2. there is a tuple v of subsets of S such that:

#v = #t + 1; v(1) 2 P; v(#v) = s; and
8 i 2 [1:::#t] j v(i + 1) 6= fg ^ v(i + 1) 2 fv(i) " t(i); v(i) \Gamma  t(i)g

34 J. P. KELLER AND R. PAIGE

The proof of Corollary 4.6 is a straightforward induction over the length of t.
It is interesting to note that the formal statement of Corollary 4.6 is the result of
many "approximations," i.e., iterations over versions differing very slightly from
one another in their wording. It was only while pursuing the systematic proof
construction with NAP's help that the defects of the successive versions were
revealed. The induction proof contains around four hundred elementary proof
steps.

Corollary 4.6 has several consequences, which are mentioned below.

COROLLARY 4.7. (Chaining) Let P, S, t be as in Corollary 4.6. If v is the
tuple associated with s in P split% t, then

1. 8 i 2 [1::#t] j v(i + 1) ae v(i)
2. 8 i 2 [1::#t] j v(i + 1) ae v(1)
3. if 9 j 2 [1::#t] j v(j + 1) = v(j) " t(j) then

ffl 8 i 2 [1::#t + 1] j i ? j ! v(i) ae t(j)

(N.B. i 2 [1::#t + 1] if and only if i 2 [1::#v])

ffl s ae t(j)

COROLLARY 4.8. (Compounding Split with a Partition -- 1) Let P,
S, t be as in Corollary 4.6. If v be the tuple associated with s in P split% t, then

1. If the elements of t are mutually disjoint, then

there is at most one j: j 2 [1::#t] j v(j + 1) = v(j) " t(j)

2. If [=t = [=P = S, then

there is at least one j: j 2 [1::#t] j v(j + 1) = v(j) " t(j)

(N.B. (1) and (2) are independent conditions entailing independent conclusions.)

If t satisfies the conditions (1) and (2) of the previous corollary, then t

0

s

members form a partition of S. We will then loosely state that t is a partition
of S.

COROLLARY 4.9. (Compounding Split with a Partition -- 2) Let P,
S, t be as in Corollary 4.6, and let t be a partition of S. Then the converse of the
previous corollary holds too; i.e.,

s 2 P split% t if and only if 9 p 2 P; q 2 t j s = p " q

COROLLARY 4.10. (Compounding Split with a Partition -- 3) Let P,
S be as in Corollary 4.6. If t and t' are two enumerations of the same partition
of S, then

s 2 P split% t if and only if s 2 P split% t

0

PROGRAM DERIVATION 35
The preceding corollary justifies an equivalence relation over ordered enumerations of the elements of partitions:

t j t

0

if and only if t and t

0

enumerate the same partition R of S

Consequently split% is well-defined for partitions of S:

P split% R = P split% t
where t enumerates R.

DEFINITION 4.11. (Meet Operation) Operation meet over partitions of
S is defined as

P meet R

def

= P split% R

This definition is motivated by the following corollary:
COROLLARY 4.12. Operation meet satisfies:

ffl Q

^

\Gamma 

A ^ Q

^

\Gamma 

B if and only if Q

^

\Gamma 

A meet B (meet takes its name from

this property)

ffl P meet P = P (idempotence)
ffl P meet R = R meet P (commutativity)
ffl (P meet R) meet U = P meet (R meet U ) (associativity)

Corollary 4.12 entails that the meet of any collection of partitions is welldefined, independently of any ordering of the collection, and has exactly the
property of the meet operation u over a semilattice. Consequently, compound
split is well-defined not only for ordered collections of subsets of S but for unordered collections too (see Definition 4.16 below).

DEFINITION 4.13. (Base Partition) Let a ae S. Consider the mapping Q:
pow(S) ! P(S) defined by the rule,

Q(a)

def

= fa; S \Gamma  ag if a 6= fg and a 6= S

fSg if a = fg or a = S

COROLLARY 4.14. If P is a partition of S, and a is a subset of S, then

P split a = P split% Q(a)

36 J. P. KELLER AND R. PAIGE

COROLLARY 4.15. (Compounding Split with a Set -- 1) Let P be a
partition of S, and let X be a set of subsets of S (i.e., X ` pow(S)). If t is a
tuple enumerating the elements of X, then

P split% t = P u% [Q(x) : x 2 t]

The proof of Corollary 4.15 is by induction on #X.

Given the associative and commutative properties of u, and since t enumerates X, we have

P u% [Q(x) : x 2 t] = P u% fQ(x) : x 2 Xg
If t and t

0

are two arbitrary enumerations of X, then

P split% t = P u% fQ(x) : x 2 Xg = P split%t

0

DEFINITION 4.16. (Compounding Split with a Set -- 2)
If P is a partition of S, and X is a set of subsets of S (i.e., X ` pow(S)),
then

P split% X = P u% fQ(x) : x 2 Xg

A simple application of Definition 4.16 is :

COROLLARY 4.17. (Splitting with Two Arbitrary Sets)
If P is a partition of S, and a,b are any two subsets of S, then

1. Q(a [ b) u Q(a) u Q(b) = Q(a) u Q(b)
2. P split%fa [ b; a; bg = P split%fa; bg

Note that (1) above may be easily verified directly, e.g., using

Q(a [ b)uQ(a)uQ(b) = Q(a [ b) split% fa; bg
and

Q(a)uQ(b) = Q(a) split b

However a direct verification of (2) without the use of Definition 4.16 does not
seem that obvious!

Another consequence of Definition 4.16 is:

COROLLARY 4.18. If P is a partition of S, and X,Y are two arbitrary subsets
of S, then

P split% (X [ Y ) = (P split% X) u (P split% Y )

PROGRAM DERIVATION 37
Mappings from S into S have simple categorical properties with regard to
the meet operation. Let us briefly recall some elementary properties of inverse
mappings. Let f: S ! S, and let a ` S. Then,

f

\Gamma 1

[a] = fx : x 2 S j f(x) 2 ag

A useful property of f

\Gamma 1

is: Let f: S ! S, and let P be a partition of S. Then

ff

\Gamma 1

[q] : q 2 P j f

\Gamma 1

[q] 6= fgg

is a partition of S.

This justifies the following notation.

DEFINITION 4.19. (Inverse-Partitioning) Let f: S ! S, and let P be a
partition of S.

f

\Gamma 1

(P )

def

= ff

\Gamma 1

[q] : q 2 P j f

\Gamma 1

[q] 6= fgg

This definition has the following consequences.
COROLLARY 4.20. Let f: S ! S, and let a ae S. Then,

f

\Gamma 1

(Q(a)) = Q(f

\Gamma 1

[a])

COROLLARY 4.21. Let f: S ! S, and let P and R be partitions of S. Then,

f

\Gamma 1

(P uR) = f

\Gamma 1

(P ) u f

\Gamma 1

(R)

We will proceed to prove differential identity (3.7) by exploiting properties of
split% in combination with properties of inverse mappings.

COROLLARY 4.22. Let g be a mapping pow(S) ! pow(S). Let P,Q be partitions of S. Let G be defined by the rule

G(Q)

def

= P split% fg(r) : r 2 Qg:

Let b in Q - G(Q) , a ae b, a =2 Q. If

g(b) = g(a) [ g(b \Gamma  a)
then

G(Q split a) = G(Q) split% fg(a); g(b \Gamma  a)g

Examples of such a g are the inverse map f

\Gamma 1

for a function f: S ! S,

or the inverse relation R

\Gamma 1

for a binary relation R ` S \Theta  S (N.B. R

\Gamma 1

[a] =

38 J. P. KELLER AND R. PAIGE
fx : [x; y] 2 R j y 2 ag). Such functions satisfy g(p [ q) = g(p) [ g(q) for
arbitrary p; q, not necessarily disjoint.

The differential identity (3.7) follows from the simple topological properties
of partitions stated in the lemma just below.

LEMMA 4.23. If R and Q are two distinct partitions of S, and R ! Q, then
1. q 2 Q \Gamma  R ! fx 2 Rjx ae qg 6= fg (every element of Q \Gamma  R contains at

least one element of R)

2. q 2 Q \Gamma  R ! #fx 2 Rjx ae qg

*

\Gamma 

2 (every element of Q \Gamma  R contains at

least two disjoint elements of R)

3. q 2 Q \Gamma  R ! 9x 2 Rjx ae q ^ #x

^

\Gamma 

#q=2

4. 9r 2 R \Gamma  QjR

^

\Gamma 

(Q split r) ! Q

5. q 2 Q \Gamma  R ! 9r 2 Rjr ae q ^ #r

^

\Gamma 

#q=2 ^ R

^

\Gamma 

(Q split r) ! Q

THEOREM 4.24. (Generalized Differential Identity)
1. Let g,P,Q,G be as in Corollary 4.22. Let F(Q) = G(Q) u Q, so that F(Q)

^
\Gamma 

Q. Let F(Q) ! Q, and let q and r be sets as those defined by Lemma 4.23

(4) or (5) applied to Q and R = F(Q), i.e., q 2 Q - F(Q) and r 2 F(Q) -
Q, r ae q. Then

F (Q split r) = F (Q) split% fg(r); g(q \Gamma  r)g

2. Let g

i

: i 2 [1..k] be mappings pow(S) ! pow(S). Let P,Q be partitions of

S, and let G satisfy

G(Q) = P split%fg

i

(r) : r 2 Q; i 2 [1::k]g

Let F(Q) = G(Q) u Q, so that F(Q)

^

\Gamma 

Q. If F(Q) ! Q, let q and r be sets

as those defined by Lemma 4.23 (4) or (5) applied to Q and R = F(Q),
i.e., q in Q - F(Q) and r 2 F(Q) - Q, r ae q. If

8i 2 [1::k] j g

i

(q) = g

i

(r) [ g

i

(q \Gamma  r)

(as in Corollary 4.22) then

F (Q split r) = F (Q) split% ([=ffg

i

(r); g

i

(q \Gamma  r)g : i 2 [1::k]g)

5. Overview of NAP
This overview describes the main features of NAP, the system used to construct the formal proof outlined in the last section. For more information see

PROGRAM DERIVATION 39
[48]. NAP is a simple reasoning system for constructing proofs in First Order
Logic in either interactive or batch mode. It is based on variants of two earlier systems, FOL, [90], and XCHECK, [83], that evolved during the 1970's and
early 1980's. As such, NAP incorporates no novel concept, and it lacks many
of the capabilities found in modern reasoning systems such as the Calculus of
Constructions, [19], Deva, [78], [5], and [88], ISABELLE, [61], HOL, [38], LP,
[37], Nqthm, [8], and many more. NAP was chosen because it was already implemented by the first author, it runs on his Macintosh computer, and it has
a syntax compatible with SETL. We were also vaguely skeptical that the capabilities currently incorporated in more sophisticated systems would help in any
major way.

We believe that our experiment in using NAP to combine mechanical verifi-
cation with algorithm design and program development within the derivation of
a difficult combinatorial algorithm is novel. There are a few related experiments
in the literature in which mechanical verification is combined with transformational programming. One example is Broy's use of Larch (see [41]) and its proof
assistant LP (see [37]) to derive a version of Quicksort by transformation; see
[9]. Like Broy's example, the other work that we have seen of this kind involves
derivations of fairly simple, known algorithms that have been studied extensively.

Because NAP has been designed to prove properties of programs written
in Cantor (see [47]), the syntax and semantics of NAP and Cantor are linked.
Axioms and theorems in NAP are Cantor Boolean expressions, and are parsed
with the Cantor parser. Since NAP is a dialect of SETL, it is specially equipped
for reasoning in Hereditarily Finite Set Theory.

Within NAP a formal language is for FOL predicate calculus, and is characterized by its non-logical symbols. Each non-logical symbol has to be declared
either as a predicate or a function, and has a specific arity. All undeclared
non-logical symbols are assumed to be variables.

Since quantification and set comprehension must be bounded in Cantor (as
in the SETL notation described earlier), only safe set expressions are allowed
in the sense of [13]. A special constant U that represents an infinite universal
set has been introduced exclusively to deal with unbounded iterators or quantifiers. NAP requires exists x in U j K(x) and fx : x in U j K(x)g instead of
exists x j K(x) and fx j K(x)g.

The well-formed formulas are inductively defined as the smallest set that
contains:

ffl (atomic formulas) R(t

1

; . . . ; t

k

), where R is a declared predicate of arity k,

and t

1

; . . . ; t

k

are terms,

ffl Cantor Boolean symbols applied to terms, provided that the axioms defining them are in the theory; e.g.,

- (in-)equality: =, /=
- set membership and containment: in, notin, subset, incs (substitutes

for 2; =2; ae; oe)

40 J. P. KELLER AND R. PAIGE

- arithmetic comparison: !;

^

\Gamma 

; ?;

*

\Gamma 

ffl the meta-formula in one variable, FM(x), which represents a predicate of

arity 1, which could be used in axiom schemes, e.g., for stating the Peano
induction rule, or the separation axiom of set theory.

Terms are inductively defined as the smallest set that contains:

ffl f(t

1

; . . .; t

k

), where f is a declared function symbol of arity k and t

1

; . . . ; t

k

are terms,

ffl variables (declared implicitly),
ffl constant U (of arity 0), which may appear only as part of an iterator or a

Boolean expression aterm 2 U (which is always deemed true), and

ffl Cantor set, tuple, and arithmetic expressions, provided axioms are introduced to define the built-in set and arithmetic functions that appear in
those expressions.

For a given formal language, a theory (which should include axioms for all
the non-logical operations and formal Boolean tests) is defined by presenting

ffl a list of abbreviations, i.e., the assignment of names to all the reference

formulas, which include the axioms, and theorems that have been derived
from these axioms,

ffl a list of axioms, and
ffl a list of theorems.

NAP supports natural deduction for proof construction.

ffl a proof has a goal (the theorem to be proved) stated in a given formal language within a given theory T, i.e., a declared set of axioms and previously
derived theorems -- presumably a consistent set

ffl a proof has a derivation, defined as a sequence of derivation steps.

Each proof step may be represented by a triple

(premises; operation; resulting formula)
and each step has a corresponding rank in the derivation sequence (e.g., the first
step is always the goal statement). The current step rank is maintained by the
NAP system. In written proof scenarios, current step rank may be represented
by the star symbol "*". The representation by triples is a further formalization
of the common way to represent a proof as a sequence of formulas:

f

1

; f

2

; . . .; f

n

= theorem

where each f

i

is either an element of the theory T or is derived by application of

a logical rule to formulas in f

1

, f

2

; . . . ; f

i\Gamma 1

or in T. At step i formula f

i

results

from applying an operation of the form

op arglist

PROGRAM DERIVATION 41
where op is an operator and arglist is a list of arguments. Each argument may
refer to formulas in T or to variable names using ranks of resulting formulas from
previous steps. For example, there is a rule

insert fla
where fla refers to a formula (either an axiom or a previously established theorem) that belongs to T. The formula that results from this operation is precisely
the text of the given fla.

Another rule has the form

modus ponens rk

a

rk

a\Gamma ?b

where rk

a

; rk

a\Gamma ?b

! current step rank represent formulas of ranks rk

a

and

rk

a\Gamma ?b

respectively. If these two formulas conform to the usual modus ponens

pattern

A
A ! B

then the resulting formula is B; otherwise, it is undefined.

There are several rules for transforming a formula by substitutions. One such
rule is,

substitute equ rk x = aterm

where rk ! current step rank designates a formula A obtained at derivation
step rk, where x represents a variable occurring free in A, and where aterm
is substitutable

1

for any free occurrence of x in A. The resulting formula is

A(x=aterm), which is obtained by substituting all the free occurrences of x by
the term aterm. Another substitution rule is

replace equ rk

eq

rk

where rk

eq

; rk ! current step rank. Rank rk

eq

designates an equality or

an equivalence formula, and rk is a previously obtained formula which should
be transformed according to the substitution defined by equality or equivalence.
This substitution requires exact term matching (in the case where rk

eq

designates

an equality) or an exact subformula matching (in the case where rk

eq

designates

an equivalence).

For convenience, working premises may be introduced freely using the operation

wp aF ormula

where aFormula is either a declared abbreviation or the text of a formula representing a hypothesis. Each such hypothesis will have to be subsequently "cancelled" either by proving it (e.g., as a separate lemma), by deriving a contradiction (i.e., proving its negation), or by deriving an implication. These three
cancellation methods appear as proof schemes in Appendix A.

1

According to [77] "[a term] a is substitutable for x in [formula] A if for each variable y

occurring in a, no part of A of the form 9 y B[or 8 y B] contains an occurrence of x which is
free in A."

42 J. P. KELLER AND R. PAIGE

The set of premises used to derive the resulting formula at each step i is
stored in the first component premises of the triple representing step i. The set
of premises at each step grows by inheritance. For example, if op derives formula
f from f

a

and f

b

(of ranks rk

f

a

and rk

f

b

respectively) at step rk, then

premises(rk) = premises(rk

f

a

) [ premises(rk

f

b

)

where premises(i) denotes the premises component of the i-th derivation step.

When at some derivation step of rank rk ? 1 the resulting formula is the
same as the goal (the first formula), and if premises(rk) is empty, then the NAP
system recognizes that the goal is proved. Otherwise, NAP signals a partial
proof completion, which requires that all the statements in premises(rk) must
be proved.

The "reasoning" power of NAP derives from its set of operations, which may
be subdivided into the following categories:

1. the inference rules,
2. the tautological rewriting rules,
3. the predicate calculus substitution rules,
4. specialized set-theoretic rules,
5. the proof schemes.
NAP is essentially a syntactic engine that compares, transforms, and writes wellformed formulas. The rules are therefore transformation rules. For the sake of
clarity, they are presented in Appendix A as sequent pattern formation rules.

In all these rules but the proof schemes, the set of premises, which results
from applying a rule at a derivation step, grows by inheritance, i.e., is just the
union of all the premises of the contributing steps. Hence, the rule that defines
the resulting set of premises is omitted. For proof schemes the full premises are
given in detail.

The rules found in Appendix A constitute a complete set of rules for predicate
calculus. These rules consist of a smaller subset that is complete (see [4]) plus
rules that are added for convenience.

The following mechanisms can upset this very standard architecture.

1. A declared theory is not proved sound. It is beyond NAP's power to

verify the consistency of submitted theories. NAP only supervises the
proof checking operations.

2. A working premise or wp operation lets us introduce "statements," i.e.,

assertions that must be verified by direct evaluation in Cantor. Examples
of such statements are

wp 1 in -1,2,3"";
wp 4 = 2+2;
wp #-"" = 0;
wp ['a','b','c'](1) = 'a';
wp forall x in [[],[],[]] -- #x = 0;

PROGRAM DERIVATION 43
An equivalent feature is described in [24] as part of what is called "the
verifier programming environment."

3. It is necessary to introduce new proof rules (new sequent patterns). The

rules for predicate calculus are clearly sufficient. However, the specific rules
for set theory and for dealing with SETL-like program expressions may
prove insufficient or inadequate. Indeed those rules are intended to capture
our understanding of set-oriented programming. The introduction of new
rules is an easy task from a software viewpoint. However, no generative
mechanisms have been introduced into NAP to discourage the introduction
of invalid rules. The required verification must be done outside NAP.

There are no other problematic forms of extensibility in NAP. New axioms
and new symbols are processed in standard ways -- by creating a new theory
declared as an extension of a given source theory. The new theory inherits all
the theorems of its source. However, conservative extensions are not considered
truly new theories, and adding (Henkin) constants does not require creation
of an extension. When new function or predicate symbols are added to the
formal language, however, an extension needs to be created. And this is just the
right thing to do, since adding such meaningful symbols always comes with the
definition of new axioms for these symbols.

NAP supports an interactive proof construction process, and a non-interactive
one, which we prefer to call a proof programming process. The rules listed in
Appendix A are the essential rules in the interactive mode. In that mode NAP
favors a transformational approach to pre-existing formulas, and discourages the
interactive definition of new formulas, unless it is absolutely necessary. This is
because purely syntactic errors, though easily recovered, are sometimes difficult
for an interactive user to distinguish from deeper logical errors. Editing formulas
by using the formal substitute equ or replace equ operations may be somewhat
clumsy, but is entirely safe. Consequently, interactive sessions characteristically
have a large number of purely clerical substitution steps.

NAP, drawing heavily on unification, supports forms of Obvious Logical Inferences (see [23]) for reducing the number of substitution steps required in a
derivation. This is obtained by specifying for a given operation, not only its
"source" formulas and parameters, but its target formula too. Unification of the
target formula with the expected result formula for a given rule defines transformations for the source formulas. If these transformations are valid substitutions,
NAP performs all the necessary substitutions. Schemes for these generalizations
are available for a number of the rules mentioned in Appendix A. These schemes
relax all the requirements for exact term or sub-formula matching in the corresponding rules. These facilities are only available in the proof programming
process (i.e., in the non-interactive mode).

5.1. Interactive Proof Construction Capabilities
The main proof construction capabilities of NAP are to:

ffl create a proof interactively, step by step:

44 J. P. KELLER AND R. PAIGE

- define a goal (a formula to prove within a given theory)
- insert axioms , established theorems, or conjectures
- derive new intermediate proof steps via built-in syllogisms (cf. the

tables in Appendix A)

- introduce formulas, abbreviations, auxiliary symbols, or comments
- detect full and conditional proof completion
ffl save (on file) a completed or uncompleted proof session
ffl review a session: restore (from file) an existing proof, and

- check it as a whole or by breakpointing one step at a time
- continue to construct the proof if it is not complete
ffl produce a session script: the session script contains both the proof script

and the proof trace; i.e., the actual list of formulas generated by the succession of derivation operations

Appendix B contains a proof script that illustrates this interactive step-bystep process. The theorem proved in Appendix B is well known, and was proved
mechanically before within the XCHECK system; see [83].

5.2. Proof Programming Capabilities
The main programming tools and capabilities of NAP are to:

ffl specify a theory declaration:

the declaration consists chiefly in a language definition, i.e., a list of nonlogical symbols, a list of abbreviations (i.e., identifiers for specific formulas),
and lists of axioms and already established theorems. Axiom and theorem
lists are lists of abbreviations terminated by a semi-colon.

ffl specify a proof script

- the required syntax is:

!proof step? ::= !derivation.operation? !Arglist? ;

[ @ comment.text @ ]

- several quoting forms are needed to distinguish strings from logic assertions, comments, and step labels, e.g.:

\Lambda  logic formula

$x/=y and Is.a.Partition(p) impl x*y = -"" $
\Lambda  comment

@ members of a partition are mutually disjoint @
\Lambda  label

:disjoint.0:

- arguments in arglist are formulas, labels, relative addresses (e.g., *-2),

integer constants (e.g., the rank of a term)

PROGRAM DERIVATION 45
ffl compile theory and script into a session: rudimentary syntactic error detection seems sufficient

ffl review the session: as in the interactive mode
ffl debug the proof: modify a script, recompile, review

Appendix C contains the theory declaration and proof text used to generate
a script equivalent to that shown in Appendix B.

Improving Readability with Obvious Logical Inferences
The available obvious logical inference mechanisms could have shortened the
proof in Appendix C. For example, the two instructions at the end of the "esias"
lemma:

comm.iff. 1 2 :SET.empty: ;
replace.equ * - 1 * - 2;

@ QED (CQFD)@

could be replaced by a single one:

replace.equ :SET.empty: * - 1

$ Set(-"")$ ;
@ QED (CQFD)@

Likewise, the following sequence of three operations:

:SET: insert SET ; @ B0 satisfies Set(B0) (cf. 10 above)@
substitute.equ $y = B0$ * - 1 ;

@ the axiom SET defines the consequences@
replace.equ *-1 :separ1:;

could be shortened to a single one:

replace.equ SET, :separ1:

$(exists x in U -- x in B0) or B0 = (-"")$ ;
@ B0 satisfies Set(B0) (cf. 10 above)@

Based on our experience, we believe that obvious logical inference mechanisms
can reduce the length of complex proof programs by at least thirty percent.

Proof Engineering
The same theory declaration may be used as the starting point for constructing proofs of several distinct theorems. For each such theorem it may be convenient to augment the original theory declaration by introducing new abbreviations, or by inserting a theorem in the theorems declaration. When submitting
a proof program to the NAP proof-checker, the display resembles the example
of a session transcript in Appendix B. Comments are listed in an extra column
on the right of the column labeled "formula."

46 J. P. KELLER AND R. PAIGE

The examples found in the appendix illustrate some of the possibilities of
engineering modular proofs in incremental steps: proof engineering is closely
related to software engineering! NAP features could be used, for instance, to
structure proofs in accordance with Lamport's suggestions; see [49]. See [48] for
a detailed step-wise proof construction and hierarchical refinement. Criteria for
a convincing proof are:

ffl it is simple

- i.e., the proof follows an understandable thread, and is modular
- the collated comments constitute an acceptable informal proof
ffl it is completed

- NAP accepts a new theorem when a proof is completed
- NAP detects partial completion when a goal is proved, and unproved

working premises remain

Modularity means:

ffl separation of concerns

- expell from the main proof thread all intermediate results that could

be derived from other theories defined in the system

- create independent lemmas for each derivation that is potentially reusable

Re-usability and portability are achieved in very simple ways:

ffl copy-paste

- use a labeling convention for specific proof-steps
- use relative addressing in referring to proof-steps within argument

lists

ffl theory update

- update a theory "theorems" list, insert abbreviations, and declare a

new theory that inherits a limited number of proved theorems.

For instance, after proving the "empty set theorem," the theory declaration may be augmented by:

theorems:

empty.set.thm esias
;

- keep theory declarations small by removing previously established theorems that are not effectively used in a given proof

One of the keys to proof design is:

ffl suspending disbelief

PROGRAM DERIVATION 47
- create a proof thread delineated by working premises (conjectures)

that can trigger a partial completion detection whenever a goal or
subgoal theorem is proved

- prove each working premise in the proof thread independently
- design an informal proof on paper before using NAP.

Areas of Current Research
NAP uses no resolution or tactics. More experience is needed to determine
whether these features are essential for the specific goal of NAP, which is to
provide a formal proof write-up after an informal one is available. Another
problem concerns induction. Induction proofs are essential, but they involve lots
of details. Establishing these proofs is as difficult as defining bounds in loops or
recursion, and guaranteeing termination. It is not yet clear how induction can
be simplified within NAP.

The main problem for future development of NAP is the development of a
theory database management system that monitors,

ffl the consistency of a dynamically changing set of independently evolving

theories, each one potentially formed by inheritance from other theories,
and

ffl multiple conservative extensions of a theory.

6. Conclusion
It remains a utopian dream to maintain software exclusively at the abstract
specification level, and to develop software solely by mapping mathematical problem specifications into efficient code by mechanically verified transformations.
Although a comprehensive framework exists to fulfill this "dream" (see [41], [9],
[62], [63], and [65]), the extreme benefits would be offset by even more extreme
costs using existing technology. Pepper and Wirsing say that verification activities would contribute the major costs, and that these costs would make a fully
formal approach infeasible with current technology. They "accept the fact that
a great part of the development process has to be carried out on a more or less
informal level," and a rigorous mathematical style (paper and pencil proof) is
most likely to be used for proving the most difficult derivational steps.

Nevertheless, progress is being made within the academic community, and a
rudimentary formal software technology has been utilized in practical settings
within industry. Large projects like VDM, [6], Larch, [41], and KORSO, [63],
or derived efforts like KIV, [65], have clarified issues and have provided well defined frameworks for implementing "sound" and "robust" program development
systems.

Within industry semi-formal graphical representations of sets and maps are
used by CASE tools to model data using the Entity-Relationship approach (see

48 J. P. KELLER AND R. PAIGE
[16]) or OMT; see [67]. They are also used to define functional specifications
with Data-Flow diagrams (see [91]), and to develop real-time specifications in a
synchronous system model by Statecharts; see [42]. Case tools used within these
software engineering approaches also have an informal capability to transform
these initial specifications. In particular, it is standard nowadays for integration
testing applications to be hand-derived with machine assistance from the kinds
of semi-formal specifications just described.

Our approach to formal program development is within the framework of
Pepper and Wirsing, but is distinguished from their work in several ways. Our
application domain is limited to the implementation of combinatorial nonnumerical algorithms. Algorithm implementation is extremely labor intensive, and
is growing in importance as the demand for computer technology shifts from
easier applications to more complex algorithmic ones; e.g., optimizing compilers for supercomputers, intricate data structures to implement efficient solutions
to operations research problems, search and analysis algorithms in genetic engineering, complex tools for personal workstations, systems for computer-aided
manufacturing, and tools for computer-aided software engineering (CASE). It
is also a difficult problem that is not solved by current CASE tools and software management disciplines, which are oriented towards data processing and
other applications, where the implementation and a prediction of its resource
utilization follow more directly from the specification.

Our transformational methodology has been tailored to algorithms. The three
basic transformation schemas (dominated convergence, finite differencing, realtime simulation) capture basic principles of algorithm design, and are guided by
complexity considerations. The use of top-down refinement to transform specifications from higher to lower levels of abstraction allows us to always focus on
local contexts for extracting the semantic detail needed to justify transformations. The expressive power of full set theory is used to model complex data
structures and algorithmic strategies. We use a set-theoretic language to specify
every abstract level of specification from the initial mathematical problem statement to the final implementation. The final implementation, which is written
in terms of set-theoretic primitives, is mapped into a conventional architecture
using a type system that depends only on the model of computation and that
conceals the actual low level implementation language. The ability to restrict our
attention to local contexts in a set-theoretic language allows us also to restrict
our logic to set-theoretic semantics, and to ignore the idiosyncratic semantics
of a low level language (whose semantics would be important if global program
analysis were needed, see [20] and [22]).

An important objective of this work has been to overcome a serious shortcoming in the APTS transformational system. Currently, all transformations
used within APTS preserve program semantics, but this is only proved on paper
outside the system. Since APTS is capable of unbridled production of dangerously large quantities of code without any human intervention or oversight, the
absence in APTS of machine-assisted meta-level support to prove transformaPROGRAM DERIVATION 49
tions correct is a major deficiency that needs to be overcome. Our experiment
with NAP indicates that this shortcoming can be overcome.

However, our experiment failed to demonstrate that this approach is practical.
We have mechanically proved the hardest derivational steps. We also showed how
the derivation of a high level bisimulation algorithm actually led to an improved
algorithm for DFA minimization. This kind of derivational reuse saved us a
great deal of labor in mechanical verification. But approximately 7,000 lines of
NAP text were used to define the theory, and to work out the whole proof of
identity (3.7). We believe that this amount of "proofware" represents about ten
times the size of a legible mathematical proof. We would hope that a high level
language approach to proof construction and reuse would help. It would also
be intriguing to find out whether fast decision procedures for fragments of set
theory (see [13]) could be utilized to automate and shorten our proofs.

Acknowledgements.
J. P. Keller was partially supported by ONR grant N00014-93-1-1036 and
by the Italian P.F. CNR, which funded an earlier implementation of a proof
construction system called Etna. R. Paige was supported by ONR grant N00014-
93-1-0924, AFOSR grant AFOSR-91-0308, and NSF grant MIP-9300210. Part
of this research was done while R. Paige was visiting DIKU at the University of
Copenhagen.

We are grateful to Carolyn Talcott for pointing out several errors in an earlier
draft of the paper. Eugenio Omodeo shared several insights that helped to clarify
our thinking and improve the paper. The set of inference operations of NAP have
been largely inspired by D. Cantone and A. Ferro of the University of Catania
in Italy.

50 J. P. KELLER AND R. PAIGE

Appendix: A. NAP Rules and Schemes
1. The Inference Rules
rule name from . . . infer . . .
modus ponens A, A ! B B
deny the disjunct B . A, :A B
disjunctive syllogism A . B, A ! C, B ! D C . D
deny the consequent B ! A, : A : B
hypothetical syllogism A ! B, B ! C A ! C

2. The Tautological Rewriting Rules
rule name from . . . infer . . .
double negation insert A : : A
double negation drop : : A A
definition of imply A ! B :A . B

:A . B A ! B

distribute negation :( A ! B ) A ^:B
over imply A ^:B :( A ! B )

contrapositive A ! B :B ! :A

A iff B :A iff :B

commute equivalents A

1

. . . op A

i

op . . . A

1

. . . op A

j

op . . .

commute equals . . . op A

j

op . . . A

n

. . . op A

i

op . . . A

n

commute conjuncts if op 2

commute disjuncts fiff, =; ^; .g

commute different A

1

6= A

2

A

2

6= A

1

law of biconditional A iff B (A ! B) ^ (B ! A)
form a conjunction A

1

; A

2

; . . . ; A

n

A

1

^ A

2

^ . . . ^ A

n

extract a conjunct A

1

^ A

2

^ . . . ^ A

i

A

i

^ . . . ^ A

n

form a disjunction A A . B
(expansion rule)

PROGRAM DERIVATION 51
3. The Predicate Substitution Rules
rule name from . . . infer . . .
commute under 8 (resp 9) . . . 8 (resp 9) . . .
quantifier x

i

2 S

i

. . . x

j

2 S

j

x

j

2 S

j

. . . x

i

2 S

i

. . . j K(x

1

; . . . ; x

n

) . . . j K(x

1

; . . . ; x

n

)

if x

k

(k

*

\Gamma 

i) doesn't occur

in S

i

nor in S

j

quantifier :8 (resp 9) x

1

2 S

1

. . . :9 (resp 8) x

1

2 S

1

. . .

negation j K(x

1

; . . .; x

n

) j :K(x

1

; . . . ; x

n

)

universal 8x 2 S j K(x) c 2 S ! K(c)
specification if c is any constant

8 . . . x

i\Gamma 1

2 S

i\Gamma 1

; 8 . . . x

i\Gamma 1

2 S

i\Gamma 1

;

x

i

2 S

i

; x

i+1

2 S

i+1

; x

i+1

2 S

i+1

;

. . . x

n

2 S

n

j K(x

1

; . . . ; x

n

) . . . x

n

2 S

n

j c 2 S

i

!

if c is any constant K(x

1

::x

i\Gamma 1

; c; x

i+1

; . . .; x

n

)

existential 9x 2 S j K(x) c 2 S ^ K(c)
specification if c is a Henkin constant

for this existential formula

universal K 8x 2 U j K
generalization if x is free in K

x 2 S ! K 8x 2 S j K
if x is free in K and doesn't occur in S

existential K 9x 2 U j K(t=x)
generalization if t is a term which occurs in K and

x is a variable substitutible to t in K

t 2 S ^ K 9x 2 S j K(t=x)
if t is a term which occurs in K and
x is a variable substitutible to t in K

52 J. P. KELLER AND R. PAIGE

3. The Predicate Substitution Rules (continued)
rule name from . . . infer . . .
replace equivalents A iff B; C C(A=B)

if A is a subformula of C and replace all occurrences of A by B
all the free variables of B while substituting the terms
other than those occurring in A corresponding to the free
do not occur in C variables common to A and B

replace equals t = u; A A(t=u)

if t, u are terms, x is a variable
not occurring in A, t or u and if
t occurs in A and u is substitutible
to x in A(t/x)

substitute equals A A(t=x)

if t is any term substitutible
to x in A

substitute FM F M (x) iff aF la; A A(F M=aF la)

if x is free in aFla and unify, i.e., for each
FM occurs in A and if every occurrence of FM(u) in A
free variable of aFla does substitute an instance of
not occur bound in A aFla(x=u)

4. The Predicate Insertion Rules
rule name from . . . infer . . .
let equ if t is a variable symbol t = aT erm

not occurring free in aTerm

if f is a new formula abbreviation f iff aF la
and does not occur in aFla

insert axiom if A is a known axiom A

or theorem of the current theory

PROGRAM DERIVATION 53
5. Specialized Set-Theoretic Rules
rule name from . . . infer . . .
extensionality x = t

1

. x = t

2

. :: . x = t

n

x 2 ft

1

; t

2

; . . .; t

n

g

x 2 ft

1

; t

2

; . . . ; t

n

g x = t

1

. x = t

2

. :: . x = t

n

K(a

1

) ^ K(a

1

) ^ :: ^ K(a

n

) 8x 2 fa

1

; a

2

; . . .; a

n

gjK(x)

K(a

1

) . K(a

1

) . :: . K(a

n

) 9x 2 fa

1

; a

2

; . . .; a

n

gjK(x)

if x is substitutible to
a

1

;a

2

;. . . ;a

n

in K

8x 2 fa

1

; a

2

; . . . ; a

n

gjK(x) K(a

1

) ^ K(a

2

) ^ :: ^ K(a

n

)

9x 2 fa

1

; a

2

; . . . ; a

n

gjK(x) K(a

1

) . K(a

2

) . :: . K(a

n

)

Recall that current step rank, denoted by ?, is the rank of the current derivation step, and that rk(A) is the rank in a derivation of formula A, i.e., the rank
of the derivation step in which A is the resulting formula.

6. The Proof Schemes
rule name from . . . infer . . .
conditional proof A, B A ! B

premises(?) :=
premises(rk(A)) [
premises(rk(B))
less rk(A) less rk(B)

working premise let the current resulting A
(postponed lemmas, conjectures, formula A be a premise premises(current step rank)
absurd hypothesis, directly i.e., a working hypothesis := fcurrent step rankg
evaluable assertion)

indirect proof A, : A :B
(proof by contradiction) if rk(B) 2 premises(rk(: A)) premises(?) :=

i.e., if B is a working hypothesis premises(rk(A)) [
presiding to the detected premises(rk(B))
contradiction less rk(B)

induction proof A

i

0

^ natural(i) A

i

p

(Peano's induction) ^ i

*

\Gamma 

i

0

premises(?) :=

^A

i

! A

i+1

premises(rk(A

i

0

)) [

let B be the formula : premises(rk(B))
i

*

\Gamma 

i

0

^ A

i

! A

i+1

less rk(A

i

0

) less rk(B)

and let rk(B) be its rank

54 J. P. KELLER AND R. PAIGE

Appendix: B. Proof Script For The Emptyset Theorem

zf0-id card
------------------------------------------------------------------------
-- -- source -- symbols --
-- -- -- --
-- -- FOLE -- U (Func 0) notin (Pred 2) Set (Pred 1) FM (Pred 1) in ( --
-- -- -- Pred 2) /= (Pred 2) = (Pred 2) --

------------------------------------------------------------------------
-- -- abbrev. -- axioms & theorem statement --
-- -- -- --
-- -- idemOr -- FM(x) iff FM(x) or FM(x) --
-- -- SET -- Set(y) iff (exists x in U -- x in y) or y = (-"") --
-- -- absence -- x notin y iff not (x in y) --
-- -- SEPARATION -- Set(A) impl (exists B in U -- Set(B) and (forall x --
-- -- -- in U -- x in B iff x in A and FM(x))) --
-- -- inequality -- x /= y iff not (x = y) --
-- -- equality -- x = x --
-- -- idemAnd -- FM(x) iff FM(x) and FM(x) --

------------------------------------------------------------------------
-- -- premi -- nb -- operation -- formula --
-- -- ses -- -- -- --
-- -- -- -- -- --
-- -- -- 1 -- play: zf0 --- -- forall x in U -- x notin (-"") --
-- -- -- 2 -- insert SEPARATION -- Set(A) impl (exists B in U -- --
-- -- -- -- -- Set(B) and (forall x in U -- x --
-- -- -- -- -- in B iff x in A and FM(x))) --
-- -- -- 3 -- substitute equ A = -- Set(-"") impl (exists B in U -- --
-- -- -- -- (-"") 2 -- Set(B) and (forall x in U -- --
-- -- -- -- -- x in B iff x in (-"") and FM(x --
-- -- -- -- -- ))) --
-- -- -- 4 -- substitute equ FM( -- Set(-"") impl (exists B in U -- --
-- -- -- -- x) iff not (x = x) -- Set(B) and (forall x in U -- --
-- -- -- -- 3 -- x in B iff x in (-"") and not --
-- -- -- -- -- (x = x))) --
-- -- 5 -- 5 -- wp -- Set(-"") --
-- -- 5 -- 6 -- modus ponens 4 5 -- exists B in U -- Set(B) and (f --
-- -- -- -- -- orall x in U -- x in B iff x i --
-- -- -- -- -- n (-"") and not (x = x)) --
-- -- 5 -- 7 -- ES B = B0 6 -- B0 in U and (Set(B0) and (for --
-- -- -- -- -- all x in U -- x in B0 iff x in --
-- -- -- -- -- (-"") and not (x = x))) --
-- -- 5 -- 8 -- conjunct 2 7 -- Set(B0) and (forall x in U -- --
-- -- -- -- -- x in B0 iff x in (-"") and not --

PROGRAM DERIVATION 55
-- -- -- -- -- (x = x)) --
-- -- 5 -- 9 -- conjunct 2 8 -- forall x in U -- x in B0 iff x --
-- -- -- -- -- in (-"") and not (x = x) --
-- -- 5 -- 10 -- conjunct 1 8 -- Set(B0) --
-- -- 11 -- 11 -- wp -- exists x in U -- x in B0 --
-- -- 11 -- 12 -- ES x = c 11 -- c in U and c in B0 --
-- -- 11 -- 13 -- conjunct 2 12 -- c in B0 --
-- -- 5 -- 14 -- US x = c 9 -- c in U and (c in B0 iff c in --
-- -- -- -- -- (-"") and not (c = c)) --
-- -- 5 -- 15 -- conjunct 2 14 -- c in B0 iff c in (-"") and not --
-- -- -- -- -- (c = c) --
-- -- 11 5 -- 16 -- replace equ 15 13 -- c in (-"") and not (c = c) --
-- -- -- 17 -- insert equality -- x = x --
-- -- -- 18 -- substitute equ x = -- c = c --
-- -- -- -- c 17 -- --
-- -- 11 5 -- 19 -- conjunct 2 16 -- not (c = c) --
-- -- 5 -- 20 -- indir proof 11 19 -- not (exists x in U -- x in B0) --
-- -- -- -- 18 -- --
-- -- -- 21 -- insert SET -- Set(y) iff (exists x in U -- x --
-- -- -- -- -- in y) or y = (-"") --
-- -- -- 22 -- substitute equ y = -- Set(B0) iff (exists x in U -- --
-- -- -- -- B0 21 -- x in B0) or B0 = (-"") --
-- -- 5 -- 23 -- replace equ 22 10 -- (exists x in U -- x in B0) or --
-- -- -- -- -- B0 = (-"") --
-- -- 5 -- 24 -- deny disjunct 23 2 -- B0 = (-"") --
-- -- -- -- 0 -- --
-- -- 5 -- 25 -- replace equ 24 20 -- not (exists x in U -- x in (-"" --
-- -- -- -- -- )) --
-- -- 5 -- 26 -- Quant Neg 25 -- forall x in U -- not (x in (-"" --
-- -- -- -- -- )) --
-- -- -- 27 -- insert absence -- x notin y iff not (x in y) --
-- -- -- 28 -- substitute equ y = -- x notin (-"") iff not (x in (- --
-- -- -- -- (-"") 27 -- "")) --
-- -- -- 29 -- comm equiv 1 2 28 -- not (x in (-"")) iff x notin ( --
-- -- -- -- -- -"") --
-- -- 5 -- 30 -- replace equ 29 26 -- forall x in U -- x notin (-"") --
---?**** you've just proved, based upon 1 yet unproved assumption(s)
the Lemma: empty.set.thm!!!
-- -- -- 31 -- play zf0 -- Set(-"") --
-- -- -- 32 -- substitute equ y = -- Set(-"") iff (exists x in U -- --
-- -- -- -- (-"") 21 -- x in (-"")) or (-"") = (-"") --
-- -- -- 33 -- substitute equ x = -- (-"") = (-"") --
-- -- -- -- (-"") 17 -- --
-- -- -- 34 -- form a disj 33 exi -- (-"") = (-"") or (exists x in U --
-- -- -- -- sts x in U -- x in ( -- -- x in (-"")) --
-- -- -- -- -"") -- --
-- -- -- 35 -- comm equiv 1 2 34 -- (exists x in U -- x in (-"")) o --
-- -- -- -- -- r (-"") = (-"") --
-- -- -- 36 -- comm equiv 1 2 32 -- (exists x in U -- x in (-"")) o --

56 J. P. KELLER AND R. PAIGE
-- -- -- -- -- r (-"") = (-"") iff Set(-"") --
-- -- -- 37 -- replace equ 36 35 -- Set(-"") --
---?**** ee.is.a.set : Set(-"") is proved!!!
---?**** empty.set.thm : forall x in U -- x notin (-"") is proved!!!

Appendix: C. NAP Proof Specification For The Emptyset Theorem

The following is the theory declaration used by NAP to generate the script
in Appendix B.

theory:

name:zf0
source:FOLE
Symbols:

U (Func 0)
Set (Pred 1)
notin (Pred 2)
/= (Pred 2)
= (Pred 2)
in (Pred 2)
FM (Pred 1)
c (Symb 0)
B0 (Symb 0)
;
abbreviations:

equality := (x = x);
SEPARATION := (Set(A) impl (exists B in U --
(Set(B) and

(forall x in U -- ( (x in B) iff

( (x in A) and FM(x)))))));
inequality := ( (x /= y) iff not ( (x = y)));
empty.set.thm := forall x in U -- (x notin (-""));
SET := (Set(y) iff ((exists x in U -- (x in y)) or (y = (-""))));
esias := Set(-"");
idemOr := (FM(x) iff (FM(x) or FM(x)));
absence := ( (x notin y) iff not ( (x in y)));
idemAnd := (FM(x) iff (FM(x) and FM(x)));
axioms:

inequality SEPARATION equality idemAnd absence idemOr SET

;

The following is the proof text that is used by NAP to generate the main
part of the script in Appendix B.

PROGRAM DERIVATION 57
theorem:

proof: empty.set.thm

theorem zf0 --- empty.set.thm $forall x in U -- (x notin (-""))$ ;

@ no element is a member of -""@
insert $SEPARATION$ ; @ separation is the key axiom here@
substitute.equ $FM(x) iff not (x = x)$ * - 1 ;

@ FM(x) is never satisfied@
substitute.equ $A = (-"")$ * - 1 ;

@ take an A for which Set(A) holds@
wp $Set(-"")$ ; @ Set(-"") is easy to verify (see below)@
modus.ponens * - 2 * - 1 ;

@ B is the empty set, defined in comprehension by
an un-satisfiable formula@
ES $B = B0$ * - 1 ; @ let B0 be an object like B@
conjunct 2, * - 1 ;
:separ1: conjunct 1, * - 1 ;@ Set(B0) @
:separ2: conjunct 2, * - 2 ;@ every element of B0 satisfies FM@
:absurd: wp $exists x in U -- (x in B0)$ ;

@ let's assume B0 has an element@
ES $x = c$ * - 1 ; @ let it be c@
conjunct 2, * - 1 ;
US $x = c$ :separ2: ;
statement $ (c in U)$ ;
modus.ponens * - 2 * - 1 ;
replace.equ *-1 *-4 ;

@ drop c in B0 to get c in (-"") and not (c = c)@
:eq: insert $equality$ ;

@ c satisfies FM, whence violates equality...@
substitute.equ $x = c$ * - 1 ;
conjunct 2, * - 3 ; @ contradiction with equality@
:non.absurd: indir.proof :absurd: * - 1 * - 2 ;

@ B0 may not have a member!@
:SET: insert $SET$ ; @ B0 satisfies Set(B0) (cf. 9 above)@

substitute.equ $y = B0$ * - 1 ;

@ the axiom SET defines the consequences@
replace.equ *-1 :separ1:

$(exists x in U -- x in B0) or B0 = (-"")$ ;
deny.disjunct * - 1 :non.absurd: ;

@ B (i.e., its instance B0) is really the empty set
(cf. 6 above)@
replace.equ *-1 :non.absurd:;

@ and *-2 above says explicitly! there is no element in B0@
Quant.Neg * - 1 ;
:almost: dbl.negation.drop * - 1 ;

insert $absence$ ;

@ the predicate notin is defined by the 'absence' axiom@
substitute.equ $y = (-"")$ * - 1 ;
comm.iff. 1 2 ,* - 1 ;

58 J. P. KELLER AND R. PAIGE

replace.equ * - 1 :almost: ; @almost QED@
play zf0 --- esias $Set(-"")$ ;

@ esias stands for: e-mpty s-et i-s a- s-et @
:SET.empty: substitute.equ $y = (-"")$ :SET: ;

substitute.equ $x = (-"")$ :eq: ;

@ equality: the 1st term of a disjunct@
form.a.disj * - 1 $exists x in U -- x in (-"")$ ;

@ the 2nd term may be anything, since the 1st term holds@
comm.or. 1 2 ,* - 1 ;
comm.iff. 1 2 :SET.empty: ;
replace.equ * - 1 * - 2;

@ QED (CQFD)@

Bibliography
[1] ADA UK News, Vol. 6, No. 1, January 1985, pp. 14-15.
[2] Aho, A., Hopcroft, J., and Ullman, J., Design and Analysis of Computer Algorithms,

Addison-Wesley, Reading, Massachusetts, 1974.
[3] Bauer, F., and Wossner, H., Algorithmic Language and Program Development, SpringerVerlag, Berlin, 1982.
[4] Bell, J. L., and Slomson, A. B., Models and Ultraproducts, North-Holland, Amsterdam,

1969.
[5] Biersack, M., Raschke, R., and Simons, M., The DevaWEB System, Technical Report

93-39, Technische Universit"at Berlin, December 1993.
[6] Bjorner, D., and Jones, C., VDM'87, Springer-Verlag, Berlin, 1987.
[7] Bloom, B., and Paige, R., Transformational design and implementation of a new efficient

solution to the ready simulation problem, Sci. Comput. Programming 24, No. 3, 1995,
pp. 189-220.
[8] Boyer, R., and Moore, J., A Computational Logic, Academic Press, New York, 1979.
[9] Broy, M., Experiences with Software Specification and Verification Using LP, the Larch

Proof Assistant, Research Report 93, DEC Systems Research Center, Palo Alto, California, 1992.
[10] Burstall, R., and Darlington, J., A transformation system for developing recursive programs, J. ACM 24, No. 1, January 1977, pp. 44-67.
[11] Cai, J., Facon, P., Henglein, F., Paige, R., and Schonberg, E., Type transformation

and data structure choice, pp. 126-124 in: Constructing Programs From Specifications,
B. Moeller, ed., North-Holland, Amsterdam, 1991.
[12] Cai, J., and Paige, R., Program derivation by fixed point computation, Sci. Comput.

Programming 11, No. 3, 1988/1989, pp. 197-261.
[13] Cantone, D., Ferro, A., and Omodeo, E., Computable Set Theory, Clarendon Press, Oxford, 1989.
[14] Cardon, A., and Crochemore, M., Partitioning a graph in o(jajlogjvj), Theoret. Comput.

Sci. 19, 1982, pp. 85-98.
[15] Cheatham, T., and Wegbreit, B., A laboratory for the study of automating programming,

pp. 208-211 in: Proceedings AFIPS 1972 Spring Joint Computer Conference, 1972.
[16] Chen, P., The entity-relationship model -- toward a unified view of data, ACM TODS 1,

No. 1, 1976.

PROGRAM DERIVATION 59
[17] Cocke, J., and Kennedy, K., An algorithm for reduction of operator strength, CACM 20,

No. 11, November 1977, pp. 850-856.
[18] Cocke, J., and Schwartz, J., Programming Languages and Their Compilers, Lecture Notes,

Courant Institute, New York University, New York, 1969.
[19] Coquand, T., and Huet, G., The calculus of constructions, Inform. and Comput. 76, 1988,

pp. 95-120.
[20] Cousot, P., and Cousot, R., Abstract interpretation: A unified lattice model for static

analysis of programs by construction or approximation of fixed points, pp. 238-252 in:
Proceedings Fourth ACM Symposium on Principles of Programming Languages, 1977.
[21] Cousot, P., and Cousot, R., Constructive versions of Tarski's fixed point theorems, Pacific

J. Math. 82, No. 1, 1979, pp. 43-57.
[22] Cousot, P., and Cousot, R., Semantic foundations of program analysis, pp. 303-342 in:

Program Flow Analysis, S. Muchnick and N. Jones, eds., Prentice Hall, Englewood Cliffs,
New Jersey, 1981.
[23] Davis, M., Obvious logical inferences, pp. 530-531 in: Proceedings of IJCAI 81, 1981.
[24] Davis, M., and Schwartz, J., Metamathematical extensibility for theorem verifiers and

proof checkers, Comp. Math. Appl. 5, 1979, pp. 217-230.
[25] Deak, E., A Transformational Approach to the Development and Verification of Programs

in a Very High Level Language, Courant Computer Science Report 22, Courant Institute,
New York University, New York, November 1980.
[26] Despeyroux, T., TYPOL a Formalism to Implement Natural Semantics, Research Report,

INRIA, Sophia Antipolis, France, 1986.
[27] Dijkstra, E., A Discipline of Programming, Prentice Hall, Englewood Cliffs, New Jersey,

1976.
[28] Dijkstra, E. W., A constructive approach to the problem of program correctness, BIT 8,

No. 3, 1968, pp. 174-186.
[29] Doberkat, E. E., and Gutenbeil, U., SETL to Ada tree transformations applied, Inform.

Software Tech. 29, No. 10, December 1987, pp. 548-557.
[30] Donzeau-Gouge, V., Dubois, C., Facon, P., and Jean, F., Development of a programming

environment for SETL, pp. 21-32 in: Proceedings ESEC 87, H. Nichols and D. Simpson,
eds., LNCS Vol. 289, Springer-Verlag, Berlin, 1987.
[31] Donzeau-Gouge, V., Kahn, G., Lang, B., Melese, B., and Morcos, E., Outline of a tool for

document manipulation, pp. 615-620 in: IFIP '83, North-Holland Elsevier, Amsterdam,
1983.
[32] Dubois, C., Determination statique des types pour le langage SETL, Doctoral Dissertation, CNAM, Paris, France, 1989.
[33] Earley, J., High level iterators and a method for automatically designing data structure

representation, J. Comput. Languages 1, No. 4, 1976, pp. 321-342.
[34] Fernandez, J.-C., An implementation of an efficient algorithm for bisimulation equivalence, Sci. Comput. Programming 13, 1989/90, pp. 219-236.
[35] Fong, A., and Ullman, J., Induction variables in very high level languages, pp. 104-112 in:

Proceedings Third ACM Symposium on Principles of Programming Languages, January
1976.
[36] Freudenberger, S., Schwartz, J., and Sharir, M., Experience with the SETL optimizer,

ACM TOPLAS 5, No. 1, 1983, pp. 26-45.
[37] Garland, S., and Guttag, J., A Guide to LP, the Larch Prover, Research Report 82, DEC

Systems Research Center, Palo Alto, California, 1991.
[38] Gordon, M., and Melham, T., eds., Introduction to HOL: A Theorem Proving Environment for Higher-Order Logic, Cambridge University Press, 1993.
[39] Gries, D., Describing an algorithm by Hopcroft, Acta Inform. 2, 1973, pp. 97-109.
[40] Gries, D., The Science of Programming, Springer-Verlag, New York, 1981.
[41] Guttag, J., and Horning, J., Larch: Languages and Tools for Formal Specification,

Springer-Verlag, New York, 1993, with S. Garland, K. Jones, A. Modet, and J. Wing.
[42] Harel, D., On visual formalisms, CACM 31, No. 5, 1988, pp. 514-530.

60 J. P. KELLER AND R. PAIGE
[43] Hoare, C. A. R., An axiomatic basis for computer programming, CACM 12, No. 10, 1969,

pp. 576-580.
[44] Hopcroft, J., An n log n algorithm for minimizing states in a finite automaton, pp. 189-

196 in: Theory of Machines and Computations, Z. Kohavi and A. Paz, eds., Proceedings
International Symposium on Theory of Machines and Computation, Academic Press, New
York, 1971.
[45] Kahn, G., Natural Semantics, Research Report, INRIA, Sophia Antipolis, France, 1987.
[46] Kanellakis, P., and Smolka, S., CCS expressions, finite state processes and three problems

of equivalence, Inform. and Comput. 86, No. 1, 1990, pp. 43-68.
[47] Keller, J., Cantor: A Tutorial and a User's Guide, Education Report 94/9, Kepler, Paris,

1994.
[48] Keller, J., and Paige, R., Algorithm Design and Verifiable APTS Transformational

Derivations with NAP, Research Report 94/11, Kepler, Paris, 1994.
[49] Lamport, L., How to Write a Proof, Technical Report 94, DEC Systems Research Center,

Cambridge, Massachusetts, February 1993.
[50] Milner, R., A Calculus of Communicating Systems, Lecture Notes in Computer Science,

Vol. 92, Springer-Verlag, Berlin, 1980.
[51] Milner, R., Calculi for synchrony and asynchrony, Theoret. Comput. Sci. 25, No. 3, 1983,

pp. 267-310.
[52] Myhill, J., Finite Automata and the Representation of Events, WADD TR-57-624, Wright

Patterson AFB, Ohio, 1957.
[53] Nerode, A., Linear automaton transformations, Proc. Amer. Math. Soc. 9, 1958, pp. 541-

544.
[54] Omodeo, E., 1994, private communication.
[55] Paige, R., Real-time simulation of a set machine on a ram, pp. 69-73 in: Computing

and Information, N. Janicki and W. Koczkodaj, eds., Vol. II, Canadian Scholars' Press,
Toronto, May 1989.
[56] Paige, R., Efficient translation of external input in a dynamically typed language, pp. 603-

608 in: Technology and Foundations -- Information Processing 94, B. Pehrson and I. Simon, eds., Vol. 1 of IFIP Transactions A-51, Conference Record of IFIP Congress 94,
North-Holland, Amsterdam, September 1994.
[57] Paige, R., Viewing a program transformation system at work, pp. 5-24 in: Programming Language Implementation and Logic, M. Hermenegildo and J. Penjam, eds., LNCS
Vol. 844, Springer-Verlag, Berlin, September 1994, Proceedings Joint 6th International
Conference on Programming Language Implementation and Logic Programming (PLILP)
and 4th International Conference on Algebraic and Logic Programming (ALP).
[58] Paige, R., and Koenig, S., Finite differencing of computable expressions, ACM Trans.

Programming Languages and Systems 4, No. 3, 1982, pp. 401-454.
[59] Paige, R., and Schwartz, J., Expression continuity and the formal differentiation of algorithms, pp. 58-71 in: Proceedings 4th ACM Symposium on Principles of Programming
Languages, January 1977.
[60] Paige, R., and Tarjan, R., Three partition refinement algorithms, SIAM J. Comput. 16,

No. 6, 1987, pp. 973-989.
[61] Paulson, L., Isabelle: The next 700 theorem provers, pp. 361-386 in: Logic and Computer

Science, P. Odifreddi, ed., Academic Press, New York, 1990.
[62] Pepper, P., A simple calculus for program transformation, Sci. Comput. Programming 9,

1987, pp. 221-262.
[63] Pepper, P., and Wirsing, M., Korso: A Methodology for the Development of Correct Software, Technical Report 93-36, Technische Universit"at Berlin, November 1994.
[64] Reif, W., Formal software development in the KIV system, in: KORSO: Methods, Languages, and Tools for the Construction of Correct Software, M. Broy and S. J"ahnichen,
eds., LNCS Vol. 1009, Springer-Verlag, Berlin, 1995.
[65] Reif, W., The KIV-approach to software verification, in: KORSO: Methods, Languages,

and Tools for the Construction of Correct Software, M. Broy and S. J"ahnichen, eds.,

PROGRAM DERIVATION 61
LNCS Vol. 1009, Springer-Verlag, Berlin, 1995.
[66] Reif, W., Schellhorn, G., and Stenzel, K., Interactive correctness proofs for software modules using KIV, pp. 151-162 in: Compass 95, Proceedings of the 10th Annual Conference
on Computer Assurance, 1995.
[67] Rumbaugh, J., Blaha, M., Premerlani, W., Eddy, F., and Lorensen, W., eds., ObjectOriented Modeling and Design, Prentice Hall, Englewood Cliffs, New Jersey, 1991.
[68] Sands, D., Total correctness by local improvement in program transformation, pp. 221-232

in: Proceedings of the 22nd ACM Symposium on Principles of Programming Languages,
January 1995.
[69] Schonberg, E., Schwartz, J., and Sharir, M., An automatic technique for selection of data

representations in SETL programs, ACM TOPLAS 3, No. 2, April 1981, pp. 126-143.
[70] Schwartz, J., On Programming: An Interim Report on the SETL Project, Installments I

and II, New York University, New York, 1974.
[71] Schwartz, J., Automatic data structure choice in a language of very high level, CACM

18, No. 12, December 1975, pp. 722-728.
[72] Schwartz, J., Optimization of very high level languages, Parts I, II, J. Comput. Languages

1, No. 2-3, 1975, pp. 161-218.
[73] Schwartz, J., On correct program technology, pp. 120-146 in: Two Papers on Program

Verification, Courant Computer Science Report No. 12, Courant Institute, New York
University, New York, September 1977.
[74] Schwartz, J., Dewar, R., Dubinsky, E., and Schonberg, E., Programming with Sets: An

Introduction to SETL, Springer-Verlag, New York, 1986.
[75] Sharir, M., Some observations on formal differentiation, ACM TOPLAS 4, No. 2, March

1982, pp. 000-000.
[76] Shields, D., Measuring SETL Performance, Doctoral Dissertation, Computer Science Department, New York University, 1983.
[77] Shoenfield, J., Mathematical Logic, Addison-Wesley, Reading, Massachusetts, 1967.
[78] Sintzoff, M., Weber, M., de Groote, P., and Cazin, J., Definition 1.1 of the generic development language Deva, Tool Use Research Report 94, Unit'e d'Informatique, Universit'e
Catholique de Louvain, Louvain-la-Neuve, Belgium, 1989.
[79] Smith, D., Kids -- a knowledge-based software development system, pp. 129-136 in: Proceedings Workshop on Automating Software Design, AAAI-88, September 1988.
[80] Snyder, K., The SETL2 Programming Language, Technical Report 490, Courant Insititute, New York University, New York, 1990.
[81] Standish, T., Kibler, D., and Neighbors, J., The Irvine Program Transformation Catalogue, Technical Report, University of California at Irvine, Department of Information
and Computer Science, Irvine, California, January 1976.
[82] Straub, R., Taliere: An Interactive System for Data Structuring SETL Programs, Doctoral Dissertation, Department of Computer Science, New York University, 1988.
[83] Suppes, P., and Sheehan, J., CAI course in axiomatic set theory, pp. 3-80 in: University

Level Computer-Assisted Instruction at Stanford: 1968-1980, P. Suppes, ed., Stanford
University, Institute for Mathematical Studies in the Social Sciences, Stanford, California,
1981.
[84] Tarjan, R., Testing flow graph reducibility, J. Comput. System Sci. 9, No. 3, 1974, pp. 355-

365.
[85] Tarski, A., A lattice-theoretical fixpoint theorem and its application, Pacific J. Math. 5,

1955, pp. 285-309.
[86] Tenenbaum, A., Type Determination for Very High Level Languages, Courant Computer

Science Report 3, Courant Institute, New York University, New York, 1974.
[87] Watson, B., A Taxonomy of Finite Automata Minimization Algorithms, Computing Science Note 93/44, Eindhoven University of Technology, Eindhoven, The Netherlands, 1993.
[88] Weber, M., Simons, M., and Lafontaine, C., The Generic Development Language Deva,

LNCS Vol. 738, Springer-Verlag, Berlin, 1993.
[89] Westin, S., Fast Decision of Strong Bisimulation Equivalence Using Partition Refine62 J. P. KELLER AND R. PAIGE

ment, Dissertation for the Licentiate Degree in Computer Science, Chalmers University,
Department of Computer Science, G"oteborg, Sweden, 1989.
[90] Weyrauch, R., A User Manual for FOL, Technical Report 235.1, Stanford AIM, Stanford,

California, 1977.
[91] Yourdon, E., Modern Structured Analysis, Yourdon Press, New York, 1989.

J. P. Keller R. Paige
Kepler Department of Computer Science
8, rue des Haies Courant Institute
F-75020 Paris 251 Mercer Street
France New York, New York 10012
E-mail: keller@cnam.fr E-mail: paige@cs.nyu.edu

Received February 1995.