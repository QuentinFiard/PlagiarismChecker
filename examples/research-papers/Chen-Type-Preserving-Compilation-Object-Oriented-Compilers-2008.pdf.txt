

Type-Preserving Compilation for Large-Scale OptimizingObject-Oriented Compilers *
Juan Chen Chris HawblitzelMicrosoft Research
{juanchen, chrishaw}@microsoft.com Frances PerryPrinceton Universityfrances@cs.princeton.edu Mike EmmiUniversity of California, Los Angelesmje@cs.ucla.edu

Jeremy Condit Derrick CoetzeeMicrosoft Research

{jcondit, dcoetzee}@microsoft.com Polyvios PratikakisUniversity of Maryland, College Parkpolyvios@cs.umd.edu

AbstractType-preserving compilers translate well-typed source code, suchas Java or C#, into verifiable target code, such as typed assembly
language or proof-carrying code. This paper presents the imple-mentation of type-preserving compilation in a complex, large-scaleoptimizing compiler. Compared to prior work, this implementation
supports extensive optimizations, and it verifies a large portion ofthe interface between the compiler and the runtime system. Thispaper demonstrates the practicality of type-preserving compilation
in complex optimizing compilers: the generated typed assemblylanguage is only 2.3% slower than the base compiler's generateduntyped assembly language, and the type-preserving compiler is
82.8% slower than the base compiler.Categories and Subject Descriptors D.3.3 [Programming Languages]: Language Constructs and Features--Classes and objectsGeneral Terms Verification

Keywords Type-preserving compilation, object-oriented compil-ers
1. IntroductionBecause of compiler bugs, compilers may not preserve safety prop-erties of source-level programs through the compilation process.
Over the last decade, many researchers have proposed techniquesfor removing the compiler from the trusted computing base by en-suring that the output of the compiler has the same safety properties
as the input. Necula and Lee proposed Proof-Carrying Code (PCC),in which low-level code is accompanied by a safety proof that canbe verified efficiently [12]. Morrisett et al. developed Typed Assembly Language (TAL), in which the compiler produces type* The work by Frances Perry, Mike Emmi, and Polyvios Pratikakis wasdone during internship at Microsoft Research

Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citation
on the first page. To copy otherwise, to republish, to post on servers or to redistributeto lists, requires prior specific permission and/or a fee.PLDI'08, June 7-13, 2008, Tucson, Arizona, USA.
Copyright cr^ 2008 ACM 978-1-59593-860-2/08/06.. . $5.00

annotated assembly code; here, the soundness of the TAL type sys-tem guarantees that a well-typed TAL program is both type-safeand memory-safe [11]. Leroy built a certified compiler with a formal proof that the compilation preserves the semantics of sourceprograms [8]. However, none of these compilers was a large-scaleoptimizing compiler that is designed for use in real-world software
development. Such compilers may be orders of magnitude larger insize than research prototypes because of their support for advancedlanguage features as well as their aggressive optimization.

This paper presents the implementation of type-preserving com-pilation in a large-scale optimizing compiler of approximately200,000 lines. By type-preserving compilation, we mean that the
compiler preserves types through each intermediate representation,from source code to assembly code, allowing a lightweight verifierto check the safety of the generated assembly code without trusting
the compiler itself. Our compiler is about an order of magnitudelarger than previously published type-preserving compilers,1 andit is used by about 30 developers on a daily basis. Although the
compiler was originally implemented with a typed intermediaterepresentation, these types were previously discarded prior to in-struction selection and register allocation. With our enhancements,
these types are preserved down to the assembly code, where theycan be checked by a standalone verifier.The source language of the compiler is Microsoft Common Intermediate Language (CIL) [6]. We chose a compiler for an object-oriented language because object-oriented languages such as Java,C#, and C++ are among the most popular programming languages
for industrial software development. For our medium-level andlow-level type system, we chose to implement LILC (Low-levelIntermediate Language with Classes) [2] and SST (Simple Stack
Types) [13] because they are sound and have decidable type check-ing. In addition, these type systems are expressive enough to sup-port type-safe implementations of language features such as dynamic dispatch and reference parameters.Our work makes the following contributions:

1. We implemented the LILC and SST type systems in an exist-ing optimizing compiler at reasonable cost. Although these typesystems were published previously, they had not been implemented or tested in a practical setting. Our compiler uses ap-proximately 40 low-level optimizations, and with our changes
1 SpecialJ has about 33k lines for the compiler and 25k for the VCGen andthe checker [4]. The TALx86 compiler has about 18k lines [7]. The LTALcompiler has about 50k for the typed backend [3].

183

to the compiler, it can preserve types through almost all of theseoptimizations. For the benchmarks we measured (ranging from4MB to 20MB in executable size), the generated TAL code is
0.95-1.09 times slower than the base compiler's generated code,with a geometric mean of 1.02. The TAL compiler is 0.99-17.12times slower than the base compiler, with a geometric mean of
1.83. The TAL checking time is 1.61%-18.78% of the compi-lation time, with a geometric mean of 5.59%. The TAL sizeis 1.69-2.52 times larger than the x86 code in terms of object
file size, with a geometric mean of 2.05. We achieved this re-sult by modifying approximately 10% of the existing compilercode (19,000 out of 200,000 lines) and by adding 12,000 lines
of code for TAL definition and verification.2. The TAL compiler generates explicit proofs about integer val-ues, and the TAL checker verifies these proofs. In particular,

the compiler generates proofs that array indices are within ar-ray bounds, even for cases where the compiler's optimizationseliminate run-time bounds checks. The compiler generates the
necessary loop invariants and proofs just before converting tothe back-end intermediate representation, and it preserves theinvariants and proofs through the back-end phases.

3. The TAL checker verifies a large portion of the interface be-tween the compiler and the runtime system. The current systemtrusts the implementation of the runtime system, including the

garbage collector, but it verifies most of the object layout andgarbage collection information that is generated by the compilerfor use by the runtime system.

We believe that type-preserving compilation is a practical wayto verify the output of large-scale optimizing compilers. Typesare a concise way to represent data properties, and they impose
few constraints on optimizations. Because many existing compilersalready use types and type checking internally, it is natural toextend these compilers to preserve types at a low level.

These benefits give type-preserving compilation a number ofadvantages over using a certified compiler, where the compiler it-self is proven correct once and for all. Whereas existing optimizing
compilers can be extended to preserve types in a natural way, it isnot obvious whether we can certify a large-scale optimizing com-piler. For example, Leroy's work [8] showed that well-understood
transformations such as layout of stack frame are difficult to provecorrect, and a production compiler will have many such analysesand transformations. Furthermore, a realistic compiler is under constant change, which requires the compiler to be re-proven after eachchange.The rest of the paper is organized as follows. Section 2 provides background information regarding the base compiler and ourtype systems. Section 3 presents our implementation of the type-preserving compiler. Section 4 discusses our techniques for verifying array bounds accesses, Section 5 presents our approach toverifying interaction with the garbage collector, and Section 6 dis-cusses our performance results. Section 7 discusses related work,
and Section 8 concludes.2. Background

2.1 The Base CompilerThe base of our implementation is Bartok, which allows the use ofmanaged languages like C# for general-purpose programming. The
compiler has about 200,000 lines of code, mostly written in C#, andis fully self-hosting.Bartok can compile each binary module individually or it can
compile them all as a whole program with interprocedural opti-mization. We use separate compilation mode to separate user pro-grams from the libraries that we currently do not verify.

\Gamma \Delta \Theta \Lambda \Xi \Pi  \Sigma \Upsilon \Phi \Lambda 

\Delta \Theta \Lambda \Xi \Psi \Omega ff fi\Lambda flffi\Lambda fi\Psi \Gamma ffli\Lambda fij\Upsilon \Gamma ffl\Pi \Gamma  `\Omega '`\Omega ' \Gamma ^\Pi \Upsilon *\Upsilon _fl\Pi \Upsilon \Gamma ffljff\Gamma *\Lambda fi\Upsilon ffl,\Pi \Gamma  ss\Omega 'ss\Omega ' \Gamma ^\Pi \Upsilon *\Upsilon _fl\Pi \Upsilon \Gamma fflj
ff\Gamma *\Lambda fi\Upsilon ffl,\Pi \Gamma  ff\Omega 'ff\Omega ' \Gamma ^\Pi \Upsilon *\Upsilon _fl\Pi \Upsilon \Gamma fflj\Psi \Gamma ffi\Lambda  ,\Lambda ffl\Lambda fifl\Pi \Upsilon \Gamma ffl\Psi \Gamma *^\Upsilon \Phi \Lambda fi
Figure 1. Architecture of Bartok
Performance of Bartok's generated code is comparable to theirperformance under the Microsoft Common Language Runtime(CLR). According to the benchmarks tested, programs compiled
by Bartok are 0.94 to 4.13 times faster than the CLR versions, witha geometric mean of 1.66.The architecture of Bartok is shown in Figure 1. Bartok translates programs written in CIL, which is an intermediate represen-tation for C# and other languages, into native x86 code, throughthree intermediate representations: High-level IR (HIR), Mediumlevel IR (MIR), and Low-level IR (LIR). Bartok performs about 40optimizations on the three IRs.HIR is similar to CIL, except that HIR does not use stack-based
computation. The type system of HIR is almost the same as thatof CIL (or C#), consisting of primitive types, classes, interfaces,arrays, and so on. All object-oriented operations are primitives in
HIR, such as virtual method invocation, type cast, and array access.These primitives are lowered during the translation from HIR toMIR. For example, virtual method invocation is translated to code
for fetching the vtable out of an object, fetching a method out of thevtable, and calling the method on the object. The original MIR hadthe same type system as HIR, which is not expressive enough to
represent the result of such a lowering. For example, no HIR typescould represent the vtable of a class that contains virtual methods.MIR is further lowered to LIR. Then several transformations
are performed on LIR: class layout, instruction selection, registerallocation, and stack frame layout. The original LIR and the backend were untyped.

The lowest level of LIR (essentially assembly) is written toobject files in a standard format. A standard linker links the objectfiles and creates native x86 executables.

2.2 The LILC Type SystemLILC is a typed intermediate language we implement as the typedMIR in Bartok to represent implementations of lowering objectoriented primitives.LILC was developed by Chen and Tarditi for compiling coreobject-oriented language features, such as classes, objects, and arrays [2]. We decided to use LILC because it faithfully representsstandard implementations of object layout, virtual method invoca-tion, and runtime libraries such as type test and array store check.
Furthermore, LILC preserves notions of classes, objects, and sub-classing, whereas other encodings compile those notions to func-tional idioms (records and functions). It is easier to implement
LILC in Bartok because LILC preserves those object-oriented no184

vtablex tagdistance

tag(Point)

Instructions for distanceFigure 2. Object Layout for Point
tions in the input language of Bartok. LILC has been proven soundand its type checking is decidable.This section explains the main ideas of how LILC represents
objects and runtime libraries. Other features can be found in earlierwork [2].
Classes and Objects LILC has both nominal (name-based) classnames and structural record types. Each class

C has a correspond-ing record type
R(C) that describes the object layout for the class
C. For a typical layout strategy, an object contains a vtable and a setof fields. The vtable contains a tag--a unique identifier to identifythe class at runtime, and a set of virtual methods.

Suppose a class Point is defined as follows:class Point {

int x;virtual int

distance(){. . .}}The class Point contains an integer field

x and a virtual method
distance that takes no parameters and returns an integer. Theobject layout of Point is shown in Figure 2.Type

R(Point) represents this layout naturally:
R(Point) ={

vtable : {tag : Tag(Point),

distance : (9o"" # Point. o"") ! int},
x : int}

R(Point) is a record type with two fields, vtable and x. The type for
vtable is another record type containing fields tag and distance.Note that objects and pointers to objects are used interchangeablyin this paper. It should be clear from the context to which we refer.

For example, the vtable field is actually a pointer to a record of twofields, but we omit the representation of the pointer in the type anduse a record type directly.

The tag of the class Point identifies Point at run time. Its type isrepresented as Tag

(Point), where T ag is an abstract type construc-tor. LILC treats tags as abstract for simplicity.The vtable contains a method pointer for the virtual method

"distance". The method "distance" now takes one parameter-- the"this" pointer--to reflect the self-application semantics where wepass an object to a virtual method when calling the method on the
object. The type of the "this" pointer requires that the "this" pointerbe an instance of Point or Point's subclasses. We explain "this"pointer types later in this section.

An instance of C can be coerced to and from a record of type
R(C) without any runtime overhead. The coercions are runtimeno-ops. Objects are lightweight because interesting operations areperformed on records. Object and class notions are preserved to

simplify the type system.The separation of the nominal and structural types eliminatesexplicit structural recursion, because the record type

R(C) can re-fer to any class name, including
C itself. Also, the separation al-lows a straightforward typing of self-application semantics, whichis the most challenging problem in typed intermediate languages

for object-oriented languages.

Virtual Method Invocation Virtual method invocation requires adistinction between the static type and the dynamic type (actualruntime type) of an object. To call a method

m on an object o withstatic type
C, we need to pass o as the "this" pointer to m, or at leastpass an object that has the same dynamic type (or its subclasses) as

o. Passing an object with the same static type C may be unsafe.Consider the following example:

void Unsafe(Point p, Point q){

vt = p.vtable;
dist = vt.distance;
dist(q); }This function is unsafe, even though the distance method fetched

from p requires an object of Point and q is indeed an object ofPoint. The function can be called in an unsafe way if

p is actually aninstance of a subclass of Point and the subclass overrides

distanceto access fields in the subclass but not in Point.To guarantee the soundness of virtual method invocation, LILC

introduces "exact" notions of classes to represent dynamic types.Unlike source languages Java and C# where a class name

C rep-resents objects of
C and C's subclasses, LILC uses C to representonly objects of "exact"

C, not C's subclasses. LILC uses an exis-tential type 9
o"" # C. o"" for objects of C and C's subclasses. Thenotion "#" represents subclassing. The type variable

o"" indicatesthe dynamic type, which must be a subclass of
C. Objects withsource-level type (or static type)
C are translated to have the ex-istential type in LILC. LILC has subtyping rules to guarantee thatsubclass objects can be used as superclass objects.

Suppose an object has dynamic type # . Any virtual methodfetched from the object has "this" pointer type 9

o"" # # . o"", mean-ing only objects of
# or # 's subclasses.The "Unsafe" example is ill-typed in LILC. The object

p hastype 9
o"" # Point. o"" in LILC. To invoke method "distance" on p,we first open

p and introduce a type variable r' for p's dynamic type.The type of the "distance" method fetched from

p requires that the"this" pointer be an object of
r' or r''s subclasses. The type checkeraccepts passing
p to "distance" but rejects passing q because wecannot guarantee and

q is an object of r' or r''s subclasses.Type-safe Runtime Libraries The type-safe runtime libraries are

represented in the LILC type system exactly like user programs.This gives the compiler freedom to inline and optimize them.Type Cast. Downward type casts check at run time whether an

arbitrary object is an instance of an arbitrary class or its subclasses.In a typical implementation, each class stores a tag in its vtable. If
C extends B, then the tag of C has a pointer pointing to the tag of
B. The pointers form a tag chain. Downward type cast fetches tagsin the chain and compares them with the tag of the target class.This typical implementation is expressed as a well-typed polymorphic function in LILC that can be applied to arbitrary objectsand arbitrary classes. The key ideas are to use types to connect anobject with the tag it contains, and to refine types according to the
tag comparison result. Two classes are the same if and only if theirtags are equal. If an object

o has type # and the tag in o is equal to
tag(C), then # = C. If one of the parent tags, which identifies aparent class of

# , is equal to tag(C), then # # C and o can be castto
C.Array Store Check. Source languages such as Java and C#have covariant array types, that is, if

A is a subtype of B, then
Array(A) is a subtype of Array(B). Covariant array types re-quire runtime "store checks" each time an object is stored into anarray. If array

a has static type array(B), to store an object of type
B in a, we have to check whether the object has the "actual" ele-ment type of

a because a might be an array of A.LILC uses invariant array types enclosed with existential typesto express source-level array types. An LILC array type is a subtype

185

of only itself. The source-level array subtyping is transferred tosubtyping on the enclosing existential types in LILC.To store an object in an array that has type array

(C), LILCprograms must explicitly check whether the object is an instanceof the element type
C, which can utilize the previous type castfunction.

2.3 The SST Type SystemSST is a simple, sound, and decidable type system that supportsmost common stack operations, aliased stack locations, and byreference parameters [13]. Other type systems for stacks eitherwere undecidable or did not support by-reference parameters.A stack is viewed as a sequence of stack slots. The stack grows
toward lower addresses. The stack can be accessed by arbitrarypointers to the stack. But only changing the value of the stackpointer "sp" can grow or shrink the stack.

SST uses a stack type to describe the current stack state. SSTchecks one function at a time. Each function can see only its ownstack frame. The previous stack frames are abstracted by a stack
variable.Each stack slot in the current stack frame is labeled by a loca-tion. A pointer to a stack slot labeled by location

` has a singletontype Ptr
(`). The stack type tracks mapping from stack locationsto current types of values stored at those locations. We call suchmapping capabilities. Capabilities are linear, that is, they cannot

be duplicated. Because of the separation between singleton pointertypes and capabilities, the capabilities can evolve, independently ofthe pointer types, to track updates and deallocation.

SST uses a non-commutative, non-associative operator "::"to glue capabilities together to form a stack type: "

`2 : int ::
`1 : int :: `0 : !" means that two integers are on top of the stack(at locations

`1 and `2) and the rest of the stack is abstracted as astack variable

!. Capabilities glued by :: form the spine of a stack.To represent aliasing, SST introduces a "^" operator to attacha capability to a stack type. The capability describes an aliased

location to some location inside the stack type. Therefore, the scopeof the capability is the stack type: the capability is safe to use aslong as the stack type is not modified. To guarantee safety, the scope
can be only expanded to a larger stack type but not contracted.By-reference Parameters. Consider a function

swap that hastwo by-reference integer parameters
x and y:

void swap(ref int x, ref int y) {...}Suppose the compiler pushes parameters onto the stack from right

to left. In SST, the stack type in the precondition of swap isnext

(next(`0)) : Ptr(`x) :: next(`0) : Ptr(`y) :: `0 : (! ^ {`y :int} ^ {

`x : int}).Upon entry to

swap, the stack holds the arguments x and y onits top, each of which is a pointer to some aliased location inside

!.Note that aliased locations
`x and `y may appear anywhere in !, inany order. In fact,
`x and `y may be the same location.The
swap function can pass x and y to other functions, furtherexpanding the scopes of

`x and `y. But swap cannot return areference to a local variable defined in itself, because this contractsthe scope of the reference to

!.2.4 Arrays

Loads and stores to array elements are primitive operations in CIL.Naively, each array element access requires a run-time boundscheck to ensure that the element index is within the array's bounds.
Compilers break up each array access into a sequence of moreprimitive machine instructions. Many compilers, including Bartok,will also optimize away many of the bounds checks. The TAL type
system must be able to ensure that every access is within boundseven after the compiler's transformations and optimizations.

\Gamma \Delta \Theta \Lambda \Xi \Pi \Sigma \Upsilon 
\Delta \Phi \Psi \Sigma \Omega ff fi\Xi \Pi \Sigma 

\Delta \Theta \Lambda \Xi \Gamma flffi \Upsilon \Sigma ffli\Sigma \Upsilon \Gamma \Delta j`\Sigma \Upsilon '\Xi \Delta jff\Delta  ^fl*^fl* \Delta \Lambda ff\Xi \Theta \Xi _fflff\Xi \Delta j'ffi\Delta *\Sigma \Upsilon \Xi j,ff\Delta ssae\Lambda \Sigma i oefl*ssae\Lambda \Sigma i oefl* \Delta \Lambda ff\Xi \Theta \Xi _fflff\Xi \Delta j'ffi\Delta *\Sigma \Upsilon \Xi j,ff\Delta ssae\Lambda \Sigma i ffifl*
ssae\Lambda \Sigma i ffifl* \Delta \Lambda ff\Xi \Theta \Xi _fflff\Xi \Delta j'\Gamma \Delta i\Sigma  ,\Sigma j\Sigma \Upsilon fflff\Xi \Delta jo/\Xi 'ffl''\Sigma \Theta \Phi \Pi \Sigma ff\Delta ssAEffissAEffiOE\Sigma \Upsilon \Xi fi\Xi \Pi \Sigma \Upsilon 
Figure 3. Architecture of the New Compiler
To solve this problem, our type system incorporates tech-niques from earlier work. First, following Xi and Harper's workon DTAL [16], it uses singleton types to represent known information about integer values. For example, the number 5 has typeint, but 5 also has the more specific type

S(5), the singleton typeof integers equal to 5. More interestingly, a variable

x might havesingleton type
S(o""), where a basic block's pre-condition specifiesa constraint
o"" < 5 on the integer type variable o"". The singletontype together with the constraint ensure that

x is less than 5. DTALused an arithmetic constraint solver to verify that the pre-conditionsfor basic blocks and array operations are satisfied. Unfortunately,

the constraint solver sits in the TAL verifier's trusted computingbase, and for decidability's sake the solver is limited to a subsetof arithmetic. Therefore, our type system follows the approach advocated by Crary and Vanderwaart [5] and Shao et al. [14], whichkeeps DTAL's singleton types and pre-conditions, but also allowsexplicit proofs of pre-conditions in the TAL program. In this approach, the TAL verifier only needs to check the supplied proofs,and does not need to automatically solve arithmetic constraints.Of course, the compiler must generate the proofs before the TAL
verifier can check the proofs; proof generation for arrays was be-yond the scope of Crary and Vanderwaart [5] and Shao et al. [14].Section 4 describes how Bartok generates these proofs.

3. Implementation of aType-Preserving Compiler
This section provides implementation details for the new type-preserving compiler. In order to preserve types, we must implementtyped MIR and LIR using the LILC and SST type systems. We must

also ensure that types are preserved across low-level optimizations.The architecture of the new compiler is shown in Figure 3. Theoriginal MIR type system was enhanced to express lowering of
object-oriented primitives. The original untyped LIR and untypedbackend were modified to preserve types. When generating objectfiles, the compiler adds a new section for type information. Other
sections in the object file have exactly the same format as the onesgenerated by the base compiler. The verifier disassembles the codeand the type information and then verifies the typed assembly code.
A standard linker generates x86 executables from the object filesand discards the type information section.Section 3.1 describes special handling of type variables to allow
optimizations. Section 3.2 shows how LILC and SST types are

186

represented in Bartok. Section 3.3 lists the optimizations performedon typed MIR and LIR.
3.1 Type VariablesType variables are important to guarantee soundness, as shown inSection 2.2. A type variable that identifies the dynamic type of an

object should be associated with only that object.Like traditional typed calculi, LILC introduces a fresh typevariable each time an existential type is opened. The type variable
identifies the dynamic type of an object. The type variable is inscope until the end of the basic block. The type checker rejects the"Unsafe" function in Section 2.2 (translated to LIR):

(1) p0 = openhr'i(p); // p0 : r'
(2) q0 = openhr^i(q); // q0 : r^
(3) vtable = p0.vtable;
(4) dist = vtable.distance // dist : (9s' # r'. s') ! int
(5) dist(q0)Instructions (1) and (2) introduce distinct type variables

r' and r^for the dynamic types of
p and q respectively. The method "dist"expects an object of
r' or r''s subclasses and q0 does not have thattype.This strategy hinders common optimizations, though. For example, two consecutive virtual method invocations on the same object
p may be translated to the following LIR code:

p1 = openho""i(p);
vtable1 = p1.vtable;
m1 = vtable1.m1
m1(p1, . . .)
p2 = openhr'i(p);
vtable2 = p2.vtable;
m2 = vtable2.m2
m2(p2, . . .)The object

p does not change between two calls, and thus it is soundto apply common subexpression elimination (CSE) to combine thetwo opens and vtable fetches. But the two distinct type variables

o""and
r' prevent CSE from optimizing the code.To work around this problem, we separate type variables used inthe programs and during type checking, to both allow optimizations

and guarantee soundness.In typed MIR, type variables in programs do not identify dy-namic types of objects. It is not required that each open instruction
introduces a fresh type variable. In fact, type variables are groupedby their bounds. Two type variables that have the same upper andlower bounds are considered the same. The bounds of type variables have to be accurate because optimizations may query infor-mation about members in the type variables, which relies on thebounds.

Opening two objects with the same existential type can reuse atype variable. For example, the two open instructions in the aboveexample can use the same type variable

o"". The sharing of typevariables allows CSE to optimize the code sequence to:

p1 = openho""i(p);
vtable1 = p1.vtable;
m1 = vtable1.m1
m1(p1, . . .)
m2 = vtable1.m2
m2(p1, . . .)Note that if

p changes between two calls or we call two methodson two distinct objects, CSE cannot unsoundly optimize the codebecause it cannot tell whether the two opens are the same.

The discrepancy of type variables used in programs and in typechecking may cause problems at control merge point. Supposebasic block B1 has an open instruction

x = openho""i(p1) and block

B2 has x = openho""i(p2) and B1 and B2 merge to block B3. Thisis acceptable in the program because

x has only one type o"". Butduring type checking, the checker introduces two type variables for

x in B1 and B2, which results in disagreement of x's type in B3.Typed MIR solves this problem by specifying for each basicblock its precondition, including for the type variables visible in

the basic block. In the above merge example, block B3 uses a newtype variable merged from the ones in B1 and B2 for

x's type.Optimizations may move the code across basic block bound-aries. To free the optimizations from maintaining preconditions of

basic blocks, typed MIR uses a two-phase type checking. The firstphase infers the precondition for each basic block, and the secondphase checks the block.

The first type-inference phase mainly deals with merging typevariables. Merging is similar to a union operation. Merging a set ofunrelated type variables produces a fresh type variable that tracks
the components from which the type variable was merged. Merginga type variable with one of its components results in the typevariable itself.

For efficiency, the type checker performs a few straightforwardoptimizations: variables whose types contain no type variables arenot tracked in type-checking environments because their types do
not change; if a method has only such variables, then type inferenceis not applied.
3.2 Type RepresentationsThe implementation of LILC and SST requires many new typessuch as type variables, existential types, and polymorphic types.

Naively adding them to the compiler would incur significantchanges to the existing code, including type representations, typechecking, and optimizations. Our implementation changed or
added only 10% of the code in the compiler, because of the choiceswe made in type representation and type checking.
3.2.1 Typed MIR RepresentationBartok, which is written in C# itself, uses a set of classes to rep-resent MIR types, which include classes in the source programs,

function types, and so on. ClassType represents classes. Inter-faceType represents interfaces. FunctionType represents func-tion types. These classes form a hierarchy naturally. For example,
ClassType and InterfaceType are subclasses of NamedType. Ir-Type is the root class. Each instance of these classes correspondsto a type.

An instance of ClassType represents a class in the source pro-gram. The instance contains all information related to the corre-sponding class, and has a table of all the members (fields and methods) of the class.Typed MIR views an instance of ClassType as a combination ofboth an LILC class name and the corresponding record type that describes the class layout. The instance has both name-based informa-tion (class name, superclasses and interfaces) and structure-basedinformation (members). This way, typed MIR can reuse ClassType
without change, yet still preserve the two views of classes in LILC.Another benefit of reusing ClassType is that typed MIR does notneed to coerce between objects and records, which saves many coercion instructions and makes the code more concise. Those coer-cions have no runtime overhead, though.The class name

C (an instance of ClassType) in typed MIR alsoserves as an encoding of the LILC existential type 9

o"" # C. o"".As a result, optimizations can remain as they are: they do not haveto deal with the new existential types. Also, the encoding makes

it unnecessary to coerce between existential types and class nameseach time a field is fetched from an object or an object is created.Typed MIR has a new type ExactClassType to represent "exact"
classes.

187

In a few cases, typed MIR still needs explicit existential typesfor those types that don't have the format 9

o"" # C. o"" and thereforecannot use the encoding mechanism. An example is the interface ta-ble entry. For this purpose, typed MIR has a class ExistentialType.

Typed MIR adds a class TypeVarClass for type variables thatwill be instantiated with classes and interfaces.To represent the explicit implementations of tags (or runtime
types) in the Bartok compiler, typed MIR introduces RuntimeTypeto model tags. An instance of RuntimeType that represents thetag of class

C, denoted as "Runtime(C)", contains a reference tothe representation of class type

C. Tags of different classes havedifferent types.Similarly to tags, typed MIR introduces a new type VTableType

for vtables. An instance of VTableType that represents the vtable ofclass

C (denoted as "Vtable(C)") contains a reference to the class
C. The new types ExactClassType, TypeVarClass, RuntimeType,and VTableType all inherit from ClassType. Therefore, each instance of these types is regarded as a normal class and containsa table of its members. The members are added by need to the ta-ble. For example, virtual methods can be fetched as normal fields
out of vtables.3.2.2 Typed LIR Representation
The implementation of Typed LIR extends SST to support stack-allocated large values, such as floating-point numbers and struc-tures. Basic SST assumes that stack-allocated values are wordsized.Typed LIR changes the symbolic representation for stack loca-tions to "

! + n", meaning n bytes between the location labeled by
!. For aliased locations, the offset n is always 0. Symbolic repre-sentations for stack locations in SST are not suitable for typed LIR,because the difference between two stack locations varies. The new

location representation makes it easier to tell whether a location isan aliased location and to compute the new location when movingalong the stack.

Typed LIR also extends SST to support other features such asobjects, classes, and arrays. In fact, typed LIR can reuse most typesin typed MIR, for example, class types, interface types, array types,
existential types, etc. To avoid duplicating the types, Bartok allowsconverting an MIR type to an LIR type by simply wrapping theMIR type.

3.3 OptimizationsOur new compiler supports more than 40 optimizations of thebase Bartok compiler, with only 2 optimizations unsupported (see
Section 3.3.3). Our experience shows that most optimizations canbe easily modified to preserve types. This is partly due to our effortto design suitable type representations.

3.3.1 MIR OptimizationsMajor optimizations performed at MIR level are as follows:

* Copy propagation* Constant propagation

* Constant folding* Common subexpression elimination (including redundant loadelimination)

* Dead-code elimination* Loop-invariant removal

* Reverse copy propagation of temporaries, which transforms"

t = e; v = t" to "v = e"* Optimizing convert instructions, which compacts chains of con-vert instructions

* Jump chain elimination, which removes jumping to jump in-structions

* Short circuiting, which removes redundant tests of boolean vari-ables

* Loop header cloning, which turns while loops into do-whileloops

* Inlining (with heuristics to limit code size increase)* Elimination of unreachable classes, methods, and fields (tree-shaking)

Because our original MIR has type information already, the im-plementation of typed MIR only needs to change three optimiza-tions. Others are performed on typed MIR as they are on the original MIR. Two optimizations CSE and treeshaking were changedto support the new operators (such as "open") and new types. CSEneeds to index all subexpressions. Treeshaking analyzes all instructions to determine which types are accessed. The inlining optimiza-tion was changed to support cloning new operators and to sup-port inlining of polymorphic functions. The changes are local and
straightforward. The fact that we can reuse almost all optimizationcode confirms our typed MIR design choices.
3.3.2 LIR OptimizationsMajor optimizations on LIR include:

* Copy propagation, including stack locations* Constant propagation

* Dead-code elimination* Jump chain elimination

* Reverse common subexpression elimination of load effective ad-dress computations

* Peephole optimizations* Elimination of redundant condition code setting

* Boolean test and branch clean up* Floating point stack optimizations

* Conversion of "add" to "lea" (load effective address), for exam-ple, "ecx = eax + ebx" to "lea ecx, [eax + ebx]"

* Graph-coloring register allocation* Code layout

* Conversion of "switch" instructions to jumping to entries in jumptablesChanges on LIR optimizations are mostly to propagate types,

since the original LIR was untyped. A few optimizations need moresignificant changes.The optimization that converts "add" instructions to "lea" instructions may introduce invalid effective addresses. It is still safebecause the addresses will never be used to access the memory. Butthe type checker needs to differentiate such cases and prevent those
addresses from being used unsafely.During register allocation, the compiler fixes the stack frameand assigns stack slots to callee-save registers, function arguments,
and spills. We need to record the types of those stack-resident val-ues to type check stack allocation upon entry to a function. Thechecker needs to know the intended types for the newly-allocated
stack slots because of stack-allocated structures. Otherwise, thechecker has difficulty finding out the boundaries of slots. The origi-nal SST did not need such annotations because each slot was wordsized.When jump tables are created for translating "switch" instruc-tions, each entry in the jump table can be viewed as a function entry
point. We need to give types as preconditions to the entries in thejump table. All entries have the same preconditions, so the jumptable can be typed as an array of function pointers.

3.3.3 Unsupported OptimizationsThe base Bartok compiler optimizes memory allocation by inliningthe allocator implementation directly into the compiler-generated
code. Currently the TAL compiler cannot type-check the internal
188

implementation of the memory allocator, so it cannot support thisinlining. Our disabling of this optimization is responsible for mostof the difference in execution time between code generated by the
base Bartok compiler and the type-preserving compiler, as reportedin section 6.Also, as explained in Section 4, we do not support the Array
Bounds Check elimination on Demand (ABCD) algorithm.4. Arrays and Proofs

Bartok implements several optimizations that can eliminate run-time bounds checks from array accesses. First, the common subex-pression elimination attempts to consolidate repeated bounds
checks for the same array element. For example, the C# expres-sion "a[i] = 1 - a[i]" makes two accesses to the same array element;Bartok's common subexpression elimination removes the bounds
check from the second access. Second, an induction variable anal-ysis looks for loops where an array index variable is initialized toa non-negative number, is incremented by one in each loop iteration, and is checked against an array length in the loop condition.Third, Bartok implements the Array Bounds Check elimination onDemand (ABCD) algorithm [1], which infers unnecessary bounds
checks by solving a system of difference constraints (constraints ofthe form

x <= y + c, where c is a constant).This section describes our extensions to Bartok to generateproofs of array access safety, including proofs for run-time bounds

checks and proofs for checks eliminated by common subexpressionelimination and by induction variable analysis, and then discusseswhy we were unable to generate proofs for Bartok's ABCD algorithm. We illustrate the proof generation with a simple C# example:public static void Main(string[] args) {for (int i = 0; i

< args.Length; i++) args[i] = null;}
With our proof-generation extensions, Bartok automatically gen-erates the following TAL code for the Main method's inner loop(omitting the stack types and irrelevant register types, and renaming and reformatting for clarity):B1 (

o"":Int, r':Int, r^:Obj, ai:(ArrIndex r^ o""),al:(ArrLen

r^ r'), gl:(Ge0LtMax o"")){eax:
Sint32(o""), ebx:Sint32(r'), ecx:Sstring[](r^)} =mov (dword ptr [ecx + eax *4+8] using ai), 0add eax, (1 :

Sint32(1)); coerce eax to

Sint32(Succ o"") using (addToSucc o"")cmp eax, ebxjl(p:Lt (Succ

o"") r') B1(Succ o"", r', r^,incrIndex
r^ o"" r' p gl al,al, incrGe0LtMax

r^ o"" r' p gl al)In this code, the basic block B1 is polymorphic over three type

variables: o"" represents an array index, r' represents an array length,and

r^ represents an array. B1 is also polymorphic over three proofvariables. First, ai is a pre-condition that requires ArrIndex

r^ r',which is defined to be equivalent to
0 <= o"" < ALen(r^), where
ALen(r^) is the length of the array r^. Similarly, al and gl requirethat

r' = ALen(r^) and 0 <= o"" < MAX, where MAX is the max-imum 32-bit signed integer. (We use the optimization-specific ab-breviations ArrIndex, ArrLen, and GeLtMax to reduce the generated annotation size and to reduce verification time.) The registerseax, ebx, and ecx hold the index, array length, and array, respec-tively, where the type

Sint32(X) is the singleton type of 32-bitsigned integers equal to X and

S#[](X) is the singleton type ofarrays equal to X.The first instruction of the loop moves null into array element

o"", where ecx + 8 is the address of element 0, and each elementoccupies 4 bytes. The type checker demands a proof that eax

hold a valid index for array held in ecx; the annotation "usingai" supplies this proof. (Bartok can also generate proofs for themore complicated address computations that occur for arrays of
large C# structs, but we discuss just the simplest case here.) Theadd instruction increments eax, coercing the resulting singletontype from

o"" + 1 to Succ o"" for conciseness, where Succ stands forsuccessor. The cmp and jl instructions branch back to the beginningof the block if

o"" + 1 less than the array length r'. The branchrequires instantiations of B1's type variables and proof variableswith valid types and proofs to satisfy B1's pre-condition. In this

case, the axiom incrIndex proves that Succ o"" is an in-bounds arrayindex, given proofs that Succ

o"" < r', that 0 <= o"" < MAX (sothat
o"" + 1 doesn't overflow), and that r' = ALen(r^). Here, the jlinstruction supplies the proof variable p, asserting that Succ

o"" < r'in the taken branch.Broadly speaking, there are two ways a compiler might generate proofs of array access safety. First, a compiler can generate theproper proofs during the array bounds check introduction and dur-ing the array bounds check elimination optimizations. In this case,
the compiler must preserve the proofs through subsequent com-piler phases, using proof-preserving compilation [14]. Alternately,a compiler can postpone proof generation until after all compiler
phases have completed, and try to infer proofs by analyzing thecompiler-generated assembly language code.We decided to use some of both approaches, to compare their
relative strengths. We postpone proof generation until the conver-sion from MIR to LIR, which happens well after the HIR's in-troduction of explicit array bounds check instructions, and after
MIR's common subexpression elimination and induction variableoptimizations. We change Bartok to annotate HIR and MIR codewith hints that highlight the variables and constants used for arrays, array indices, and array lengths. Just before MIR-to-LIR con-version, Bartok runs two dataflow analyses. The first analysis prop-agates the highlighted variables and constants forward as they flow
from one variable to another. The analysis classifies these variablesand constants into equivalence classes at each program point, intro-ducing a type variable for each equivalence class. In the example
above, o"" represents the equivalence class containing just the vari-able eax. The second analysis computes properties about the equiv-alence classes (for example,

r' holds the length of array r^); eachproperty becomes a proof variable (e.g. "al"). If the analyses failto find a safety proof for an array access, they insert an extra runtime bounds check; this ensures that compilation to verifiable TALcan proceed even if the HIR and MIR optimizations outsmart theanalyses.

After conversion to LIR with proof annotations, the compilerpreserves the proofs through the LIR phases. For example, the LIRjump chain elimination optimization takes jumps from block Bi to
block Bj, where Bj consists of a single jump to block Bk, and mod-ifies Bi to jump directly to Bk. Both the Bi-to-Bj jump and Bj-to-Bk jump may have proofs that prove their target's precondition, so
we modified the compiler to compose these Bi-to-Bj and Bj-to-Bkproofs together to produce valid Bi-to-Bk proofs. No optimizationor transformation required anything more sophisticated than this to
preserve proofs. Nevertheless, due to the large number of optimiza-tions, we did not implement proof preservation for every instanceof every optimization. In particular, constant propagation may want
to replace a variable of type S(o"") with a constant c of type S(c);rather than fixing the types (e.g. by proving

o"" = c), we simplydon't propagate c to the
S(o"") variable in this case.Our experience so far shows that practical proof preservation ispossible, but requires effort proportional to the number of compiler

optimizations and transformations. Implementing the proof infer-ence for the MIR-to-LIR conversion required less effort than im-plementing the LIR proof preservation. Nevertheless, proof preser189

vation has one potential advantage over inference: if the initial in-put to the compiler already has proofs of array access safety (orother integer-related properties), a proof-preserving compiler can
preserve these proofs, even if the proofs are too sophisticated forinference to re-discover [14]. For example, we've used Bartok tocompile a hand-written LIR program, hand-annotated with proofs
showing that the program correctly computes the factorial, to TALcode annotated with proofs showing correct factorial computation.This works even though Bartok knows nothing about factorials.

4.1 ABCDWe also attempted to generate safety proofs for Bartok's ABCDarray-bounds check elimination optimization, but the "proofs"
seemed to require axioms that are unsound for modular arithmetic.This observation led to a counterexample:void F(int[] arr, int i)

{if (i
<= arr.Length) {int j = i - 1;if (j

>= 0) arr[j]++; } }At the "arr[j]++" statement, Bartok's ABCD analysis correctly concludes that 0 <= j and i <= arr.Length and j <= i - 1, but then incor-rectly eliminates the bounds check based on the erroneous conclu-sion that

0 <= j <= arr.Length - 1. This conclusion fails when iis the minimum signed 32-bit integer, so that i - 1 underflows. Theout-of-bounds array store clobbers nearby memory, undermining

type safety.Since the original paper on ABCD [1] did not claim to work formodular arithmetic, it was no surprise that the algorithm might fail
for 32-bit integer indices, and in fact Bartok already had heuristicsto squelch the ABCD algorithm whenever it saw large integerconstants. Unfortunately, the method above contains no constants
larger than 1, so Bartok still (incorrectly) eliminates the boundscheck. We were disappointed at not generating safety proofs forABCD, but we were grateful that our attempt led us to discover a
real vulnerability in Bartok.5. GC Information Verification

Object files generated by the compiler also include informationused by the garbage collector. This information allows the garbagecollector to identify all references on the heap and stack at run time.
If this information is incorrect, the garbage collector may attemptto trace non-reference data or prematurely reclaim live data, both ofwhich can lead to safety violations. Thus, in order to ensure safety,
we must check that all of the garbage collection information in theobject files is consistent with our type information.The compiler generates three kinds of information for the
garbage collector's use:The static data table is a bitmap that indicates which words inthe static data section contain traceable references.

The object table in each class's vtable contains information thatallows the garbage collector to locate reference-containing fields ininstances of that class.
The stack table maps every valid return address in the programto information about how to find references in the caller's stackframe. At collection time, the collector walks the stack, using the
information for each return address on the stack to find referencesin each frame.For each kind of information, we must verify that the garbage
collector's information is consistent with our type information.Checking the static data table is straightforward: we simply com-pare each bit in the bitmap with the type of the corresponding word
of memory. Checking the object tables is similar: each vtable ob-ject in static data is identified by a special type, so we can extractthe word containing pointer information and use it to check the corresponding class's type information. We give this word a singletontype that depends on the class, which ensures that it cannot be al-tered and that invalid vtables cannot be constructed by user code.
We currently omit checks for fields of classes defined in other com-pilation units when those fields are not directly accessed by theuser's code.

Checking stack tables is the most involved aspect of our garbagecollector verification. After checking each call instruction, we lookup the garbage collection information for the associated return address. This information indicates which arguments, spill slots, andcallee-saved registers contain references, which can be comparedagainst the stack type at that call instruction. In addition, this information indicates whether callee-saved registers have been saved onthis stack. In order to ensure that these values are traced by the col-lector, we must ensure that they were correctly saved to the stack,
which we can do by comparing the fresh type variables assigned toeach callee-saved register with the type of the corresponding stackslot. Finally, we must ensure that the return address saved on the
stack has not been replaced with a return address that happens tohave the same type but is not a valid index into the garbage collec-tor's tables. To do so, we refine the function pointer type to create
an immutable function pointer type that can only be pushed on thestack with a "call" instruction and can only be popped off the stackwith a "ret" instruction.

Finally, we must ensure that any write barriers required by aconcurrent garbage collector have been placed appropriately bythe compiler. In our compiler, write barriers are implemented as
a method invocation that does any appropriate checks in additionto performing the write. Thus, we can check write barriers bymaking locations that require barriers immutable; any writes to
these locations must be performed by invoking the write barrier,which is trusted code outside of the object file. Currently, we use acollector that does not require write barriers.

When implementing these checks, we found it helpful to adaptthe source code for the runtime system directly for use in the ver-ifier. The encodings for these garbage collection tables are quite
complex, so we reused the runtime system's table decoder, addingfurther checks where necessary to guard against invalid encodings.To design the checker, we abstracted the garbage collector's reference scanning code, replacing all direct memory references withreferences to the associated type information. This approach gaveus greater confidence that our verifier's checks match the assumptions of the runtime system.
6. MeasurementsThe compiler has about 150 benchmarks for day-to-day testing.This section shows the numbers for the seven largest ones (Table 1).

The performance numbers were measured on a PC running Win-dows Vista with two 2.66GHz CPUs and 4GB of memory. We useseparate compilation mode to compile the benchmarks separately
from the libraries. The runtime uses a mark-sweep garbage collec-tor. The running times are averaged over five runs of each program.Execution Time. Table 2 shows the performance comparison between TAL, the base-compiler-generated code, and CLR-generated code. The fourth column "TAL/Base" is the second col-umn "TAL" divided by the third column "Base". The TAL code is
0.95-1.09 times slower than the base compiler's generated code,with a geometric mean of 1.02. The TAL code is slightly less effi-cient than the base compiler-generated code because the new compiler does not support inlining of heap allocations. The two bench-marks ahcbench and asmlc are allocation-intensive, and thereforeare affected most. The TAL code is nearly as efficient as the basecompiler-generated code if the base compiler turns off inlining ofheap allocation.

190

Name Description Executable Size (in bytes)ahcbench An implementation of Adaptive Huffman Compression. 4,820,992asmlc A compiler for Abstract State Machine Language. 20,586,496
lcscbench The front end of a C# compiler. 7,766,016mandelform An implementation of mandelbrot set computation. 12,775,424sat solver An implementation of SAT solver written in C#. 4,943,872
selfhost1421 A version of the Bartok Compiler. 7,847,936zinger A model checker to explore the state of the zing model. 13,074,432

Table 1. Benchmarks.

Benchmark TAL Base TAL/Base CLRmandelform 1262.93 1234.92 1.02 1423.14selfhost1421 256.30 235.46 1.09 93.99
ahcbench 4.08 3.75 1.09 4.50lcscsbench 6.82 6.66 1.02 10.55asmlc 0.96 0.92 1.04 1.58
zinger 2.89 3.01 0.96 3.32sat solver 4.50 4.76 0.95 4.31geomean 1.02

Table 2. Execution Time (in seconds)
Benchmark TAL Base TAL/Basemandelform 16.15 15.65 1.03selfhost1421 1087.99 63.54 17.12
ahcbench 6.37 6.24 1.02lcscsbench 153.60 78.09 1.97asmlc 158.78 160.76 0.99
zinger 25.59 17.68 1.45sat solver 9.64 7.18 1.34geomean 1.83

Table 3. Compilation Time (in seconds)
Compilation Time. Table 3 shows the performance comparisonof the TAL compiler and the base compiler. The TAL compiler is0.99-17.12 times slower than the base compiler, with a geometric
mean of 1.83. The slowdown is largely due to type inference andwriting type information to object files. Typed MIR and LIR havetype inference to infer the precondition of each basic block. Typed
LIR type inference and writing type information to object files ac-count for 49% of the compilation slowdown in lcscbench, 20% inzinger, and 13% in sat solver, and 70% in selfhost1421. The benchmark selfhost1421 has significant slowdown because of inefficienttype inference for arrays. Other slowdown is due to a larger numberof IR instructions and MIR/LIR types. Typed MIR and LIR insert
coercion instructions to change types, most of which are no-ops atruntime.Object File Size. Table 4 shows the size comparison of object
files generated by the TAL compiler and the base compiler. TheTAL object files are significantly larger than those generated bythe base compiler: 1.69-2.52 times larger, with a geometric mean
of 2.05. We decided to make the TAL checker simple because thechecker is in the TCB. Therefore, the TAL programs are extensivelyannotated with types. Excluding the type information, the object
files generated by the TAL compiler are as small as those generatedby the base compiler. A smarter checker might not require as muchtype information because it could infer type information, but would
also be more difficult to trust or verify than a simple checker.Type Checking Time. Table 5 shows the TAL checking timecompared with compilation time. The TAL checking time is 1.61%-

Benchmark TAL Base TAL/Basemandelform 117088 65445 1.79selfhost1421 18871862 9012642 2.09
ahcbench 148880 61077 2.44lcscsbench 14205932 8004809 1.78asmlc 31851094 18856426 1.69
zinger 2400600 1076470 2.23sat solver 883589 351134 2.52geomean 2.05

Table 4. Object File Size (in bytes)
Benchmark Checking Time (% of Compilation Time)mandelform 1.61 %selfhost1421 5.67 %
ahcbench 4.40 %lcscsbench 18.78 %asmlc 2.14 %
zinger 18.13 %sat solver 5.87 %geomean 5.59 %

Table 5. Checking Time Compared with Compilation Time
18.78% of the compilation time, with a geometric mean of 5.59%.Type checking TAL is fast, as in prior TAL compilers.
7. Related WorkMany compilers use typed intermediate representations. We com-pare our compiler with a few ones that preserve types to assembly
code. None of them has as many optimizations as Bartok has, orsupports compiler-runtime interface verification.SpecialJ [4] is a certifying compiler for Java. SpecialJ produces
PCC binaries (code with a safety proof), which provides a moregeneral framework than TAL. It supports exception handling viapushing and popping handlers. (Our system uses exception handling tables, and we cannot yet check the tables.) SpecialJ treatsall runtime libraries as primitives, and thus it cannot verify themor inline them into user programs. The SpecialJ paper used handoptimized code as examples and mentioned only two optimizationsthat SpecialJ supported.The TALx86 compiler is a type-preserving compiler from a Clike language to x86 code [10], but it does not perform aggressiveoptimizations such as array bound check elimination. The typesystem lacks support for many modern language features such as
CIL's by-reference parameters.The LTAL compiler compiles core ML programs to typedSPARC code [3]. It has a minimal runtime (no GC) and does not
support proofs about integers.

191

Leroy implemented a certified compiler from a C-like languageto PowerPC assembly code [8], verifying most of the compiler codeitself, not just the compiler's output. The compiler has about 5,000
lines of code with few optimizations. The proof of the compiler isabout 8 times bigger in size than the compiler. It is difficult to scaleto large-scale compilers like Bartok.

Menon et al. proposed an SSA presentation that uses proof vari-ables to encode safety information (for example array bounds) [9].It supports optimizations by treating proof variables as normal variables. SSA is not suitable for TAL. Section 2.4 has discussed otherwork about verifying array-bounds checking.Vanderwaart and Crary developed a type theory for the compilerGC interface, but did not have an implementation [15].8. Conclusion

In this paper, we have presented a large-scale, type-preserving, op-timizing compiler. This work shows that preserving type informa-tion at the assembly language level is practical even in the presence
of aggressive low-level optimizations. We were also able to imple-ment a lightweight verifier that checks the safety of our compiler'soutput, including many aspects of its interaction with the runtime
system. We believe that type-preserving compilation is a useful ap-proach to minimizing the trusted computing base.
AcknowledgmentsWe would like to thank David Tarditi, Jim Larus, and Galen Huntfor their guidance and support of this work, David Tarditi and the
Bartok team for their help with Bartok-related issues, and anony-mous reviewers for their comments on an earlier version of the pa-per.

References[1] R. Bodik, R. Gupta, and V. Sarkar. ABCD: eliminating array-bounds

checks on demand. In ACM SIGPLAN Conference on ProgrammingLanguage Design and Implementation, pages 321-333, 2000.
[2] J. Chen and D. Tarditi. A simple typed intermediate language forobject-oriented languages. In ACM Symposium on Principles ofProgramming Languages, pages 38-49, 2005.

[3] J. Chen, D. Wu, A. W. Appel, and H. Fang. A provably soundTAL for back-end optimization. In ACM SIGPLAN Conference onProgramming Language Design and Implementation, 2003.

[4] C. Colby, P. Lee, G. C. Necula, F. Blau, K. Cline, and M. Plesko.A certifying compiler for Java. In ACM SIGPLAN Conference onProgramming Language Design and Implementation, June 2000.
[5] K. Crary and J. C. Vanderwaart. An expressive, scalable type theoryfor certified code. In ACM SIGPLAN International Conference onFunctional Programming, pages 191-205, 2002.
[6] Microsoft Corp. et al. Common Language Infrastructure. 2002.http://msdn.microsoft.com/net/ecma/.
[7] D. Grossman and J. G. Morrisett. Scalable certification fortyped assembly language. In International Workshop on Types inCompilation, pages 117-146, 2001.

[8] X. Leroy. Formal certification of a compiler back-end or: program-ming a compiler with a proof assistant. In ACM Symposium onPrinciples of Programming Languages, pages 42-54, 2006.
[9] V. S. Menon, N. Glew, B. R. Murphy, A. McCreight, T. Shpeisman,A. Adl-Tabatabai, and L. Petersen. A verifiable ssa programrepresentation for aggressive compiler optimization. In ACM

Symposium on Principles of Programming Languages, pages 397-408, 2006.
[10] G. Morrisett, K. Crary, N. Glew, D. Grossman, R. Samuels, F. Smith,D. Walker, S. Weirich, and S. Zdancewic. TALx86: A realistictyped assembly language. In ACM SIGPLAN Workshop on Compiler

Support for System Software, pages 25-35, 1999.[11] G. Morrisett, D. Walker, K. Crary, and N. Glew. From System F
to typed assembly language. In ACM Symposium on Principles ofProgramming Languages, pages 85-97, 1998.
[12] G. Necula. Proof-Carrying Code. In ACM Symposium on Principlesof Programming Languages, pages 106-119, 1997.
[13] F. Perry, C. Hawblitzel, and J. Chen. Simple and flexible stack types.In International Workshop on Aliasing, Confinement, and Ownership(IWACO), July 2007.

[14] Z. Shao, B. Saha, V. Trifonov, and N. Papaspyrou. A type system forcertified binaries. In ACM Symposium on Principles of ProgrammingLanguages, 2002.
[15] J. C. Vanderwaart and K. Crary. A typed interface for garbagecollection. In ACM SIGPLAN workshop on Types in languagesdesign and implementation, pages 109-122, 2003.
[16] H. Xi and R. Harper. A dependently typed assembly language.In 2001 ACM SIGPLAN International Conference on FunctionalProgramming, pages 169-180, September 2001.

192