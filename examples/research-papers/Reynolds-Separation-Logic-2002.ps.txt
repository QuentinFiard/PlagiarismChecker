

This is a preprint of a paper to appear in the Proceedings of theSeventeenth Annual IEEE Symposium on Logic in Computer Science, to
be held July 22-25, 2002 in Copenhagen, Denmark. Copyright 2002 IEEE.
Separation Logic: A Logic for Shared Mutable Data Structures

John C. Reynolds\Lambda Computer Science Department
Carnegie Mellon Universityjohn.reynolds@cs.cmu.edu

Abstract
In joint work with Peter O'Hearn and others, based onearly ideas of Burstall, we have developed an extension of
Hoare logic that permits reasoning about low-level imperative programs that use shared mutable data structure.

The simple imperative programming language is extended with commands (not expressions) for accessing and
modifying shared structures, and for explicit allocation anddeallocation of storage. Assertions are extended by introducing a "separating conjunction" that asserts that its subformulas hold for disjoint parts of the heap, and a closelyrelated "separating implication". Coupled with the inductive definition of predicates on abstract data structures, thisextension permits the concise and flexible description of
structures with controlled sharing.

In this paper, we will survey the current development ofthis program logic, including extensions that permit unrestricted address arithmetic, dynamically allocated arrays,
and recursive procedures. We will also discuss promisingfuture directions.

1. Introduction

The use of shared mutable data structures, i.e., of struc-tures where an updatable field can be referenced from more
than one point, is widespread in areas as diverse as systemsprogramming and artificial intelligence. Approaches to reasoning about this technique have been studied for threedecades, but the result has been methods that suffer from either limited applicability or extreme complexity, and scalepoorly to programs of even moderate size. (A partial bibliography is given in Reference [28].)

The problem faced by these approaches is that the cor-rectness of a program that mutates data structures usually

\Lambda Portions of the author's own research described in this survey were
supported by National Science Foundation Grant CCR-9804014, and bythe Basic Research in Computer Science (

http://www.brics.dk/)Centre of the Danish National Research Foundation.

depends upon complex restrictions on the sharing in thesestructures. To illustrate this problem, and our approach to
its solution, consider a simple example. The following pro-gram performs an in-place reversal of a list:

j := nil ; while i 6= nil do

(k := [i + 1] ; [i + 1] := j ; j := i ; i := k):

(Here the notation [e] denotes the contents of the storage ataddress

e.)
The invariant of this program must state that i and j arelists representing two sequences

ff and fi such that the re-flection of the initial value
ff0 can be obtained by concate-nating the reflection of
ff onto fi:

9ff; fi: list ff i ^ list fi j ^ ffy0 = ffy\Delta fi;
where the predicate list ff i is defined by induction on thelength of

ff:

list ffl i def= i = nil list(a\Delta ff) i def= 9j: i ,! a; j ^ list ff j
(and ,! can be read as "points to").

Unfortunately, however, this is not enough, since the pro-gram will malfunction if there is any sharing between the

lists i and j. To prohibit this we must extend the invariant toassert that only

nil is reachable from both i and j:

(9ff; fi: list ff i ^ list fi j ^ ffy0 = ffy\Delta fi)

^ (8k: reach(i; k) ^ reach(j; k) ) k = nil); (1)

where

reach(i; j) def= 9n * 0: reachn(i; j)
reach0(i; j) def= i = j
reachn+1(i; j) def= 9a; k: i ,! a; k ^ reachn(k; j):
Even worse, suppose there is some other list x, repre-senting a sequence

fl, that is not supposed to be affected by

the execution of our program. Then it must not share witheither

i or j, so that the invariant becomes

(9ff; fi: list ff i ^ list fi j ^ ffy0 = ffy\Delta fi) ^ list fl x

^ (8k: reach(i; k) ^ reach(j; k) ) k = nil)
^ (8k: reach(x; k) ^ (reach(i; k) . reach(j; k))

) k = nil):

(2)

Even in this trivial situation, where all sharing is prohibited,it is evident that this form of reasoning scales poorly.

The key to avoiding this difficulty is to introduce a novellogical operation

P \Lambda  Q, called separating conjunction (orsometimes independent or spatial conjunction), that asserts

that P and Q hold for disjoint portions of the addressablestorage. In effect, the prohibition of sharing is built into this
operation, so that Invariant (1) can be written as

(9ff; fi: list ff i \Lambda  list fi j) ^ ffy0 = ffy\Delta fi; (3)
and Invariant (2) as

(9ff; fi: list ff i \Lambda  list fi j \Lambda  list fl x) ^ ffy0 = ffy\Delta fi: (4)
In fact, one can go further: Using an inference rule calledthe "frame rule", one can infer directly that the program

does not affect the list x from the fact that assertions such as(3) do not refer to this list.

The central concept of a separating conjunction is im-plicit in Burstall's early idea of a "distinct nonrepeating tree
system" [2]. In lectures in the fall of 1999, I described theconcept explicitly, and embedded it in a flawed extension of
Hoare logic [16, 17]. Soon thereafter, an intuitionistic logicbased on this idea was discovered independently by Ishtiaq
and O'Hearn [19] and by myself [28]. Realizing that thislogic was an instance of the logic of bunched implications
[23, 26], Ishtiaq and O'Hearn also introduced a separatingimplication

P \Gamma \Lambda  Q.The intuitionistic character of this logic implied a monotonicity property: that an assertion true for some portion ofthe addressable storage would remain true for any extension
of that portion, such as might be created by later storage al-location.

In their paper, however, Ishtiaq and O'Hearn also pre-sented a classical version of the logic that does not impose
this monotonicity property, and can therefore be used to rea-son about explicit storage deallocation; they showed that
this version is more expressive than the intuitionistic logic,since the latter can be translated into the classical logic.

In both the intuitionistic and classical version of thelogic, addresses were assumed to be disjoint from integers, and to refer to entire records rather than particularfields, so that "address arithmetic" was precluded. More
recently, I generalized the logic to permit reasoning aboutunrestricted address arithmetic, by regarding addresses as

integers which refer to individual fields [24, 30, 29]. It isthis form of the logic that will be described and used in
most of the present paper. We will also describe O'Hearn'sframe rule [24, 35, 34, 19], which permits local reasoning
about components of programs.

Since these logics are based on the idea that the structureof an assertion can describe the separation of storage into

disjoint components, we have come to use the term sepa-ration logics, both for the extension of predicate calculus
with the separation operators and the resulting extension ofHoare logic. A more precise name might be

storage separa-tion logics, since it is becoming apparent that the underlying

idea can be generalized to describe the separation of otherkinds of resources [3, 11, 12, 9, 10].

2. The Programming Language

The programming language we will use is the simple im-perative language originally axiomatized by Hoare [16, 17],
extended with new commands for the manipulation of mu-table shared data structures:

hcommi ::= \Delta  \Delta  \Delta 

j hvari := cons(hexpi; : : : ; hexpi) allocation
j hvari := [hexpi] lookup
j [hexpi] := hexpi mutation
j dispose hexpi deallocation

Semantically, we extend computational states to containtwo components: a store (or stack), mapping variables into
values (as in the semantics of the unextended simple imper-ative language), and a heap, mapping addresses into values
(and representing the mutable structures).

In the early versions of separation logic, integers, atoms,and addresses were regarded as distinct kinds of value,

and heaps were mappings from finite sets of addresses tononempty tuples of values:

Values = Integers [ Atoms [ Addresses

where Integers, Atoms, and Addresses are disjoint

Heaps =

S

A

fin

`Addresses

(A ! Values+):

To permit unrestricted address arithmetic, however, inthe version of the logic used in most of this paper we will
assume that all values are integers, an infinite number ofwhich are addresses; we also assume that atoms are integers that are not addresses, and that heaps map addresses

2

into single values:

Values = Integers
Atoms [ Addresses ` Integers

where Atoms and Addresses are disjoint

Heaps =

S

A

fin

`Addresses

(A ! Values):

(To permit unlimited allocation of records of arbitrary size,we require that, for all

n * 0, the set of addresses must con-tain infinitely many consecutive sequences of length

n. Forinstance, this will occur if only a finite number of positive

integers are not addresses.) In both versions of the logic, weassume

nil 2 Atoms
StoresV = V ! Values
StatesV = StoresV \Theta  Heaps;
where V is a finite set of variables.Our intent is to capture the low-level character of machine language. One can think of the store as describing thecontents of registers, and the heap as describing the contents of an addressable memory. This view is enhanced byassuming that each address is equipped with an "activity
bit"; then the domain of the heap is the finite set of activeaddresses.

The semantics of ordinary and boolean expressions is thesame as in the simple imperative language:

[[e 2 hexpi]]exp 2 (

S

V

fin

'FV(e)

StoresV ) ! Values

[[b 2 hboolexpi]]bexp 2

(

S

V

fin

'FV(b)

StoresV ) ! ftrue; falseg

(where FV(p) is the set of variables occurring free in thephrase

p). In particular, expressions do not depend upon theheap, so that they are always well-defined and never cause

side-effects.Thus expressions do not contain notations, such as

consor
[\Gamma ], that refer to the heap. It follows that none of the newheap-manipulating commands are instances of the simple

assignment command hvari := hexpi (even though we writethem with the familiar operator

:=). In fact, they will notobey Hoare's inference rule for assignment. However, since

they alter the store at the variable v, we will say that thecommands

v := cons(\Delta  \Delta  \Delta ) and v := [e], as well as v := e(but not
[v] := e or dispose v) modify v.A simple way to define the meaning of the new commands is by small-step operational semantics, i.e., by defin-ing a transition relation

; between configurations, whichare either

ffl nonterminal: A command-state pair hc; (s; h)i, where

FV(c) ` dom s.

ffl terminal: A state (s; h) or abort.
We write fl ;\Lambda  fl0 to indicate that there is a finite sequenceof transitions from

fl to fl0, and fl " to indicate that there isan infinite sequence of transitions beginning with

fl.In this semantics, the heap-manipulating commands are

specified by the following inference rules:

ffl Allocation

hv := cons(e1; : : : ; en); (s; h)i

; ([ s j v: ` ]; [ h j `: [[e1]]exps j : : : j `+n\Gamma 1: [[en]]exps ]);

where `; : : : ; ` + n \Gamma  1 2 Addresses \Gamma  dom h.
ffl Lookup

When [[e]]exps 2 dom h:

hv := [e]; (s; h)i ; ([ s j v: h([[e]]exps) ]; h);
When [[e]]exps =2 dom h:

hv := [e]; (s; h)i ; abort:
ffl Mutation

When [[e]]exps 2 dom h:

h[e] := e0; (s; h)i ; (s; [ h j [[e]]exps: [[e0]]exps ]);
When [[e]]exps =2 dom h:

h[e] := e0; (s; h)i ; abort:
ffl Deallocation

When [[e]]exps 2 dom h:

hdispose e; (s; h)i ; (s; he(dom h \Gamma  f[[e]]expsg));
When [[e]]exps =2 dom h:

hdispose e; (s; h)i ; abort:
(Here [ f j x: a ] denotes the function that maps x into aand all other arguments

y in the domain of f into f y. Thenotation
f eS denotes the restriction of the function f to thedomain
S.)The allocation operation activates and initializes

n cellsin the heap. Notice that, aside from the requirement that

the addresses of these cells be consecutive and previouslyinactive, the choice of addresses is indeterminate.

The remaining operations all cause memory faults (de-noted by the terminal configuration

abort) if an inactiveaddress is dereferenced or deallocated.

3

An important property of this language is the effect ofrestricting the heap on the execution of a command. Essentially, if the restriction removes an address that is derefer-enced or deallocated by the command, then the restricted
execution aborts; otherwise, however, the executions aresimilar, except for the presence of unchanging extra heap
cells in the unrestricted execution.To state this property precisely, we write

h0 ? h1 toindicate that the heaps
h0 and h1 have disjoint domains,and
h0 \Delta  h1 to indicate the union of such heaps. Then, when
h0 ` h,

ffl If hc; (s; h)i ;\Lambda  abort, then hc; (s; h0)i ;\Lambda  abort.
ffl If hc; (s; h)i ;\Lambda  (s0; h0) then hc; (s; h0)i ;\Lambda  abortor

hc; (s; h0)i ;\Lambda  (s0; h00), where h00 ? h1 and h0 =
h00 \Delta  h1.

ffl If hc; (s; h)i " then either hc; (s; h0)i ;\Lambda  abort or

hc; (s; h0)i ".

3. Assertions and their Inference Rules

In addition to the usual formulae of predicate calculus,including boolean expressions and quantifiers, we introduce
four new forms of assertion that describe the heap:

hasserti ::= \Delta  \Delta  \Delta 

j emp empty heap
j hexpi 7! hexpi singleton heap
j hasserti \Lambda  hasserti separating conjunction
j hasserti \Gamma \Lambda  hasserti separating implication

Because of these new forms, the meaning of an assertion(unlike that of a boolean expression) depends upon both the

store and the heap:

[[p 2 hasserti]]asrt 2

(

S

V

fin

'FV(p)

StoresV ) ! Heaps ! ftrue; falseg:

Specifically, emp asserts that the heap is empty:

[[emp]]asrts h iff dom h = fg;
e 7! e0 asserts that the heap contains one cell, at address ewith contents

e0:

[[e 7! e0]]asrts h iff

dom h = f[[e]]expsg and h([[e]]exps) = [[e0]]exps;
p0 \Lambda  p1 asserts that the heap can be split into two disjointparts in which

p0 and p1 hold respectively:

[[p0 \Lambda  p1]]asrts h iff

9h0; h1: h0 ? h1 and h0 \Delta  h1 = h and

[[p0]]asrts h0 and [[p1]]asrts h1;

and p0 \Gamma \Lambda  p1 asserts that, if the current heap is extendedwith a disjoint part in which

p0 holds, then p1 will hold inthe extended heap:

[[p0 \Gamma \Lambda  p1]]asrts h iff

8h0: (h0 ? h and [[p0]]asrts h0) implies

[[p1]]asrts (h \Delta  h0):

When this semantics is coupled with the usual interpretationof the classical connectives, the result is an instance of the
"resource semantics" of bunched implications as advancedby David Pym [23, 26].

It is useful to introduce several more complex forms asabbreviations:

e 7! \Gamma  def= 9x0: e 7! x0 where x0 not free in e
e ,! e0 def= e 7! e0 \Lambda  true
e 7! e1; : : : ; en

def= e 7! e

1 \Lambda  \Delta  \Delta  \Delta  \Lambda  e + n \Gamma  1 7! en

e ,! e1; : : : ; en

def= e ,! e

1 \Lambda  \Delta  \Delta  \Delta  \Lambda  e + n \Gamma  1 ,! en

iff e 7! e1; : : : ; en \Lambda  true:

By using 7!, ,!, and the two forms of conjunction, it iseasy to describe simple sharing patterns concisely:

1. x 7! 3; y asserts that x points to an adjacent pair ofcells containing

3 and y (i.e., the store maps x and yinto some values

ff and fi, ff is a address, and the heapmaps
ff into 3 and ff + 1 into fi):

y
3-x

2. y 7! 3; x asserts that y points to an adjacent pair ofcells containing

3 and x:

x
3-y

3. x 7! 3; y \Lambda  y 7! 3; x asserts that situations (1) and (2)hold for separate parts of the heap:

ffi
3-x

ffi
3 oe y*Y

4

4. x 7! 3; y ^ y 7! 3; x asserts that situations (1) and (2)hold for the same heap, which can only happen if the

values of x and y are the same:

ffi
3HHj\Phi \Phi *
x

y

y

5. x ,! 3; y ^ y ,! 3; x asserts that either (3) or (4) mayhold, and that the heap may contain additional cells.
There are also simple examples that reveal the occasionallysurprising behavior of separating conjunction. Suppose

s xand
s y are distinct addresses, so that

h1 = fhs x; 1ig and h2 = fhs y; 2ig
are heaps with disjoint singleton domains. Then

If p is: then [[p]]asrts h is:
x 7! 1 h = h1
y 7! 2 h = h2
x 7! 1 \Lambda  y 7! 2 h = h1 \Delta  h2
x 7! 1 \Lambda  x 7! 1 false
x 7! 1 . y 7! 2 h = h1 or h = h2
x 7! 1 \Lambda  (x 7! 1 . y 7! 2) h = h1 \Delta  h2

(x 7! 1 . y 7! 2)

\Lambda  (x 7! 1 . y 7! 2) h = h1 \Delta  h2

x 7! 1 \Lambda  y 7! 2

\Lambda  (x 7! 1 . y 7! 2) false

x 7! 1 \Lambda  true h1 ` h
x 7! 1 \Lambda  : x 7! 1 h1 ` h:

To illustrate separating implication, suppose that an as-sertion

p holds for a store that maps the variable x into anaddress

ff and a heap h that maps ff into 16. Then

(x 7! 16) \Gamma \Lambda  p
holds for the same store and the heap he(dom h \Gamma  fffg)obtained by removing

ff from the domain of h, and

x 7! 9 \Lambda  ((x 7! 16) \Gamma \Lambda  p)
holds for the same store and the heap [ h j ff: 9 ] that differsfrom

h by mapping ff into 9. (Thus, anticipating the conceptof partial-correctness specification introduced in the next

section, fx 7! 9 \Lambda  ((x 7! 16) \Gamma \Lambda  p)g [x] := 16 fpg.)The inference rules for predicate calculus remain sound
in this enriched setting. Before presenting sound rules forthe new forms of assertions, however, we note two rules that
fail. Taking p to be x 7! 1 and q to be y 7! 2, one can see

that neither contraction, p ) p \Lambda  p, nor weakening, p \Lambda 
q ) p, are sound. Thus separation logic is a substructurallogic. (It is not, however, a species of linear logic, since in

linear logic P ) Q can be written (!P ) \Gamma \Lambda  Q, so the rule ofdereliction gives the validity of

(P \Gamma \Lambda  Q) ) (P ) Q). Butin a state where
x 7! 1 is true, (x 7! 1) \Gamma \Lambda  false is true but
(x 7! 1) ) false is false [19].)Sound axiom schemata for separating conjunction include commutative and associative laws, the fact that empis a neutral element, and various distributive and semidistributive laws:

p1 \Lambda  p2 , p2 \Lambda  p1

(p1 \Lambda  p2) \Lambda  p3 , p1 \Lambda  (p2 \Lambda  p3)

p \Lambda  emp , p
(p1 . p2) \Lambda  q , (p1 \Lambda  q) . (p2 \Lambda  q)
(p1 ^ p2) \Lambda  q ) (p1 \Lambda  q) ^ (p2 \Lambda  q)

(9x: p) \Lambda  q , 9x: (p \Lambda  q) when x not free in q
(8x: p) \Lambda  q ) 8x: (p \Lambda  q) when x not free in q:
There is also an inference rule showing that separating con-junction is monotone with respect to implication:

p1 ) p2 q1 ) q2

p1 \Lambda  q1 ) p2 \Lambda  q2;
and two further rules capturing the adjunctive relationshipbetween separating conjunction and separating implication:

p1 \Lambda  p2 ) p3
p1 ) (p2 \Gamma \Lambda  p3)

p1 ) (p2 \Gamma \Lambda  p3)

p1 \Lambda  p2 ) p3:
There are several semantically defined classes of asser-tions that have useful special properties, and contain an easily defined syntactic subclass.An assertion is said to be pure if, for any store, it is independent of the heap. Syntactically, an assertion is pure ifit does not contain

emp, 7!, or ,!. The following axiomschemata show that, when assertions are pure, the distinction between the two kinds of conjunctions, or of implica-tions, collapses:

p1 ^ p2 ) p1 \Lambda  p2 when p1 or p2 is pure
p1 \Lambda  p2 ) p1 ^ p2 when p1 and p2 are pure
(p ^ q) \Lambda  r , p ^ (q \Lambda  r) when p is pure

(p1 \Gamma \Lambda  p2) ) (p1 ) p2) when p1 is pure

(p1 ) p2) ) (p1 \Gamma \Lambda  p2) when p1 and p2 are pure:
We say that an assertion p is intuitionistic iff, for allstores

s and heaps h and h0:

h ` h0 and [[p]]asrt(s; h) implies [[p]]asrt(s; h0):

5

One can show that p \Lambda  true is the strongest intuitionisticassertion weaker than

p, and that true \Gamma \Lambda  p is the weakestintuitionistic assertion stronger than

p. Syntactically, If pand
q are intuitionistic assertions, r is any assertion, and eand
e0 are expressions, then the following are intuitionisticassertions:

Any pure assertion e ,! e0

r \Lambda  true true \Gamma \Lambda  r

p ^ q p . q
8v: p 9v: p
p \Lambda  q p \Gamma \Lambda  q:

For intuitionistic assertions, we have

(p \Lambda  true) ) p when p is intuitionistic

p ) (true \Gamma \Lambda  p) when p is intuitionistic:

It should also be noted that, if we define the operations

i:

p def= true \Gamma \Lambda  (: p)

p i) q def= true \Gamma \Lambda  (p ) q)
p i, q def= true \Gamma \Lambda  (p , q);

then the assertions built from pure assertions and e ,! e0,using these operations and

^, ., 8, 9, \Lambda  , and \Gamma \Lambda  form theimage of Ishtiaq and O'Hearn's modal translation from intuitionistic separation logic to the classical version [19].

Yang [33, 34] has singled out the class of strictly exactassertions; an assertion

q is strictly exact iff, for all s, h, and
h0, [[q]]asrtsh and [[q]]asrtsh0 implies h = h0. Syntactically,assertions built from expressions using

7! and \Lambda  are strictlyexact. The utility of this concept is that

(q \Lambda  true) ^ p ) q \Lambda  (q \Gamma \Lambda  p) when q is strictly exact.

Strictly exact assertions also belong to the broader classof
domain-exact assertions; an assertion q is domain-exactiff, for all

s, h, and h0, [[q]]asrtsh and [[q]]asrtsh0 implies
dom h = dom h0. Syntactically, assertions built from ex-pressions using

7!, \Lambda  , and quantifiers are domain-exact.When
q is such an assertion, the semidistributive laws givenearlier become full distributive laws:

(p1 \Lambda  q) ^ (p2 \Lambda  q) ) (p1 ^ p2) \Lambda  q

when q is domain-exact

8x: (p \Lambda  q) ) (8x: p) \Lambda  q

when x not free in q and q is domain-exact:

Finally, we give axiom schemata for the predicate 7!.

(Regrettably, these are far from complete.)

e1 7! e01 ^ e2 7! e02 , e1 7! e01 ^ e1 = e2 ^ e01 = e02
e1 ,! e01 \Lambda  e2 ,! e02 ) e1 6= e2

emp , 8x: :(x ,! \Gamma ):

Both the assertion language developed in this section andthe programming language developed in the previous section are limited by the fact that all variables range over theintegers. Later in this paper we will go beyond this limitation in ways that are sufficiently obvious that we will notformalize their semantics, e.g., we will use variables denoting abstract data types, predicates, and assertions in the as-sertion language, and variables denoting procedures in the
programming language. (We will not, however, considerassignment to such variables.)

4. Specifications and their Inference Rules

We use a notion of program specification that is similarto that of Hoare logic, with variants for both partial and total
correctness:

hspeci ::= fhassertig hcommi fhassertig partial

j [ hasserti ] hcommi [ hasserti ] total

Let V = FV(p) [ FV(c) [ FV(q). Then

fpg c fqg holds iff 8(s; h) 2 StatesV : [[p]]asrts h implies

: (c; (s; h) ;\Lambda  abort)
and (8(s0; h0) 2 StatesV :

c; (s; h) ;\Lambda  (s0; h0) implies [[q]]asrts0 h0);

and

[ p ] c [ q ] holds iff 8(s; h) 2 StatesV : [[p]]asrts h implies

: (c; (s; h) ;\Lambda  abort)
and : (c; (s; h) ")
and (8(s0; h0) 2 StatesV :

c; (s; h) ;\Lambda  (s0; h0) implies [[q]]asrts0 h0):

Notice that specifications are implicitly quantified over bothstores and heaps, and also (since allocation is indeterminate) over all possible executions. Moreover, any executiongiving a memory fault falsifies both partial and total specifications.As O'Hearn [19] paraphrased Milner, "Well-specified
programs don't go wrong." As a consequence, during theexecution of programs that have been proved to meet their
specifications, it is unnecessary to check for memory faults,or even to equip heap cells with activity bits (assuming that

6

programs are only executed in initial states satisfying therelevant precondition).

Roughly speaking, the fact that specifications precludememory faults acts in concert with the indeterminacy of allocation to prohibit violations of record boundaries. Butsometimes the notion of record boundaries dissolves, as in
the following valid specification of a program that tries toform a two-field record by gluing together two one-field
records:

fx 7! \Gamma  \Lambda  y 7! \Gamma g
if y = x + 1 then skip else

if x = y + 1 then x := y else

(dispose x ; dispose y ; x := cons(1; 2))
fx 7! \Gamma ; \Gamma g:

In our new setting, the command-specific inference rulesof Hoare logic remain sound, as do such structural rules as

ffl Consequence

p0 ) p fpg c fqg q ) q0

fp0g c fq0g:

ffl Auxiliary Variable Elimination

fpg c fqg
f9v: pg c f9v: qg;
where v is not free in c.
ffl Substitution

fpg c fqg
(fpg c fqg)=v1 ! e1; : : : ; vn ! en;
where v1; : : : ; vn are the variables occurring free in p,
c, or q, and, if vi is modified by c, then ei is a variablethat does not occur free in any other

ej.

(All of the inference rules presented in this section are thesame for partial and total correctness.)

An exception is what is sometimes called the "rule ofconstancy" [27, Section 3.3.5; 28, Section 3.5]:

fpg c fqg
fp ^ rg c fq ^ rg;
where no variable occurring free in r is modified by c. Ithas long been understood that this rule is vital for scalability, since it permits one to extend a "local" specification of
c, involving only the variables actually used by that com-mand, by adding arbitrary predicates about variables that

are not modified by c and will therefore be preserved by itsexecution.

Unfortunately, however, the rule of constancy is notsound for separation logic. For example, the conclusion of
the instance

fx 7! \Gamma g [x] := 4 fx 7! 4g
fx 7! \Gamma  ^ y 7! 3g [x] := 4 fx 7! 4 ^ y 7! 3g

is not valid, since its precondition does not preclude thealiasing that will occur if

x = y.O'Hearn realized, however, that the ability to extend local specifications can be regained at a deeper level by usingseparating conjunction. In place of the rule of constancy, he
proposed the frame rule:

ffl Frame Rule

fpg c fqg

fp \Lambda  rg c fq \Lambda  rg;
where no variable occurring free in r is modified by c.

By using the frame rule, one can extend a local specifica-tion, involving only the variables and parts of the heap that

are actually used by c (which O'Hearn calls the footprint of
c), by adding arbitrary predicates about variables and partsof the heap that are not modified or mutated by

c. Thus, theframe rule is the key to "local reasoning" about the heap:

To understand how a program works, it shouldbe possible for reasoning and specification to be
confined to the cells that the program actually ac-cesses. The value of any other cell will automatically remain unchanged [24].
Every valid specification fpg c fqg is "tight" in the sensethat every cell in its footprint must either be allocated by

c or asserted to be active by p; "locality" is the oppositeproperty that everything asserted to be active belongs to the
footprint. The role of the frame rule is to infer from a localspecification of a command the more global specification
appropriate to the larger footprint of an enclosing command.To see the soundness of the frame rule [35, 34], assume
fpg c fqg, and [[p \Lambda  r]]asrts h. Then there are h0 and h1such that

h0 ? h1, h = h0 \Delta  h1, [[p]]s h0 and [[r]]s h1.

ffl Suppose hc; (s; h)i ;\Lambda  abort. Then, by by the prop-erty described at the end of Section 2, we would have

hc; (s; h0)i ;\Lambda  abort, which contradicts fpg c fqgand

[[p]]s h0.

ffl Suppose hc; (s; h)i ;\Lambda  (s0; h0). As in the previ-ous case,

hc; (s; h0)i ;\Lambda  abort would contradict
fpg c fqg and [[p]]s h0, so that, by the property atthe end of Section 2,

hc; (s; h0)i ;\Lambda  (s0; h00), where
h00 ? h1 and h0 = h00 \Delta  h1. Then fpg c fqg and [[p]]s h0implies that

[[q]]s0h00. Moreover, since hc; (s; h)i ;\Lambda 
(s0; h0), the stores s and s0 will give the same value

7

to the variables that are not modified by c. Then, sincethese include all the free variables of

r, [[r]]s h1 impliesthat
[[r]]s0h1. Thus [[q \Lambda  r]]s0h0.

Yang [35, 34] has also shown that the frame rule is com-plete in the following sense: Suppose that all we know

about a command c is that some partial correctness speci-fication

fpg c fqg is valid. Then, whenever the validity of
fp0g c fq0g is a semantic consequence of this knowledge,
fp0g c fq0g can be derived from fpg c fqg by use of theframe rule and the rules of consequence, auxiliary variable

elimination, and substitution.Using the frame rule, one can move from local versions
of inference rules for the primitive heap-manipulating com-mands to equivalent global versions. For mutation, for example, the obvious local rule

ffl Mutation (local)

fe 7! \Gamma g [e] := e0 fe 7! e0g
leads directly to

ffl Mutation (global)

f(e 7! \Gamma ) \Lambda  rg [e] := e0 f(e 7! e0) \Lambda  rg:
(One can rederive the local rule from the global one by tak-ing

r to be emp.) Moreover, by taking r in the globalrule to be

(e 7! e0) \Gamma \Lambda  p and using the valid implication
q \Lambda  (q \Gamma \Lambda  p) ) p, one can derive a third rule for mutationthat is suitable for backward reasoning [19], since one can

substitute any assertion for the postcondition p:

ffl Mutation (backwards reasoning)

f(e 7! \Gamma ) \Lambda  ((e 7! e0) \Gamma \Lambda  p)g [e] := e0 fpg:
(One can rederive the global rule from the backward one bytaking

p to be (e 7! e0) \Lambda  r and using the valid implication
(p \Lambda  r) ) (p \Lambda  (q \Gamma \Lambda  (q \Lambda  r))).)A similar development works for deallocation, except

that the global form is itself suitable for backward reason-ing:

ffl Deallocation (local)

fe 7! \Gamma g dispose e fempg:
ffl Deallocation (global, backwards reasoning)

f(e 7! \Gamma ) \Lambda  rg dispose e frg:
In the same way, one can give equivalent local and globalrules for "noninterfering" allocation commands that modify
"fresh" variables. Here we abbreviate e1; : : : ; en by e.

ffl Allocation (noninterfering, local)

fempg v := cons(e) fv 7! eg;
where v is not free in e.
ffl Allocation (noninterfering, global)

frg v := cons(e) f(v 7! e) \Lambda  rg;
where v is not free in e or r.
However, to avoid the restrictions on occurrences of theassigned variable, or to give a backward-reasoning rule,

or rules for lookup, we must introduce and often quantifyadditional variables. For both allocation and lookup, we
can give equivalent rules of the three kinds, but the rele-vant derivations are more complicated than before since one
must use auxiliary variable elimination, properties of quan-tifiers, and other laws.

In these rules we indicate substitution by priming meta-variables denoting expressions and assertions, e.g., we write
e0i for ei=v ! v0 and r0 for r=v ! v0. We also abbreviate
e1; : : : ; en by e and e01; : : : ; e0n by e0.

ffl Allocation (local)

fv = v0 ^ empg v := cons(e) fv 7! e0g;
where v0 is distinct from v.
ffl Allocation (global)

frg v := cons(e) f9v0: (v 7! e0) \Lambda  r0g;
where v0 is distinct from v, and is not free in e or r.
ffl Allocation (backwards reasoning)

f8v0: (v0 7! e) \Gamma \Lambda  p0g v := cons(e) fpg;
where v0 is distinct from v, and is not free in e or p.
ffl Lookup (local)

fv = v0 ^ (e 7! v00)g v := [e] fv = v00 ^ (e0 7! v00)g;
where v, v0, and v00 are distinct.
ffl Lookup (global)

f9v00: (e 7! v00) \Lambda  (r=v0 ! v)g v := [e]

f9v0: (e0 7! v) \Lambda  (r=v00 ! v)g;

where v, v0, and v00 are distinct, v0 and v00 do not occurfree in

e, and v is not free in r.

8

ffl Lookup (backwards reasoning)

f9v0: (e 7! v0) \Lambda  ((e 7! v0) \Gamma \Lambda  p0)g v := [e] fpg;
where v0 is not free in e, nor free in p unless it is v.
Finally, since e 7! v0 is strictly exact, it is easy to obtain anequivalent but more succinct rule for backward reasoning
about lookup:

ffl Lookup (alternative backward reasoning)

f9v0: (e ,! v0) ^ p0g v := [e] fpg;
where v0 is not free in e, nor free in p unless it is v.
For all four new commands, the backward reasoning formsgive complete weakest preconditions [19, 34].

As a simple illustration, the following is a detailed proofoutline of a local specification of a command that uses allocation and mutation to construct a two-element cyclic struc-ture containing relative addresses:

fempg
x := cons(a; a) ;
fx 7! a; ag
y := cons(b; b) ;
f(x 7! a; a) \Lambda  (y 7! b; b)g
f(x 7! a; \Gamma ) \Lambda  (y 7! b; \Gamma )g
[x + 1] := y \Gamma  x ;
f(x 7! a; y \Gamma  x) \Lambda  (y 7! b; \Gamma )g
[y + 1] := x \Gamma  y ;
f(x 7! a; y \Gamma  x) \Lambda  (y 7! b; x \Gamma  y)g
f9o: (x 7! a; o) \Lambda  (x + o 7! b; \Gamma  o)g:

5. Lists

To specify a program adequately, it is usually necessaryto describe more than the form of its structures or the sharing patterns between them; one must relate the states of theprogram to the abstract values that they denote. To do so, it
is necessary to define the set of abstract values algebraicallyor recursively, and to define predicates on the abstract values by structural induction. Since these are standard meth-ods of definition, we will treat them less formally than the
novel aspects of our logic.

For lists, the relevant abstract values are sequences, forwhich we use the following notation: When

ff and fi aresequences, we write

ffl ffl for the empty sequence.
ffl [x] for the single-element sequence containing x. (Wewill omit the brackets when

x is not a sequence.)

ffl ff\Delta fi for the composition of ff followed by fi.
ffl ffy for the reflection of ff.
ffl #ff for the length of ff.
ffl ffi for the ith component of ff.

The simplest list structure for representing sequences isthe

singly-linked list. To describe this representation, wewrite

list ff (i; j) when there is a list segment from i to j rep-resenting the sequence

ff:

ffi
ff1-i

ffi
ff2

j
ffn* *
\Delta  \Delta  \Delta  *

It is straightforward to define this predicate by induction onthe structure of

ff:

list ffl (i; j) def= emp ^ i = j
list a\Delta ff (i; k) def= 9j: i 7! a; j \Lambda  list ff (j; k);
and to prove some basic properties:

list a (i; j) , i 7! a; j
list ff\Delta fi (i; k) , 9j: list ff (i; j) \Lambda  list fi (j; k)

list ff\Delta b (i; k) , 9j: list ff (i; j) \Lambda  j 7! b; k:
(The second property is a composition law that can beproved by structural induction on

ff.)In comparison with the definition of

list in the intro-duction, we have generalized from lists to list segments,

and we have used separating conjunction to prohibit cycles
within the list segment. More precisely, when list ff1 \Delta  \Delta  \Delta  \Delta  \Delta 
ffn (i0; in), we have

9i1; : : : in\Gamma 1:

(i0 7! ff1; i1) \Lambda  (i1 7! ff2; i2) \Lambda  \Delta  \Delta  \Delta  \Lambda  (in\Gamma 1 7! ffn; in):

Thus i0, . . . , in\Gamma 1 are distinct, but in is not constrained, sothat

list ff1 \Delta  \Delta  \Delta  \Delta  \Delta  ffn (i; i) may hold for any n * 0.Thus the obvious properties

list ff (i; j) ) (i = nil ) (ff = ffl ^ j = nil))
list ff (i; j) ) (i 6= j ) ff 6= ffl)
do not say whether ff is empty when i = j 6= nil. How-ever, there are some common situations that insure that

ff isempty when
i = j 6= nil, for example, when j refers to aheap cell separate from the list segment, or to the beginning

of a separate list:

9

list ff (i; j) \Lambda  j ,! \Gamma  ) (i = j , ff = ffl)
list ff (i; j) \Lambda  list fi (j; nil) ) (i = j , ff = ffl):

On the other hand, sometimes i = j simply does not de-termine emptiness. For example, a cyclic buffer containing
ff in its active segment and fi in its inactive segment is de-scribed by

list ff (i; j) \Lambda  list fi (j; i). Here, the buffer may beeither empty or full when

i = j.
The use of list is illustrated by a proof outline for a com-mand that deletes the first element of a list:

flist a\Delta ff (i; k)g
f9j: i 7! a; j \Lambda  list ff (j; k)g
fi 7! a \Lambda  9j: i + 1 7! j \Lambda  list ff (j; k)g
j := [i + 1] ;
fi 7! a \Lambda  i + 1 7! j \Lambda  list ff (j; k)g
dispose i ;
fi + 1 7! j \Lambda  list ff (j; k)g
dispose i + 1 ;
flist ff (j; k)g
i := j
flist ff (i; k)g

A more complex example is the body of the while commandin the list-reversing program in the Introduction; here the
final assertion is the invariant of the while command:

f9ff; fi: (list ff (i; nil) \Lambda  list fi (j; nil))

^ ffy0 = ffy\Delta fi ^ i 6= nilg
f9a; ff; fi: (list a\Delta ff (i; nil) \Lambda  list fi (j; nil))

^ ffy0 = (a\Delta ff)y\Delta fig
f9a; ff; fi; k: (i 7! a; k \Lambda  list ff (k; nil) \Lambda  list fi (j; nil))

^ ffy0 = (a\Delta ff)y\Delta fig
k := [i + 1] ;
f9a; ff; fi: (i 7! a; k \Lambda  list ff (k; nil) \Lambda  list fi (j; nil))

^ ffy0 = (a\Delta ff)y\Delta fig
[i + 1] := j ;
f9a; ff; fi: (i 7! a; j \Lambda  list ff (k; nil) \Lambda  list fi (j; nil))

^ ffy0 = (a\Delta ff)y\Delta fig
f9a; ff; fi: (list ff (k; nil) \Lambda  list a\Delta fi (i; nil))

^ ffy0 = ffy\Delta a\Delta fig
f9ff; fi: (list ff (k; nil) \Lambda  list fi (i; nil)) ^ ffy0 = ffy\Delta fig
j := i ; i := k
f9ff; fi: (list ff (i; nil) \Lambda  list fi (j; nil)) ^ ffy0 = ffy\Delta fig:

A more elaborate representation of sequences isprovided by

doubly-linked lists. Here, we write
dlist ff (i; i0; j; j0) when ff is represented by a doubly-linkedlist segment with a forward linkage (via second fields) from

i to j, and a backward linkage (via third fields) from j0 to i0:

i0

ffi
ff1-i

ffi
ffi
ff2

ffi

j
ffn j0oe* * *k k k
\Delta  \Delta  \Delta 

\Delta  \Delta  \Delta 

The inductive definition is

dlist ffl (i; i0; j; j0) def= emp ^ i = j ^ i0 = j0
dlist a\Delta ff (i; i0; k; k0) def= 9j: i 7! a; j; i0 \Lambda  dlist ff (j; i; k; k0);
from which one can prove the basic properties:

dlist a (i; i0; j; j0) , i 7! a; j; i0 ^ i = j0
dlist ff\Delta fi (i; i0; k; k0) ,

9j; j0: dlist ff (i; i0; j; j0) \Lambda  dlist fi (j; j0; k; k0)

dlist ff\Delta b (i; i0; k; k0) ,

9j0: dlist ff (i; i0; k0; j0) \Lambda  k0 7! b; k; j0:

The utility of unrestricted address arithmetic is illus-trated by a variation of the doubly-linked list, in which the
second and third fields of each record are replaced by a sin-gle field giving the exclusive or of their contents. If we
write xlist ff (i; i0; j; j0) when ff is represented by such an xorlinked list:

\Phi 

i0

ffi
ff1-i

\Phi 

ffi

ffi
ff2

\Phi 

ffi

j
ffn j0oe* * *k k k
\Delta  \Delta  \Delta 

\Delta  \Delta  \Delta 

we can define this predicate by

xlist ffl (i; i0; j; j0) def= emp ^ i = j ^ i0 = j0
xlist a\Delta ff (i; i0; k; k0) def=

9j: i 7! a; (j \Phi  i0) \Lambda  xlist ff (j; i; k; k0):

The basic properties are analogous to those for dlist [24].

Finally, we mention an idea of Richard Bornat's [1], thatinstead of denoting a sequence of data items, a list (or other

structure) should denote the sequence (or other collection)of addresses at which the data items are stored. In the case
of simply linked lists, we write listN oe (i; j) when there is

10

a list segment from i to j containing the sequence oe of ad-dresses:

ffi
oe1

?
-i

ffi

oe2

?

j

oen

?
* * \Delta  \Delta  \Delta  *

This view leads to the definition

listN ffl (i; j) def= emp ^ i = j
listN l\Delta oe (i; k) def= l = i ^ 9j: i + 1 7! j \Lambda  listN oe (j; k):

Notice that listN is extremely local: It holds for a heap con-taining only the second cell of each record in the list structure.

The reader may verify that the body of the list-reversingprogram preserves the invariant

9oe; ffi: (listN oe (i; nil) \Lambda  listN ffi (j; nil)) ^ oey0 = oey\Delta ffi;
where oe0 is the original sequence of addresses of the datafields of the list at

i. In fact, since it shows that these ad-dresses do not change, this invariant embodies a stronger

notion of "in-place algorithm" than that given before.

6. Trees and Dags

When we move from list to tree structures, the possiblepatterns of sharing within the structures become richer.
In this section, we will focus on a particular kind of ab-stract value called an "S-expression" in the LISP community. The set S-exps of these values is the least set such that

o/ 2 S-exps iff

o/ 2 Atoms
or o/ = (o/1 \Delta  o/2) where o/1; o/2 2 S-exps:

(Of course, this is just a particular, and very simple, initialalgebra. We could take carriers of any anarchic many-sorted
initial algebra to be our abstract data, but this would com-plicate our exposition while adding little of interest.)

For clarity, it is vital to maintain the distinction betweenabstract values and their representations. Thus, we will call
abstract values "S-expressions", while calling representa-tions without sharing "trees", and representations with sharing but no loops "dags" (for directed acyclic graphs).

We write tree o/ (i) (or dag o/ (i)) to indicate that i is theroot of a tree (or dag) representing the S-expression

o/ . Bothpredicates are defined by induction on the structure of

o/ :

tree a (i) iff emp ^ i = a
tree (o/1 \Delta  o/2) (i) iff

9i1; i2: i 7! i1; i2 \Lambda  tree o/1 (i1) \Lambda  tree o/2 (i2)

dag a (i) iff i = a
dag (o/1 \Delta  o/2) (i) iff

9i1; i2: i 7! i1; i2 \Lambda  (dag o/1 (i1) ^ dag o/2 (i2)):

Here, since emp is omitted from its definition, dag a (i) ispure, and therefore intuitionistic. By induction, it is easily
seen that dag o/ i is intuitionistic for all o/ . In fact, this isvital, since we want

dag o/1 (i1) ^ dag o/2 (i2) to hold for aheap that contains the (possibly overlapping) sub-dags, but

not to assert that the sub-dags are identical.To express simple algorithms for manipulating trees, we
must introduce recursive procedures. However, to avoidproblems of aliased variables and interfering procedures,
we will limit ourselves to first-order procedures withoutglobal variables (except, in the case of recursion, the name
of procedure being defined), and we will explicitly indi-cate which formal parameters are modified by the procedure
body. Thus a procedure definition will have the form

h(x1; \Delta  \Delta  \Delta  ; xm; y1; \Delta  \Delta  \Delta  ; yn) = c;
where y1; \Delta  \Delta  \Delta  ; yn are the free variables modified by c, and
x1; \Delta  \Delta  \Delta  ; xm are the other free variables of c, except h.We will not declare procedures at the beginning of

blocks, but will simply assume that a program is reasonedabout in the presence of procedure definitions. In this setting, an appropriate inference rule for partial correctness is

ffl Procedures

When h(x1; \Delta  \Delta  \Delta  ; xm; y1; \Delta  \Delta  \Delta  ; yn) = c,

fpg h(x1; \Delta  \Delta  \Delta  ; xm; y1; \Delta  \Delta  \Delta  ; yn) fqg.

..

fpg c fqg

fpg h(x1; \Delta  \Delta  \Delta  ; xm; y1; \Delta  \Delta  \Delta  ; yn) fqg:

In essence, to prove some specification of a call of a pro-cedure in which the actual parameters are the same as the
formal parameters in the procedure definition, one proves asimilar specification of the body of the definition under the
recursion hypothesis that recursive calls satisfy the specifi-cation being proved.

Of course, one must be able to deduce specificationsabout calls in which the actual parameters differ from the
formals. For this purpose, however, the structural inferencerule for substitution suffices, if one takes the variables modified by h(x1; \Delta  \Delta  \Delta  ; xm; y1; \Delta  \Delta  \Delta  ; yn) to be y1; \Delta  \Delta  \Delta  ; yn. Note

11

that the restrictions on this rule prevent the creation ofaliased variables or expressions.

For example, we would expect a tree-copying procedure
copytree(i; j) =

if isatom(i) then j := i else

newvar i1; i2; j1; j2 in

(i1 := [i] ; i2 := [i + 1] ;
copytree(i1; j1) ; copytree(i2; j2) ;

j := cons(j1; j2))

to satisfy

ftree o/ (i)g copytree(i; j) ftree o/ (i) \Lambda  tree o/ (j)g: (5)
The following is a proof outline of a similar specification ofthe procedure body:

ftree o/(i)g
if isatom(i) then

fisatom(o/ ) ^ emp ^ i = o/ g
fisatom(o/ ) ^ ((emp ^ i = o/ ) \Lambda  (emp ^ i = o/ ))g
j := i
fisatom(o/ ) ^ ((emp ^ i = o/ ) \Lambda  (emp ^ j = o/ ))g
else

f9o/1; o/2: o/ = (o/1 \Delta  o/2) ^ tree (o/1 \Delta  o/2)(i)g
newvar i1; i2; j1; j2 in

(i1 := [i] ; i2 := [i + 1] ;
f9o/1; o/2: o/ = (o/1 \Delta  o/2) ^ (i 7! i1; i2 \Lambda 

tree o/1 (i1) \Lambda  tree o/2 (i2))g
copytree(i1; j1) ;
f9o/1; o/2: o/ = (o/1 \Delta  o/2) ^ (i 7! i1; i2 \Lambda 

tree o/1 (i1) \Lambda  tree o/2 (i2) \Lambda  tree o/1 (j1))g
copytree(i2; j2) ;
f9o/1; o/2: o/ = (o/1 \Delta  o/2) ^

(i 7! i1; i2 \Lambda  tree o/1 (i1) \Lambda  tree o/2 (i2) \Lambda 
tree o/1 (j1) \Lambda  tree o/2 (j2))g
j := cons(j1; j2)
f9o/1; o/2: o/ = (o/1 \Delta  o/2) ^

(i 7! i1; i2 \Lambda  tree o/1 (i1) \Lambda  tree o/2 (i2) \Lambda 
j 7! j1; j2 \Lambda  tree o/1 (j1) \Lambda  tree o/2 (j2))g
f9o/1; o/2: o/ = (o/1 \Delta  o/2) ^

(tree (o/1\Delta o/2) (i) \Lambda  tree (o/1\Delta o/2) (j))g)
ftree o/(i) \Lambda  tree o/ (j)g:

To obtain the specifications of the recursive calls in thisproof outline from the recursion hypothesis (5), one must
use the frame rule to move from the footprint of the recur-sive call to the larger footprint of the procedure body. In
more detail, the specification of the first recursive call in theoutline can be obtained by the inferences

ftree o/(i)g copytree(i; j) ftree o/ (i) \Lambda  tree o/ (j)g
ftree o/1(i1)g copytree(i1; j1) ftree o/1(i1) \Lambda  tree o/1(j1)g

f(o/ = (o/1 \Delta  o/2) ^ i 7! i1; i2) \Lambda 

tree o/1 (i1) \Lambda  tree o/2 (i2)g
copytree(i1; j1) ;
f(o/ = (o/1 \Delta  o/2) ^ i 7! i1; i2) \Lambda 

tree o/1 (i1) \Lambda  tree o/2 (i2) \Lambda  tree o/1 (j1)g
fo/ = (o/1 \Delta  o/2) ^ (i 7! i1; i2 \Lambda 

tree o/1 (i1) \Lambda  tree o/2 (i2))g
copytree(i1; j1) ;
fo/ = (o/1 \Delta  o/2) ^ (i 7! i1; i2 \Lambda 

tree o/1 (i1) \Lambda  tree o/2 (i2) \Lambda  tree o/1 (j1))g

f9o/1; o/2: o/ = (o/1 \Delta  o/2) ^ (i 7! i1; i2 \Lambda 

tree o/1 (i1) \Lambda  tree o/2 (i2))g
copytree(i1; j1) ;
f9o/1; o/2: o/ = (o/1 \Delta  o/2) ^ (i 7! i1; i2 \Lambda 

tree o/1 (i1) \Lambda  tree o/2 (i2) \Lambda  tree o/1 (j1))g;

using the substitution rule, the frame rule, the rule of con-sequence (with the axiom that

(p ^ q) \Lambda  r , p ^ (q \Lambda  r)when
p is pure), and auxiliary variable elimination.
What we have proved about copytree, however, is not asgeneral as possible. In fact, this procedure is insensitive to

sharing in its input, so that it also satisfies

fdag o/(i)g copytree(i; j) fdag o/ (i) \Lambda  tree o/ (j)g: (6)
But if we try to prove this specification by mimicking theprevious proof, we encounter a problem: For (say) the first
recursive call, at the point where we used the frame rule, wemust infer

f(\Delta  \Delta  \Delta ) \Lambda  (dag o/1(i1) ^ dag o/2(i2))g
copytree(i1; j1)
f(\Delta  \Delta  \Delta ) \Lambda  (dag o/1(i1) ^ dag o/2(i2)) \Lambda  tree o/1(j1)g:

Now, however, the presence of ^ instead of \Lambda  prevents usfrom using the frame rule -- and for good reason: The recursion hypothesis (6) is not strong enough to imply thisspecification of the recursive call, since it does not imply
that copytree(i1; j1) leaves unchanged the portion of the

12

heap shared by the dags representing o/1 and o/2. For ex-ample, suppose

o/1 = ((3 \Delta  4) \Delta  (5 \Delta  6)) o/2 = (5 \Delta  6):
Then (6) would permit copytree(i1; j1) to change the statefrom

ffi
ffi-i1

6
5\Phi \Phi *
i2

4
3*

j
into

ffi
ffi-i1

4
3\Phi \Phi *
i2

6
53

s

ffi

ffi-j1

6
5

4
3*

j
where dag o/2 (i2) is false.

One way of surmounting this problem is to extend asser-tions to contain

assertion variables, and to extend the sub-stitution rule so that the

vi's include assertion variables aswell as ordinary variables, with assertions being substituted

for these assertion variables.

Then we can use an assertion variable p to specify thatevery property of the heap that is active before executing

copytree(i; j) remains true after execution:

fp ^ dag o/ (i)g copytree(i; j) fp \Lambda  tree o/ (j)g: (7)
We leave the proof outline to the reader, but show the infer-ence of the specification of the first recursive call from the
recursion hypothesis, using the substitution rule and auxil-iary variable elimination:

fp ^ dag o/ (i)g copytree(i; j) fp \Lambda  tree o/ (j)g
f(o/ = (o/1 \Delta  o/2) ^ p ^ dag o/2 (i2)) ^ dag o/1 (i1)g
copytree(i1; j1) ;
f(o/ = (o/1 \Delta  o/2) ^ p ^ dag o/2 (i2)) \Lambda  tree o/1 (j1)g

f9o/1; o/2: (o/ = (o/1 \Delta  o/2) ^ p ^ dag o/2 (i2)) ^ dag o/1 (i1)g
copytree(i1; j1) ;
f9o/1; o/2: (o/ = (o/1 \Delta  o/2) ^ p ^ dag o/2 (i2)) \Lambda  tree o/1 (j1)g:

7. Arrays and Iterated Separating Conjunction

It is straightforward to extend our programming lan-guage to include heap-allocated one-dimensional arrays, by

introducing an allocation command where the number ofconsecutive heap cells to be allocated is specified by an
operand. It is simplest to leave the initial values of thesecells indeterminate. We will use the syntax

hcommi ::= \Delta  \Delta  \Delta  j hvari := allocate hexpi
with the operational semantics

hv := allocate e; (s; h)i ; ([ s j v: ` ]; h \Delta  h0)

where h ? h0 and dom h0 = f i j ` ^ i ! ` + [[e]]exps g:

To describe such arrays, it is helpful to extend the con-cept of separating conjunction to a construct that iterates

over a finite consecutive set of integers. We use the syntax

hasserti ::= \Delta  \Delta  \Delta  j

Jhexpi

hvari=hexpi hasserti

with the meaning

[[

Je0

v=e p]]asrt(s; h) =

let m = [[e]]exps; n = [[e0]]exps;

I = f i j m ^ i ^ n g in
9H 2 I ! Heaps:

8i; j 2 I: i 6= j implies Hi ? Hj
and h =

S

f Hi j i 2 I g

and 8i 2 I: [[p]]asrt([ s j v: i ]; Hi):
The following axiom schemata are useful:

m ? n ) (

Jn

i=m p(i) , emp)

m = n ) (

Jn

i=m p(i) , p(m))

k ^ m ^ n + 1 )

(

Jn

i=k p(i) , (

Jm\Gamma 1

i=k p(i) \Lambda 

Jn

i=m p(i)))

Jn

i=m p(i) ,

Jn\Gamma k

i=m\Gamma k p(i + k)

m ^ n )

((

Jn

i=m p(i)) \Lambda  q ,

Jn

i=m(p(i) \Lambda  q))

when q is pure

m ^ j ^ n ) ((

Jn

i=m p(i)) ) (p(j) \Lambda  true)):

A simple example is the use of an array as a cyclic buffer.We assume that an

n-element array has been allocated ataddress
l, e.g., by l := allocate n, and we use the variables

m number of active elements

i address of first active element
j address of first inactive element:

13

Then when the buffer contains a sequence ff, it should sat-isfy the assertion

0 ^ m ^ n ^ l ^ i ! l + n ^ l ^ j ! l + n ^
j = i \Phi  m ^ m = #ff ^
((

Jm\Gamma 1

k=0 i \Phi  k 7! ffk+1) \Lambda  (

Jn\Gamma m\Gamma 1

k=0 j \Phi  k 7! \Gamma ));

where x \Phi  y = x + y modulo n, and l ^ x \Phi  y ! l + n.Somewhat surprisingly, iterated separating conjunction

is useful for making assertions about structures that do notinvolve arrays. For example, the connection between our
list and Bornat's listN is given by

list ff (i; j) ,

9oe: #oe = #ff ^ (listN oe (i; j) \Lambda 

J#ff

k=1 oek 7! ffk):

A more elaborate example is provided by a program thataccepts a list

i representing a sequence ff of integers, andproduces a list

j of lists representing all (not necessarily con-tiguous) subsequences of

ff. This program is a variant of avenerable LISP example that has often been used to illustrate the storage economy that can be obtained in LISP bya straightforward use of sharing. Our present interest is in
using iterated separating conjunction to specify the sharingpattern in sufficient detail to show the amount of storage
that is used.First, given a sequence

oe of sequences, we define extaoeto be the sequence of sequences obtained by prefixing the

integer a to each sequence in oe:

#extaoe def= #oe
(extaoe)i def= a\Delta oei:
Then we define ss(ff; oe) to assert that oe is a sequence of thesubsequences of

ff (in the particular order that is producedby an iterative program):

ss(ffl; oe) def= oe = [ffl]
ss(a\Delta ff; oe) def= 9oe0: ss(ff; oe0) ^ oe = (extaoe0)y\Delta oe0:
(To obtain the different order produced by a simple recur-sive program, we would remove the reflection (

y) operatorhere and later.) Next, we define
Q(oe; fi) to assert that fi isa sequence of lists whose components represent the components of oe:

Q(oe; fi) def= #fi = #oe ^ 8#fii=1(list oei (fii; nil) \Lambda  true):

At this stage, we can specify the abstract behavior of thesubsequence program

subseq by

flist ff (i; nil)g
subseq
f9oe; fi: ss(ffy; oe) ^ (list fi (j; nil) \Lambda  (Q(oe; fi)))g:

To capture the sharing structure, however, we use iteratedseparating conjunction to define

R(fi) to assert that the lastelement of
fi is the empty list, while every previous elementconsists of a single integer cons'ed onto some later element

of fi:

R(fi) def= (fi#fi = nil ^ emp) \Lambda 

J#fi\Gamma 1

i=1 (9a; k: i ! k ^ #fi ^ fii 7! a; fik):

Here the heap described by each component of the iterationcontains a single cons-pair, so that the heap described by

R(fi) contains #fi \Gamma  1 cons-pairs. Finally, the full specifi-cation of

c is

flist ff (i; nil)g
subseq
f9oe; fi: ss(ffy; oe) ^ (list fi (j; nil) \Lambda  (Q(oe; fi) ^ R(fi)))g:

Here the heap described by list fi (j; nil) contains #fi cons-pairs, so that the entire heap described by the postcondition
contains (2 \Theta  #fi) \Gamma  1 = (2 \Theta  2#ff) \Gamma  1 cons-pairs.

8. Proving the Schorr-Waite Algorithm

The most ambitious application of separation logic hasbeen Yang's proof of the Schorr-Waite algorithm for marking structures that contain sharing and cycles [33, 34]. Thisproof uses the older form of classical separation logic [19]
in which address arithmetic is forbidden and the heap mapsaddresses into multifield records -- each containing, in this
case, two address fields and two boolean fields.

Several significant features of the proof are evident fromits main invariant:

noDanglingR ^ noDangling(t) ^ noDangling(p) ^
(listMarkedNodesR(stack; p) \Lambda 

(restoredListR(stack; t) \Gamma \Lambda  spansR(STree; root))) ^
(markedR \Lambda  (unmarkedR ^ (8x: allocated(x) )

(reach(t; x) . reachRightChildInList(stack; x))))):

The heap described by this invariant is the footprint of theentire algorithm, which is exactly the structure that is reachable from the address root. Since addresses now refer to en-tire records, it is easy to assert that the record at

x has beenallocated:

allocated(x) def= x ,! \Gamma ; \Gamma ; \Gamma ; \Gamma ;
that all active records are marked:

markedR def= 8x: allocated(x) ) x ,! \Gamma ; \Gamma ; \Gamma ; true;

14

that x is not a dangling address:

noDangling(x) def= (x = nil) . allocated(x);
or that no active record contains a dangling address:

noDanglingR def= 8x; l; r: (x ,! l; r; \Gamma ; \Gamma ) )

noDangling(l) ^ noDangling(r):

(Yang uses the helpful convention that the predicates withnames ending in "

R" are those that are not intuitionistic.)In the second line of the invariant, the subassertion

listMarkedNodesR(stack; p) holds for a part of the heapcalled the spine, which is a linked list of records from
root to the current address t in which the links havebeen reversed; the variable

stack is a sequence of four-tuples determining the contents of the spine. In the next

line, restoredListR(stack; t) describes what the contentsof the spine should be after the links have been restored,
and spansR(STree; root) asserts that the abstract structure
STree is a spanning tree of the heap.The assertion

spansR(STree; root) also appears in theprecondition of the algorithm. Thus, the second and third

lines of the invariant use separating implication elegantly toassert that, if the spine is correctly restored, then the heap
will have the same spanning tree as it had initially. (In fact,the proof goes through if

spansR(STree; root) is any predi-cate that is independent of the boolean fields in the records;

spanning trees are used only because they are enough to de-termined the heap, except for the boolean fields.) To the
author's knowledge, this part of the invariant is the onlyconceptual use of separating implication in a real proof (as
opposed to its formal use in expressing weakest precondi-tions).

In the rest of the invariant, the heap is partitioned intomarked and unmarked records, and it is asserted that every active unmarked record can be reached from the vari-able

t or from certain fields in the spine. However, sincethis assertion lies within the right operand of the separating conjunction that separates marked and unmarked notes,the paths by which the unmarked records are reached must
consist of unmarked records. Anyone (such as the author[27, Section 5.1]) who has tried to verify this kind of graph
traversal, even informally, will appreciate the extraordinarysuccinctness of the last two lines of Yang's invariant.

9. Computability and Complexity Results

The existence of weakest preconditions for each of ournew commands assures us that a central property of Hoare
logic is preserved by our extension: that a program spec-ification annotated with loop invariants and recursion hypotheses (and, for total correctness, variants) can be reduced

to a collection of assertions, called verification conditions,whose validity will insure the original specification. Thus
the central question for computability and complexity is todecide the validity of assertions in separation logic.

Yang [8, 34] has examined the decidability of classicalseparation logic without arithmetic (where expressions are
variables, values are addresses or nil, and the heap maps ad-dresses into two-field records). He showed that, even when
the characteristic operations of separation logic (emp, 7!,

\Lambda  , and \Gamma \Lambda , but not ,!) are prohibited, deciding the validityof an assertion is not recursively enumerable. (As a consequence, we cannot hope to find an axiomatic descriptionof

7!.) On the other hand, Yang and Calcagno showed thatif the characteristic operations are permitted but quantifiers

are prohibited, then the validity of assertions is algorithmi-cally decidable.

For the latter case, Calcagno and Yang [8] have investi-gated complexity. Specifically, for the languages tabulated
below they considered
MC The model checking problem: Does [[p]]asrtsh hold fora specified state

(s; h)?

VAL The validity problem: Does [[p]]asrtsh hold for allstates

(s; h)?

In each case, they determined that the problem was com-plete for the indicated complexity class:

Language MC VAL
L P ::= E 7! E; E j : E ,! \Gamma ; \Gamma  P coNP

j E = E j E 6= E j false
j P ^ P j P . P j emp

L\Lambda  P ::= L j P \Lambda  P NP \Pi P2
L: \Lambda  P ::= L j : P j P \Lambda  P PSPACE
L\Gamma \Lambda  P ::= L j P \Gamma \Lambda  P PSPACE
L: \Lambda \Gamma \Lambda  P ::= L j : P j P \Lambda  P j P \Gamma \Lambda  P PSPACE

10. Garbage Collection

Since our logic permits programs to use unrestricted ad-dress arithmetic, there is little hope of constructing any
general-purpose garbage collector. On the other hand, thesituation for the older logic, in which addresses are disjoint
from integers, is more hopeful. However, it is clear that thislogic permits one to make assertions, such as "The heap
contains two elements" that might be falsified by the exe-cution of a garbage collector, even though, in any realistic
sense, such an execution is unobservable.

The present author [28] has given the following example

15

(shown here as a proof outline):

ftrueg
x := cons(3; 4) ;
fx ,! 3; 4g
f9x: x ,! 3; 4g
x := nil
f9x: x ,! 3; 4g;

where the final assertion describes a disconnected piece ofstructure, and would be falsified if the disconnected piece
were reclaimed by a garbage collector. In this case, the as-sertions are all intuitionistic, and indeed it is hard to concoct a reasonable program logic that would prohibit such aderivation.

Calcagno, O'Hearn, and Bornat [7, 6, 5, 4] have exploredways of avoiding this problem by defining the existential
quantifier in a nonstandard way, and have defined a richvariety of logics that are insensitive to garbage collection.
Unfortunately, there is no brief way of relating these logicsto those discussed in this paper.

11. Future Directions
11.1. New Forms of Inference

In Yang's proof of the Schorr-Waite algorithm, there arethirteen assertions that have been semantically validated but
do not seem to be consequences of known inference rules,and are far too specific to be considered axioms. This surprising state of affairs is likely a consequence of the novelcharacter of the proof itself -- especially the quantification
over all allocated addresses, and the crucial use of sepa-rating implication -- as well as the fact that the algorithm
deals with sharing in a more fundamental way than othersthat have been studied with separation logic.

The generalization of such assertions may be a fertilesource of new inference rules. For example, suppose

^p isintuitionistic, and let
p be 8x: allocated(x) ) ^p. Then

emp ) p
(9x: (x 7! \Gamma ; \Gamma ; \Gamma ; \Gamma ) ^ ^p) ) p

(p \Lambda  p) ) p:

For a more elaborate example, suppose we say that twoassertions are

immiscible if they cannot both hold for over-lapping heaps. More precisely,

p and q are immiscible iff,for all stores
s and heaps h and h0 such that h [ h0 is afunction, if
[[p]]asrtsh and [[q]]asrtsh0 then h ? h0.On the one hand, it is easy to find immiscible assertions:

If ^p and ^q are intuitionistic and ^p ^ ^q ) false is valid, then

8x: allocated(x) ) ^p and 8x: allocated(x) ) ^q

are immiscible. Moreover, if p0 and q0 are immiscible and
p ) p0 and q ) q0 are valid, then p and q are immiscible.On the other hand, if

p and q are immiscible, then

(p \Lambda  true) ^ (q \Lambda  r) ) q \Lambda  ((p \Lambda  true) ^ r)
is valid.Both of these examples are sound, and useful for at least

one proof of a specification of a real program. But theymay well be special cases of more generally useful rules or
other inference mechanisms. O'Hearn suspects that thereare good prospects to find a useful proof theory for separation logic using "labeled deduction" [13, 14].It should also be noted that Yang's proof depends critically on the fact that the Schorr-Waite algorithm performsan in-place computation. New problems may arise in trying
to prove, say, a program that copies a reachable structurewhile preserving its sharing patterns -- somehow, one must
express the isomorphism between the structure and its copy.Beyond these particulars, in its present state separation
logic is not only theoretically incomplete, but pragmaticallyincomplete: As it is applied in new ways, there will be a
need for new kinds of inference.
11.2. Taming Address Arithmetic

When we first realized that separation logic could begeneralized and simplified by permitting address arithmetic,
it seemed an unalloyed benefit. But there are problems. Forexample, consider the definition of

dag given in Section 6:The assertion
dag ((1 \Delta  2) \Delta  (2 \Delta  3)) (i) holds in states wheretwo distinct records overlap, e.g.

ffi
ffi-i

3
2
1
-

-

Similarly, iff one were to try to recast the Schorr-Waiteproof in the logic with address arithmetic, it would be difficult (but necessary) to assert that distinct records do notoverlap, and that all address values in the program denote
the first fields of records.These problems, of what might be called

skewed sharing,become even more difficult when there are several types of

records, with differing lengths and field types.A possible solution may be to augment states with a mapping from the domain of the heap into "attributes" that couldbe set by the program, described by assertions, and perhaps
tested to determine whether to abort. These attributes wouldbe similar to auxiliary variables (in the sense of Owicki and
Gries [25]), in that their values could not influence the val-ues of nonauxiliary variables or heap cells, nor the flow of
control.

16

To redefine dag to avoid skewed sharing, one could, ateach allocation

x := cons(e1; e2) that creates a record in adag, give
x (but not x + 1) the attribute dag-record. Thenthe definition of a non-atomic dag would be changed to

dag (o/1 \Delta  o/2) (i) iff is-dag-record(i) ^

9i1; i2: i 7! i1; i2 \Lambda  (dag o/1 (i1) ^ dag o/2 (i2)):

In a version of the Schorr-Waite proof using addressarithmetic, one might give the attribute record to the address of the beginning of each record. Then one would add

8x: is-record(x) ) 9l; r; c; m: x ,! l; r; c; m

^ (is-record(l) . l = nil) ^ (is-record(r) . r = nil)
^ is-boolean(c) ^ is-boolean(m)

to the invariant, and use the assertion is-record(x) in placeof

allocated(x).

There is a distinct flavor of types in this use of attributes:The attributes record information, for purposes of proof,

that in a typed programming language would be checkedby the compiler and discarded before runtime. An alternative, of course, would be to move to a typed programminglanguage, but our goal is to keep the programming language
low-level and, for simplicity and flexibility, to use the proofsystem to replace types rather than supplement them.

11.3. Concurrency

In the 1970's, Hoare and Brinch Hansen argued persua-sively that, to prevent data races where two processes attempt to access the same storage without synchronization,concurrent programs should be syntactically restricted to
limit process interference to explicitly indicated critical re-gions. In the presence of shared mutable structures, however, processes can interfere in ways too subtle to be de-tected syntactically.

On the other hand, when one turns to program verifica-tion, it is clear that separation logic can specify the absence
of interference. In the simplest situation, the concurrent ex-ecution

c1 k c2 of two processes that do not interfere withone another is described by the inference rule

fp1g c1 fq1g fp2g c2 fq2g

fp1 \Lambda  p2g c1 k c2 fq1 \Lambda  q2g;

where no variable free in p1 or q1 is modified by c2, or vice-versa.

Going beyond this trivial case, O'Hearn [22, 21] has ex-tended the Hoare-like logic for concurrency devised by Owicki and Gries [25] (which is in turn an extension of workby Hoare [18]), to treat critical regions in the framework of
separation logic.

The basic idea is that, just as program variables are syn-tactically partitioned into groups owned by different processes and resources, so the heap should be similarly par-titioned by separating conjunctions in the proof of the program. The most interesting aspect is that the partition ofthe heap can be changed by executing critical regions associated with resources, so that ownership of a particularaddress can move from a process to a resource and from
there to another process.

Thus, for example, one process may allocate addressesand place them in a buffer, while another process removes

the addresses from the buffer and deallocates them. Simi-larly, in a concurrent version of

quicksort, an array segmentmight be divided between two concurrent recursive calls,

and reunited afterwards.

Unfortunately, at this writing there is no proof thatO'Hearn's inference rules are sound. The difficulty is that,

in O'Hearn's words, "Ownership is in the eye of the as-serter", i.e., the changing partitions of the heap are not determined by the program itself, but only by the assertions inits proof.

In concurrent programming, it is common to permit sev-eral processes to read the same variable, as long as no process can modify its value simultaneously. It would be natu-ral and useful to extend this notion of passivity to heap cells,
so that the specification of a process might indicate that aportion of the heap is evaluated but never mutated, allocated
or deallocated by the process. (This capability would alsoprovide an alternative way to specify the action of

copytreeon dags discussed in Section 6.) Semantically, this would

likely require activity bits to take on a third, intermediatevalue of "read-only".

Concurrency often changes the focus from terminatingprograms to programs that usefully run on forever -- for
which Hoare logic is of limited usefulness. For such pro-grams, it might be helpful to extend temporal logic with
separating operators.

11.4. Storage Allocation

Since separation logic describes a programming lan-guage with explicit allocation and deallocation of storage,
without any behind-the-scenes garbage collector, it shouldbe suitable for reasoning about allocation methods themselves.

A challenging example is the use of regions, in the senseof Tofte et al [31]. To provided an explicitly programmable

region facility, one must program an allocator and dealloca-tor for regions of storage, each of which is equipped with its
own allocator and deallocator. Then one must prove that theprogram using these routines never deallocates a region unless it is safe to deallocate all storage that has been allocatedfrom that region.

17

Although separation logic is incompatible with general-purpose garbage collection (at least when unrestricted address arithmetic is permitted), it should be possible to con-struct and verify a garbage collector for a specific program.
In this situation, one should be able to be far more aggres-sive than would be possible for a general-purpose garbage
collector, both in recovering storage, and in minimizing theextra data needed to guide the traversal of active structure.
For example, for the representation described by dag in Sec-tion 6, it would be permissible for the collector to increase
the amount of sharing.

The key here is that a correct garbage collector need onlymaintain the assertion that must hold at the point where the

collector is called, while preserving the value of certain in-put variables that determine the abstract values being computed. (In addition, for total correctness, the collector mustnot increase the value of the loop variants that insure termination.) For instance, in the list-reversal example in Section5, a collector called at the end of the while body would be
required to maintain the invariant

9ff; fi: (list ff (i; nil) \Lambda  list fi (j; nil)) ^ ffy0 = ffy\Delta fi;
while preserving the abstract input ff0, and not increasingthe variant, which is the length of

ff.
It is interesting to note that the garbage collector is re-quired to respect the partial equivalence relation determined

by the invariant, in which two states are equivalent whenthey both satisfy the invariant and specify the same values
of the input variables. Considering the use of partial equiv-alence relations to give semantics to types, this reinforces
the view that types and assertions are semantically similar.

11.5. The Relationship to Type Systems

Although types and assertions may be semantically sim-ilar, the actual development of type systems for programming languages has been quite separate from the develop-ment of approaches to specification such as Hoare logic, refinement calculi, or model checking. In particular, the ideathat states have types, and that executing a command may
change the type of a state, has only taken hold recently, inthe study of type systems for low-level languages, such as
the alias types devised by Walker and Morrisett [32].

Separation logic is closely related to such type systems.Indeed, their commonality can be captured roughly by a

simple type system:

hvalue typei ::= integer j haddress variablei
hstore typei ::=

hvariablei: hvalue typei; : : : ; hvariablei: hvalue typei

hheap typei ::= emp

j haddress variablei 7!hvalue typei; : : : ; hvalue typei
j hheap typei \Lambda  hheap typei

hstate typei ::= hstore typei ; hheap typei
It is straightforward to translate state types in this systeminto assertions of classical separation logic (in the formulation without address arithmetic) or into alias types.It may well be possible to devise a richer type system that
accommodates a limited degree of address arithmetic, per-mitting, for example, addresses that point into the interior
of records, or relative addresses.Basically, however, the real question is whether the dividing line between types and assertions can be erased.
11.6. Embedded Code Pointers

Even as a low-level language, the simple imperative lan-guage axiomatized by Hoare is deficient in making no provision for the occurrence in data structures of addresses thatrefer to machine code. Such code pointers appear in the
compiled translation of programs in higher-order languagessuch as Scheme or SML, or object-oriented languages such
as Java or C]. Moreover, they also appear in low-levelprograms that use the techniques of higher-order or objectoriented programming.Yet they are difficult to describe in the first-order world
of Hoare logic; to deal with embedded code pointers, wemust free separation logic from both Hoare logic and the
simple imperative language. Evidence of where to gocomes from the recent success of type theorists in extending types to machine language (in particular to the outputof compilers) [20, 32]. They have found that a higher-order
functional language (with side-effects) comes to resemble alow-order machine language when two restrictions are imposed:

ffl Continuation-passing style (CPS) is used, so that func-tions receive their return addresses in the same way as

they receive other parameters.
ffl Free variables are prohibited in expressions that denotefunctions, so that functions can be represented by code

pointers, rather than by closures that pair code pointerswith data.

In fact, this restricted language is formally similar to an im-perative language in which programs are "flat", i.e., control
paths never come together except by jumping to a commonlabel.

This raises the question of how to marry separation logicwith CPS (with or without the prohibition of free variables
in function expressions, which appears to be irrelevant froma logical point of view).

18

A simple example of CPS is provided by the function
append(x; y; r), which appends the list x to the list y (with-out mutation, so that the input lists are unchanged), and

passes the resulting list on to the continuation r:

letrec append(x; y; r) = if x = nil then r(y) else

let a = [x]; b = [x + 1];

k(z) = let w = cons(a; z) in r(w)
in append(b; y; k):

We believe that the key to specifying such a CPS pro-gram is to introduce a

reflection operator # that allows CPSterms to occur within assertions (in a manner reminiscent

of dynamic logic [15]). Specifically, if t is a CPS term then
# t is an assertion that holds for a state if t never abortswhen executed in that state -- in this situation we say that

it is safe to execute t in that state.Suppose

c is a command satisfying the Hoare specifica-tion
fpg c fqg, and t is a CPS term such that executing t hasthe same effect as executing

c and then calling the continu-ation
r(x1; :::xn). Then we can specify t by the assertion

(p \Lambda  (8x1; : : : ; xm: q \Gamma \Lambda  # r(x1; : : : ; xn))) ) # t
(where x1; : : : ; xm is a subset of the variables x1; : : : ; xn).If we ignore the difference between the separating operators

and ordinary conjunction and implication, this asserts that itis safe to execute

t in any state satisfying p and mapping rinto a procedure such that it is safe to execute

r(x1; :::; xn)in a state satisfying
q. Taking into account the separativenature of
\Lambda  and \Gamma \Lambda , it asserts that it is safe to execute t inany state where part of the heap satisfies

p, and r is mappedinto a procedure such that it is safe to execute

r(x1; :::; xn)when the part of the heap that satisfied
p is replaced by a partthat satisfies
q. Finally, the universal quantifier indicatesthe variables whose value may be changed by

t before itexecutes
r(x1; :::; xn).For example, the CPS form of

append can be specifiedby

((list ff (x; nil) \Lambda  list fi (y; nil))

\Lambda  (8z: (list ff (x; nil) \Lambda  list ff (z; y) \Lambda  list fi (y; nil))

\Gamma \Lambda  # r(z)))
) # append(x; y; r):

It can be discouraging to go back and read the "futuredirections" sections of old papers, and to realize how few of
the future directions have been successfully pursued. It willbe fortunate if half of the ideas and suggestions in this section bear fruit. In the meantime, however, the field is young,the game's afoot, and the possibilities are tantalizing.

Acknowledgements

For encouragement and numerous suggestions, I am in-debted to many researchers in separation logic, as well as
the students in courses I have taught on the subject. I'm par-ticularly grateful to Peter O'Hearn for many helpful comments after reading a preliminary draft of this paper.

However, most of the opinions herein, and all of the er-rors, are my own.

References

[1] R. Bornat. Explicit description in BI pointer logic. Unpublished, June 2001.[2] R. M. Burstall. Some techniques for proving correctness

of programs which alter data structures. In B. Meltzer and
D. Michie, editors, Machine Intelligence 7, pages 23-50.
Edinburgh University Press, Edinburgh, Scotland, 1972.[3] L. Caires and L. Monteiro. Verifiable and executable specifications of concurrent objects in Lss. In C. Hankin, editor,
Programming Languages and Systems -- ESOP '98, volume
1381 of Lecture Notes in Computer Science, pages 42-56,
Berlin, 1998. Springer-Verlag.[4] C. Calcagno. Program logics in the presence of garbage collection (abstract). In F. Henglein, J. Hughes, H. Makholm,
and H. Niss, editors, SPACE 2001: Informal Proceedings of
Workshop on Semantics, Program Analysis and Computing
Environments for Memory Management, page 11. IT University of Copenhagen, 2001.[5] C. Calcagno.

Semantic and Logical Properties of Stateful
Programmming. Ph. D. dissertation, Universit`a di Genova,
Genova, Italy, 2002.[6] C. Calcagno and P. W. O'Hearn. On garbage and program

logic. In Foundations of Software Science and Computation
Structures, volume 2030 of Lecture Notes in Computer Science, pages 137-151, Berlin, 2001. Springer-Verlag.[7] C. Calcagno, P. W. O'Hearn, and R. Bornat. Program logic

and equivalence in the presence of garbage collection. Submitted July 2001, 2001.[8] C. Calcagno, H. Yang, and P. W. O'Hearn. Computability

and complexity results for a spatial assertion language for
data structures. In R. Hariharan, M. Mukund, and V. Vinay,
editors, FST TCS 2001: Foundations of Software Technology
and Theoretical Computer Science, volume 2245 of Lecture
Notes in Computer Science, pages 108-119, Berlin, 2001.
Springer-Verlag.
[9] L. Cardelli, P. Gardner, and G. Ghelli. A spatial logic for

querying graphs. In M. Hennessy and P. Widmayer, editors,
Automata, Languages and Programming, Lecture Notes in
Computer Science, Berlin, 2002. Springer-Verlag.[10] L. Cardelli and G. Ghelli. A query language based on the

ambient logic. In D. Sands, editor, Programming Languages
and Systems -- ESOP 2001, volume 2028 of Lecture Notes
in Computer Science, pages 1-22, Berlin, 2001. SpringerVerlag.
[11] L. Cardelli and A. D. Gordon. Anytime, anywhere: Modal

logics for mobile ambients. In Conference Record of POPL

19

'00: The 27th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 365-377, New
York, 2000. ACM.[12] G. Conforti, G. Ghelli, A. Albano, D. Colazzo, P. Manghi,

and C. Sartiani. The query language TQL. In Proceedings of
the 5th International Workshop on the Web and Databases
(WebDB), Madison, Wisconsin, 2002.[13] D. Galmiche and D. M'ery. Proof-search and countermodel

generation in propositional BI logic. In N. Kobayashi and
B. C. Pierce, editors, Theoretical Aspects of Computer Software, volume 2215 of Lecture Notes in Computer Science,
pages 263-282, Berlin, 2001. Springer-Verlag.[14] D. Galmiche, D. M'ery, and D. J. Pym. Resource tableaux.

Submitted, 2002.[15] D. Harel, D. Kozen, and J. Tiuryn. Dynamic Logic. MIT
Press, Cambridge, Massachusetts, 2000.[16] C. A. R. Hoare. An axiomatic basis for computer programming. Communications of the ACM, 12(10):576-580 and
583, October 1969.[17] C. A. R. Hoare. Proof of a program: FIND. Communications

of the ACM, 14(1):39-45, January 1971.[18] C. A. R. Hoare. Towards a theory of parallel programming.
In C. A. R. Hoare and R. H. Perrott, editors, Operating Systems Techniques, volume 9 of A.P.I.C. Studies in Data Processing, pages 61-71, London, 1972. Academic Press.[19] S. Ishtiaq and P. W. O'Hearn. BI as an assertion language

for mutable data structures. In Conference Record of POPL
2001: The 28th ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages, pages 14-26, New
York, 2001. ACM.[20] G. Morrisett, K. Crary, N. Glew, and D. Walker. Stack-based

typed assembly language. In X. Leroy and A. Ohori, editors, Types in Compilation, volume 1473 of Lecture Notes
in Computer Science, pages 28-52, Berlin, 1998. SpringerVerlag.[21] P. W. O'Hearn. Notes on conditional critical regions in spatial pointer logic. Unpublished, August 31, 2001.[22] P. W. O'Hearn. Notes on separation logic for shared-variable
concurrency. Unpublished, January 3, 2002.[23] P. W. O'Hearn and D. J. Pym. The logic of bunched implications. Bulletin of Symbolic Logic, 5(2):215-244, June
1999.[24] P. W. O'Hearn, J. C. Reynolds, and H. Yang. Local reasoning about programs that alter data structures. In L. Fribourg, editor, Computer Science Logic, volume 2142 of Lecture Notes in Computer Science, pages 1-19, Berlin, 2001.
Springer-Verlag.[25] S. S. Owicki and D. Gries. Verifying properties of parallel

programs: An axiomatic approach. Communications of the
ACM, 19(5):279-285, May 1976.[26] D. J. Pym. The Semantics and Proof Theory of the Logic of

Bunched Implications. Applied Logic Series. Kluwer Academic Publishers, Boston, Massachusetts, 2002. (to appear).[27] J. C. Reynolds. The Craft of Programming. Prentice-Hall

International, London, 1981.[28] J. C. Reynolds. Intuitionistic reasoning about shared mutable data structure. In J. Davies, B. Roscoe, and J. Woodcock,
editors, Millennial Perspectives in Computer Science, pages
303-321, Houndsmill, Hampshire, 2000. Palgrave.

[29] J. C. Reynolds. Lectures on reasoning about shared mutable

data structure. IFIP Working Group 2.3 School/Seminar on
State-of-the-Art Program Design Using Logic, Tandil, Argentina, September 6-13, 2000.[30] J. C. Reynolds and P. W. O'Hearn. Reasoning about shared

mutable data structure (abstract of invited lecture). In
F. Henglein, J. Hughes, H. Makholm, and H. Niss, editors, SPACE 2001: Informal Proceedings of Workshop on
Semantics, Program Analysis and Computing Environments
for Memory Management, page 7. IT University of Copenhagen, 2001. The slides for this lecture are available at
ftp://ftp.cs.cmu.edu/user/jcr/spacetalk.ps.gz.[31] M. Tofte and J.-P. Talpin. Implementation of the typed callby-value *-calculus using a stack of regions. In Conference
Record of POPL '94: 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 188-
201, New York, 1994. ACM Press.[32] D. Walker and G. Morrisett. Alias types for recursive data

structures. In R. W. Harper, editor, Types in Compilation,
volume 2071 of Lecture Notes in Computer Science, pages
177-206, Berlin, 2001. Springer-Verlag.[33] H. Yang. An example of local reasoning in BI pointer logic:

The Schorr-Waite graph marking algorithm. In F. Henglein,
J. Hughes, H. Makholm, and H. Niss, editors, SPACE 2001:
Informal Proceedings of Workshop on Semantics, Program
Analysis and Computing Environments for Memory Management, pages 41-68. IT University of Copenhagen, 2001.[34] H. Yang. Local Reasoning for Stateful Programs. Ph. D.

dissertation, University of Illinois, Urbana-Champaign, Illinois, July 2001.[35] H. Yang and P. W. O'Hearn. A semantic basis for local reasoning. In M. Nielsen and U. Engberg, editors, Foundations
of Software Science and Computation Structures, volume
2303 of Lecture Notes in Computer Science, pages 402-416,
Berlin, 2002. Springer-Verlag.

20