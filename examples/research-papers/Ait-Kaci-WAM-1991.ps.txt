

Warren's Abstract Machine
A TUTORIAL RECONSTRUCTION

HASSAN A"IT-KACI

hak@cs.sfu.ca

Intelligent Software Group
http://www.isg.sfu.ca/~hak

School of Computing ScienceSimon Fraser University

Burnaby, British ColumbiaV5A 1S6, Canada

February 18, 1999
(REPRINTED FROM MIT PRESS VERSION)

Copyright cfl Hassan A"IT-KACI

cfl 1991, 1997, 1999 by Hassan A"it-Kaci
No part of this book may be reproduced in any form by any electronic or me-chanical means (including photocopying, recording, or information storage and
retrieval) without permission in writing from the author. All rights reserved.

WARREN'S ABSTRACT MACHINE

Because they have seen it grow from the start,
this modest work is dedicated to:

ELI `ES, JAYD, NASSIM, AND JULIETA

for much needed love
and trusting me, always

NANIE

for tranquil unconditional faith

and being there

FOR ^ET DES FLAMBERTINS

for peaceful mornings
and conniving whispers
giving me some answers

iv Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
Contents
1 Introduction 3

1.1 Existing literature . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 This tutorial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

1.2.1 Disclaimer and motivation . . . . . . . . . . . . . . . . . 5
1.2.2 Organization of presentation . . . . . . . . . . . . . . . . 6

2 Unification--Pure and Simple 9

2.1 Term representation . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.2 Compiling L0 queries . . . . . . . . . . . . . . . . . . . . . . . . 11
2.3 Compiling L0 programs . . . . . . . . . . . . . . . . . . . . . . . 13
2.4 Argument registers . . . . . . . . . . . . . . . . . . . . . . . . . 19

3 Flat Resolution 25

3.1 Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.2 Rules and queries . . . . . . . . . . . . . . . . . . . . . . . . . . 27

4 Prolog 33

4.1 Environment protection . . . . . . . . . . . . . . . . . . . . . . . 34
4.2 What's in a choice point . . . . . . . . . . . . . . . . . . . . . . 36

5 Optimizing the Design 45

5.1 Heap representation . . . . . . . . . . . . . . . . . . . . . . . . . 46
5.2 Constants, lists, and anonymous variables . . . . . . . . . . . . . 47

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press v

WARREN'S ABSTRACT MACHINE
5.3 A note on set instructions . . . . . . . . . . . . . . . . . . . . . 52
5.4 Register allocation . . . . . . . . . . . . . . . . . . . . . . . . . 54
5.5 Last call optimization . . . . . . . . . . . . . . . . . . . . . . . . 56
5.6 Chain rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
5.7 Environment trimming . . . . . . . . . . . . . . . . . . . . . . . 58
5.8 Stack variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

5.8.1 Variable binding and memory layout . . . . . . . . . . . . 62
5.8.2 Unsafe variables . . . . . . . . . . . . . . . . . . . . . . 64
5.8.3 Nested stack references . . . . . . . . . . . . . . . . . . . 67
5.9 Variable classification revisited . . . . . . . . . . . . . . . . . . . 69
5.10 Indexing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
5.11 Cut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83

6 Conclusion 89
A Prolog in a Nutshell 91
B The WAM at a glance 97

B.1 WAM instructions . . . . . . . . . . . . . . . . . . . . . . . . . . 97
B.2 WAM ancillary operations . . . . . . . . . . . . . . . . . . . . . 112
B.3 WAM memory layout and registers . . . . . . . . . . . . . . . . . 117

vi Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
List of Figures

2.1 Heap representation of p(Z; h(Z; W ); f (W )): . . . . . . . . . . . 11
2.2 M0 machine instructions for query terms . . . . . . . . . . . . . 14
2.3 Compiled code for L0 query ?-p(Z; h(Z; W ); f (W )): . . . . . . 14
2.4 Compiled code for L0 program p(f (X); h(Y; f (a)); Y ): . . . . . . 16
2.5 The deref operation . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.6 M0 machine instructions for programs . . . . . . . . . . . . . . . 18
2.7 The unify operation . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.8 M1 instructions for variable arguments . . . . . . . . . . . . . . 23
2.9 Argument registers for L1 query ?-p(Z; h(Z; W ); f (W )): . . . . 24
2.10 Argument registers for L1 fact p(f (X); h(Y; f (a)); Y ): . . . . . . 24

3.1 M2 machine code for rule p(X; Y ) :- q(X; Z); r(Z; Y ): . . . . . 31
4.1 M3 choice point instruction try me else . . . . . . . . . . . . 41
4.2 M3 choice point instruction retry me else . . . . . . . . . . 41
4.3 M3 choice point instruction trust me . . . . . . . . . . . . . . 42
4.4 M3 code for a multiple clause definition . . . . . . . . . . . . . . 43

5.1 Better heap representation for term p(Z; h(Z; W ); f (W )) . . . . . 46
5.2 Specialized instructions for constants . . . . . . . . . . . . . . . . 49
5.3 Specialized instructions for lists . . . . . . . . . . . . . . . . . . 50
5.4 Specialized code for query ?-p(Z; [Z; W ]; f (W )): . . . . . . . . 51
5.5 Specialized code for fact p(f (X); [Y; f (a)]; Y ): . . . . . . . . . . 51

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press vii

WARREN'S ABSTRACT MACHINE
5.6 Anonymous variable instructions . . . . . . . . . . . . . . . . . . 53
5.7 Instructions for fact p( ; g(X); f ( ; Y; )): . . . . . . . . . . . . . 53
5.8 Better register use for p(X; Y ) :- q(X; Z); r(Z; Y ): . . . . . . . 55
5.9 M2 code for p(X; Y ) :- q(X; Z); r(Z; Y ):, with LCO . . . . . . 57
5.10 Environment trimming code . . . . . . . . . . . . . . . . . . . . 61
5.11 Unsafe code for p(X) :- q(Y; X); r(Y; X): . . . . . . . . . . . . 64
5.12 Code for a :- b(X; X); c:, by our classification . . . . . . . . . . 72
5.13 Code for a :- b(X; X); c:, by Warren's classification . . . . . . . 72
5.14 Delayed trimming for a :- b(X; X); c: . . . . . . . . . . . . . . 73
5.15 Useless delayed trimming for a :- b(X; X); c: . . . . . . . . . . 74
5.16 Indexing code for subsequence S1 . . . . . . . . . . . . . . . . . 80
5.17 Indexing code for subsequence S4 . . . . . . . . . . . . . . . . . 81
5.18 Encoding of conc=3 . . . . . . . . . . . . . . . . . . . . . . . . . 82

viii Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
Foreword to the reprinted edition
This document is a reprinted edition of the book bearing the same title that waspublished by MIT Press, in 1991 under ISBN 0-262-51058-8 (paper) and ISBN 0-
262-01123-9 (cloth). The MIT Press edition is now out of print, and its copyrighthas been transferred to the author. This version is available for free to anyone who
wants to use it for non-commercial purposes, from the author's web site at:

http://www.isg.sfu.ca/~hak/documents/wam.html
If you retrieve it, please let me know who you are, and for what purpose youintend to use it.

Thank you very much.

H:A:-K:
Burnaby, BC, Canada

May 1997

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press ix

WARREN'S ABSTRACT MACHINE
x Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI
A TUTORIAL RECONSTRUCTION
Foreword
Prolog was conceived in the early 1970s by Alain Colmerauer and his colleaguesat the University of Marseille. It was the first practical embodiment of the concept of logic programming, due to Robert Kowalski. The key idea behind logicprogramming is that computation can be expressed as controlled deduction from
declarative statements. Although the field has developed considerably since thoseearly days, Prolog remains the most fundamental and widely used logic programming language.
The first implementation of Prolog was an interpreter written in Fortran by mem-bers of Colmerauer's group. Although in some ways quite crude, this implementation was a milestone in several ways: it established the viability of Prolog, ithelped to disseminate the language, and it laid the foundations of Prolog implementation technology. A later milestone was perhaps the DEC-10 Prolog systemdeveloped at the University of Edinburgh by myself and colleagues. This system
built on the Marseille implementation techniques by introducing the notion of
compiling Prolog into a low-level language (in this case DEC-10 machine code),as well as various important space-saving measures. I later refined and abstracted

the principles of the DEC-10 Prolog implementation into what is now known asthe WAM (Warren Abstract Machine).

The WAM is an abstract machine consisting of a memory architecture and in-struction set tailored to Prolog. It can be realised efficiently on a wide range of
hardware, and serves as a target for portable Prolog compilers. It has now become
accepted as a standard basis for implementing Prolog. This is personally gratify-ing, but somewhat embarrassing in that the WAM is perhaps too readily accepted

as the standard. Although the WAM is a distillation of a long line of experiencein Prolog implementation, it is by no means the only possible point to consider
in the design space. For example, whereas the WAM adopts "structure copying"to represent Prolog terms, the "structure sharing" representation used in the MarCopyright cfl Hassan A"IT-KACI Reprinted from MIT Press xi

WARREN'S ABSTRACT MACHINE
seille and DEC-10 implementations still has much to recommend it. Be that asit may, I believe the WAM is certainly a good starting point for studying Prolog
implementation technology.
Regrettably, until now, there has not been a good source for getting acquaintedwith the WAM. My original technical report is not easily accessible, and contains

only a "bare bones" definition of the abstract machine, written for an expert reader.
Other works have discussed the WAM from various points of view, but there hascontinued to be a lack of a good tutorial introduction.

It is therefore a great pleasure to see the emergence of this excellent tutorial byHassan A"it-Kaci. The book is a delight to read. It explains the WAM with great
clarity and elegance. I believe readers with an interest in computer science willfind this book provides a stimulating introduction to the fascinating subject of Prolog implementation. I am most grateful to Hassan for making my work accessibleto a wider audience.

David H. D. Warren

Bristol, UK
February 1991

xii Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
Acknowledgments
First and foremost, David H. D. Warren is the person to whom I must expressnot only my awe for having invented and described the WAM, but also my most
sincere gratitude for never minding my repeatedly pestering him with questionsabout minute details he had to dredge out from the deeps of his memory. I am all
the more indebted to him as I know how busy he never ceases being, designingbetter and faster architectures, way ahead of most of us, no longer worrying about
this prehistoric part of his research life. In addition, I am particularly flattered that
he spontaneously cared to judge this humble opus to be a contribution worth beingpart of the prestigious MIT Press Logic Programming Series. Finally, let him be

again thanked for granting me the honor of introducing it with a foreword.
To be honest, it is rather ironical that I, slow-witted as I notoriously am, be creditedwith explaining the WAM to the world at large. In truth, my own deciphering of

the WAM's intricacies has been a painful and lengthy process. As a matter offact, I owe my basic understanding of it to two dear friends, specifically. Thus,
I would like to thank Roger Nasr for introducing the WAM to me and ManuelHermenegildo for patiently explaining it to me a hundred times over. They deserve
most of the credit bestowed on me as this monograph's author, having given methe foundations upon which I structured my presentation.

This tutorial was, in an earlier version, a technical report of the Digital Equip-ment Corporation's Paris Research Laboratory (PRL). Several people contributed
to improve on the form and contents of that report and thence of this ensuing
monograph. Thus, I am very much in debt to Manuel Hermenegildo, DavidBarker-Plummer, and David H. D. Warren, for the precious favor of proofreading a first draft, making some important corrections. Many thanks are due alsoto Patrick Baudelaire, Michel Gangnet, Solange Karsenty, Richard Meyer, and
Asc'ander Su'arez, for suggesting several emendations, having gracefully volun-teered to plow through the draft.

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press xiii

WARREN'S ABSTRACT MACHINE
As the PRL report was being disseminated, I began receiving feedback from at-tentive readers. Some of them caught a few serious bugs that remained in that
report making some material, as presented there, insidiously incorrect. Naturally,all those mistakes have now been corrected in this monograph, and, where appropriate, mention is made of those who brought to my attention my erroneous ac-count. Nevertheless, I would like to express here my gratitude to those who kindly
reported bugs, made insightful comments, gave disturbing counter-examples, orproposed better explanations. They are: Christoph Beierle, Andr'e Bolle, Damian
Chu, William Clocksin, Maarten van Emden, Michael Hanus, Pascal van Hen-tenryck, Juhani Jaakola, Stott Parker, Fernando Pereira, Frank Pfenning, Dave
Raggett, Dean Rosenzweig, David Russinoff, and two anonymous reviewers. Allremaining mistakes are to be blamed on my own incompetence and still imprecise
understanding.
Having been presumptuous enough to envisage elaborating the original tutorialinto book form, I have benefitted from the kind advice and efficient assistance

of Bob Prior, editor at MIT Press. I thank him for everything--his patience inparticular.

Finally, I gratefully acknowledge the benevolent agreement kindly given to me byPatrick Baudelaire, director of PRL, and Sam Fuller, Digital's vice-president for
Corporate Research and Architecture, to let me publish the work as a book. I amquite obliged for their

laissez-faire.

H:A:-K:
Rueil-Malmaison, France

January 1991

xiv Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

Nobody told them what it was. The thing was going veryslowly. I said that the first thing there has to be is that these
technical guys know what we're doing. ... I could give anice lecture about what we were doing, and they were all
excited ... They understood everything; ... and all that hadto be done was to tell them what it was.

RICHARD P. FEYNMANSurely You're Joking, Mr. Feynman

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 1 OF 129

WARREN'S ABSTRACT MACHINE
PAGE 2 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI
Chapter 1
Introduction
In 1983, David H. D. Warren designed an abstract machine for the execution of
Prolog consisting of a memory architecture and an instruction set [War83]. Thisdesign became known as the Warren Abstract Machine (WAM) and has become

the de facto standard for implementing Prolog compilers. In [War83], Warrendescribes the WAM in a minimalist's style, making understanding very difficult
for the average reader, even with a foreknowledge of Prolog's operations. Too
much is left untold, and very little is justified in clear terms.1 This has resultedin a very scant number of WAM

aficionados who could boast understanding thedetails of its workings. Typically, these have been Prolog implementors who decided to invest the necessary time to learn by doing and to reach enlightenmentpainstakingly.

1.1 Existing literature
Witness to this lack of understanding is the fact that in six years there has beenlittle published that would teach the WAM, let alone formally justify its correctness. Indeed, besides Warren's original hermetic report [War83], there has beenvirtually no official publication on the WAM. A few years ago, one could come
across a draft authored by a group at Argonne National Laboratory [GLLO85].But, to be honest, we found that manuscript even harder to understand than Warren's report. The flaw was that it insisted in presenting the complete WAM as is,

1David H. D. Warren's confides privately that he "felt [that the WAM] was important, but [its]
details unlikely to be of wide interest. Hence, [he used a] `personal notes' style."

3

WARREN'S ABSTRACT MACHINE
rather than as a gradually transformed and optimized design.
A gradual refinement style has in fact been used by David Maier and David S. War-ren

2 in [MW88]. There, one can find a description of techniques of Prolog compilation akin to the WAM's.3 However, we believe that this otherwise quite com-mendable effort still suffers from a few drawbacks as a definitive tutorial. First,

it describes a close variant of the WAM rather than, strictly speaking, the WAM
itself. That is, not all of the WAM's features are covered. Moreover, explanationsare limited to illustrative examples and seldom make explicitly and exhaustively

clear the specific context in which some optimizations apply. Second, the partdevoted to compilation of Prolog comes very late in the book--in the penultimate chapter--relying, for implementation details, on overly detailed Pascal pro-cedures and data structures incrementally refined over the previous chapters. We
feel that this sidetracks reading and obfuscates to-the-point learning of the ab-stract machine. Finally, although it presents a series of gradually refined designs,
their tutorial does not separate orthogonal pieces of Prolog in the process. All theversions presented are full Prolog machines. As a result, the reader interested in
picking and choosing partial techniques to adapt somewhere else cannot discriminate among these easily. Now, in all fairness, Maier and Warren's book has thedifferent ambition of being a first course in logic programming. Thus, it is actually a feat that its authors were able to cover so much material, both theoreticaland practical, and go so far as to include also Prolog compiling techniques. More
important, their book is the first available official publication to contain a (real)
tutorial on the WAM techniques.

After the preliminary version of this book had been completed, another recent
publication containing a tutorial on the WAM was brought to this author's atten-tion. It is a book due to Patrice Boizumault [Boi88] whose Chapter 9 is devoted to

explaining the WAM. There again, its author does not use a gradual presentationof partial Prolog machines. Besides, it is written in French--a somewhat restrictive trait as far as its readership is concerned. Still, Boizumault's book is very well
conceived, and contains a detailed discussion describing an explicit implementa-tion technique for the

freeze meta-predicate.4

2A different person than the WAM's designer, for whose research the WAM has been of great
inspiration. In turn, interestingly enough, David H. D. Warren has lately been working on a parallelarchitecture for Prolog whose abstract model shares its essence with some ideas independently

conceived by David S. Warren.3

Op. Cit., Chapter 11.4
Op. Cit., Chapter 10.

PAGE 4 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
Even more recently, a formal verification of the correctness of a slight simplifica-tion of the WAM was carried out by David Russinoff [Rus89]. That work deserves
justified praise as it methodically certifies correctness of most of the WAM withrespect to Prolog's SLD resolution semantics. However, it is definitely not a tutorial, although Russinoff defines most of the notions he uses in order to keep hiswork self-contained. In spite of this effort, understanding the details is considerably impeded without working familiarity with the WAM as a prerequisite. Atany rate, Russinoff's contribution is nevertheless a

premi`ere as he is the first toestablish rigorously something that had been taken for granted thus far. Needless

to say, that report is not for the fainthearted.

1.2 This tutorial
1.2.1 Disclaimer and motivation
The length of this monograph has been kept deliberately short. Indeed, this author
feels that the typical expected reader of a tutorial on the WAM would wish toget to the heart of the matter quickly and obtain complete but short answers to

questions. Also, for reasons pertaining to the specificity of the topic covered,it was purposefully decided not to structure it as a real textbook, with abundant
exercises and lengthy comments. Our point is to make the WAM explicit as it was
conceived by David H. D. Warren and to justify its workings to the reader withconvincing, albeit informal, explanations. The few proposed exercises are meant

more as an aid for understanding than as food for further thoughts.
The reader may find, at points, that some design decisions, clearly correct as theymay be, appear arbitrarily chosen among potentially many other alternatives, some

of which he or she might favor over what is described. Also, one may feel that thisor that detail could be "simplified" in some local or global way. Regarding this,
we wish to underscore two points: (1) we chose to follow Warren's original design
and terminology, describing what he did as faithfully as possible; and, (2) we warnagainst the casual thinking up of alterations that, although that may appear to be

"smarter" from a local standpoint, will generally bear subtle global consequencesinterfering with other decisions or optimizations made elsewhere in the design.
This being said, we did depart in some marginal way from a few original WAM
details. However, where our deviations from the original conception are proposed,an explicit mention will be made and a justification given.

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 5 OF 129

WARREN'S ABSTRACT MACHINE
Our motivation to be so conservative is simple: our goal is not to teach the worldhow to implement Prolog optimally,

nor is it to provide a guide to the state ofthe art on the subject. Indeed, having contributed little to the craft of Prolog

implementation, this author claims glaring incompetence for carrying out such
a task. Rather, this work's intention is to explain in simpler terms, and justifywith informal discussions, David H. D. Warren's abstract machine

specificallyand
exclusively. Our source is what he describes in [War83, War88]. The expectedachievement is merely the long overdue filling of a gap so far existing for whoever

may be curious to acquire basic knowledge of Prolog implementation techniques,as well as to serve as a spring board for the expert eager to contribute further to
this field for which the WAM is, in fact, just the tip of an iceberg. As such, itis hoped that this monograph would constitute an interesting and self-contained
complement to basic textbooks for general courses on logic programming, as wellas to those on compiler design for more conventional programming languages. As
a stand-alone work, it could be a quick reference for the computer professional in
need of direct access to WAM concepts.

1.2.2 Organization of presentation
Our style of teaching the WAM makes a special effort to consider carefully eachfeature of the WAM design in isolation by introducing separately and incrementally distinct aspects of Prolog. This allows us to explain as limpidly as possible specific principles proper to each. We then stitch and merge the differentpatches into larger pieces, introducing independent optimizations one at a time,

converging eventually to the complete WAM design as described in [War83] or asoverviewed in [War88]. Thus, in Chapter 2, we consider unification alone. Then,
we look at flat resolution (that is, Prolog without backtracking) in Chapter 3. Following that, we turn to disjunctive definitions and backtracking in Chapter 4. Atthat point, we will have a complete, albeit

na"ive, design for pure Prolog. In Chap-ter 5, this first-cut design will be subjected to a series of transformations aiming at

optimizing its performance, the end product of which is the full WAM. We havealso prepared an index for quick reference to most critical concepts used in the
WAM, something without which no (real) tutorial could possibly be complete.
It is expected that the reader already has a basic understanding of the operationalsemantics of Prolog--in particular, of unification and backtracking. Nevertheless,

to make this work also profitable to readers lacking this background, we haveprovided a quick summary of the necessary Prolog notions in Appendix A. As

PAGE 6 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
for notation, we implicitly use the syntax of so-called Edinburgh Prolog (see, forinstance, [CM84]), which we also recall in that appendix. Finally, Appendix B
contains a recapitulation of all explicit definitions implementing the full WAMinstruction set and its architecture so as to serve as a complete and concise summary.

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 7 OF 129

WARREN'S ABSTRACT MACHINE
PAGE 8 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI
Chapter 2
Unification--Pure and Simple
Recall that a (first-order) term is either a variable (denoted by a capitalized identifier), a constant (denoted by an identifier starting with a lower-case letter) or a
structure of the form f (t1; : : : ; tn) where f is a symbol called a functor (denotedas a constant), and the

ti's are first-order terms--the term's subterms. The numberof subterms for a given functor symbol is predetermined and called its

arity. Inorder to allow a symbol to be used with possibly different arities, we shall use

the explicit notation `f =n' when referring to the functor consisting of the symbol
f and arity n. Hence, two functors are equal if and only if they have the samesymbol

and arity. Letting n = 0, a constant is seen as a special case of a structure.Thus, a constant

c will be designated as the functor c=0.

We consider here L0, a very simple language indeed. In this language, one canspecify only two sorts of entities: a

program term and a query term. Both programand query are first-order terms but not variables. The semantics of

L0 is simply
tantamount to computing the most general unifier of the program and the query.As for syntax,

L0 will denote a program as t and a query as ?-t where t is aterm. The scope of variables is limited to a program (resp., a query) term. Thus,

the meaning of a program (resp., a query) is independent of its variables' names.An interpreter for

L0 will dispose of some data representation for terms and use
a unification algorithm for its operational semantics. We next describe M0 =
h D0 ; I0 i, an abstract machine design for L0 consisting of a data representation
D0 acted upon by a set I0 of machine instructions.

The idea is quite simple: having defined a program term p, one can submit anyquery

?-q and execution either fails if p and q do not unify, or succeeds with a

9

WARREN'S ABSTRACT MACHINE
binding of the variables in q obtained by unifying it with p.

2.1 Term representation
Let us first define an internal representation for terms in L0. We will use a globalblock of storage in the form of an addressable

heap called HEAP which is an array
of data cells. A heap cell's address is its index in the array HEAP.

It will be sufficient, in order to represent arbitrary terms in HEAP, to encode variables and `structures' of the form f (@1; : : : ; @n) where f =n is a functor and the
@i's are references to the heap addresses the n subterms. Thus, there are two sortsof data to be stored in the array

HEAP: variables and structures. Explicit tags,appearing as part of the format of some heap cells, will be used to discriminate

between these two sorts of heap data.
A variable will be identified to a reference pointer and represented using a singleheap cell. Thus, we shall speak of

variable cells. A variable cell will be identified
by the tag REF, as denoted as h REF ; k i where k is a store address; i.e., an indexinto

HEAP. This convenience is meant to facilitate variable binding by establish-ing a reference to the term the variable is to be bound to. Thus, upon binding

a variable, the address part of the REF cell that represents it will be set accord-ingly. The convention for representing an

unbound variable is to set the address
part of the REF cell to contain its own address. Therefore an unbound variable isa self-referential

REF cell.

Structures are non-variable terms. Thus, the heap format used for representing astructure

f (t1; : : : ; tn) will consist of n + 2 heap cells. The first two of these n + 2cells are not necessarily contiguous. In effect, the first of the two acts as a sorted

reference pointer to the second, itself used to represent the functor f =n. (Thereason for this apparently odd indirection is to accommodate structure sharing as
will become clear shortly.) The n other cells are destined to contain references to
the roots of the n subterms in proper order. More specifically, the first of the n + 2cells representing

f (t1; : : : ; tn) is formatted as a tagged structure cell, denoted as
h STR ; k i, containing the tag STR and the address k where (the representation of)the functor

f =n is stored. This cell is called a functor cell and, quite importantly, itis
always immediately followed by a sequence of n contiguous cells, one for each
subterm ti, respectively. That is, if HEAP[k] = f =n then HEAP[k + 1] willrefer to the first subterm

(t1), ... HEAP[k + n] to the n-th (and last) subterm (tn).

PAGE 10 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

0 STR 1
1 h=2
2 REF 2
3 REF 3
4 STR 5
5 f =1
6 REF 3
7 STR 8
8 p=3
9 REF 2
10 STR 1
11 STR 5

Figure 2.1: Heap representation of p(Z; h(Z; W ); f (W )):
For example, a possible heap representation of the term p(Z; h(Z; W ); f (W ))starts at address 7 in the heap shown in Figure 2.1. Note that only one occurrence of a given variable is represented by an unbound REF cell, whereas its otheroccurrences are

REF cells containing the heap address of the first occurrence. Ob-serve also that, although it is true that the structure cells at addresses 0, 4, and 7

do contiguously precede their respective functor cells, such is not the case for the
structure cells at address 10, and 11.

2.2 Compiling L0 queries
According to L0's operational semantics, the processing of a query consists ofpreparing one side of an equation to be solved. Namely, a query term

q is trans-lated into a sequence of instructions designed to build an exemplar of

q on theheap from
q's textual form. Hence, due to the tree structure of a term and multiple occurrences of variables, it is necessary, while processing a part of the term,to save temporarily someplace pieces of terms yet to be processed or a variable

that may occur again later in the term. For this purpose, M0 is endowed with asufficient number of (variable)

registers X1, X2, etc., to be used to store heap datatemporarily as terms are being built. Thus, the contents of such a register will

have the format of a heap cell. These variable registers are allocated to a termon a least available index basis such that (1) register

X1 is always allocated to the

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 11 OF 129

WARREN'S ABSTRACT MACHINE
outermost term, and (2) the same register is allocated to all the occurrences of agiven variable. For example, registers are allocated to the variables of the term
p(Z; h(Z; W ); f (W )) as follows:

X1 = p(X2; X3; X4)
X2 = Z
X3 = h(X2; X5)
X4 = f (X5)
X5 = W:

This amounts to saying that a term is seen as a flattened conjunctive set of equations of the form Xi = X or Xi = f (Xi1 ; : : : ; Xin ), (n * 0) where the Xi's areall distinct new variable names. There are two consequences of register allocation:

(1) external variable names (such as Z and W in our example) can all be forgotten;and, (2) a query term can then be transformed into its

flattened form, a sequence ofregister assignments only of the form
Xi = f (Xi1; : : : ; Xin). This form is what is
to guide the building of the term's heap representation. Thus, for left-to-right codegeneration to be well-founded, it is necessary to order a flattened query term so as

to ensure that a register name may not be used in the right-hand side of an assign-ment (

viz., as a subterm) before its assignment, if it has one (viz., being the left-hand side). For example, the flattened form of query term

p(Z; h(Z; W ); f (W ))
is the sequence X3 = h(X2; X5), X4 = f (X5), X1 = p(X2; X3; X4).

Scanning a flattened query term from left to right, each component of the form
Xi = f (Xi1; : : : ; Xin) is tokenized as a sequence Xi = f =n, Xi1, : : :, Xin; that is,a register associated with an

n-ary functor followed by exactly n register names.Therefore, in a stream of such tokens resulting from tokenizing a full flattened

term, there are three kinds of items to process:

1. a register associated with a structure functor;
2. a register argument not previously encountered anywhere in the stream;
3. a register argument seen before in the stream.
From this stream, a token-driven heap representation of the term is easy to ob-tain. To build it, the actions to be taken for each of the three sorts of tokens are,

respectively:

1. push a new STR (and adjoining functor) cell onto the heap and copy thatcell into the allocated register address;

PAGE 12 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

2. push a new REF cell onto the heap containing its own address, and copy itinto the given register;
3. push a new cell onto the heap and copy into it the register's value.
Each of these three actions specifies the effect of respective instructions of the
machine M0 that we note:

1. put structure f =n; Xi
2. set variable Xi
3. set value Xi

respectively.
From the preceding considerations, it has become clear that the heap is implicitly
used as a stack for building terms. Namely, term parts being constructed areincrementally piled on

top of what already exists in the heap. Therefore, it isnecessary to keep the address of the next free cell in the heap somewhere, precisely

as for a stack.1 Adding to M0 a global register H containing at all times thenext available address on the heap, these three instructions are given explicitly
in Figure 2.2. For example, given that registers are allocated as above, thesequence of instructions to build the query term

p(Z; h(Z; W ); f (W )), is shownin Figure 2.3.

Exercise 2.1 Verify that the effect of executing the sequence of instructions shown
in Figure 2.3 (starting with H = 0) does indeed yield a correct heap representation
for the term p(Z; h(Z; W ); f (W ))--the one shown earlier as Figure 2.1, in fact.

2.3 Compiling L0 programs
Compiling a program term p is just a bit trickier, although not by much. Observe
that it assumes that a query ?-q will have built a term on the heap and set register
X1 to contain its address. Thus, unifying q to p can proceed by following the termstructure already present in

X1 as long as it matches functor for functor the struc-ture of
p. The only complication is that when an unbound REF cell is encountered

1As a matter of fact, in [War83], Warren refers to the heap as the global stack.

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 13 OF 129

WARREN'S ABSTRACT MACHINE
put structure f =n; Xi j HEAP[H]  h STR ; H + 1 i;

HEAP[H + 1]  f =n;
Xi  HEAP[H];
H  H + 2;

set variable Xi j HEAP[H]  h REF ; H i;

Xi  HEAP[H];
H  H + 1;

set value Xi j HEAP[H]  Xi;

H  H + 1;

Figure 2.2: M0 machine instructions for query terms

put structure h=2; X3 % ?-X3 = h
set variable X2 % (Z;
set variable X5 % W );
put structure f =1; X4 % X4 = f
set value X5 % (W );
put structure p=3; X1 % X1 = p
set value X2 % (Z;
set value X3 % X3;
set value X4 % X4):

Figure 2.3: Compiled code for L0 query ?-p(Z; h(Z; W ); f (W )):
PAGE 14 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
in the query term in the heap, then it is to be bound to a new term that is built onthe heap as an exemplar of the corresponding subterm in

p. Therefore, an L0 pro-gram functions in two modes: a
read mode in which data on the heap is matchedagainst, and a
write mode in which a term is built on the heap exactly as is a
query term.

As with queries, register allocation precedes translation of the textual form of a
program term into a machine instruction sequence. For example, the followingregisters are allocated to program term

p(f (X); h(Y; f (a)); Y ):

X1 = p(X2; X3; X4)
X2 = f (X5)
X3 = h(X4; X6)
X4 = Y
X5 = X
X6 = f (X7)
X7 = a:

Recall that compiling a query necessitates ordering its flattened form in such away as to build a term once its subterms have been built. Here, the situation is
reversed because query data from the heap are assumed available, even if only in
the form of unbound REF cells. Hence, a program term's flattened form followsa top-down order. For example, the program term

p(f (X); h(Y; f (a)); Y ) is putinto the flattened sequence:
X1 = p(X2; X3; X4), X2 = f (X5), X3 = h(X4; X6),
X6 = f (X7), X7 = a.

As with query compiling, the flattened form of a program is tokenized for left-to-right processing and generates three kinds of machine instructions depending on

whether is met:

1. a register associated with a structure functor;
2. a first-seen register argument; or,
3. an already-seen register argument.

These instructions are,

1. get structure f =n; Xi
2. unify variable Xi

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 15 OF 129

WARREN'S ABSTRACT MACHINE
get structure p=3; X1 % X1 = p
unify variable X2 % (X2;
unify variable X3 % X3;
unify variable X4 % Y );
get structure f =1; X2 % X2 = f
unify variable X5 % (X);
get structure h=2; X3 % X3 = h
unify value X4 % (Y;
unify variable X6 % X6);
get structure f =1; X6 % X6 = f
unify variable X7 % (X7);
get structure a=0; X7 % X7 = a:

Figure 2.4: Compiled code for L0 program p(f (X); h(Y; f (a)); Y ):
3. unify value Xi
respectively.
Taking for example the program term p(f (X); h(Y; f (a)); Y ), the M0 machine
instructions shown in Figure 2.4 are generated. Each of the two unify in-structions functions in two modes depending on whether a term is to be matched

from, or being built on, the heap. For building (write mode), the work to bedone is exactly that of the two

set query instructions of Figure 2.2. For matching(
read mode), these instructions seek to recognize data from the heap as those
of the term at corresponding positions, proceeding if successful and failing other-wise. In

L0, failure aborts all further work. In read mode, these instructions seta global register

S to contain at all times the heap address of the next subterm tobe matched.

Variable binding creates the possibility that reference chains may be formed.Therefore,

dereferencing is performed by a function deref which, when appliedto a store address, follows a possible reference chain until it reaches either an unbound REF cell or a non-REF cell, the address of which it returns. The effect of
dereferencing is none other than composing variable substitutions. Its definitionis given in Figure 2.5. We shall use the generic notation

STORE[a] to denote the

PAGE 16 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

function deref (a : address) : address;begin

h tag ; value i  STORE[a];if

(tag = REF) ^ (value 6= a)then return

deref (value)else return
aend
deref ;

Figure 2.5: The deref operation
contents of a term data cell at address a (whether heap, X register, or any other
global structure, yet to be introduced, containing term data cells). We shall usespecific area notation (

e.g., HEAP[a]) whenever we want to emphasize that theaddress
a must necessarily lie within that area.

Mode is set by get structure f =n; Xi as follows: if the dereferenced valueof

Xi is an unbound REF cell, then it is bound to a new STR cell pointing to f =npushed onto the heap and mode is set to

write; otherwise, if it is an STR cellpointing to functor
f =n, then register S is set to the heap address following that
functor cell's and mode is set to read. If it is not an STR cell or if the functoris not

f =n, the program fails. Similarly, in read mode, unify variable Xisets register

Xi to the contents of the heap at address S; in write mode, a newunbound
REF cell is pushed on the heap and copied into Xi. In both modes, S isthen incremented by one. As for

unify value Xi, in read mode, the value of
Xi must be unified with the heap term at address S; in write mode, a new cell ispushed onto the heap and set to the value of register

Xi. Again, in either mode, Sis incremented. All three instructions are expressed explicitly in Figure 2.6.

In the definition of get structure f =n; Xi, we write bind(addr; H) to effec-tuate the binding of the heap cell rather than

HEAP[addr]  h REF ; H i for rea-sons that will become clear later. The
bind operation is performed on two storeaddresses, at least one of which is that of an unbound

REF cell. Its effect, fornow, is to bind the unbound one to the other--
i.e., change the data field of the
unbound REF cell to contain the address of the other cell. In the case where bothare unbound, the binding direction is chosen arbitrarily. Later, this will change

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 17 OF 129

WARREN'S ABSTRACT MACHINE
get structure f =n; Xi j addr  deref (Xi);case

STORE[addr] of
h REF ; i : HEAP[H]  h STR ; H + 1 i;

HEAP[H + 1]  f =n;
bind(addr; H);
H  H + 2;
mode  write;
h STR ; a i : if HEAP[a] = f =nthen

begin

S  a + 1;
mode  readend

else fail  true;other
: fail  true;endcase
;

unify variable Xi j case mode of

read : Xi  HEAP[S];
write : HEAP[H]  h REF ; H i;

Xi  HEAP[H];
H  H + 1;endcase
;
S  S + 1;

unify value Xi j case mode of

read : unify(Xi; S);
write : HEAP[H]  Xi;

H  H + 1;endcase
;
S  S + 1;

Figure 2.6: M0 machine instructions for programs
PAGE 18 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
as a correctness-preserving measure in order to accommodate an optimization.Also, we will see that

bind is the logical place, when backtracking needs to beconsidered, for recording effects to be undone upon failure (see Chapter 4, and

appendix Section B.2 on Page 113). If wished, bind may also be made to perform
the occurs-check test in order to prevent formation of cyclic terms (by failing atthat point). However, the occurs-check test is omitted in most actual Prolog implementations in order not to impede performance.
We must also explicate the unify operation used in the matching phase (in readmode). It is a unification algorithm based on the

UNION/FIND method [AHU74],where variable substitutions are built, applied, and composed through

dereferencepointers. In
M0 (and in all later machines that will be considered here), thisunification operation is performed on a pair of store addresses. It uses a global

dynamic structure, an array of store addresses, as a unification stack (called PDL,for

Push-Down List). The unification operation is defined as shown in Figure 2.7,where

empty, push, and pop are the expected stack operations.

Exercise 2.2 Give heap representations for the terms f (X; g(X; a)) and f (b; Y ).
Let a1 and a2 be their respective heap addresses, and let aX and aY be the heap
addresses corresponding to variables X and Y , respectively. Trace the effects of
executing unify(a1; a2), verifying that it terminates with the eventual dereferenced
bindings from aX and aY corresponding to X = b and Y = g(b; a).

Exercise 2.3 Verify that the effect of executing the sequence of instructions shown
in Figure 2.4 right after that in Figure 2.3 produces the MGU of the terms p(Z; h(Z; W ); f (W ))
and p(f (X); h(Y; f (a)); Y ). That is, the (dereferenced) bindings corresponding to
W = f (a), X = f (a), Y = f (f (a)), Z = f (f (a)).

Exercise 2.4 What are the respective sequences of M0 instructions for L0 query
term ?-p(f (X); h(Y; f (a)); Y ) and program term p(Z; h(Z; W ); f (W ))?

Exercise 2.5 After doing Exercise 2.4, verify that the effect of executing the sequence you produced yields the same solution as that of Exercise 2.3.

2.4 Argument registers
Since we have in mind to use unification in Prolog for procedure invocation, wecan introduce a distinction between atoms (terms whose functor is a predicate) and

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 19 OF 129

WARREN'S ABSTRACT MACHINE
procedure unify(a1; a2 : address);

push(a1; PDL); push(a2; PDL);
fail  false;while

:(empty(PDL) . fail) dobegin

d1  deref (pop(PDL)); d2  deref (pop(PDL));if

d1 6= d2 thenbegin

h t1 ; v1 i  STORE[d1]; h t2 ; v2 i  STORE[d2];if

(t1 = REF) . (t2 = REF)then

bind(d1; d2)else

begin

f1=n1  STORE[v1]; f2=n2  STORE[v2];if

(f1 = f2) ^ (n1 = n2)then

for i  1 to n1 dobegin

push(v1 + i; PDL);
push(v2 + i; PDL)end

else fail  trueend
endend
end unify;

Figure 2.7: The unify operation

PAGE 20 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
terms (arguments to a predicate). We thus extend L0 into a language L1 similarto

L0 but where a program may be a set of first-order atoms each defining at mostone

fact per predicate name. Thus, in the context of such a program, execution ofa query connects to the appropriate definition to use for solving a given unification

equation, or fails if none exists for the predicate invoked.
The set of instructions I1 contains all those in I0. In M1, compiled code is stored
in a code area (CODE), an addressable array of data words, each containing apossibly labeled instruction over one or more memory words consisting of an opcode followed by operands. For convenience, the size of an instruction storedat address

a (i.e., CODE[a]) will be assumed given by the expression instruction size(a). Labels are symbolic entry points into the code area that may be usedas operands of instructions for transferring control to the code labeled accordingly.

Therefore, there is no need to store a procedure name in the heap as it denotes akey into a compiled instruction sequence. Thus, a new instruction

call p=n canbe used to pass control over to the instruction labeled with
p=n, or fail if nonesuch exists.

A global register P is always set to contain the address of the next instruction toexecute (an instruction counter). The standard execution order of instructions is
sequential. Unless failure occurs, most machine instructions (like all those seen
before) are implicitly assumed, to increment P by an appropriate offset in thecode area as an ultimate action. This offset is the size of the instruction at address

P. However, some instructions have for purpose to break the sequential order ofexecution or to connect to some other instruction at the end of a sequence. These
instructions are called control instructions as they typically set P in a non-standard
way. This is the case of call p=n, whose explicit effect, in the machine M1, is:

call p=n j P  @(p=n);
where the notation @(p=n) stands for the address in the code area of instructionlabeled

p=n. If the procedure p=n is not defined (i.e., if that address is not allocatedin the code area), a unification failure occurs and overall execution aborts.

We also introduce another control instruction, proceed, which indicates the endof a fact's instruction sequence. These two new control instructions' effects are
trivial for now, and they will be elaborated later. For our present purposes, it issufficient that

proceed be construed as a no-op (i.e., just a code terminator),
and call p=n as an unconditional "jump" to the start address of the instructionsequence for program term with functor

p=n.

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 21 OF 129

WARREN'S ABSTRACT MACHINE
Having eliminated predicate symbols from the heap, the unification problem be-tween fact and query terms amounts to solving, not one, but many equations,
simultaneously. Namely, there are as many term roots in a given fact or query asthere are arguments to the corresponding predicate. Therefore, we must organize
registers quite specifically so as to reflect this situation. As we privileged X1 be-fore to denote the (single) term root, we generalize the convention to registers

X1to
Xn which will now always refer to the first to n-th arguments of a fact or queryatom. In other words, registers

X1; : : : ; Xn are systematically allocated to termroots of an
n-ary predicate's arguments. To emphasize this, we use a conspicuousnotation, writing a register

Ai rather than Xi when it is being used as an argument
of an atom. In that case, we refer to that register as an argument register. Oth-erwise, where register

Xi is not used as an argument register, it is written Xi, asusual. Note that this is just notation as the

Ai's are not new registers but the sameold
Xi's used thus far. For example, registers are now allocated for the variablesof the atom

p(Z; h(Z; W ); f (W )) as follows:

A1 = Z
A2 = h(A1; X4)
A3 = f (X4)
X4 = W:

Observe also that a new situation arises now as variables can be arguments andthus must be handled as roots. Therefore, provision must be made for variables
to be loaded into, or extracted from, argument registers for queries and facts,
respectively. As before, the necessary instructions correspond to when a variableargument is a first or later occurrence, either in a query or a fact. In a query,

1. the first occurrence of a variable in i-th argument position pushes a newunbound

REF cell onto the heap and copies it into that variable's register aswell as argument register

Ai; and,

2. a later occurrence copies its value into argument register Ai. Whereas, in afact,

3. the first occurrence of a variable in i-th argument position sets it to the valueof argument register

Ai; and,

4. a later occurrence unifies it with the value of Ai.
The corresponding instructions are, respectively:

PAGE 22 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

put variable Xn; Ai j HEAP[H]  h REF ; H i;

Xn  HEAP[H];
Ai  HEAP[H];
H  H + 1;

put value Xn; Ai j Ai  Xn
get variable Xn; Ai j Xn  Ai
get value Xn; Ai j unify(Xn; Ai)

Figure 2.8: M1 instructions for variable arguments
1. put variable Xn; Ai
2. put value Xn; Ai
3. get variable Xn; Ai
4. get value Xn; Ai
and are given explicitly in Figure 2.8. For example, Figure 2.9 shows code gener-ated for query

?- p(Z; h(Z; W ); f (W )):, and Figure 2.10 that for fact p(f (X); h(Y; f (a)); Y ).

Exercise 2.6 Verify that the effect of executing the sequence of M1 instructions
shown in Figure 2.9 produces the same heap representation as that produced by the
M0 code of Figure 2.3 (see Exercise 2.1).

Exercise 2.7 Verify that the effect of executing the sequence of M1 instructions shown in Figure 2.10 right after that in Figure 2.9 produces the MGU of
the terms p(Z; h(Z; W ); f (W )) and p(f (X); h(Y; f (a)); Y ). That is, the binding
W = f (a), X = f (a), Y = f (f (a)), Z = f (f (a)).

Exercise 2.8 What are the respective sequences of M1 instructions for L1 query
term ?-p(f (X); h(Y; f (a)); Y ) and L1 program term p(Z; h(Z; W ); f (W ))?

Exercise 2.9 After doing Exercise 2.8, verify that the effect of executing the sequence you produced yields the same solution as that of Exercise 2.7.

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 23 OF 129

WARREN'S ABSTRACT MACHINE
put variable X4; A1 % ?-p(Z;
put structure h=2; A2 % h
set value X4 % (Z;
set variable X5 % W );
put structure f =1; A3 % f
set value X5 % (W )
call p=3 % ):

Figure 2.9: Argument registers for L1 query ?-p(Z; h(Z; W ); f (W )):

p=3 : get structure f =1; A1 % p(f

unify variable X4 % (X);
get structure h=2; A2 % h
unify variable X5 % (Y;
unify variable X6 % X6);
get value X5; A3 % Y );
get structure f =1; X6 % X6 = f
unify variable X7 % (X7);
get structure a=0; X7 % X7 = a
proceed % :

Figure 2.10: Argument registers for L1 fact p(f (X); h(Y; f (a)); Y ):

PAGE 24 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

Chapter 3
Flat Resolution
We now extend the language L1 into a language L2 where procedures are no
longer reduced only to facts but may also have bodies. A body defines a proce-dure as a conjunctive sequence of atoms. Said otherwise,

L2 is Prolog withoutbacktracking.

An L2 program is a set of procedure definitions or (definite) clauses, at most oneper predicate name, of the form `

a0 :- a1; : : : ; an:' where n * 0 and the ai's areatoms. As before, when
n = 0, the clause is called a fact and written without the`
:-' implication symbol. When n ? 0, the clause is called a rule, the atom a0
is called its head, the sequence of atoms a1; : : : ; an is called its body and atomscomposing this body are called

goals. A rule with exactly one body goal is calleda
chain (rule). Other rules are called deep rules. L2 queries are sequences ofgoals, of the form `

?-g1; : : : ; gk:' where k * 0. When k = 0, the query is calledthe
empty query. As in Prolog, the scope of variables is limited to the clause or
query in which they appear.

Executing a query `?-g1; : : : ; gk:' in the context of a program made up of a set of
procedure-defining clauses consists of repeated application of leftmost resolutionuntil the empty query, or failure, is obtained. Leftmost resolution amounts to

unifying the goal g1 with its definition's head (or failing if none exists) and, ifthis succeeds, executing the query resulting from replacing

g1 by its definitionbody, variables in scope bearing the binding side-effects of unification. Thus,

executing a query in L2 either terminates with success (i.e., it simplifies into the
empty query), or terminates with failure, or never terminates. The "result" of an
L2 query whose execution terminates with success is the (dereferenced) binding

25

WARREN'S ABSTRACT MACHINE
of its original variables after termination.
Note that a clause with a non-empty body can be viewed in fact as a conditionalquery. That is, it behaves as a query provided that its head successfully unifies with

a predicate definition. Facts merely verify this condition, adding nothing new tothe query but a contingent binding constraint. Thus, as a first approximation, since
an L2 query (resp., clause body) is a conjunctive sequence of atoms interpreted
as procedure calls with unification as argument passing, instructions for it maysimply be the concatenation of the compiled code of each goal as an

L1 querymaking it up. As for a clause head, since the semantics requires that it retrieves

arguments by unification as did facts in L1, instructions for L1's fact unificationare clearly sufficient.

Therefore, M1 unification instructions can be used for L2 clauses, but with twomeasures of caution: one concerning continuation of execution of a goal sequence,
and one meant to avoid conflicting use of argument registers.

3.1 Facts
Let us first only consider L2 facts. Note that L1 is all contained in L2. Therefore,it is natural to expect that the exact same compilation scheme for facts carries over
untouched from L1 to L2. This is true up to a wee detail regarding the proceedinstruction. It must be made to continue execution, after successfully returning
from a call to a fact, back to the instruction in the goal sequence following the
call. To do this correctly, we will use another global register CP, along with P, setto contain the address (in the code area) of the next instruction to follow up with

upon successful return from a call (i.e., set to P+instruction size(P) at procedurecall time). Then, having exited the called procedure's code sequence, execution
could thus be resumed as indicated by CP. Thus, for L2's facts, we need to alterthe effect of

M1's call p=n to:

call p=n j CP  P + instruction size(P);

P  @(p=n);

and that of proceed to:

proceed j P  CP;
As before, when the procedure p=n is not defined, execution fails.

PAGE 26 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
In summary, with the simple foregoing adjustment, L2 facts are translated exactlyas were

L1 facts.

3.2 Rules and queries
We now must think about translating rules. A query is a particular case of a rulein the sense that it is one with no head. It is translated exactly the same way,
but without the instructions for the missing head. The idea is to use L1's instruc-tions, treating the head as a fact, and each goal in the body as an

L1 query termin sequence; that is, roughly translate a rule `
p0(: : :) :- p1(: : :); : : : ; pn(: : :):'
following the pattern:

get arguments of p0
put arguments of p1
call p1 .

..

put arguments of pn
call pn

Here, in addition to ensuring correct continuation of execution, we must arrangefor correct use of argument registers. Indeed, since the same registers are used by
each goal in a query or body sequence to pass its arguments to the procedure itinvokes, variables that occur in many different goals in the scope of the sequence
need to be protected from the side effects of put instructions. For example, con-sider the rule `

p(X; Y ) :- q(X; Z); r(Z; Y ):' If the variables Y; Z were allowedto be accessible only from an argument register, no guarantee could be made that

they still would be after performing the unifications required in executing the bodyof

p.

Therefore, it is necessary that variables of this kind be saved in an environment
associated with each activation of the procedure they appear in. Variables whichoccur in more than one body goal are dubbed

permanent as they have to outlivethe procedure call where they first appear. All other variables in a scope that are

not permanent are called temporary. We shall denote a permanent variable as Yi,and use

Xi as before for temporary variables. To determine whether a variable is
permanent or temporary in a rule, the head atom is considered to be part of thefirst body goal. This is because

get and unify instructions do not load registers

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 27 OF 129

WARREN'S ABSTRACT MACHINE
for further processing. Thus, the variable X in the example above is temporary asit does not occur in more than one goal in the body (

i.e., it is not affected by morethan one goal's
put instructions).

Clearly, permanent variables behave like conventional local variables in a proce-dure. The situation is therefore quite familiar. As is customary in programming

languages, we protect a procedure's local variables by maintaining a run-time
stack of procedure activation frames in which to save information needed for thecorrect execution of what remains to be done after returning from a procedure call.

We call such a frame an environment frame. We will keep the address of the latestenvironment on top of the stack in a global register

E.1

As for continuation of execution, the situation for rules is not as simple as thatfor facts. Indeed, since a rule serves to invoke further procedures in its body, the

value of the program continuation register CP, which was saved at the point ofits call, will be overwritten. Therefore, it is necessary to preserve continuation
information by saving the value of CP along with permanent variables.
Let us recapitulate: M2 is an augmentation of M1 with the addition of a new data
area, along with the heap (HEAP), the code area (CODE), and the push-down list(

PDL). It is called the stack (STACK) and will contain procedure activation frames.Stack frames are called

environments. An environment is pushed onto STACKupon a (non-fact) procedure entry call, and popped from

STACK upon return.Thus, an
allocate/deallocate pair of instructions must bracket the code
generated for a rule in order to create and discard, respectively, such environmentframes on the stack. In addition,

deallocate being the ultimate instructionof the rule, it must connect to the appropriate next instruction as indicated by

the continuation pointer that had been saved upon entry in the environment beingdiscarded.

Since the size of an environment varies with each procedure in function of itsnumber of permanent variables, the stack is organized as a linked list through a
continuation environment slot; i.e., a cell in each environment frame bearing thestack index of the environment previously pushed onto the stack.

To sum up, two new I2 instructions for M2 are added to the ones we defined for
I1:

1. allocate
1 In [War83], this stack is called the local stack to distinguish it from the global stack (see
Footnote 1 at the bottom of Page 13).

PAGE 28 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

2. deallocate
with effect, respectively:

1. to allocate a new environment on the stack, setting its continuation environ-ment field to the current value of

E, and its continuation point field to thatof
CP; and,

2. to remove the environment frame at stack location E from the stack andproceed, resetting

P to the value of its CP field and E to the value of its CEfield.

To have proper effect, an allocate instruction needs to have access to the sizeof the current environment in order to increment the value of

E by the right stack
offset. The necessary piece of information is a function of the calling clause (i.e.,the number of permanent variables occurring in the calling clause). Therefore, it

is easily statically available at the time the code for the calling clause is generated.Now, the problem is to transmit this information to the called procedure that, if
defined as a rule (i.e., starting with an allocate), will need it dynamically,
depending on which clause calls it. A simple solution is to save this offset in thecalling clause's environment frame from where it can be retrieved by a callee that

needs it. Hence, in M2, an additional slot in an environment is set by allocateto contain the number of permanent variables in the clause in question.

Summing up again, an M2 stack environment frame contains:

1. the address in the code area of the next instruction to execute upon (suc-cessful) return from the invoked procedure;

2. the stack address of the previous environment to reinstate upon return (i.e.,where to pop the stack to);
3. the offset of this frame on the stack (the number of permanent variables);and,
4. as many cells as there are permanent variables in the body of the invoked

procedure (possibly none).

Such an M2 environment frame pushed on top of the stack looks thus:
Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 29 OF 129

WARREN'S ABSTRACT MACHINE
E CE (continuation environment)
E + 1 CP (continuation point)
E + 2 n (number of permanent variables)
E + 3 Y1 (permanent variable 1).

..

E + n + 2 Yn (permanent variable n)

This necessitates giving allocate an explicit argument that is the number ofpermanent variables of the rule at hand, such that, in

M2:

allocate N j newE  E + STACK[E + 2] + 3;

STACK[newE]  E;
STACK[newE + 1]  CP;
STACK[newE + 2]  N ;
E  newE;
P  P + instruction size(P);

Similarly, the explicit definition of M2's deallocate is:

deallocate j P  STACK[E + 1];

E  STACK[E];

With this being set up, the general translation scheme into M2 instructions for an
L2 rule `p0(: : :) :- p1(: : :); : : : ; pn(: : :):' with N permanent variables will followthe pattern:

p0 : allocate N

get arguments of p0
put arguments of p1
call p1.

..

put arguments of pn
call pn
deallocate

For example, for L2 clause `p(X; Y ) :- q(X; Z); r(Z; Y ):', the corresponding
M2 code is shown in Figure 3.1.

PAGE 30 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

p=2 : allocate 2 % p

get variable X3; A1 % (X;
get variable Y1; A2 % Y ) :-
put value X3; A1 % q(X;
put variable Y2; A2 % Z
call q=2 % );
put value Y2; A1 % r(Z;
put value Y1; A2 % Y
call r=2 % )
deallocate % :

Figure 3.1: M2 machine code for rule p(X; Y ) :- q(X; Z); r(Z; Y ):

Exercise 3.1 Give M2 code for L2 facts q(a; b) and r(b; c) and L2 query ?-p(U; V ),
then trace the code shown in Figure 3.1 and verify that the solution produced is
U = a; V = c.

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 31 OF 129

WARREN'S ABSTRACT MACHINE
PAGE 32 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI
Chapter 4
Prolog
The language L3 (resp., the machine M3) corresponds to pure Prolog, as it extends the language L2 (resp., the machine M2) to allow disjunctive definitions.As in

L2, an L3 program is a set of procedure definitions. In L3, a definition isan ordered sequence of clauses (

i.e., a sequence of facts or rules) consisting ofall and only those whose head atoms share the same predicate name. That name

is the name of the procedure specified by the definition. L3 queries are the same
as those of L2. The semantics of L3 operates using top-down leftmost resolu-tion, an approximation of SLD resolution. Thus, in

L3, a failure of unification nolonger yields irrevocable abortion of execution but considers alternative choices

of clauses in the order in which they appear in definitions. This is done by chrono-logical backtracking;

i.e., the latest choice at the moment of failure is reexamined
first.

It is necessary to alter M2's design so as to save the state of computation at each
procedure call offering alternatives to restore upon backtracking to this point ofchoice. We call such a state a

choice point. We thus need to analyze what in-formation must be saved as a choice point in order to create a record (a choice

point frame) wherefrom a correct state of computation can be restored to offeranother alternative, with all effects of the failed computation undone. Note that
choice point frames must be organized as a stack (just like environments) in orderto reflect the compounding of alternatives as each choice point spawns potentially
more alternatives to try in sequence.
To distinguish the two stacks, let us call the environment stack the AND-stack andthe choice point stack the

OR-stack. As with the AND-stack, we organize the

33

WARREN'S ABSTRACT MACHINE
OR-stack as a linked list. The head of this list always corresponds to the latestchoice point, and will be kept in a new global register

B, such that upon failure,computation is made to resume from the state recovered from the choice point

frame indicated by B. When the latest frame offers no more alternatives, it is
popped off the OR-stack by resetting B to its predecessor if one exists, otherwisecomputation fails terminally.

Clearly, if a definition contains only one clause, there is no need to create a choicepoint frame, exactly as was the case in

M2. For definitions with more than onealternative, a choice point frame is created by the first alternative; then, it is updated (as far as which alternative to try next) by intermediate (but non ultimate)alternatives; finally, it is discarded by the last alternative.

4.1 Environment protection
Before we go into the details of what exactly constitutes a choice frame, we mustponder carefully the interaction between the AND-stack and the OR-stack. As
long as we considered (deterministic) L2 program definitions, it was clearly safeto deallocate an environment frame allocated to a rule after successfully falling
off the end of the rule. Now, the situation is not quite so straightforward as later
failure may force reconsidering a choice from a computation state in the middleof a rule whose environment has long been deallocated. This case is illustrated by

the following example program:

a :- b(X); c(X):
b(X) :- e(X):
c(1):
e(X) :- f (X):
e(X) :- g(X):

f (2):
g(1):
Executing `?-a:' allocates an environment for a, then calls b. Next, an envi-ronment for

b is allocated, and e is called. This creates a choice point on the
OR-stack, and an environment for e is pushed onto the AND-stack. At this pointthe two stacks look thus:

1

1In these diagrams, the stacks grow downwards; i.e., the stack top is the lower part.

PAGE 34 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

...
Environment for aEnvironment for

b
E ! Environment for e

...
B ! Choice point for e

The following call to f succeeds binding X to 2. The environment for e is deallo-cated, then the environment for

b is also deallocated. This leads to stacks lookingthus:

...
E ! Environment for a

...
B ! Choice point for e

Next, the continuation follows up with execution of a's body, calling c, whichimmediately hits failure. The choice point indicated by

B shows an alternativeclause for
e, but at this point b's environment has been lost. Indeed, in a moreinvolved example where

c proceeded deeper before failing, the old stack space for
b's environment would have been overwritten by further calls in c's body.

Therefore, to avoid this kind of misfortune, a setup must be found to prevent
unrecoverable deallocation of environment frames whose creation chronologicallyprecedes that of any existing choice point. The idea is that every choice point

must "protect" from deallocation all environment frames already existing beforeits creation. Now, since a stack reflects chronological order, it makes sense to
use the same stack for both environments and choice points. A choice point now
caps all older environments. In effect, as long as it is active, it forces allocation offurther environments on top of it, preventing the older environments' stack space

to be overwritten even though they may explicitly be deallocated. This allowstheir safe resurrection if needed by coming back to an alternative from this choice
point. Moreover, this "protection" lasts just as long as it is needed since as soon asthe choice point disappears, all explicitly deallocated environments can be safely
overwritten.
Hence, there is no need to distinguish between the AND-stack from the OR-stack,
calling the single one the stack. Choice point frames are stored in the stack alongwith environments, and thus

B's value is an address in the stack.

Going back to our example above, the snapshot of the single stack at the same firstinstant looks thus:

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 35 OF 129

WARREN'S ABSTRACT MACHINE
...
Environment for aEnvironment for

b
B ! Choice point for e
E ! Environment for e

and at the same second instant as before, the stack is such that having pushed on
it the choice point for e protects b's deallocated environment (which may still beneeded by future alternatives given by

e's choice point), looking thus:

...
E ! Environment for aDeallocated environment for

b
B ! Choice point for e

Now, the computation can safely recover the state from the choice point for eindicated by

B, in which the saved environment to restore is the one current atthe time of this choice point's creation--

i.e., that (still existing) of b. Having nomore alternative for
e after the second one, this choice point is discarded uponbacktracking, (safely) ending the protection. Execution of the last alternative for

e proceeds with a stack looking thus:

B ! .

..

Environment for aEnvironment for

b
E ! Environment for e

4.2 What's in a choice point
When a chosen clause is attempted among those of a definition, it will createside effects on the stack and the heap by binding variables residing there. These
effects must be undone when reconsidering the choice. A record must be kept ofthose variables which need to be reset to `unbound' upon backtracking. Hence,
we provide, along with the heap, the stack, the code area, and the PDL, a new(and last!) data area called the

trail (TRAIL). This trail is organized as an array

PAGE 36 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
of addresses of those (stack or heap) variables which must be reset to `unbound'upon backtracking. Note that it also works as a stack, and we need a new global
register TR always set to contain the top of the trail.
It is important to remark that not all bindings need to be remembered in the trail.Only

conditional bindings do. A conditional binding is one affecting a variableexisting before creation of the current choice point. To determine this, we will use

a new global register HB set to contain the value of H at the time of the latest choicepoint.

2 Hence only bindings of heap (resp., stack) variables whose addresses are

less than HB (resp., B) need be recorded in the trail. We shall write trail(a) whenthat this operation is performed on store address

a. As mentioned before, it isdone as part of the
bind operation.

Let us now think about what constitutes a computation state to be saved in a choicepoint frame. Upon backtracking, the following information is needed:

fflThe argument registers A1, ..., An, where n is the arity of the procedure offeringalternative choices of definitions. This is clearly needed as the argument registers,
loaded by put instructions with the values of arguments necessary for goal beingattempted, are overwritten by executing the chosen clause.
fflThe current environment (value of register E), to recover as a protected environ-ment as explained above.
fflThe continuation pointer (value of register CP), as the current choice will over-write it.
fflThe latest choice point (value of register B), where to backtrack in case all alter-natives offered by the current choice point fail. This acts as the link connecting
choice points as a list. It is reinstated as the value of the B register upon discardingthe choice point.
fflThe next clause, to try in this definition in case the currently chosen one fails.
This slot is updated at each backtracking to this choice point if more alternativesexist.

fflThe current trail pointer (value of register TR), which is needed as the boundarywhere to

unwind the trail upon backtracking. If computation comes back to thischoice point, this will be the address in the trail down to which all variables that

must be reset have been recorded.
fflThe current top of heap (value of register H), which is needed to recover (garbage)heap space of all the structures and variables constructed during the failed attempt

2Strictly speaking, register HB can in fact be dispensed with since, as we see next, its value is
that of H which will have been saved in the latest choice point frame.

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 37 OF 129

WARREN'S ABSTRACT MACHINE
which will have resulted in coming back to this choice point.
In summary, a choice point frame is allocated on the stack looking thus:3

B n (number of arguments)
B + 1 A1 (argument register 1).

..

B + n An (argument register n)
B + n + 1 CE (continuation environment)
B + n + 2 CP (continuation pointer)
B + n + 3 B (previous choice point)
B + n + 4 BP (next clause)
B + n + 5 TR (trail pointer)
B + n + 6 H (heap pointer)

Note in passing that M2's explicit definition for allocate N must be altered inorder to work for

M3. This is because the top of stack is now computed differentlydepending on whether an environment or choice point is the latest frame on the

stack. Namely, in M3:

allocate N j if E ? Bthen

newE  E + STACK[E + 2] + 3else
newE  B + STACK[B] + 7;
STACK[newE]  E;
STACK[newE + 1]  CP;
STACK[newE + 2]  N ;
E  newE;
P  P + instruction size(P);

To work with the foregoing choice point format, three new I3 instructions areadded to those already in

I2. They are to deal with the choice point manipulation

3In [War83], David Warren does not include the arity in a choice point, as we do here. He sets
up things slightly differently so that this number can always be quickly computed. He can do thisby making register

B (and the pointers linking the choice point list) reference a choice point frameat its
end, rather than its start as is the case for environment frames. In other words, register Bcontains the stack address immediately following the latest choice point frame, whereas register

E contains the address of the first slot in the environment. Thus, the arity of the latest choicepoint predicate is always given by

n = B \Gamma  STACK[B \Gamma  4] \Gamma  6. For didactic reasons, we choseto handle
E and B identically, judging that saving one stack slot is not really worth the entailedcomplication of the code implementing the instructions.

PAGE 38 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
needed for multiple clause definitions. As expected, these instructions correspond,respectively, to (1) a first, (2) an intermediate (but non ultimate), and (3) a last,
clause of a definition. They are:

1. try me else L
2. retry me else L
3. trust me

where L is an instruction label (i.e., an address in the code area). They have foreffect, respectively:

1. to allocate a new choice point frame on the stack setting its next clause field

to L and the other fields according to the current context, and set B to pointto it;

2. having backtracked to the current choice point (indicated by the currentvalue of the

B register), to reset all the necessary information from it andupdate its next clause field to

L; and,

3. having backtracked to the current choice point, to reset all the necessaryinformation from it, then discard it by resetting

B to its predecessor (thevalue of the link slot).

With this setup, backtracking is effectively handled quite easily. All instructionsin which failure may occur (

i.e., some unification instructions and all procedurecalls) must ultimately test whether failure has indeed occurred. If such is the case,

they must then set the instruction counter accordingly. That is, they perform thefollowing operation:

backtrack j P  STACK[B + STACK[B] + 4];
as opposed to having P be unconditionally set to follow its normal (successful)course. Naturally, if no more choice point exists on the stack, this is a terminal
failure and execution aborts. All the appropriate alterations of instructions regard-ing this precaution are given in Appendix B.

The three choice point instructions are defined explicitly in Figures 4.1, 4.2, and 4.3,respectively. In the definition of

try me else L, we use a global variable

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 39 OF 129

WARREN'S ABSTRACT MACHINE
num of args giving the arity of the current procedure. This variable is set by
call that we must accordingly modify for M3 from its M2 form as follows:4

call p=n j CP  P + instruction size(P);

num of args  n;
P  @(p=n);

As we just explained, we omit treating the case of failure (and therefore of back-tracking) where

p=n is not defined in this explicit definition of call p=n. Its
obvious complete form is, as those of all instructions of the full WAM, given inAppendix B.

Finally, the definitions of retry me else L and trust me, use an ancillaryoperation,

unwind trail, to reset all variables since the last choice point to an un-bound state. Its explicit definition can be found in Appendix B.

In conclusion, there are three patterns of code translations for a procedure defini-tion in

L3, depending on whether it has one, two, or more than two clauses. Thecode generated in the first case is identical to what is generated for an

L2 programon
M2. In the second case, the pattern for a procedure p=n is:

p=n : try me else L

code for first clause
L : trust me

code for second clause

and for the last case:

4As for num of args, it is legitimate to ask why this is not a global register like E, P, etc.,
in the design. In fact, the exact manner in which the number of arguments is retrieved at choicepoint creation time is not at all explained in [War83, War88]. Moreover, upon private inquiry,

David H. D. Warren could not remember whether that was an incidental omission. So we chose tointroduce this global variable as opposed to a register as no such explicit register was specified for
the original WAM.

PAGE 40 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

try me else L j if E ? Bthen

newB  E + STACK[E + 2] + 3else
newB  B + STACK[B] + 7;
STACK[newB]  num of args;
n  STACK[newB];for

i  1 to n do STACK[newB + i]  Ai;
STACK[newB + n + 1]  E;
STACK[newB + n + 2]  CP;
STACK[newB + n + 3]  B;
STACK[newB + n + 4]  L;
STACK[newB + n + 5]  TR;
STACK[newB + n + 6]  H;
B  newB;
HB  H;
P  P + instruction size(P);

Figure 4.1: M3 choice point instruction try me else

retry me else L j n  STACK[B];for

i  1 to n do Ai  STACK[B + i];
E  STACK[B + n + 1];
CP  STACK[B + n + 2];
STACK[B + n + 4]  L;
unwind trail(STACK[B + n + 5]; TR);
TR  STACK[B + n + 5];
H  STACK[B + n + 6];
HB  H;
P  P + instruction size(P);

Figure 4.2: M3 choice point instruction retry me else
Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 41 OF 129

WARREN'S ABSTRACT MACHINE
trust me j n  STACK[B];for

i  1 to n do Ai  STACK[B + i];
E  STACK[B + n + 1];
CP  STACK[B + n + 2];
unwind trail(STACK[B + n + 5]; TR);
TR  STACK[B + n + 5];
H  STACK[B + n + 6];
B  STACK[B + n + 3];
HB  STACK[B + n + 6];
P  P + instruction size(P);

Figure 4.3: M3 choice point instruction trust me
p=n : try me else L1

code for first clause
L1 : retry me else L2

code for second clause.

..

Lk\Gamma 1 : retry me else Lk

code for penultimate clause
Lk : trust me

code for last clause

where each clause is translated as it would be as a single L2 clause for M2. Forexample,

M3 code for the definition:

p(X; a):
p(b; X):
p(X; Y ) :- p(X; a); p(b; Y ):

is given in Figure 4.4.

Exercise 4.1 Trace the execution of L3 query ?-p(c; d) with code in Figure 4.4,
giving all the successive states of the stack, the heap, and the trail.

PAGE 42 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

p=2 : try me else L1 % p

get variable X3; A1 % (X;
get structure a=0; A2 % a)
proceed % :

L1 : retry me else L2 % p

get structure b=0; A1 % (b;
get variable X3; A2 % X)
proceed % :

L2 : trust me %

allocate 1 % p
get variable X3; A1 % (X;
get variable Y1; A2 % Y ) :-
put value X3; A1 % p(X;
put structure a=0; A2 % a
call p=2 % );
put structure b=0; A1 % p(b;
put value Y1; A2 % Y
call p=2 % )
deallocate % :

Figure 4.4: M3 code for a multiple clause definition

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 43 OF 129

WARREN'S ABSTRACT MACHINE
Exercise 4.2 It is possible to maintain separate AND-stack and OR-stack. Discuss
the alterations that would be needed to the foregoing setup to do so, ensuring a
correct management of environments and choice points.

PAGE 44 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI
Chapter 5
Optimizing the Design
Now that the reader is hopefully convinced that the design we have reached forms
an adequate target language and architecture for compiling pure Prolog, we canbegin transforming it in order to recover Warren's machine as an ultimate design.

Therefore, since all optimizations considered here are part of the definitive design,we shall now refer to the abstract machine gradually being elaborated as the

WAM.In the process, we shall abide by a few principles of design pervasively motivating

all the conception features of the WAM. We will repeatedly invoke these principlesin design decisions as we progress toward the full

WAM engine, as more evidencejustifying them accrues.

WAM PRINCIPLE 1 Heap space is to be used as sparingly as possible, as terms
built on the heap turn out to be relatively persistent.

WAM PRINCIPLE 2 Registers must be allocated in such a way as to avoid unnecessary data movement, and minimize code size as well.

WAM PRINCIPLE 3 Particular situations that occur very often, even though correctly handled by general-case instructions, are to be accommodated by special
ones if space and/or time may be saved thanks to their specificity.

In the light of WAM Principles 1, 2, and 3, we may now improve on M3.

45

WARREN'S ABSTRACT MACHINE
0 h=2
1 REF 1
2 REF 2
3 f =1
4 REF 2
5 p=3
6 REF 1
7 STR 0
8 STR 3

Figure 5.1: Better heap representation for term p(Z; h(Z; W ); f (W ))
5.1 Heap representation
As many readers of [AK90] did, this reader may have wondered about the necessity of the extra level of indirection systematically introduced in the heap byan

STR cell for each functor symbol. In particular, Fernando Pereira [Per90]suggested that instead of that shown in Figure 2.1 on Page 11, a more economical

heap representation for p(Z; h(Z; W ); f (W )) ought to be that of Figure 5.1, wherereference to the term from elsewhere must be from a store (or register) cell of the
form h STR ; 5 i. In other words, there is actually no need to allot a systematic STRcell before each functor cell.

As it turns out, only one tiny modification of one instruction is needed in order toaccommodate this more compact representation. Namely, the

put structureinstruction is simplified to:

put structure f =n; Xi j HEAP[H]  f =n;

Xi  h STR ; H i;
H  H + 1;

Clearly, this is not only in complete congruence with WAM Principle 1, but it alsoeliminates unnecessary levels of indirection and hence speeds up dereferencing.
The main reason for our not having used this better heap representation in Sec-tion 2.1 was essentially didactic, wishing to avoid having to mention references
from outside the heap (e.g., from registers) before due time. In addition, we didnot bother bringing up this optimization in [AK90] as we are doing here, as we

PAGE 46 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
had not realized that so little was in fact needed to incorporate it.1

5.2 Constants, lists, and anonymous variables
To be fully consistent with the complete WAM unification instruction set and inaccordance with WAM Principle 3, we introduce special instructions for the specific handling of 0-ary structures (i.e., constants), lists, and variables which appearonly once within a scope--so-called

anonymous variables. These enhancements
will also be in the spirit of WAM Principles 1 and 2 as savings in heap space, codesize, and data movement will ensue.

Constants and lists are, of course, well handled by the structure oriented get,
put, and unify instructions. However, work and space are wasted in the pro-cess, that need not really be. Consider the case of constants as, for instance, the

code in Figure 2.10, on Page 24. There, the sequence of instructions:

unify variable X7
get structure a=0; X7

simply binds a register and proceeds to check the presence of, or build, the con-stant

a on the heap. Clearly, one register can be saved and data movement opti-mized with one specialized instruction:

unify constant a. The same situa1After dire reflection seeded by discussions with Fernando Pereira, we eventually realized that
this optimization was indeed cheap--a fact that had escaped our attention. We are grateful to himfor pointing this out. However, he himself warns [Per90]:

"Now, this representation (which, I believe, is the one used by Quintus, SICStusProlog,

etc.) has indeed some disadvantages:

1. If there aren't enough tags to distinguish functor cells from the other cells,garbage collection becomes trickier, because a pointed-to value does not in

general identify its own type (only the pointer does).
2. If you want to use [the Huet-Fages] circular term unification algorithm, redi-recting pointers becomes messy, for the [same] reason...

In fact, what [the term representation in Section 2.1 is] doing is enforcing a con-vention that makes every functor application tagged as such by the appearance of a
STR cell just before the functor word."

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 47 OF 129

WARREN'S ABSTRACT MACHINE
tion in a query would simplify a sequence:

put structure c=0; Xi
set variable Xi

into one specialized instruction set constant c. Similarly, put and get in-structions can thus be specialized from those of structures to deal specifically with

constants. Thus, we define a new sort of data cells tagged CON, indicating that thecell's datum is a constant. For example, a heap representation starting at address
10 for the structure f (b; g(a)) could be:

8 g=1
9 CON a
10 f =2
11 CON b
12 STR 8

Exercise 5.1 Could the following (smaller) heap representation starting at address 10 be an alternative for the structure f (b; g(a))? Why?

10 f =2
11 CON b
12 g=1
13 CON a

Heap space for constants can also be saved when loading a register with, or bind-ing a variable to, a constant. Rather than systematically occupying a heap cell
to reference, a constant can be simply assigned as a literal value. The followinginstructions are thus added to

I0:

1. put constant c; Xi
2. get constant c; Xi
3. set constant c
4. unify constant c

and are explicitly defined in Figure 5.2.
Programming with linear lists being so privileged in Prolog, it makes sense totailor the design for this specific structure. In particular, non-empty list functors

PAGE 48 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

put constant c; Xi j Xi  h CON ; c i;
get constant c; Xi j addr  deref (Xi);case

STORE[addr] of
h REF ; i : STORE[addr]  h CON ; c i;

trail(addr);
h CON ; c0 i : fail  (c 6= c0);other

: fail  true;endcase
;

set constant c j HEAP[H]  h CON ; c i;

H  H + 1;

unify constant c j case mode of

read : addr  deref (S);case

STORE[addr] of
(\Delta  \Delta  \Delta )endcase;
write : HEAP[H]  h CON ; c i;

H  H + 1;endcase
;

Figure 5.2: Specialized instructions for constants

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 49 OF 129

WARREN'S ABSTRACT MACHINE
put list Xi j Xi  h LIS ; H i;
get list Xi j addr  deref (Xi);case

STORE[addr] of
h REF ; i : HEAP[H]  h LIS ; H + 1 i;

bind(addr; H);
H  H + 1;
mode  write;
h LIS ; a i : S  a;

mode  read;other
: fail  true;endcase
;

Figure 5.3: Specialized instructions for lists
need not be represented explicitly on the heap. Thus again, we define a fourth sortfor heap cells tagged

LIS, indicating that the cell's datum is the heap address of
the first element of a list pair. Clearly, to respect the subterm contiguity conven-tion, the second of the pair is always at the address following that of the first. The

following instructions (defined explicitly in Figure 5.3) are thus added to I0:

1. put list Xi
2. get list Xi

For example, the code generated for query ?-p(Z; [Z; W ]; f (W )):, using Prolog'snotation for lists, is shown in Figure 5.4 and that for fact

p(f (X); [Y; f (a)]; Y ):,in Figure 5.5. Note the hidden presence of the atom
[] as list terminator.

Of course, having introduced specially tagged data cells for constants and non-empty lists will require adapting accordingly the general-purpose unification algorithm given in Figure 2.7. The reader will find the complete algorithm in appendix
Section B.2, on Page 117.

Exercise 5.2 In [War83], Warren also uses special instructions put nil Xi, get nil Xi,
and to handle the list terminator constant ([]). Define the effect of these instrucPAGE 50 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

put list X5 % ?-X5 = [
set variable X6 % W j
set constant [] % []];
put variable X4; A1 % p(Z;
put list A2 % [
set value X4 % Zj
set value X5 % X5];
put structure f =1; A3 % f
set value X6 % (W )
call p=3 % ):

Figure 5.4: Specialized code for query ?-p(Z; [Z; W ]; f (W )):

p=3 : get structure f =1; A1 % p(f

unify variable X4 % (X);
get list A2 % [
unify variable X5 % Y j
unify variable X6 % X6];
get value X5; A3 % Y );
get list X6 % X6 = [
unify variable X7 % X7j
unify constant [] % []];
get structure f =1; X7 % X7 = f
unify constant a % (a)
proceed % :

Figure 5.5: Specialized code for fact p(f (X); [Y; f (a)]; Y ):
Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 51 OF 129

WARREN'S ABSTRACT MACHINE
tions, and give explicit pseudo-code implementing them. Discuss their worth being provided as opposed to using put constant []; Xi, get constant []; Xi,
set constant [], and unify constant [].

Last in the rubric of specialized instructions is the case of single-occurrence vari-ables in non-argument positions (

e.g., X in Figure 2.4 on Page 16, Figure 2.10 on
Page 24, and Figure 5.5 on Page 51). This is worth giving specialized treatmentinsofar as no register need be allocated for these. In addition, if many occur in

a row as in f ( ; ; ), say, they can be all be processed in one swoop, saving ingenerated code size and time. We introduce two new instructions:

1. set void n
2. unify void n
whose effect is, respectively:

1. to push n new unbound REF cells on the heap;
2. in write mode, to behave as set void n and, in read mode, to skip thenext

n heap cells starting at location S.

These are given explicitly in Figure 5.6.
Note finally, that an anonymous variable occurring as an argument of the head ofa clause can be simply ignored. Then indeed, the corresponding instruction:

get variable Xi; Ai
is clearly vacuous. Thus, such instructions are simply eliminated. The code forfact

p( ; g(X); f ( ; Y; )):, for example, shown in Figure 5.7, illustrates this point.

Exercise 5.3 What is the machine code generated for the fact p( ; ; ):? What
about the query ?-p( ; ; ):?

5.3 A note on set instructions
Defining the simplistic language L0 has allowed us to introduce, independentlyof other Prolog considerations, all WAM instructions dealing with unification.

PAGE 52 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

set void n j for i  H to H + n \Gamma  1 do

HEAP[i]  h REF ; i i;
H  H + n;

unify void n j case mode of

read : S  S + n;
write : for i  H to H + n \Gamma  1 do

HEAP[i]  h REF ; i i;
H  H + n;endcase

Figure 5.6: Anonymous variable instructions

p=3 : get structure g=1; A2 % p( ; g

unify void 1 % (X);
get structure f =3; A3 % f
unify void 3 % ( ; Y; )
proceed % ):

Figure 5.7: Instructions for fact p( ; g(X); f ( ; Y; )):

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 53 OF 129

WARREN'S ABSTRACT MACHINE
Strictly speaking, the set instructions we have defined are not part of the WAMas described in [War83] or in [War88]. There, one will find that the corresponding unify instructions are systematically used where we use set instructions.The reason is, as the reader may have noticed, that indeed this is possible provided that the put structure and put list instructions set mode to write.Then, clearly, all

set instructions are equivalent to unify instructions in writemode. We chose to keep these separate as using

set instructions after put in-structions is more efficient (it saves mode setting and testing) and makes the code

more perspicuous. Moreover, these instructions are more natural, easier to ex-plain and motivate as the data building phase of unification before matching work
comes into play.
Incidentally, these instructions together with their unify homologues, make "onthe-fly" copying part of unification, resulting in improved space and time con-sumption, as opposed to the more

na"ive systematic copying of rules before usingthem.

5.4 Register allocation
As in conventional compiler technology, the code generated from the source maygive rise to obviously unnecessary data movements. Such can be simplified away
by so-called "peep-hole" optimization. This applies to this design as well. Con-sider for example the

na"ive translation of the fact `conc([]; L; L):':

conc=3 : get constant []; A1 % conc([];

get variable X4; A2 % L;
get value X4; A3 % L)
proceed % :

Now, there is clearly no need to allocate register X4 for variable L since its onlyuse is to serve as temporary repository--but so can

A2. Thus, the get variable
becomes get variable A2; A2, and can be eliminated altogether, yielding bet-ter code:

conc=3 : get constant []; A1 % conc([];

get value A2; A3 % L; L)
proceed % :

More generally, since argument and temporary variable registers are the same, the

PAGE 54 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

p=2 : allocate 2 % p

get variable Y1; A2 % (X; Y ) :-
put variable Y2; A2 % q(X; Z
call q=2 % );
put value Y2; A1 % r(Z;
put value Y1; A2 % Y
call r=2 % )
deallocate % :

Figure 5.8: Better register use for p(X; Y ) :- q(X; Z); r(Z; Y ):
following instructions are vacuous operations:

get variable Xi; Ai
put value Xi; Ai

and can be eliminated. For example, looking back at the example shown in Figure 3.1 on Page 31, we realize that the temporary variable X is the first argumentin the head as well as the first atom in the body. Therefore, allocating register

X3to the variable
X is clearly silly as it has for consequence the useless movementof the contents of register

A1 to X3, then back, as well as two more instructionsincreasing the code size. Thus, with this observation, it makes sense to allocate

register A1 to X and apply the above vacuous operation elimination, resulting inthe obviously better instruction sequence shown in Figure 5.8.

Register allocation must try to take advantage of this fact by recognizing situationswhen appropriate argument registers may also safely be used as temporary variables. Algorithms that do this well can be quite involved. A general method dueto Debray [Deb86] works well in reasonable time. A more sophisticated but more
(compile-time) expensive technique using Debray's method combined with a re-ordering of unification instructions can be found in [JDM88]. Register allocation
is really auxiliary to the WAM design and can be performed by an independentmodule in the compiler.

In the sequel, we shall implicitly use this optimization whenever better than na"iveregister allocation can be obviously inferred by the reader.

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 55 OF 129

WARREN'S ABSTRACT MACHINE
5.5 Last call optimization
The refinement that we introduce next is a generalization of tail-recursion optimization, the effect of which is to turn some recursive procedures into equivalentiterative forms. It is called here

last call optimization (LCO), as it is applied sys-tematically with or without recursion. If the last procedure call happens to be

recursive, then it does amount to tail recursion optimization. However, it is moregeneral as a stack frame recovery process.

The essence of LCO resides in the fact that permanent variables allocated to a ruleshould no longer be needed by the time all the

put instructions preceding the last
call in the body are passed. Hence, it is safe to discard the current environment
before calling the last procedure of the rule's body. This could be achieved quitesimply by swapping the

call, deallocate sequence that always conclude arule's instruction sequence (

i.e., into deallocate, call).

A consequence of this is that deallocate is never the last instruction in a rule'sinstruction sequence as it used to be for

M2 and M3. Therefore, it must bemodified accordingly. Namely, it must reset

CP, rather than P, to the value of the
continuation slot of the current environment being discarded, and set P to continuein sequence. Thus,

deallocate j CP  STACK[E + 1];

E  STACK[E];
P  P + instruction size(P)

But then, call being now the last instruction, there is no need for it to set CP.As a matter of fact, it would be wrong if it did since the right continuation will
now have been set a priori by deallocate. A simple setting of P to the callee's
address is all that is needed. We shall not modify call, since it works correctlyfor non-ultimate procedure calls. Rather, we introduce

execute p=n, definedas:

execute p=n j num of args  n;

P  @(p=n);

to be used systematically for the last call in a rule instead of call . To see
an example, consider the rule `p(X; Y ) :- q(X; Z); r(Z; Y ):' whose "last calloptimized" code is shown in Figure 5.9.

The effect of LCO is subtler than it first appears due to the interleaving of envi-ronment and choice point frames on the same stack. Thus, if the topmost frame

PAGE 56 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

p=2 : allocate 2 % p

get variable Y1; A2 % (X; Y ) :-
put variable Y2; A2 % q(X; Z
call q=2 % );
put value Y2; A1 % r(Z;
put value Y1; A2 % Y
deallocate % )
execute r=2 % :

Figure 5.9: M2 code for p(X; Y ) :- q(X; Z); r(Z; Y ):, with LCO
on the stack is the current environment and not the current choice point (i.e., if
E ? B), its space can then be re-used in the next stack allocation (e.g., allocateor

try me else). This slows downs growth of the stack considerably. On theother hand, if the top of the stack is a choice point, LCO does not have immediate

effect on the stack due to environment protection. In the case where the last call
of the last rule is a recursive call, the stack does not grow at all, re-using over andover the exact same space for successive activation frames of the same procedure,

resulting in an iterative loop.2

5.6 Chain rules
A consequence of LCO is that the generated code translating chain rules can begreatly simplified. Indeed, the generic translation of a chain rule of the form
`p(: : :) :- q(: : :):':

2In pure L

2, this would of course be of little interest since any recursive program would alwayseither fail or loop indefinitely anyway--albeit with a small stack! At any rate, LCO is nonetheless

an interesting optimization of execution of (non-recursive) L2 programs as it keeps stack spacewell utilized.

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 57 OF 129

WARREN'S ABSTRACT MACHINE
p : allocate N

get arguments of p
put arguments of q
call q
deallocate

is transformed by LCO into:

p : allocate N

get arguments of p
put arguments of q
deallocate
execute q

Now, note that all variables in a chain rule are necessarily temporary. Hence, theonly information which is saved on the stack by an initial

allocate is the con-tinuation register
CP. But this effect of allocate is undone before executeby
deallocate. Therefore, this is totally wasted work, and both allocate
and deallocate can be eliminated. Thus, LCO allows translation of the chainrule `

p(: : :) :- q(: : :):' simply into:

p : get arguments of p

put arguments of q
execute q

That is, chain rules need no run-time activation frame on the stack at all!

5.7 Environment trimming
The correctness of LCO hinges on having observed that permanent variables in
the current environment are needed only as long as all the put instructions forthe last call's arguments are not yet done. This observation can be sharpened by

noticing that a permanent variable is in fact no longer needed after the argumentsof its ultimate occurrence's goal have all been loaded by

put instructions. Thisentails a natural generalization of LCO to allow maximal reuse of stack space at

each (i.e., not only the last) call in the body of the rule. More specifically, eachpermanent variable in the environment can be associated with the goal in which

PAGE 58 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
it is used last, and therefore can safely be disposed of before performing the call.The intended effect of such a process is to make the current environment frame's
size on the stack shrink gradually, until it eventually vanishes altogether by LCO,this latter optimization being simply the special case of the last goal.

This gradual environment trimming can be made to work automatically by care-fully ordering the variables in their environment so as to reflect the ordering of
their associated last occurrence goals. Namely, the later a permanent variable'slast occurrence's goal is in the body, the lower its offset in the current environment frame is. Thus, the call instruction is given a second argument countingthe number of variables still needed in the environment after the point of call. This
count allows later stack allocating instructions to compute a lower top of stack, ifpossible. Namely, if the topmost frame on the stack is the current environment
(i.e., if E ? B).
Note that the explicit definition of allocate again needs to be changed from
what it was for M3. In order to reflect a correct value at all times, the offset that itgets from the preceding environment must be updated by each trimming

call. Infact, such updates are not needed. Since a more precise environment stack offset is

now explicitly passed as an argument to call's, the argument of allocate be-comes superfluous. Indeed, the offset can be dynamically retrieved by

allocate
(and try me else) as a consequence of the following fact: the continuation slot
of the latest environment frame, STACK[E + 1], always contains the address
of the instruction immediately following the appropriate call P; N instruction
where N is precisely the desired offset. Hence, allocate no longer takes anargument, and an environment frame no longer needs an offset slot. Instead, the

right offset is calculated by allocate as CODE[STACK[E + 1] \Gamma  1].3
With this simplification, an environment frame on top of the stack now looks thus:

E CE (continuation environment)
E + 1 CP (continuation point)
E + 2 Y1 (permanent variable 1).

..

3Recall that by CODE[i], we mean the contents of STORE[i] in the code area at address i.
This works under the (reasonable) assumption that an integer occupies one memory word.

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 59 OF 129

WARREN'S ABSTRACT MACHINE
and the (now definitive) definition of allocate is:4

allocate j if E ? Bthen

newE  E + CODE[STACK[E + 1] \Gamma  1] + 2else
newE  B + STACK[B] + 7;
STACK[newE]  E;
STACK[newE + 1]  CP;
E  newE;
P  P + instruction size(P);

The rule `p(X; Y; Z) :- q(U; V; W ); r(Y; Z; U ); s(U; W ); t(X; V ):', for example,
is one in which all variables are permanent. The last occurrence's goal of eachvariable is given in the following table, along with a consistent ordering assigning

to each a Yi indexed by its offset in the environment frame:

Variable Last goal Offset

X t Y1

Y r Y5

Z r Y6
U s Y3
V t Y2
W s Y4

That is, after the CE and CP slots, X; V; U; W; Y; Z come in this order in the envi-ronment. Environment trimming code for this rule is shown in Figure 5.10.

5.8 Stack variables
Recall that, according to WAM Principle 1, allocation of heap space is to beavoided whenever possible. Thus, we may go even farther in optimizing austerity in the case of a permanent variable which first occurs in the body of a ruleas a goal argument. From what we have seen, such a variable

Yn is initialized witha
put variable Yn; Ai which sets both the environment slot Yn and argumentregister

Ai to point to a newly allocated unbound REF heap cell (see Figure 2.8, onPage 23). Now, since

Yn is to be treated as a local variable, it has been allocated

4Note incidentally, that a similar alteration must be done for try me else. The definitive
version for that instruction is given in Appendix B.

PAGE 60 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

p=3 : allocate % p

get variable Y1; A1 % (X;
get variable Y5; A2 % Y;
get variable Y6; A3 % Z) :-
put variable Y3; A1 % q(U;
put variable Y2; A2 % V;
put variable Y4; A3 % W
call q=3; 6 % );
put value Y5; A1 % r(Y;
put value Y6; A2 % Z;
put value Y3; A3 % U
call r=3; 4 % );
put value Y3; A1 % s(U;
put value Y4; A2 % W
call s=2; 2 % );
put value Y1; A1 % t(X;
put value Y2; A2 % V
deallocate % )
execute t=2 % :

Figure 5.10: Environment trimming code

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 61 OF 129

WARREN'S ABSTRACT MACHINE
a cell in the environment which is to exist during this rule's body execution--assuming, for the time being, that environment trimming or LCO are not performed. So why not save, rather than systematically waste, that (global) heapcell? More specifically, a more appropriate semantics for the

put variable
instruction, when used on a permanent variable, ought to be:

put variable Yn; Ai j addr  E + n + 1;

STACK[addr]  h REF ; addr i;
Ai  STACK[addr];

That is, it should not allocate a heap cell as done for a temporary register.
Unfortunately, there are rather insidious consequences to this apparently innocu-ous change as it interferes with environment trimming and LCO. The trouble then

is that environment variables may be disposed of while still unbound. Therefore,any reference to an unbound stack variable runs the risk of potential catastrophe,
becoming a dangling reference upon careless discarding of the variable. As a re-sult, it is no longer correct to let the

bind operation set an arbitrary direction whenestablishing a reference between two unbound variables. More pathologically, the

following instructions have now become incorrect if used blindly in some situa-tions:

put value and set value (thus also unify value in write mode).

The following three subsections treat each problem by (1) first giving a correctbinding convention, then (2) analyzing what may go wrong with

put value, and(3) with
set value and unify value, explaining how to repair the trouble.

5.8.1 Variable binding and memory layout
Three cases of variable-variable bindings are possible: (1) heap-heap, (2) stackstack, and (3) heap-stack. In Case (1), as alluded to before on Page 17, when the
bind operation is performed on two unbound (heap) addresses, which of the twois made to reference the other does not affect correctness. However, making an

arbitrary choice does affect performance as it may lead to much more work thannecessary by causing more variable trailing and resetting than may be needed.
Consider for example two heap variables, one before HB and the other after HB.
Making the first reference the second requires trailing it (and its potential subse-quent resetting) while the contrary is unconditional and requires none.

In Cases (2) and (3), the symptoms are quite more serious as which directionthe binding occurs can be incorrect due to potential discarding of stack variables.

PAGE 62 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
For example, the rule `p(X; X) :- q(X); r(X):' invoked with `?-p(Y; Z):' willnot work correctly if

Y is bound to X since q=1 may leave the stack variable Xunbound. The other direction is the only correct one. As it turns out, most correct

bindings can be ensured following a simple chronological reference rule:

WAM BINDING RULE 1 Always make the variable of higher address reference
that of lower address.

In other words, an older (less recently created) variable cannot reference a younger(more recently created) variable.

Let us examine what is gained. In Case (1), as explained above, unconditionalbindings are thus favored over conditional ones, avoiding unnecessary trailing
and resulting in swift heap space recovery upon backtracking.
In Case (2), WAM Binding Rule 1 rule is also clearly beneficial for the same rea-sons as for Case (1). It happens to be also consistent with the ordering among

variables within a single environment set up to allow environment trimming. Thisis all the better. Unfortunately, this rule is not sufficient to prevent dangling references in a stack-stack binding as will be seen in the next subsection.
In Case (3), the problem (as exposed in the next two subsections) is that stack
space is volatile while heap space is persistent, making references to the stackpotentially dangerous. Clearly, it would be a source of complication ever to establish a binding from the heap toward the stack, whereas the contrary presents noproblem. Therefore, the WAM enforces the following:

WAM BINDING RULE 2 Heap variables must never be set to a reference into the
stack.

To suit this, the WAM organizes its memory layout specifically so that WAMBinding Rule 1 is naturally consistent with:

WAM BINDING RULE 3 The stack must be allocated at higher addresses than
the heap, in the same global address space.

This must be done, of course, allowing sufficient slack for growth of the heap.This rule entails forbidding the participation of stack variables in a reference chain
in any way other than grouped as a subchain prefix. That is, a reference chaincontaining any stack variables at all will have them all appear contiguously and

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 63 OF 129

WARREN'S ABSTRACT MACHINE
h 0 i p=1 : allocate % p
h 1 i get variable Y1; A1 % (X) :-
h 2 i put variable Y2; A1 % q(Y;
h 3 i put value Y1; A2 % X
h 4 i call q=2; 2 % );
h 5 i put value Y2; A1 % r(Y;
h 6 i put value Y1; A2 % X
h 7 i deallocate % )
h 8 i execute r=2 % :

Figure 5.11: Unsafe code for p(X) :- q(Y; X); r(Y; X):
early in the chain. Then, discarding a stack variable cannot break a chain. (Thisis guaranteed in the subchain prefix of stack variables by WAM Binding Rule 1.)
However, we see next that this rule is violated by some instructions (put value,
set value, and unify value). We presently examine this and adapt the de-sign so that no incorrect binding may ever occur.

5.8.2 Unsafe variables
A stack variable is discarded before calling the goal in which it last occurs al-though it may still be unbound or bound to another unbound permanent variable in
the same (current) environment (i.e., one which is to be also disposed of). Clearly,the danger is then that the call may refer to the discarded variables. For this reason, a permanent variable which is initialized by a put variable (i.e., whichfirst occurs as the argument of a body goal) is called an

unsafe variable.

Let us take an example with the rule `p(X) :- q(Y; X); r(Y; X):' in which both
X and Y are permanent variables, but only Y is unsafe. This is because Y is ini-tialized with a

put variable (since its first occurrence is in a body goal) while
X, first occurring in the head, is initialized with a get variable. Figure 5.11
shows the (incorrect) translation as it is done in our current setting. Let us tracewhat happens when

put value Y2; A1 is used on Line 5. Let us assume that p

PAGE 64 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
is called with an unbound variable; that is, with the sequence of the form:

put variable Xi; A1
execute p=1

Thus, at the point right before Line 0, A1 points to the heap address (say, 36)of an unbound

REF cell at the top of the heap. Then, allocate creates an
environment on the stack (where, say, Y1 is at address 77 and Y2 at address 78in the stack). Line 1 sets

STACK[77] to h REF ; 36 i, and Line 2 sets A1 (and
STACK[78]) to h REF ; 78 i. Line 3 sets A2 to the value of STACK[77]; thatis,

h REF ; 36 i. Let us assume that the call to q on Line 4 does not affect thesesettings at all (

e.g., the fact q( ; ) is defined). Then, (the wrong) Line 5 would set
A1 to h REF ; 78 i, and Line 6 sets A2 to h REF ; 36 i. Next, deallocate throwsaway

STACK[77] and STACK[78]. Suppose now that the code for r starts withan
allocate re-using stack space 77 and 78 then, lo!, the get instructions of rwill find nonsensical data in

A1.

Note however that an unsafe variable's binding can easily be checked at run-timeso that trouble may be averted on the fly by taking appropriate measures only if

needed. Let us reflect on the possible situations of a given unsafe variable Yn in
the last goal where it occurs. There are two cases that will need attention: (1) when
Yn appears only as an argument of its last goal; and, (2) when Yn appears in thatgoal nested in a structure, whether or not it is also an argument. We will treat

later the second case, as it interacts with a more general source of unsafety thatwe shall analyze after treating the first case.

So let us consider for now the case where all occurrences of unsafe Yn are ar-guments of the last goal where

Yn appears. That is, all these correspond to
put value Yn; Ai instructions. As explained, it is desirable to ensure that a
run-time check be carried out verifying that no reference chain leading to Yneventually points to an unbound slot in the environment. The solution is to use a

modification of the effect of put value Yn; Ai to ascertain this. More specif-ically, let us call this modified instruction

put unsafe value Yn; Ai. It be-haves exactly like
put value Yn; Ai if the reference chain from Yn does not
lead to an unbound variable in the current environment. Otherwise, this alteredinstruction binds the referenced stack variable to a new unbound

REF cell pushed

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 65 OF 129

WARREN'S ABSTRACT MACHINE
on the heap, and sets Ai to point to that cell. Explicitly,5

put unsafe value Yn; Ai j addr  deref (E + n + 1);if

addr ! Ethen

Ai  STORE[addr]else

begin

HEAP[H]  h REF ; H i;
bind(addr; H);
Ai  HEAP[H];
H  H + 1end

;

Looking back at the example of Figure 5.11, if Line 5 is not as shown but re-placed with

put unsafe value Y2; A1, then HEAP[37] is created and setto
h REF ; 37 i, STACK[78] and A1 are set to h REF ; 37 i, then A2 is set to
h REF ; 36 i (the value of STACK[77]) on the following line. Discarding STACK[77]
and STACK[78] is now quite safe as executing r will get correct values from A1and

A2.

The question still remaining is to decide which among several occurrences of
put value Yn; Ai must be replaced with the safety check modification for agiven unsafe

Yn. In fact, it is sufficient to replace only one of them, although notan arbitrary one but, quite importantly, the

first occurrence in this last goal. To

5Note incidentally that having a global address space with the relative memory layout of the
stack being allocated at higher addresses than the heap has as a nice consequence to make it quiteeasy to test whether

Yn's value is an unbound variable in the current environment with a merecomparison of memory addresses. Strictly speaking, this test is not quite correct because deref

may actually yield the address of an X register containing a literal constant. To prevent this trivialpoint from complicating matters unnecessarily, we may assume that

X registers conveniently resideat the highest end of the global store.

PAGE 66 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
see this on an example, consider the clause:6

p :- q(X); r(X; X):
If the safety check is done last, as in:

p=0 : allocate

put variable Y1; A1
call q=1; 1
put value Y1; A1
put unsafe value Y1; A2
deallocate
execute r=2

then argument register will still contain a reference to the discarded environment
when r=2 is called. Therefore, the following is the only possible correct code:

p=0 : allocate

put variable Y1; A1
call q=1; 1
put unsafe value Y1; A1
put value Y1; A2
deallocate
execute r=2

It has the effect of "globalizing" the value of Y1 so as to guarantee that it may bediscarded without leaving a nonsensical reference in

A1.

5.8.3 Nested stack references
When an unsafe variable occurs in its last goal nested in a structure (i.e., with acorresponding

set value or a unify value), the situation pertains to a more

6This example is due to Michael Hanus [Han90] and, independently, to Pascal van Hentenryck [vH90]. Both pointed out to this author the incorrect replacement rule described (as that ofthe

last occurrence of an unsafe variable) in [AK90]. In fact, the incorrect rule in [AK90] hadbeen simply inherited verbatim from Warren's original report [War83] (Page 14, Line 3), and later

explained by him as follows [War90]:

"I agree that this is ambiguous or misleading. I think it may be partly explainedby the fact that the memo (tacitly?) assumes that goal arguments are compiled in

reverse order and therefore the last arguments will be compiled first!"

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 67 OF 129

WARREN'S ABSTRACT MACHINE
general pathology which may affect temporary variables as well. Consider therule `

a(X) :- b(f (X)):' for example. As we have it, this is translated thus:

a=1 : get variable X2; A1

put structure f =1; A1
set value X2
execute b=1

Let us consider now the query `?-a(X); c(X):' being translated as:

allocate
put variable Y1; A1
call a=1; 1.

..

and let us examine in detail what happens during execution. Before the call to a=1,a stack frame containing the variable

Y1 is allocated and initialized to unboundby
put variable Y1; A1. The code of a=1 begins by setting X2 to referencethat stack slot (the value of

A1), and pushes the functor f =1 on the heap. Then,behold!,
set value X2 pushes the value of X2 onto the heap, establishing a
reference from the heap to the stack. This violates WAM Binding Rule 2 andcreates a source of disaster when

Y1 is eventually discarded.

Of course, the same ill-fated behavior plagues unify value as its write modesemantics is identical to

set value's. Then, the question is: When can it bestatically guaranteed that

set value (resp., unify value) will not create anunwanted heap-to-stack reference? The answer is: Any time its argument has not

been explicitly initialized to be on the heap in the given clause. Then indeed,
the first set value (resp., unify value) performed on it may cause potentialhavoc. Specifically,

set value Vn (resp., unify value Vn) is unsafe when-ever the variable
Vn has not been initialized in this clause with set variableor
unify variable, nor, if Vn is temporary, with put variable.

Again, the cure amounts to performing appropriate run-time checks which cantrigger dynamic globalizing of a guilty stack variable whenever needed. Namely,

the first set value (resp., unify value) instruction of the clause to be per-formed on a variable which is not guaranteed to lead to the heap (

i.e., meeting the
explicit conditions described in the previous paragraph), must be replaced witha new one,

set local value (resp., unify local value, behaving like

PAGE 68 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
unify value in read mode), which tests whether the dereferenced value re-sides in the heap or in the stack. If the dereferenced value is an unbound heap
address, it behaves as set value (resp., as unify value in write mode);
i.e., it pushes it on the heap. If the dereferenced value is an unbound stack address,
it pushes a new unbound heap cell and binds the stack variable to it. Explicitly,7

set local value Vn j addr  deref (Vn);if

addr ! Hthen

HEAP[H]  HEAP[addr]else

begin

HEAP[H]  h REF ; H i;
bind(addr; H)end

;
H  H + 1;

An explicit expression for unify local value is readily derived from thedefinition of

unify value (given in Figure 2.6, on Page 18) by replacing the
body of the write mode case with the above code.

If set local value X2 replaces set value X2 in the example above, it will
realize that the value of X2 is a stack address and then bind it to a new unboundcell on the heap. This maintains a stack-to-heap reference, and WAM Binding

Rule 2 is respected.
As a final observation, let us consider the particular case where an unsafe vari-able

Yn occurs both as an argument of its last goal and also nested in a structuresomewhere in the same clause. Then, it is only necessary to make the appropriate

change to whichever instruction comes first between the last goal's put value Yn; Ai'sand the clause's

set value Yn's (resp., unify value Yn's). This is because
changing the first such instruction will ensure that the variable is safely globalizedfor the other, making the run-time check unnecessary at that later point.

5.9 Variable classification revisited
At this point, the time is ripe for setting things right about the definition of tem-porary and permanent variables. In our opinion, the way the WAM classifies

7See Footnote 5 at the bottom of Page 66.
Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 69 OF 129

WARREN'S ABSTRACT MACHINE
variables is perhaps the most puzzling item to justify for the learning reader. In-deed, although the definition we have used thus far is correct as given, it is not
exactly that given and used in [War83] or [War88]. We presently analyze this dis-crepancy and discuss in particular the motivation, as well as some rather subtle
consequences, of the original WAM designer's definition.
In fact, our definition and justification of variable classification take David H. D. Warren's actual conception back to front. In his view, a permanent variable is simplya conventional local variable. Therefore,

all variables appearing in a clause are a
priori permanent variables; i.e., local variables to be allocated on the stack. Thisis because a local variable's lifetime ought to be that of the activation of the clause

in which it occurs. On the other hand, allocating a variable on the heap wouldentail making it a global variable insofar as computation does not backtrack to a
previous choice point. However, some variables in a clause need not be allocatedon the stack, either because they are initialized with previously existing data or because they must be part of a structure on the heap. Obviously, these considerationscall for a careful reformulation of variable classification.

Warren's original definition is as follows:

Warren's variable classification A temporary variable is one which
does not occur in more than one body goal (counting the head as part
of the first body goal) and first occurs in the head, or in a structure, or
in the last goal. A permanent variable is one which is not temporary.

First of all, let us observe that both our and Warren's classification consider per-manent any variable occurring in more than one body goal. However, whereas this
criterion is a necessary and sufficient characterization of permanent variables byour definition, it is only a sufficient one for Warren's. Now, from our presentation,
let us try to recover and justify Warren's variable classification.
The point is to restrict our definition of temporary variables (and thus broadenthat of permanent variables) to minimize heap allocation, and consequently the

size of global data. As we saw, local variables are quite thriftily managed byenvironment trimming and LCO, and therefore offer a preferable alternative to
X registers whenever possible. Therefore, in order to abide by Warren's view, we
must completely change our perspective and consider that a variable is permanent
by default unless it is required to be explicitly allocated on the heap.

The question now, is: When does a non-void variable originally deemed tempo-rary in our definition (

i.e., occurring in no more than one body goal) really require

PAGE 70 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
to be systematically allocated in the heap? Memory allocation for a variable hap-pens, if at all, at its first occurrence in the clause. Let us now analyze all possible
situations of first occurrence.
If the first occurrence is in the head of the rule, then the variable is either an ar-gument, and therefore bound to an already existing cell on the heap, or deeper in

the stack; or, it is nested in a structure and hence will necessarily be allocated on
the heap by the unification instructions. This is the case because the convention isthat a functor cell on the heap must be followed by as many cells as its arity dictates. As a result, a head variable never requires stack space allocation. Therefore,it is sufficient to manipulate it through an

X register; i.e., treat it as a temporaryvariable.

For the same reasons, if the first occurrence is nested inside a structure in the body,then a heap cell will necessarily be allocated for it by the unification instructions.
So it might as well be treated as a temporary variable, saving a stack slot.
Another clear case is when the first occurrence of the variable is as an argument inthe

last body goal. Then indeed, since performing LCO will require systematically
globalizing it anyway, it makes more sense to treat it as a temporary variable.

The foregoing situations cover all those in Warren's definition of a temporary
variable. Therefore, the criteria for that part of the definition which characterizesa temporary variable are justifiably sound. But is this true for Warren's definition

of a permanent variable? Indeed, there is one last situation not covered by ourforegoing analysis; namely, the case of a variable occurring only in one body goal
which is neither the first nor the last goal. As it turns out, unfortunately, Warren's
variable classification is inconsistent with environment trimming, even with the
setup of run-time checks that we have explained in the previous sections.

Let us argue with a specific example.8 Consider the rule `a :- b(X; X); c:'.According to our definition,

X is treated as a temporary variable. This resultsin the compiled code shown as Figure 5.12, where

X is correctly handled as atemporary variable, admittedly at the expense of systematically allocating a heap

slot for it.
On the other hand, according to Warren's variable classification, the variable X isa

permanent variable. Therefore, following the compilation rules described in the
previous sections, the generated code would be that shown in Figure 5.13. Now,observe what happens when calling

a with the instruction sequence of Figure 5.13.

8The lame behavior of this example was pointed out to the author by Damian Chu [Chu90] and
Michael Hanus [Han90].

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 71 OF 129

WARREN'S ABSTRACT MACHINE
a=0 : allocate % a :-

put variable A1; A2 % b(X; X
call b=2; 0 % );
deallocate % c
execute c=0 % :

Figure 5.12: Code for a :- b(X; X); c:, by our classification

a=0 : allocate % a :-

put variable Y1; A1 % b(X;
put unsafe value Y1; A2 % X
call b=2; 0 % );
deallocate % c
execute c=0 % :

Figure 5.13: Code for a :- b(X; X); c:, by Warren's classification

PAGE 72 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

a=0 : allocate % a :-

put variable Y1; A1 % b(X;
put value Y1; A2 % X
call b=2; 1 % );
deallocate % c
execute c=0 % :

Figure 5.14: Delayed trimming for a :- b(X; X); c:
A stack slot is allocated for Y1, then register A1 is made to point to that slot. Then,a run-time check is carried out because

Y1 is unsafe and it is found that Y1 must beglobalized. This is done, making both
Y1 and A2 point to a new heap cell. Then,
b=2 is called after trimming Y1 out of the environment. However, register A1 still
points to the discarded slot! It is therefore clear that for Warren's classification tobe correct, something must be done to prevent this particular ailment.

As far as this author could read, it has not been explained anywhere (includ-ing in [War83, War88]) how to prevent incorrect code as that of Figure 5.13 to
be generated because of Warren's classification, let alone what correct code toproduce with that classification. Upon private inquiry, this is what Warren proposes [War90]:

"The general principle is that one should make variables permanent if at all
possible, and use put variable Xn; Ai only as a last resort. The problem is what to do about variables which occur in only one goal. If it is
the last call, one has no (real) option but to globalise the variable using
put variable Xn; Ai. If it is other than the last call, then one can either
globalise the variable in the same way, or avoid trimming the variable at the
immediately following call, but rather trim it at the next call, by which time
it is certainly safe to do so (provided variable-variable bindings always point
to the `last-to-die' variable and the variable to be trimmed is allocated as the
`first to die' in the next call)."

Warren goes on to illustrate how, for our specific example, delayed trimming
would repair the code of Figure 5.13 to that of Figure 5.14 where Y1 is keptin the environment until the time when execution returns from

b=2, at which point

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 73 OF 129

WARREN'S ABSTRACT MACHINE
a=0 : allocate % a :-

put variable Y1; A1 % b(X;
put unsafe value Y1; A2 % X
call b=2; 1 % );
deallocate % c
execute c=0 % :

Figure 5.15: Useless delayed trimming for a :- b(X; X); c:
it is discarded.9
Some comments are in order regarding this fix. First, Warren's last (cryptic)
parenthetical comment simply recalls the proviso that is already ensured by ourvariable-variable binding convention from higher to lower addresses (WAM Binding Rule 1) and the convention set up for environment trimming allocating perma-nent variables in the environment so as to put the "last-to-die" first (

i.e., at smalleraddresses)--as explained in Section 5.7.

Second, one must convince one's self that delayed trimming is indeed safe so as towarrant the simpler

put value Y1; A2 instead of the expected put unsafe value Y1; A2,as prescribed for unsafe variables. For clearly, if rather than the code of Figure 5.14, we had to generate that shown in Figure 5.15, then the whole compli-cation would be useless. Indeed,

Y1 would still be globalized on the heap, withthe additional penalty of the untrimmed stack slot--not to mention the run-time

check. The simpler code of Figure 5.12 would thus be clearly superior. Therefore,for the code in Figure 5.14 to be of any value, it must be guaranteed that delaying trimming Y1 makes it no longer unsafe, even though it has been initializedwith

put variable Y1; A1. Such is the case, of course, as the unsafety of Y1would only be caused precisely by trimming it by the call to its last goal. That it

is safe to trim it at the following call is clear under the proviso of our binding andstack-allocation conventions.

In view of the foregoing considerations, the reader may now understand why

9This solution using delayed trimming was also pointed out to this author by Michael
Hanus [Han90] who apparently figured it out for himself, probably like many others who workedout a WAM implementation.

PAGE 74 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
we did not start out with Warren's definition for variable classification. Ourreasons are quite deliberate. Firstly, in the gradual construction of our partial
machines, Warren's definition is unjustifiable before the environment stack orLCO are considered. Secondly, although historically, David H. D. Warren made
put variable Xn; Ai systematically allocate a heap cell as a patch to accom-modate LCO,

10 this instruction can be justified otherwise, as we have indeed

shown, as evolved from the simple unification machine M0. Lastly, we favor our
a posteriori approach rather than Warren's since starting with a suboptimal, butsimpler to understand, variable classification constitutes a greater tutorial value as

it focuses the reader's attention on mechanisms that are not affected by the fine
points of this issue.

In conclusion, although Warren's variable classification is historically the original definition used in the WAM as it was conceived, and can be explained as anoptimization--with the need of delayed environment trimming to be correct--this

author finds his own variable classification somewhat less contrived and surelymuch easier to justify.

5.10 Indexing
The three choice point manipulation instructions impose a strictly sequential searchover the list of clauses making up a definition. If all the arguments of a calling
predicate are unbound variables, then there is clearly no better manner to proceed.On the other hand, when some of the arguments are at least partially instantiated,
that information can be used to access unifiable clause heads more directly. In thecases where the number of clauses constituting a definition is large (as is not so

10He states [War89]:

"The put variable Xn; Ai instruction is a kind of hack which exists only totake care of the problems of a variable that is bound to be unsafe at LCO, namely
a variable which has its first occurrence in the last call and isn't otherwise bound.This special instruction should NOT be used for any other purpose.

Variables occurring only as arguments to the last goal cannot be treated as [perma-nent variables] since they are bound to be unsafe, and would necessarily be globalised by the put unsafe value Yn; Ai instruction if they were treated this way.To avoid this round-about approach of creating a permanent variable only to then
globalise it, the put variable Xn; Ai instruction is introduced, and these vari-ables are instead treated as [temporary variables]."

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 75 OF 129

WARREN'S ABSTRACT MACHINE
rarely the case for facts), this is clearly desirable. Ideally, this requires a techniquefor partially interleaving unification with search. Indeed, if it can be decided at
the outset that a clause head will unify exclusively with a category of data, all themore selective the search code will be. On the other hand, it may be impossible in general, or very costly at best, to generate an optimal discrimination filterbased on the patterns of all arguments. Fortunately, a suboptimal compromise
turns out to be quite reasonable in practice. Since Prolog programmers have anatural tendency to write code in a data structure-directed manner using discriminating patterns as first argument, it is quite acceptable to limit indexing to key onthe first argument only. The WAM uses this idea as a basis for optimizing clause
selection. Naturally, this applies only to procedure definitions that contain morethan one clauses. In what follows, we refer to a clause head's first argument as its
(indexing) key.
First, note that any clause head whose key is a variable creates a search bottleneckin a procedure definition in which it appears. Indeed, that key will unify with

anything and thus its clause must be explored in all cases. For this reason, aprocedure

p defined by the sequence of clauses C1; : : : ; Cn is partitioned as a
sequence of subsequences S1; : : : ; Sm, where each Si is either a single clausewith a variable key, or a

maximal subsequence of contiguous clauses whose keysare not variables. For example, the following definition is partitioned into the four

subsequences S1, S2, S3, and S4, as shown:11

S1

8????
??!

??????:

call(XorY ) :- call(X):
call(trace) :- trace:
call(XorY ) :- call(Y ):
call(notrace) :- notrace:
call(nl) :- nl:

S2 n call(X) :- builtin(X):

S3 n call(X) :- extern(X):

S4

8???!

???:

call(call(X)) :- call(X):
call(repeat):
call(repeat) :- call(repeat):
call(true):

As expected, the general translating scheme for a procedure p with a definitionthus partitioned into subsequences

S1; : : : ; Sm, where m ? 1, is:

11This example is a slight modification of that given in [War83]. The (admittedly silly) splitting
of the two or=2 clauses is only to illustrate that this will not affect performance thanks to indexing.

PAGE 76 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

p : try me else S2

code for subsequence S1
S2 : retry me else S3

code for subsequence S2.

..

Sm : trust me

code for subsequence Sm

where retry me else is necessary only if m ? 2. If m = 1, none of theabove is needed and the translation boils down only to the code necessary for the
single subsequence chunk. Furthermore, the simpler case where the subsequenceis reduced to a single variable-key clause degenerates into the expected simpler
translation pattern requiring nothing more than we had before. Thus, the code for
call =1 above looks like:

call=1 : try me else S2 %

indexed code for S1 %
S2 : retry me else S3 % call(X)

execute builtin=1 % :- builtin(X):
S3 : retry me else S4 % call(X)

execute extern=1 % :- extern(X):
S4 : trust me %

indexed code for S4 %

Let us then focus on indexing within a non variable-key subsequence.
The technique of clause indexing for a subsequence uses one or two levels of dis-patching according to the run-time sort of the calling procedure's first argument,

and possibly a third level consisting of sequential threading together some prese-lected clauses. A first level of dispatching is performed depending on whether the
dereferenced value of A1 is a variable, a constant, a (non-empty) list, or a general
structure. In each case, control is to jump to a (possibly void) bucket of clauses.Respectively, the code bucket of a variable corresponds to full sequential search

through the subsequence (thus, it is never void), that of a constant (resp., of astructure) corresponds to a further dispatching level discriminating among different constants (resp., different structures), and that of a list corresponds either to
the single clause with a list key or to a linked list of all those clauses in the sub-sequence whose keys are lists. For those constants (or structures) having multiple

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 77 OF 129

WARREN'S ABSTRACT MACHINE
clauses, a possible third level bucket corresponds to the linked list of these clauses(just like the possible second level for lists).
Hence, the general indexing code pattern for a subsequence is of the form:

first level indexing;
second level indexing;
third level indexing;
code of clauses in subsequence order;

where second and third levels are only necessary as dictated by what sort of keys
are present in the subsequence and in what number. In particular, they may be al-together eliminated as appropriate in the degenerate cases. The last part following

the dispatching code is simply the regular sequential choice control construction.For example, the subsequence

S1 of call=1 is translated thus:

first level indexing for S1
second level indexing for S1
third level indexing for S1
S11 : try me else S12

code for `call(XorY ) :- call(X):'
S12 : retry me else S13

code for `call(trace) :- trace:'
S13 : retry me else S14

code for `call(XorY ) :- call(Y ):'
S14 : retry me else S15

code for `call(notrace) :- notrace:'
S15 : trust me

code for `call(nl) :- nl:'

Therefore, we need instructions for each dispatching level in the general case.
First level dispatching is done with the switch on term V; C; L; S instructionwhose effect is to make control jump to the instruction labeled, respectively,

V ,
C, L, or S, depending on whether the dereferenced value of A1 is a variable, aconstant, a non-empty list, or a structure, respectively.

Second level dispatching for N distinct constants (having realized that A1 derefer-ences to one) is done with

switch on constant N; T , where T is a hash-table
of size N of the form fci : Lci gNi=1 associating to each distinct constant ci used asa key in the subsequence a label

Lci where control must jump when ci is passed

PAGE 78 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
as first argument. If the constant found in A1 is not one in the table, backtrackingoccurs. The similar instruction

switch on structure N; T does the samefor all distinct (non-constant non-list) functors in the subsequence. Second level

list indexing is really third level indexing on list structures, the second level being
skipped by special handling of lists in the spirit of WAM Principle 3.

Third level indexing corresponds to threading together multiple (but not necessarily contiguous) clauses whose keys are lists, or a same constant or structure.Since the code of all clauses in the subsequence are already so-threaded by a

try me else, (retry me else) trust me choice control construction, itsclause code subsequence relevant only to the recognized key at hand (be it list,
constant, or structure) must be accessed explicitly with an alternative choice con-trol construction. This is achieved by using three instructions,

try L, retry L,
trust L. These are almost identical to try me else L, retry me else L,and

trust me, respectively. The only difference is that they use the specifiedlabel

L for the instruction to jump to, saving the next one in sequence as the nextclause alternative in the choice point (except for

trust, of course).

The complete indexing code for subsequence S1 of the call=1 example is givenin Figure 5.16, and that for subsequence

S4 is given in Figure 5.17. Thecomplete indexing code for
call=1 can thus be patched together from these and
the partitioned scheme for S1, S2, S3, and S4 given earlier. An illustration of adegenerate case where all three levels of indexing are not necessary is given in

Figure 5.18 for the familiar conc definition for concatenating two lists:

conc([]; L; L):
conc([HjT ]; L; [HjR]) :- conc(T; L; R):

It is interesting to observe that when the conc=3 procedure is called with an in-stantiated first argument, no choice point frame for it is ever needed. As a matter
of fact, using indexing has a major incidental benefit as it substantially reduces thecreation and manipulation of choice point frames on the stack. This has as corollary that it also reduces the effects of environment protection, and thus magnifiesthe gain of LCO and environment trimming. For example, let us assume that a list
processing procedure lp is defined as:

lp([HjT ]) :- process(H); lp(T ):
lp([]):

Some programmers have indeed taken the habit of specifying the []-clause last us-ing Prolog interpreters. They reason that since there are many more

[ j ]'s in a list

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 79 OF 129

WARREN'S ABSTRACT MACHINE
switch on term S11; C1; fail; F1 % 1st level dispatch for S1
C1 : switch on constant 3; f trace : S1b;

notrace : S1d;
nl : S1e g

% 2nd level for constants

F1 : switch on structure 1; f or=2 : F11 g % 2nd level for structures
F11 : try S1a % 3rd level for or=2

trust S1c %

S11 : try me else S12 % call
S1a : get structure or=2; A1 % (or

unify variable A1 % (X;
unify void 1 % Y ))
execute call=1 % :- call(X):

S12 : retry me else S13 % call
S1b : get constant trace; A1 % (trace)

execute trace=0 % :- trace:

S13 : retry me else S14 % call
S1c : get structure or=2; A1 % (or

unify void 1 % (X;
unify variable A1 % Y ))
execute call=1 % :- call(Y ):

S14 : retry me else S15 % call
S1d : get constant notrace; A1 % (notrace)

execute notrace=0 % :- notrace:

S15 : trust me % call
S1e : get constant nl; A1 % (nl)

execute nl=0 % :- nl:

Figure 5.16: Indexing code for subsequence S1

PAGE 80 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

S4 switch on term S41; C4; fail; F4 % 1st level dispatch for S4
C4 : switch on constant 3; f repeat : C41;

true : S4d g

% 2nd level for constants

F4 : switch on structure 1; f call=1 : S41 g % 2nd level for structures
C41 : try S4b % 3rd level for `repeat'

trust S4c %

S41 : try me else S42 % call
S4a : get structure call=1; A1 % (call

unify variable A1 % (X))
execute call=1 % :- call(X):

S42 : retry me else S43 % call
S4b : get constant repeat; A1 % (repeat)

proceed % :

S43 : retry me else S44 % call
S4c : get constant repeat; A1 % (repeat)

put constant repeat; A1 % :- call(repeat)
execute call=1 % :
S44 : trust me % call
S4d : get constant true; A1 % (true)

proceed % :

Figure 5.17: Indexing code for subsequence S4

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 81 OF 129

WARREN'S ABSTRACT MACHINE
conc=3 : switch on term C1a; C1; C2; fail %
C1a : try me else C2a % conc
C1 : get constant []; A1 % ([];

get value A2; A3 % L; L)
proceed % :

C2a : trust me % conc
C2 : get list A1 % ([

unify variable X4 % Hj
unify variable A1 % T ]; L;
get list A3 % [
unify value X4 % Hj
unify variable A3 % R])
execute conc=3 % :- conc(T; L; R):

Figure 5.18: Encoding of conc=3
than the single final [], this procedure, when invoked with an instantiated first ar-gument, will backtrack only once at the end of list before reaching initial success.
However, with a compiler using LCO but no clause indexing, this will annihilate
all effects of LCO because a choice point will systematically cover every recur-sive call's environment to

lp with a non-empty list in the key position. Whereas,for such calls clause indexing will eliminate all choice points and transform either

ordering of the two clauses into fast iterative code.
All explicit definitions for the indexing instructions are given in Appendix B.

Exercise 5.4 The effect of the switch on constant instruction described above
is that given originally by Warren in [War83]. However, it does useless work as
it eventually leads to a get constant instruction that redundantly tests whether
register A1 contains that very same constant that was seen in A1 by switch on constant.
Can you propose a simple optimization to avoid this redundancy? [Hint: Beware
of intermediate levels of indexing.]

Exercise 5.5 If you could figure out a solution for Exercise 5.4, can it also work or
be adapted to avoid a redundant A1 check by get structure after switch on structure?
[Hint: Beware of the setting of the S register and that of read/write mode.]

PAGE 82 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
5.11 Cut
In this last section, we explain how the design obtained after the compounding of
all the foregoing optimizations can be augmented to accommodate implementa-tion of the Prolog cut. The treatment of cut was not part of the original WAM

report [War83]. The material we present here is extrapolated from what wassketched in [War88] and suggestions from Dean Rosenzweig [Ros91].

As is well-known, the cut predicate (noted `!') is an extra-logical control anno-tation that helps the Prolog programmer trim the search space. The operational
semantics of `!' is simply to succeed with a side effect on backtracking information; namely, once passed, it makes control forget any other potential alternativefor the procedure in whose clause it appears as well as any other arising from

preceding body goals.
In terms of the machine architecture at hand, this effect is obtained by discardingall choice point frames that were created after the choice point frame that was

current right before the call of the procedure containing the cut. Let us assumethat the appropriate choice point where to return upon backtracking over a cut is
maintained in a global register called the cut register and noted B0. Clearly, the
value of B0 must be set to the address of the choice point that is current at the timea procedure call is made. This is achieved by altering the

call and executeinstructions to set
B0 to the value of the current value of B. In this way, executinga cut amounts essentially to resetting

B to the value of B0.

There are really two sorts of cuts to consider: shallow and deep cuts. A shallowcut is one located between the head and the body of the rule immediately after the

`:-' symbol (thus also called neck cut--somewhat euphemistically), as in:

h :- !; b1; : : : ; bn:
while a deep cut is any other, of the form:

h :- : : : ; bi; !; : : : ; bn: (1 ^ i ^ n):

Before looking at each case in detail, let us first make an important observationregarding the creation of choice points. While it is easy to see that clause indexing may bypass creating a choice point for a multiple-clause procedure, it is perhaps less immediate to realize that indexing may also cause creating an additional
choice point for such a procedure. For instance, refer to the call =1 procedure given

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 83 OF 129

WARREN'S ABSTRACT MACHINE
on Page 76. Its instruction sequence (Page 77) starts with:

call =1 : try me else S2
which creates one choice point for call =1. Further down in that procedure's in-struction sequence, what constitutes subsequence

S1 (given as Figure 5.16 onPage 80) contains the instruction:

F11 : try S1a
that will create a second choice point for call =1 if executed.12
Let us first consider the simpler case where the clause containing the cut, be it shal-low or deep, is the first (not necessarily in sequence thanks to indexing) among

those of the called procedure's to be executed upon entering the procedure with
call or execute. We shall see later what to do in the other case where back-tracking occurred since the original call.

Upon starting executing the instruction sequence of the clause containing a shal-low cut,

B0 is either equal to B (since indexing could have bypassed creating achoice point for the procedure containing the rule), or equal to one of the two

previous choice points preceding B. If B0 and B are equal, there is no action tobe taken by the cut since this call does not have alternatives anyway. Otherwise
(B ? B0), the shallow cut must discard any (one or two) choice points following
B. This action is the same in either case (i.e., B  B0; HB  B:H). This is themain effect of the new instruction

neck cut into which a shallow cut occurrenceis translated. For instance, the body of:

a :- !; b:
is compiled into:

neck cut
execute b=0

The resetting of B and HB is not quite all that may be done upon a cut. Indeed,discarding a choice point may change the status of some bindings recorded as conditional in the trail which will have become unconditional for those heap (resp.,

12This fact was brought to our attention by Dean Rosenzweig [Ros91] who noticed that the
scheme given in [AK90] was erroneous. We gratefully acknowledge his suggesting the correctedscheme we describe next.

PAGE 84 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
stack) variables whose addresses will now be greater than HB (resp., B). There-fore, this calls for a clean-up of the trail up to the trail address indicated by the reinstated choice point frame. This amounts to removing the appropriate addressesfrom the trail and tamping it down to eliminate the holes this may have created
in it. (This operation is given as an ancillary operation called tidy trail in Ap-pendix B.)

Regarding deep cuts, the complication is that the calls to intervening body goalswill overwrite the cut register

B0 between the time the clause containing the cutis entered and the time the cut is executed. However, all choice points created between the setting of B0 upon calling this clause's procedure and this deep cut mustbe discarded. Therefore, in exactly the same way as for

CP, it will be necessary tosave
B0 further as part of the cut clause's environment. Fortunately, it is not necessary to do so systematically for all clauses since most do not contain deep cuts.Thus, rather than extending the format of an environment frame with a special slot

for B0 as we did for CP, it is preferable to save it as a pseudo permanent variable.The situation then is to assimilate a deep cut occurrence as

!(Y ), where Y is apermanent variable distinct from all other variables in the clause. This variable is

allocated in the current environment to accommodate environment trimming justlike the other real permanent variables (

i.e., its offset is greater than the subse-quent goal's and less that the preceding goal's). With this, we only need two new

instructions: (1) get level Yn

(2) cut Yn
such that (1) get level Yn always appear immediately after allocate in theclause, having for effect to set

Yn to contain the current value of B0; whereas,(2)
cut Y n discards all (if any) choice points after that indicated by Yn, and
cleans up new unconditional bindings from the trail up to that point. For example,

a :- b; !; c:
is compiled into:

allocate
get level Y1
call b=0; 1
cut Y1
deallocate
execute c=0

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 85 OF 129

WARREN'S ABSTRACT MACHINE
Note now that the presence of a deep cut in a rule that would otherwise be achain rule (

e.g., a :- b; !:) makes it a deep rule requiring environment allocation.However, this not the case when a shallow cut occurs in rule that would otherwise

be a chain rule. A neck cut does not make such a rule a deep rule, even though it
might be construed as having two body goals.

The scheme just described deals correctly with cuts in the case where the clause
in question is the first executed among those of the called procedure. However, itis not quite complete as given to be correct if the clause is executed after backtracking within the same procedure. Then, there is yet the problem of restoring acorrect value for

B0. For example, consider the following pattern:

a :- b; c:
a :- !; d:
a :- : : :

At the time a is called, B0 is set to the current choice point. When the clause
a :- b; c: is executed, the call to b overwrites B0. Now, if the call to b fails,backtracking to the following clause will execute the shallow cut and reset

B tothe spurious value left in
B0 by the failed call to b.

This simple example illustrates the need for restoring B0 to a correct value uponbacktracking. But what (and where) is this correct value? It would

not be correctto reset
B0 to the choice point preceding the current B since, as observed before,indexing may have created two choice points for the procedure and there is no

simple way to detect this. The solution that we adopt here is simple. (Moreefficient but more complicated alternatives can be devised--

e.g., see Exercise 5.6.)Namely, the value of the cut register
B0 is systematically saved as part of a choicepoint at its creation and restored upon backtracking from that information saved

in the current choice point. Accordingly, the format of a choice point frame given
on Page 38 is extended with an additional slot for this purpose (as shown in thecomplete layout on Page 117) and the

try me else and try instructions aremodified appropriately. (So is
allocate since it must account for this additionalslot space when
B is the top of the stack.)

We now have a complete and correct scheme for handling cut. All the machinerywe just presented is taken into account in the explicit definitions of the complete

WAM instructions given in Appendix B.

Exercise 5.6 The following is suggested by [Ros91]. The problem of knowing to
which value B0 must be restored upon backtracking may be solved in a better way

PAGE 86 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

than systematically saving it as part of a choice point. It is possible to use two
slightly different versions for retry me else (resp., retry) depending on the
level of indexing they are used for. An "outer" version, say retry me else 1
(resp., retry 1) would reset B0 to the previous choice point, and an inner version
, say retry me else 2 (resp., retry 2) would reset B0 to the choice point
preceding the previous one. Give a correct translation scheme and all appropriate
modifications needed in the explicit definitions of Appendix B to implement this
solution.

Exercise 5.7 Refer to the remark made in Footnote 3 on Page 38. Give all appropriate modifications needed in the pseudo-code given in Appendix B defining the
WAM instructions and ancillary operations in order to eliminate the arity slot from
choice point frames.

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 87 OF 129

WARREN'S ABSTRACT MACHINE
PAGE 88 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI
Chapter 6
Conclusion
In this tutorial, we have conducted an exhaustive reconstruction of an abstract
machine architecture for Prolog as completely as it has been conceived by DavidH. D. Warren. Starting from a mere first-order term unification machine, we have

elaborated its design gradually, introducing each needed concept as didacticallyas we thought possible. Doing so, we have striven to justify every single design
decision, explicating often subtle interplays as they arose.
Obviously, the design that we have attained necessarily does not contain all thetools needed for constructing a complete Prolog compiler and run-time environment. Namely, many details regarding other interesting Prolog control con-structs (

e.g., conditional, freeze, etc.), extra-logical and built-in procedures (e.g.,assert/retract, setof,

etc.), input/output facilities, etc., are yet to be spelled out.However, all this is clearly beyond the scope of this tutorial. These additional

features, as well as any other operationally well-defined interesting extensions,
can be integrated into the pure WAM design we have presented without excessivelabor.

For those interested in more advanced material using WAM ideas, some works areworth mentioning for further reading. In particular, the manner in which the WAM
regards unification has a strong relation with partial evaluation techniques as ex-plained in particular in [Kur87]. Also, since compilation as done by the WAM
treats each procedure independently of the rest of the program, global analysistechniques can therefore be considered as in [Mel85, DW86a, DW86b, Deb89,
DW89, vR90, vRD90]. Finally, one will find copious material about improvingbacktracking in Prolog in [PP82, BP84, Cox84, CCF88, ZTU90, Zho90].

89

WARREN'S ABSTRACT MACHINE
At any rate, we hope to have contributed to give the reader a rapid and clearunderstanding with thorough, if not formal, justification of all the details of the
essential abstract machine. Our aim has been to leave as little unexplained aspossible, opening up Warren's original design. Seen thus decomposed and reconstructed from its logical components, the WAM's organization and workings loseall mystery. Yet, far from lessening its designer's credit, understanding the WAM
can make one appreciate all the more David H. D. Warren's feat, as he conceivedthis architecture in his mind as one whole. We are pleased to have shared with the
reader our admiration for his contribution's adequacy and elegance.

PAGE 90 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI
Appendix A
Prolog in a Nutshell
We first present basic notions for first-order terms and substitutions. Then, we
describe a non-deterministic unification algorithm as a set of solution-preservingtransformations on a set of equations due to Herbrand [Her71] (

cf., Page 148) andrediscovered by Martelli-Montanari [MM82]. Then, we summarize the essence

of Prolog's operational semantics.

Terms and substitutions
Let f\Sigma ngn*0 be an indexed family of mutually disjoint sets of (function) symbolsof arity

n. Let \Sigma  = Sn*0 \Sigma n be the set of all function symbols. Let V be acountably infinite set of

variables. By convention, variables will be capitalized
not to confuse them with constants in \Sigma 0.

Let T be the set of (first-order) terms defined as the smallest set such that:

fflif X 2 V then X 2 T ;
fflif a 2 \Sigma 0 then a 2 T ;
fflif f 2 \Sigma n, (n * 1), and ti 2 T ; (1 ^ i ^ n), then f (t1; : : : ; tn) 2 T .

For example, given the signature \Sigma  such that p 2 \Sigma 3, h 2 \Sigma 2, f 2 \Sigma 1,
and a 2 \Sigma 0, and given that W , X, Y , and Z are variables in V, the terms
p(Z; h(Z; W ); f (W )) and p(f (X); h(Y; f (a)); Y ) are in T .

A substitution is a finitely non-identical assignment of terms to variables; i.e., afunction

oe from V to T such that the set fX 2 V j X 6= oe(X)g is finite. This

91

WARREN'S ABSTRACT MACHINE
set is called the domain of oe and denoted by dom(oe). Such a substitution is alsowritten as a set such as

oe = fti=Xigni=1 where dom(oe)= fXigni=1 and oe(Xi) = tifor
i = 1 to n.

A substitution oe is uniquely extended to a function oe from T to T as follows:

ffloe(X) = oe(X), if X 2 V;
ffloe(a) = a, if a 2 \Sigma 0;
ffloe(f (t1; : : : ; tn)) = f (oe(t1); : : : ; oe(tn)), if f 2 \Sigma n, ti 2 T ; (1 ^ i ^ n).

Since they coincide on V, and for notation convenience, we deliberately confuse asubstitution

oe and its extension oe. Also, rather than writing oe(t), we shall write toe.Given two substitutions

oe = fti=Xigni=1 and ` = fsj =Yjgmj=1, their composition
oe` is the substitution which yields the same result on all terms as first applying oethen applying

` on the result. One computes such a composition as the set:

oe` = ( ft`=X j t=X 2 oeg \Gamma  fX=X j X 2 dom(oe)g )

[ ( ` \Gamma  fs=Y j Y 2 dom(oe)g ):

For example, if oe = ff (Y )=X; U=V g and ` = fb=X; f (a)=Y; V =U g, then com-posing

oe and ` yields oe` = ff (f (a))=X; f (a)=Y; V =U g, while composing ` and
oe gives `oe = fb=X; f (a)=Y; U=V g.

Composition defines a preorder (i.e., a reflexive and transitive relation) on substi-tutions. A substitution

oe is more general than a substitution ` iff there exists asubstitution
ae such that ` = oeae. For example, ff (Y )=Xg is more general than
ff (f (a))=X; f (a)=Y g.

Unification algorithm
An equation is a pair of terms, written s = t. A substitution oe is a solution (or a
unifier) of a set of equations fsi = tigni=1 iff sioe = tioe for all i = 1; : : : ; n. Two
sets of equations are equivalent iff they both admit all and only the same solutions.Following [MM82], we define two transformations on sets of equations--

term
decomposition and variable elimination. They both preserve solutions of sets ofequations.

Term Decomposition If a set E of equations contains an equation of the form
f (s1; : : : ; sn) = f (t1; : : : ; tn), where f 2 \Sigma n; (n * 0), then the set E0 =

PAGE 92 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
E \Gamma  ff (s1; : : : ; sn) = f (t1; : : : ; tn)g [ fsi = tigni=1 is equivalent to E.1
Variable Elimination If a set E of equations contains an equation of the form
X = t where t 6= X, then the set E0 = (E \Gamma  fX = tg)oe [ fX = tg where
oe = ft=Xg, is equivalent to E.

A set of equations E is partitioned into two subsets: its solved part and its unsolvedpart. The solved part is its maximal subset of equations of the form

X = t suchthat
X occurs nowhere in the full set of equations except as the left-hand side of
this equation alone. The unsolved part is the complement of the solved part. A setof equations is said to be

fully solved iff its unsolved part is empty.

Following is a unification algorithm. It is a non-deterministic normalization pro-cedure for a given set

E of equations which repeatedly chooses and performs oneof the following transformations until none applies or failure is encountered.

(u:1) Select any equation of the form t = X where t is not a variable, and

rewrite it as X = t.

(u:2) Select any equation of the form X = X and erase it.
(u:3) Select any equation of the form f (s1; : : : ; sn) = g(t1; : : : ; tm) where

f 2 \Sigma n and g 2 \Sigma m; (n; m * 0); if f 6= g or n 6= m, stopwith failure; otherwise, if

n = 0 erase the equation, else (n * 1) replaceit with
n equations si = ti; (i = 1; : : : ; n).

(u:4) Select any equation of the form X = t where X is a variable whichoccurs somewhere else in the set of equations and such that

t 6= X.
If t is of the form f (t1; : : : ; tn), where f 2 \Sigma n, and if X occurs in t,then stop with failure; otherwise, let

oe = ft=Xg and replace every otherequation
l = r by loe = roe.

If this procedure terminates with success, the set of equations which emerges as
the outcome is fully solved. Its solved part defines a substitution called the most
general unifier (MGU) of all the terms participating as sides of equations in E. Ifit terminates with failure, the set of equations

E is unsatisfiable and no unifier forit exists.

The set E = fp(Z; h(Z; W ); f (W )) = p(f (X); h(Y; f (a)); Y )g, for example,is normalized as follows:

1If n = 0, the equation is simply deleted.
Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 93 OF 129

WARREN'S ABSTRACT MACHINE
fZ = f (X) ; h(Z; W ) = h(Y; f (a)) ; f (W ) = Y g [by (u:3)]
fZ = f (X) ; h(f (X); W ) = h(Y; f (a)) ; f (W ) = Y g [by (u:4)]
fZ = f (X) ; f (X) = Y ; W = f (a) ; f (W ) = Y g [by (u:3)]
fZ = f (X) ; Y = f (X) ; W = f (a) ; f (W ) = Y g [by (u:1)]
fZ = f (X) ; Y = f (X) ; W = f (a) ; f (W ) = f (X)g [by (u:4)]
fZ = f (X) ; Y = f (X) ; W = f (a) ; f (f (a)) = f (X)g [by (u:4)]
fZ = f (X) ; Y = f (X) ; W = f (a) ; f (a) = Xg [by (u:3)]
fZ = f (X) ; Y = f (X) ; W = f (a) ; X = f (a)g [by (u:1)]
fZ = f (f (a)) ; Y = f (f (a)) ; W = f (a) ; X = f (a)g [by (u:4)]
producing the substitution oe = ff (f (a))=Z; f (a)=W; f (f (a))=Y; f (a)=Xg whichis the MGU of

p(Z; h(Z; W ); f (W )) and p(f (X); h(Y; f (a)); Y ) and both yield
the same term p(f (f (a)); h(f (f (a)); f (a)); f (f (a))) when applied the substitu-tion

oe.

Prolog
Logic programming, of which Prolog is the canonical language, expresses pro-grams as relational rules of the form:

r0(~t0) :- r1(~t1); : : : ; rn(~tn):
where the ri's are relationals symbols and the ~ti's are tuples of first-order terms.One reads such a rule as: "

For all bindings of their variables, the terms ~t0 are in
relation r0 if the terms ~t1 are in relation r1 and ... the terms ~tn are in relation rn."
In the case where n = 0, the rule reduces to the simple unconditional assertion,or

fact, r0(~t0) that the terms ~t0 are in relation r0. A fact will be written omittingthe

:- symbol. These rules are called definite clauses; expressions such as ri(~ti)are called

atoms; the head of a definite clause is the atom on the left of the :-symbol, and its

body is the conjunction of atoms on its right.

For example, the following are two definite clauses, the first one being a fact:

conc([]; L; L):
conc(H:T; L; H:R) :- conc(T; L; R):

PAGE 94 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
where `[]' 2 \Sigma 0 is a constant and the function symbol `:' 2 \Sigma 2 is written in infixnotation. This may be used as a program to concatenate two lists where

[] is usedas a list terminator.
2

A query is a clause of the form:

:- q1(~s1); : : : ; qm(~sm):
A query as shown above may be read: "Does there exist some binding of variables
such that the terms ~s1 are in relation q1 and ... ~sm are in relation qm?" To em-phasize that this is interpreted as a question, the symbol

:- is then written ?- asin:

?- q1(~s1); : : : ; qm(~sm):
SLD resolution is a non-deterministic deduction rule by which queries are trans-formed. It owes its origins to Automatic Theorem Proving based on the Resolution Principle discovered by J. A. Robinson [Rob65] and was proposed by
R. A. Kowalski [Kow79] as a computation rule. Technically, it is characterized aslinear resolution over definite clauses, using a selection function. Linear resolution is a particular restriction of the non-deterministic application of the generaldeduction rule defined in [Rob65] so that one single fixed clause keeps being
transformed by resolving it against other clauses in a given set. SLD resolution
is a further restriction of linear resolution where (1) the fixed clause is a query,(2) clauses in the set are definite, and (3) an oracular function selects which atom

in the query to resolve on and which definite clause in the set to resolve against.Thus, the letters "SLD" stand respectively for "

Selection," "Linear," and "Definite."

More specifically, using the above Prolog notation for queries and rules, SLDresolution consists in choosing an atom

qi(~si) in the query's body and a definite
clause in the given set whose head r0(~t0) unifies with qi(~si) thanks to a variablesubstitution

oe (i.e., qi(~si)oe = r0(~t0)oe), then replacing it by the body of that clausein the query, applying substitution

oe to all the new query. That is,

?- q1(~s1)oe; : : : ; qi\Gamma 1(~si\Gamma 1)oe; r1(~t1)oe; : : : ; rn(~tn)oe; qi+1(~si+1)oe; : : : ; qm(~sm)oe:
The process is repeated and stops when and if the query's body is empty (success) or no rule head unifies with the selected atom (failure). There are two non-deterministic choices made in the process: one of an atom to rewrite in the query

2For example, 1:2:3:[] is a list. Edinburgh Prolog syntax uses [XjY ] instead of X:Y ; it also uses
a simplified variant to express a list in extenso, allowing writing [1,2,3] rather than [1j[2j[3j[]]]].

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 95 OF 129

WARREN'S ABSTRACT MACHINE
and one among the potentially many rules whose head to unify with this atom. Inany case, SLD resolution is

sound (i.e., it does not derive wrong solutions) and,provided these choices are made by a fair non-deterministic selection function, it

is also complete (i.e., it derives all solutions).
Prolog's computation rule is a particular deterministic approximation of SLD res-olution. Specifically, it is a flattening of SLD resolution emulating a depth-first

search. It sees a program as an ordered set of definite clauses, and a query ordefinite clause body as an

ordered set of atoms. These orders are meant to pro-vide a rigid guide for the two choices made by the selection function of SLD

resolution. Thus, Prolog's particular computation strategy transforms a query byrewriting the query attempting to unify its leftmost atom with the head of the first
rule according to the order in which they are specified. If failure is encountered,
a backtracking step to the latest rule choice point is made, and computation re-sumed there with the next alternative given by the following rule. For example,

if the two clauses for predicate conc are given as above, then the Prolog query`

?- conc(1:2:T; 3:4:[]; L):' succeeds with the substitution T = []; L = 1:2:3:4:[],while the query `

?- conc(1:2:[]; X; 3:Y ):' fails.

Strategies for choice of where to apply linear resolution are all logically consistentin the sense that if computation terminates, the variable binding exhibited is a

legitimate solution to the original query. In particular, like non-deterministic SLDresolution, Prolog resolution is

sound. However, unlike non-deterministic SLDresolution, it is
incomplete. Indeed, Prolog's particular strategy of doing linearresolution may diverge although finitely derivable solutions to a query may exist.

For example, if the definite clauses for conc are given in a different order (i.e.,
first the rule, then the fact), then the query `?- conc(X; Y; Z):' never terminatesalthough it has (infinitely many) finitely derivable solutions!

PAGE 96 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

Appendix B
The WAM at a glance

B.1 WAM instructions

We summarize here for quick reference all the instructions of the WAM. In some
instructions, we use the notation Vn to denote a variable that may be indifferentlytemporary or permanent.

97

WARREN'S ABSTRACT MACHINE
The complete set:

Put instructions
put variable Xn; Ai
put variable Yn; Ai
put value Vn; Ai
put unsafe value Yn; Ai
put structure f; Ai
put list Ai
put constant c; Ai

Get instructions
get variable Vn; Ai
get value Vn; Ai
get structure f; Ai
get list Ai
get constant c; Ai

Set instructions
set variable Vn
set value Vn
set local value Vn
set constant c
set void n

Unify instructions
unify variable Vn
unify value Vn
unify local value Vn
unify constant c
unify void n

Control instructions
allocate
deallocate
call P; N
execute P
proceed

Choice instructions
try me else L
retry me else L
trust me
try L
retry L
trust L

Indexing instructions
switch on term V; C; L; S
switch on constant N; T
switch on structure N; T

Cut instructions
neck cut
get level Yn
cut Yn

PAGE 98 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
Put instructions

put variable Xn; Ai
Push a new unbound REF cell onto the
heap and copy it into both register Xn
and register Ai. Continue execution with
the following instruction.

HEAP[H]  h REF ; H i;
Xn  HEAP[H];
Ai  HEAP[H];
H  H + 1;
P  P + instruction size(P);

put variable Yn; Ai
Initialize the n-th stack variable in the
current environment to `unbound' and
let Ai point to it. Continue execution
with the following instruction.

addr  E + n + 1;
STACK[addr ]  h REF ; addr i;
Ai  STACK[addr ];
P  P + instruction size(P);

put value Vn; Ai
Place the contents of Vn into register Ai.
Continue execution with the following
instruction.

Ai  Vn;
P  P + instruction size(P);

put unsafe value Yn; Ai
If the dereferenced value of Yn is not an
unbound stack variable in the current environment, set Ai to that value. Otherwise, bind the referenced stack variable
to a new unbound variable cell pushed
on the heap, and set Ai to point to that
cell. Continue execution with the following instruction.

addr  deref (E + n + 1);
if addr ! E

then Ai  STORE[addr ]
else

begin

HEAP[H]  h REF ; H i;
bind(addr ; H);
Ai  HEAP[H];
H  H + 1
end;
P  P + instruction size(P);

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 99 OF 129

WARREN'S ABSTRACT MACHINE
put structure f; Ai
Push a new functor cell containing f
onto the heap and set register Ai to an
STR cell pointing to that functor cell.
Continue execution with the following
instruction.

HEAP[H]  f =n;
Ai  h STR ; H i;
H  H + 1;
P  P + instruction size(P);

put list Ai
Set register Ai to contain a LIS cell
pointing to the current top of the heap.
Continue execution with the following
instruction.

Ai  h LIS ; H i;
P  P + instruction size(P);

put constant c; Ai
Place a constant cell containing c into
register Ai. Continue execution with the
following instruction.

Ai  h CON ; c i;
P  P + instruction size(P);

Get instructions

get variable Vn; Ai
Place the contents of register Ai into
variable Vn. Continue execution with
the following instruction.

Vn  Ai;
P  P + instruction size(P);

get value Vn; Ai
Unify variable Vn and register Ai.
Backtrack on failure, otherwise continue execution with following instruction.

unify(Vn; Ai);if

failthen

backtrackelse
P  P + instruction size(P);

PAGE 100 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

get structure f; Ai
If the dereferenced value
of register Ai is an unbound variable, then bind
that variable to a new STR
cell pointing to f pushed
on the heap and set mode
to write; otherwise, if it
is a STR cell pointing to
functor f , then set register
S to the heap address following that functor cell's
and set mode to read.
If it is not a STR cell or
if the functor is different
than f , fail. Backtrack
on failure, otherwise continue execution with following instruction.

addr  deref (Ai);
case STORE[addr ] of

h REF ; i : HEAP[H]  h STR ; H + 1 i;

HEAP[H + 1]  f ;
bind(addr ; H);
H  H + 2;
mode  write;
h STR ; a i : if HEAP[a] = f

then

begin

S  a + 1;
mode  read
end
else fail  true;
other : fail  true;
endcase;if

failthen

backtrackelse
P  P + instruction size(P);

get list Ai
If the dereferenced value
of register Ai is an unbound variable, then bind
that variable to a new LIS
cell pushed on the heap
and set mode to write;
otherwise, if it is a LIS
cell, then set register S to
the heap address it contains and set mode to
read. If it is not a
LIS cell, fail. Backtrack
on failure, otherwise continue execution with following instruction.

addr  deref (Ai);
case STORE[addr ] of

h REF ; i : HEAP[H]  h LIS ; H + 1 i;

bind(addr ; H);
H  H + 1;
mode  write;
h LIS ; a i : S  a;

mode  read;
other : fail  true;
endcase;if

failthen

backtrackelse
P  P + instruction size(P);

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 101 OF 129

WARREN'S ABSTRACT MACHINE
get constant c; Ai
If the dereferenced value
of register Ai is an unbound variable, bind that
variable to constant c.
Otherwise, fail if it is not
the constant c. Backtrack
on failure, otherwise continue execution with following instruction.

addr  deref (Ai);
case STORE[addr ] of

h REF ; i : STORE[addr ]  h CON ; c i;

trail (addr );
h CON ; c0 i : fail  (c 6= c0);
other : fail  true;
endcase;if

failthen

backtrackelse
P  P + instruction size(P);

Set instructions

set variable Vn
Push a new unbound REF cell onto the
heap and copy it into variable Vn. Continue execution with the following instruction.

HEAP[H]  h REF ; H i;
Vn  HEAP[H];
H  H + 1;
P  P + instruction size(P);

set value Vn
Push Vn's value onto the heap. Continue
execution with the following instruction.

HEAP[H]  Vn;
H  H + 1;
P  P + instruction size(P);

PAGE 102 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

set local value Vn
If the dereferenced value of Vn is an
unbound heap variable, push a copy
of it onto the heap. If the dereferenced value is an unbound stack address, push a new unbound REF cell
onto the heap and bind the stack variable to it. Continue execution with the
following instruction.

addr  deref (Vn);
if addr ! H

then HEAP[H]  HEAP[addr ]
else

begin

HEAP[H]  h REF ; H i;
bind (addr ; H)
end;
H  H + 1;
P  P + instruction size(P);

set constant c
Push the constant c onto the heap. Continue execution with the following instruction.

HEAP[H]  h CON ; c i;
H  H + 1;
P  P + instruction size(P);

set void n
Push n new unbound REF cells onto the
heap. Continue execution with the following instruction.

for i  H to H + n \Gamma  1 do

HEAP[i]  h REF ; i i;
H  H + n;
P  P + instruction size(P);

Unify instructions

unify variable Vn
In read mode, place the contents of
heap address S into variable Vn; in
write mode, push a new unbound
REF cell onto the heap and copy it
into Xi. In either mode, increment S
by one. Continue execution with the
following instruction.

case mode of

read : Vn  HEAP[S];
write : HEAP[H]  h REF ; H i;

Vn  HEAP[H];
H  H + 1;
endcase;
S  S + 1;
P  P + instruction size(P);

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 103 OF 129

WARREN'S ABSTRACT MACHINE
unify value Vn
In read mode, unify variable Vn and
heap address S; in write mode, push
the value of Vn onto the heap. In either
mode, increment S by one. Backtrack
on failure, otherwise continue execution with following instruction.

case mode of

read : unify(Vn; S);
write : HEAP[H]  Vn;

H  H + 1;
endcase;
S  S + 1;if

failthen

backtrackelse
P  P + instruction size(P);

unify local value Vn
In read mode, unify
variable Vn and heap
address S. In write
mode, if the dereferenced value of Vn is an
unbound heap variable,
push a copy of it onto
the heap. If the dereferenced value is an unbound stack address,
push a new unbound
REF cell onto the heap
and bind the stack variable to it. In either
mode, increment S by
one. Backtrack on failure, otherwise continue
execution with following instruction.

case mode of

read : unify(Vn; S);
write : addr  deref (Vn);

if addr ! H

then HEAP[H]  HEAP[addr ]
else

begin

HEAP[H]  h REF ; H i;
bind (addr ; H)
end;
H  H + 1;
endcase;
S  S + 1;if

failthen

backtrackelse
P  P + instruction size(P);

PAGE 104 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

unify constant c
In read mode, dereference the heap address S. If the result is an unbound variable, bind that variable to the constant c; otherwise, fail if the result is different
than constant c. In write mode, push the constant c onto the heap. Backtrack
on failure, otherwise continue execution with following instruction.

case mode of

read : addr  deref (S);

case STORE[addr ] of

h REF ; i : STORE[addr ]  h CON ; c i;

trail (addr );
h CON ; c0 i : fail  (c 6= c0);
other : fail  true;
endcase;
write : HEAP[H]  h CON ; c i;

H  H + 1;
endcase;if

failthen

backtrackelse
P  P + instruction size(P);

unify void n
In write mode, push n new
unbound REF cells onto the
heap. In read mode, skip the
next n heap cells starting at location S. Continue execution
with the following instruction.

case mode of

read : S  S + n;
write : for i  H to H + n \Gamma  1 do

HEAP[i]  h REF ; i i;
H  H + n;
endcase;
P  P + instruction size(P);

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 105 OF 129

WARREN'S ABSTRACT MACHINE
Control instructions

allocate
Allocate a new environment on the stack, setting its continuation environment
and continuation point fields to current E and CP, respectively. Continue execution with the following instruction.

if E ? B

then newE  E + CODE[STACK[E + 1] \Gamma  1] + 2
else newE  B + STACK[B] + 8;
STACK[newE]  E;
STACK[newE + 1]  CP;
E  newE;
P  P + instruction size(P);

deallocate
Remove the environment frame at stack
location E from the stack by resetting E
to the value of its CE field and the continuation pointer CP to the value of its
CP field. Continue execution with the
following instruction.

CP  STACK[E + 1];
E  STACK[E];
P  P + instruction size(P);

call P; N
If P is defined, then save the current choice point's address in B0
and the value of current continuation in CP, and continue execution
with instruction labeled P , with N
stack variables remaining in the current environment; otherwise, backtrack.

if defined(P )

then

begin

CP  P + instruction size(P);
num of args  arity (P );
B0  B;
P  @(P )
end
else backtrack;

PAGE 106 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

execute P
If P s defined, then save the current
choice point's address in B0 and continue execution with instruction labeled
P ; otherwise, backtrack.

if defined(P )

then

begin

num of args  arity (P );
B0  B;
P  @(P )
end
else backtrack;

proceed
Continue execution at instruction whose
address is indicated by the continuation
register CP.

P  CP;

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 107 OF 129

WARREN'S ABSTRACT MACHINE
Choice instructions

try me else L
Allocate a new choice point frame on the stack setting its next clause field to L
and the other fields according to the current context, and set B to point to it.
Continue execution with following instruction.

if E ? B

then newB  E + CODE[STACK[E + 1] \Gamma  1] + 2
else newB  B + STACK[B] + 8;
STACK[newB]  num of args;
n  STACK[newB];
for i  1 to n do STACK[newB + i]  Ai;
STACK[newB + n + 1]  E;
STACK[newB + n + 2]  CP;
STACK[newB + n + 3]  B;
STACK[newB + n + 4]  L;
STACK[newB + n + 5]  TR;
STACK[newB + n + 6]  H;
STACK[newB + n + 7]  B0;
B  newB;
HB  H;
P  P + instruction size(P);

retry me else L
Having backtracked to the current choice point, reset all the
necessary information from it
and update its next clause field
to L. Continue execution with
following instruction.

n  STACK[B];
for i  1 to n do Ai  STACK[B + i];
E  STACK[B + n + 1];
CP  STACK[B + n + 2];
STACK[B + n + 4]  L;
unwind trail (STACK[B + n + 5]; TR);
TR  STACK[B + n + 5];
H  STACK[B + n + 6];
HB  H;
P  P + instruction size(P);

PAGE 108 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

trust me
Having backtracked to the current choice point, reset all the
necessary information from it,
then discard it by resetting B to
its predecessor. Continue execution with following instruction.

n  STACK[B];
for i  1 to n do Ai  STACK[B + i];
E  STACK[B + n + 1];
CP  STACK[B + n + 2];
unwind trail (STACK[B + n + 5]; TR);
TR  STACK[B + n + 5];
H  STACK[B + n + 6];
B  STACK[B + n + 3];
HB  STACK[B + n + 6];
P  P + instruction size(P);

try L
Allocate a new choice point frame on the stack setting its next clause field to the
following instruction and the other fields according to the current context, and
set B to point to it. Continue execution with instruction labeled L.

if E ? B

then newB  E + CODE[STACK[E + 1] \Gamma  1] + 2
else newB  B + STACK[B] + 8;
STACK[newB]  num of args;
n  STACK[newB];
for i  1 to n do STACK[newB + i]  Ai;
STACK[newB + n + 1]  E;
STACK[newB + n + 2]  CP;
STACK[newB + n + 3]  B;
STACK[newB + n + 4]  P + instruction size(P);
STACK[newB + n + 5]  TR;
STACK[newB + n + 6]  H;
STACK[newB + n + 7]  B0;
B  newB;
HB  H;
P  L;

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 109 OF 129

WARREN'S ABSTRACT MACHINE
retry L
Having backtracked
to the current choice
point, reset all the
necessary information
from it and update its
next clause field to
following instruction.
Continue execution with instruction
labeled L.

n  STACK[B];
for i  0 to n \Gamma  1 do Ai  STACK[B + i];
E  STACK[B + n + 1];
CP  STACK[B + n + 2];
STACK[B + n + 4]  P + instruction size(P);
unwind trail (STACK[B + n + 5]; TR);
TR  STACK[B + n + 5];
H  STACK[B + n + 6];
HB  H;
P  L;

trust L
Having backtracked to the current choice point, reset all necessary information from it, then
discard it by resetting B to its
predecessor. Continue execution with instruction labeled L.

n  STACK[B];
for i  1 to n do Ai  STACK[B + i];
E  STACK[B + n + 1];
CP  STACK[B + n + 2];
unwind trail (STACK[B + n + 5]; TR);
TR  STACK[B + n + 5];
H  STACK[B + n + 6];
B  STACK[B + n + 3];
HB  STACK[B + n + 6];
P  L;

Indexing instructions

switch on term V; C; L; S
Jump to the instruction labeled, respectively, V , C, L, or S, depending on
whether the dereferenced value of argument register A1 is a variable, a constant,
a non-empty list, or a structure, respectively.

case STORE[deref (A1)] of

h REF ; i : P  V ;
h CON ; i : P  C;
h LIS ; i : P  L;
h STR ; i : P  S;
endcase;

PAGE 110 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

switch on constant N; T
The dereferenced value of register
A1 being a constant, jump to the
instruction associated to it in hashtable T of size N . If the constant
found in A1 is not one in the table,
backtrack.

h tag ; val i  STORE[deref (A1)];
h found ; inst i  get hash(val; T; N );
if found

then P  inst
else backtrack;

switch on structure N; T
The dereferenced value of register
A1 being a structure, jump to the
instruction associated to it in hashtable T of size N . If the functor of
the structure found in A1 is not one
in the table, backtrack.

h tag ; val i  STORE[deref (A1)];
h found ; inst i  get hash(val; T; N );
if found

then P  inst
else backtrack;

Cut instructions

neck cut
If there is a choice point after that indicated by B0, discard it and tidy the
trail up to that point. Continue execution with following instruction.

if B ? B0

then

begin

B  B0;
tidy trail
end;
P  P + instruction size(P);

get level Yn
Set Yn to the current value of B0. Continue execution with following instruction.

STACK[E + 2 + n]  B0;
P  P + instruction size(P);

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 111 OF 129

WARREN'S ABSTRACT MACHINE
cut Yn
Discard all (if any) choice points after
that indicated by Yn, and tidy the trail up
to that point. Continue execution with
following instruction.

if B ? STACK[E + 2 + n]

then

begin

B  STACK[E + 2 + n];
tidy trail
end;
P  P + instruction size(P);

B.2 WAM ancillary operations

procedure backtrack;

begin

if B = bottom of stack

then fail and exit program
else

begin

B0  STACK[B + STACK[B] + 7];
P  STACK[B + STACK[B] + 4]
end
endbacktrack;

The backtrack operation

PAGE 112 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

function deref (a : address) : address;

begin

h tag ; value i  STORE[a];
if (tag = REF) ^ (value 6= a)

thenreturn deref (value)
elsereturn a
endderef ;

The deref operation

procedure bind(a1; a2 : address);

h t1 ; i  STORE[a1]; h t2 ; i  STORE[a2];
if (t1 = REF) ^ ((t2 6= REF) . (a2 ! a1))

then

begin

STORE[a1]  STORE[a2]; trail (a1)
end
else

begin

STORE[a2]  STORE[a1]; trail (a2)
end
endbind ;

The bind operation

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 113 OF 129

WARREN'S ABSTRACT MACHINE
procedure trail (a : address);

if (a ! HB) . ((H ! a) ^ (a ! B))

then

begin

TRAIL[TR]  a;
TR  TR + 1
end
endtrail;

The trail operation

procedure unwind trail(a1; a2 : address);

for i  a1 to a2 \Gamma  1 do

STORE[TRAIL[i]]  h REF ; TRAIL[i] i;
endunwind trail ;

The unwind trail operation

PAGE 114 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

procedure tidy trail;

i  STACK[B + STACK[B] + 5];
while i ! TR do

if (TRAIL[i] ! HB) . ((H ! TRAIL[i]) ^ (TRAIL[i] ! B))

then i  i + 1
else

begin

TRAIL[i]  TRAIL[TR \Gamma  1];
TR  TR \Gamma  1
end
endtidy trail ;

The tidy trail operation

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 115 OF 129

WARREN'S ABSTRACT MACHINE
procedure unify(a1; a2 : address);

push(a1; PDL); push(a2; PDL);
fail  false;
while :(empty (PDL) . fail) do

begin

d1  deref (pop(PDL)); d2  deref (pop(PDL));
if d1 6= d2 then

begin

h t1 ; v1 i  STORE[d1]; h t2 ; v2 i  STORE[d2];
if t1 = REF then bind(d1; d2)

else

case t2 of

REF : bind(d1; d2);
CON : fail  (t1 6= CON) . (v1 6= v2);
LIS : if t1 6= LIS then fail  true

else

begin

push(v1; PDL); push(v2; PDL);
push(v1 + 1; PDL); push(v2 + 1; PDL)
end;
STR : if t1 6= STR then fail  true

else

begin

f1=n1  STORE[v1]; f2=n2  STORE[v2];
if (f1 6= f2) . (n1 6= n2) then fail  true

else

for i  1 to n1 do

begin

push(v1 + i; PDL); push(v2 + i; PDL)
end
end
endcase
end
end
endunify;

The complete unify operation

PAGE 116 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
B.3 WAM memory layout and registers

Argument Registers:
A1; A2; : : : ; An; : : :
Registers:

PCP

S
HB
H

B0
B

E

TR

(low)
Code Area

Heap

Stack
choice point
environment

Trail

PDL

(high)

Yn nth local variable

...Y1
1st local variableCP

cont. codeCE
cont. environment

Environment frame:

B0 cut pointerH

heap pointerTR
trail pointerBP
next clause

B previous choice pt.CP

cont. codeCE

cont. environment
An nth argument

...A1
1st argument

n arity

Choice point frame:

@@

@@

@@

@

hhhhhhh
(((((((\Delta 

\Delta \Delta 

\Delta \Delta 

\Delta \Delta 

\Delta \Delta 

\Delta \Delta 

\Delta \Delta 

\Delta 

-
-
-
-

-
-
-

--

*+
+

+
+

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 117 OF 129

WARREN'S ABSTRACT MACHINE
PAGE 118 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI
A TUTORIAL RECONSTRUCTION
Bibliography
[AHU74] Alfred Aho, John Hopcroft, and Jeffrey Ullmann. The Design and

Analysis of Computer Algorithms. Addison-Wesley, Reading, MA,
1974.

[AK90] Hassan A"it-Kaci. The WAM: a (real) tutorial. PRL Research Report

Number 5, Digital Equipment Corporation, Paris Research Labora-tory, Rueil-Malmaison, France, 1990.

[Boi88] Patrice Boizumault. Prolog: l'implantation. Etudes et recherches en

informatique. Masson, Paris, France, 1988.

[BP84] Maurice Bruynooghe and Luis M. Pereira. Deduction revision byintelligent backtracking. In John A. Campbell, editor,

Implementations of Prolog, pages 194-215. Ellis Horwood, Ltd., Chichester, UK,1984.

[CCF88] Christian Codognet, Philippe Codognet, and Gilberto Fil`e. Yet another intelligent backtracking method. In Robert Kowalski and Ken-neth Bowen, editors,

Logic Programming: Proceedings of the Fifth
International Conference and Symposium, pages 447-465, Cam-bridge, MA, 1988. MIT Press.

[Chu90] Damian Chu. Private communicatiion. Electronic Mail, August 1990.
[CM84] William F. Clocksin and Christopher S. Mellish. Programming in

Prolog. Springer-Verlag, Berlin, Germany, 2nd edition, 1984.

[Cox84] Peter T. Cox. Finding backtrack points for intelligent backtracking. In

John A. Campbell, editor, Implementations of Prolog, pages 216-133.Ellis Horwood, Ltd., Chichester, UK, 1984.

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 119 OF 129

WARREN'S ABSTRACT MACHINE
[Deb86] Saumya K. Debray. Register allocation in a Prolog machine. In Proceedings of the Symposium on Logic Programming, pages 267-275.IEEE Computer Society, September 1986.

[Deb89] Saumya K. Debray. Static inference of modes and data dependencies

in logic programs. ACM Transactions on Programming Languages
and Systems, 11(3):418-450, July 1989.

[DW86a] Saumya K. Debray and David S. Warren. Automatic mode inferencefor Prolog programs. In

Proceedings of the Symposium on Logic Programming, pages 78-88. IEEE Computer Society, September 1986.

[DW86b] Saumya K. Debray and David S. Warren. Detection and optimiza-tion of functional computations in Prolog. In Ehud Shapiro, editor, Proceedings of the Third International Conference on Logic Programming, Berlin, Germany, 1986. Springer-Verlag. Lecture Notes in
Computer Science 225.

[DW89] Saumya K. Debray and David S. Warren. Functional computations inlogic programs.

ACM Transactions on Programming Languages and
Systems, 11(3):451-481, July 1989.

[GLLO85] John Gabriel, Tim Lindholm, Ewing Lusk, and Ross Overbeek. Atutorial on the Warren abstract machine. Privately circulated draft,

Argonne National Laboratory, Mathematics and Computer ScienceDivision, Argonne, IL 60439., 1985.

[Han90] Michael Hanus. Private communication. Technical correspondence,December 1990.
[Her71] Jacques Herbrand. Logical Writings. Harvard University Press, Cam-bridge, MA, 1971. Edited by Warren D. Goldfarb.
[JDM88] Gerda Janssens, Bart Demoen, and Andre Mari"en. Improving the reg-ister allocation in WAM by reordering unification. In Robert Kowalski and Kenneth Bowen, editors, Logic Programming: Proceedings of
the Fifth International Conference and Symposium, pages 1388-1402,Cambridge, MA, 1988. MIT Press.

[Kow79] Robert A. Kowalski. Logic for Problem Solving, volume 7 of Artificial

Intelligence Series. North Holland, New York, NY, 1979.

PAGE 120 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
[Kur87] Peter Kursawe. How to invent a Prolog machine. New Generation

Computing, 5:97-114, 1987.

[Mel85] Christopher S. Mellish. Some global optimizations for a Prolog com-piler.

Journal of Logic Programming, 1:143-166, 1985.

[MM82] Alberto Martelli and Ugo Montanari. An efficient unification algo-rithm.

ACM Transactions on Programming Languages and Systems,4(2):258-282, April 1982.

[MW88] David Maier and David S. Warren. Computing with Logic: Logic

Programming with Prolog. Benjamin/Cummings, Menlo Park, CA,1988.

[Per90] Fernando Pereira. Personal communication. Electronic mail, April1990.
[PP82] Luis M. Pereira and Antonio Porto. Selective backtracking. InKeith L. Clark and Sten- *Ake T"arnlund, editors,

Logic Programming,pages 107-114. Academic Press, New York, NY, 1982.

[Rob65] John A. Robinson. A machine-oriented logic based on the resolutionprinciple.

Journal of the ACM, 12:23-41, January 1965.

[Ros91] Dean Rosenzweig. Personal communication. Electronic mail, March

1991.

[Rus89] David M. Russinoff. A verified Prolog compiler for the Warren ab-stract machine. MCC Technical Report Number ACT-ST-292-89, Microelectronics and Computer Technology Corporation, Austin, TX,July 1989.

[vH90] Pascal van Hentenryck. Private communication. Electronic mail,September 1990.
[vR90] Peter L. van Roy. Can Logic Programming Execute as Fast as Imperative Programming? PhD thesis, University of California at Berkeley,
Berkeley, CA, December 1990.

[vRD90] Peter L. van Roy and Alvin Despain. The benefits of global dataflowanalysis for an optimizing Prolog compiler. In Saumya Debray and

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 121 OF 129

WARREN'S ABSTRACT MACHINE
Manuel Hermenegildo, editors, Logic Programming: Proceedings of
the 1990 North American Conference, pages 491-515, Cambridge,MA, 1990. MIT Press.

[War83] David H. D. Warren. An abstract Prolog instruction set. TechnicalNote 309, SRI International, Menlo Park, CA, October 1983.
[War88] David H. D. Warren. Implementation of Prolog. Lecture notes, Tu-torial No. 3, 5th International Conference and Symposium on Logic

Programming, Seattle, WA, August 1988.
[War89] David H. D. Warren. Private communication. Electronic mail, Octo-ber 1989.

[War90] David H. D. Warren. Private communication. Electronic mail,September 1990.
[Zho90] Neng-Fa Zhou. Backtracking Optimizations in Compiled Prolog. PhDthesis, Kyushu University, Fukuoka, Japan, November 1990.
[ZTU90] Neng-Fa Zhou, T. Takagi, and K. Ushijima. A matching tree ori-ented abstract machine for prolog. In David H. D. Warren and Peter

Szeredi, editors, Logic Programming: Proceedings of the Seventh International Conference, pages 159-173, Cambridge, MA, 1990. MITPress.

PAGE 122 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
Index
A register, 22
allocate, 28

example, 31, 43, 55, 57, 61, 64,72-74

explicit definition

for M2, 30
for M3, 38
for WAM, 60, 106
AND-stack, 33
argument registers, 19, 22
arity, 9

B register, 34
backtrack operation, 39
backtrack operation, 112
backtracking, 6, 19, 25, 36

chronological, 33
bind operation, 113
binding, 10, 16, 17

conditional, 37, 63
convention, 62
rule, 63, 64, 68, 69
side-effect, 25, 36
trailing, 37
unconditional, 63
B0 register, 83
Boizumault, Patrice, 4

call, 21, 59

example

with environment trimming, 61,

64, 68, 72-74, 85
without environment trimming,24, 31, 43, 51, 55, 57

explicit definition

for M1, 21
for M2, 26
for M3, 40
for WAM, 106
chain

reference, 16, 63, 65
rule, 25, 57, 86
choice point, 33

contents, 36
discarding, 34, 36, 37, 39, 83-85
Chu, Damian, 71
code area, 21, 26, 28, 36, 39
constant, 9

cell, 48
CP register, 26
cut, 83

deep, 83, 85
neck, 84
semantics, 83
shallow, 83, 84
cut, 85

explicit definition, 112
cut register, 83

deallocate, 29
Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 123 OF 129

WARREN'S ABSTRACT MACHINE
example, 31, 43, 55, 57, 61, 64,72-74
explicit definitionfor

M2, 30
for WAM, 56, 106deep

cut, 83, 85rule, 25, 86
delayed trimming, 73
deref operation, 113
dereferencing, 16

E register, 28environment, 27

allocation, 29
discarding, 56frame, 28

protection, 34, 35, 57, 79trimming, 58-62, 64, 68, 70-74,

85
magnification of, 79
execute, 56example, 57, 61, 64, 72-74, 80-

82explicit definition, 56
for WAM, 107
fact, 21compilation, 22, 26

failure, 16, 19, 25, 33-35flattened form, 12

of program, 15of query, 12
tokenized, 12, 15functor, 9

get constant, 48

example, 80-82explicit definition, 49

for WAM, 102
get level, 85explicit definition

for WAM, 111
get list, 50example, 51, 82

explicit definition, 50for

WAM, 102
get nil, 50
get structure, 15example, 16, 24, 43, 51, 53, 80,

81
explicit definition, 18for

WAM, 101
get value, 23

example, 24, 51explicit definition, 23

for WAM, 101
get variable, 23example, 31, 43, 53, 55, 57, 61,

64
explicit definition, 23for

WAM, 100

H register, 13
Hanus, Michael, 67
HB register, 37heap, 10

pointer, 38
I0, 9, 48, 50
I1, 28
I2, 28, 38
I3, 38

Kowalski, Robert A., 95
L0, 9, 21

abstract machine, 9

PAGE 124 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION

interpreter, 9program, 9

compiling, 13, 16query, 9
compiling, 11, 14semantics, 9, 11
syntax, 9
term representation, 10
L1, 21, 24, 25
L2, 25program, 25

query, 25semantics, 25
L3, 33program, 33

query, 33semantics, 33
last call optimization, 56
LCO, 56, 57, 62, 70, 71correctness, 58

effect, 56, 57, 82generalization, 58
interference, 62magnification, 79
patch, 75list, 50

M0, 9, 11, 19, 75

instructions, 13, 16for programs, 18

for queries, 14
M1, 21augmentation, 28

instructionsfor variable arguments, 23
M2, 28, 40, 42extension, 33
M3, 33, 42

improvement, 45instructions

for choice point, 41, 42Maier, David, 4
mode, 15, 16x

read, 15x
write, 15setting, 17

neck cut, 84

explicit definitionfor

WAM, 111

OR-stack, 33
P register, 21
partial evaluation, 89
PDL, 19, 36Pereira, Fernando, 46

permanent variable, 27, 28
proceed, 21

example, 24, 43, 51, 53, 81, 82explicit definition, 26

for WAM, 107push-down list, 19
put constant, 48

example, 81explicit definition, 49

for WAM, 100
put list, 50example, 51

explicit definition, 50

for WAM, 100
put nil, 50
put structure, 13example, 14, 24, 43, 51

explicit definition, 14, 46

for WAM, 100
put unsafe value, 65

Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 125 OF 129

WARREN'S ABSTRACT MACHINE
example, 72-74
explicit definition, 66for

WAM, 100
put value, 23

example, 31, 43, 55, 57, 61, 64,72

explicit definition, 23for

WAM, 99
put variable, 23

example, 24, 31, 51, 55, 57, 61,64, 72-74

explicit definition, 62

for WAM, 99for a temporary variable, 23

query, 9

compilation, 11, 22, 27
empty, 25

reference

chain, 16, 63, 65
chronological, 63
dangling, 62, 63stack, 62, 63, 65, 67, 68

register allocation, 12, 54
resolution

flat, 6, 25leftmost, 25, 33

SLD, 5, 33
top-down, 33
retry, 79explicit definition

for WAM, 110
retry me else, 39

example, 43, 80-82explicit definition, 41

for WAM, 108
Robinson, John Alan, 95

Rosenzweig, Dean, 83, 84, 86rule, 25

binding, 64, 68, 69chain, 25, 57, 86
compilation, 27deep, 25, 86
Russinoff, David, 5
S register, 16
set constant, 48example, 51

explicit definition, 49for

WAM, 103
set local value, 68explicit definition, 69

for WAM, 103
set value, 13

example, 14, 24, 51explicit definition, 14

for WAM, 102
set variable, 13example, 14, 24, 51

explicit definition, 14for

WAM, 102
set void, 52explicit definition, 53

for WAM, 103shallow cut, 83, 84
signature, 91stack, 28

frame, 28
global, 13local, 28

structure, 9cell, 10
substitution, 19, 91application, 19, 94

composition, 19, 92
PAGE 126 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
switch on constant, 78example, 80, 81

explicit definitionfor

WAM, 111
switch on structure, 79

example, 80, 81explicit definition

for WAM, 111
switch on term, 78

example, 80-82explicit definition

for WAM, 110
term

L0 program, 9
L0 query, 9
first-order, 9, 91representation, 10

tidy trail operation, 85
tidy trail operation, 115
TR register, 37trail, 36

cleanup, 85pointer, 37
tamping, 85
unwinding, 40
trail operation, 114
trailingunnecessary, 62

variable, 37, 62trimming
delayed, 73environment, 58, 59, 61, 64, 68,

71-74, 85
trust, 79

example, 80, 81explicit definition

for WAM, 110

trust me, 39example, 43, 80-82

explicit definition, 42for

WAM, 109
try, 79example, 80, 81

explicit definition

for WAM, 109
try me else, 39

example, 43, 80-82explicit definition

for M3, 41for

WAM, 108

unbound variable, 10
unification, 9, 92algorithm, 19, 93

unify operationfor

M0, 20
for WAM, 116
unify constant, 47example, 51

explicit definition, 49for

WAM, 105
unify local value, 68explicit definition, 69

for WAM, 104
unify value, 16

example, 16, 82explicit definition, 18

for WAM, 104
unify variable, 15example, 16, 24, 51, 80-82

explicit definition, 18for

WAM, 104
unify void, 52example, 53, 80

explicit definition, 53
Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 127 OF 129

WARREN'S ABSTRACT MACHINE
for WAM, 105
unwind trail operation, 114

van Hentenryck, Pascal, 67
variable, 9anonymous, 47

instructions, 53
binding, 10, 16, 62
cell, 10classification, 69

Warren's, 70
cut, 85
global, 70globalizing, 67-69, 71

local, 28, 60, 70
permanent, 27, 28, 69discarding, 62, 64, 66-68

ordering, 63
register, 11
temporary, 27, 69unsafe, 64

WAM, 3

clause treatment, 89complete, 3, 6

correctness, 5
designer, 4features, 4

publication, 3
simplification, 5
techniques, 4unification, 89

variant, 4
WAM Binding Rule, 63, 64, 68, 69
WAM Principle, 45, 47, 60, 79Warren

Abstract Machine, 3
David H. D., 3, 45, 89, 90

David S., 4
X register, 11

PAGE 128 OF 129 Reprinted from MIT Press Copyright cfl Hassan A"IT-KACI

A TUTORIAL RECONSTRUCTION
Copyright cfl Hassan A"IT-KACI Reprinted from MIT Press PAGE 129 OF 129