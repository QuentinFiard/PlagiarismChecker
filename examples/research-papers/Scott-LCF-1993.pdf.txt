

Theoretical  Computer  Science  121  (1993)  41 l-440 

Elsevier 

411 

A  type-theoretical  alternative  to ISWIM,  CUCH,  OWHY 

Dana  S.  Scott 

Carnegie-Mellon  University.  Pittsburgh,  PA,  USA,  and  RISC-Linz,  Austria 

Abstract 

Scott,  D.S.,  A  type-theoretical  alternative  to  ISWIM,  CUCH,  OWHY,  Theoretical  Computer 

Science  121  (1993)  411-440. 

The  paper  (first  written  in  1969  and  circulated  privately)  concerns  the  definition,  axiomatization,  and 
applications  of  the  hereditarily  monotone  and  continuous  functionals  generated  from  the  integers 
and  the  Booleans  (plus  "undefined"  elements).  The  system  is  formulated  as  a  typed  system  of 
combinators  (or  as  a  typed  I-calculus)  with  a  recursion  operator  (the  least  fixed-point  operator),  and 
its  proof  rules  are  contrasted  to  a  certain  extent  with  those  of  the  untyped  d-calculus.  For 

publication  (1993),  a  new  preface  has  been  added,  and  many  bibliographical  references  and  com- 
ments  in  footnotes  have  been  appended. 

Preface  (1993) 

The  main  part  of  the  text  of  this  paper  was  written  in  England  in  October,  1969, 
mid-way  through  the  term  the  author  spent  on  leave  from  Princeton  University 

visiting  Professor  Christopher  Strachy  and  his  Programming  Research  Group  at 
Oxford  University.  The  preparation  of  this  paper  for  its  long-delayed  publication  has 
been  done  while  the  author  was  on  sabbatical  leave  from  Carnegie  Mellon  University 
visiting  Professor  Bruno  Buchberger  at  his  Research  Institute  for  Symbolic  Computa- 

tion  at  the  Johannes  Kepler  University,  Linz,  Austria.  The  author  is  very  much 
indebted  not  only  to  the  universities  mentioned  for  these  various  opportunities  to  take 
leave  and  to  enjoy  hospitality  at  the  places  visited,  but  also  to  Todd  and  Mary  Wilson 
and  Kim  Wagner  for  typing  the  manuscript  from  a  very  old  photocopy  of a  typescript, 
for  writing  and  fixing  the  necessary  TEX  macros  to  typeset  the  new  version  as 

a  report,  and  for  helping  assemble  the  bibliography. 

Correspondence  to:  D.S.  Scott,  Computer  Science,  Carnegie-Mellon  University,  Pittsburgh,  PA  15213- 
3890,  USA.  Email:  dana.scott@cs.cmu.edu. 

0304-3975/93/$06.00  0  1993-Elsevier  Science  Publishers  B.V.  All  rights  reserved 

412  D.S.  Scott 

The  author  is  also  much  indebted  to  the  editors  of  this  volume  for  the  welcome 
suggestion  that  such  "historical"  papers  might  be  published  this  way.  This  particular 
paper  has, of course,  an  odd  historical  role: in it the  author  argues  against  the  type-free 
calculi  of  Church  and  Curry,  Kleene  and  Rosser,  and  their  later  uses  by  Bohm  and 
Strachey.  But  then  in  November  of  1969, after  writing  this  report,  the  author  himself 
saw that  the  method  of monotone  continuous  functions  (which  grew  out  of traditional 
recursive  function  theory  in  discussing  certain  kinds  of  functionals)  could  be  applied 
to  posets  other  than  just  those  generated  from  the  integers  (with  bottom)  by  the  very 
simple  type  constructors.  In  particular,  it was recognized  that  there  were posets  whose 
continuous  function  spaces  of  self-maps  could  be  identified  with  the  given  posets 
themselves.  And  so  there  actually  were  "traditional"  models  of  il-calculus  that  could 
be defined  without  first  going  through  the  proof  theory  of the  formal  system  itself (and 
which  could  be  related  to  many  other  mathematically  meaningful  structures  and 

theories  as  well). 

This  revelation  was  both  gratifying  and  embarrassing.  After  writing  with  tiresome 
sarcasm  about  the  lack  of  meaning  in  the  type-free  I-calculus  introduced  only 
formally,  the  author  himself  found  an  interesting  kind  of semantical  interpretation  for 

the  "type-free"  language.  This  total  shift  of  gears  is the  reason  the  present  paper  was 
not  published:  the  foundational  program  being  advocated  had  apparently  been 
completely  outmoded  by  the  discovery  of  the  more  general  lattice-theoretic  models. 

However,  the  axiomatic  program  laid  out  here  had  much  to  recommend  it, and  it was 
continued  and  extended  in  many  directions  by  Milner,  Plotkin,  and  many  others,  to 

whom  the  paper  had  been  circulated  privately  -  often  at  nth  hand.  Gunter  in his book 

[9,  p.  1431 remarks,  "The  language  PCF  itself  was  introduced  by  Scott  in  what  is 
probably  the  most  well-known  unpublished  manuscript  in  Programming  Language 
Theory".  That  exaggerates  history  somewhat,  but  the  proof  system  proposed  was  in 
fact  one  of  the  main  motivations  for  Milner  to  make  automated  proofs  for  such 

axiomatics  (see  [S,  p.  1533). And  this  project  in  turn  led  directly  to  the  definition, 
design  and  implementation  of  the  programming  language  ML  -  a  very  important 
event  in  the  history  of computer  languages,  since  ML  has  since  prospered  and  taken 
on  a role,  importance  and  life of its own  never  dreamed  of in the mid-1970s.  Moreover, 
the  completeness  problem  for  the  theory  of the  type  system  of this  paper  turned  out  to 
be far  more  delicate  than  was initially  apparent  (see the  historical  introduction  to  the 
Kahn-Plotkin  paper  in  this  volume). 

On  the  other  hand,  the  type-theoretical  approach  has  not  died  out  at  all, because  it 
has  been  taken  over  and  absorbed  into  the  applications  of  category  theory  to 

semantics  and  computation  theory.  The  author  is  fond  of  saying  that  a  category 
represents  the  "algebra  of  types",  just  as  abstract  rings  give  us  the  algebra  of 
polynomials,  originally  understood  to  concern  only  integers  or  rationals.  One  can  of 
course  think  only  of  particular  type  systems,  but,  for  a full  understanding,  one  really 
needs  also  to  take  into  account  the  general  theory  of types,  and  especially  translations 
or  interpretations  of one  system  in another.  Category  theory  together  with  the  notion 
of  functor  and  natural  transformation  between  functors  has  been  proved  over  and 

A  type-theoretical  alternative  to  IS WIM,  CUCH,  0 WHY  413 
over  again  in  the  last  half-century  to  be  the  appropriate  way  to  go  about  these  studies. 
The  author  himself  does  not  always  like  or  enjoy  the  discipline  of  category  theory, 

which  seems  ofttimes  to  carry  along  very,  very  heavy  machinery  and  odd  terminology, 
but  he  long  ago  came  to  the  conclusion  that  it  is  quite  unavoidable.  The  extremely 

active  current  research  in  semantics  also  shows  that  it  is  an  especially  fruitful  way  to 

think.  The  book  of  Gunter  [9]  with  its  wide-ranging  historical  comments  and 
references  is  adequate  proof  of  this  assertion. 

The  strange  title  of  this  paper  ought  perhaps  to  be  explained.  In  1966,  Landin 
published  an  influential  paper  [14]  which  introduced  a  syntactical  design  style  for 

programming  languages,  one  of  which  he  called  ISWIM,  standing  for  "If  you  See 
What  I  Mean".  Also  Biihm  in  1966  published  the  paper  [3]  which  named  a  language 

of  combinators  called  CUCH,  standing  for  "Curry-Church".  There  seemed  to  be 
a  worrisome  trend  in  funny  acronyms  starting  here  (of  which  perhaps  the  ultimate 
examplar  is  the  well-known  and  very  widely  used  editing/programming  interface 
called  GNU,  recursively  standing  for  "GNU  is  NOT  Unix").  The  author  hoped  to 

stop  some  proliferation  by  suggesting  a  return  to  the  logically  standard  type-theoret- 
ical  framework  and  thereby  deter  the  creation  of  programming  languages  of  doubtful 
foundation  called  (as  a  group)  OWHY,  standing  for  "Or  What  Have  You."  No  one 
really  understood  the  joke,  and  the  effort  was  doomed  to  be  of  no  avail.  And  history 
proved  the  author  to  be  too  conservative  in  any  case. 

In  the  body  of  the  paper  footnotes  giving  relevant  comments  and  some  references 
have  been  added  for  this  publication.  A  brief  afterthought  has  been  added  as  a  last 
section.  Some  comments  in  the  original  text  have  also  been  transposed  to  footnotes  to 
help  readability.  Several  editorial  changes  and  corrections  were  incorporated,  but  the 
original  text  has  essentially  been  preserved.  The  bibliography  is to  be  found  at  the  end 

of  the  paper. 

0.  Introduction  (1969) 

No  matter  how  much  wishful  thinking  we  do,  the  theory  of  types  is  here  to  stay. 
There  is  no  other  way  to  make  sense  of  the  foundations  of  mathematics.  Russell  (with 
the  help  of  Ramsey)  had  the  right  idea,  and  Curry  and  Quine  are  very  lucky  that  their 

unmotivated  formalistic  systems  are  not  inconsistent.'  This  is  not  to  disparage 
formalistic  work.  In  my  view  it  is  only  through  formalism  that  we  can  find  a  clear  idea 

of the  scope  of mathematical  knowledge.  And  I  freely  admit  that  one's  research  may  be 
advanced  by  some  purely  formalistic  play  with  symbols.  My  point  is  that  formalism 

without  eventual  interpretation  is in  the  end  useless.  Now,  it  may  turn  out  that  a  system 
such  as  the  ;l-calculus  will  have  an  interpretation  along  standard  lines  (and  I  have 

spent  more  days  than  I  care  to  remember  trying  to  find  one),  but  until  it  is  produced 

' The  author  still  believes  this  statement. 

414  D.S.  Scott 

I  would  like  to  argue  that  its  purposes  can  just  as  well  be  fulfilled  by  a  system 
involving  types.  Indeed,  as far  as proofs  are  concerned,  the  system  with  types  seems  to 
be  much  better.2 

It is a pity  that  a system  such  as Zermelo-Fraenkel  set theory  is usually  presented  in 
a purely  formal  way,  because  the  conception  behind  it is quite  straightforwardly  based 
on  type  theory.  One  has  the  concept  of an  arbitrary  subset  of a giuen  domain  and  that 
the  collection  of  all  subsets  of  the  given  domain  can  form  a  new  domain  (of  the  next 
type!).  Starting  with  a  domain  of  individuals  (possibly  empty),  this  process  of 
forming  subsets  is  then  iterated  into  the  transfinite.  Thus,  each  set  has  a  type 

(or  rank)  given  by  the  ordinal  number  of  the  stage  at  which  it  is  first  to  be  found 
in  the  iteration.  One  advantage  of  this  method  is  that  the  types  are  built  into  the 
sets  themselves  and  need  not  be  made  manifest  in  the  formalism.  (Computer  people 
might  say that  the  type  checking  in set theory  is done  at  runtime  rather  than  at  compile 

time.)  One  disadvantage  is  that  people  tend  to  forget  what  is  out  of  sight.  But  it  is 
there,  and  one  can  make  quite  clear  what  is  the  type-theoretic  background  of  set 

theory.3 

For  the  purposes  of  understanding  computation,  however,  set-theoretical  formal- 
ism  is  not  too  helpful  in  any  direct  way.  In  the  first  place,  too  much  of  set  theory 
concerns  the  transfinite,  and  ordinary  computation  has  rather  to  do  with  finite 

processes.  In  the  second  place  the  axioms  of  set  theory  are  meant  to  capture 
something  essential  of  the  idea  of  an  arbitrary  subset,  while  computation  theory  is 
more  interested  in  the  notion  of  an  algorithmically  dejined  subset  (or  function).  Of 
course,  one  can  define  in  set  theory  such  notions  as  that  of  a  general  recursive 
function,  but  such  definitions  do  not  emphasize  enough  what  is  special  about  algo- 
rithms.  Nor  is it generally  clear  when  a defined  function  is recursive.  So what  we want 
is a "restricted"  system  that  is specially  designed  for  algorithms.4  What  I shall  present 
below  is an independent  system  with  its own  axioms  and  rules; but,  since I observe  the 
canons  of type  theory,  it can  be (and  indeed  must  be) read  as a fragment  of set  theory 

so that  its  theorems  can  be  recognized  as  uulid.  This  is the  main  feature  missing  from 
the  A-calculus. Now  I have  only  thought  of this  system  in  the  last  few days,  so it  may 
still  be  imperfect.5  However,  none  of  it  is  wrong  (as  will  be  seen  from  the  simple 
character  of the  system),  and  it does  seem  to  do  in a much  better  way  what  I discussed 

r This statement  remarkably  remains  true!  Even  with  the  subsequent  20-year  development  of  the  theory 
of domains,  the  proof  principles  for  recursively  defined  (or  reflexive)  domains  are  still  being  discovered.  This 
can  be  well  appreciated  by  reading  the  two  very  recent  papers  of  Pitts  [18,  191. a Though 

set  theory,  and  especially  ZF  set  theory,  underwent  a  truly  vast  development  in  the  last  30 
years,  the  common  understanding  of  the  type  structure  is  probably  not  yet  fully  appreciated  -  to  judge  by 
the  many  arguments  the  author  has  had  with  category  theorists. 

4The  subsequent  development  of  intuitionistic  ZF  and  the  expansion  of  realizability  interpretations  has 
completely  changed  this  position.  Unfortunately,  the  axiomatics  of  "synthetic  domain  theory"  have  not 
been  completely  clarified  so  that  a  convenient  foundation  for  computation  theory  and  semantics  can  be 
given  in  set-theoretic  terms  (see  [12,  251  and  the  references  therein). 

SThe  report  was  written  at  one  sitting  over  a  very  short  period  and  never  revised. 

A  type-theoretical  alternative  to  IS WIM,  CVCH,  0 WHY  415 
as  the  "algebraic  theory  of  procedures"  in  my  talk  on  the  last  day  of  the  W.G.  2.2 
meeting  in  Essex.6 

1. Types 

The  first  confusion  we  should  avoid  is  that  between  logical  types  and  what  we  might 
call  data  types.  The  former  are  what  we  invoke  to  study  the  latter.  The  theory  of  data 
types  requires  the  logical  types  (and  certain  notions  about  objects  of these  types)  for  its 
formalization.  For  example,  the  idea  of  the  set  of  all  subsets  of  a  set  is  a  logical  notion 
(or  mathematical  notion,  if  you  prefer),  because  neither  it,  nor  for  that  matter  the 

"general"  set,  can  ever  present  itself  as  an  object  of data  -  in  any  ordinary  sense  of that 
word.  However,  set-theoretical  notions  can  be  quite  useful  when,  for  instance,  we  wish 

to  say  that  the  set  of  all  data  (of  a  certain  kind)  has  no  proper  subset  with  a  particular 
property  (say,  of  being  closed  under  a  specific  operation).  I  like  to  imagine  the  data  at 

the  lowest  logical  type  being  structured  by  certain  (fixed)  relations  and  functions 
(objects  of a higher  logical  type)  and  the  theory  of these  allowing  reference  to  (variable) 

objects  of  all  the  higher  types  -  as  in  the  example  mentioned  above.7 

Obviously,  for  a  good  theory  we  want  to  be  able  to  sort  out  data  into  different 
categories  (types?),  and  (this  is  where  the  confusion  begins)  we  want  to  study  many 

derived  types.  As  a  simple  example  we  could  think  of  persons  as  forming  a  basic  type 
and  organizations  as  forming  a  derived  type.  By  an  organization  I  suppose  we  all 
understand  a  sort  of  tree  of  persons.  It  has  a  head (a  person)  and  some  brunches  which 
give  the  "chain  of  command".  Well,  there  is  no  confusion  here;  where  the  problem  lies 

is  in  this:  the  objects  of  the  various  logical  types  are  richly  structured.  As  we  all  know, 
the  theory  of  trees  (of  persons,  say)  can  easily  be  simulated  or  modelled  by  objects  of 
higher  logical  types  (oh,  a  "mathematical"  tree  is  an  ordered  pair  of a  set  together  with 
a  relation  that  partially  orders  the  set  -  in  a  special  way).  Now  is  this  a  good  thing? 

Sometimes  yes,  sometimes  no.  If  we  think  of  organizations  as  occurring  in  "nature", 
then  we  do  not  want  to  identify  them  with  the  mathematical  models.  It  is  like  the 
distinction  between  abstract  and  concrete  syntax:  there  is  not  always  a good  reason  to 
consider  an  expression  to  be  a  string  (sequence  mathematically?)  of  symbols.  Why? 

Because  the  string-oriented  methods  of  breaking  an  "expression"  into  "parts"  might 
not  lead  to  the  correct  or  useful  notion  of  part.  Similarly  with  organizations. 

To  put  the  point  in  another  way:  it  seems  best  to  allow  ourselves  the  freedom  of 
keeping  our  data  types  as  primitives.  Computers  have  taught  us  this.  Think  of  the 
numbers.  There  is  no  unique  representation  of numbers,  and  we  do  not  want  to  choose 
a particular  one.  All  we  really  want  to  say  is that  they  form  one data  type  and  that  they 

6 The  meeting  was  in  the  summer  of  1969,  and  the  method  discussed  there  was  from  the  long-unpublished 
de  Bakker-Scott  paper  [6]. 

`And  consider  the  subsequent  development  of  "concrete  domains"  by  many  authors  after  the 
Kahn-Plotkin  report. 

416  D.S.  Scott 

have  a  certain  structure.  (I  suppose  they  can  be  added,  say,  and  can  be  tested  in  pairs 
for  equality  or  order.)  Indeed  we  may  require  several  different  types  of numbers.  In  our 
theory  about  these  numbers  we  very  much  want  the  results  to  be  "machine  indepen- 
dent".  That  is,  the  theorems  ought  to  be  valid  for  all  representations  -  satisfying 
certain  explicit  structural  conditions.  (Equals  added  to  equals  remain  equal?) 

This  attitude  was  somewhat  obscured  by  Russell's  approach  to  mathematics:  he 
wanted  to  reduce  mathematics  to  logic.  How?  By  dejning  number.  (The  number  two is 

the  set  of  all  two-element  sets  -  be  careful  of  type  distinctions  here!)  The  program  is 
not  quite  successful.  Why?  Because  one  must  still  postulate  the  existence  of  an  infinite 

set  (a data  type?)  in  order  to  have  "enough"  numbers.  Set  theory  gets  around  this  point 
by  going  to  transfnite  types! It  is  not  exactly  cheating,  but  it  is  not  really  satisfactory 
either.  For  our  purposes  it  is  much  better  to  give  a  theory  (theories)  of  number  rather 

than  definitions  of number.  And  the  same  applies  to  the  other  data  types.  What  we  are 
going  to  do  is  to  take  what  we  need  from  Russell's  logic  without  taking  over  his 
philosophy  of  mathematics. 

Actually,  it  is  fairer  to  say  that  we  are  stealing  from  Church's  logic  rather  than 
Russell's,  because  my  notation  is  closer  to  that  of  Church.8  The  reason  is  that  for 
algorithms  it  is  more  natural  to  considerfunctions  rather  than  sets. We  can  reduce  the 
notion  of function  to  that  of set,  but  it  is  not convenient  to  do  so.  A  much  better  plan  is 
to  treat  sets  (and  relations)  as  special  functions  (truth-valued)  as  Church  does. 

To  start  with  we  have  two  "logical"  types  represented  by  the  Greek  letters  1 and  o in 
Church's  notation.  The  first,  z, stands  for  the  type  of  all  indiuiduals. I  consider  it  to  be 
the  largest  data type; all  other  data  types  are  to  be  "included"  in  it  -  hence  it  is  clearly 
a  logical  notion.  (The  exact  way  of  treating  the  other  data  types  will  be  discussed 
below.)  The  second,  o, is  the  type  of  the  truth values -  a  very  logical  notion.  These  are 

the  two  logical  types  of the  lowest  order.  The  higher  types  are  represented  as  follows:  If 
a and  p  are  types,  then  so  is (cl+fi).  What  (u-+/?)  represents  is  the  type  offunctionsfrom 

objects  of  type  a  to  those  of  type  b.  Again  a  logical  construct. 

To  be  more  precise,  we  must  distinguish  between  type  symbols  and  the  (sets  of) 
objects  of  the  corresponding  type.  Type  symbols  are  strings  of  "L", "o",  "+"  "(",  and 
")".  From  my  informal  remarks  in  the  last  paragraph,  anyone  can  write  down  the 
"grammar"  for  this  (context-free)  "language",  and  I  shall  not  bother  to  do  so.  Aside 
from  this  collection  of  type  symbols,  we  will  also  have  a  rich  language  of  expressions. 

Each  expression  will  have  a  (unique)  type,  and  I  shall  write  X:  tl  to  mean  that  the 
expression  X  is  of  type  01. Again  we  must  be  careful  to  distinguish  the  expressions  from 

what  objects  they  denote.  More  on  this  later. 

In  the  first  place  as  expressions  we  shall  allow,  for  each  type  c(, an  infinite  list  of 
variables  denoted  as  follows  by  lower-case  letters: 

a,,b,,  . . . . x,,y,,z,,a&,...,zh,a~,... 

8 This  goes  back  to  the  well-known  paper  [4]. 

A  type-theoretical  alternative  to  IS  WlM,  CUCH,  0  WHY  417 
(I suppose  I should  do  all  this  in  abstract  syntax  because  no  one  really  cares  what  my 

variables  look  like.  But  I  don't  have  time  to  fuss.) Those  subscripted  variables  are  in 
the  object  language  of expressions.  In  my  metalanguuge  I use  unsubscripted  x, y, z, etc. 

to  range  over  variables  of  any  type.  The  unsubscripted  capital  letters  range  over 
compound  expressions. 

The  compounds  are  made  from  the  variables  and  constants  by  a  certain  rule.  The 
constants  are  divided  into  two  main  classes:  the  logical  and  nonlogical  constants. 

I  choose  not  to  discuss  the  latter  at  the  moment  -  just  remember  to  save  room  for 
them.  And  remember  in general  that  any  expression  has  its type  -which  must  be given 
in the  case  of constants  (and  variables).  The  logical  constants  are  infinite  in number  (so 

are  the  types);  I  list  them  along  with  their  types  (a,/?, y  are  arbitrary  type  symbols): 

52,:1,  R,:o,  T  :o,  I  :o, 

~~:(o+(cI+(c(+a))), 
Kx,  : (a+@-,  E)), 
S=,,:((CI~(B~Y))j((a-,8)~(a~r))), 
Y,:  ((ct-m)+cr). 

One  can  probably  guess what  they  mean,  but  I leave that  discussion  for  the next  section.' 

As for  the  other  expressions,  suppose  X:  c1 and  F : (a+&;  then  we  have 

F(X)  : /?. 
That  is  to  say  the  standard  function-value  notation  is  the  only  way  in  which 
compounds  can  be made.  Note  that  the  type  of X  must&  the  type  of F  for  F(X)  to  be 

well formed.  Note,  too,  how  complicated  the  types  of our  constants  are.  In  particular, 
it is possible  to  form  an  expression  of any  type  using  only  constants  and  no  variables. 

By  a formula,  we  understand  either  an  atomic  formula  of  the  form 

x<  Y, 
where  X  and  Y are  expressions,  or  a  list 

(possibly  empty!)  of  atomic  formulae  @i. We  identify  the  one-termed  list  with  the 

atomic  formula.`O  (S' mce  my  natural  syntax  is string-like,  note  that  in view  of the  fact 
that  the  concatenation  of  expressions  is  never  an  expression,  we  can  write  a  list 
of  atomic  formulae  without  commas.  Let  us  not  worry  about  such  small  points, 
however.)  If  Y  and  @ are  two  lists,  then  (in  the  metalanguage!) 

`Kleene  and  many  other  logicians  had  used  typed  combinators,  but  the  languages  with  the  typed 
fixed-point  combinator  had  not  been  considered  all  that  much  in  1969  except  by  Platek  in  his  thesis  [20]. 

lo The  lists  of  atomic  formulae  are  really  conjunctions,  as  is  explained  later. 

418  D.S.  Scott 
means  that  every  atomic  formula  in  @ also  occurs  in  Y. (The  symbol  "G"  is a symbol 

of  the  object  language;  similarly  for  "k".) 

By  an  assertion,  we  understand  a  string  of  the  form 

where  @ and  Y  are  lists  of  atomic  formulae.  Intuitively  lists  of  formulae  are  just 
conjunctions  and  I-  gives  an  implication  between  conjunctions  -  but  this  will  all  be 
clear  when  we find  out  in  the  next  section  the  meanings  of all  our  symbols.  After  that 
we will discuss  axioms  and  rules  ofinference  for  generating  the  (or  a good  part  of the) 
valid  assertions  in  an  "algebraic"  way. 

If  @ is  a  list  and  F  is  an  expression  of  the  same  type  as  a  variable  x,  then 

@[F/xl 
denotes  the  result  of  substituting  F  for  x  throughout  @. 

We  need  a  few  abbreviations.  If  X  and  Y are  expressions  of  the  same  type,  then 

X=Y 
stands  for  the  list 

x<  Y,  Y<X. 
If  F  is  an  expression,  and  if F(X)(Y)(Z)  is  well  formed,  then  we  abbreviate  this  as 

F(X,  Y, Z). 
Similarly,  for  more  than  three  terms.  (Note  this  abbreviation  is  a  convention  in  the 
metalanguage  and  is not  -  at  the  moment  -  "sugaring"  in  the  object  language.)  Also, 

we have  only  Q, and  52, at  the  lowest  type.  For  higher  types  we define  Q,a_8) to  be the 
expression: 

K+&%):(~+P). 
The  notation 

is not  common.  We  usually  write 

(B-+X,  Y), 
but  we  also  need  1a  for  general  purposes." 

r1 Subsequently,  the  author's  notation  changed  because  it  was  confusing  (he  felt)  to  use  either  $  or  c  for 
the  information  ordering  within  a  domain.  He  thus  adopted  the  "square"  notation  of  E  from  lattice  theory. 
This  brought  along  n  and  L.  and  took  I  for  the  bottom  element  instead  of  .Q. It  still  seems  to  the  author 

a better  notation,  but  too  many  people  prefer  to  write  the  easier  <.  However,  in  a  system  that  might  involve 
integers,  sets  (say  as  elements  of  power  domains),  and  the  information  ordering,  it  seems  cleaner  to  have 
different  symbols  for  different  notions. 

A  type-theoretical  alternative to  IS WIM,  CUCH,  0 WHY  419 
2.  Interpretation 

The  classical  way  of  viewing  the  theory  of  types  is  to  assign  to  each  type  u  a  domain 
D,,  where  D,  is  a  given  domain  of  individuals,  D,  is  the  domain  of  two  truth  values 
(denoted  by  T  and  I  for  true  andfalse),  and  each  Dtn+Dj is  the  domain  of all functions 
from  D,  with  values  in  D,  . This  point  of  view  is  not  convenient  for  our  purposes.  The 

reason  is  simple:  classical  type  theory  supposes  total  (everywhere  defined)  functions, 
while  algorithms  in  general  produce  partial  functions.  We  do  not  wish  to  reject 

a  program  if  the  function  defined  is  partial  -  because  as  everyone  knows  it  is  not 
possible  to  predict  which  programs  will  "loop"  and  which  will  define  total  functions. 

The  solution  to  this  problem  of  total  versus  partial  functions  is  to  make  a  "math- 
ematical  model"  for  the  theory  of partial  functions  using  ordinary  total  functions.  The 
idea  is  not  at  all  original  to  the  author  (he  has  taken  it  from  more  "standard"  versions 
of  recursive  function  theory  -  in  particular,  from  the  thesis  of  Platek  [20]).  Other 
authors  in  recursive  function  theory  discussed  monotone  and  "hereditarily  consistent" 
functionals,  notably  Kleene,  Rogers,  Putnam  and  Davis,  but  there  may  be  a few  points 

of  originality.  In  one  direction,  the  axiomatization  of  the  next  section  -  especially  the 

induction  rule  -  is  original  as  far  as  the  author  knows. 

What  we  do  is  to  adjoin  a  "fictitious"  element  Q,  to  the  domain  D, and  an  element 
Q,  to  D,.  We  call  C& the  "undefined"  individual  and  52, the  "undefined"  truth  value. 
However,  we  need  to  distinguish  the  new  elements  from  the  old.  To  do  this  we  create 
a  relation  <  on  D, and  d  on  D,  (same  symbol  -  different  relations)  meaning,  roughly, 
"is  less  or  equally  defined  as".  Thus,  a  reasonable  assumption  is  that 

for  all  XED,;  but  that 

xdy 
implies 

x=y 

for  x,  LED,  with  x  #  s2,.12  We  make  this  assumption  about  <  on  D,,  but  for  the 

moment  not  about  D,.  Thus,  a  "picture"  of  D,  could  be 

I2 [from  the  original  text]  I  am  sorry  that  I  must  use  the  same  symbols  in  the  metalanguage  as  some  of 
those  in  the  object  language.  It  is  a  sad  fact  that  there  are  just  toofew  symbols.  If  I were  more  careful  with 
quotes  I  would  say  that  the  symbol  "<"  is  being  given  the  interpretation  of  denoting  the  relation  <. 
Hopefully,  the  reader  can  take  the  required  care  in  his  own  thought. 

420  D.S.  Scott 
with  the  slanting  lines  indicating  <.  We  are  therefore  involved  with  a  three-valued 
"logic"  with  the  new  value  0,  "in between"  T  and  I  but  placed  "a little  lower  down". 
(Look  at  the  picture!)  We shall  see presently  how  good  a three-valued  logic  we have.'  3 
(Similar  pictures  could  be  given  for  the  "reasonable"  view  of  D,.) 

The  upshot  of  all  this  is  that  D,  and  D,  are  partially  ordered  by  <.  What  about 
D,,,,,  for  example?  Well,  what  is DC,,,). 7 In  the  classical  version  we took  all functions; 
not  so  here.  We  want  only  the  monotonic  functions  (as  we  shall  find  later  -  at  most 

these).  By  monotonic  we  understand  a functionf:  D, +  D, (this  is the  usual  mathemat- 
ical notation)  where  x < y impliesf(x)  < f(y).  In  words  this means:  the  more  you  dejine 

an  argument,  the  more  you  define  its value under  a "computable"  function.  The  same 
idea  can  be  applied  to  a variable fin  the  combination  f  (x).  That  is to  say,  if we de$ne 

to  mean 
for  all  XED,,  then  the  combination  f(x)  is  monotonic  in  both  f  and  x,  and  the  set 
DC,_,,) is partially  ordered.  Note  that  there  is a  natural  "smallest"  element  among  the 
elements  of  Do_,,),  namely  the  function  fit,,,),  such  that 

Q(,+,)(x)=G 
for  all XED,.  Note  too  that  once  D+,)  has  its own  6,  we can  then  define  D+,),,  and 
D~++,~,~,,,~~, etc., by  the  same plan  of taking  only  the  monotonic  functions.  Indeed  we 

can  now  define  D,  for  every  type  symbol  a. 

The  plan  just  described  of  using  monotonic  functions  is  almost  correct,  but  not 
quite.  An  example  will  make  it  all  clear.  Let  D, =  (0, 1,2,  . . . } u  {Q,}, the  domain  of 
ordinary  integers  plus  Q,.  In  pictures: 

0  1  2  3  ...  n  ... 

\\\I/  Q, 
Now  the  monotonic  functions  fare  almost  like  ordinary  functions  except  we  allow 

f(n)=Q, 
for  certain  arguments  n if we so  desire.  If we read  this  equation  as: fis  undefined  at  n, 
then  we  agree  that  f  is  dejined  only  for  a  subset  of  the  integers.  Conversely,  iffo  is 
defined  on  a  subset  S  of  the  integers,  with  integer  values,  then  we  can  extend  f.  to 

"  Kleene  introduced  such  a  three-valued  logic  in  [13],  and  it  has  been  discussed  by  many,  many  authors. 

A  type-theoretical  alternative  to  IS  WIM,  CUCH,  0  WHY  421 
a  monotonic  function  f  by  defining 

This  requires  f(Q,)  = 52,. Such  functions  we call  strict. Not  all functions  need  be strict; 
we  allow  the  constant  functions,  say 

g(x)=O,  all  xEDI, 
where  g(C?,) 252,.  The  need  for  the  distinction  between  strict  and  nonstrict  functions 
will  become  clear  later.14 

We  have  not  yet  seen  the  difficulty  with  monotonic  functions,  however.  The 
D(,,,)  just  described  is  fine.  It  is  only  when  we  come  to  DC+,),,)  that  there  is 

a  question.  An  element  ~ED~(,,,),,)  is  a functional.  The  equation 

h(f)=n, 
wherefED(,,,)  and  nsD,  means  that  h "computes"  the  value  n from  the  argument  1: 

But  fis  afunction;  an  infinite  object  (e.g. it may  have  infinitely  many  function  values). 
What  does  it  mean  to  "compute"  with  an  "infinite"  argument?  In  this  case  it  means 
most  simply  that  h(f)  is  determined  by  asking  of  f  (maybe  by  some  algorithmic 

process)  finitely  many  questions  -  that  is  to  say,  oalues: 

f(mO)J(ml  ), . . . ,fh-  I 1. 
That  is, as a functional  h is continuous  in some  sense. Now  fortunately  we do  not  need 

at  this  point  to  involve  ourselves  in  topology  to  any  great  extent.  We  can  use  our 
partial  ordering  6  to  pin  down  what  we  need  most. 

Note  that  even  though  D,  is  a  very  trivial  partially  ordered  set,  DC,,,,  is  not.  The 
partial  ordering  on  DC,,,) is q uite  complex  (at least  as bad  as the  Boolean  algebra  of all 

sets of integers  -  if not  worse).  In  particular,  we can  form  in D(,,,)  many  infinite  chains 
of  functions: 

fO<fi <fz < ... <fn < ... 
For  example,  let f(x)  = x  for  all  XED,  and  define 

fn (x) =  x  for  x=0,1,2  ,...,  n, 62,  otherwise. 
Then  the  fn form  a  chain  in  the  above  sense  and  each  fn<f:  Actually  it  is easy  to  see 
that  f  is the  least upper  bound  of the  fn in the  sense  of the  partial  ordering  <  on  D,,,,,. 

I4 But  of  course  it  did  not  become  clear  until  very  much  later  that  there  are  many  different  categories  of 
domains,  and  that  sometimes  it  is  necessary  to  work  only  with  strict  functions  and  to  have  functors  that 
"lift"  continuous  functions  to  strict  functions. 

422  D.S. Scott 
We  can  write 

f=  q  f". 

n=O 

In  fact,  I+,,)  has  the  property  that  every  chain  has  a  lub.  What  does  this  have  to  do 
with  continuity?  Well,  it  is easy  to  show  in  an  intuitive  way  that 

h (  ) T? .Ll  = Q h(h) 

n=O  n=O 

holds  for  every  chain  offn  if h  is continuous.  What  we  are  going  to  do  is to  take  the 

above  equation  as  the  abstract  definition  of  continuity.  (Note  that  the  lub  operation 
trivially  works  in  the  ground  domain  D,.) 

To  be  a  bit  more  precise:  suppose  D,  and  D,  have  been  defined  and  partially 
ordered  by  <  relations  in  such  a  way  that  lubs  of  chains  always  exist.  Then  for 

4  a+Bj we allow  only  those  functions  h:  D, +  D,  that  are  at  the  same  time  monotonic and  continuous.  This  space  DCa_pJ has  a  natural  partial  ordering,  and  (as  should  be 

proved  by  the  reader)  lubs  of  chains  always  exist  in  DCa+BJ. (Hint: 

Note  that  with  this  convention  the  application  operation  f(x)  is  always  monotonic 
and  continuous  in  each  of  f  and  x. 

We  have  now  defined  the  domains  D,  for  all  CI, which  means  that  all  we have  done 
so far is to  specify  the  ranges  ofour  variables  x,.  (Note  that  D,  can  be any  "abstract"  set 
with  a  Q  satisfying  the  lub  condition.  One  should  not  always  restrict  attention  to  the 
integers.)  The  next  step  is to  specify  the  meanings  of the  constants.  Now  we obviously 
want  the  symbols  "Q,",  `X2,,", "T  , "  "I"  to  stand  for  the  values  L?,, Sz,,  T,  1.  The  other 
constants  are  functions  and  must  be  so  defined.  Thus, 

~a(P1  x3 Y)' 

! 

X  if  p=T, 
Y  if  p=l, 
52,  if  p=sZ,, 

for  all  PED,,  x,  YED,,  where  C&ED,  has  already  been  defined.  Next 

I&(x,  Y)=X 
for  all  XED,  and  YED,.  Then 

S&f;  9,x)=.&  g(x)) 
for  allf~DC,+,p+,jj,  gED(,+B),  and  XED,.  The  definition  of  Y  is more  difficult. Already  we  should  be  forced  to  prove  something  in  order  to  see  that  the  above 

definitions  "make  sense".  In  particular,  we  need  to  know  that  the  functions 

3=,  K,,,  S,,,  do  belong  to  the  correct  domains.  This  means  that  we  must  show  (1) 

A  type-theoretical  alternative  to IS WIM,  CUCH,  0  WHY  423 
that  they  are  well-defined  functions,  and  (2) that  they  are  monotonic  and  continuous. 

Actually,  this  all  follows  from  a  "composition  theorem"  for  monotonic  and  continu- 

ous  functions;  we  give  a  representative  example  of  an  instance  of  this  theorem. 

Theorem  2.1.  Suppose functions  g, h, k are  monotonic  and  continuous,  and  suppose  f is 
dejined  by  the  equation 

f(x,y)=g(h(x,y)>  k(x,y)). 
Then,  f  is monotonic  and  continuous  (in  each  of  its variables). 

The  monotonic  part  of  the  theorem  is  obvious.  To  prove  the  continuity  we  must 
calculate 

=g ( q h(x,>y), i/  k(xm,y) 

I=0  m=O  1 

=lgo $`/, g(W,> vh k(xm, y)). 
Now  we  note  that  in  a  partially  ordered  set  a  lub  of lubs  is just  the  lub  over  the  double 
index  set  (in  this  case  over  the  pairs  (1, m).)  Furthermore,  to  calculate  a  lub  it  is 

sufficient  to  find  the  lub  of  a  cojinal  subset.  In  our  case  the  elements 
g(h(x,,  y), k(x,,  y))  are  cofinal,  because  we  assume  the  x,  form  a  chain (x,<x,+~),  and 
if  n=max(E,  m),  then 

g(h(xi,  Y), Wx,,  y))<g(h(x,,  Y), k(x,,  Y)) 
by  monotonicity.  Hence, 

f(n$'ox.,y)=nv'g(h(x., y),k(x.,y)) 

=nvof  (XII, y). 
Thus,  f  (x, y)  is  continuous  in  x. 

It  is  clear  that  the  proof  just  given  applies  for  any  number  of  variables.  Note  too 
that  the  variables  are  independent  and  may  belong  to  di;fSerent domains.  Thus,  in 
our  example  we  could  have  had:  XED,,  YED~,  h&C,,C8,,,,,  kEDC,+C8+ajj,  and 
g%+6-rE))  producingfED~OI-(P~E)).  The  reader  should  note  in  particular  that  our 
functions  are  all  "Curried"  (as  the  saying  goes),  so  that  f  (x,  y)  really  means  f(x)(y). 

Thus,  f(x)  E &  + Ej 3 and  one  should  take  care  to  remark  that  with  x jixed,  f  (x)(y)  is 

424  D.S.  Scott 
continuous  in  y;  whencef(x)E&,,)  is  correct.  Then,  with  x  uariable,f(x)  is  continu- 

ous  in  x,  which  gives  the  result. 

It  is  also  clear,  as  remarked  before,  that  appZicationf(x)  is  monotonic  and  continu- 
ous  in  fand  in  x.  Therefore,  any  combination  defines  a  good  function.  This  remark,  for 
example,  justifies  the  definition  of  S,,,: 

S&(f;  93 x)=f(x)(g(x)), 
in  Curried  form,  and  shows  why 

S,B,E((a-*(P-,Y))~((cc~B)~(a~Y))). 
We  have  followed  Curry  (and  others)  in  giving  names  only  to  "combinators" 
K,,  and  &,  because  all  others  can  be  defined  in  terms  of  these.  For  example: 

I,(x)  =  x  (the  outright  definition) 

=  K .(,-.,(x)(K&)) 

=  S&K+,)  )(K,,)(x). 
Hence, 

I,=S~yor(Ka(a+Ea,  )( K,,),  (the  indirect  definition), 
where  P=(,+((u+u)-+u))  and  r=(a+(a-+a)).  Another  example: 

C(x)(f)=.04 

=I(f)(K(x)(f)) 
=  S(I)(K(x))(f) 
=  K(S(I))(x)(K(x))(f) 
=S(K(S(I)))(K)(x)(fL 
whence 

C=S(K(S(I)))(K), 
where  we  have  left  it  to  the  reader  to  fill  in  the  type  subscripts.  Obviously,  this 
"economy"  has  only  "theoretical"  importance.  The  proper  thing  to  do  is  to  introduce 

a  A-operator  and  write 

C,,=~f,,+,,  kC&s,c41. 
We  shall  not  do  so  at  the  moment  because  we  do  not  want  to  formulate  all  the  rules 
about  free  and  bound  variables.  Our  axioms  are  simpler  if we  keep  to  the  "algebraic" 
theory  that  has  only  free  variables. 

So  K,,  and  Saa,,  are  the  so-called  combinutors,  but  what  is  x~?  Answer:  X=  is 
the  McCarthy  operator  for  forming  the  conditional  expression.  Thus,  instead  of 

A  type-theoretical  alternative  to  IS WIM,  CUCH,  0  WHY  425 
3 &)(x)(y)  we may  write  (informally)  the  more  usual  (p +x,  y). Its  properties  are  well 
known.  What  might  not  be  so well known  is its use at  higher  types.  Thus,  consider  the 
definition 

Pa(x,Y)(P)=  ~,(P)(X>  Y)? 
where  x,y~D,.  What  is  Por(x,y)~D~,,,)  ? Answer:  it  is an  ordered  pair.  Indeed  let  us 

write  informally 

(x,  Y > =  P&,  Yk 
then  we  see 

(x, y)(T)=x, 

<x,Y)(J-)=Y, 
which  shows  why  (x,  y)  is a pair.  Note,  however,  that  this is a logical construction  and 

should  not  be  confused  with  a  data  type  for  pairs  of  individuals.  (By  the  way,  one 
should  check  that  X= really  is monotonic  and  continuous.) 

Finally,  we  must  interpret  Y,.  We  use  the  notation  for  Curry's  "paradoxical" 
combinator,  but  we  cannot  use  Curry's  definition.  Why?  Simple:  my  view  is that  the 

type-free  theory  makes  no  sense  whatsoever.  This  remark  applies  to  the  theory  as 
a whole and  does  not  prevent  me  from  gaining  inspiration  from  parts  of the  theory.15 
The  inspiration  we  need  about  Y,  is  the  so-called  fixed-point  property:  if FEDS,,,), 

then  we  want 

Ydf)  =f(Ydf))* 
Two  questions:  how  do  we know  f has  a fixed  point;  and  if it does,  which  one  shall  we 
choose  for  Y,( f )? Answer:  we are  lucky  in  being  able  to  choose  the  least fixed  point 
-  "least"  in  the  sense  of  <  on  Dta__,.  It  is  constructed  by  iteration  as  follows:  For 

XED,,  let 

f"(x)=f(f(...f(x)...)), \  I 

n  times 
where  f" (x) =x. Then  note 

r5 [from  the  original  text]  The  same  could  have  been  said  about  infinitesimals  in  the  calculus  -  except 
that  nowadays  they  have  a  reasonable  interpretation  [in  nonstandard  analysis].  [comment  19931  Of 
course,  once  models  for  the  type-free  theory  had  been  defined,  the  "paradoxical"  definition  could  be  used. 
Surprisingly,  it  turned  out  that  these  definitions,  although  giving  fixed  points,  did  not  always  have  simple 
properties.  See  the  discussion  and  references  in  Barendregt  [l]. 

426  D.S.  Scott 

and  so 

and  by  induction 

_P(~a)~f"+`mz)~ 
because  f  is  monotonic.  We  can  thus  define 

Y,(f)=  Q  f"(G)> n=O 
and  then  calculate  by  continuity: 

f(Y,(f))=  \j;  f"+`(%)=Y,(f). n=O 
This  proves  that  f  has  a  fixed  point.  Now  suppose  a  is  any  other.  Then 

&da, 
and  so 

whence 

Thus, 

Y,(f)=  t  f"(%)ba. 

n=O 

This  proves  that  Y,(f)  is  the  least  fixed  point. 

We must  still prove  that  Y,(f)  is monotonic  and  continuous  inf.16  The  monotonic- 
ity  is  very  easy,  but  the  continuity  requires  some  computation.  Suppose  fn <  fn + r  is 

a  chain  of  functions  in  D,, _ aj. Then 

Let 

I6 [from  the  original  text]  Sorry  about  that!  When  one  says  what  one  means,  one  must  demonstrate  the 
correctness  of  one's  definitions. 

A type-theoretical alternative to IS WIM,  CUCH,  0 WHY  427 
we  must  show 

It  is  obvious  from  monotonicity  that 

To  complete  the  proof,  compute: 

= a. 
Hence,  a  is  a  fixed  point,  and  so  Y (Vn"=ofn)<a.  Thus,  we  have  proved 

Y&D((  a-a)-a). 
Having  now  interpreted  all  of  our  constants,  we  can  define  the  important  notion  of 
validity.  Suppose  X  and  Y  are  two  expressions  of  the  same  type.  In  general,  they 
contain  variables;  hence  they  do  not  denote,  as  they  stand,  anything  in  particular.  But, 
if  we  assign  values  in  the  appropriate  D,  to  each  of  the  variables,  then  all  of  the 

symbols  in  the  expressions  become  meaningful  and  X  and  Y have  values.  Thus,  under 
the  assignment  to  the  variables,  the  atomic  formula  X  <  Y is  either  true  or false.  Now 
consider  an  assertion  @ F  Y.  It  is  said  to  be  valid  if under  every  assignment  of values  to 

variables  that  makes  all  the  atomic  formulae  of  the  list  @ true,  all  the  atomic  formulae 
of  Y  are  true  also.  That  is,  @  "implies"  Y  with  the  variables  being  universally 
quantified. 

Strictly  speaking  we  have  only  defined  validity  with  respect  to  the  given  domain  D,. 
We  are  more  interested  at  the  moment  in  those  assertions  that  are  universally  valid  for 

all  choices  of  D,.  We  shall  see  many  examples  of  valid  assertions  in  the  next  section. 

428  D.S. Scott 

Notice,  however,  how  different  our  method  is  compared  to  the  I-calculus.  In  the  latter 

theory,  validity  of  equations  (interconvertibility)  is  defined  in  a  purely  formal  manner. 
Here  we  have  dejned  validity  "semantically"  and  must  discover  the  formal  properties 
of this  notion.  Of  course  in  our  metalanguage  we  are  taking  "on  faith"  the  existence  of 
the  various  higher-type  continuous  functions  in  the  D,  that  we  have  been  defining.  But 
this  is  normal  mathematics.  The  validity  of  conversions  in  A-calculus  has  no  such 
mathematical  foundation.`7 

3.  Axiomatization 

In  the  first  place  there  are  some  very  general  properties  oft-  that  would  be  the  same 
for  any  similar  theory.  We  give  "axioms"  (quite  self-evidently  valid  assertions)  and 
"rules  of  inference"  (simple  deduction  methods  that  clearly  preserve  validity). 

(INCLUSION) 

(CONJUNCTION) 

(CUT)  (OR  SYLLOGISM)  Q/-Y  YE@ @FO 

(SUBSTITUTION)  @kY @ [X/x]  t  Y [X/x]  ' 
where  X  and  x  are  of  the  same  type. 

Clearly,  the  rule  of  substitution  can  be  generalized  to  simultaneous  substitution  for 
several  variables.  However,  this  more  general  rule  can  be  proved  as  a  derived  rule  of 
inference. 

Next  the  relation  <  enjoys  several  useful  properties: 

(REFLEXIVITY)  Ex<x. 
(TRANSITIVITY)  x<y,  y<zkxQz. 
(MONOTONICITY)  x<y,  f<skf(x)Gs(y). 

(EXTENSIONALITY)  @ ;y;:  ;(`),  where  x  is  not  in  @. \ 
Note  especially  that  we  have  stated  these  principles  without  type  subscripts,  This  is 

a very  convenient  trick  available  in  the  metalanguage.  The  point  is  that,  say,  t-  x <  x  is 
valid  for  variables  x  of  all  types.  Similarly  for  the  axiom  of  transitivity  with  the 

understanding  that  x,  y,  z are  all  of  the  same  type  -  otherwise  the  formulae  would  not 

"  [from  the  original  text]  At  least  to  date,  but  I  despair  of  ever  seeing  an  adequate  justification. 

A  type-theoretical  alternative  to  IS  WIM,  CUCH,  0  WHY  429 
all  be  well  formed.  In  the  axiom  of  monotonicity  and  the  rule  of  extensionality,  x  and 
y  must  be  of  the  same  type,  say  tl,  and  f  and  g  must  be  of  a  type  (a+p)  for  the 

assertions  to  make  sense. 

It  is  interesting  to  ask  why  the  rule  of  extensionality  is  correct.  Well,  suppose 
@/-f(x)  <g(x)  is  valid.  Since  x  is  a  variable  being  assumed  not  to  occur  in  any 
formula  of  @, it  is  a  "free"  variable  inf(x)  <  g(x).  "Free"  in  the  strongest  sense  that  we 

are  free  to  give  it  any  value.  Now  consider  @ k  f<  g.  Give  values  to  the  variables  in 

@ (and  to  f and  g)  to  make  all  the  formulae  of  @ true.  By  assumption  f(x)  <g(x)  is  true 
for  all  values  of x  (of  the  correct  type!).  Hence,  by  dejnition  of  $  for  functions,  f  < g is 

true.  Thus,  we  have  shown  that  from  the  validity  of  @ Ff  (x) <g(x),  the  validity  of 

@ k  f  <  g  follows. 

By  the  way,  remember  that  X=  Y is  short  for  X<  Y,  Y<X.  Note  that  =  is  indeed 
an  equality  relation  because  we  can  now  prove,  as  theorems  from  the  axioms  and  rules 
we  already  have,  that 

x=  Y,  Y=ZkX=Z, 

X=YFY=X, 

@[X/x],  x=  Yk@[Y/x]. 
The  last  one  relies  heavily  on  monotonicity. 

Inasmuch  as  we  have  assumed  no  "non-logical"  constants,  there  is  in  general  very 
little  to  say  about  individuals  of  type  I  except  for  the  "undefined"  individual  52,: 

(MINIMALITY,)  k  52, <  x,. 
Later  we  shall  be  able  to  prove  this  about  all  the  types  and  all  the  52, ~  which  are 
defined  and  not  primitive.  If,  and  only  in  the  case  of  individuals  (and  truth  values),  we 

wanted  to  have  the  "reasonable"  view  of  Sz, and  the  other  individuals,  we  might  want 
to  assume:" 

(DISCRETENESS,)  @, @ [Q,/x,],  y,  d  x,  k  @[y,/xJ  . 
This  principle  means  that  the  only  y,  <x,  are  Q,  and  x,  itself.  This  is  definitely  not 
correct  for  higher  types.  In  the  case  of  truth  values  we  shall  not  have  to  assume  it,  but 

shall  prove  it  from  the  properties  of  the  conditional  expression. 

The  first,  most  trivial,  property  of  truth  values  is19 

(MINIMALITY,)  t-  52, <  x,. 

I8 The  principle  was  stated  incorrectly  in  the  original  paper.  The  revision  was  suggested  by  Todd  Wilson. 
I9 Note  here,  as  for  type  1, we  can  use  the  particular  variable  x,  for  emphasis  of  the  type. 

430  D.S.  Scott 
Then  we  have  for  1 #:`O 

(CONDITIONALITY)  E  I=(  T,  x,  y ) = x, 

I-- 3,(1,x,y)=y, 
t-  z~.(Q,,x,y)=Q,,  where  x,y:a. 
Finally,  we  must  express  the  idea  that  T,  I  are  the  only  allowed  truth  values: 

(EXHAUSTION) 

This  principle  allows  us  to  argue  by  cases.  For  example,  we  can  prove  all  the 
well-known  laws  of the  conditional  expression  by  simple  (but  lengthy!!)  arguments  by 
cases. Thus,  they  are  not  needed  as axioms,  but  they  must be proved  as "lemmas"  very 
early  on. 

The  axioms  for  the  combinators  come  directly  from  their  definitions: 

(K-CONVERSION)  t-  K&x,  y)=x,  where  x:01, y:/% 
(S-CONVERSION)  I- S&f,  g, x)=.0x,  g(x)), 
where  f:(a+(P+r)),  g:(a+P),  and  X:CI. 

Sometimes  people  give  other  complicated  equations  for  the  combinators.  This 
seems  to  be  a  desire  to  avoid  using  the  rule  of  extensionality  (say,  in  the  work  of 
Rosser).  This  would  seem  to  be  important  only  in  the  situation  where  one  wanted  to 
do  away  with  variables  altogether.  We  are  not  using  bound  variables  here,  but  there 
seems  to  be  no  reason  not  to  have  free  variables.  Hence,  all  those  equations  may  be 
proved  by  extensionality  -just  as we  prove  propositional  truths  by  using  the  rule  of 
exhaustion  of  cases. 

Finally,  we  must  isolate  the  basic  facts  about  the  fixed-point  operator: 

(STATIONARINESS)  t-f(Y,(f))<Y,(f),  wheref:(cl+a). 
This  is  slightly  weaker  than  stating  an  equation  -  but  the  other  direction  can  be 
proved  by  using  the  very  important  rule: 

(INDUCTION)  @k  y  CWXI  @, y I- y Cf(Wl @ k `y ~~,(f)lxl  ' 
where  the  type  restrictions  are  these:  x : CI and  f:  (a -+ c(), and where  x must  not  occur  in 

@. (We  need  x  as  a  "free"  variable  as  in  the  rule  of  extensionality.) 

2o [from  the  original  text]  Note  that  we  could  drop  all  the  type  subscripts,  and  the  reader  could  very 
safely  put  them  back  in.  It  would  be  interesting  to  formalize  rules  for  the  metalanguage  for  doing  this 
automatically.  [comment  19931  This  was  done  together  with  a  type  of  polymorphism  by  Milner  in  ML  to 
excellent  effect. 

A  type-theoretical  alternative  to  IS WIM,  CUCH,  0  WHY  431 
As a  first  example  of  the  use  of  induction  we  prove  the  fixed-point  equation:  thus, 

8,  Qf(Q;2,) 
is  obviously  provable.  Further, 

x G(x)  E  f(x)  G f  (f(x)) 
is provable  by  monotonicity.  Therefore,  by  induction 

Y&I-)<  f(Y,(f)). 
As  a  second  example,  we  show  that  the  fixed  point  is minimal:  thus, 

f(a)<a  t-  Sz, <a 
is  obvious.  Further, 

f(a)<a,x<al-f(x)<a 
is easy  to  establish  by  monotonicity  and  transitivity.  Therefore,  by  induction 

f(a)<a  k  Y,(.f)<a. 
Although  he  does  not  see how  to  prove  it  at  the  moment,  the  author  is reasonably 
certain  that  these  two  instances  of  induction,  though  useful,  are  not  the  whole  story. 

That  is, it is not  enough  only  to  assume  them  as axioms;  the  full  rule  is really  needed. 

For  example,  let  us define  (as a suitable  combination  of  K's and  S's)  the  composition 
operator  6  where  for f:  (/I -+ y),  g : ( ct +  p)  and  x : cx we  have  as  provable: 

I-  B&L  9, x)=f(g(x)). 
Now  a  mildly  interesting  theorem  about  fixed  points  is the  following: 

B(f,  g)=  B(sJ-),  B(.L h)=  B(U)>  s(Q)Gh(fi)  k  s(Y(f))  d  h(Y(f))> 
where  type  subscripts  must  be  written  in so  that  it  all makes  sense. The  easy  proof  by 

induction  is clear.  The  author  does  not  see  how  to  prove  this  in  any  other  way.2' 

Having  seen  how  useful  induction  is, one  may  well  ask:  why  is it  valid?  We  shall 
argue  its  correctness  by  a  method  that  brings  out  some  other  useful  points  of  the 
theory.  Note  in  particular  this  property  of  the  pairs  we  defined  in  the  last  section: 

x<x',  yby'  H  P(x,  y)  <  P(x',  y'), 
where  in  general  @ H  Y is short  for  the  two  assertions  @ E  !P and  YE  CD. Hence,  we 
see that  any  list @ of formulae  is such  that  there  exists  a (long) "paired"  formula  X <  Y, 

where 

`l  In  fact,  there  have  been  many  subsequent  studies  of  induction  principles.  See  for  example  [18,  191,  but 
also  compare  the  proofs  in  [S]. 

432  D.S.  Scott 
is  provable.  Next  we  recall  the  property  of  combinators  whereby,  if  X  is  any 
expression  and  x is any  variable,  then  there  exists  an expression  F  such  that  F  does  not 
contain  x  and 

F F(x)=X 
is  provable.  "  By  these  remarks  it  becomes  clear  that  without  loss  of  generality  the 

Y of the  induction  rule  can  be  taken  to  be  of the  form  g(x)  <  h(x),  where  g and  h are 
variables  and  their  "definitions"  g = G, h = H  can  be pushed  into  the  @. Thus,  the  rule 
has  the  simpler  looking  form 

@tg(Q)6W)  ~,s(x)dh(x)t-g(f(x))gh(f(x)) 

@ l--s(Y(f))  d  V(f))  2 

where  still  x  may  not  occur  in  @. 

The  validity  of  the  conclusion  of the  rule  is now  almost  self-evident:  assume  values 
are  given  to  the  variables  (except  for  x) so that  @ is true.  Then  by  the  first  assumption 

g(Q)  <  W4 
is true.  Then  by  the  second  assumption  where  x  is  valued  as  Q, we  have 

s(f(Q))dW-(52)). 
Applying  the  second  (inductive)  assumption  repeatedly,  we  find 

s(f"(Q))  <  Wf"(W) 
to  be  true.  Now  passing  to  the  limit  and  using  the  continuity  of  g  and  h, we  have 

true  as  desired. 

It is this  rule  of induction  that  I consider  the  main  advantage  of my  system.  As far as 
I know  there  is nothing  like it for  the  &calculus. 23 Of course,  the  rule  is very  much  like 

McCarthy's  principle  of recursion-induction  (see  [ 151). In  fact,  I view my  rule  as being 
a more  general  version  of McCarthy's  principle,  which,  after  we gain  more  experience 
with  it,  will be  even  easier  to  apply.  Note  that  the  principle  as  I  state  it  is "abstract" 
and  does  not  require  any  "knowledge"  of  the  integers  in  its  formulation.  Of  course, 
I used  ordinary  integer-indexed  iteration  tojustgy  the  rule  -  but  that  argument  was in 
the  metalanguage.  The  "formal"  connection  with  integer  induction  and  recursion  will 
be  discussed  in  the  next  section. 

"  [from  the  original  text]  The  F  can  effectively  be  found  from  the  X  and  can  be  called  Ix  [X]  -  this  is 
a  short-hand  in  the  metalanguage. 

23 [from  the  original  text]  Morris  in  his  thesis  [17]  proves  a  minimal  fixed-point  property  for  the 
I-calculus.  The  proof,  however,  is  very  complicated  -  like  the  Church-Rosser  theorem  -  and  it  is  not  quite 
clear  exactly  what  it  gives  one  for  proofs  about  I-expressions. 

A  type-theoretical  alternative  to IS WIM,  CUCH,  0  WHY  433 
Completeness 
We  have  presented  a  language  (Section  1);  we  have  interpreted  it  semantically 
(Section  2); we  have  written  down  many  self-evident  "axioms"  and  validity-preserving 
rules  (Section  3); now  it  is  time  to  ask:  How  much  progress  have  we  made?  1fwe  could 

only  show  that  the  axioms  were  complete  in  the  sense  of  allowing  us  to  derive  eoery 
valid  assertion  by  formal  proofs  based  on  the  axioms  and  using  the  rules,  then  we 
could  be  very  satisfied  by  our  progress.  Such  completeness  is impossible,  however.  The 

argument  is  a  standard  one  of  showing  that  the  class  (of  Giidel  numbers)  of  valid 
assertions  is  not recursively  enumerable. (The  class  of  theorems  is recursively  enumer- 

able.)  Standard  as  the  argument  is,  it  may  be  instructive  to  see  how  it  can  be  expressed 
in  our  language.  We  will,  by  the  way,  assume  the  discreteness  rule  throughout. 

What  we  shall  do  is  to  introduce  four  nonlogical constants 0 : z, Z  : (z --) o),  +  1 : (I +  I), 
and  -  1:  (I -+ z) of  the  types  indicated.  We  shall  then  write  down  five  simple  equations 
involving  these  constants,  in  a  list  called  d,  such  that  any  interpretation  making 
&  true  must  be  isomorphic to  the  system  of  integer arithmetic. That  is to  say  the  system 
where  0  is  zero,  Z  is  the  monadic  predicate  of  being  equal  to  zero,  and  where  +  1 and 

-  1 are  the  successor  and  predecessor  functions.  It  will  thus  be  "difficult"  to  enumer- 
ate  valid  assertions  of  the  formz4 

&t-Q. 
Now  that  we  are  moving  a  little  away  from  "theory"  and  a  little  closer  to  "practice", 
we  must  begin  to  "soften"  our  notation.  Thus,  the  (-*,)-notation  is  better  than  the 

3  -notation.  We  shall  also  use  the  A-notation,  but  keep  in  mind  that  Ax[X]  is 
a  metalinguistic  abbreviation  for  an  expression  F  not  involving  the  variable  x  such 
that  l-  F(x)  =  X  is  provable  -  which  expression  is  not  important.25  We  shall  also  write 
x +  1 (with  no  parentheses)  instead  of  +  1 (x)  and  similarly  for  x -  1.  We  could  even 
introduce  the  many  excellent  features  of  format  used  in  Landin's  ISWIM  -  but 

remember:  our  variables  are  forever  typed!26 

The  first  four  equations  of  d  are  very  elementary: 

Z(Q,)=Q,, 

Z(O)=T, 
o-l=sz,, 

Ax,[x,+l-l]=~xI[xJ, 
Y(~,,)(~fi,,,)C~x,C(Z(xl)~O,Jil-r,(x,-1)+  1)11)=~x,L-&l. 

24 Because,  as  is  argued  below,  there  is  no  recursive  enumeration  of  all  true  equations  between  primitive 
recursive  functions. 

"  [from  the  original  text]  I  could  make  it  precise,  if  it  were  necessary. 
I6 [from  the  original  text]  Remember  too  that  we  have  no  "program  points"  nor  assignment  statements 
-just  a  "pure"  functional  calculus. 

434  D.S.  Scott 
The  last  is harder  to  appreciate  -  especially  with  the  subscripts  -but  all will be  made 

clear.  (As  a  first  step,  drop  the  subscripts.)  The  first  equation  means  that  the  truth 
value  for  Q's being  zero  is "undefined".  The  second  equation  means  the  zero  is  zero; 
hence,  0 #  Q because  T # 52. We  have  not  used  #  as  part  of  our  "official"  notation 

before  and  we will not  make  it official  now.  What  it means  to  say that  T # 52 is that  the 
assertion 

where  @ is  arbitrary,  is provable  (and  hence  valid).  In  other  words,  a false  equation 

implies  anything.  The  third  equation  means  that  the  predecessor  of  zero  is  "unde- 
fined".  The  fourth  equation  means  that 

x+1-1=x 
is true  for  all  values  of  XED, -  and  everyone  knows  what  that  means. 

The  last  equation  involves  a  recursive  definition.  Indeed,  let  N : (z +  1) stand  for  the 
left-hand  side  of  the  equation.  Then  by  "definition", a"

N(x)=(Z(x)+O,  N(x-l)+l). 
What  the  fifth  equation  is telling  us  is that 

N(x)=x 
must  hold  for  all  x.  That  is  telling  us  a  lot!  For  one  thing  we  will  know  that 

(Z(x)+O,  x-l  +  1)=x 
holds  for  all x. From  our  other  equations  we know  that  x +  1 #  0 for  all x #  Q (because 
0-l=O#x=x+l-1).  We  also  know  that  O#O+l#O+l+l,  etc.  (Hence 

Sz +  1 = D by  discreteness,  but  that  is not  too  important.)  What  is new  from  the  last 
equationisthatifx#Oandx#52,thenZ(x)=Iandx-l+l=x.(Hencex-1#52 

for  x #  0,  x  #  a.)  In  particular,  we  see  Z(x)=  T  iff  x =O.  In  other  words,  zero, 

successor,  and  predecessor  have  all  the  elementary  relations  they  should.  But  even 
more  than  this  is  contained  in  the  fifth  equation,  and  that  more  is  mathematical 

induction. 

To  be  more  precise  we  shall  derive  this  rule: 

&,@E!P[Q/x]  JCz,@t-Y[O/x]  d,@,YYY[x+l/x] 

&d,@PY  7 

where  x  is not  in  @. To  make  the  derivation  even  more  transparent,  we may  assume 
without  loss  of  generality  that  Y  has  the  form 

9 (x) G h (x)3 
where  x:  z and  g, h: (1 +a)  for  a  suitable  CL The  problem  then  is  to  show  that  the 
hypotheses  imply 

&,@  k  g6h. 

A  type-theoretical  alternative to  IS WIM,  CUCH,  0 WHY  435 
In  view  of  what  we  have  assumed  in  d,  it  will  be  sufficient  to  prove 

Now  look  at  the  dejnition  of  N.  Note  first  that  we  can  prove  (by  assumption) 

The  desired  conclusion  will  follow  by  our  general  rule  of  induction  if  only  we  can 

prove 

-($, @, Ax Csu-(x))l d Ax C4fc4)l 

We  can  now  drop  the  Ax's  in  the  conclusion,  and  we  can  argue  by  cases  Z(x)=Q, 
Z(x)  =  T, Z(x)  =  1.  Each  case  is  easily  done  in  view  of  our  three  assumptions.  Thus, 

the  proof  is  complete. 

That  shows  the  deductive  power  of  d.  The  semantic  import  should  now  be  clear: 
any  interpretation  making  ~2  true  must  be  isomorphic  to  the  integers.  Now  using  the 
0,  Z,  +  1,  -  1 notation  it  is  clear  that  we  can  write  down  Y-definitions  of any  primitive 
recursive  function  (of  any  number  of  arguments).  Consider  two  such  definitions  F  and 

G.  Clearly,  now 

is  valid  (in  all  interpretations)  if  the  functions  defined  by  F  and  G  in  the  standard 
integers  are  equal.  It  is  well  known  that  one  cannot  enumerate  the  pairs  of  equal 

primitive  recursive  functions;  hence  the  class  of valid  assertions  of  our  calculus  cannot 

be  recursively  enumerable. 

Is  this  incompleteness  result  a  cause  for  despair?  I  think  not.  The  system  is  really 
very  strong.  Much  stronger  than  what  logicians  call  primitive  recursive  arithmetic 
because  of  the  use  of  the  higher  types.  It  is  more  in  the  line  of  a  higher-type  theory  of 

recursive  functionals  proposed  by  Gijdel  and  studied  by  him  and  others.  Our  system  is 
more  usable  that  Gbdel's  (I  won't  say  "stronger"  because  I  do  not  know  at  the 
moment)  in  view  of  our  use  of  partial  functions.  In  particular,  we  can  define  not  only 

primitive  recursive  but  also  general  recursive  functions  -  on  the  basis  of  d. 

It  is  enough  to  see  how  to  define  Kleene's  p-operator.  The  trick  is  a  common  one: 
suppose  fis  a  given  function  and  we  want  to  find  the  least  integer  x  such  thatf(x)  =  0 
andf(y)  #  52 for  smaller  y.  We  define  by  recursion  the  function  g  such  that 

and  then  the  desired  integer  is  g(0).  More  formally 

436  D.S.  Scott 

It  would  be  interesting  to  have  a clearer  idea  of  what  the  various  definable  partial 
functions  of  higher  type  really  are  -  maybe  the  recursive  functionals  people  already 

know.27  One  particularly  important  question  is  this:  suppose  we  fix  the  standard 
arithmetic  interpretation  of 0, 2,  +  1, -  1. Suppose  we consider  a dejinition  of a func- 
tion  f:(z+r).  We  have  just  shown  that  every  general  recursive  function  may  be 
defined,  but  conversely?  Is every  definable  function  general  recursive?  The  answer  may 
be  yes,  but  the  higher  types  make  it  complicated  to  see.28 A  possible  outline  of  an 
argument  might  be  as  follows:  use  an  abstract  machine  to  calculate  f  by  symbolic 
manipulation  of  its  definition.  In  fact,  we might  even  be  able  to  use  (almost  directly) 

Landin's  A-calculus machine.  Our  axioms  for  =), K, S, Y (and  for  0, Z,  +  1, -  1) tell  us 
exactly  how  to  make  conversions.  If  the  expression  for  fis  F,  we just  try  reducing 

F(O+l+l  . . . +1) \  , Y 

n  times 
until  we  "finally"  get 

O+l+l...  +1 M' 

f(n)  times 
Clearly,  if the  reduction  rules  always  work  to  produce  an  answer  whenf(n)  is defined, 
thenfis  indeed  general  recursive  because  the  rules  are  effective. 

Note  that  the  rules  can  never  give  a  wrong  answer.  The  big  question  is  whether, 
when  we  expect  an  answer  semantically,  we  can  alwaysJind  it  formally.  If  we  could 
prove  that,  it  would  be  a  kind  of  completeness  of  our  calculus  -  completeness  for 
numerical  equations  -  and  would  be  the  most  we  could  hope  for.  It  would  also  be _ 

theoretically  at  least  -  a  very  important  result.  One  wonders  what  the  situation 
would  be  on  domains  other  than  the  well-worn  integers.  It  strikes  the  author  as 
significant  that  this  question  cannot  even  be  asked  of  the  A-calculus  because  the 
I-calculus  has  no  meaning.  One  has  to  take  the  reduction  rules  "on  faith"  and  has  no 
"standards"  to  which  they  must  match. 

A  third  question  about  completeness  concerns  the  power  of  expression  of 
our  language:  can  we  define  everything  we  want?  The  answer  is  not  all  that  clear 
because  it is difficult  to  see just  what  we do  want.  Certainly  we want  what  we have  at 

"They  did.  See,  for  example,  the  study  of  Hyland  [ll]  and  the  brief  remarks  in  [7]. 

`s  [from  the  original  text]  The  proof  might  be  contained  in  the  thesis  of  Platek  [20],  but  that  is  a  very 
difficult  work  to  read,  to  say  the  least.  [comment  19931  The  document  is still  difficult  for  the  author  to  read, 

but  the  proof  required  is  more  simply  based  on  the  idea  of  "effectively  given  domains".  This  means  that 
since  the  domains  used  here  are  all  algebraic  cpos  with  effectively  enumerable  bases  of  finite  elements,  the 
genera1  recursiveness  of  all  definable  functions  on  the  integers  can  be  proved  semantically  by  structural 
induction  on  the  size  of  the  definition.  See  [22,  10,  231  for  various  expositions. 

A  type-theoretical  alternative  to IS WIM,  CVCH,  0  WHY  437 
the  moment,  but  consider  the  simple  truth  function  V  :(o+(odo))  given  the  table: 

Axiomatically,  V  is determined  by  the  following  equations: 

k  v (P,  4)=  v (4, PI, 

b  V(P,  T)=T, 
k  v (P, J-)=p. 
Then  all  the  other  facts  about  the  table  follow  by  monotonicity.  (Note  that  this  V  is 
indeed  monotonic.)  Now  the  question  is: do  we want  it? Is there  any  reason  to  exclude 

any  monotonic  truth  function. 

By  the  way,  it  is an  interesting  exercise  to  prove  that  all monotonic  truth  functions 
of any  number  of arguments  can  be defined  in terms  of  V  and  1,  where  1  : (o +  o) is 
defined  in  the  usual  way: 

1  (P)=(P-  J-2  T). 
The  main  trouble  with  V  is  that  it  is  symmetric.  Thus,  consider  the  function 
definition: 

W=(V(fW)  v ZMX))`X,  x+ 113 
where  I  inadvertently  wrote  (p  V q)  for  V  (p,  q).  To  evaluate  h in  terms  of  the  given 
functions  f  and  g, we  must  calculate  fand  g  in  parallel.  Thus,  for  a  particular  x,  we 

start  to  calculate  bothf(x)  and  g(x).  If one  of them  gives the  answer  0, we stop  and  are 
sure  that  h(x) = x. If both  give  answers  other  than  0, we know  h(x) = x +  1. Otherwise, 
h(x) = 52. Well,  that  is a kind  of  algorithm,  but  it  has  a flavor  different  from  the  usual 

bread-and-butter  calculations  that  proceed  one  step  at  a  time.  Do  we enjoy  this  new 
flavor  enough  to  call  it  computable?  Some  people  would  say  yes,  but  I  wonder.  It 

seems  harmless,  but  maybe  we  should  think  about  it  more.2g 

"  [from  the  original  text]  Platek  also  discussed  and rejected  the  idea  in  his  thesis  in  his  study  of  Kleene's 
work.  [comment  19931  Plotkin  settled  all  the  questions  asked  here  when  the  symmetric  V  is  allowed  in 

[21].  As  the  discussion  in  the  preface  to  the  Kahn-Plotkin  paper  shows,  extensive  research  on  "sequential 
functions"  has  still  not  come  to  complete  satisfactory  conclusions. 

438  D.S.  Scott 
5.  Conclusions 

It  seems  to  me  that  the  idea  of  a  monotonic  and  continuous  function  is  a  very 
natural  one  for  anyone  thinking  about  computability.30  What  I  have  tried  here  is to 
give a logical  calculus  (or  even:  algebra)  for  the  notion  using  type  theory.  The  point  is 
that  the  types  are  natural  -  the  higher-type  functionals  are  useful -  and  they  have  the 
advantage  of possessing  a semantical  interpretation.  It  is important  to  remember  that 
I consider  the  higher-type  functionals  as logical  notions  to  be kept  separated  from  the 
data  types  (more  on  this  below).  I  think  I  have  given  enough  detail  here  to  demon- 

strate  that  this  "algebra  of computation"  works  very  smoothly  and  naturally  -  though 
I admit  that  fully  formal  proofs  would  be  very  lengthy.  (Should  we think  of automat- 
ing  any  proof  procedure. q31 ) It  is not  very  surprising  that  there  is a nice  algebra  since 
all  we  really  need  are  the  conditional  expression  and  the  possibility  of  explicit 
definition  (S  and  K)  and  of recursive  definition  (Y). (I ask  again:  do  we want  V ? any 

others??)  There  is the  question  of computational  completeness  mentioned  in Section  4, 
however,  and  this  should  be  given  more  thought. 

Now  what  about  other  data  types?  My  present  view  is that  all  the  data  should  be 
kept  in  type  1. In  Section  4, I  showed  how  one  might  structure  the  objects  of type  z as 
the  integers  with  the  aid  of  0, Z,  +  1, -  1. Numbers  are  only  one  type  of  data.  We 
could  imagine  D,  as  being  divided  into  many  disjoint  parts,  each  part  with  its  own 

structure  and  with  axioms  for  the  structure  given  in  the  same  style  as  in  d.  For 
example,  one  part  might  be  LISTS  (regarded  as data  rather  than  as logical  constants), 
and  we  would  need  structure  such  as  NIL,  cons,  cur,  cdr.  The  axioms  would  be  very 
similar  to  those  in  d.  The  advantage  of  the  "axiomatic"  approach  over  the  "defini- 
tional"  method  would  be in the  freedom  we would  allow  ourselves  in representing  the 

data,  say,  in  a  machine.  We  would  only  have  to  check  that  the  data  structure  as 
implemented  satisfied  the  axioms.  (Question:  what  to  do  about  "overflow",  that  is, the 
finite  character  of  most  representations?)  Now  Hoare  (to  mention  only  one  person) 

has  already  started  writing  down  axioms  for  data  structures,  and  it  would  seem  that 

the  present  theory  offers  a  rigorous  framework  for  this  activity.  The  idea  requires 
much  more  study,  however. 

Finally,  we  must  agree  that  the  study  of  A-calculus  cannot  replace  the  study  of 
programming  languages.  It  is true  that  the  logical  notation  allows  us to  express  many 

3o [from  the  original  text]  The  recursive  function  people  have  been  considering  monotonic  functions  for 
a  long  time,  and  BekiC  recently  came  across  the  idea  again  in  his  study  of  automata  theory  [2].  Park  and 

Florentin  also  used  the  notion  in  discussions  of  the  Floyd-Manna  proof  theory  for  programs.  It  is 
mentioned,  for  example,  in  the  Eilenberg-Wright  automata  theory  via  categories.  Indeed,  it  is  a  "folk" 

notion.  The  author  became  interested  in  it  when  trying  to  find  the  best  induction  principle  for  de  Bakker's 
algebraic  approach  to  program  equivalence  -  and  he  is  indebted  to  de  Bakker  for  many  discussions  on 
trying  to  develop  a  useful  algebra.  (See  the  later  work  in  [S].)  Of  course,  he  knew  about  Platek's  work,  but 
Platek  does  not  "seem"  to  discuss  continuity. 

31 Milner  and  coworkers  did  in  his  LCF! 

A  type-theoretical  alternative  to  IS  WIM,  CUCH,  0  WHY  439 
computations  and  that  the  system  could  be  given  the  look  of  a  programming 

language,  but  we  have  not  built  into  the  theory  logical  notions  corresponding  to  the 
full glory  of the  assignment  statement  and  to  the  idea  of jumps  and  goto's.  Landin  has 

tried  to  do  this  with  ISWIM,  but  the  personal  view  of  the  author  is that  the  result  is 
not  quite  successful.  Landin  does  have  a clean,  regular,  and  powerful  language,  but  in 
a certain  sense  it is just  another  language:  evaluation  still must  be done  on  a machine. 

Now  maybe  Landin's  evaluations  are  easier  to  follow  than  some  other  methods,  but 
somehow  I  do  not  feel  that  he  has  given  a  "logical"  explication  of  the  notion  of 
assignment.  32 M Y current  idea  is that  we should  take  up  Strachey's  plan  of  giving  an 

"axiomatic"  discussion  of  the  store  and  its  transformations  using  the  theory  of  L and 
R values.  The  locations  (addresses)  and  the  stores  would  be  treated  as new  data  types. 
Why?  Because  they  have  machine  representations.  33 Well,  these  ideas  are  still  in 
a state  of flux, but  the  author  hopes  that  the  distinction  between  logical  and  data  types 
can  help  us  sort  out  the  features  of  a  rather  murky  landscape. 

6.  Afterthought  (1993) 

The  historical  overview  presented  here  by  the  revisions  to  the  paper  has  been  very 
brief  and  very,  very  selective.  A long  and  tiring  literature  search  would  be required  to 
write  a really  satisfactory  discussion  of all  major  developments  and  influences.  There 
are  many  names  that  should  have  been  mentioned.  Twenty-five  years  is  not  such 

a  long  time,  but  the  enormous  number  of  conference  proceedings  and  journal 
literature  produced  in  theoretical  areas  of  computer  science  and  programming- 

language  semantics  over  that  period  make  a bibliographer's  task  quite  daunting.  Also, 
each  month  brings  several  new  papers  from  many  very  active  centers  of research.  So, 
I  doubt  whether  a  good  history  can  ever  be  written  in  my  lifetime.  But  the  author  is 
not  trying  to  excuse  himself  for  not  doing  better! 

Rereading  this  paper  after  such  an  interval  was  a surprise  for  me, however.  Despite 
some  doubtful  rhetoric,  the  paper  still  makes  reasonable  sense,  has  clear  definitions, 
and  suggests  possible  trends  or  problems  that  were  indeed  taken  up  with  positive 

results.  Moreover,  not  all the  questions  raised  have  been  settled.  The  paper,  therefore, 
served  a  function,  and  it  is still  worthwhile  to  stop  to  consider  why. 

32 [from original text]  I  hope  Landin  will  reply  to  this  criticism,  because  it  seems  to  me  to  be  a  basic 
"philosophical"  point  that  should  be  cleared  up.  [comment  19933  The  author  rather  doubts  that  he  did. 

However.  work  by  Milner,  Plotkin,  and  many,  many  others  have  refined,  contrasted  and  related  "opera- 
tional  semantics"  to  "denotational  semantics". 

s3 Indeed,  the  ideas  were  much  developed,  as  shown  by  the  books  of  Stoy  [24]  and  Milne-Strachey  [16]. 
Nevertheless,  a  really  satisfactory  mathematical  theory  of  store  and  assignment  is  still  missing. 

440  D.S.  Scott 

References 

[l]  H.P.  Barendregt,  Tne  Lambda  Calculus  -  Its  Syntax  and  Semanti&&udies  in 

2 

Logic  and  the 
Foundations  of  Mathematics,  Vol.  103  (North-Holland,  Amsterdam,`%Qvised  editi  1984). 
[Z]  H.  BekiC,  Definable  operations  in  general  algebras,  and  the  theory  of  auto%fi-&i  fldwcharts,  Tech. 

Report,  IBM  Laboratory,  Vienna,  1969. 
[3]  C.  Bljhm,  The  CUCH  as  a  formal  and  description  language,  in:  T.B.  Steel,  Jr.,  ed.,  Formal  Language 

Description  Languages  for  Computer  Programming,  IFIP  Working  Conference  on  Formal  Language 

Description  Languages,  Vienna  1964  (North-Holland,  Amsterdam,  1966)  179-197. 
[4]  A.  Church,  A  formulation  of  the  simple  theory  of  types,  J.  Symbolic  Logic  5  (1940)  5668. 

[S]  J.W.  de  Bakker,  Mathematical  Theory  of Program  Correctness  (Prentice-Hall,  Englewood  Cliffs,  NJ, 

1980). 
[6]  J.W.  de  Bakker  and  D.  S.  Scott,  A  theory  of  programs,  IBM  Seminar,  Vienna,  August  1969,  in:  J.W. 

Klop  et  al.,  eds.,  J.  W. de  Bakker,  25 Jaar  Semantiek:  Liber  Amicorum  (CWI,  Amsterdam,  1989)  l-30. 
[7]  R.O.  Gandy  and  J.M.E.  Hyland,  Computable  and  recursively  countable  functions  of  higher  type,  in: 

R.O.  Gandy  and  J.M.E.  Hyland,  eds.,  Logic  Colloquium  76 (North-Holland,  Amsterdam,  New  York, 
Oxford,  1977)  407-438. 
[S]  M.J.  Gordon,  A.J.  Milner  and  C.P.  Wadsworth,  Edinburgh  LCF,  Lecture  Notes  in  Computer  Science, 

Vol.  78  (Springer,  Berlin,  Heidelberg,  New  York,  1979). 
[9]  C.A.  Gunter,  Semantics  of  Programming  Languages  -  Structures  and  Techniques  (MIT  Press,  Cam- 

bridge,  MA,  1992). 
[lo]  C.A.  Gunter  and  D.S.  Scott,  Semantic  domains,  in:  J.  van  Leeuwen,  ed.,  Handbook  of  Theoretical 

Computer  Science  (Elsevier,  Amsterdam,  1990)  633-674. 
[ll]  J.M.E.  Hyland,  Recursion  theory  on  the  countable  functionals,  Ph.D.  Thesis,  Univ.  of  Oxford,  1975. 
1121  J.M.E.  Hyland,  First  steps  in  synthetic  domain  theory,  in:  A.  Carboni,  M.C.  Pedicchio  and  G. 

Rosolini,  eds.,  Category  Theory,  Proc.  Como  1990,  Lecture  Notes  in  Mathematics,  Vol.  1488  (Springer, 
Berlin,  1990). 
[13]  S.C.  Kleene,  Introduction  to  Metamathematics,  The  University  Series  in  Higher  Mathematics  (van 

Nostrand,  New  York,  1952). 
[14]  P.J.  Landin,  The  next  700  programming  languages,  Comm.  ACM  9  (1966)  157-164. 
[lS]  J.  McCarthy,  A  basis  for  a  mathematical  theory  of  computation,  in:  P.  Braffort  and  D.  Hirschberg, 

eds.,  Computer  Programming  and  Formal  Systems  (North-Holland,  Amsterdam,  1963)  33-70. 
1163  R.  Milne  and  C.  Strachey,  A  Theory  of  Programming  Language  Semantics  (Chapman  and  Hall, 

London,  1976). 
[17]  J.H.  Morris,  Lambda  calculus  models  of  programming  languages,  Ph.D.  Thesis,  MIT,  Cambridge, 

MA,  1968. 
[18]  A.M.  Pitts,  A  co-induction  principle  for  recursively  defined  domains,  Theoret.  Comput.  Sci.  1994,  to 

appear;  available  as  University  of  Cambridge  Computer  Laboratory  Tech.  Report  No.  252,  April 

1992. 
[19]  A.M.  Pitts,  Relational  properties  of  recursively  defined  domains,  in:  Proc.  8th  Ann.  IEEE  Symp.  of 

Logics  in Computer  Science,  Montreal  1993  (1993). 
[20]  R.A.  Platek,  Foundations  of  recursion  theory,  Ph.D.  Thesis,  Stanford  Univ.,  1966. 
[21]  G.D.  Plotkin,  LCF  considered  as  a  programming  language,  Theoret.  Comput.  Sci.  5 (1977)  223-257. 
[22]  D.S.  Scott,  Lectures  on  a  mathematical  theory  of  computation,  in:  M.  Broy  and  G.  Schmidt,  eds., 

Theoretical  Foundations  of Programming  Methodology  (Reidel,  Dordrecht,  1982)  145-292. 
[23]  M.B.  Smyth,  Topology,  in:  S. Abramsky,  D.M.  Gabbay  and  T.S.E.  Maibaum,  eds.,  Handbook  of Logic 

in  Computer  Science,  Vol.  1 (Clarendon  Press,  Oxford,  1992)  641-761. 
[24]  J.E.  Stoy,  Denotational  Semantics:  The  Scott-Strachey  Approach  to  Programming  Language  Theory 

(MIT  Press,  Cambridge,  MA,  1977). 
[25]  P.  Taylor,  The  fixed  point  property  in  synthetic  domain  theory,  in:  Proc.  6th  Ann.  IEEE  Symp.  of Logics 

in  Computer  Science,  Amsterdam  1991  (1991)  152-161. 