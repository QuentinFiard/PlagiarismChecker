

Acta Informatica 35, 353-400 (1998)

cfl Springer-Verlag 1998

A fully abstract semantics for causalityin the

ss-calculus

Michele Boreale1, Davide Sangiorgi2
1 Dipartimento di Scienze dell'Informazione, Universit`a di Roma "La Sapienza", Roma, Italy
2 INRIA Sophia-Antipolis, 2004, route des Lucioles, B.P. 93,

F-06902 Sophia Antipolis Cedex, France (e-mail: davide.sangiorgi@sophia.inria.fr)

Received 11 December 1995 / 16 June 1997

Abstract. We examine the meaning of causality in calculi for mobile processes likethe

ss-calculus, and we investigate the relationship between interleaving and causalsemantics for such calculi. We separate two forms of causal dependencies on actions

of ss-calculus processes, called subject and object dependencies: The former originatefrom the nesting of prefixes and are propagated through interactions among processes
(they are the only form of causal dependencies present in CCS-like languages); thelatter originate from the binding mechanisms on names. We propose a notion of causal
bisimulation which distinguishes processes which differ for the subject or for the ob-ject dependencies. We show that this causal equivalence can be reconducted to, or
implemented into, the ordinary interleaving observation equivalence. We prove thatour encoding is fully abstract w.r.t. the two behavioural equivalences. This allows
us to exploit the simpler theory of the interleaving semantics to reason about thecausal one. In [San94b] a similar programme is carried out for location bisimulation
[BCHK91], a non-interleaving spatial-sensitive (as opposed to causal-sensitive) be-havioural equivalence. The comparison between the encodings of causal bisimulation
in this paper, and of location bisimulation in [San94b], evidences the similarities andthe differences between these two equivalences.

1 Introduction
In the field of semantics for process algebras, a major division is between interleavingsemantics and non-interleaving semantics. In the former case, only the temporal dependencies of action executions are observed; in the latter case also either the causaldependencies among actions (causal semantics) or the spatial distribution of systems
(location semantics) are considered. Non-interleaving semantics give a better accountof the concurrency and of the dependencies among the activities of a system. But
the easier mathematics of interleaving semantics has permitted the development, forinstance, of simpler algebraic characterisations and modal logics.

Lately, there has been a growth of interest for name-passing process algebras,in which the possibility of communicating names (a synonymous for `channels')
permits the modeling of systems with dynamic linkage reconfigurations. The best

354 M. Boreale, D. Sangiorgi
known of these calculi is the ss-calculus [MPW92]. Most theoretical research on the ss-calculus has dealt with interleaving semantics [MPW92, BD92, PS93, San92, Wal95];
a major concern has been investigation of its expressiveness. The non-interleaving sideremains largely unexplored; only very recently, the analysis of a location semantics
has appeared [San94b].The main goal of this paper is to investigate the the relationship between interleaving and causal semantics for the ss-calculus. We focus on the interleaving observationequivalence (also called weak bisimulation), written ss [Mil89, MPW92], and on the
non-interleaving causal bisimulation, written ssc [DDM88, DD89, Kie91]; these areamong the most studied behavioural equivalences in the respective groups. W.r.t. observation equivalence, causal bisimulation makes additional distinctions on processeson the basis of the causal dependencies for their actions. Our major result shows that
causal bisimulation can be reconducted to, or implemented into, observation equiva-lence. This allows us to use the simpler theory of the interleaving semantics to reason
about the causal one; the result also evidences the expressive power of the ss-calculus.Causal bisimulation represents quite a stable relation. Several approaches to
causality have independently led to it. Examples include Degano, De Nicola and Mon-tanari' history preserving bisimulation [DDM88], Darondeau and Degano' causal-tree
bisimulation [DD89] and Kiehn's global-cause bisimulation [Kie91]. (Comparisonsof causal bisimulation with other interleaving and non-interleaving equivalences can
be found in [Ace92b, Ace92a, DDM93].) Known descriptions of causal bisimulationover CCS-like languages are based on operational transition rules more complex than
the standard interleaving ones [Mil89], because they carry information about causaldependencies, in form of pointer-like objects among actions. This extra structure also
has to be lifted at the syntactic level, forcing an extension of the basic language;existing algebraic theories and proof systems of causal bisimulation [DD89, Kie93]
heavily rely on the added causal operators. A motivation for trying to implementcausal bisimulation at the interleaving level is to seek ways for avoiding the introduction of this extra structure.Another goal of this paper is to investigate the meaning of causality in calculi for
mobile processes. We have individuated two forms of dependencies between actions,which we have called subject and object dependencies. Subject dependencies originate
from the nesting of prefixes and are propagated through internal communications ofprocesses. For example, the processes

a.b.0 + b.a.0 can perform actions a and b ineither order. In either case, there is a dependency between the two actions - the firing

of the first enables the firing of the second. The propagation of subject dependenciesthrough interactions is shown in

* c (a.c.0 | c.b.0) (we use the ss-calculus notation
* a P for the restriction of name a in process P ). In this process, the interaction at
c creates a dependency of b from a; indeed, it holds that * c (a.c.0 | c.b.0) is causallyequivalent with

a.b.0. In CCS-like languages, i.e., languages in which actions representpure synchronisation, subject dependencies are the only form of causal dependencies.

Subject dependencies cannot be detected at the interleaving level. For example, onecannot detect the dependency between the actions

a and b of the process a.b.0 + b.a.0.Hence, in an interleaving semantics this process is equated to the process

a.0 | b.0;this equality is rejected in a causal semantics because in
a.0 | b.0 actions a and b areindependent.

The other form of dependencies, namely the object dependencies, are determinedby
ss-calculus binding mechanisms on actions; therefore they do not exist in CCS.Approximately, an action

* is object dependent from a previous action u if u acts asbinder for some name of
*; as such, u can be thought of as supplying some names to

Causality in the ss-calculus 355
*. There are two binding actions in the ss-calculus, namely input and bound output -the latter denoting the output of a restricted, i.e., private, name.

By contrast with subject dependencies, the possible object dependencies in a pro-cess can be detected at the interleaving level, simply revealed by the occurrences of

names in actions. For instance, consider the bound output transition P1 (* b)ahbi-! P2,which says that

P1 can perform the output at a of the private name b and become
P2 in doing so; the object dependencies from the action (* b)ahbi are given by thoseactions in the successive behaviour of

P2 in which b occurs free. A similar consideration can be made for input transitions P1 ahbi-! P2 - which says that P1 can performthe input at

a of b so becoming P2 - if name b is not already present in P1. Inother words, observationally equivalent processes show the same object dependencies

(see Proposition 4.3 in Sect. 4). In most of the cases, an object dependency is also asubject dependency (for instance, this is always the case for the object dependencies
arising from input actions). There are, however, exceptions. For instance, the process
P def= * b (ahbi. 0 | bhyi. 0) can perform the output at a of the restricted name b andthen an output at

b; the former action enables the latter, because it "opens" the re-striction at
b. There is an object dependency of the action at b from the action at a,but no subject dependency.

Various ways of observing subject and object dependencies are possible. Intu-itively, in any reasonable bisimulation-like equivalence, object dependencies are automatically taken into account, since they are detectable within the standard transitionsystem. In other words, observing object dependencies alone leads to the standard
observation equivalence. For the same reason, observing separately subject and ob-ject dependencies reduces to observing subject dependencies alone. A different choice
would be to take the union and not to distinguish between the two forms of depen-dencies.

We have chosen to keep subject dependencies separate from object dependencies.The reason is that they represent logically different forms of causality. Indeed, subject
dependencies are responsible for important properties of causal processes, that wouldnot hold for object dependencies alone. For example, in [BS97] we prove that, with
our choice, causal bisimulation is a congruence for the language considered in thispaper. By contrast, this property fails to hold for observation equivalence, where only
object dependencies are considered. This fact is further discussed in the concludingsection.

Therefore in this paper we distinguish processes which differ for the subject or forthe object dependencies. For instance, we shall discriminate between the process

P

above and the process Q def= * b (ahbi. bhyi. 0). Both processes can perform an actionat

a and then an action at b. However, in Q there is a double causal dependencybetween the two actions: One given by a prefix-nesting (i.e., a subject dependency),

and the other given by a name-binding (i.e., an object dependency). By contrast, in
P there is a single dependency between the two actions, given by name-binding.To define a causal bisimulation which reflects these discriminations on processes,

we enrich the standard (i.e., interleaving) transition system of the ss-calculus withinformation about the subject dependencies. Our enriched system is defined following
Kiehn's system [Kie91], which describes causal bisimulation in CCS (as noted above,in CCS all causal dependencies are subject dependencies). This implies adding explicit
causes to ss-calculus syntax and operational semantics. Causal bisimulation is definedon top of the enriched system using familiar bisimulation techniques.

356 M. Boreale, D. Sangiorgi

We then show that the sophisticated observational machinery of causal bisimu-lation (ss

c) can be represented from within the interleaving observation equivalence(ss). We define a compositional encoding [[ . ]] from the enriched calculus to the basic

one, and we prove that it is fully abstract w.r.t. ssc and ss; i.e., for each pair of terms
A and B in the enriched calculus we have:

A ssc B if and only if [[A]] ss [[B]] .
Through various examples, we show how this result and the algebraic theory of sscan be used to prove properties about ss

c. When restricted to the sublanguage with noobject dependencies, that is CCS, our encoding yields a characterisation of the classic

causal bisimulation of [DD89, Kie91] in terms of the observation equivalence of themonadic

ss-calculus.In the definition of encoding, we exploit an important feature of the

ss-calculus:The calculus naturally permits the description of data structures which can be created

and combined at run time. These data structures are modeled as standard processesaccessible via private names, which can be passed around and used to form more
complex structures. We use data structures called wires to encode the observability ofthe explicit causes used in the definition of causal bisimulation. A wire receives names
at an entrance-point and retransmit them at a bunch of end-points. Several wires canbe connected together, connection representing union of causes. Reachability among
wires, i.e. if an end-point of a wire can be reached from the entrance-point of anotherwire, encodes the subject dependencies between actions. When a target process [[

A]]performs a visible action
u, a wire is generated and a pointer to the wire - underthe form of a fresh name - is emitted. This pointer can be used to `explore' the

wire and determine the subject dependencies for u. Transmission of causes across aparallel composition, a distinguishing feature of causal bisimulation, is rendered as an
exchange of a private name, by which two previously separated wires get connected.The proof of our full abstraction theorem exploits a few properties of concatenation
of wires that permit symbolic manipulations of these structures.The paper mainly relates to [San94b]. There, a programme similar to that of the
present paper is carried out for location bisimulation [BCHK91], one of the mostconvincing spatial-sensitive semantics. An encoding from an enriched

ss-calculus oflocated processes to the standard
ss-calculus is given and proved to be fully abstractw.r.t. location bisimulation and observation equivalence. The treatment of causes in

communications is what distinguishes causal bisimulation from location bisimulation:In the former, the causes of the interacting actions must be appropriately merged,
whereas in the latter they remain separate. This explains the difference between thedata structures (the wires) used in the encoding of the two equivalences, here and in
[San94b]. In the case of causal bisimulation, the connections among wires, establishedas the computation proceeds, may create branching chains; by contrast, in the case
of location bisimulation connections among wires may only create linear chains.The difference also shows up in the proofs: Although the overall structure of the
proofs of full abstraction is similar in the two papers, the technical details are quitedifferent, sometimes strikingly so. A thorough comparison between causal and location
bisimulation via their encodings into observation equivalence presented here and in[San94b] is in Sect. 10. The outcomes of this comparison are certainly not new.
Careful studies of the relationship between causal- and spatial-sensitive equivalencescan be found in the literature [Ace92b, CD93, MY92, Kie91, Kie93, DP92]. The use
of the encodings gives us a refreshing perspective on the issue. (By the time of the

Causality in the ss-calculus 357
review of this paper, various works have continued to investigate the meaning ofcausality in the

ss-calculus. Causal semantics different from ours have been proposed,an overview of which can be found in [Pri96].) A study of the congruence properties

of the causal equivalence presented in this paper is in [BS97].The remainder of the paper is organised as follows. In Sect. 2, we review the syntax
of the standard polyadic ss-calculus and the definition of observation equivalence. InSect. 3 we present the proof techniques (mainly various forms of "bisimulation up-to")
that will be extensively used in later sections. In Sect. 4 we introduce the enrichedlanguage of causal terms, and we define causal bisimulation. In Sect. 5 we present the
encoding [[ . ]]. Its full abstraction is proved in Sects. 6 through 8, which are organisedas follows. In Sect. 6, some important properties of wire processes are shown. These
properties are used in Sect. 7, to establish the operational correspondence betweentransitions of causal terms and transitions of encoded processes, and in Sect. 8, to
derive the main theorems. In Sect. 9 we show some examples of applications ofthe encoding. In Sect. 10 causal bisimulation and location bisimulation are compared
using their encodings into observation equivalence presented here and in [San94b].The concluding section mentions directions for possible future research.

2 The polyadic ss-calculus
We start by reviewing the standard syntax and the interleaving semantics of thepolyadic

ss-calculus, as in [Mil91].

2.1 Syntax of standard processes
Definition 2.1 (Standard processes) Consider an infinite countable set N of namess.t. N " {

o/ } = ; . The class of the polyadic standard processes over names N ,written P (N ), is built from the operators of inaction, input prefix, output prefix,

silent prefix, sum, parallel composition, restriction, and replication:

P := 0 | a(eb). P | ahebi. P | o/. P | P1 + P2 | P1 | P2 | * a P | !P .
In the sequel, a, b, . . . , x, y, . . . range over names, and P, Q, R over standard pro-cesses. A tilde denotes a tuple. When the tilde is empty, the surrounding brackets ()

and hi will be omitted. 0 is the inactive process. An input-prefixed process a(eb). P ,
where eb has pairwise distinct components, waits for a tuple of names ec to be sent
along a and then behaves like P {ec/eb}, where {ec/eb} is the simultaneous substitution
of names eb with names ec. An output-prefixed process ahebi. P sends eb along a andthen continues like

P . A silent prefix o/.P denotes a process which may evolve to Pwithout interactions with the environment. Sum and parallel composition are used,

as in CCS, to express a choice and to run two processes in parallel. The restriction
* a P makes name a local, or private, to P . A replication ! P stands for a countableinfinite number of copies of

P in parallel. We write \Pi i2I Pi as an abbreviation for
Pi1 | . . . | Pin, I = {i1, . . . , in}. Symbol I will range over finite indexing sets. Weassign parallel composition and sum the lowest precedence among the operators.

In prefixes a(eb) and ahebi, we call a the subject and eb the object. We use ff to range
over prefixes and often abbreviate ff.0 as ff. The operators a(eb).P and * b P bind all

358 M. Boreale, D. Sangiorgi
free occurrences of names eb and b in P . These binders give rise in the expected way tothe definition of free names of a term. The definitions of name substitution and alpha
conversion are standard too, with renaming possibly involved to avoid capture of freenames. We identify processes or actions which only differ on the choice of the bound
names; this will avoid some tedious side conditions, especially in the definitions ofbisimulations. The symbol = will mean "syntactic identity modulo alpha conversion".

Sometimes, we use def= as abbreviation mechanism, to assign a name to an expressionto which we want to refer later. In an assertion, we say that a name is fresh to mean
that it it is different from any other name which is nominated in the assertion or whichoccurs in a process nominated in the assertion.

Remark 2.2 The above grammar does not have the operator of matching (often con-sidered in the

ss-calculus), and uses replication rather than recursive definitions ofagents. We have omitted matching not because of difficulties in its treatment, but because causal bisimulation has interesting congruence properties in the calculus with-out matching [BS97]. A discussion on this is deferred to the concluding section. We
adopted replication because it suffices to code up recursion (the encoding showed byMilner [Mil91, section 3.1] for the ordinary interleaving bisimilarity also works well
for causal bisimulation) and is easier to handle algebraically. \Lambda 

2.2 Sorting
Virtually all ss-calculus processes described in the literature obey some discipline inthe use of names. The introduction of sorts and sortings into the

ss-calculus [Mil91]intends to make this name discipline explicit. In the polyadic
ss-calculus, sorts arealso essential to avoid disagreement in the arities of tuples carried by a given name,

or applied to a given constant. Below, we review the definition of sorting and ofwell-sorted process.

Names are partitioned into a collection of subject sorts, each of which contains aninfinite number of names. We write

a : s to mean that name a belongs to the subjectsort
s; this notation is extended to tuples componentwise. Then object sorts, are justtuples of subject sorts, such as (

s1, . . . , sn) or (s). Finally, a sorting is a function Obmapping each subject sort onto an object sort. We write

s 7! ( ~s) 2 Ob, if Ob assignsthe object sort ( ~
s) to s. By assigning the object sort (s1, s2) to the subject sort s, oneforces the tuples carried by any name in

s to be a pair whose first component is a

name of s1 and whose second component is a name of s2. Thus, a prefix a(eb). P or
ahebi. P is well-sorted for a sorting Ob if a : s and s 7! (es) 2 Ob imply eb : es. Aprocess

P is well-sorted for Ob if all input and output prefixes in P are well-sorted.The sorting system also affects substitutions, which are only defined between names

of the same sort. In the remainder of the paper, all processes are supposed to bewell-sorted for some sorting

Ob.

2.3 The standard transition system and observation equivalence
We now come to the definition of observation equivalence. In an interleaving semantics, a ss-calculus process has three possible forms of action. A silent action P o/-! P 0represents interaction, i.e. an internal activity in

P . Input and output actions are, re-spectively, of the form

Causality in the ss-calculus 359

P ahebi-! P 0 and P (* eb0 )ahebi-! P 0 .
In both cases, the action occurs at a. In the input action, eb is the tuple of names which
are received. In the output action, eb is the tuple of names which are emitted, and ~b0 ` ebare private names which are carried out from their current scope. The angled brackets

in the input action are to stress that names eb represent the values communicated -
similarly to their role in an output action ahebi - as opposed to binders, which are
found in an input prefix a(eb). P .We use

u to represent the label of a generic action (not to be confused with ff,which represents prefixes). All names in an input action are free. In an output action

(* eb0)ahebi, names eb0 are bound, the remaining ones free. Bound and free names of anaction

u, respectively written bn(u) and fn(u), are defined accordingly. The namesof
u, briefly n(u), are bn(u) [ fn(u). Table 1 shows the standard transition systemof the

ss-calculus.1 We have omitted the symmetric versions of rules S-sum, S-parand
S-com. We work up to alpha conversion on processes also in transition systems,for which alpha convertible agents are deemed to have the same transitions. We often

abbreviate P o/-! Q with P -! Q, and write P bu-! Q to mean P u-! Q, if
u 6= o/ , and P = Q or P o/-! Q, if u = o/ . We write the composition of two relationsR and R0 as R R0.

Table 1. SOS for the standard processes

S-inp: a(ec). P ahebi-! P {eb/ec} S-out: ahebi. P ahebi-! P
S-tau: o/. P o/-! P S-par: P

u-! P 0

P | Q u-! P 0 | Q bn(

u) " fn(Q) = ;

S-com: P

(* eb0)ahebi-! P 0 Q ahebi-! Q0

P | Q o/-! * eb0 (P 0 | Q0) eb0 " fn(Q) = ;

S-res: P

u-! P 0

* c P u-! * c P 0

c 62 n(u) S-open: P

(* eb0)ahebi-! P 0

* c P (* eb0c)ahebi-! P 0

c /= a, c 2 eb - eb0

S-sum: P

u-! P 0

P + Q u-! P 0 S-rep:

P | ! P u-! P 0

! P u-! P 0

Definition 2.3 (strong bisimilarity) A symmetric relation R ` P (N )*P (N )
is a strong bisimulation if P R Q and P u-! P 0 imply that2 there exists Q0 s.t.

1 In the transition system of Table 1 the bound names of an input are instantiated as soon as possible,

in the input rule; therefore it is an early transition system [San92], as opposed to a late transition
system [MPW92, Mil91] in which the instantiation is done later, in the communication rule. Theadoption of an early transition system naturally leads to the adoption of an

early bisimulation. See[PS93, San93], for instance, for discussions about different forms of transition system and bisimulation

for the ss-calculus. In this paper we only consider the early system.2
We omit the requirement bn(u) " fn(Q) = ; , since we work up-to alpha conversion.

360 M. Boreale, D. Sangiorgi
Q u-! Q0 and P 0 R Q0. Two processes P and Q are strongly bisimilar written
P , Q, if P R Q, for some strong bisimulation R .

Relation , is preserved by all ss-calculus operators but input prefix, for which,however, a special inference rule is valid:

if, for each ec, P {ec/eb} , Q{ec/eb} then a(eb). P , a(eb). Q.
We sometimes also use the largest congruence contained in ,, denoted by ,c, anddefined as follows:

P ,c Q if and only if, for each name substitution oe, P oe , Qoe.
Table 2 lists a few simple algebraic laws which are valid for ,c (and hence forthe relations coarser than it, such as ,). For ,, an expansion law similar to that used

in CCS [Mil89] is valid; details can be found in [MPW92] and [PS93].
Remark 2.4 The third law for restriction in Table 2 allows us to extrude the scope ofa restriction across a parallel composition, provided that the restricted name is fresh.

Since we work up-to alpha conversion, we shall apply the law without recalling thisside condition. \Lambda 

Table 2. A few laws for strong bisimulation congruence (,c)
Abelian monoid laws for + : P1 + P2 ,c P2 + P1

P1 + (P2 + P3) ,c (P1 + P2) + P3

P + 0 ,c PAbelian monoid laws for | :
P1 | P2 ,c P2 | P1
P1 | (P2 | P3) ,c (P1 | P2) | P3

P | 0 ,c PRestriction:

* a 0 ,c 0
* a * b P ,c * b * a Pif
a 62 fn(P2), then (* a P1) | P2 ,c * a (P1 | P2)Replication: !

P ,c P | ! P

As usual, the `weak' arrow =) is the reflexive and transitive closure of -!, that is
=)= Sn>=0 -!n; then u=) stands for =) u-!=). If P =) Q then we say that Q

is a o/ -derivative of P . Finally, similarly to P bu-! Q, we use P bu=) Q to mean:
P u=) Q, if u 6= o/ , and P =) Q, if u = o/ .On the weak arrows we define observation equivalence, sometimes called weak

bisimulation. This is the interleaving behavioural equivalence we are most interestedin.

Definition 2.5 (observation equivalence) A symmetric relation R ` P (N ) *
P (N ) is a weak bisimulation (or a ss-bisimulation) if P R Q and P u=) P 0

imply that there exists Q0 s.t. Q bu=) Q0 and P 0 R Q0. Two processes P and Q areobservationally equivalent, (or weakly bisimilar), written

P ss Q, if P R Q, for someweak bisimulation R .

Observation equivalence is preserved by all operators except input prefix and sum;for input prefix, a congruence rule similar to that stated for , is valid.

Causality in the ss-calculus 361
3 Proof techniques
For the proof of the results in this paper, we appeal to the expansion relation and tovarious "up-to" techniques.

3.1 The expansion relation
The expansion relation . [AKH92, SM92] is an asymmetric variant of ss whichallows us to count the number of

o/ -actions performed by the processes. Thus, P . Qholds if
P ss Q but also Q has at least as many o/ -moves as P . The expansion relationprovides us with better `control' on

o/ -moves than the ordinary ss.

Definition 3.1 (expansion) A relation R is an expansion if P R Q implies:

1. Whenever P u-! P 0, there exists Q0 s.t. Q u=) Q0 and P 0 R Q0;
2. whenever Q u-! Q0, there exists P 0 s.t. P bu-! P 0 and P 0 R Q0.
We say that Q expands P , written P . Q, if P R Q, for some expansion R .

Note that, in contrast with the other relations considered in the paper, . is notsymmetric. For instance,

P . o/. P , but o/. P 6. P since o/. P has to perform more
o/ -actions than P in order to mimic its actions. Relation ., which is indeed a preorder,enjoys the same congruence properties as ss. The following proposition sums up the

relationship among the behavioural relations considered in the paper.
Proposition 3.2 The following is a chain of strict inclusions: ,c ae , ae.ae ss. \Lambda 

3.2 The "up-to" techniques
The "up-to" techniques allow us to reduce the size of a relation R to exhibit forproving bisimilarities [Mil89, SM92, San94a]. The up-to techniques which we use
in the paper allow us to manipulate the derivatives of a pair of processes in twoways: First, we can replace such derivatives with behaviourally-equivalent processes;
secondly, we can cut common contexts of the derivaitves. The contexts we cut are ofthe form

* ec (R | [*]); they are called static contexts.

Definition 3.3 (ss-bisimulation up-to context and up-to &) A symmetric relation
R is a ss-bisimulation up-to context and up-to & if P R Q and P u=) P 00 im-ply that there are a static context

C[*] and processes P 0 and Q0 s.t. P 00 & C[P 0],

Q bu=) & C[Q0] and P 0 R Q0.
Proposition 3.4 If R is a ss-bisimulation up-to context and up-to &, then R ` ss.
Proof. By showing that

R 0 = {(P, Q) : for some static context C[*] and processes P 0, Q0it holds that

P & C[P 0], Q & C[Q0] and P 0 R Q0 }

362 M. Boreale, D. Sangiorgi
is a ss-bisimulation. This proves the thesis because R ` R 0. Details of the proofcan be found in [San94b]. \Lambda 

We do not know whether the above soundness theorem holds if Definition 3.3 isweakened by replacing all occurrences of . with ss.
The next up-to technique is new; it is the corresponding for expansion of Definition3.3. Note the occurrence of , in clause (1) of Definition 3.5: We do not know whether
the replacement of , with . preserves the soundness of the technique.
Definition 3.5 (expansion up-to context and up-to &) A relation R is an expan-sion up-to context and up-to & if

P R Q implies:

1. Whenever P u-! P 00, there are a static context C[*] and processes P 0 and Q0

s.t. P 00 , C[P 0], Q u=) & C[Q0] and P 0 R Q0;
2. whenever Q u-! Q00, there are a static context C[*] and processes P 0 and Q0 s.t.

Q00 & C[Q0], P bu-! . C[P 0] and P 0 R Q0.
Proposition 3.6 If R is an expansion up-to context and up-to &, then R ` ..
Proof. By showing that

{(P, Q) : for some static context C[*] and processes P 0, Q0it holds that

P . C[P 0], Q & C[Q0] and P 0 R Q0 }

is an expansion relation. The proof is similar to the proof of Proposition 3.4. \Lambda 

4 Causal bisimulation
The section begins with a discussion on the way object and subject dependenciesin processes can be detected. Whereas the former are detectable with the standard
transition system, the latter require a richer system, with explicit causes. We show afew properties of the richer system and define causal bisimulation on top of it.

Object dependencies are the dependencies determined by the binding mechanismson names. To know the object dependencies in a process, we look at its runs. A run
of a process is a sequence of transitions starting from that process.
Definition 4.1 A run P1 u1-! P2 . . . Pn u

n-! Pn+1, n >= 1, is fresh when for all

1 <= i <= n if ui = * eb aheci or ui = ahebi, then eb " fn(Pi) = ; and for all j < i,e
b " n(uj) = ; ; in this case, we say that names eb are introduced in ui.

In a fresh run, names exported in an output or imported in an input are all fresh.Fresh runs reveal all possible name dependencies in processes in a simple way.

Definition 4.2 Given a fresh run P1 u1-! P2 . . . Pn u

n-! Pn+1, we say that ui is

object dependent on uj, 1 <= j < i <= n, if there is a name introduced in uj whichis among the free names of

ui.

For instance, in the run * b (b | a(x). xhbi) ahci-! * b chbi-! b-! 0 | 0, the secondaction is object dependent on the first and the third is object dependent on the second.
Object dependencies can already be observed at the interleaving level, since they are

Causality in the ss-calculus 363
shown by the occurrences of names in actions: Names themselves act as pointersamong actions. Thus, observationally equivalent processes exhibit the same object
dependencies.
Proposition 4.3 If P1 ss Q1 and P1 has a fresh run P1 u1-! P2 . . . Pn u

n-! Pn+1 where

ui is object dependent on uj, then also Q1 has a fresh run Q1 u1-! Q2 . . . Qn u

n-! Qn+1

where ui is object dependent on uj. \Lambda 

The other form of dependencies among actions, namely the subject dependencies,are determined by the nesting of prefixes. By contrast with the object dependencies,

the subject dependencies are not detected in an interleaving semantics (see discussionin Sect. 1). To capture them, we add explicit causal information to the standard syntax
and operational semantics of the calculus. For this, we follow Kiehn's system [Kie91],which describes the causal dependencies for CCS processes, and transport it onto the
ss-calculus.For the extra causal information, we use an auxiliary set K of causes. In the

enriched system, visible transitions are of the form A

u-!

K; k A0. The visible actionu is associated with a unique cause k 2 K . The set of actions from which u is

causally related is revealed by means of the set K `fin K of their associated causes.We call

K the cause-set for u. If a successive action u0 is caused by u, then u0will have

k (i.e., the cause associated with u) in its cause-set. In the term syntax, theexplicit use of causes is accompanied by the introduction of a causal prefix

K :: A,for
K `fin K , which says that the cause-set of any action of the process A mustcontain

K.By contrast, silent actions are not observable and do not exhibit causes. However,

causes do play an important role in the communication rule, for the causes of twointeracting actions must be appropriately merged. Consider the sequence of transitions

* b (a. b. c. 0 | d. b. 0)

a-!

; ; k1 * b ({k1} :: b. c. 0 | d. b. 0)
d-!

; ; k2 * b ({k1} :: b. c. 0 | {k2} :: b. 0)

o/-! * b ({k1, k2} :: c. 0 | {k1, k2} :: 0) c-!{

k1, k2}; k3

* b ({k1, k2} :: {k3} :: 0 | {k1, k2} :: 0) .

They show that the actions at a and d have no cause; and that the action at c iscausally dependent on

a and d. The latter makes sense: c could fire only becauseboth
a and d fired. Note that in the o/ -transition, the cause-sets {k1} and {k2} of theconsumed actions are merged.

4.1 Syntax and transitional semantics of causal terms
The notations and conventions introduced on standard processes - including the iden-tification of alpha equivalent processes - extend to the enriched language of causal
processes below, and will not be repeated.We let

k and h range over causes (i.e., elements of K ), K and H over finitesubsets of K , and

A and B over causal processes. We denote by K (A) the set ofcauses which appear in the causal process

A. To improve readability, in the syntax ofour terms and of our operational rules we abbreviate a union of sets of causes

K [ K0

364 M. Boreale, D. Sangiorgi
as K, K0 and we write a singleton set {k} simply as k. Thus, A

u-!

K, K0; k A0 stands

for A

u-!

K [ K0; k A0, and k :: A stands for {k} :: A. 3

Definition 4.4 (causal processes) Consider an infinite countable set N of names,and an infinite countable set K of causes with K , N and {

o/ } pairwise disjoint.The language of polyadic causal processes over causes K and names N , written

Pc(N , K ), is given by the following grammar:

A := K :: A | A | A | * a A | P
where P is a P (N ) term (i.e., a standard term) and K `fin K .
We do not allow the presence of causes underneath dynamic operators (prefixes, sumsand replications), because we are only interested in derivatives of standard processes,

for which these cases may never arise. This is clear from the transition rules forcausal terms, in Table 3. The rules for the prefixes and for communication express the
essence of the whole semantics: Inp and Out, for input and output prefixes, introducecauses into processes;

Cau, for the causal prefix K :: A, makes any action performedby
A dependent upon K. Rule Com implements the cause exchange discussed atthe beginning of this section. In the rule, the notation

A[k  K] indicates thereplacement of
k with the elements of K, in any set of causes in A. For instance,({
k1, k2} :: ff.0)[k2  {k3, k4}] is {k1, k3, k4} :: ff.0. The rules for the remainingoperators are formally identical to the standard rules of Table 1.

Our rules, when restricted to the sublanguage of finite 0-adic (i.e. CCS) causalterms, coincide with Kiehn's rules [Kie91]. The only variation is in rule

Com: Inour case, processes
A1 and A2 use the same fresh cause k, whereas the obvious
ss-calculus's adaptation of Kiehn's rule, called Com0 below, uses two distinct causes:

Com0 :

A1

(*eb0)ahebi-!

K1; k1 A01 A2

ahebi-!
K2; k2 A02

A1 | A2 o/-! * eb0 (A01[k1  K2] | A02[k2  K1])

k1 /2 K (A1), k2 /2 K (A2), eb0 " fn(A2) = ; .

Anyhow, the two rules Com and Com0 are interderivable, as can be proved usingLemma 4.5 below to rename causes.

Weak causal transitions are defined in the usual manner, therefore A

u=)

K; k A0

is A =)

u-!

K; k =) A0. The definition of well-sortedness for causal terms is givenas for standard processes (all input and output prefixes in a term must respect the

discipline imposed by the chosen sorting). All causal terms considered in the paperare supposed to be well-sorted.

3 The prefix k :: A is taken as a primitive operator in [Kie91].

Causality in the ss-calculus 365

Table 3. SOS of causal terms
Rules for visible transitions:

Out : ahebi. A

ahebi-!

; ; k k :: A Inp : a(eb). A

aheci-!
; ; k k :: A{ec/eb}

Cau :

A

u-!

K; k A0

K0 :: A

u-!

K, K0; k K0 :: A0

Par :

A1

u-!

K; k A01

A1 | A2

u-!

K; k A01 | A2

bn(u) " fn(A2) = ;

Res :

A

u-!

K; k A0

* c A

u-!

K; k * c A0

, c /2 n(u) Open :

A

(* eb0)ahebi-!

K; k A0

* c A

(* eb0c)ahebi-!

K; k A0

, c /= a, c 2 eb - eb0

Sum :

A1

u-!

K; k A01

A1 + A2

u-!

K; k A01

Rep :

A | ! A

u-!

K; k A0

! A

u-!

K; k A0

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Rules for silent transitions:

T-pre : o/.A o/-! A T-sum :

A1 o/-! A01

A1 + A2 o/-! A01

T-par :

A1 o/-! A01

A1 | A2 o/-! A01 | A2

T-res :

A o/-! A0

* c A o/-! * c A0

Com :

A1

(* eb0)ahebi-!

K1; k A01 A2

ahebi-!
K2; k A02

A1 | A2 o/-! * eb0 (A01[k  K2] | A02[k  K1]) e

b0 " fn(A2) = ; , k /2 K (A1, A2)

T-cau :

A o/-! A0

K :: A o/-! K :: A0

T-rep :

A | ! A o/-! A0

! A o/-! A0

4.2 Basic properties of the transitional semantics of causal terms
A cause substitution ae is a function from K to K . We write kae = k0 if ae mapscause

k onto k0; and
Kae for the set of causes {kae : k 2 K}. Also, Aae is the process obtained from Aby replacing each of its causes

k with kae. The cause substitution that maps k onto k0and is the identity elsewhere, will be sometimes written as a replacement [

k  k0].

366 M. Boreale, D. Sangiorgi

Lemma 4.5 allows us to rename the cause k of a transitions A

u-!

K; k A0. Later,Lemma 4.8 will extend this result to weak transitions. Lemmata 4.6 and 4.7 relate

the actions of causal terms A and Aae.
Lemma 4.5 Let k, k0 2 K , with k0 62 K (A).

1. If A

u-!

K; k A0, then A

u-!
K; k0 A00 and A00[k0  k] = A0.

2. If A

u-!

K; k0 A0, then A

u-!
K; k A00 and A0[k0  k] = A00.

Proof. Easy transition inductions. \Lambda 
Lemma 4.6 Let ae be a cause substitution. Then

1. A

u-!

K; k A0 implies Aae

u-!
Kae; kae A0ae.
2. A o/-! A0 implies Aae o/-! A0ae.3.

A =) A0 implies Aae =) A0ae.

4. A

u=)

K; k A0 implies Aae

u=)
Kae; kae A0ae.

Proof. (1) is proved by transition induction; (2) is also proved by transition induction,but in the case when

Com is the last rule applied, we also need part (1) of thislemma and Lemma 4.5. Assertion (3) is proven by iterating (2) and finally (4) is a

consequence of (1) and (3). \Lambda 
Lemma 4.7 Let ae be a cause substitution. Then

1. If Aae

u-!

K; k A0 then there are K0, k0 and A00 s.t. A

u-!
K0; k0 A00 with K0ae = K, k0ae = kand
A00ae = A0.

2. If Aae o/-! A0 then there is A00 s.t. A o/-! A00 with A00ae = A0.3. If

Aae =) A0 then there is A00 s.t. A =) A00 with A00ae = A0.

4. If Aae

u=)

K; k A0 then there are K0, k0 and A00 s.t. A

u=)
K0; k0 A00 with K0ae = K, k0ae = kand
A00ae = A0.

Proof. Similar to that of Lemma 4.6. \Lambda 

By combining Lemmata 4.5, 4.6(3) and 4.7(3), we obtain the analogue of Lemma4.5 for weak transitions:

Lemma 4.8 Let k, k0 2 K , with k0 62 K (A).

1. If A

u=)

K; k A0, then A

u=)
K; k0 A00 and A00[k0  k] = A0.

2. If A

u=)

K; k0 A0, then A

u=)
K; k A00 and A0[k0  k] = A00. \Lambda 

4.3 Causal bisimulation
Weak causal bisimulation is defined on top of weak causal transitions by means offamiliar bisimulation techniques. We shall omit the adjective "weak" and simply call
it causal bisimulation.

Causality in the ss-calculus 367
Definition 4.9 (causal bisimulation) A symmetric relation R ` Pc(N , K ) *P

c(N , K ) is a causal bisimulation (or ssc-bisimulation) if A R B implies:

1. whenever A =) A0 there exists B s.t. B =) B0 and A0 R B0;
2. whenever A

u=)

K; k A0 with k /2 K (A, B), there exists B0 s.t. B

u=)
K; k B0 and
A0 R B0.

Two causal terms A, B are causally bisimilar, written A ssc B, if A R B for somecausal bisimulation R . \Lambda 

Lemma 4.10 For any cause substitution ae, if A ssc B then Aae ssc Bae.
Proof. We show that

R = {(Aae, Bae) : ae is a cause substitution and A ssc B}
is a ssc-bisimulation. Suppose A ssc B, and consider clause (2) of Definition 4.9. If

Aae

u=)

K; k A0 with k /2 K (Aae) (1)

then from Lemma 4.7, for some K0, k0 and A00, we have

A

u=)

K0; k0 A00 with A00ae = A0, K0ae = K and k0ae = k. (2)

Let k0 /2 K (A, B); by Lemma 4.8,

A

u=)

K0; k0 A000 with A000[k0  k0] = A00

and, since A ssc B, also

B

u=)

K0; k0 B000 with A000 ssc B000 .

Reversing, the steps, we get

B

u=)

K0; k0 B00 with B000[k0  k0] = B00

and

Bae

u=)

K00; k00 B0 with K00 = K0ae, k00 = k0ae and B0 = B00ae . (3)
The action in (3) matches the one in (1): The conditions in (2) and (3) imply K = K00and

k00 = k; moreover, it holds that B0 = B000[k0  k0]ae, A0 = A000[k0  k0]ae and
A000 ssc B000. We conclude that (A0, B0) 2 R .The other clause of the definition of ss

c can be dealt with similarly. \Lambda 

The following is a characterisation of causal bisimulation which will be exploitedin the proof of completeness for the encoding. The difference from Definition 4.9 is

that the requirement k 62 K (A, B) in clause (2), input case, is dropped.
Definition 4.11 A symmetric relation R ` Pc(N , K ) * Pc(N , K ) is a loosecausal bisimulation (or ss0

c-bisimulation) if A R B implies:

368 M. Boreale, D. Sangiorgi

1. whenever A =) A0 there exists B s.t. B =) B0 and (A0, B0) 2 R;
2. whenever A

ahebi=)

K; k A0 there exists B0 s.t. B

ahebi=)
K; k B0 and (A0, B0) 2 R;

3. whenever A

*eb0 ahebi=)

K; k A0 with k /2 K (A, B), there exists B0 s.t. B

*eb0 ahebi=)

K; k B0 and(
A0, B0) 2 R.

Two causal terms A and B are loose cause bisimilar, written A ss0c B, if A R B, forsome loose causal bisimulation R .

Proposition 4.12 The relations ssc and ss0c coincide.
Proof. We only have to show that ssc ` ss0c; the converse follows immediately becausethe defining clauses for ss0

c are equal or stronger (in the input case) than those for ssc.We proof that ss
c is a ss0c-bisimulation. Let A ssc B. We check clause (2) of

Definition 4.11. Suppose A

ahebi=)

K; k A0, and take a fresh cause k0. Then from Lemma4.8, letting
u = ahebi,

A

u=)

K; k0 A00, with A00[k0  k] = A0,

which implies, since A ssc B,

B

u=)

K; k0 B00, with A00 ssc B00 .

Again, by Lemma 4.8, we get

B

u=)

K; k B0, with B00[k0  k] = B0 .

From Lemma 4.10, A00 ssc B00 implies A0 = A00[k0  k] ssc B00[k0  k] = B0,which concludes the case. \Lambda 

5 The encoding
In this section we present and discuss the encoding of causal bisimulation into obser-vation equivalence.

We use two new sorts C and T of names, `new' meaning that these sortsdo not occur in the original sorting of causal terms. There will be a one-to-one
correspondence between causes - i.e. elements of the set K - in a causal process,and names of sort C in the standard process enconding it. Hence, for notational
convenience, we identify the sets C and K (any ambiguity is resolved by thedifferent classes of processes in which elements of these sets occur as causes and as
names, and by the different use of causes and names), and call a name in C a cause,or also a cause name. Names in C carry names of sort T . We sometimes call a
name in T a token.The intuition underlying the encoding is as follows. Causes in a causal term

Aare encoded in the standard term [[
A]] as standard processes called wires. A wire
k B K takes a token in input at the entrance point k and emits it, as an output,at each end-point

k0 2 K. Two wires are connected when an end-point of the first

Causality in the ss-calculus 369
is the entrance-point of the other; this connection represents union of causes. When[[

A]] performs a visible action, a name k is emitted and a wire with entrance-point
k is created. Intuitively, k represents the unique cause associated to that transition,whose cause-set is represented by all end-points of all wires reachable from

k. Anobserver that receives
k can determine this cause-set by sending a token at k and andobserving where (i.e., at which end-points) the token can be retrieved.

As an example, the encoding of the causal process K :: a.b.0 will have thefollowing transitions:

[[K :: a.b.0]] ahki-! k B K | [[k :: b.0]] bhhi-! k B K | h B k | [[h :: 0]] .
(This shows how linear chains are generated.) A crucial point is the modeling ofsilent transitions. The "merging of causes" discussed at the beginning of Sect. 4 is

implemented as an exchange of a bound name, by which the appropriate wires getconnected. As an example, we have:

[[K :: a.P | H :: a.Q]] o/-! * k ( k B K | k B H | [[k :: P | k :: Q]])
(This shows how branching chains arise.) After the communication, P and Q havea common cause-set

K [ H, referred to by k. Indeed, the derivative of the abovetransition will turn out to be observationally equivalent to [[

K [ H :: P | K [ H :: Q]].We come to the actual definition of wire processes. In the sequel, we use

v torange over T , and we abbreviate N [ C [ T as N +.

Definition 5.1 (Wire processes) The standard process k B K 2 P (N +), calledwire process, is so defined:

k B K def= ! k(v). \Pi k02K ! k0hvi .
Name k is the entrance point of the wire, and names in K are the end-points.

We explain the presence of the replication operators in wire processes. In general,once a wire is created, an unbounded number of other wires may get connected to

it, both at its entrance point, and - due to the branching structure of causal chains -at each of its end-points. The outermost replication in the definition of

k B K isto make the wire persistent, so that it can be used an arbitrary number of times. The

innermost replications in the definition of k B K make a token availability at anend-point persistent. This ensures that a token which has reached an end-point can be
transmitted to all unboundedly many wires connected with it.The encoding is presented in Table 4. It acts as a homomorphism, from P

c(N , K )to P (N +), everywhere but on prefixes. The encoding adds extra components (the

wires) into processes, thus increasing the number of their states. The problem, how-ever, is strongly alleviated by Lemmata 7.1 and 7.2 and the Cancelation Lemma 8.4,
in Sect. 8, which allow us to get rid of wires when they appear at the outermost levelof processes.

The encoding has to be defined on sortings too, since an extra component (ofsort C ) is added to the arities of names in the target processes. To ease the notation,
we abbreviate k B (K [ H) as k B (K, H) , and k B ({k0}, K) as k B(

k0, K) . Similarly, we abbreviate [[A]](K [ K0) as [[A]](K, K0) and [[A]]({k}, K0) as[[

A]](k, K0). Note that when the source term are 0-adic terms (i.e., CCS terms), Table4 gives us an encoding of the classical causal bisimulation of CCS [DD89, Kie91]

370 M. Boreale, D. Sangiorgi

Table 4. The encoding of causal terms
Encoding of the sorting:

[[Ob]] def= {s 7! (es, C ) : s 7! (es) 2 Ob} S{

C 7! (T ), T 7! ()}
where C and T are new sorts, i.e. sorts which do not appear in Ob.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Encoding of causal processes:

We assume k : C , K `fin C and v : T ; we set [[A]] def= [[A]]; , where [[A]]K is defined by inductionon the structure of

A as follows:

[[a(eb). A]]K def= a(ebk). ( k B K | [[A]]{k}), k 2 K - K
[[ahebi. A]]K def= * k ahebki. ( k B K | [[A]]{k}), k 2 K - K
[[K0 :: A]]K def= [[A]](K0, K) [[o/.A]]K def= o/.[[A]]K

[[A1 | A2]]K def= [[A1]]K | [[A2]]K [[A1 + A2]]K def= [[A1]]K + [[A2]]K

[[* a A]]K def= * a [[A]]K [[0]]K def= 0

[[ ! A]]K def= ![[A]]K

into the observation equivalence of the monadic ss-calculus. It is straightforward tosee that there is agreement between the definitions of the encoding on processes and
on sortings:
Proposition 5.2 If A is well-sorted for Ob then [[A]] is well-sorted for [[Ob]]. \Lambda 

6 Fundamental lemmata
All processes in this section are standard processes from P (N +).

6.1 Properties of the replication operator
We recall some basic properties of the replication operator, which will be used severaltimes in this and in the next section.

Lemma 6.1

1. ! P | ! P ,c ! P .2. !

ff.P ,c ff.( ! ff.P | P ), if bn(ff) " fn(ff. P ) = ; . \Lambda 

Lemma 6.2 asserts the distributivity of restricted replications over parallel com-position and replication. It generalises an original result by Milner [Mil91] to the case

in which there is a finite set of restricted replications - rather than one only.
Lemma 6.2 Suppose name k occurs free in the processes P1, P2, P and Ri (i 2 I)only in output subject position. Then

1. * k (P1|P2|\Pi i2I ! k(v). Ri) ,c * k (P1|\Pi i2I ! k(v). Ri)|* k (P2|\Pi i2I ! k(v). Ri).

Causality in the ss-calculus 371

2. * k ( ! P | \Pi i2I ! k(v). Ri) ,c ! * k (P | \Pi i2I ! k(v). Ri).
Proof. Analogous to the proofs of Milner's distributivity laws in [Mil91]: One hasto define strong bisimulations that are preserved by name substitutions. Some simplification can be achieved by working "up-to static contexts", as shown in [San94a].\Lambda 

Lemma 6.3

1. * k ( ! ff.P | \Pi i2I!k(v).Ri) , ! ff. (* k (P | \Pi i2I ! k(v). Ri)), if k 62 n(ff), bn(ff) "fn(

\Pi i2I ! k(v). Ri) = ; , and k occurs free in P and Ri (i 2 I) only in outputsubject position;

2. * k (\Pi i2I !k(v).\Pi k02Ki ! k0hvi | ! khvi) & \Pi k02[i2IKi ! k0hvi, if k 62 [i2IKi.
Proof. (1) is proved by first using Lemma 6.2(2), to distribute the restricted replications
\Pi i2I!k(v).Ri over the replication ! ff.P , and then using expansion and restriction laws(Table 2), in the order; (2) is proved by exhibiting the appropriate expansion relation.

\Lambda 

6.2 Properties of wire-processes
Wire processes k B K are the core of our implementation of causal bisimulation.To prove the correctness of the implementation, we first need to establish a few
properties of compositions of such wires (Lemmata 6.4 and 6.6). To begin with, twowires

k0 B (k, K0) and k B K , in which the entrance point of the second is amongthe end-points of the first, are, so to speak, `connected': Hence the end-points

K ofthe second can be added to the end-points
K0 of the first. This is, roughly, the contentof Lemma 6.4.

If in addition to the above hypothesis, we assume that the common extreme k of thetwo wires is inaccessible, then their composition yields a single wire

k0 B (K, K0) ,whose end-points include all end-points of the given wires, except the inaccessible

k.This is, roughly, the content of Lemma 6.6 (the lemma is slightly more general, in

that it allows a product \Pi i2I k B Ki of wires in place of the single wire k B K .)The content of Lemmata 6.4 and 6.6 is depicted in Fig. 1.

Lemma 6.4 k0 B (k, K0) | k B K & k0 B (k, K0, K) | k B K .
Proof. Fixed K, K0, k and k0, let

P def= k B K | k0 B (k, K0, K)
Q def= k B K | k0 B (k, K0) .

We prove that the singleton {(P, Q)} is an expansion up to context and up to ,(This technique is just a particular case of the expansion up to context and up to &

(Definition 3.5), since , ae&). We show how the moves of P are matched by Q; theconverse is similar. The only non-trivial case is when the move of

P originates from
k0 B (k, K0, K) , as follows:

P k

0hwi-! , \Pi 

k002{k}[K0[K ! k00hwi | P .

This can be matched by Q thus:

372 M. Boreale, D. Sangiorgi

Fig. 1. Graphical representations of wire processes
Q k

0hwi-! , k B K | \Pi 

k002{k}[K0 ! k00hwi | k0 B (k, K0)o/-! ,
\Pi k002K ! k00hwi | \Pi k002{k}[K0 ! k00hwi | Q,
\Pi k002{k}[K0[K ! k00hwi | Q

where the last step, in the case when K " ({k} [ K0) /= ; , Lemma 6.1 is needed. Thisis enough because {(

P, Q)} is an expansion up-to context and up-to ,. \Lambda 

To prove Lemma 6.6, we use the following lemma:

Causality in the ss-calculus 373
Lemma 6.5

1. * k ( ! ff.P | \Pi i2I k B Ki ) , ! ff.(* k (P | \Pi i2I k B Ki )), if k 62 n(ff) and k canoccur free in

P only in output subject position;2.
* k (\Pi i2I k B Ki | !khvi) & \Pi k02[i2IKi ! k0hvi, if k 62 [i2IKi.

Proof. The clauses of this lemma are instances of clauses (1) and (2), respectively, ofLemma 6.3. \Lambda 

Lemma 6.6 If k /2 ({k0} [ K0 [ ([i2IKi)), then

* k ( k0 B (K0, k) | \Pi i2I k B Ki ) & k0 B (K0, [i2IKi) .

Proof. Let Q def= * k ( k0 B (K0, k) | \Pi i2I k B Ki ), P def= k0 B (K0, [i2I Ki) and

R def= \Pi k002K0 ! k00hvi | ! khvi. Then, recalling that k0 B (K0, k) = ! k0(v).R, wehave:

Q , ! k0(v).* k (R | \Pi i2I k B Ki ) Lemma 6.5 (1),

! k0(v).(\Pi k002K0 ! k00hvi | * k ( ! khvi | \Pi i2I k B Ki )) laws forrestrictions

in Table 2&
! k0(v).(\Pi k002K0 ! k00hvi | \Pi k002[i2IKi!k00hvi)) Lemma 6.5 (2),
P

where in the last step, in the case when K0 " ([i2I Ki) /= ; , Lemma 6.1 is needed. \Lambda 

7 Operational correspondences
This section is devoted to showing the close operational correspondence betweenthe actions of a causal process

A and those of the encoding standard process [[A]].In Subsect. 7.1 we study the operational correspondence on strong visible transitions

(Proposition 7.8), and on strong silent transitions (Proposition 7.9). These results areused in Subsect. 7.2 to establish the correspondences on weak transitions.

7.1 Operational correspondence on strong transitions
In the proof of the operational correspondence between A and [[A]] on strong tran-sitions, we use Lemma 7.1 and Corollary 7.6. To prove the latter result, we rely on
Lemmata 7.2 through 7.5. Lemma 7.1 and Lemma 7.2 are consequences of Lemma 6.4and Lemma 6.6, dealing with compositions of wires. To see the relationship between
Lemma 7.1 and Lemma 6.4, and between Lemma 7.2 and Lemma 6.6, remember thatthe parameter (

K0, k) is used in [[A]](K0, k) to define wires of the form k B K0 ,for some
K0. Lemmata 7.1 and 7.2 are also useful when reasoning about processesreturned by the encoding, in that they avoid us having to `unfold' the definition of

wire processes. The proofs of Lemmata 7.1 and 7.2 are reported in Appendix A.
Lemma 7.1 [[A]](K0, k) | k B K & [[A]](k, K0, K) | k B K \Lambda 
Lemma 7.2 If k /2 K0 [ ( [i2I Ki), then

374 M. Boreale, D. Sangiorgi

* k ([[A]](K0, k) | \Pi i2I k B Ki ) & [[A]](K0, [iKi) \Lambda 
The next three lemmata are used to prove Corollary 7.6. We have decomposed thecorollary in this way to help understanding its meaning. (The corollary can also be

proved directly, using Lemma 7.2 and transition induction; all cases of the inductionwould be fairly simple.) Lemma 7.3 is a variation of Lemma 7.2. The intuition of
Lemma 7.4 is the following: Any cause in [[A]]K0 depends on K0 (in the sense thatthere is a wire-connection from this cause to the causes in

K0); therefore, in the
process * k ([[A]]K0 | \Pi i2I k B (Ki, K0) ), the dependency of k on K0, expressed

in the wires k B (K0, Ki) , is superfluous. In Lemma 7.5, the transition A

u-!

K; k A0shows that k depends on K in A0: Therefore, the wire k B K can be safely removed.

The first two lemmata are easily proved by exhibiting appropriate expansion relations,in the same style as Lemma 7.2. Lemma 7.5 is proved by transition induction on

A

u-!

K; k A0.

Lemma 7.3 If k 62 K [ K0, then * k ([[A]]K0 | k B K ) & [[A[k  K]]]K0. \Lambda 

Lemma 7.4 If k 62 ([i2IKi) [ K0, then

* k ([[A]]K0 | \Pi i2I k B (Ki, K0) ) & * k ([[A]]K0 | \Pi i2I k B Ki ) . \Lambda 

Lemma 7.5 If A

u-!

K; k A0 with k /2 K (A), and k /2 K0 [ K0, then

* k ([[A0]]K0 | k B K | k B K0 ) & * k ([[A0]]K0 | k B K0 ) . \Lambda 

Corollary 7.6 If A

u-!

K; k A0 with k /2 K (A), and k /2 K0 [ K0, then

* k ([[A0]]K0 | k B (K0, K) | k B (K0, K0) ) & [[A0[k  K0]]]K0 .
Proof. Using Lemmata 7.4, 7.5 and 7.3, in the order. \Lambda 

Propositions 7.8 and 7.9 below are slightly stronger than we shall need. Theyrelate the transitions of a causal process

A with those of a standard process [[A]]K,for a generic parameter
K, while we shall only need the results for K = ; . Thesemore general statements are easier to handle in the inductive proofs (especially when

dealing with rule Cau). First, an elementary property of the encoding:
Lemma 7.7 For any A, K and oe, [[A]]Koe = [[Aoe]]K.
Proposition 7.8 (correspondence on strong visible transitions)

1. a) If A

ahebi-!

H; k A0 then [[A]]K

ahebki-! & k B (H, K) | [[A0]]K.

b) If k /2 K (A) [ K and A

* eb0 ahebi-!

H; k A0 then [[A]]K

(* eb0k)ahebki-! & k B

(H, K) | [[A0]]K.2. The converse of (1), on the actions from [[

A]]K:

a) If [[A]]K ahebki-! P then there exist A0, H s.t. A

ahebi-!

H; k A0 and P & k B(
H, K) | [[A0]]K.

Causality in the ss-calculus 375

b) If [[A]]K (* eb0k)ahebki-! P then there exist A0, H s.t. A

* eb0 ahebi-!

H; k A0 and P & k B(
H, K) | [[A0]]K.

Proof. The proof exploits Lemma 7.1. We only deal with case (1.a), since the others
are similar or easier. We proceed by induction on the derivation of A

ahebi-!

H; k A0.

Case 1: Inp rule.

(This is the base case.) Then we have A = a(ec). P

ahebi-!

; ; k k :: P {eb/ec}. Therefore,exploiting Lemma 7.7:

[[A]]K = a(eck). ( k B K | [[P ]]{k}) ahebki-! k B K | [[P {eb/ec}]]{k}&

k B K | [[P {eb/ec}]](k, K) = k B K | [[k :: P {eb/ec}]]K

where the use of & is due to Lemma 7.1.
Case 2: Par rule.Then

A = A1 | A2 and, supposing that the move originates from A1, the last ruleapplied is of the form

Par :

A1

ahebi-!

H; k A01

A1 | A2

ahebi-!

H; k A01 | A2

.

From the inductive hypothesis, we know that for all K there is P s.t.

[[A1]]K ahebki-! P & k B (H, K) | [[A01]]K .
From this, using rule S-par, we get

[[A1 | A2]]K ahebki-! P | [[A2]]K & k B (H, K) | [[A01 | A2]]K
which is the thesis.
Case 3: Cau rule.Then

A = K0 :: A0. The last rule applied in deriving the transition from A is ofthe form

Cau :

A0

ahebi-!

H0; k A00

A = K0 :: A0

ahebi-!

H; k K0 :: A00

where H = K0 [ H0. From the inductive hypothesis, for each K0 there exists some
P such that

376 M. Boreale, D. Sangiorgi

[[A0]]K0 ahebki-! P & k B (H0, K0) | [[A00]]K0 .
Recalling that [[A]]K = [[A0]](K0, K) and letting K0 = K0 [ K in the abovetransition, the thesis follows.

Case 4: Out, Sum, Res, Open or Repl.These cases easily follow from the inductive assumption. \Lambda 
Proposition 7.9 (correspondence on strong silent transitions)

1. If A o/-! A0, then [[A]]K o/-! & [[A0]]K.
2. If [[A]]K o/-! P , then there exists A0 such that A o/-! A0 and P & [[A0]]K.

Proof. We only deal with (1), since (2) is similar. The proof is an induction on the
derivation of A o/-! A0 and exploits the previous proposition (7.8), Lemma 6.2 andCorollary 7.6. The most difficult case is when

Com is the last rule applied:

Com :

A1

ahebi-!

H1; k A01, A2

(* ec)ahebi-!

H2; k A02

A = A1 | A2 o/-! * ec (A01[k  H2] | A02[k  H1]) def= A0 (4)

where ec ` eb and k /2 K (A). By applying Lemma 4.5 to rename k, if needed, wecan also suppose

k /2 K. From Proposition 7.8(1), applied to the premises of (4) weget

[[A1]]K ahebki-! & k B (H1, K) | [[A01]]K def= R1
[[A2]]K (* ck)ahebki-! & k B (H2, K) | [[A02]]K def= R2 .
From the above two transitions and rule S-com we infer

[[A]]K = [[A1]]K | [[A2]]K o/-! & (* eck)(R1 | R2), (

* eck)( k B (H1, K) |
k B (H2, K) | [[A01]]K | [[A02]]K)
def= P .

Now, k can occur free in [[A0i]]K (i 2 {1, 2}) only in output subject position; thus wecan apply Lemma 6.2 (1) to distribute the restricted replications

k B (H1, K) | k B(
H2, K) over the parallel composition [[A01]]K | [[A02]]K in P , obtaining:

P , * ec \Pi i=1,2(* k ( k B (H1, K) | k B (H2, K) | [[A0i]]K))

def= * ec (S1 | S2) .

Now, since Ai

u-!

Hi; k A0i and k /2 K [ Hi [ K (Ai), (i 2 {1, 2}), we can applyCorollary 7.6 to
Si, so to replace k with Hj inside [[A0i]] and eliminate the wires,thus:

Si & [[A0i[k  Hj]]]K
where if i = 1 then j = 2, and if i = 2 then j = 1. Since parallel composition andrestriction preserve &,

* ec (S1 | S2) & * ec ([[A01[k  H2]]]K | [[A02[k  H1]]]K) =

[[A0]]K olds. Summarising, we have obtained that [[A]]K o/-! & P & [[A0]]K. Thisconcludes the case. \Lambda 

Causality in the ss-calculus 377
7.2 Operational correspondences on weak transitions
In this subsection we study the operational correspondence between encoded andencoding processes on weak transitions.

Proposition 7.10 (correspondence on weak silent transitions)

1. If A =) A0, then [[A]] =) & [[A0]].2. If [[

A]] =) P , then there exists A0 such that A =) A0 and P & [[A0]].

Proof.

1. It must be A o/

n-! A0, for some n >= 0. We proceed by induction on n. For

n = 0 the statement holds trivially. Suppose n > 0; then, for some A00, we have
A o/

n-1-! A00 o/-! A0. From the inductive hypothesis, for some Q,

[[A]] =) Q & [[A00]] .
Now, from Proposition 7.9(1) and A00 o/-! A0 we have that, for some P ,

[[A00]] o/-! P & [[A0]] .
Since Q & [[A00]], from the above transition we get, for some Q0,

Q o/=) Q0 & P .
From this and P & [[A0]], we get Q0 & [[A0]]. To sum up, we have obtained:

[[A]] =) Q o/=) Q0 & [[A0]]
which proves the thesis.
2. For some n >= 0 it holds that [[A]] o/

n-! P . Again, we proceed by induction on n.

The case n = 0 is trivial. If n > 0, the for some Q, we have [[A]] o/

n-1-! Q o/-! P .

By the inductive hypothesis, we have, for some A00,

A =) A00 with Q & [[A00]] .
From this and Q o/-! P , we deduce that, for some R,

[[A00]] bo/-! R with P & R .
Now, by Proposition 7.9(2), there is A0 s.t.

A00 bo/-! A0 with R & [[A0]].
Thus, we have found A0 s.t. A =) A0 and P & [[A0]], and proved the thesis. \Lambda 

378 M. Boreale, D. Sangiorgi
Remark 7.11 In the proof of item (2) of the above proposition the use of the expansionrelation turns out to be necessary to close up the induction of the proof. Had we used
ss in place of & in the above proof, from Q ss [[A00]] and Q o/-! P , we could have
only inferred [[A00]] =) R (in place of the stronger [[A00]] bo/-! R); as a consequence,we could not have applied Proposition 7.9 to close up the induction. \Lambda 

Proposition 7.12 (correspondence on weak visible transitions)

1. a) If A

ahebi=)

K; k A0, then [[A]]

ahebki=) & k B K | [[A0]].

b) If k /2 K (A) and A

(* eb0)ahebi=)

K; k A0, then [[A]]

(* eb0k)ahebki=) & k B K | [[A0]].

2. The converse of (1), on the actions from [[A]]:

a) If [[A]] ahebki=) P , then there exist A0 and K s.t. A

ahebi=)

K; k A0 and P & k B
K | [[A0]].

b) If [[A]] (* eb

0k)ahebki=) P , then there exist A0 and K s.t. A * eb0 ahebi=)

K; k A0 and P &
k B K | [[A0]].

Proof. We only consider case (1.a); the remaining ones are similar. For some A00 and
A000 we have

A =) A00

ahebi-!

K; k A000 =) A0 .
From Proposition 7.10, we have, for some P 00,

[[A]] =) P 00 & [[A00]] .
Furthermore, from Proposition 7.8, for some Q,

[[A00]] ahebki-! Q & k B K | [[A000]] .
Therefore, since P 00 & [[A00]], there exists P 000 such that:

P 00 ahebki=) P 000 & Q .
Since A000 =) A0, by Proposition 7.10(1), [[A000]] =) & [[A0]]. Then, using rule
S-par, we get, for some Q0,

k B K | [[A000]] =) Q0 & k B K | [[A0]] (5)
We have proved that P 000 & Q & k B K | [[A000]]. From this and (5) we deduce thatthere is

P 0 s.t.

P 000 =) P 0 & Q0 & k B K | [[A0]] .

To sum up, we have

[[A]] =) P 00 ahebki=) P 000 =) P 0 & k B K | [[A0]]
which concludes the case. \Lambda 

Causality in the ss-calculus 379
Remark 7.13

1. Suppose [[A]] =) P ; then P cannot perform actions along a cause name k.This is a consequence of Proposition 7.10 (2) and of the fact that the processes

returned by the encoding cannot immediately perform actions along a cause name(the latter fact holds by the definition of the encoding).

2. Suppose [[A]] u=) P ; then P cannot perform output actions along a cause name

k. This is a consequence of Proposition 7.12(2) and of the item (1) of this remark.\Lambda 

8 Full abstraction
In the first part of the section we prove two results about wires processes, the Can-celation Lemma and the Saturation Lemma, that will be crucial in the second part to
prove the full abstraction theorem.

8.1 The Cancelation and the Saturation Lemmata
The main result of this section is the Cancelation Lemma. It is very useful whenverifying observation equivalence between processes returned by the encoding, to
cancel wires which appear at the outermost level. The Cancelation Lemma will beused in combination with the Saturation Lemma in the proof of soundness for the
encoding.To state the two lemmata, we need two auxiliary notions: Cause-Sets of a process
and Saturation. The cause-sets of a process A contains all sets of causes which can
appear in transitions from A; that is, if A

u-!

K; k A0, then K is in the cause-sets of A(this property is formalised in Lemma B.3 (1), in Appendix B).

Definition 8.1 (Cause-Sets of a process) For a causal process A, the cause-sets of
A, written CS (A), is the set of sets of causes inductively defined as follows:

CS (P ) def= {; }
CS (K :: A) def= {K [ H |H 2 CS (A)}

CS (* a A) def= CS (A)
CS (A1 | A2) def= CS (A1) [ CS (A2) .

Note that in general not all sets in CS (A) need to appear in causal transitionfrom

A; e.g., if A is * a (K :: a.0), then K 2 CS (A), but A cannot perform anytransition.

Now, the definition of saturation. Intuitively, a causal process A is saturated fora product of wires

W if each set of causes H which appear in A is "closed" w.r.t.the causality relation encoded by

W ; i.e., if a wire in W has its entrance-point in H,then all end-points of the wire are in

H.

Definition 8.2 (Saturation) Let A be a causal process and let W def= \Pi i2I ki B Ki .We say that

A is saturated for W if for each H 2 CS (A) and i 2 I, ki 2 H implies
Ki ` H.

380 M. Boreale, D. Sangiorgi

We can now state the Saturation and the Cancelation Lemmata; their proofs arereported in Appendix B.
Lemma 8.3 (Saturation Lemma) Let A be a causal process saturated for the product
of wires W def= \Pi i2I ki B Ki .

1. If A

u-!

K; k A0 with k fresh, then A0 is saturated for k B K | W .2. If
A o/-! A0, then A0 is saturated for W .

3. If A

u=)

K; k A0 with k fresh, then A0 is saturated for k B K | W . \Lambda 

Lemma 8.4 (Cancelation Lemma) Let A be saturated for k B K and B be satu-rated for

k B K0 . Then

k B K | [[A]] ss k B K0 | [[B]] implies K = K0 and [[A]] ss [[B]] . \Lambda 

8.2 Soundness and completeness of the encoding
Theorem 8.5 (Soundness) [[A]] ss [[B]] implies A ssc B.
Proof. We prove that the relation

R = {(A, B) : [[A]] ss [[B]]}
is a ssc-bisimulation. Let (A, B) 2 R . We only show how the moves of A arematched by

B, since the relation is symmetrical. We check clauses (1) and (2) of thedefinition of ss

c.
Clause (1)

A =) A0 implies, from Proposition 7.10(1)[[

A]] =) P , for some P with P ss [[A0]]. Since [[A]] ss [[B]][[
B]] =) Q , for some Q with P ss Q. From Proposition 7.10(2) we get
B =) B0 , for some B0 with Q ss [[B0]] .

To sum up, we have [[A0]] ss P ss Q ss [[B0]]. Hence (A0, B0) 2 R .
Clause (2) We only look at the input case; the output case is similar. We supposethat

k 62 K (A, B).

A

ahebi=)

K; k A0 implies, from Proposition 7.12(1),

[[A]] ahebki=) P , for some P with P ss k B K | [[A0]]. Since [[A]] ss [[B]],we get

[[B]] ahebki=) Q , for some Q with Q ss P . From Proposition 7.12(2), we get
B

ahebi=)

K0; k B0 , for some Q with Q ss k B K0 | [[B0]].

Causality in the ss-calculus 381
We have obtained that k B K | [[A0]] ss k B K0 | [[B0]]. From the SaturationLemma applied to above two weak causal transitions, it follows that

A0 is saturatedfor
k B K and that B0 is saturated for k B K0 . From the Cancelation Lemma, itfollows that

K = K0 and [[A0]] ss [[B0]]. Hence (A0, B0) 2 R , and we are done. \Lambda 

Theorem 8.6 (Completeness) A ssc B implies [[A]] ss [[B]].
Proof. We exploit the characterisation of causal bisimulation in terms of loose causalbisimulation (ss0

c) in Proposition 4.12 (we need ss0c for the treatment of clause (2)below). We prove that the relation

R = {([[A]], [[B]]) : A ss0c B }
is a weak bisimulation up to & and up to context. Let ([[A]], [[B]]) 2 R . We look atclause (1) and (2) of the definition of observation equivalence, on the moves by [[

A]].

Clause (1)

[[A]] =) P implies, from Proposition 7.10(2)
A =) A0 , for some A0 with P & [[A0]]. Since A ss0c B
B =) B0 , for some B0 with A0 ss0c B0. From Proposition 7.12(1)[[

B]] =) Q & [[B0]] , for some Q.

To sum up, we have [[A]] =) P & [[A0]], [[B]] =) Q & [[B0]], and A0 ss0c B0. Hence([[

A0]], [[B0]]) 2 R , and we are done.

Clause (2) We check only the input case (the output case is similar, only note that inthat case, one can suppose w.l.o.g.

k /2 K (A, B)):

[[A]] ahebki=) P implies, from Proposition 7.12(2),

A

ahebi=)

K; k A0 , for some A0 with P & k B K | [[A0]]. Since A ss0c B

B

ahebi=)

K; k B0 , for some B0 with A0 ss0c B0. From Proposition 7.12(1)

[[B]] ahebki=) Q for some Q with Q & k B K | [[B0]] .
Processes k B K | [[A0]] and k B K | [[B0]] allow us to close the bisimulation R ,up to & and up to the static context

k B K | [ . ]. \Lambda 

Theorems 8.5 and 8.6 prove the full abstraction of the encoding. That is, (usingalso Proposition 5.2) causal bisimulation on causal terms in P

c(N , K ) and whichrespect a sorting Ob can be reconducted to the ordinary observation equivalence

among standard terms in P (N +) and which respect a sorting [[Ob]], where N + isan appropriate extension of the class of names N .

In particular, this result holds for the standard processes in Pc(N , K ), i.e., thoseterms in P

c(N , K ) which do not contain the causal prefix K :: A. Formally, theseare the processes in the class P (N ). The applicability of the encoding to standard

processes was the result we were most interested in.
Corollary 8.7 (Full abstraction of the encoding on standard processes) For all Pand

Q 2 P (N ), we have [[P ]], [[Q]] 2 P (N +) and P ssc Q iff [[P ]] ss [[Q]]. \Lambda 

382 M. Boreale, D. Sangiorgi
Remark 8.8 An interesting issue is how much of the expressive power of the ss-calculus is needed to get the full abstraction theorem. Indeed, the definition of wire
processes does not mention summation and only uses a limited form of output prefix,with an empty continuation. Thus, the result as it stands is still meaningful, e.g., for
the asyncronous ss-calculus [HT91, Bou92]. Also, it is not difficult to prove that, byslightly complicating the definition of wire process, the output prefix can be replaced
by a bound output. This implies that the encoding still works when applied to ssI,a fragment of the

ss-calculus [San96] where only private names can be exchangedamong processes.

9 Examples of applications
In this section we give some examples of how the encoding and the full abstractiontheorems can be used to reason about causal bisimulation.

Some simple laws for causal bisimulation follow directly from the definition ofthe encoding; for example:

K :: (A | B) ssc (K :: A) | (K :: B) ,

K :: H :: A ssc K [ H :: A ,; ::

A ssc A .

As an easy corollary of the definition of our encoding and its full abstraction, we canderive the congruence of ss

c w.r.t. the causal prefix K :: - and all operators whichpreserve ss. For instance, the following sequence of implications, that holds for any

causal processes A, B and C, provides the argument for parallel composition:

A ssc B implies [[A]] ss [[B]]which implies [[

A | C]] = [[A]] | [[C]] ss [[B]] | [[C]] = [[B | C]] ,which implies
A | C ssc B | C .

Similarly, we can use the encoding and the property that ss is not preserved by thesum operator to check that also ss

c is not preserved by sum. We have

[[o/.a]] = o/.[[a]] ss [[a]]
and [[

o/.a + b]] = o/.[[a]] + [[b]] 6ss [[a]] + [[b]] = [[a + b]] .

Hence o/.a ssc a, but o/.a + b 6ssc a + b, which gives us a counterexample for thecongruence of ss

c w.r.t. sum. Thus, the only congruence property for ssc remainedis the one for the input prefix operator

a(eb). -. The known counter-examples showing that a(eb). - does not preserve ss involve either the match operator [a = b] -not considered here - or the expansion law. For example,

a | b ss a.b + b.a, but
c(a). (a | b) 6ss c(a). (a.b + b.a), since the process c(a). (a | b), after receiving b, can
perform a communication, whereas c(a). (a.b + b.a) cannot. The same kind of counter-example does not apply to ss

c, as the expansion law a | b ss a.b + b.a does not holdfor this equivalence. Indeed, as proved in [BS97], ss

c is a congruence, but the prooffor input prefix is non-trivial. A discussion on this topic is also contained in the

concluding section.More complex laws can be proved by taking advantage of the well-understood

algebraic theory of ss. Below, we show an example of this. Consider P def=

Causality in the ss-calculus 383
* b (a.b.c | b.d) and Q def= * b (a.b.d | b.c). Processes P and Q differ only because
c and d are placed at different locations. However, in both cases c and d have thesame cause, namely

a. Indeed, it holds that P ssc Q. We can prove this equality viaalgebraic manipulations, by showing that [[

P ]] ss [[Q]]. If

R def= h B ; | [[d]]h (6)
then we have

[[P ]] = * b (a(k). ( k B ; | [[b.c]]k) | * h bhhi. R) . (7)
From (7), applying the expansion law and simple laws for restriction (Table 2) weobtain

[[P ]] , a(k). ( k B ; | * b ([[b.c]]k | * h bhhi. R)) . (8)
Since, by definition, [[

b.c]]k = b(h). ( h B k | [[c]]h) (9)

applying expansion and restriction laws we obtain

* b ([[b.c]]k | * h bhhi. R) , (10)
o/.* h ( h B k | [[c]]h | R) =

o/.* h ( h B k | [[c]]h | h B ; | [[d]]h) def= T .
Rearranging subterms, we can write

T , o/.* h ( h B k | h B ; | [[c | d]]h) . (11)
Applying Lemma 7.2 to the right-hand side of (11):

T & o/.[[c | d]]k . (12)
From (10) and (12), and since & is preserved by parallel composition,

k B ; | * b ([[b.c]]k | * h bhhi. R) & k B ; | o/.[[c | d]]k . (13)
Relation (13) holds or any cause k. From this, due to the congruence property of &,and from (8), we get:

[[P ]] & a(k). ( k B ; | o/.[[c | d]]k) . (14)
In a symmetric way - exchanging the roles of c and d - one proves that

[[Q]] & a(k). ( k B ; | o/.[[d | c]]k) . (15)
But by the definition of encoding and by the commutativity law for parallel compo-sition operator, we have

[[c | d]]k , [[d | c]]k ,
for any k. Hence,

a(k). ( k B ; | o/.[[c | d]]k) , a(k). ( k B ; | o/.[[d | c]]k) . (16)
Finally, we deduce the original claim [[P ]] ss [[Q]] from (14), (15) and (16).

384 M. Boreale, D. Sangiorgi

The above sequence of manipulations consists of applications of familiar algebraiclaws (expansion and restriction laws), plus an application of the absorption Lemma
7.2, to manipulate wire processes. In particular, Lemma 7.2 avoided us having tounfold the definition of wire processes and hence having explicitly to deal with the
replication operator. A proof of P ssc Q is also possible utilising a proof systemfor ss

c on CCS, like Kiehn's [Kie93]. However, the laws for the interleaving ss aresimpler than those for the non-interleaving ss

c, and do not require the introduction ofextra operators. Kiehn's system, besides the causal prefix K :: -, uses the left merge

and the communication merge operators, to express an appropriate form of expansionlaw for causal bisimulation. Moreover, Kiehn's system is not purely equational - it
includes non-trivial inference rules.

10 Comparing the encodings of location and causal bisimulation
We have presented an encoding of the non-interleaving causal bisimulation into theinterleaving observation equivalence in the

ss-calculus. In a previous paper [San94b],the same issue has been tackled for location bisimulation. Causal bisimulation and

location bisimulation aim, respectively, to capture the causal and the spatial depen-dencies on processes. Location bisimulation can be described using a syntax and an
operational semantics similar to those for causal bisimulation in Definition 4.9 andTable 3. What distinguishes location bisimulation is that an action

u causes a lateraction
u0 only if u0 emanates from a position (called location) syntactically under-neath
u's position. In this framework, a communication between two subterms of aprocess has no causal effect whatsoever because the spatial disposition of the process

components is left unchanged.The processes

P1 def= a | c and P2 def= * b (a. b|b. c) + * b (c. b|b. a)
are location bisimilar: Actions a and c are unrelated both in P1 and P2, because theyemanate from unrelated locations. However,

P1 and P2 are not causally bisimilar:Only in
P2 the firing of either a or c is necessary to liberate the other action. On theother hand, and for opposite reasons, processes

Q1 def= * b (a.b.c|b.d) and Q2 def= * b (a.b.d|b.c)
are causally bisimilar (see Sect. 9) but not location bisimilar. The contrasts betweenthe two equivalences are only imputable to the treatment of communications between

processes. Indeed, Aceto's work [Ace92b] shows that they coincide in a calculus withno such communications. Affinities and differences are evidenced in their encodings
into observation equivalence presented here and in [San94b]. If we limit ourselves tostandard processes, then the two encodings act as a homomorphism on all operators
but input and output prefixes. There is a basic schema in the two encodings of theprefixes:

[[a(eb). P ]]k def= a(ebx). ( >x, h h B k | [[P ]]h) (17)
[[ahebi. P ]]k def= (* x)ahebxi. ( < x, h h B k | [[P ]]h) (18)

Causality in the ss-calculus 385
where h is a fresh name. Formally, >x, h and < x, h stand for unary operators
on the argument h B k | [[P ]]h. But it is more enlightening - and this explains theway in which they appear in (17) and (18) - to think of them as `missing parts', still

to be filled, of the expressions (17) and (18). Indeed, >x, h and < x, h play the
role of the access key to the wire h B k , which holds the causality information.In the case of location bisimulation, the access key is a restricted particle

xhhi inparallel with the wire, that is:

" >x, h " is "* h xhhi | " and (19)
" < x, h " is "* h xhhi | " .

If the prefix a(ebx) or ahebxi in (17-18) is consumed in a visible action, the externalobserver can use the key

xhhi to access name h and, with it, the wire h B k . By

contrast, if the two complementary prefixes a(ebx) and ahebxi communicate with eachother, the respective keys remain deadlocked, because of the same output polarity.

Consequently, the related wires remain disconnected. This reflects the fact that inlocation bisimulation a communication between processes has no effect on causes. A
few simplifications on this schema lead to the final encoding, reported in Table 10.(Note in particular the elimination of the innermost replication of the wire: This is
possible because, in the case of location bisimulation, only `linear' causal chains maybe generated, hence at most one wire may get connected to the end-point of another
wire.)In the encoding of location bisimulation interactions between a process and the
external observer and between processes are different - the key xhhi is to be usedonly by the external observer. The encoding of causal bisimulation is obtained by
eliminating this asymmetry. A key takes the same polarity as the related prefix. Writing"

x(h). " as abbreviation for the bound output "* h xhhi. " to emphasise the symmetry,then

" >x, h " is "x(h). " and (20)
" < x, h " is "x(h). " .

Thus, if the prefix a(ebx) or ahebxi in (17-18) produces a visible action, the key x(h) or
x(h) can be used by the external observer, in the same way as for location bisimulation,

to access the wire h B k . But now, if the prefixes a(ebx) and ahebxi communicate witheach other, the respective keys coalesce, because of opposite polarities. Consequently,

the related wires get connected, which reflects the fact that in causal bisimulation acommunication involves a merge of cause sets (the fact that a key is not blocking
also justifies the sequentialisation between a key and the associated wire in (20), inplace of the parallel composition as in (19)). In this schema of encoding, a key

x(h)or
xhhi only delays the access to the wire h B k one step. The encoding used in thispaper is obtained by removing this level of indirection. To facilitate the comparison

with the encoding location bisimulation, the clauses for input and output prefixes havebeen rewritten in Table 10.

386 M. Boreale, D. Sangiorgi

Table 10.
Encoding of Location Bisimulation; process h \Sigma  k is ! h. k.

[[a(eb). A]]k def= a(ebx). * h (xhhi | h \Sigma  k | [[A]]h)
[[ahebi. A]]k def= * h ahebxi. * h (xhhi | h \Sigma  k | [[A]]h)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Encoding of Causal Bisimulation.

[[a(eb). A]]k def= a(ebh). ( h B k | [[A]]h)
[[ahebi. A]]k def= * h ahebhi. ( h B k | [[A]]h)

11 Conclusions and future work
We have examined causality in the ss-calculus and provided a fully abstract encodingwhich reconducts the non-interleaving causal bisimulation to the interleaving observation equivalence. This permits re-using all the easier mathematical theory of thelatter to reason about the former. As a particular case, CCS causal bisimulation is
reconducted to the observation equivalence of the monadic ss-calculus.Wire processes play a crucial role in modeling the pointer mechanism of causal
bisimulation. The proofs of our main theorems rely heavily on the ability of manipu-lating wires, as provided by the lemmata of Sect. 6. The proofs of these lemmata, in
turn, rely on the "up-to techniques" of Sect. 3.We believe that the full abstraction of our encoding extends to the congruence
relations; i.e., if ssc and sscc are the congruences induced by observation equivalenceand causal bisimulation, respectively, then for all causal terms

A and B, it holds that

A sscc B iff [[A]] ssc [[B]] .
It should be possible to prove this exploiting the full abstraction on the bisimilarityrelations. The result could then be used to examine whether a proof system for ssc

ccan be obtained from one for ssc plus, possibly, Lemmata 6.4, 6.6, 7.1 and 7.2 to

manipulate wires.An interesting issue is the study of the congruence properties for the input-prefix

operator in causal-sensitive behavioural equivalences. Intuitively, a relation over ss-calculus processes is preserved by input prefix if it is preserved by name substitutions.
This property usually fails if the matching operator is present, as for the calculusin [MPW92]. A matching [

a = b]P represents an if-then construct with guard"
a = b". Thus, if a and b are different names, one might want to regard [a = b]a.0 and0 as equivalent since both are deadlocked, but ([

a = b]a.0){a/b} and 0{a/b} as distinctsince only the latter is deadlocked. Indeed, with the matching operator observation

equivalence, location bisimulation and causal bisimulation are all not preserved bysubstitutions. However, in many situations matching can be avoided or simulated.
Hence one might wish to consider a matching-free calculus, as we did in the presentpaper. On such a language, the congruence for substitutions still fails for observation
equivalence and location bisimulation [San94b]. The failure is essentially due to thefact that both equivalences may relate processes with different degrees of parallelism
(consider, for instance, the processes P1 and P2 in Sect. 10); as a consequence, whena substitution applied to such two processes identifies previously distinct channels,

Causality in the ss-calculus 387
new communication possibilities may be created in one process, but not in the other.However, it can be expected that the situation for ss

c is better, since ssc does respectparallelism. Indeed, in [BS97] it is proved that in the matching-free calculus ss

c isindeed preserved by substitutions and hence by input prefix. This has interesting

consequences: proving causal bisimulation equalities is easier; the proliferation ofdifferent forms of bisimulation, like the early, late and open of interleaving and
location bisimilarities [PS93, San94a, San94b], is avoided in this causality framework.We have individuated two forms of causal dependency between

ss-calculus actions,which we called subject and object dependencies. The former originate from prefixnesting and are propagated through interactions within processes, the latter originatefrom the name-binding mechanisms. Our formulation of causal bisimulation distinguishes between processes which differ for the subject or for the object dependencies.
Thus we discriminate between P def= * b (ahbi. 0|bhyi. 0) and Q def= * b (ahbi. bhyi. 0),because in

P there is both a subject and an object dependency between the actionsat
a and at b, whereas in Q there is only an object dependency. Our notion of causalbisimulation is strictly finer than a notion in which subject and object dependencies

are not separated, where P and Q would be equated (see [Pri96] for comparisonsamong various forms of causality in the

ss-calculus). In this paper we have presenteda fully abstract encoding of the former notion; it would be interesting to see whether

there is also a fully abstract effective encoding for the latter.

Acknowledgements. We are most grateful to Rocco De Nicola for comments and discussions on the topicof the paper. Michele Boreale has been partially supported by the HCM Network "EXPRESS". Davide
Sangiorgi's research has been supported by the ESPRIT BRA project 6454 "CONFER".

References
[Ace92a] L. Aceto: History preserving, Causal and Mixed-Ordering Equivalence over Stable EventStructure (Note). in Fundamentae Informaticae 17(4):319-331, 1992
[Ace92b] L. Aceto: Relating Distributed, Temporal and Causal Observations of Simple Processes. inFundamentae Informaticae 17(4):369-397, 1992
[AKH92] S. Arun-Kumar, M. Hennessy: An efficiency preorder for processes. Acta Informatica 29:737-760, 1992
[Bou92] G. Boudol: Asynchrony and the ss-calculus. Technical Report 1702, INRIA Sophia Antipolis,1992
[BCHK91] G. Boudol, I. Castellani, M. Hennessy, A. Kiehn: A theory of processes with localities. TR13/91, University of Sussex, 1991. To appear in Formal Aspects of Computing
[BD92] M. Boreale, R. De Nicola: Testing equivalence for mobile processes. Information and Com-putation 120: 279-303, 1995
[BS97] M. Boreale, D. Sangiorgi: Some congruence properties for ss-calculus bisimilarities. TechnicalReport RR-2870, INRIA-Sophia Antipolis, 1996
[CD93] F. Corradini, R. De Nicola: Locality and causality in distributed process algebras. ReportSI/R/R-93/05, Dipartimento di Scienze dell'Informazione, Universit`a degli studi di Roma "La

Sapienza", 1993[DDM88] P. Degano, R. De Nicola, U. Montanari: Partial Ordering Descriptions and Observations of
Concurrent Processes. in Linear Time, Branching Time and Partial Order in Logics and Modelsof Concurrency. Lecture Notes in Computer Science, vol. 354, pp. 438-466. Berlin, Heidelberg,
New York: Springer 1988[DDM93] P. Degano , R. De Nicola, U. Montanari: Observation Trees. in Proc. North American
Conference on Process Algebras (NAPAW) '92. Berlin, Heidelberg, New York: Springer 1993[DD89] P. Degano, P. Darondeau: Causal trees. In 15th ICALP. Lecture Notes in Computer Science,
vol. 374, pp. 234-248. Berlin, Heidelberg, New York: Springer 1989

388 M. Boreale, D. Sangiorgi
[DP92] P. Degano, C. Priami: Proved trees. In: W. Kuich (ed.) Proc. ICALP 92. Lecture Notes inComputer Science, vol. 623. Berlin, Heidelberg, New York: Springer 1992
[HT91] K. Honda, M. Tokoro: On asynchronous communication semantics. In M. Tokoro, O. Nier-strasz, P. Wegner, A. Yonezawa (eds.) ECOOP '91 Workshop on Object Based Concurrent

Programming, Geneva, Switzerland, 1991 . Lecture Notes in Computer Science, vol. 612,pp. 21-51. Berlin, Heidelberg, New York: Springer 1992
[Kie91] A. Kiehn: Comparing locality and causality based equivalences. Acta Informatica 31:697-718(1994). Revision of Local and global causes, report TUM-I9132, 1991
[Kie93] A. Kiehn: Proof systems for cause based equivalences. In: Proc. MFCS 93. Lecture Notes inComputer Science, vol. 711. Berlin, Heidelberg, New York: Springer 1993
[Mil89] R. Milner: Communication and Concurrency. Prentice Hall, 1989[Mil91] R. Milner: The polyadic

ss-calculus: a tutorial. Technical Report ECS-LFCS-91-180, LFCS,Dept. of Comp. Sci., Edinburgh Univ., October 1991

[MPW92] R. Milner, J. Parrow, D. Walker: A calculus of mobile processes, (Parts I and II). Informationand Computation 100:1-77, 1992
[MY92] M. Montanari, D. Yankelevich: A parametric approach to localities. In: W. Kuich (ed.) Proc.ICALP 92. Lecture Notes in Computer Science, vol. 623, pp. 617-628. Berlin, Heidelberg,

New York: Springer 1992[Par81] D.M. Park: Concurrency on automata and infinite sequences. In: P. Deussen (ed.) Conf. on
Theoretical Computer Science. Lecture Notes in Computer Science, vol. 104. Berlin, Heidel-berg, New York: Springer 1981
[PS93] J. Parrow, D. Sangiorgi: Algebraic theories for name-passing calculi. Information and Com-putation 120(2):174-197, 1 August 1995
[Pri96] C. Priami: Enhanced Operational Semantics for Concurrency. PhD thesis TD-8/96, Departmentof Computer Science, University of Pisa, 1996
[San92] D. Sangiorgi: Expressing Mobility in Process Algebras: First-Order and Higher-OrderParadigms. PhD thesis CST-99-93, Department of Computer Science, University of Edinburgh, 1992[San93] D. Sangiorgi: A theory of bisimulation for the

ss-calculus. Acta Informatica 33:69-97 (1996)[San94a] D. Sangiorgi: On the bisimulation proof method. To appear in Journal of Mathematical

Structures in Computer Science. (A summary appeared in Proc. MFCS'95, Lecture Notes inComputer Science, vol. 969. Berlin, Heidelberg, New York: Springer 1995)
[San94b] D. Sangiorgi: Locality and non-interleaving semantics in calculi for mobile processes. Theo-retical Computer Science 155:39-83, 1996
[San96] D. Sangiorgi: ss-calculus, internal mobility and agent-passing calculi Theoretical ComputerScience 167(2):235-274, 1996
[SM92] D. Sangiorgi, R. Milner: The problem of "Weak Bisimulation up to". In: W.R. Cleveland(ed.) Proceedings of CONCUR '92. Lecture Notes in Computer Science, vol. 630, pp. 32-46.

Berlin, Heidelberg, New York: Springer 1992[Wal95] D. Walker: Objects in the

ss-calculus. Information and Computation 116(2):253-271, 1995

A Proofs of Lemmata 7.1 and 7.2.
To facilitate the proof of a few lemmata below, and to better capture the strengthof their assertions we use the structural congruence relation. Roughly, two processes
are structurally congruent if they may only differ syntactically with each other on theway in which their respective subcomponents are assembled. Structural congruence
[Mil91] is written j and is defined as the smallest congruence over processes whichsatisfies the rules in Table 5. These are self-evident rules which must be valid in any
reasonable behavioural equivalence.
Lemma A.1 The relation j is a strong bisimulation and is preserved by name sub-stitution. \Lambda 

Causality in the ss-calculus 389

Table 5. The rules of structural congruence (j)
Abelian monoid laws for + : P1 + P2 j P2 + P1

P1 + (P2 + P3) j (P1 + P2) + P3

P + 0 j PAbelian monoid laws for | :
P1 | P2 j P2 | P1
P1 | (P2 | P3) j (P1 | P2) | P3

P | 0 j PRestriction:

* a 0 j 0
* a * b P j * b * a Pif
a 62 fn(P2), then (* a P1) | P2 j * a (P1 | P2)Replication: !

P j P | ! P

A.1 Proofs of Lemma 7.1 and of Lemma 7.2
The following lemma allows us to rename names in the object part of input transitions.

Lemma A.2 (Renaming Lemma) Suppose P ahebi-! P 0. Then there are ex and P 00 s.t.

P 0 = P 00{eb/ex} and for all ec, P aheci-! P 00{ec/ex}.
Proof. Simple transition induction. \Lambda 

Corollary A.3 1. If P ahebhi-! P 0, and k " fn(P ) = ; , then also P ahebki-! P 00, for some
P 00 with P 00{h/k} = P 0.

2. If P ahebki-! P 00 and k " fn(P ) = ; , then, for all h, also P ahebhi-! P 0 = P 00{h/k}. \Lambda 
The proofs of Lemmata 7.1 and 7.2 use Lemmata 6.4 and 6.6. We also rely onLemma A.5 below. This lemma, basically, says that the actions a process [[

A]]Kcan perform do not depend from the parameter
K. That is, two processes [[A]]Kand [[
A]]K0 can perform the same actions and, in doing so, they evolve to derivativeswhich are the same up to the choice of the parameter

K or K0. To express this fact, we

have to accurately study how the parameter K is used in a transition [[A]]K u-! P .In the proof of Lemma A.5 we use the following fact:

Lemma A.4 If ! Q u-! Q0 with u /= o/ , then there exists Q00 s.t. Q0 j Q00 | ! Q and
Q u-! Q00. \Lambda 

Lemma A.5 Let A be a causal term, K, K0 `fin K and suppose [[A]]K u-! P .Then
1. If u = ahebhi or u = (* ech)ahebhi, for h /2 K (A) [ K [ K0, then there exist
K0 ` K (A), ed, A0 and R with h /2 K (A0), s.t.

P j h B (K, K0) | (* ed)([[A0]]K | [[h :: R]])
and moreover

[[A]]K0 u-! j h B (K0, K0) | (* ed)([[A0]]K0 | [[h :: R]]) .

2. If u = o/ , then either

390 M. Boreale, D. Sangiorgi

- there exists A0 s.t. P j [[A0]]K and also [[A]]K0 o/-! Q j [[A0]]K0, or-

there are Ki `fin K (A) i 2 {1, 2}, ed, A0, R, and some h /2 (K (A0) [ K [ K0)s.t.

P j (* h)( h B (K1, K) | h B (K2, K) | (* ed)([[A0]]K | [[h :: R]]))
and also

[[A]]K0 o/-! j (* h)( h B (K1, K0) | h B (K2, K0) | (* ed)([[A0]]K0 | [[h :: R]])).
Proof. The proof of (1) is by induction on the structure of A. We consider the mostsignificant cases in detail, namely the cases in which the outermost operator of

A isa causal prefix or a replication.

Cause prefix A = H :: B.
Thus [[A]]K = [[B]](K, H) and [[A]]K0 = [[B]](K0, H). We have [[B]](K, H) u-! P .
Hence, by the inductive assumption, there are K00 ` K (B), ed, A00 and R s.t.

P j h B (K, H, K00) | (* ed)([[A00]](K, H) | [[h :: R]])

= h B (K, H, K00) | (* ed)([[H :: A00]]K | [[h :: R]])

and

[[B]](K0, H) u-! j h B (K0, H, K00) | (* ed)([[A00]](K0, H) | [[h :: R]])

= h B (K0, H, K00) | (* ed)([[H :: A00]]K0 | [[h :: R]]) .

For K0 def= H [ K00 and A0 def= H :: A00, this concludes the case.
Replication A = ! Q.
Thus [[A]]K = ! [[Q]]K u-! P . By Lemma A.4, we have P j P1 | ! [[Q]]K, for some
P1 s.t. [[Q]]K u-! P1. Using the inductive assumption on [[Q]]K u-! P1 we get
that for some processes R and S and name tuple ed,

P1 j h B K | (* ed)([[S]]K | [[h :: R]])
and also [[Q]]K0 u-! P2 with

P2 j h B K0 | (* ed)([[S]]K0 | [[h :: R]]) .
Now, from [[Q]]K0 u-! P2 and rule S-par, we infer

[[Q]]K0 | ! [[Q]]K0 u-! P2 | ! [[Q]]K0 def= P 0
and hence, applying S-rep:

! [[Q]]K0 = [[ ! Q]]K0 u-! P 0 .
To sum up, extruding the scope of (* d), we have:

P j P1 | ! [[Q]]K j h B K | (* ed)([[S]]K | [[h :: R]]) | [[ ! Q]]K j

h B K | (* ed)([[S | ! Q]]K | [[h :: R]])

Causality in the ss-calculus 391
and similarly

P 0 j P2 | [[ ! Q]]K0 j h B K0 | (* ed)([[S | ! Q]]K0 | [[h :: R]]) .

For K0 def= ; and A0 def= S | ! Q, this proves our claim.

Now, the proof of assertion (2) of the lemma. The proof is again an induction onthe structure of

A. The non-trivial cases are when the outermost operator in A is aparallel composition or a replication. We only consider the former case. Thus suppose

A = A1 | A2; the last rule applied in deriving [[A]]K o/-! P must be either a S-paror a

S-com; we analyse the latter case; the former is easier. We have:

S-com : [[

A1]]K ahebhi-! P1 [[A2]]K (* ech)ahebhi-! P2

[[A]]K = [[A1]]K | [[A2]]K o/-! (* ech)(P1 | P2) def= P

.

For simplicity we suppose that h /2 K [ K0 [ K (A); the case h 2 K [ K0 [ K (A)can be accommodated using Corollary A.3(1) to first rename

h to a fresh name.

From part (1) of this lemma, we know that there are A0i, Ki ` K (Ai), edi and Riwith

h /2 A0i, for i = 1, 2, such that

Pi j h B (Ki, K) | (* edi)([[A0i]]K | [[h :: Ri]]) .
and also

[[A1]]K0 ahebhi-! Q1 and [[A2]]K0 (* ech)ahebhi-! Q2 (21)
with

Qi j h B (Ki, K0) | (* edi)([[A0i]]K0 | [[h :: Ri]]), for i 2 {1, 2}.

From the two transitions in (21) and rule S-com, we infer

[[A]]K0 = [[A1]]K0 | [[A2]]K0 o/-! (* ech)(Q1 | Q2) def= Q .
Finally, defining A0 def= A01 | A02, ed def= ec ed1 ed2 and R def= R1 | R2, and using the equality[[

h :: R1]] | [[h :: R2]] = [[h :: (R1 | R2)]], we can write:

P j (* h)( h B (K1, K) | h B (K2, K) | (* ed)([[A0]]K | [[h :: R]]))
and, similarly,

Q j (* h)( h B (K1, K0) | h B (K2, K0) | (* ed)([[A0]]K0 | [[h :: R]])) .
This concludes the case. \Lambda 

We are now ready to prove Lemma 7.1. We first recall its assertion:

h B K | [[A]](h, K0, K) . h B K | [[A]](h, K0) .
Proof of Lemma 7.1. Let

H def= {h} [ K0 [ K and H0 def= {h} [ K0 . (22)

392 M. Boreale, D. Sangiorgi
We show that the relation

R = {( h B K | [[A]]H, h B K | [[A]]H0) : A 2 Pc(N , K )}

is an expansion up to context and up to &. Let (Q1, Q2) 2 R , for Q1 def= h B
K | [[A]]H and Q2 def= h B K | [[A]]H0. We show that the moves of Q1 can bematched by

Q2. The converse, on the actions from Q2, is proved by reversing thesteps. If
Q1 moves, then the move originates either from h B K , or from [[A]]H(an interaction among these two processes is impossible, see Remark 7.13). We only

show the details of the second case, since the first case is easier. Thus, we suppose
that Q1 u-! h B K | P1 def= Q01, for some P1 s.t. [[A]]H u-! P1. We have to finda static context

C[*], and processes Q02, T1, and T2 s.t.

Q01 , C[T1], Q2 u=) Q02 & C[T2] and (T1, T2) 2 R . (23)
We distinguish the cases in which u is an input, an output, or a silent action.
Case 1: u is an input, say u = ahebh0i.

Let h0 be a fresh name. From [[A]]H ahebh

0i-! P

1 and Corollary A.3(1), we have

[[A]]H ahebh0i-! P 01 with P 01{h0/h0} j P1 .
From Lemma A.5(1), we know that there are H0 ` K (A), ed and A0 with h0 /2K (

A0) and R, s.t.

P 01 j h0 B (H0, H) | (* ed)([[A0]]H | [[h0 :: R]])
and moreover

[[A]]H0 ahebh0i-! P 02 j h0 B (H0, H0) | (* ed)([[A0]]H0 | [[h0 :: R]]) .
From the above transition and Corollary A.3(2), we obtain

[[A]]H0 ahebh

0i-! P

2 with P2 j P 02{h0/h0}.

Now, from this, using rule S-par, we get a transition for Q2:

Q2 = h B K | [[A]]H0 ahebh

0i-! h B K | P

2

def= Q0

2 .

Thus, we have found the process Q02 to use in (23); we have now to find C[*], T1and

T2, and prove the requirements in (23). From the definitions of Q01, P1 and P 01,we get

Q01 j h B K | P 01{h0/h0} j h B K | h0 B (H0, H) | (* ed)([[A0]]H | [[h0 :: R]]) (24)
and, similarly, from the definitions of Q02, P2 and P 02, we get

Q02 j h B K |P 02{h0/h0} j h B K | h0 B (H0, H0) |(* ed)([[A0]]H0|[[h0 :: R]]) . (25)

Causality in the ss-calculus 393
From (24) and (25), extruding the scope of (* ed), and commuting the order of somecomponents, we get

Q01 j (* ed)([[h0 :: R]] | h0 B (H0, H) | h B K | [[A0]]H) (26)
Q02 j (* ed)([[h0 :: R]] | h0 B (H0, H0) | h B K | [[A0]]H0) . (27)

Recall by (22) that H = H0 [ K. Therefore, by Lemma 6.4, we deduce that

h0 B (H0, H0) | h B K & h0 B (H0, H) | h B K .
From this and (27) we get

Q02 & (* ed)([[h0 :: R]] | h0 B (H0, H) | h B K | [[A0]]H0). (28)

Finally, define C[*] def= (* ed)([[h0 :: R]] | h0 B (H0, H) | [ . ]), T1 def= h B K | [[A0]]H
and T2 def= h B K | [[A0]]H0. The results in (26) and (28) show that Q01 j C[T1]and that

Q02 & C[T2]; this suffices to prove (23), since j implies ,.

Case 2: u is an output.This case is similar to the previous one.

Case 3: u = o/.
If [[A]]H o/-! P1, then, by Lemma A.5(2), one of the following holds:

(a) there is A0 s.t. P1 j [[A0]]H and also [[A]]H0 o/-! P2 j [[A0]]H0, or
(b) there are ed, A0, K0i ` K (A) (i 2 {1, 2}), R and h0 fresh s.t.

P1 j (* h0)( h0 B (K01, H) | h0 B (K02, H) | (* ed)([[A0]]H | [[h0 :: R]]))
and also

[[A]]H0 o/-! P2 j (* h0)( h0 B (K01, H0) |

h0 B (K02, H0) | (* ed)([[A0]]H0 | [[h0 :: R]])) .

In either cases, using rule S-par, we get

Q2 = h B K | [[A]]H0 o/-! h B K | P2 def= Q02 .
If (a) holds, then (23) is immediately validated, since the pair (Q01, Q02) belongs toR , up to j. Let us consider case (b). By definitions of

Q01, Q02, P1 and P2, we have:

Q01 = h B K | P1

= h B K | (* h0)( h0 B (K01, H) | h0 B (K02, H) | (* ed)([[A0]]H | [[h0 :: R]]))and

Q02 = h B K | P2

= h B K | (* h0)( h0 B (K01, H0) | h0 B (K02, H0) | (* ed)([[A0]]H0 | [[h0 :: R]]))

which, extruding the scope of * ed and (* h0), and commuting the order of somecomponents, gives:

394 M. Boreale, D. Sangiorgi

Q01 j (29)

(* h0 ed)([[h0 :: R]] | h0 B (K01, H) | h0 B (K02, H) | h B K | [[A0]]H)
Q02 j (30)

(* h0 ed)([[h0 :: R]] | h0 B (K01, H0) | h0 B (K02, H0) | h B K | [[A0]]H0) .

Since H = H0 [ K, by Lemma 6.4 (applied twice), we derive

h0 B (K01, H0) | h0 B (K02, H0) | h B K & h0 B (K01, H) |

h0 B (K02, H) | h B K .

From this and (30) we get

Q02 & (* h0 ed)([[h0 :: R]]| h0 B (K01, H) | h0 B (K02, H) | h B K |[[A0]]H0) . (31)
Finally, define

C[*] def= (* h0 ed)([[h0 :: R]] | h0 B (K01, H) | h0 B (K02, H) | [ . ])

T1 def= h B K | [[A0]]H
T2 def= h B K | [[A0]]H0 .

The results in (29) and (31) show that Q01 j C[T1] and that Q02 & C[T2], thus proving(23). \Lambda 

Now Lemma 7.2, whose assertion is:For any

K0, Ki, i 2 I, h /2 K0 [ ([i2I Ki), A:

B def= (* h)(\Pi i2I h B Ki | [[A]](K0, h)) & [[A]](K0, [iKi) def= A .
Proof of Lemma 7.2. The proof of this lemma only differ from that of Lemma 7.1because it uses Lemma 6.6 in place of of Lemma 6.4. \Lambda 

B Proofs of the Saturation and Cancelation lemmata
To ease the reading of this appendix, we recall the definitions of Cause-Sets andSaturation.

Definition B.1 (Process cause-sets, Definition 8.1) If A is a causal process, thenCS (

A) is the set of sets of causes inductively defined as follows:

CS (P ) def= {; }
CS (K :: A) def= {K [ H |H 2 CS (A)}

CS (* a A) def= CS (A)
CS (A1 | A2) def= CS (A1) [ CS (A2) .

Definition B.2 (Saturation, Definition 8.2) A causal process A is saturated for a
product of wires W def= \Pi i2I ki B Ki if for each H 2 CS (A) and i 2 I, ki 2 Himplies

Ki ` H.

Causality in the ss-calculus 395

To prove the Saturation Lemma, we use Lemma B.3 below on cause-sets ofprocesses. Item (1) says that all sets of causes that appear in causal transitions of a
process are also in the cause-sets of that process; item (2) and (3) relate the cause-setsof a process to those of its derivatives; finally, item (4) represents a generalization of
item (1) to weak transitions.
Lemma B.3 Let A be a causal term.
1. If for some u, k, A0, A

u-!

K; k A0 then K 2 CS (A).

2. If A

u-!

K; k A0 with k 62 K (A), and H 2 CS (A0), then:

(a) if k 62 H, then H 2 CS (A);(b) if

k 2 H, then H = K [ {k}.

3. Suppose A o/-! A0 and H 2 CS (A0). Then either H 2 CS (A) or H = H1 [ H2,with

H1, H2 2 CS (A).

4. If for some u, k, A0, A

u=)

K; k A0 then K = H1 [ * * * [ Hn, for some n > 0 andH
1, . . . , Hn 2 CS (A).

Proof.
1. A simple transition induction on A

u-!

K; k A0.

2. By transition induction on A

u-!

K; k A0; we only consider the non-trivial cases,namely Par and Cau.

Case a: Par rule.The last rule applied is of the form

Par :

A1

u-!

K; k A01

A = A1 | A2

u-!

K; k A01 | A2

def= A0 .

If H 2 CS (A0) = CS (A01) [ CS (A02) then, by definition, either H 2 CS (A01) or
H 2 CS (A2). If H 2 CS (A2) then k 62 H and, since A2 is a component of A,also

H 2 CS (A). Suppose H 2 CS (A01). If k 62 H then, by inductive hypothesis,
H 2 A1. Hence H 2 CS (A) since A1 is a component of A. Suppose now k 2 H:By inductive hypothesis,

H = K [ {k}, that proves the claim.

Case b: Cau rule.The last rule applied is of the form

Cau :

A0

u-!

K0; k A00

A = K0 :: A0

u-!

K; k K0 :: A00

def= A0

where K = K0 [ K0. If H 2 CS (A0) then, by definition, H = K0 [ H0, for some
H0 2 CS (A00). If k /2 H, then, by inductive hypothesis, H0 2 CS (A0) and hencealso

H 2 CS (A). If k 2 H, then it must be k 2 H0. By inductive hypothesis,
H0 = K0 [ {k}. To sum up, we have: H = K0 [ H0 = K0 [ K0 [ {k} = K [ {k},

396 M. Boreale, D. Sangiorgi
which is our claim.3. The proof goes by transition induction. All cases are trivial, except the case when
Com is the last rule applied, where the last step of the derivation is of the form

Com :

A1

u1-!

K1; k A01, A2

u2-!
K2; k A02

A = A1 | A2 o/-! (* ec)(A01[k  K2] | A02[k  K1]) def= A0
with k /2 K (A1, A2). Let H 2 CS (A01[k  K2]) (the case when H 2 CS (A02[k 
K1]) is perfectly symmetrical). It must be H = H0[k  K2], for some H0 2 CS (A01).Now, there are two cases, namely

k /2 H0 and k 2 H0. If k /2 H0, then, from item(2) of this lemma,
H0 2 CS (A1) and furthermore H = H0, which proves our claim.If
k 2 H0, then, from item (2) of this lemma, H0 = K1 [ {k}; since k /2 K1, thisimplies

H = K1 [ K2; but, from item (1) of this lemma, K1, K2 2 CS (A); thisproves the claim.

4. Using item (3) of this lemma, one can prove, by induction on m, that

if A o/

m-! A0, m >= 0, and H 2 CS (A0), then (32)

H = H1 [ * * * [ Hn for some H1, . . . , Hn 2 CS (A).
Now, suppose A

u=)

K; k A0; this means that A

o/m-! A00 u-!

K; k A000 =) A0, for some
m, A00 and A000. Now, the thesis follows using item (1) of this lemma and (32). \Lambda 

We now come to the proof of the Saturation Lemma; we first recall its assertion:Let

A be a causal process saturated for the product of wires \Pi i2I ki B Ki . Then:

1. If A

u-!

K; k A0 with k fresh, then A0 is saturated for k B K | \Pi i2I ki B Ki .2. If
A o/-! A0, then A0 is saturated for \Pi i2I ki B Ki .

3. If A

u=)

K; k A0 with k fresh, then A0 is saturated for k B K | \Pi i2I ki B Ki .

Proof of the Saturation Lemma.1. Let

H 2 CS (A0) and h 2 H " ({ki : i 2 I} [ {k}). We have to show that
K ` H in case h = k, and that Ki ` H in case h = ki. This fact follows fromLemma B.3(1) and (2) and from the fact that

A is saturated for \Pi i2I ki B Ki .2. Follows from Lemma B.3(3) and from the definition of saturation.

3. It holds that A =) A00

u-!

K; k A000 =) A0, for some A00 and A000; the thesis isobtained by (repeatedly) using item (2) of this lemma on the transition

A =) A00,

then item (1) on the transition A00

u-!

K; k A000, then item (2) again (repeatedly) on thetransition
A000 =) A0. \Lambda 

We are left with the proof of the Cancelation lemma. For this, we use the followingauxiliary technical result:

Lemma B.4 Let A, B be saturated for W def= \Pi i2I ki B Ki . Suppose that for some
u

A

u=)

H0; k0 A0 and B

u=)
H00; k0 B0
with k0 fresh and, furthermore, suppose that

Causality in the ss-calculus 397

W | k0 B H0 | [[A0]] ss W | k0 B H00 | [[B0]] .
Then H0 = H00.
Proof. We prove that H0 ` H00 (symmetrically, it will be H00 ` H0). Given k 2 H0,we show that

k 2 H00 as well. Let

R def= W | k0 B H0 | [[A0]]
and

S def= W | k0 B H00 | [[B0]].

Since k 2 H0, for all v there is R0 s.t.

R k0hvi-! khvi-! R0 .
Since R ss S, there is S0 s.t.

S k0hvi=) khvi=) S0 .
We now analyse the above sequence of transitions. For some S1,S2,S3,S4 and S5 wehave

S =) S1 k0hvi-! S2 =) S3 khvi-! S4 =) S5 = S0 .
We now argue on the form of S1, S2 and S3. From Remark 7.13, it follows thatprocess

W | k0 B H00 does not contribute to the weak transition S =) S1; in otherwords, there exists

P s.t. [[B0]] =) P and

S1 = W | k0 B H00 | P .
From Remark 7.13 and from the fact that k0 /2 {ki : i 2 I}, we get that the
transition S1 k0hvi-! S2 originates from k0 B H00 ; thus, letting Z def= W | k0 B
H00 | \Pi k002H00 ! k00hvi, it holds that

S2 = Z | P .
Again by Remark 7.13 we know that processes Z and P , as well as any o/-derivativeof them, cannot interact with one another. Hence the transition

S2 =) S3 can bedecomposed into two independent sequences of transitions from
Z and P ; formally,there exist
V and P 0 s.t. Z =) V , P =) P 0 and

S3 = V | P 0 .

Since [[B0]] =) P 0, from Remark 7.13 it follows that the transition S3 khvi-! S4
originates from V (i.e. V khvi-! V 0, for some V 0). This and the fact that V is a
o/ -derivative of Z imply that Z khvi=) V 0, and therefore, by definition of Z we getthat: Either

k 2 H00, or there is i 2 I s.t. ki 2 H00 and k 2 Ki (so that the transition
khvi-! can occur after an interaction of ! k

ihvi with ki B Ki ). Now, if k 2 H00 wehave finished our proof. Assume therefore that, for some i, k

i 2 H00 and k 2 Ki; we

show that also k 2 H00 holds. From Lemma B.3(4) and from B

u=)

H00; k0 B0, it followsthat
H00 = H1 [ * * * [ Hn, for some n, with the Hj 2 CS (B), 1 <= j <= n; hence for

398 M. Boreale, D. Sangiorgi
some j 2 {1, . . . , n}, we have ki 2 Hj; but, since by hypothesis B is saturated for
W , it holds that Ki ` Hj; since k 2 Ki and Hj ` H00, we conclude that k 2 H00,which proves our claim. \Lambda 

We are now ready to prove the Cancelation Lemma. We first recall its assertion:Let

A be saturated for k B K and B be saturated for k B K0 . Then

k B K | [[A]] ss k B K0 | [[B]] implies K = K0 and [[A]] ss [[B]] .
Proof of Cancelation Lemma 8.4. If k B K | [[A]] ss k B K0 | [[B]], then it must be
K = K0, otherwise the two processes would be distinguishable through interactionsat cause names (by Remark 7.13 processes [[

A]], [[B]], and o/ -derivatives of them,cannot perform actions at a cause name).

The difficult part is to prove [[A]] ss [[B]]. For this, we show that
R = {([[Aae]], [[Bae]]) : ae is a finite cause substitutionand for some

I and ki, Ki (i 2 I),
A and B are saturated for \Pi i2I ki B Kiand

\Pi i2I ki B Ki | [[A]] ss \Pi i2I ki B Ki | [[B]]}

is a weak bisimulation up to & and up to context. Thus the thesis will follow byletting

ae equal to the empty cause substitution and letting \Pi i2I ki B Ki equalto
k B K in the definition of R . In the rest of the proof, we abbreviate \Pi i2Ias
\Pi i. We only show how the moves of [[Aae]] are matched by [[Bae]], since R issymmetrical. We consider the case of weak input transitions; the cases of output and

invisible transitions are similar or easier. Thus suppose:

[[Aae]] ahebki=) P (33)
We have to find processes Q, P 0 and Q0 and a static context C[ . ] s.t.

P & C[P 0] , [[Bae]] ahebki=) Q & C[Q0] and (P 0, Q0) 2 R . (34)
From Proposition 7.12(2), we know that, for some H and A0,

P & k B H | [[A0]] (35)
and

Aae

ahebi=)

H; k A0. (36)

Take a fresh k0 2 K ; since k0 is fresh, it holds that k0 /2 K (A, B)[ ([i2IKi)[{ki :
i 2 I} and, moreover, k0ae = k0 iff k0 = k0 (note that such a fresh k0 exists becausethe nominated sets of causes are finite and

ae is a finite cause substitution).By applying first Lemma 4.8 and then Lemma 4.7 to (36), we infer that

A

ahebi=)

H0; k0 A00 (37)

with

H0ae = H and A00ae[k0  k] = A0. (38)

Applying again Proposition 7.12 to (37), we get

Causality in the ss-calculus 399

[[A]] ahebk0i=) P1 & k0 B H0 | [[A00]] (39)
from which, using rule S-par, we infer

\Pi i ki B Ki | [[A]] ahebk0i=) \Pi i ki B Ki | P1 def= R . (40)
Since, by hypothesis, \Pi i ki B Ki | [[A]] ss \Pi i ki B Ki | [[B]], there is S s.t.

\Pi i ki B Ki | [[B]] ahebk0i=) S ss R . (41)
Since \Pi i ki B Ki cannot perform an action at aand, moreover,

\Pi i ki B Ki cannot interact with any T such that [[B]] =) T or

[[B]] ahebk0i=) T (by Remark 7.13), the term \Pi i ki B Ki does not contribute to theabove weak transition. That is, there exists

Q1 such that

[[B]] ahebk0i=) Q1 (42)
and

S = \Pi i ki B Ki | Q1 . (43)

From (42) and Proposition 7.12(2), we deduce that, for some B00 and H00,

B

ahebi=)

H00; k0 B00 (44)
with

Q1 & k0 B H00 | [[B00]]. (45)

Now, since & implies ss and parallel composition preserves ss, we have:

\Pi i ki B Ki | k0 B H0 | [[A00]] ss \Pi i ki B Ki | P1 (from (39))=

R (from (40))ss
S (from (41))=
\Pi i ki B Ki | Q1 (from (43))ss
\Pi i ki B Ki | k0 B H00 | [[B00]] (from (45)) .(46)

Thus, from (37), (44) and (46), we have found that A

ahebi=)

H0; k0 A00, that B

ahebi=)
H00; k0 B00and that
\Pi i ki B Ki | k0 B H0 | [[A00]] ss \Pi i ki B Ki | k0 B H00 | [[B00]]; moreover,by definition of R , we know that

A and B are saturated for \Pi i ki B Ki . Fromthese facts, using Lemma B.4 we deduce that

H0 = H00.Having proved that
H0 = H00, we can now conclude the proof of this lemma.Applying Lemma 4.8 and Lemma 4.6 to the transition in (44) we get

Bae

ahebi=)

H00ae; k B0

with B0 = B00ae[k0  k]. Since H0 = H00 and H0ae = H (by 38), the above transitioncan be rewritten as

400 M. Boreale, D. Sangiorgi

Bae

ahebi=)

H; k B0 .
Moreover, by Proposition 7.12(1) we also have

[[Bae]] ahebki=) Q & k B H | [[B0]] . (47)
We can now show that transition (47) matches the one by [[Aae]] in (33). For this, wehave to exhibit two processes

P 0, Q0 and a static context C[ . ] s.t. (34) is satisfied.

We set P 0 def= [[A0]], Q0 def= [[B0]] and C[ . ] def= k B H | [ . ]. We first show that(

P 0, Q0) 2 R . We recall that A0 = A00ae[k0  k], that B0 = B00ae[k0  k] andthat

ae[k0  k] is a finite cause substitution. From the Saturation Lemma, appliedto the transitions in (37) and (44), and since

H0 = H00, we deduce that A00 and B00are saturated for
\Pi i ki B Ki | k0 B H0 . These facts and (46) demonstrate that(
P 0, Q0) 2 R . Finally, (35) and (47) demonstrate that P & C[P 0] and Q & C[Q0];hence (34) holds, which closes up the bisimulation. \Lambda 