

Science  of Computer 
Programming 

Functional  algorithm  design  ' 

Richard  S.  Bird 
Programming  Research  Group,  Oxford  University,  Wolfson  Building,  Parks  Road Oxford,  OXI  3QD.  UK 

Abstract 

For  an  adequate  account  of  a  functional  approach  to  the  principles  of  algorithm  design  we 
need  to  find  new  translations  of  classical  algorithms  and  data  structures,  translations  that  do 
not  compromise  efficiency.  For  an  adequate  formal  account  of  a  functional  approach  to  the 

specification  and  design  of  programs  we  need  to  include  relations  in  the  underlying  theory. 
These  and  other  points  are  illustrated  in  the  context  of  sorting  algorithms. 

1.  Introduction 

As  a  subject  taught  in  the  core  curriculum  of  most  undergraduate  computing  degrees, 
Algorithm  Design  is  concerned  with  explaining  basic  strategies  for  efficient  computa- 
tion,  strategies  such  as  greedy  algorithms,  dynamic  programming,  and  divide  and  con- 
quer,  together  with  the  use  of  appropriate  data  structures  for  representing  information. 
These  strategies  are  illustrated  with  descriptions  of  famous  algorithms  from  the  liter- 

ature  of  computing  science.  A  comprehensive  treatment  is  given  in  the  excellent  text 

[S].  Normally  the  subject  is  studied  using  imperative  dictions,  but  one  can  also  attempt 
a  functional  approach.  Trying  to  express  standard  algorithms  in  functional  form  is  both 
an  exhilarating  and  challenging  experience.  It  is  exhilarating  because  of  the  amount  of 
ground  that  can  be  covered  in  a  short  course,  and  challenging  because  many  traditional 
algorithms  need  to  be  completely  rethought  in  a  functional  setting. 

New  algorithms  for  old  are  beginning  to  emerge.  For  example,  one  can  cite  David 
King's  and  John  Launchbury's  elegant  treatment  [13]  of  various  graph  algorithms,  de 

Moor's  characterisation  of  dynamic  programming  [21],  a  functional  approach  to  pattern 
matching  [2,  10,  Ill,  and  Chris  Okasaki's  recent  work  [22,23]  on  purely  functional 
queues.  But  much  remains  to  be  done;  for  instance,  as  far  as  we  are  aware,  there  is  no 
effective  treatment  of  the  Union-Find  problem  in  a  functional  setting,  a  point  returned 
to  below.  It  may  be  the  case,  as  is  suggested  in  [24,  141, that  some  classes  of  algorithm 

' Modified  version  of  an  invited  talk  at  MPC,  1995 
0167-6423/96/$15.00 @  1996 Elsevier  Science  B.V.  All  rights  reserved 

SSDIO167-6423(95)00033-X 

16  R.S.  BirdlScience  of  Computer  Programming  26  (1996)  15-31 
are  inherently  inefficient  in  any  formalism  for  programming  that  lacks  updatable  state, 
but  this  point  has  not  yet  been  settled  in  a  satisfactory  manner. 

Unlike  other  formalisms,  functional  programming  offers  a  unique  opportunity  to  ex- 
ploit  a  compositional  approach  to  Algorithm  Design,  and  to  demonstrate  the  effective- 
ness  of  the  mathematics  of  program  construction  in  the  presentation  of  many  algorithms. 

However,  for  an  adequate  formal  account  of  programming  with  functions  we  need  to 

include  relations  in  the  underlying  theory.  With  relations  our  powers  of  description  are 
increased,  and  calculations  can  be  unified.  But,  like  the  embedding  of  the  real  line  in 
the  complex  plane,  the  extension  to  relations  should  be  as  seamless  as  possible,  and 
preserve  the  shape  and  simplicity  of  its  functional  subset  as  much  as  possible.  In  recent 

years  there  has  been  a  growing  appreciation  of  the  need  for  relations  in  formal  pro- 

gram  development  ([ 1,8,  12,  18,  19,21,25],  to  cite  just  a  few  references),  though  not 
everyone  takes  the  view  that  relational  programming  is  a  generalisation  of  functional 
programming. 

In  the  rest  of  the  paper  these  remarks  are  amplified,  using  problems  in  sorting  and 
searching  as  illustrations.  Our  aim  is  to  indicate  something  of  the  unique  perspective 
on  programming  that  a  functional  approach  can  reveal. 

2.  On  composition 

Consider  the  following  well-known  functional  version  of  Hoare's  quicksort: 

sort  x  =  [I, 

=  sort  y  -!+ [a] it  sort  z, 

where  (y,  a, z)  =  split  x 

ifx=[] 
otherwise 

split  (a  : x)  =  (jilter  ( <a)  x, a,  jilter  ( >  a)  x). 
It  is  an  academic  point  (i.e.  interesting,  even  intriguing,  but  probably  not  of  practical 
consequence)  whether  this  program  can  legitimately  be  called  quicksort.  After  all,  the 
heart  of  quicksort  -  the  partition  phase  that  burns  the  candle  at  both  ends  -  is  missing, 

and  there  is  no  notion  of  an  in  situ  algorithm  in  functional  programming.  What  is 
more,  quicksort  is  a  terrible  algorithm  in  functional  form:  its  expected  running  time  is 

easily  beaten  by  mergesort,  among  others,  and  it  contains  a  space  leak. 

However,  that  is  not  the  point  at  issue  here.  In  most  texts  on  Algorithm  Design, 
sorting  is  quickly  followed,  in  the  same  chapter  or  the  following  one,  with  a  discussion 
of  selection.  The  standard  expected  linear  time  selection  algorithm  is  introduced  by 
phrases  such  as  "can  be  modelled  on  quicksort",  or  "follows  the  structure  of  quicksort". 
But  suppose  we  define 

select  x  k  =  index  (sort  x)  k, 
where  index  x  k  is  the  kth  element  of  x  (counting  from  0): 

index  (a  : x)  0  =  a 
index  (a  : x)  (k  +  1)  =  index  k  x. 

R.S.  BirdlScience  of' Computer  Programming  26  (1996)  15-31  17 
Then  we  can  calculate,  for  nonempty  x: 

select  x  k 
=  {definition  of  select) 

index  (sort  x)  k 
=  {definition  of  sort,  with  (y,a,z)  =  split x} 

index  (sort  y  +  [a] +  sort  z)  k 
=  {since  index  (u +  u)  k  =  (k  <  #u +  index  u  k,  index  v  (k  -  #u))} 

(k  <  #sort  y  +  index  (sort  y)  k,  index  ([a]  +  sort  z)  (k  -  #sort  y)) 
=  {since  #sort  y  =  #y  =  n  (say)} 

(k  <  n +  index  (sort  y)  k,  index  ([a]  Stsort  z)  (k  -  n)) 
=  {last  but  one  step  again,  and  definition  of  index} 

(k  <  n --+ index  (sort  y)  k,  k  =  n +  a,  index  (sort  z)  (k  -  n -  1)) 
=  {definition  of  select} 

(k<n+selectyk,  k=n+a,  selectz(k-n-1)). 

The  above  calculation  makes  two  appeals  to  the  identity 

index  (u +tv)  k =  (k  <  #u +  index  u  k,  index  v  (k  -  #u)), 
which  can  be  justified  by  an  induction  argument.  In  summary,  we  have  derived  the 

following  algorithm  for  select: 

select  x  k  =  select  y  k,  ifk<n 

=  a,  ifk=n 
=  select  z  (k-n-l),  ifk>n 

where  (y,  a, z)  =  split  x  and  n  =  #y 

The  form  of  this  calculation  is  a  common  one:  a  new  function  is  expressed  as  the 

composition  of  given  ones  (since  select  =  index . sort)  and  then  massaged  to  improve 
efficiency.  Studying  the  ideas  of  equational  reasoning  in  the  setting  of  functional  pro- 
gramming  is  simple,  attractive,  and  has  the  not  so  inconsiderable  psychological  benefit 
of  showing  why  it  is  useful:  just  compare  the  running  times  of  the  two  versions  of  the 
program. 

The  second  version  of  select  does  not  exploit  laziness  in  any  essential  way,  so  it  is 
straightforward  to  write  down  a  recurrence  for  estimating  its  asymptotic  worst  (average, 
best)  case  running  time.  In  the  absence  of  a  precise  cost  model,  this  analysis  is  a  little 
crude,  but  the  same  criticism  holds  for  algorithms  expressed  in  `pidgin'  Pascal  or  C. 
It  is  a  little  more  complicated  to  construct  a  recurrence  relation  for  the  running  time 
of  index  (sort  x)  k  that  takes  account  of  lazy  evaluation,  so  another  advantage  of 
the  calculation  above  is  that  it  leads  to  a  program  with  a  more  tractable  asymptotic 

18  R.S.  BirdlScience  of  Computer  Programming  26  (1996)  15-31 
analysis.  In  this  context,  consider  insertion  sort: 

isort  []  =  [I 
isort  (a  : x)  =  insert  (a,  isort  x) 

insert  (a,  [ 1)  =  [a] 
insert  (a, b  : x)  =  a  : b  : x,  if  a<b 

=  b  : insert  (a,x),  otherwise 

It  is  well-known  (see  e.g.  [3])  that  under  lazy  evaluation  isort  turns  out  not  to  be 

sorting  by  insertion  at  all,  but  a  version  of  selection  sort.  Consequently,  defining  min  = 
head.  isort  gives  a  linear  time  method  for  computing  the  minimum  of  a  list.  Plugging 

in  sort  instead  of  isort  in  the  definition  of  min  gives  a  quadratic  time  algorithm  as 
one  can  easily  check.  Although  the  consequences  of  a  lazy  evaluation  strategy  should 
be  appreciated  and  understood,  the  technique  should  be  exploited  only  in  well-defined 

situations,  and  certainly  not  in  the  definition  of  min. 

Exercise.  By  the  way,  while  on  the  subject  of  sorting  and  selection,  here  is  an  attractive 
programming  exercise:  the  function  merge  merges  two  ascending  sequences  into  one; 

given  x  and  y  both  of  positive  length  n,  and  assuming  that  index  x  k  can  be  evaluated 
in  constant  time,  find  an  O(log  n)  algorithm  for  computing  index  (merge  (x, y))  n. 

3.  On  datatypes 

Both  quicksort  and  insertion  sort  were  presented  above  as  flat  recursion  equations, 
which  seems  to  run  contrary  to  the  point  about  a  compositional  approach  to  program- 
ming  made  in  the  introduction.  So,  what  is  the  compositional  form  of  quicksort?  The 
answer  is  tree  sort,  defined  by 

sort  =jatten  . mktree. 
Here,  Batten  :  list  A  t  tree  A  and  mktree  :  tree  A  c  list  A,  where  A  is  the  type 

of  elements  to  be  sorted,  so  trees  are  used  as  an  intermediate  datatype.  The  notation 
f  : A  +-  B  to  describe  a  function  of  type  "A  from  B"  is  non-standard  but  fits  in  well 
with  the  definition  of  composition,  which  now  takes  the  smooth  form:  if  f  : A  c  B 
and  g  :  B  t  C,  then  f  g  :  A  +  C.  The  notation  (due  to  Lambert  Meertens) 
is  consistent  with  the  normal  way  of  writing  functional  application,  and  extends  to 
relational  composition  as  we  shall  see. 

The  type  tree  A  is  defined  by  a  type  equation 

tree  A  :I= null  1 fork  (tree  A, A,  tree  A). 
In  a  similar  spirit,  the  datatype  of  (cons-)  lists  can  be  introduced  by 

list  A  ::= nil  1  cons  (A,  list  A). 

R.S.  BirdlScience  of  Computer  Programming  26  (1996)  IS-31  19 
From  now  on,  nil  is  used  as  an  alternative  to  [ ]  and  COHS (a,~)  as  an  alternative  to 

a  : x. 

The  experienced  functional  programmer  will  have  realised  by  now  that  in  these  and 
other  definitions  already  given,  some  traditionally  curried  functions  have  been  replaced 
by  non-curried  ones.  One  can  argue  that  currying  -  like  lazy  evaluation  -  should  only 
be  exploited  in  those  situations  that  really  need  it.  After  all,  a  function  space  is  a  more 

complicated  object  than  a  Cartesian  product,  a  point  universally  appreciated  by  first 
year  undergraduates  as  well  as  category  theorists. 

But,  back  to  treesort.  The  function  patten  is  defined  by 

flatten  null  =  [I 

flatten  (fork  (x, a, y))  =  Jatten  x  +t  [a] St  flatten  y. 

This  recursion  is  an  instance  of  a  fold  operation  on  trees.  More  precisely,  defining 

foldtree  (c, f)  null  =c 
foldttree  (c, f)  Cfork  (x, a, y))  =  f  Cfoldtree  (c, f)  x, a,  foldtree  (c, f)  y), 

we  have  j&ten  =  foldtree  (nil,join),  where  join  (x, a, y)  =  x  +[a]  sty.  In  the  same 
spirit,  the  fold  operator  on  the  datatype  of  cons-lists  is  defined  by 

foldlist  (c, f)  nil  =  c 
foldlist  (c, f)  (cons  (u,x))  =  f  (a, foldlist  (c,f)  x), 

In  particular,  the  function  isort  of  the  previous  section  is  now  expressed  by  isort  = 
foldlist  (nil,insert).  Fold  operators  are  of  fundamental  importance  in  functional  pro- 

gramming,  not  least  because  they  provide  one  disciplined  use  of  recursion,  namely  a 
recursive  decomposition  that  follows  the  structure  of  the  datatype.  For  this  reason,  they 

have  been  called  catumorphisms,  a  term  intended  to  mean  "downwards,  according  to 

form".  It  was  Grant  Malcolm  [ 161 who  suggested  using  the  so-called  "banana"  brackets 
to  denote  fold  operators,  in  which  both  foldtree  (c, f)  and  foldlist  (c, f)  are  written 

@c,fD,  the  ambiguity  being  resolved  by  contextual  type  information.  We  will  use  this 
notation  in  what  follows. 

But,  back  to  treesort.  The  function  mktree  produces  an  ordered  tree  t  in  the  sense 
that  for  any  subtree  fork  (~,a,  v)  of  t,  nodes  in  u  have  values  which  are  no  greater 
than  a,  and  nodes  in  v  have  values  which  are  no  smaller  than  a.  One  way  of  defining 
mktree  is 

mktree  x  =  null, 

=  fork  (mktree  y,u,mktree  z), 

where  (y,  a, z)  =  split  x 

if  x  =  nil 
otherwise 

split  (cons  (u,x))  =  (jilter  (<a)  x,a,Jilter  (>  a)  x). 
This  recursion  defines  mktree  as  an  instance  of  an  unfold  operation.  We  will  use 

Meijer's  concave  lenses  [ 171 to  denote  unfold  operators  (which  are  also  given  a  fancy 
name  unamorphisms,  meaning  "upwards,  according  to  form").  On  trees  the  operation 

20  R.S.  BirdlScience  of' Computer  Programming  26  (1996)  15-31 

Kp,f]  is  defined  by 

Kp,fJ  x  =  null  if  p  x 

=fori  (Kp,f1  v,a,K~,f%  z),  otherwise 

where  (v,  a, z)  =  f  x 

Thus,  mktree  =  [isnil,splitJ,  where  isnil  x  =  (x  =  nil). 

There  is  a  subtle  point  to  make  in  connection  with  [p,f]:  in  a  formalism  for 
functional  programming  in  which  elements  of  datatypes  may  be  infinite,  [p,f]  is  a 
well-defined  function,  though  it  may  return  an  infinite  tree.  Under  an  interpretation 
that  makes  the  constructor  fork  strict,  the  recursion  is  not  well-defined  unless  f  is 

"well-founded"  in  a  suitable  sense  that  we  will  not  make  precise. 

Like  folds,  unfolds  can  be  defined  for  arbitrary  datatypes,  and  we  will  see  the  re- 
lationship  between  them  in  Section  6.  For  example,  over  cons-lists  [p,f]  has  the 
definition 

Kp,f1  x  =  nil,  if  p  x 

=  cons  (a,  [p,  f]  y),  otherwise 

where  (a,  y)  =  f  x. 

The  same  remarks  about  [p,f]  on  trees  apply  here:  under  a  strict  interpretation  of 
cons  the  function  [p,fJ  will  not  be  total  unless  f  is  well-founded. 

Given  these  definitions,  one  can  appreciate  -  and  a  short  calculation  of  the  kind 
seen  earlier  will  justify  -  that  sort  is  simply  that  version  of  tree  sort  in  which  the 
intermediate  datatype  has  been  eliminated.  In  fact  the  calculation  is  a  standard  one:  any 
function  expressed  as  the  composition  of  a  fold  with  an  unfold,  using  an  intermediate 
datatype  T,  can  be  recast  as  a  recursion  in  which  elements  of  T  do  not  appear.  This 
remark  is  formalised  in  Section  6. 

4.  On  new  algorithms  for  old 

By  changing  the  definitions  of  flatten  and  mktree  one  can  arrive  at  a  version  of 
heapsort.  Define 

Jutten  =  (nil,  join] 
join  (x, a, y)  =  cons  (a, merge  (x, y)), 

where  merge  merges  two  sorted  lists  into  one.  Also,  define 

mktree  =  [isnil,  split], 
where  isnil  x  =  (x  =  nil),  and  split  is  defined  by 

split  (cons  (a,~))  =  abase  a, step]  x 

base  a  =  (nil,  a, nil) 
step  (a, (x, b, y))  =  (cons  (b,  y),a,x),  if  a <  b 

=  (cons  (a,  y),  b,x),  otherwise 

R.S.  BirdlScience  of  Computer Programming 26  (1996)  15-31  21 
This  time,  the  function  split  takes  a  nonempty  list  and  returns  a  triple  in  which  the 
middle  value  is  a  minimum  element  of  the  list  and  the  outer  two  values  are  lists  of 
the  remaining  elements,  chosen  to  be  of  as  equal  length  as  possible.  With  the  above 

definitions,  mktree  generates  a  balanced  tree  t  that  satisfies  the  condition  that  for  any 

subtree  fork  (u, a, u)  of  t,  no  value  in  u  or  2: is  smaller  than  a.  This  is  the  condition 
that  the  tree  forms  a  min-heap. 

It  is  probable  that  J.W.J.  Williams,  the  inventor  of  heapsort  [28],  would  wince  if  he 
heard  the  above  program  described  as  heapsort.  Translators  of  literary  texts  are  allowed 

some  degree  of  freedom,  but  the  spirit  of  the  original  should  be  preserved,  and  it  is 
arguable  whether  or  not  the  two  functions  above  do  preserve  the  spirit  of  heapsort. 
For  a  start,  Jlatten  is  usually  expressed  as  an  iterative  algorithm,  one  that  repeatedly 
removes  the  top  element  and  combines  the  two  subheaps  into  one  until  nothing  remains. 

Calling  this  process  unheap  : list  A  +-  tree  A,  we  have 

sort  =  unheap  . mkheup 

unheap  =  [isnull,  remove], 

where  isnull  x  =  (x  =  null)  and  remove  (fork  (~,a,  y))  =  (a,  combine  (x,  y)).  Thus, 

unheap  is  defined  using  an  unfold  operation  on  lists.  We  will  not  spell  out  the  definition 
of  combine,  except  to  say  that  it  will  satisfy  the  equation 

unheup  . combine  =  merge.  (unheap  x  unheap), 
where  (f  x  g)  (a,  b)  =  (f  u,g  b).  This  equation  can  be  used  in  a  short  calculation  to 

show  unheup  =fIatten,  so  the  two  programs  coincide. 

The  second,  more  telling,  point  is  that  mktree  as  defined  above  builds  a  heap  in 
O(n  log  n)  steps,  where  n  is  the  length  of  the  given  list.  The  textbooks  emphasise  that 
one  can  build  a  heap  in  linear  time.  This  is  important  in  a  number  of  applications, 

for  example  in  Kruskal's  algorithm  for  finding  minimum  cost  spanning  trees,  where 
complete  sorting  may  not  be  required. 

So,  let  us  try  to  translate  the  standard  array-based  linear  time  algorithm  into  purely 
functional  form.  The  function  mkheap  : tree  A  +  list  A  is  defined  by 

mkheup  =  heapzfy  mktree, 
where  mktree  :  tree  A  +  list  A  builds  a  tree,  not  necessarily  a  heap,  and  heapiJji  : 

tree  A  +-  tree  A  turns  it  into  a  valid  heap.  We  can  define  heupzfy  as  a  tree  cata- 
morphism.  In  the  following  definition,  a  fictitious  element  WA  is  used  to  represent  the 
largest  element  of  type  A;  such  a  device  can  be  avoided  but  it  does  make  the  algorithm 
simpler: 

heupify  =  (null,  sift] 
sift  (x, u, y)  =  fork  (x, a, Y),  if  a d  min  (b, c) 

=  fork  (sift  (move  (a,~)),  b, y),  if  b d  min  (a, c) 

=  fork  (x,c,szft  (moue  (a,  y))),  if  cd  min  (a, b) 

where  b  =  root  x  and  c  =  root  y 

22  R.S.  Bird/Science  cf  Computer  Programming  26  (1996)  15-31 
The  functions  move  and  root  are  given  by 

move  (a,  fork  (x, b, Y>>  =  (x,  a,  y> 
root  null  =  %jA 

root  (fork  (~,a,  y))  =  a. 

The  value  min  (a,  b)  is  the  smaller  of  a  and  6. 

It  remains  to  implement  the  function  mktree.  The  standard  algorithm  takes  a  list 
[a~, al,.  . .]  and  builds  a  tree  with  a0  at  the  top,  al,  a2  at  the  next  level,  as,  a4,a5,  a6  at 
the  next  level,  and  so  on  until  the  list  is  exhausted.  The  length  of  the  list  at  the  bottom 

level  will  not  in  general  be  a  power  of  two.  Of  course,  in  the  array  based  algorithm 
no  building  actually  takes  place;  the  array  is  just  viewed  as  forming  such  a  tree  and 
everything  is  done  by  juggling  subscripts. 

We  can  build  the  tree  from  bottom  to  top  by  constructing  a  sequence  of  trees  at 
each  level;  the  trees  at  the  next  level  higher  up  are  formed  by  combining  trees  in  pairs 
with  appropriate  elements  of  the  list.  At  the  end  of  this  process  we  are  left  with  a  list 
containing  a  single  tree.  To  implement  the  idea  we  define 

mktree  =  head  . mktrees  . levels. 
The  function  levels  : list  (list  A)  +  list  A  applied  to  [a~, al,.  .]  produces  the  list 

[[aOk  [al,a2],  [a3,a4,a5,  a61>  ..1 
and  is  defined  by  an  unfold: 

levels  =  [isrnil,  level]  . start 
start  x  =  (1,x) 

isrnil  (k,x)  =  (x  =  nil) 
level  (k,x)  =  (take  k  x,(2  x  k,drop  k  x))). 

The  curried  functions  take  k  and  drop  k,  respectively,  take  and  drop  the  first  k  elements 
of  a  list. 

The  function  mktrees  : list  (tree  A)  +-  list  (list  A)  is  defined  by 

mktrees  =  @[null], layer), 
where  layer  : list  (tree  A)  +  (list  A  x  list  (t ree  A))  is  defined  by  an  unfold;  applied 
to  the  pair  ([ao,ai,.  .], [ug, UI , . . .)]  this  function  produces  the  list 

York  (~~,a~,ulMork  (~2,a1,~3),..  .I. 
If  the  list  [UO, ~1,.  . .]  of  trees  is  not  long  enough,  it  is  filled  with  a  sufficient  number 
of  empty  trees.  The  definition  of  layer  is: 

layer  =  [is&l,  step] 
islnil  (x,  ts)  =  (x  =  nil) 
step  (cons  (a,~),  nil)  =  (Jerk  (null,  a, null),  (x, nil)) 

step  (cons  (a,~),  cons  (24, nil))  =  cfork  (u, a, null),  (x, nil)) 
step  (cons  (a,~),  cons  (u,  cons  (v, ts)))  =  cfork  (u, a, v),  (x, ts)). 

R.S.  BirdIScience  of  Computer  Programming  26  (1996)  15-31  23 
This  completes  the  new  definition  of  mkheap.  It  is  an  instructive  exercise,  left  to  the 
reader,  to  show  that  mkheap  takes  linear  time. 

The  new  version  of  heapsort  shows  that  some  standard  algorithms  can  be  translated  to 
functional  form  while  preserving  the  spirit  of  the  original.  But  there  are  other  algorithms 
whose  functional  translations  are  not  obvious.  In  particular,  Kruskal's  algorithm  for 
minimum  cost  spanning  trees  uses,  in  addition  to  a  heap,  an  algorithm  for  the  Union- 
Find  problem.  The  Union-Find  problem  concerns  the  efficient  implementation  of  three 

operations  on  disjoint  sets,  specified  as  follows: 

units  :  setA-+set  (setA) 
units  x  =  Ha)  I  a Exl 

:  set  (set  A)+A+set  A 
=  "the  (unique)  set  x  in  xs  that  contains  a" 

union  :  set  (set  A)  --7` set  A  +  set  A  4  set  (set  A) 

union  xs  x  y  =  (xs  -  {x}  -  {y})  U {x  U y}. 

Various  schemes  for  maintaining  partitions  are  known  [26,27],  but  currently  it  is  not 

clear  how  to  achieve  comparable  efficiency  in  a  purely  functional  setting. 

5.  On  relations 

Relations  have  been  knocking  at  the  door,  demanding  entry  for  some  time  now,  and 
it  is  time  to  let  them  in.  One  reason  concerns  the  nature  of  the  relationship  between  the 

fold  and  unfold  operations,  and  another  concerns  program  specification  in  general.  In  a 
purely  functional  framework  one  can  model  relations  by  set-valued  functions,  but  the 
mathematics  becomes  fussy.  It  becomes  even  fussier  if  we  have  to  model  set-valued 

functions  by  list-valued  ones.  With  relations  things  are  significantly  simpler.  Moreover, 
unlike  functions  every  relation  has  a  converse,  and  the  use  of  converse  operations  are 

important  both  in  specification  and  program  development. 

Consider  for  instance  the  following  purely  functional  specification  of  sorting  a  list 
of  elements  under  a  preorder  9: 

sort  =  head  . jlter  uplist  . perms 

uplist  x  =  and  [a  d  b  1  (a, 6)  c  zip  (x,  tail  x)]. 

The  function  perms  returns  a  list  of  all  permutations  of  a  sequence,  and  the  boolean- 
valued  function  uplist  determines  whether  a  sequence  is  ascending  under  4.  While  this 
is  an  acceptable  specification  of  sort,  a  better  one  is  to  specify  sort  to  be  a  function 
satisfying  the  inclusion 

sort  C  uplist?  perm,  (1) 

24  R.S.  BirdlScience  of  Computer  Programming  26  (1996)  15-31 
where  perm  and  uplist?  are  now  relations  rather  than  functions.  (If  _a  is  a  linear  order, 
then  uplist?  . perm  is  itself  a  function,  but  the  expression  is  not  capable  of  being 

implemented  directly  in  a  standard  functional  language,  so  there  is  still  work  to  do.) 

Since  we  want  to  preserve  compatibility  with  functions,  we  think  of  relations  as  tak- 
ing  arguments  on  the  right  and  delivering  results  on  the  left,  so  our  relational  composi- 
tion  takes  the  same  form  as  functional  composition  (we  want  an  ordered  permutation, 
not  a  permutation  of  an  ordered  list). 

As  a  relation,  uplist?  C  id,  where  id  is  the  identity  relation  on  lists,  and  holds  for  x 
just  when  uplist  x  is  true.  A  relation  R  such  that  R  C  id  is  called  a  corejexive  (because  a 

relation  R  satisfying  id  CR  is  a  reflexive  relation).  More  generally,  p?  is  the  coreflexive 
that  holds  for  x  just  when  the  predicate  p  x  is  true.  One  can  define  coreflexives  by 

translating  the  corresponding  predicate,  but  it  is  usually  more  satisfactory  to  define 
them  directly.  In  particular,  one  can  define  uplist?  as  a  relational  catamorphism 

uplist?  =  @nil, cons  . ok?], 

ok  (a,x)  =  (`db : b  inlist  x  : a  a  b). 
The  relation  inlist  : A  +  list  A  is  the  membership  relation  for  lists.  It  is  not  immediately 
clear  how  to  define  the  membership  relation  for  an  arbitrary  datatype,  but  the  matter 

was  finally  settled  by  Hoogendijk  and  de  Moor  in  [9].  It  would  take  too  long  to  explain 
how  to  define  inlist  in  the  relational  calculus,  so  we  will  just  accept  it.  For  the  same 
reason,  the  following  formal  definition  of  ok?  is  given  without  explanation: 

ok?  =  id  n  (outlo  . (a  /inZist")  outr). 
This  "point-free"  style  is  typical  in  a  number  of  presentations  of  the  relational  calculus 

(see  [7]);  at  first  sight  it  seems  arcane,  but  one  soon  gets  used  to  it  and  calculations 
without  variables  are  significantly  simpler. 

To  define  the  relation  perm  we  need  the  fundamental  operation  of  taking  the  converse 
R"  of  a  relation  R,  defined  by  xROy  =  yRx.  Then  we  can  define 

perm  =  bagify"  . baggy, 
where  bagzjj  turns  a  list  into  a  bag  of  its  elements.  Thus  perm  is  defined  using  bags  as 

an  intermediate  type:  turning  a  list  into  a  bag  and  then  turning  it  back  into  a  list  gives 
a  permutation  of  the  original.  The  function  bagify  can  be  defined  as  a  catamorphism 

bagify  =  (nilbag,  consbag], 
where  nilbag  is  the  empty  bag  and  consbag  adds  an  element  to  a  bag. 

R.S.  BirdlScience  of  Computer  Programming  26  (1996)  15-31  25 
6.  On  fold  and  unfold 

Now  let  us  mm  to  the  formal  definitions  of  fold  and  unfold  in  a  relational  setting. 
What  follows  will  be  rather  brief  and  incomplete  in  various  ways,  but  it  is  hoped 

enough  of  the  general  idea  comes  across  to  stimulate  further  reading.  Grant  Malcolm's 
thesis  [ 161  is  a  good  starting  point,  as  is  [ 171  which  was  written  for  functional  pro- 
grammers.  And,  if  you  can  wait  long  enough,  a  complete  account  will  appear  in  the 

forthcoming  text  [4]  to  be  published  later  this  year. 

As  we  have  seen  in  the  case  of  lists  and  trees,  whenever  one  declares  a  datatype  a 
number  of  functions  are  brought  into  play.  In  part,  declaring  a  datatype  as  an  equation 

asserts  the  existence  of  an  isomorphism  between  the  types  on  the  left  and  right.  In  the 
case  of  lists  this  takes  the  form 

list  A  E  1 +  (A  x  list  A). 
The  type  1  consists  of  just  one  member  and  serves  as  the  source  type  for  constants. 
The  type  constructor  x  is  Cartesian  product,  and  +  is  disjoint  sum.  The  right-hand  side 
can  be  rephrased  as 

list  A  E  F(A,  list  A), 
where  F(A,B)  =  1 +  (A  x  B)  is  a  mapping  from  types  to  types.  We  can  also  use  F  as 
a  mapping  from  functions  to  functions  by  defining 

F(f,g)  =  id1  +  (f  x  s), 
where  id,  is  the  identity  function  on  1.  A  function  having  a  dual  role  both  as  a 
mapping  between  types  and  a  mapping  between  functions  is,  provided  certain  properties 
are  satisfied,  called  a  functor.  The  functor  F  defined  above  takes  a  pair  of  types  or 

functions  as  argument  and  so  is  sometimes  called  a  bzjiinctor.  One  property  we  require 
of  a  functor  F  is  that  if  f : A  c  B,  then  Ff  : FA  +-  FB.  The  other  properties  are  the 
identity  and  composition  rules: 

Fid  =  id 
F(f . g)  =  Ff . Fg. 

In  the  case  of  bifunctors  the  rules  are,  firstly,  that  if  f  : A  c  C  and  g  : B  c  D,  then 

F( f,  g) : F(A,B)  +-  F(C, 0);  and,  secondly,  that 

F( id,  id)  =  id 
F(f.s,h.k)=F(f,h).F(g,k). 
The  Cartesian  product  constructor  x  can  also  be  defined  as  a  mapping  between  ftmc- 
tions:  if  f  : A  +  C  and  g  :  B  +  D,  then  f  x  g  : A  x  B  t  C  x  D  is  defined  by 

(f  x  g)  (c>d)  =  (f  c,g 0 
This  mapping  satisfies  the  identity  and  composition  rules  for  bifunctors,  so  x  is  a 

26  R.S.  BirdlScience  of  Compuier  Programming  26  (1996)  15-31 
bifunctor.  Similarly,  the  coproduct  constructor  +  can  be  defined  on  functions:  applied  to 

a  left  component  c,  the  function  f  +g  : A +B  +  C+D  returns  f  c  as  a  left  component 
of  the  result;  dually,  applied  to  a  right  component  d,  the  value  of  (f  +  g)  d  is  the 
right  component  g  d.  Again,  the  identity  and  composition  rules  are  satisfied,  so  +  is 

a  bifunctor. 

The  declaration  of  list  A  also  introduces  two  functions 

nil  : list  A  +-  1  and  cons  : list  A  +  A  x  list  A 
that  serve  to  construct  lists.  We  can  parcel  these  functions  together  as  one  function 

[nil, cons]  : list  A  t  F(A,  list  A). 
In  general,  if  f : A  +  B  and  g  : A  +  C,  then  [f, g] : A  t  B  +  C  applies  f to  left 
components  and  g  to  right  components.  The  function  [nil,  cons]  has  a  special  property, 
which  captures  the  fact  that  we  can  define  functions  on  lists  by  pattern-matching:  given 

any  function  [c, f ] : B  +  F(A, B)  there  is  a  unique  function  h  : B  +-  list  A  such  that 

h  . [nil, cons]  =  [c, f ] . F( id, h). 
Unwrapping  this  compact  equation,  we  get  two  equations 

h.nil  =  c 

h  cons  =  f . (id  x  h). 

Thus,  h  =  [c,  f 1. 

In  a  general  datatype  declaration,  which  we  can  write  in  the  form 

data  A  2--  F(A,  data  A), 
the  catamorphism  (f 1 : B  +  data  A,  taking  an  argument  f : B  +  F(A, B),  is  the 
unique  function  h  satisfying 

h.a=  f .F(id,h). 
As  a  consequence  of  the  defining  property  of  a  we  get  that  [cl]  =  id.  For  example, 
[nil,  consj  (which  we  should  have  written  as  ([nil,  cons]])  but  will  not)  is  the  identity 
function  on  lists.  Less  obviously,  it  also  follows  from  its  defining  property  that  c( is  an 
isomorphism,  meaning 

CI . cP =  id  and  cP  a =  id, 
where  CC', more  usually  written  a-`,  denotes  the  inverse  function  of  CC. The  first  id  is 
the  identity  relation  on  data  A,  and  the  second  is  the  identity  relation  on  F(A,dutu  A). 

Since  CI is  an  isomorphism,  we  can  move  it  to  the  other  side  of  the  defining  equation 
for  (f 1. Thus,  h  =  (f 1 is  the  unique  solution  of  the  equation 

h =  f. F(id,  h)  . 8. 
We  will  abbreviate  this  by  writing  flf 1 = (vh  : h  =  f . F(id,  h)  . CC'). 

R.S.  BirdlScience  of  Computer  Programminy  26  (1996)  15-31  21 
Finally,  we  also  obtain  the  extremely  useful  fusion  rule  for  combining  two  functions 
into  one: 

f  .[gJ=[hJ+f  .g=h.Ff. 
Now,  let  us  extend  all  this  stuff  to  relations.  Everything  we  have  said  above  goes 
through  when  functions  are  extended  to  relations,  provided  only  that  we  restrict  atten- 
tion  to  functors  that  are  monotonic;  that  is,  if  R  C S  then  FR c  FS.  It  can  be  shown 

that  monotonic  functors  preserve  relational  converse,  that  is,  (FR)"  =  F(R").  It  follows 
that  the  expression  FR"  is  not  ambiguous.  In  particular,  since 

(R  . S)"  =  S"  . R", 
we  get  for  a  relation  R  : data  A  t  F(A,data  A)  that 

[RI  =  (v/Y  : X  =  R.  F(id,X)  x") 
QRJ)" =  (vX  : X  =  ct. F(id,X)  R"). 

Given  a  function  ,f  : F(A,B)  +  B  the  unfold  operator  Kf ] is  defined  by 

Kf I  = tf"D". 
The  (now)  non-standard  form  [R,  f ] that  we  have  used  previously  for  unfold  on  lists 

stands  more  properly  for  [p + !, f 1, where  ! :  1  +  B  and  f : (A  x  B)  +  B. 

With  relations  we  also  get  two  variants  of  the  fusion  rule: 

R.aSDCaTD~R.ScT.F(id,R) 
R  (SD >[Tj)  e  R  . S >  T  F(id,  R). 

Finally,  writing  @_X : X  =  &Y)  for  the  least  fixed  point  (under  relational  inclusion) 
of  4,  we  get  the  following  formalisation  of  the  remark  made  in  Section  3  about  the 

simplification  of  the  composition  of  a  fold  over  a  parameterised  type  data  A  with  an 
unfold: 

(2) 
It  is  this  transformation  that  is  behind  Wadler's  deforestation  algorithm  [29]. 

7.  On  the  derivation  of  sorting  algorithms 

The  formal  derivation  and  classification  of  sorting  algorithms  is  not,  of  course,  new 
(see  e.g.  [6,20]),  but  let  us  end  with  a  brief  sampler  of  the  kinds  of  derivations  we 
can  accomplish  with  the  above  material. 

28  R.S.  BirdlScience  of Computer  Programming  26  (1996)  15-31 

7.1.  Insertion  sort 

Our  first  sorting  algorithm  arises  as  a  result  of  the  following  three-step  development: 

uplist?  perm 
=  {expressing  perm  in  the  form  [nil,  add)} 

uplist?  [nil, addD 
>  {fusion} 

(nil,  uplist? . add) 
>  {supposing  insert 2  uplist?  add} 

(nil,  insert]). 
In  outline,  we  can  express  perm  as  a  relational  catamorphism,  use  fusion  with  uplist? 
to  obtain  a  second  catamorphism,  and  finally  refine  the  result  to  a  functional  catamor- 
phism. 

The  relation  add  for  which  perrn =  [nil,  add)  can  be  defined  by 

add  (a,x  ity)  =  x +[a]  ii-y. 
It  can  also  be  defined  recursively  by 

add =  cons  U  cons  . (id  x  add)  swap  (id  x  cons'),  (3) 

where  swap  (a,(b,x))  =  (b,(a,x)).  We  omit  the  proof  of  this  fact,  as  well  as  most 

others  in  this  section.  The  function  insert  that  refines  uplist?  add  can  be  defined  by 

insert =  (ok'  -+  cons, cons  . (id  x  insert)  swap  . (id  x  cons')), 

where 

ok'  (a, nil)  =  true 

ok'  (a, cons  (b,x))  =  (a<b). 
The  remaining  step  of  the  development  requiring  justification  is  the  appeal  to  fusion, 
whose  proviso  is 

uplist? . [nil, add] >[nil,  uplist? . add]  . F(uplist?). 

But  this  follows  quickly  from  the  monotonicity  of  the  functor  F  and  the  fact  that 

uplist?  is  a  coreflexive.  The  resulting  sorting  algorithm,  namely  sort  =  @nil,insertl), 
is,  of  course,  insertion  sort. 

7.2.  Selection  sort 

Our  second  sorting  algorithm  comes  from  the  following  development: 

uplist? . perm 
=  {since  perm  =  perm'  and  uplist? =  uplist?"} 

R.S.  BirdlScience  of  Computer  Programming  26  (1996)  15-31  29 
(perm  . uplist?)" 
=  {fusion} 

$nil,perm  . cons.  ok?)" 
2  {supposing  select  C_ ok?  . cons'  . perm) 

[nil,  select']' 
=  { anamorphisms} 

[isnil,  select]. 

The  result  is  selection  sort. 

7.3.  Quicksort 

Finally,  to  derive  quicksort  we  need  the  fact  that  if  f  is  a  function,  then  f.  f"  C  id. 
This  inclusion  captures  the  fact  that  functions  map  arguments  to  at  most  one  result. 
We  also  need  the  coreflexive  uptree?  defined  by 

uptree?  =  {null,fork  okt?], 
where 

okt  (x, a, y)  =  (`v'b : b  intree  x  : b < a)  A  (Vb  : b  intree  y  : a < b). 
Then  we  can  argue  along  the  same  lines  as  in  selection  sort: 

uplist?  perm 
>  {since  flatten  : list  A  +-  tree  A  is  a  function} 

uplist?  .$atten  .JEatten'  .perm 
=  {claim:  uplist?  . flatten  = flatten  . uptree?) 

jlatten  . uptree?  .$atten'  ' perm 
=  (since  perm  =  perm'  and  uptree?  =  uptree?") 

jlatten  ' (perm  .jlatten  uptree?)" 
=  {fusion} 

Patten  flnull,perm  . join  . okt?]" 
2  {supposing  split  C  okt?  . join"  perm;  converses} 

Patten  . ~null,split"l)" 
=  { anamorphisms} 

flatten.  Kisnull,split]. 
=  {introducing  mktree  =  [isnull,  split]} 

Jlatten  . mktree. 

We  omit  the  proof  of  the  claim,  and  the  detailed  justification  of  the  fusion  step. 

30  R.S.  BirdlScience  of  Computer  Programming  26  (1996)  15-31 
References 

[I]  R.C.  Backhouse,  P.J  de  Bruin,  G.  Malcolm,  E.  Voermans  and  J.C.S.P.  van  der  Woude,  Relational 

catamorphisms.,  in:  B.  Miiller,  ed.,  Proc.  the  ZFZP  TC2/WG2.1  Working  Conf:  on  Constructing 

Programs  from  Specijications  (I 991)  287-3  18. 
[2]  R.S.  Bird,  J.  Gibbons  and  G.  Jones,  Formal  derivation  of  a  pattern  matching  algorithm,  Sci.  Comput. 

Programming  12  (1989)  93-104. 
[3]  R.  Bird  and  P.  Wadler,  Introduction  IO Functional  Programming  (Prentice  Hall,  Englewood  Cliffs,  NJ, 

1988). 
[4]  R.  Bird  and  0.  de  Moor,  The  Algebra  of  Programming  (Prentice  Hall,  Englewood  Cliffs,  NJ,  1996), 

To  be  published. 
[5]  T.H.  Cormen,  C.E.  Leiserson  and  R.L.  Rivest,  Introduction  to  Algorithms  (MIT  Press,  Cambridge, 

MA,  1990). 
[6]  J.  Darlington,  A  synthesis  of  several  sorting  algorithms,  Acta  Inform.  11  (1978)  I-30. 
[7]  P.J.  Freyd  and  A.  SEedrov,  Categories,  Allegories,  Mathematical  Library,  Vol.  39  (North-Holland, 

Amsterdam,  1990). 
[8]  A.M.  Haeberer  and  P.A.S.  Veloso,  Partial  relations  for  program  development.  in:  B.  Moller,  ed., 

Construciing  Programs  from  Spec$cations,  Proc.  IFIP  TC2iWG2.1  Conference,  Pacific  Grove,  CA, 
(1991),  (North-Holland,  Amsterdam,  1991)  3733397. 
[9]  P.  Hoogendijk  and  0.  de  Moor,  Membership  of  datatypes,  Unpublished  Draft,  1993. 
[IO]  R.  Hoogerwoord,  The  design  of  functional  programs:  a  calculational  approach,  Ph.D  Thesis,  University 

of  Eindhoven,  1989. 
[l  I]  J.  Jeuring,  Polytypic  pattern  matching,  in:  S.  Peyton  Jones,  ed.,  Con$  Record  of  FPCA  1995. 

SZGPLAN-SZGARCH-  WG2.8  (I 995)  238-248. 
[I21  G.  Jones  and  M.  Sheeran,  Circuit  design  in  Ruby,  in:  Jorge"  Staunstmp,  ed.,  Formal  Methods  for 

VLSI  Design  (North-Holland,  Amsterdam,  1990)  13-70. 
[13]  D.  King  and  J.  Launchbury,  Structuring  depth-first  search  algorithms  in  Haskell,  Proc.  ACM  Principles 

of  Programming  Languages,  San  Francisco,  1995. 
[14]  J.  Launchbury  and  S.P.  Jones,  State  in  Haskell,  University  of  Glasgow,  Preprint,  1995. 
[I51  G.  Malcolm,  Homomorphisms  and  promotability,  in:  J.  Snepscheut,  ed.,  1989  Groningen  Mathematics 

of  Program  Construction  Conf  (Springer,  Berlin,  Lecture  Notes  in  Computer  Science,  Vol.  375,  1989) 

335-347. 
[I61  G.  Malcolm,  Algebraic  types  and  program  transformation,  Ph.D  Thesis,  University  of  Groningen,  The 

Netherlands,  1990. 
[17]  E.  Meijer,  M.  Fokkinga  and  R.  Paterson,  Functional  programming  with  bananas,  lenses,  envelopes  and 

barbed  wire,  in:  J.  Hughes,  ed.,  Proc.  1991  ACM  Conf  on  Functional  Programming  and  Computer 

Architecture,  Lecture  Notes  in  Computer  Science,  Vol.  523  (Springer,  Berlin,  1991). [ 
181 A.  Mili,  A  relational  approach  to  the  design  of  deterministic  programs,  Acta  Znform.  20  ( 1983)  3 155328. 
[I91  B.  Miiller,  Relations  as  a  program  development  language,  in:  B.  Mijller,  ed.,  Constructing  Programs 

fkom  Specijcafions,  Proc.  ZFZP  TCZIWG2.Z  Conj,  Pacific  Grove,  CA,  1991,  (North-Holland, 

Amsterdam,  1991),  3733397. 
[20]  B.  Moller,  Algebraic  calculation  of  graph  and  sorting  algorithms,  in:  D.  Bjorner,  M.  Broy,  I.V.  Pottosin, 

eds.,  Formal  methods  in  Programming  and  their  Applications,  Lecture  Notes  in  Computer  Science, 
Vol.  735  (Springer,  Berlin,  1993)  3944413. 
[21]  0.  de  Moor,  Categories,  relations  and  dynamic  programming,  D.Phil.  thesis,  Technical  Monograph  PRG- 

98,  Computing  Laboratory,  Oxford,  1992;  Also  in  Math.  Strut.  in  Comput.  Sci.  4  (1994)  33-70. 
[22]  C.  Okasaki,  Simple  and  efficient  purely  functional  queues  and  deques,  J.  Functional  Programming  5, 

To  appear. 
[23]  C.  Okasaki  and  G.  Brodal,  Optimal  purely  functional  priority  queues,  J.  Functional  Programming,  To 

appear. 
[24]  G.C.  Ponder,  P.C.  McGeer  and  A.P-C.  Ng,  Are  applicative  languages  inefficient?  SZGPLAN  Notices 

23  (1988)  1355139. 
[25]  G.  Schmidt  and  T.  Strohlein,  Relations  and  Graphs,  EATCS  Monographs  on  Theoretical  Computer 

Science  (Springer,  Berlin,  1991). 
[26]  R.E.  Tarjan,  Efficiency  of  a  good  but  not  linear  set  union  algorithm,  J.  ACM.  22  (1975)  215-225. 

R.S.  BirdlScience  of' Computer  Proyramminy  26  (1996)  15-31  31 
[27]  R.E.  Tarjan  and  J.  van  Leeuwen,  Worst-case  analysis  of  set  union  algorithms,  J.  ACM.  31  (1984) 

245-281. 
[28]  J.W.J.  Williams,  Algorithm  232  (heapsort),  Commun.  ACM  7  (1964)  347-348. 
[29]  P.L.  Wadler,  Deforestation:  transforming  programs  to  eliminate  trees,  Theoret.  Comput.  Sci.  2  (1990) 

461493. 