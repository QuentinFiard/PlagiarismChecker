

Chaotic  fixpoint  iteration guided  by  dynamic  dependency 

Niels  JCrgensen nielsj@ruc.dat.dk 
Rosldlde  University Center DK-4000  Roskilde,  Denmark 

Abstract.  An  algorithm  for  abstract  interpretation  of logic  programs is defined  and  analyzed. The  algorithm is  proved  to  be  correct  with re- 
spect  to  an  abstract  semantics for  (a  variant  of)  Prolog.  This  abstract semantics  associates  a  given  progra~n with  a  function  that  maps  each 
call pattern  for  a  predicate  to  a  distinct  success  pattern.  The  proposed algorithm  employs a  variant  of chaotic  iteration,  and is  based  on  what 

may be  termed  a  dynamic dependency  relation.  A  low  worst-case corn* plexity is achieved:  the  number  of passes  of dataflow analysis  over  each 
program clause is proved to be independent  of the size of the rest  of the program. 

1  I n t r o d u c t i o n  
Compilers  for  Prolog  may produce  more  efficient low-level code  if they  employ 
static  analysis  to  obtain  information  about  a  program's  runtime  behavior.  An 

early proposal for this was made by Mellish [21]. More recent  research  has looked into  the  technique's  relevance  for  the  compilation of constraint  logic  program- 

ming  (CLP)  languages,  including  the  case  of  CL P (~ )   as  studied  by  Jaffar  el al. 

[11] and  Jr  et  al.  [13]. Although  some  optimizing program  transfor- 
mations  may  benefit  from  information  about  how  predicates  will  succeed,  as 

demonstrated  by  Marriott  and  Sr  [19], most  optimizations  rely  on 
information about  how  predicates  will be  called  during  runtime. 

Abstract  interpretation  as originally proposed  by Cousot  and  Cousot  [6] pro- 
vides  a  mathematical  framework  for  proving  an  analysis  correct  with  respect to  a  programming  language's  operational  semantics.  Also,  a  formal framework 

m ay  serve  as  a  basis  for  describing  and  understanding  various fixpoint  comput- i n g   strategies,  for  instance  the  chaotic  iteration  strategies  discussed  in  [8]. An 

abstract  interpretation-based  approach  for  the  analysis  of constraint  logic  pro- 
grams  is  presented  by  Marriott  and  SOndergaard  [18]. This  work  suggests  that 
the  analysis of Prolog  can  be  given  a  very  simple  formulation within  a  general 

CL P  framework,  an  approach  followed in  the  present  paper. 

An  algorithm for an  analysis problem in the  context of imperative languages 
was proposed  by Kildall [17]. This algorithm employs a  static  dependency  relation to guide the fixpoint iteration.  A  similar idea is employed in O'Keefe's algorithm 

[15],  which  was  shown  to  apply  to  the  Prolog  analysis  problem  considered  by 

28 
Mellish,  and  in  Debray's  algorithm for a  more refined variant of mode analysis 
for Prolog  [9]. 

The  top-down analysis considered in the  present  paper  maintains a  distinc- 
tion  between  different  ways in  which  a  predicate  is  called.  The  basic  analysis 
problem is similar to the one solved by the algorithm given by Le Charlier  et  al. 

[4]. The  top-down algorithm we propose  employs a  variant of the idea proposed 
in  [4]  of guiding the fixpoint iteration  by  a  dynamic  dependency  relation.  That 

is,  the  choice  of what  to  compute  (which  clause  and  call  pattern  to  evaluate) in  an  iteration is  controlled by  a  data  structure  which is  modified dynamically 

during the fixpoint iteration. 

The  algorithm  of  [4]  has  been  shown  to  be  optimal  for  certain  classes  of 
programs,  as well  as to  perform very well in practice  [5]. The  algorithm's abso- 
lute  worst-case  complexity has  been  analyzed in  terms  of the  total  number  of 
times  that  a  clause  in  the  given program  is  entered  for  dataflow analysis with respect  to  a  call  pattern.  This  number  is  shown  in  [4]  to  be  of the  order  of 

DomSize 2 * DomHeight  *  Tree 2, where  DomSize  is the maximum size of the  do- 
main of patterns  for a  predicate,  DomHeight  is  the  height of that  domain, and 

Tree  is  the  size  of the  so-called  static  call  tree  for the  given program  (that  size in  turn  being of the  order  of the  number 

Atoms  of atoms in the  program).  The 
corresponding complexity measure for the  top-down algorithm presented  in the present  paper  is of the  order 

l)omSize  fi9 DomHeight  * Atoms.  Thus,  the  worst- 
case  bound of [4] is improved by two orders of magnitude. Our complexity result 

also implies that  the  number of times that  each  clause is subjected  to  a  pass  of 
dataflow analysis is independent  of the size of the  rest  of the  program. 

Related  work  includes  the  frontiers  algorithm [22], originally developed  for 
the analysis of functional languages. The algorithm was designed for the compu- 
tation  of a  function with  a  binary  codomain (say,  {3_, T}).  In  a  logic program- ming  context  we  would  like  to  operate  with  a  multitude  of success  patterns, 

and  indeed subsequent  research  [20] has extended the basic idea in the frontiers algorithm so that  an  arbitrary finite codomain is allowed. 

In a comparative study of a number of techniques for the evaluation of queries 
to deductive databases, Bancilhon and Ramakrishnan [2] give particular credit to 
the method of semi-naive or differential evaluation, a generalized variant of which 

is  analyzed in  [1]. If used  in  combination with  a  program transformation based 
on  magic sets  [3], the  method  may  produce  information about  how predicates 

are  called in  a  logic  program. 

The  main parts  of the  paper  are  sections  4  and  5.  Section  4  introduces  the 
notion  of  a  static  dependency  relation,  and  defines  an  algorithm  which  uses 
such  a  relation in the  computation of an abstract  bottom-up semantics. Section 
5  defines  a  dynamic dependency  relation,  and  employs it  in  an  algorithm for the  computation  of  an  abstract  top-down  semantics.  The  abstract  top-down 

semantics of a  program may express  more information about  the  program than 
the  bottom-up semantics; the relevant fixpoint is more difficult to compute, and 
emphasis in the presentation is on explaining why and how dynamic dependency is useful for solving the  more complex fixpoint problem. 

29 
2  Lattices  and  fixpoints 
This  section  recapitulates  some  definitions  and  results  from  lattice  theory. 

A  complete  lattice  is  a  set  D  equipped  with  a  partial  ordering  relation  E_ 
with  the  property  that  for any subset  S  C_ D  the  least  upper  bound  US  and  the 
greatest  lower  bound  lqS  exist.  We  write  3-  for  fq D  when  D  is  given  from  the 

context.  If D  is  such  that  lq D  does  not  exist  we  sometimes  add  I  to  D  as  an 

"artificial"  bottom  element.  Dually for  t3 D  and  T.  The  size of D  is  the  number 
of elements  in  D.  A  subset  S  C  D  is  a  chain  if  d  r-  d'  or  d'  r--  d  holds  for  all 

d, d'  E  S.  The  height  of D  is the  maximum size  of any  chain  in  D. 

Suppose  D  and  E  are  complete  lattices.  The  Cartesian  product  D  x  E,  the 
set  D*  of finite  sequences  of elements  of D,  and  the  function  space  D  ---* E  (of 

monotonic and nonmonotonic functions)  are all assumed to be ordered pointwise. 

Under  these  ordering  relations  the  composite  lattices  are  complete. 

The  power  set  of D  is  written  P  D. 
All  sets  of syntactic  objects  (programs,  clauses,  etc.)  are  assumed  to  be  flat 
domains,  that  is,  ordered  by  the  identity  relation.  If  D  is  a  flat  domain  and  E 
is  a  complete  lattice,  then  the  function  space  D  --+  E  (ordered  pointwise)  is  a 

complete  lattice;  and  any  function  F  :  D  --~  E  is  monotonic,  i.e.,  d  E_  d'  :=r 

(F   d)  _  (F  d')  holds for  all  d,  d'  E  D. 

A  fixpoint  of  a  function  F  :  D  ---+ D  is  an  element  x  E  D  which  satisfies 
F  x  =  x.  Let  S  C  D  be  the  set  of fixpoints  of  F.  If  n  S  exists  and  is  itself  a 
fixpoint  of F,  then  it  is  called  the  least  fixpoint  and  denoted  lfp  F. 

The  following  theorem  gives  sufficient  criteria  for  the  existence  of  the  least 
fixpoint  of a  function.  It  follows from  Tarski's  theorem  [23]: 

T h e o r e m   2.1  Let  D  be  a  complete  lattice,  and  let  F  :  D  ---+ D  be  monotonic. 

Then  lfp  F  exists. 

The first  of the following two propositions gives sufficient criteria for the exis- 
tence of an algorithm to compute the least fixpoint of a function. The proposition 
is  expressed  using  the  following notation:  Let  a  function  F  :  D  ---* D,  where  D 

is  a  complete  lattice,  be  given.  Then  F  T n  denotes  the  element  of  D  defined 
inductively  by: 

FTO  =  3_ 
F T n   =  F  ( F T ( n -   1))  for  n  E  { 1,2,...}  

P r o p o s i t i o n   2.2  Let  D  be  a  complete  lattice  having  finite  height,  and  let  F  : 

D  --+  D  be  monotonic.  Then  there  exists  n  E  .M  such  that  lfp  F  =  F  T n,  and  n 
is  at  most  the  height  of D. 

The  sequence  F  T 0 , . . . ,   F  I  n , . . .   is  called  the  ascending  Kleene  sequence  for  F. 

The  second  proposition  is  used  in  proving  the  fixpoint  algorithms  correct. 
Both  parts  of the  proposition  are  easily  verified. 

30 
P r o p o s i t i o n   2.3  Let  D  be  a complete  lattice,  and  let F  : D  ~  D  be monotonic. 

(i)  Suppose  the  sequence  z ~   z n , . . ,   of elements  of D  is  given  by: 

z  ~  =.J_ x" =  z "-i u  (F.  x "-i) 

where  Fn  : D  --~ D  and  Fr~ ~  F  holds for  all n.  Then  x n  E  lip F  holds for  all n. (ii)  Suppose  that  x  E  D  is such  that  F  z  E_ x.  Then  lfp F  E  x. 

3  A n   e x a m p l e :   g r o u n d n e s s   a n a l y s i s   o f   P r o l o g   p r o g r a m s  
The  example  language  considered  in  the  paper  is  a  variant  of Prolog.  The  lan- 
guage is defined as a simple constraint logic language, to simplify the formulation 

of abstract  semantics  for  the  language,  and  to  indicate  that  semantics  and  al- 
gorithms  may  be  generalized  so  as  to  apply  to  a  broad  class  of CLP-languages 

[10]. 

Pred  is a  collection of predicate  symbols p, g , . . .   of arity ~_ 0.  Atom  contains atoms  of the  form p ( T 1 . . .   Tn),  where  p  E  Pred  and  Ti  E  Term  is  a  term  built 

in  the  usual  way from function symbols f ,  g , . . .   of arity >_ 0,  constants  a, b , . . . ,  and  variables  X ,   Y , . . .   E  Var.  The  function  vats  :  Term  --,  P  Vat  returns 
the  set  of variables  occuring  in  a  term.  Con  contains  all  equations  of the  form 

A  =  A', where  A, A'  E  Atom,  and the trivial constraint  true.  Con  is closed under 
conjunction. 

A  constraint  ~r  E  Con  is  satisfiable  if it  has  a  solution.  A  solution  ~  to 
is  a  ground  substitution  (see  [16]) such  that  the  constraint  0~r  obtained  from applying 0  to  lr is true.  Constraint  O(A =  A ~) is true  if OA and  0A '  are  identical. 

Constraint  0(~- A ~f)  is true  if 0~r and  0~r ~ are  both  true. 

A  program  P  E  Progr  is  a  collection  of clauses.  A  clause  C  E  Clause  has 
the  form H  : -   ~r, B,  where  H  E  Atom,  ~r E  Con,  and  B  E Atom*.  (The  demand that  constraints  precede  atoms  in  a  clause  body  is  made  merely to  simplify the 

presentation). 

In the sequel we will refer to the standard top-down and bottom-up semantics of a  program.  We  indicate how  these  notions  can  be  defined: 

The  standard  top-down  semantics  can  be  defined  in  terms  of  query  rewrit- 
ing.  Query  (A :: G, ~r)  is  rewritten,  using  renamed  clause  H  : -   ~r', B,  into  query 

(B :: G, (Tr A (A  =  B)  A ~r')),  if constraint  r  A (A  =  B)  ^  ~r'  is  satisfiable.  The standard  top-down semantics maps the  initial query  (A, ~r) to the  set  of all pairs 

(A, ~rans) such  that  query  (nil, zca,,s) can  be  obtained  by  query  rewriting  from the  initial  query. 

The  standard  bottom-up  semantics of a program is the set of all pairs  (A, ~rans) 
associated  by  the  standard  top-down  semantics  to  some  initial  query.  The  set will be  referred  to  as  the 

success  set  associated  with  the  program. 
Note  that  the notions of top-down and bottom-up semantics are unrelated  to 
the  "direction" of the fixpoint computation, which is bottom-up in all algorithms discussed  in  the  paper. 

31 
The  algorithms and  abstract  semantics  defined  in  subsequent  sections  are 
generic in the sense that they accept  a collection of externally given abstract  do- 
mains and operators.  As an example type of analysis we define  a simple ground- 

ness  analysis.  A  term  T  is  said  to  be  ground  if  (vats  T)   =  0.  Constraint  7r 
grounds  term  T  if for  all solutions 0 to  7r we  have that  the  term  01r is ground. 

For instance, constraint p ( X ,   Y)   =  p(a,  Z)  grounds X  but neither  Y nor Z. The 
groundness analysis is  defined in terms of two abstract  domains (both  complete lattices)  and four abstract  operators  (all monotonic). 

The  first  abstract  domain  is  Pat  which  consists  of  a  number  of so-called patterns for each predicate 

p  E  Pred.  Each argument to a predicate is described 
by one of the modes G (for ground)  and A (for any). 

D efi n it i o n  3.1  For  a  given  predicate  p  E  Pred  with  arity  k,  the  set  Patp  is given by 

Patp  =  { p ( m l . . . m k )   l mi  e  {G,A}} 
The set  Pat  of all patterns  is given by  Pat  =  (IJpePred  Patp)  U {_1_, T}. 
The ordering relation E  on  Pat  is  defined by 

I p ( m l   . . . m k )   E_p'(m~  . . . m ~ )   r  (p  =  p'  and  ( V i : m , - G o r m , = A ) )  

[] 

The  second  abstract  domain is  Acon.  An  abstract  constraint  1I  E  Acon  is  a finite set  of variables. 

Defi ni t io n 3.2  The  set  Acon  of  abstract  constraint  is  given  by  Acon  = 

{11  E  7 )  Vat  I I I   is finite} t.J {_l_} and  is ordered  by the superset  relation.  [] 

Note that  Acon  has top  element 0. The four abstract  operators  are  defined as follows. 

D ef in it i on  3.3  The  four abstract  operators  have functionalities 

add  :  Con  --~ Acon  ~  Acon 
meet  : Acon  ~  Acon  --+ Acon 
project  : A t o m   --~  Acon  ---* Pat 

generate  : A t o m   --+ Pat  --* Acon 

and  are  defined by the  equations: 

add  7r  W  =  W t2 { V  [ 7r grounds  V} 
meet  W  W '   =  W  t.J  W '  
project  p( T1.  ..Tk)  W  =  p ( m l .  . .ink) 

~G a  i f ( v a t s   Ti)  C_  W where mi =  otherwise k n  

generate  p( T1.  ..Tk)  p ( m l .  . .mk)  =  I421 U  ..  . O  Wk 

where  W,  =  (vo ars  Ti  if  m'  =  G otherwise 

with  meet  _1_ W  =  meet  W  .k  =  add  r  _1_ =  generate  p( T1  ...  Tn )  _k =  .1_ 

and project  p( T1  ...  Tn)  _1_ =  _k  [] 

32 
Example 4.2 below indicates how the operators  can be  combined in the analysis 
of a  clause.  A  constant  v  E  Acon  is  used  to  describe  the  constraint  true.  r  is 
given by r  =  generate  q q =  0,  with  q being  any zero-ary predicate. 

A function pred  return  predicate symbols of atoms, patterns,  and clauses: 

(pred A)  -  p  ifA =  p( T1 ...  Tk)  (A  e  Atoms) 
(pred  A)  =  p  if z~ =  p ( m l . . ,   m~)  ( A  e  Pat) 
(pred  C)  =  p  if C  =  ( p ( T 1 . . .   Tk)  :-- 7r, B  )  (C  e  Clause) 

In the  complexity analysis the  following notation is  used: 
Atomsc:  the number of atoms in clause  C,  couting the head as well as the body atoms. 

DomSizev  : the size of Patp  where p  =  (pred C)  is the predicate defined by clause 

C. Domtteightc:  the height of the largest set  Patp U {A_} for any predicate p  called 

in  the body of C. 

any predicate p  defined in P.  since there are four elements in any of the sets 

4  S t a t i c   d e p e n d e n c y   a n d   b o t t o m - u p   a n a l y s i s  
The  bottom-up  algorithm  defined  and  analyzed  in  this  section  computes  the abstract  bottom-up  semantics  Pbu  [P]  of  a  given  program  P.  The  semantics 

Pbu  is  defined formally, and  the  algorithm is proved correct  with respect  to the semantics.  The  simple  nature  of the  bottom-up  semantics  allows  for  a  chaotic 
iteration strategy based  on  a static  dependency relation. 

The abstract bottom-up semantics associates a single pattern with each pred- 
icate. 

Definition  4.1  The  semantic  domain  Sembu  is  defined  by  Sembu  =  Pred 

Pat.  D 

The  intuitive  idea  is  that  the  pattern  associated  with  a  predicate  p  is  a 
success  pattern.  That  is,  the purpose  of the  abstract  bottom-up semantics is to 
provide an  approximation of the  standard bottom-up semantics in the following 
sense:  Let  S  E  Sembu  be the  abstract  bottom-up semantics of a given program, 

and  suppose  that  S  p  =  p(ml  ...  ink)  for  some predicate  p.  Then  for  any pair 

( p ( T 1 . . .   T~), ~r)) belonging to the success  set  of the  program it  must hold that 
constraint r  grounds the  i'th argument  T~ whenever m~ =  G. A formal definition and  proof of this property  of Pbu  is outside the  scope  of the paper. 

Definition  4.2  The  abstract  bottom-up semantics Pbu  is  defined as follows: 
The  semantic functions have functionalities: 

Pbu  : Progr  ---+ Sembu 

Cbu  : Clause  ---* Semb~  ~  Sembu 
Bbu  : Atom*  ~  ,-,r  ---+ Acon  -..* Acon 
Abu  : Atom  ~  $embu  "--* Acon  --* Acou 

33 
and  are  defined by: 

Pbu  [P]  =  lfp (U c eP   (Cbu  [C])) Cbu  [H  : -   r,  B] S p  =  if (pred H) ~- p then *  else 

let 11 =  Bbu  [B] S  (add ~r r)  in  project H II (nil] S  11 
[A :: B] 5'/7 [A] s  11 Bbu Bbu 
Abu [] 

Pbu  is  well-defined, (Cbu [c]) 

=11 =  let 11' =  Abu [A] S 11  in  Bbu  [B] S  11' 
=  le t // '   =  generate A  (S (pred A))  in  meet 11 II' 

due  to  completeness  of  Sem bu  and  monotonicity  of 
E x a m p l e   4.1  Consider the  following simple  Prolog  program which will be  re- ferred to  as  Pex. 

(C1)  p(a,Y):-  true. (C-2)  p(x,Y) :-  true,  p(Y,X). 
The  abstract  bottom-up  semantics of the program is given by 

Pbu  [P,~] P =  p(A, A) Pbu  [P~x]] q =  _L  if q #  p 

[] 
Stated  in terms of the standard  bottom-up semantics, the  abstract  semantics of program  P ~   expresses  that  only  queries  to  predicate  p  may possibly  succeed, 

and  that  none of the  arguments to  p  has  been  established to  be  always ground. 
E x a m p l e   4.2  Consider the  evaluation of the expression  Cbu [C2] S p,  where S satisfies  S  p  =  p(G, A).  From  the  definition of clause  C~  (in  example 4.1)  it  is 

intuitively clear that  with  the  given S,  the  clause grounds the second  argument to  p.  That  is,  one  would  expect  the  value  of the  expression  to  be  p(A, G).  To 
evaluate the  expression  we note that 

add true r  =  O  (i) generatep(Y,X)p(G,A)  =  {Y}  (ii) 

meet 0 { Y}  =  { Y}  (iii) project p(X,  Y) { Y} -- p(A, G) (iv) 

and  obtain: 

Cbu  [p(X,  Y ) : -   true, p(Y, X)] S p =  l e t / / =   Bbu  [p(Y, X)] S  (add true r)  in 

project 11 p(X,  Y) =  l e t / / =   Abu [p( Y, X)] S O in 
project p(X,  Y)  II =  let 11' =  generate p(Y, X) p(G, A)  in 
let 11 =  meet $11'  in  project p(X,  Y) 1-1 

(by def of Cbu) 
(by  (i)  and  def of Bbu) 

(by def of Abu) 
l e t / / =   meet 0 { Y}  in  project p(X,  Y) 11 (by  (ii)) project p(X,  Y) { Y}  (by  (iii)) 
p(A, G)  (by  (iv)) 

34 
Now we turn to the  computation of Pbu [P]  for a  given input program P.  A 
brute-force  approach  is suggested  by proposition  2.2.  One may simply compute 

the  ascending Kleene sequence for [[cep  (Cbu [C])  given by 

S O =  Ap.* s"  =  (cb.  [cl) s 

where  S"  6  Semb,~.  At  the  point  where  S n+l  :  Sn  we  have  S  n  = l#  (Cb   [Cl)). 

The  notion of  chaotic  iteration  was  introduced  by  Cousot  and  Cousot  [7] 
[8]. Chaotic iteration may be  applied in the  present  context to yield the follow- 
ing improvement over  the  brute-force method: Immediately upon  evaluation of 

Cbu [C] S  p,  the  result  is  "added"  to  the  iterand  S.  By contrast,  in  the  brute- force method, all clauses in the program axe scanned  once using a  fixed iterand 

before the  iterand is modified. Chaotic iteration yields a  sequence 

S O =  Ap.* 
S"  =  S  u  (Cbu  [C.!  S 

for some sequence of clauses  C1, fi9 fi9  C., ....  Several  methods exist to secure  that 

a  chaotic iteration sequence  eventually stabilizes  at  U c e p   (Cbu  [CI).  Perhaps the most simple is to select  clauses according to their order in the  program, and 

stop  when all clauses in  P  has been  tried and  seen  not to  change the iterand. 

The bottom-up algorithm can be seen  as choosing a chaotic iteration strategy 
based  on  a  static  dependency relation. The  strategy prescribes  that  if the  n'th iteration  leads  to  an  updaie  of the  pattern  for  predicate  p,  i.e.,  if (S n-1  p)  F- 

(S n  p),  then  the  clauses  that  depend  on  p  should  be  selected  for  evaluation. 
Clause  C  depends on predicate p  if for some atom  Bi  in the body of C  we have @red  &)  =  p. 

The observation underlying this approach is that if clause C  does not depend 
on  the  updated  predicate  p,  then  there  is no point  in  (re)evaluating  C.  This  is 
expressed  in the following proposition, which is easily proved  by induction over the  number of atoms in the  body of a  given clause. 

P r o p o s i t i o n  4.1  Let  C  be the  clause  (H  : -   ~r, B1...  Bin),  with (pred B,)  =  p,. Let  S ,S '  6  Semb~  salisfy 

S'  =  Cbu  [C']  S  for  some  clause  C'.  Then  (Vi  6 [1, m]:  S'  p,  =  S  p,)  =~ Cb~  [C]  S'  = 

Cbu IC] S. 

The  relation between  predicates  and the  clauses  they depend  on is  static in the sense that the relation can be  computed once and for all prior to the fixpoint 

iteration.  The  dependency information can  be  obtained on  a  syntactic basis  in the  sense  that  only  the  predicate  symbols occuring  in  a  clause  body  need  be 
examined,  while it  is  unnecessary  to  consider for  instance  the  definition of the abstract  operators. 

The  bottom-up algorithm is defined as follows in pseudo-code. 

35 
Input:  Program  P. Output:  The  value of S  upon  termination  of the  algorithm  is  Pbu [P]. 

%~  initialization workset :=  all clauses  in  P 
for  all predicates  p  defined in  program  P  do S(p)  :=  * 

depend(p)=  {C  E P [ p i s   cMled in  body of C}  od ~  iteration 
while  workset ~ 0 do select  and  delete  clause  C  from workset 

let  ( H : - T r ,  B ) =   C  and  p = ( p r e d H)  A  :=  Cbu [C]  S p 
if  A [~ S(p) then S(p) 

:=  S(p) mm A workset :=  workset 

U depend(p) fl o d 

The  definition  is  in  terms  of the  following variables  ranging  over  composite 
domains: 

The  variable  S  6  Sem b~  represents  the  iterands.  Initially,  S  has  the  bottom value  Ap._l_. Upon  termination,  S  holds  the  algorithm output  Pbu  [P]. 

The  variable  workset  E  (7)  Clause)  holds  the  set  of  clauses  that  must  be 
scanned  in  the  following iterations.  Initially,  workset  contains  all  clauses  in  the 
given program.  The  algorithm  terminates  when  workset  is  empty. 

The  variable  depend  E  (Pred  --~ 7 9 Clause)  represents  the  dependency  infor- 
mation.  For  a  given predicate  q  defined in  P,  clauses  C  belongs  to  depend(q)  if 
the  clause  contains  a  call  to  q in  the  body. 

The  variables  ranging  over  simple  domains  are  P  E  Progr,  C  E  Clause, 
p  6  Pred,  and  A  E  P a t  

The  questions  of what  data  structures  should  be  used  to  represent  the  vari- 
ables,  and  how  the  various  operations  on  them  should  be  implemented,  are  not 
considered.  Of course,  the  values of S  q  and  depend(q)  are  irrelevant  if  q  is  not 
defined  by  a  clause  in  the  input  program. 

In  the following analysis of the  algorithm,  an  iteration  refers to  an  execution 
of the  while-loop. S"  and  workset n  denote the  values of S  and  workset upon  the 

n'th  iteration  (say,  at  the  point  of the  test  workset :~ @). Note  that  S '*-1  E_ S" 
always  holds,  since  S  is  only updated  "upwards".  We  refer  to  the  evaluation  of 
an  expression  Ctd  [C]  S  p  (for given  C,  S,  and  p  =  (pred  C))  as  a  clause  scan. 

E x a m p l e   4.3  Consider  program  Pe,  from example 4.1  as  input  to the  bottom- up  algorithm.  It  is  assumed  that  clauses  are  added  to  the  workset  according  to 
their  order  in  the  program,  and  deleted  from  the  workset  on  a  first-in  first-out 
basis. 

At  initialization,  the  variable  depend  is  assigned  the  function 

depend(p)  =  { C2} depend(q)  =  @  if  q  :p 

36 
while  S  and  workset  are  initialized as  shown  in  the  top  row  of the  table:  The 
bottom-up  algorithm performs three  iterations: 

n  Clause scan 

2 Cbu [C2] S' p =  p(A, G) 
3 Cb. [C2]] S 2 p =  p(A, A) 

S ~ 
p -~  p(G, A) 
p --, p(A, A) 

workset n {c,, 

A  field with (i)  has the  same value as the field above it.  The value p  ~  p(G, A) 
of  the  field  for  S 1 stands  for  the  function  that  maps  p  to  p(G, A)  and  other 
predicates  to  _L. 

In  the  first  iteration,  clause  C1  is selected.  Evaluation of C  [C1]  S o p  leads 
to  an  update  of p,  and  so  {C2} is  added  to  workset,  which  however  already contains  the  clause.  The  second  iteration  leads  to  another  update  of  p;  this 

iteration  corresponds  to  the  computation  detailed  in  example  4.2.  The  suc- cess  pattern  p(A, A)  =  (S 2 p)  is  obtained  as  the  least  upper  bound  of the  set 

{p(G, A), p(A, G)}; the  analysis misses out  the  information that  either  the  first 
or the second argument is ground, because each predicate is described by only a single success  pattern.  The third  iteration is the last  one since  it  leaves 

workset 
empty.  [] 

Correctness  of the  bottom-up  algorithm is  defined  with  respect  to  the  ab- 
stract  bottom-up  semantics:  For  an  arbitrary  program  P,  the  output  of  the 

algorithm as  executed with input  P  must be  equal to  Pbu [P]- 

P r o p o s i t i o n  4.2  The  boitom-up  algorithm  is  correct. 

The  complexity  of  the  bottom-up  algorithm  is  expressed  in  terms  of  the 
number of scans of each  clause in a  given program, i.e., the number of passes  of 

dataflow over each  clause. 

P r o p o s i t i o n 4 . 3   The  bottom-up  algorithm  performs  at  most  Atomsv  * 

DomHeightv  scans  of  any  clause  C  in  any  program  on  which  the  algorithm 
is  run. 

The bottom-up algorithm's iteration strategy, being based on a static depen- 
dency relation, is similar to the strategies underlying the algorithms proposed by 
for instance Kildall [17], O'Keefe [15], and Debray [9]. Significantly, by the above complexity result,  the number of passes  of dataflow analysis over  each clause  in 

a  program is independent of the  remainder of the  program, i.e., of any measure 
on the size  of the rest  of the program. This is the property that  we would like to carry over to the more difficult problem which arises when a  distinction is made 

between  different calls  to  the same predicate. 

37 
5  Dynamic dependency and  top-down  analysis 
The  top-down algorithm defined and  analyzed in this section  computes the  ab- 
stract top-down semantics Ptd [P]  of a given program P. The abstract top-down 
semantics of a  program  may express  more information about  the  program than 

the  abstract  bottom-up  semantics.  On  the  other  hand,  it  is  more  difficult  to compute. The top-down algorithm copes  with the more difficult problem by em- 

ploying a  chaotic iteration strategy which can be said to be  based on a  dynamic dependency relation. 

Definitio n  5.1  The semantic domain Semtd  is defined by Semtd  =  Pat  -~  Pat. 1"3 

The  abstract  top-down  semantics  is  intended  to  approximate  the  standard 
top-down semantics in the following sense:  Let  F  E  Semtd  be  the  abstract  top- 

down  semantics  of  a  program,  let  p ( m l . . ,   mk)  be  an  arbitrary  pattern,  and 
let  (p(T1  ..  Tk), 7r)  be  an  arbitrary initial query to  the  program.  Suppose  that 

constraint ~r grounds  Ti whenever mi  =  G. Then for all pairs  (p( T~ ,..  T~), ~rans) 
associated by the standard top-down semantics to the initial query it must hold 

'  G, with p ( m ~ . . . m ~ )   =  F  p(ml  .ink). that  tans  grounds  T~ whenever  m i =  .. 

Defi ni ti on  5.2  The  top-down semantics Ptd  is  defined as follows: 
The  semantic functions have functionalities: 

Ptd  : Progr  ---* Semtd 

Ctd  :  Clause  --+ Semtd  ---+ Semtd B t d :   Atom* 

---+ Semtd  --+ Acon  ---+ Acon 
Atd  : At om  ---+ Semtd  ---+ Acon  ---+ Acon 

and  are  defined by: 

Ptd  [P]  =  lfp ( U c e P   (Ctd[C~)) Ctd  [H  : -   7r, B[  F  z~ :  l e t / / =   generate H  A  in 

let 11 ~ =  add  7r II in 

let H"   =  Btd [B]  F / / i   in  project  H  II" 
Btd  [nil]  F  11  =  II 
B t d [ A : : B ] F I I   = B t d [ B [ F ( A t d [ A ] F H )  Atd  [A] F  H  =  let A =  project  A  1I in 

let I P  =  generate  A  ( F  A )   in  meet  II  II ~ [] 

Ptd  is  well-defined,  due  to  completeness  of  Semtd  and  monotonicity  of 

Uce p   (Ctd  [C])  (cf. theorem 2.1). 

E x a m p l e   5.1  The  abstract  top-down  semantics  of program  P~z  (see  example 
4 .1 )  is given by 

Ptd  [Rex]  p(G, G) Ptd 

[P,x] p(G, A) 
Ptd  [Pex] p(A, G) Ptd 

[Per] p(A, A) 
[Po  ]  q(m    ...  m )   = 

=  p(G,G) 

=  p(G,J) 
=  p(A,G) =  p(A,A) 

* 
[] 

if  q#-p 

38 
The  difference  between  the  abstract  top-down  and  bottom-up  semantics  is that  the  former  takes  call  patterns  into  account:  First,  when  a  clause  is en- 
tered,  an  abstract  constraint  is  "generated"  which may possibly ground  certain variables  occuring  in  the  clause  head,  due  to  the  calling  pattern  for  the  clause. 

Second,  during  scanning  of a  clause, when  a  body atom is to be evaluated, a  call pattern  for  the  atom  is  computed  by  "projecting"  the  information  contained  in 
the  abstract  constraint  onto  the  atom.  The  two semantics  are  otherwise  closely 

related.  In fact, if the top-down version had operated  on a  domain which allowed 
only for  a  single  call  pattern  per  predicate,  the  two semantics  would have  been 
equivalent. To  compute  Ptd  [P]  for  a  given  program  P,  one  may  proceed  in  a  brute- 

force  manner,  constructing  the  ascending  Kleene  sequence  for  the  function 

To  improve efficiency,  one  may  use  some form  of chaotic  iteration.  We first define  the  content  of an  iteration,  and  then  consider the  over-all iteration  strat- 
egy. An  iteration  computes  iterand  F n  E  Semtd  from  the  previous  one  in  three 

steps: 

Step  one:  A  clause  scan.  That  is,  the  evaluation  of  the  expression Ctd [Cn]  F n-t  An,  with  (Cn,  An)  being  the  pair  chosen  for  the  n'th  iteration. 
Step  two:  The  result  of the  clause  scan  is  added  to  the  iterand:  F  =  F =-1  U S(Cn,  F n-l,  An)  with 

S(C, F, ,5) =  AA'.(if A' #  A then  *  else  Ctd [C] f  .4) 
Step  three:  Monotonicity  of the  iterand  is  preserved.  By  steps  one  and  two,  at most a single pattern  Aupd will have been updated,  i.e., (Fn-aA~pd)  E  (FA~pd); 

and  there  may exist patterns  Al where Aupd  E  A  and  (FAupd) fl: (FA).  Working 
with monotonic iterands  is intended  to speed  up convergence by assigning  (F A) the  larger  value  (F  ,5) U (F  Aup4).  Step  three  may be  expressed  in  terms  of the 

function  M  defined  by  M ( F )   =  AA.(U {F A'  [ A'  E  ,5}). 

Chaotic  iteration  based  on  steps  one through  three  yields a  sequence 

F ~  =  AA._I_ F n  =  M ( F  n-1  U  S(Cn, F n-l,  A,)) 

for  a  sequence  of pairs/Cn,  An).  The  problem  then  is  to  find  a  good  strategy 
for selecting  the  sequence  of pairs.  That  is,  a  strategy  which ensures  quick con- vergence  to  the  least  fixpoint,  and  in  which  the  determination  of the  sequence 

of pairs  is itself not  too expensive.  Of course,  we should  only select  pairs  (C, A) where (pred C) =  (predA), since otherwise we trivially have that  Ctd [C]FA  =  * 

(for  all  F). 

A  straightforward  generalization  of the  method  underlying  the  bottom-up algorithm  of section  4  is to  define a  dependency  relation  as follows: 

39 
The  pair  (C,  A)  depends  on  A,pd  if  there  is  an  atom  B~  in  the  body  of C  such  that  (pred  Bi)  =  (pred  Aupd),  with  Zi  being  any  pattern  satisfying 
(pred  Zi)  =  (pred  C).  As  an  example,  consider  clause  C2  in  program  P,=  from example 4.1: 

(62)  p(X,Y):-  true,  p(Y,X). 
This dependency relation would lead to the following: If any pattern  for predicate 
p  is updated,  then  all four  tuples  (C~,p(G, G)),  (C2,p(G, A)),  (C2, p(A, G)),  and ((72, p(A, A))  should  be  reprocessed. 

However, this  approach  seems  to  be  too  coarse.  It  appears  to  be  possible to achieve a more efficient analysis of program  Pex by relying on a more fine-grained 
dependency  relation  where 

(C2, p(G, A))  depends  on  (p(A, G)) (C2,p(A, G)) depends  on  (p(G, A)) 

etc. 
Defining  a  more  fine-grained  dependency  relation  leads  to  the  consideration  of a  dynamic relation,  that  is,  one  which  is modified  during  the  fixpoint  iteration. 

As  an  example,  consider  the  following program: 

(CI)  p(a,Y) :- true. ((72) 

p(X,Y):-true,  p(Y,X),  q(Y,X). 
(C3)  q(X,Y) :-  ... 

Suppose  an  iterand  F"  in  a  chaotic  sequence  for  the  program  satisfies 

F " p(A, A)  =  p(G, A).  If we in the  n +  l'st  iteration  evaluate  Ctd [C2] F " p(A, A), the  call  pattern  q(G,A)  will  be  computed  for  q ( Y , X ) .   Thus,  we  are  led  to 

consider  (C2,p(A,A))  as  depending  on  q(G,A).  Now,  suppose  a  subsequent iterand  F ~+d  satisfies  F "+a  p(A, A)  =  p(A, A).  That  would lead  us  to  consider 

(C2, p(A, A))  as depending  on  q(A, A). Thus  given the  above program  we obtain: 

(C2, p(A, A))  depends  on  (p(A, A)) and  (q(G, A))  upon  iteration  (n  +  1) (C2, p(A; A))  depends  on  (p(A, A)) and  (q(A, A))  upon  iteration  (n  +  d +  1) 

etc. 
These  considerations  motivate  the  definition  of a  dynamic  dependency  rela- tion,  where  (C, A)  depends  on  A'  if A'  has  been  computed  as a  call  pattern  for 

an  atom  in  the  body of  C,  during  the  most  recent  evaluation  of A  against  C. 

The  usefulness  of a  dynamic  dependency  relation  can  be  traced  to  the  def- inition  of  the  semantic  function  Atd  (in  definition  5.2  of  the  abstract  top- 
down  semantics).  To  evaluate  Atd  [A]  F  //,  the  function  F  must  be  applied to  A  =  (project A H),  where the  abstract  constraint  H  is the  result  of processing 
the part  of the  clause to the left of atom A, implying t h a t / / d e p e n d s   on F. Thus, the  value  of A  =  (project  A/7)  cannot  (at  a  reasonably  low cost)  be  predicted 
prior  to  the  fixpoint  iteration.  By  contrast,  definition  4.2  of the  bottom-up  se- mantics  prescribes  that  the  evaluation Of Abu  [A] S / 7   requires  the  application 
of S  to  the  syntactically  given entity  (pred A). 

40 
The  top-down  algorithm  is  specified  in  figure  1  in  pseudo-code.  The  main program  is  divided  into  an  initialization  and  an  iteration  part.  In  the  latter, 
procedures  update  and  scan  are  called.  Procedure  scan  implements  step  one. It  is  a  function  which  returns  the  value  of  an  expression  Ctd  [C 1 t? A  and  as 

a  side-effect  modifies  the  dynamic  dependency  information.  Procedure  update implements  steps  two and  three.  Monotonicity of the  iterand  is preserved  by re- 

cursively working upwards in the lattice of patterns  for the predicate in question. The  procedure  also takes  care  of adding  pairs  to  workset. 

The  variables ranging  over  composite domains  are: The  variable  F  E  Semtd  represents  the  iterand.  Initially,  F  is  assigned  the 
bottom value A,5.I.  Upon termination  F  is the least fixpoint of l i v e r   (Ctd [C]). The  variable  workset  E  7 9  (Clause  x  Pat)  holds  the  pairs  (C,,5 /  that  are 
waiting  to  be processed. The  variable  depend  e  (Pat  --* 79 (Clause  x  Pat  x Af))  maps each  pattern  ,5 

to  the  set  of tuples  (C, ,5~, i /  that  are  currently  depending  on  A.  The  index  i 
refers  to  an  atom  in  the  body  of  C,  namely  the  atom  for  which  call pattern  ,5 was  computed  during  a  previous scan  of  C  with  input  pattern  ,5~. 

The  variable  last_call  E  (Clause  x  Pat  x  Af)  ---, Pat  is continuously modified so  that  the  value'of  last_call(C,  ,5, i)  is  always the  call  pattern  which  has  been 
computed  for  the  i'th  body  atom  of  C,  during  the  most  recent  evaluation  of ,5 against  C.  Initially,  last_call  maps  all triples  t o / .   Upon each iteration,  last_call 
is  the  inverse  function  of  depend  (see  the  invariant  (1)  below).  The  variable 
serves  as  a  means  of deciding  in which points the  dependency  relation  should  be modified  during  a  clause  scan. 

The  variable fathers  E  (Pat  --* 79 Pat)  is assigned  a function which maps any 
pattern  A  to  the  set  of patterns  placed  immediately  above  ,5,  with  respect  to the  ordering  relation  on  Pat.  This  function  is  used  by  procedure  update  in  the 

preservation  of monotonicity of the  iterand. 

In  the  analysis  of the  algorithm,  an  iteration  refers  to  ma execution  of the while-loop controlled by the test  workset  4  0.  F",  depend n (C, A, i), etc., denote 
the  values of the  variables upon  the  n'th  iteration. It  can  be  shown  that  for all  input  programs  P  the  following properties  hold 
invariantly  for  all  n: 

(C, ,sdep,  i)  e  dependn ( ,5 ) r  A  =  last_call'*(  C, `sdep, i) 

last_call'-x(  C,  A,  i)  E_  last_call"(  C, ,5, i) 
F n -  l ( last_calln-  X ( C , ,5,  i ) )  E  Fn ( last-calln  ( C, ,5,  i ) ) 

(1), (2) 
(3) 
Furthermore,  we have  that  for  n  _  1 

(C,  A)  is added  to  workset n 
3i,  A~upd : (F n-1  Auvd)  C  (F"  Aupd)  and  (C,  A,  i)  e  dependsn(A~pd) 
r  3i:  F"'l(last_calln(C,  A,  i))  U  fn(last-call"(C,,5,  i))  (by inv  1)  (4) 

41 
Input:  Program  P. Output:  The  value  of F  upon  termination  of the  algorithm  is  Ptd  [P]. 

Y.g  initialization workset :=  {(C,  "d)  I C  e  P and (pred C) =  (pred "d)} 
for  all  relevant  "d  d o   F  :=  I  7. ,d  is  relevant  if  (pred ,4) 
f or  all  relevant  "d  do  depend('d)  =  ~ o d   ~. is  defined  in  P. 
f or  all  relevant  (C, "d, i)  do   last_call(C, ,d, i)  =  _L o d  

i  is  relevant  if 1 <  i  <  Atomsc. for  all  relevant  ,d  do  lathers(A)=  {A'  E Pat(predz~) I 'd  r - ' d '   o d  

Y,g  iteration while  workset ~. ~ d o  

select  and  delete  (C, Acall)  from  workset 

za~c,  :=  scan(G, F, A,an) if Asucc ft. f(Acatt)  t h e n   update('dcalt,  Asucc)  fl 

o d  
p r o c e d u r e   update( ,dcan, ,dsucc ) b e g i n  

for  all  (C,A,  i)  E  depend(Acalt)  do  workset :=  worksetO  {(C,  ,d)}  o d  

for  all  ,d  E fathers(,dcau)  do  if,dsucc  if: F(,d)  t h e n   update(,d,,dsucc)  fl  od  
e n d  

f u n c t i o n   scan(C, F, A call ) 
b e g in  

let  ( H  : -   r , Ba ...  Bin)  =  C 

H  :----- generate(H,  ,dealt ) 

H  :=  add(r, H) for  i : = l . . . m   d o  

Anew  := project(Bs,  H) 
H '   :=  generate(Bi,  F(Anew)) H  :=  meet(H, H') 

"dotd :=  last_call(C, A calt, i) last_call(C, "dcalh i)  :=  Anew 

depend( "dold) :=  depend( "dold)  \  { ( C, A cau, i)} depend('d,~) 

:=  depend('dn~)  U {(C, ,dean, i)} 
od  ,dsucc  := project( H,H ) 

r e t u r n   ,dsucc e n d  

modifying  the  dynamic 
dependency  relation 

P i g .   1.  The  top-down  algorithm  based  on  dynamic  dependency  information. 
Th e  variables  ranging  over  simple  domains  are  P  E  Progr,  C  ~  Clause, H,  B,  E  A~om,  7r E  Con,  1I  E  Acon,  A, A~an,...  E  Pat, 

and  i  E A/'. 

42 
Since  F n-1  ~  F n  we  have  that  if  (C,A)  ~  workset n  then  the  equation 

Fn-l(last_cal-fn( C, A,  i)) =  Fn(last_catln( C, A, i)) holds for all  i  <  Atomsc.  It 
follows that 

( c ,   a )   r  workset"  i t ]   F"  za =  Ctd i t ]   za  (5) 
Correctness  of the  top-down algorithm is defined with respect to the abstract 
top-down semantics:  For an  arbitrary  program  P,  the output  of the  algorithm  as 
executed  with  input  P  must be equal to Ptd [P]-  We assume that  the  algorithm implements  iteration  steps  one  through  three  as  defined  above. 

P r o p o s i t i o n   5.1  The  top-down  algorithm  is  correct. 

The  complexity of the  top-down  algorithm  is expressed  in  terms of the num- ber  of clause scans,  i.e.,  evaluations  of Ctd  ~C] F  A. 

P r o p o s i t i o n 5 . 2   The  top-down  algorithm  performs  at  most  Atomsc  . 

DomHeightc  fi9 DomSizec  scans  of  any  clause  C  in  any  program  on  which  the 

algorithm  is  run. 

Recall the  definition  of Atomsc  etc.  from the  end  of section 4.  Letting 

DomSizep  =  maxcep  DomSizec 
DomHeightp  =  maxcep  DomHeightc 
Atomsp  =  SceP Atomsc 

we  obtain  from  the  above  proposition  that  the  total  number  of  clause  scans during  anaysis of an arbitrary  program  P  is bound by DomSizep * DomHeightp fi9 

Atomse. 

6  D i s c u s s i o n  
An  efficent  algorithm  for top-down  or query  directed  analysis of logic programs has  been  defined  in  which  the  flxpoint  iteration  is  controlled  using  a  dynamic 

dependency  relation. The  complexity result  for  the  top-down  algorithm  is  promising  because  the 
number  of passes of dataflow analysis over each  clause is independent  of the  size 

of the  rest  of the  program.  Considering  all clauses in  a  given program,  the  total number  of clause  scans  was  shown  to  be  bound  by  the  following product:  The 

maximum size of the  domain of patterns  for a predicate,  times the  height  of that domain,  times  the  number  of atoms  in  the  given program.  Thus,  the  worst-case 

bound  of [4] is improved by two orders  of magnitude. In the groundness analysis that  has served as an example analysis, the number 
of patterns for a predicate is exponential in the arity of the predicate;  and indeed, most  practically  useful  analyses  will make  use  of domains  than  even  are  larger 

than  those  considered  here.  To  deal  with  this  kind  of combinatorial  explosion, 
it  may  be  useful  to  combine the  application  of a  dynamic  dependency  relation 
with  the  use of widening  and  narrowing  operators  [8]. 

43 
Another  useful  modification  of  the  top-down  algorithm  is  to  incorporate  a 
minimal function  graph  approach  [14],  as  discussed  in  [12].  Execution  of a  mini- 
mal  function  graph  (rafg)  version  of the  algorithm  begins  with  a  set  of patterns 
provided  by  the  programmer,  implying  a  constraint  on  how  the  program  m a y 
be  queried.  Clauses  are  then  only  scanned  for the  externally  given patterns  plus 

those whose evaluation is required  for the  evaluation of the  externally given ones. 
fi9 This  approach  m ay   reduce  the  average  cost  of the  algorithm.  In  addition,  infor- 

mation  m a y  be  provided  about  how  predicates  will  be  called  during  runtime, 
rather  then  merely  about  a  programs  input-output  behavior  as  in  the  (non-mfg) 

abstract  top-down  semantics. 

R e f e r e n c e s  

1.  I.  Balbin  and  K. Ramamohanarao.  A generalization  of the differential  approach  to recursive  query  evaluation.  J.  Logic  Programming 1987, 4,  pp.  259-262. 
2.  F.  Bancilhon  and  R.  Ramakrishnan.  An  Amateur's  Intorduction  to  Recursive Query  Processing Strategies. MCC  Technical  Report  No.  DB-091-86,  March  1986. 
3.  C.  Beeri  and  R.  Ramakrishnan.  On  the  power  of  magic.  J.  Logic  Programming 1991,  10,  pp.  225-299. 
4.  B.  Le Charlier,  K. Musumbu,  P. Van Hentenryck. A generic abstract interpretation algorithm  and  its complexity analysis.  Proc.  8th International Conference  on Logic 

Programming,  1991,  Paris,  pp.  64-78. 
5.  B.  Le  Charlier  and  P.  Van  Hentenryck.  Experimental  evaluation  of  a  generic  ab- stract interpretation algorithm for Prolog. Technical Report CS-91-55,  August  1991, 

Brown  University. 
6.  P.  Cousot  and  R. Cousot.  Abstract Interpretation:  a unified  lattice model for static analysis of programs by construction  or approximation  of fixpoints.  Proc. ~th A CM 

Symposium  on Principles  of Programming Languages,  1977, pp.  238-252,  . 
7.  P.  Cousot  and  R.  Cousot.  Automatic  synthesis  of  optimal  invariant  assertions: Mathematical  foundations.  Proc.  A CM  Symposium  on  Artificial Intelligence  and 

programming  languages,  SIGPLAN  Notices,  12  (8),1977,  pp.  1-12. 
8.  P.  Cousot  and  R.  Cousot.  Abstract  interpretation  and  application  to  logic  pro- grams.  J.  Logic  Programming 1992 13,  pp.  103-179. 

9.  S.  K.  Debray.  Static  inference  of modes  and  data  dependencies  in  logic  programs. ACM  Transactions  on  Programming  Languages  and  Systems,  11  (3),  1989,  pp. 

418-450. 

J.  Jaffar and  J.-L. Lassez.  Constraint logic programming. Technical Report, Monash University,  June  1986. 

J.  Jaffar,  S.  Michaylov,  P.  Stuckey  and  R.  Yap.  An  abstract  machine for CLP(~). SIGPLAN PLDI,  San  Francisco,  June  1992. 
N. Jergensen.  Abstract interpretation of constraint logic programs.  Ph.D.  thesis.  To appear  in  Datalogiske  skriIter,  1993,  Computer  Science  Dept.,  Rosldlde  University 
Center. 
N.  Jergensen,  K.  Marriott,  and  S.  Michaylov.  Some global compile-time optimiza- tions  of CLP(~).  Proc.  International Logic  Programming  Symposium,  San  Diego, 

1991,  pp.  420-434. 

10. 
11. 
12. 

13. 

44 
14.  N. D.  Jones and  A.  Mycroft. Data flow analysis of applicative programs using min- imal function  graphs.  Proc. 13th ACM Symposium  on  Principles  o] Programming 

Languages, Florida,  1986, pp.  296-306. 15.  R.A.  O'Keefe. Finite  fixed-point  problems.  Prac. $th International Conference on 

Logic Programming, 1987, Melbourne,  729-743. 16.  J.W.  Lloyd.  Foundations  o] logic programming. Springer-Verlag,  1984. 
17.  G.A.  Kildall.  A  unified  approach  to  global  program  optimization.  Proc. POPL 1973, Boston,  194-206. 

18.  K. Marriott and H. S~nderga~rd~  Analysis of constraint logic  programs. Proc. North American  Con]erence on Logic Programming, Austin,  1998,  pp.  521-540. 
19.  K.  Marriott  and  H.  Scnderga~rd.  Bottom-up  dataflow  analysis  of  normal  logic programs.  J.  Logic Programming 1992, 13,  pp.  181-204. 
20.  C.  Martin  and  C.  Hanking.  Finding  fixed  points  in  finite  lattices.  FPLCA  1987 Portland,  Ohio. 
21.  C.  S.  Mellish.  The automatic  generation o] mode declarations ]or Prolog programs. DAI Research  Paper  163, University  of Edinburgh,  1981. 

22.  S.  Peyton-Jones  and  C.  Clack.  Finding  fixpoints  in  abstract  interpretation,  in:  S. Abramsld  and  C.  Hankin  (eds)~ Abstract  interpretation  o] declarative languages. 

Ellis  Horwood,  1987, pp.  246-265. 23.  A. Taxsld. A lattice-theoretical fixpoint theorem and its application.  Pacific Journal 

of Mathematics,  1955, 5, pp.  285-309. 