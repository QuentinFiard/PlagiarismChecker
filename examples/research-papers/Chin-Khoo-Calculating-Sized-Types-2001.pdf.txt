

Higher-Order and Symbolic Computation, 14, 261-300, 2001
c\Theta  2001 Kluwer Academic Publishers. Manufactured in The Netherlands.

Calculating Sized Types*
WEI-NGAN CHIN chinwn@comp.nus.edu.sg
SIAU-CHENG KHOO khoosc@comp.nus.edu.sg
School of Computing, National University of Singapore

Abstract. Many program optimizations and analyses, such as array-bounds checking, termination analysis,
etc., depend on knowing the size of a function's input and output. However, size information can be difficult
to compute. Firstly, accurate size computation requires detecting a size relation between different inputs of a
function. Secondly, different optimizations and analyses may require slightly different size information, and thus
slightly different computation. Literature in size computation has mainly concentrated on size checking, instead
of size inference. In this paper, we provide a generic framework on which different size variants can be expressed
and computed. We also describe an effective algorithm for inferring, instead of checking, size information. Size
information are expressed in terms of Presburger formulae, and our algorithm utilizes the Omega Calculator to
compute as exact a size information as possible, within the linear arithmetic capability.

Keywords: type inference, Presburger formulae, program analysis and optimization

1. Introduction
Many program optimizations and analyses, such as array-bounds checking, termination
analysis, etc., depend on knowing the size of a function's input and output. However, size
information can be difficult to compute.

Firstly, accurate size computation requires detecting size relation between different inputs
of a function. Thus, the computation of the size cannot be easily expressed using the usual
"independent attribute analysis"; rather, a "relational analysis" is required.

Secondly, different optimizations and analyses may require slightly different size information, and thus a slightly different computation. For instance, consider the following
append function:

append xs ys = case xs of Nil \Xi  ys

(x : xs\Pi ) \Xi  x : (append xs\Pi  ys)

There are several kinds of size information that are of interest to us. First and foremost,
we are interested in knowing how the output size of append relates to the sizes of its two
inputs, since this input/output size relationship is fundamental to the computation of "interprocedural" size information. Let m, n be the sizes (i.e., lengths) of the parameters xs, ys,
respectively, and p be the output size, then we have m + n = p. Next, we are also interested

*This is a revised and expanded version of the paper presented in the 2000 ACM SIGPLAN Workshop on Partial
Evaluation and Semantics-Based Program Manipulation.

262 CHIN AND KHOO
in finding how the sizes of the argument of the recursive calls to append compare to the sizes
of the original arguments. In the case of append, the size of the first argument to the recursive
call of append is monotonically decreasing, while the second remains constant in size. Such
information is useful for analyzing, among other properties, the termination of the program,
or the termination of a transformation process, such as partial evaluation (e.g., [9, 15]).

The literature on size computation has mainly concentrated on size checking, instead
of size inference [16, 29]. In this paper, we aim to infer as accurate size information as
possible for various optimizations and analyses. In the case of append, we can express its
size (and its type) as follows:

append :: [t]m \Xi  [t]n \Xi  [t]p

s.t. size m >= 0 \Psi  n >= 0 \Psi  p = m + n

inv 0 <= m+ < m \Psi  n+ = n

Here, we capture two pieces of size information. The first, prefixed by the keyword size,
relates the size of the function's input to that of its output. The second, prefixed by inv, is
used for recursive functions. We let m and n be the sizes of the arguments of an (initial)
call to append, and m+ and n+ be the sizes of the arguments of an arbitrary recursive (inner)
call to append. The constraint expressed by the declaration describes the call invariant for
all nested recursive calls (i.e., monotonic decrease of m+ and constancy of n+), with respect
to the initial call.

While the amount of size information may appear overwhelming, it is worth noting that the
computation of these kinds of information shares some commonality; i.e., a major portion
of the computation can be reused. This prompts us to find a representation that acts as a
base for representing size information, from which kinds of information can be computed.
We express size information using Presburger formulae, a class of logical formulae which
can be built from affine constraints over integer variables, the logical connetives ~, \Psi ,fi

, and the quantifiers fl and ffi. There are a couple of practical reasons for limiting size
information to Presburger formulae: Firstly, there exist efficient tools for solving these
formulae. Secondly, Presburger formulae can be used to capture the size information in a
wide range of practical applications.

We employ the Omega Calculator [24] for solving and inferring suitable Presburger
formulae in our size computation. The entire computation is divided into two steps: The
first step computes the relation between input/output size of a non-recursive function, as
well as the size change during one unfolding of a recursive function. In the second step,
we employ fixed-point techniques to compute a safe approximation of the size of the inputs
and outputs of recursive functions.

Corresponding to the use of fixed-point theory in computing the size of a recursive function, we interpret the underlying Presburger formulae as automorphic relations, and adapt
the transitive-closure operation [24] to compute a suitable size. Consequently, we propose
an effective algorithm for inferring, instead of checking, the required size information of
any function.

The outline for this paper is as follows: in the next section, we motivate the use of
size information in various applications. Then, in Section 3, we describe the syntax and
semantics of the source language. Section 4 describes the constraint system used to capture

CALCULATING SIZED TYPES 263
size information. Section 5 describes the inference algorithm declaratively, using a type
system. Type inference is further illustrated through an example in Section 6. In Section 7,
we describe how fixed-point computation is performed via the transitive-closure operation.
Section 8 describes recursive-call-invariant computation. Section 9 discusses a strategy
for obtaining a constraint that approximates fixed-point computation. Section 10 provides
general discussion of issues related to our type system, followed by discussion of related
work in Section 11. Section 12 concludes the paper by presenting our contributions.

2. Why sized types?
Sized types have a wide variety of uses. They can be used to prove some size-related
properties, such as termination. They can also be used to support program optimization,
such as elimination of bounds checks, dead codes, and repeated calls. In this section, we
motivate the need for sized types by briefly explaining how they can be used to support
termination property detection, vector-based memoization, and bounds-check elimination.

2.1. Termination property
Sized types can be used to confirm the termination property of recursive functions. A recursive function terminates if there exists a set of arguments that can be shown to decrease across
recursion. Termination proofs normally use well-founded orderings (or quasi-orderings)
of the arguments of recursive functions. Termination property is needed by safety-critical
systems [16]. It is also used to ensure that the partial evaluation process terminates [9].
To briefly illustrate termination property is detected, consider the following Ackermann
function:

ack :: Inti \Xi  Int j \Xi  Int k

s.t. inv (0 <= j+ < j \Psi  1 <= i \Psi  i+ = i)fi

(0 <= i+ < i \Psi  0 <= j)
ack x y = case (x, y) of (0, m) \Xi  m + 1

(n + 1, 0) \Xi  ack n1
(n + 1, m + 1) \Xi  ack n (ack(n + 1) m)

The recursive-call invariant about its two parameters indicates a well-founded lexicographic
ordering. This invariant confirms that recursive calls to ack will eventually terminate, even
though the second parameter has an increasing value.

2.2. Vector-based memoisation
Sized types can also be used to determine the sizes of the vectors needed to avoid the
re-computation of repeated calls, through a technique known as dynamic-sized memoisation [4]. A simple example is the following binomial function with exponential time

264 CHIN AND KHOO
complexity:

bin :: (Int n, Int k) \Xi  Intp

s.t. inv (k+, 1 <= k < n) \Psi  (k+, 1 <= n+ < n)\Psi 

(k + n+ <= n + k+) \Psi  0 <= k+
bin (n, k) = case n of 0 \Xi  1

m + 1 \Xi  if k <= 0 ffl k >= n then 1

else bin(m, k i 1) + bin(m, k)

If the redundant calls of binomial function is eliminated, the complexity becomes quadratic.
How can this be done? The recursive-call invariant for binomial function states that parameter k+ is bounded below by max(0, n+ + k i n), and above by min(n+, k). Using this
bound, the dynamic-sized memoisation technique can derive the following less readable
but much efficient vector-based definition of binomial (using Haskell-like notation).

bin (n, k) =

let bin\Pi  (n\Pi ) = case n+ of

0 \Xi  (\Sigma k+ \Xi  1)
(m + 1) \Xi 
let z = bin\Pi (m) ;

bn k+ = if (k+ <= 0 fi k+ >= n+) then 1

else z(k+ i 1) + z(k+) ;
l = max(0, n+ i (n i k)) ;

u = min(n+, k) ;
vec = array(l, u)[a := bn a | a ` [l..u]]
in (!) vec
in if (0 <= k \Psi  max(1, k) <= n) then bin\Pi (n) (k) else 1

2.3. Bound-check elimination
Run-time bounds checking on arrays is crucial for ensuring robust programs but can be
expensive. Many compilers can eliminate bounds checks. However, they do not fare well,
due to their inability to synthesize loop invariants and perform inter-procedural analysis.
Sized type can give better results than standard techniques.

Consider the following binary search function bsearch:

data Cmp = LESS | EQUAL | MORE
bsearch cmp key arr =

let look :: (Int u, Int v) \Xi  Int'

s.t. inv u <= u+ \Psi  v >= v+

CALCULATING SIZED TYPES 265

look(lo, hi) = if (hi >= lo)

then let m = lo + (hi i lo) div 2

x = sub(arr, m)
in case cmp(key, x) of

LESS \Xi  look(lo, m i 1)
EQUAL \Xi  m
MORE \Xi  look(m + 1, hi)
else NONE
in look(0, (length arr) i 1)

The array access function sub requires a bound check to ensure that the index is within
bounds. Here, the size context of the initial call to look is (l0 = 0 \Psi  h0 = i i 1), where
l0, h0 are the sizes of the two arguments of look, and i is the size of the array. Combining

this context, the recursive-call invariant for look, and the computation for local variable m,
we can determine that 0 <= m <= h0 for all nested invocations of look. The result is sufficient
to allow the replacement of sub by an optimized version (without bound-checking).

An approach to allow bound checks to be safely eliminated was proposed by Xi and
Pfenning [29]. In their type-checking framework, it is the responsibility of the user to specify
where bound checks can be eliminated. Automatic inference of sized types presents a new
challenge. With the help of automatic size type inference, we have recently formulated a
new approach to identifying and eliminating redundant bounds check [5]. Our approach is
able to identify both totally-redundant and partially-redundant bound checks. In the case of
partially-redundant bound checks, we make use of a program specialisation to strengthen
the context under which bound checks can be safely eliminated.

3. Language syntax and semantics
We apply our technique to a higher-order typed functional language with a strict semantics.
Recursive functions in the language are confined to self-recursion for simplicity. Mutual
recursion can be encoded into self-recursion by appropriate tagging of input and output, as
described in Section 10. The syntax of our source language is shown in Figure 1.

x ^ Var -- Variables
n ^ Int -- Integer constants
c ^ Con -- Data Constructors

p ^ Pat -- Patterns
e ^ Exp -- Expressions

e ::= x | n | c e1 . . . en | \Sigma x . e | (e1 e2)|

let x = e1 in e2 | letrec x = e1 in e2|
case e0 of {ci x1i . . . xmi \Xi  ei }ni=1

Figure 1. Syntax of the source language.

266 CHIN AND KHOO

E :: Exp \Xi  Env \Xi  UE

[[n]]\Omega  = nE
[[x]]\Omega  = \Omega [[x]]E
[[c e1 . . . en]]\Omega  = c (E[[e1]]\Omega ) . . . (E[[en]]\Omega )E
[[\Sigma x . e]]\Omega  = f

where f (v) = E[[e]]\Omega [v/x]E
[[e1 e2]]\Omega  = (E[[e1]]) (E[[e2]])E
[[let x = e1 in e2]]\Omega  = E[[e2]]\Omega [v/x]

where v = E[[e1]]\Omega E
[[letrec x = e1 in e2]]\Omega  = E[[e2]]\Omega [v/x]

where v = fix (\Sigma x . e1)E
[[case e0 of {ci x1i . . . xmi \Xi  ei }ni=1]]\Omega  =

case (E[[e0]]\Omega ) in

c1v11. . . vm1 * E [[e1]]\Omega [v11/x11, . . . , vm1/xm1]

..
.

cnv1n . . . vmn * E [[en]]\Omega [v1n /x1n , . . . , vmn /xmn ]

Figure 2. Semantics of the source language.

Our method is accurate for only a restricted form of higher-order functions, as will be
discussed in Section 10.

We assume that expressions are well-typed under Milner-Damas type system. Only
well-formed and well-typed expressions are assigned a meaning. This meaning is defined
by the denotational semantics.

The denotational semantics of the language has two semantic domains--one to model
values (U ) and the other to model the value environment (Env).

U = Int, ss Con, ss [U + U] ss [U * U] ss [U \Xi  U]
Env = Var \Xi  U

The flat domains of integers and data constructors are denoted by Int, and Con, respectively. The definitions of domain operations , (lifting), + (separated sum), * (cartesian
products), \Xi  (continuous function space), and ss (coalesced sum) are standard. We will
write a series of cartesian products as one n-ary product. The semantic function E is shown
in Figure 2. \Omega [v1/x1, . . . , vk/xk] denotes environment update, and is defined by

(\Omega [v1/x1, . . . , vk/xk])x = \Theta  v j if x = x j for some j ^ {1, . . . , k}\Omega x otherwise

4. Constraints
Our technique requires manipulation of constraints represented as Presburger formulae.
Figure 3 depicts their syntax. Note that arithmetic expressions are restricted to affine

CALCULATING SIZED TYPES 267

n ^ Z -- Integer constants
v ^ V -- Variables
fi ^ C -- Constraints

::= flSv . fi | ffi
ffi ^ F -- Presburger Formulae (terms)

::= b | ffi1 \Psi  ffi2 | ffi1 fi ffi2 | ~ffi | flv . ffi
b ^ BExp -- Boolean Expressions

::= True | False | a1 = a2 | a1 oe= a2|

a1 < a2 | a1 > a2 | a1 <= a2 | a1 >= a2
a ^ AExp -- Arithmetic Expressions

::= n | v | n * a | a1 + a2 | i a

Figure 3. Syntax of constraints.

functions of the variables. For example, 2 * v is legal, but v * v is not. When X = {v1, . . . ,
vn} o/ V, we write flX.ffi or flv1 * * * vn.ffi instead of flv1. * * * .flvn.ffi.

The basic Presburger algorithm that decides the validity (or satisfiability) of Presburger
formulae is given by Hilbert and Bernays [14]. D.C. Cooper [6] extends the algorithm
to include subtraction and formulae on both positive and negative integers. In this paper,
Presburger formulae are simplified and satisfied via the Omega calculator [24]. We view
our constraint system as a cylindric algebra [13], where constraints are identified modulo
logical equivalence. Cylindric algebra provides an algebraic-style reasoning about the
formulae. Our cylindric algebra, P, is a boolean algebra where existential quantification
(flX) is the (idempotent) projection operation, fi and \Psi  are the meet (denoted by OEP) and
the join (denoted by O/P) operator respectively, ~ is the complement, and False and True
are bottom and top elements, respectively. The algebra has the following cylindrification
properties [21, 22].

P1. ffi P flv.ffi
P2. ffi P ffl implies flv.ffi P flv.ffl
P3. flv.(ffi \Psi  flv.ffl) =P (flv.ffi) \Psi  (flv.ffl)
P4. flv.flw.ffi =P flw.flv.ffi.

In the above, P denotes logical entailment. We say that ffi entails ffl, written ffi P ffl, if
ffl is a logical consequence of ffi. Also, ffi =P ffl if and only if ffi P ffl and ffl P ffi; in this
case, we say ffi and ffl are logically equivalent.

Since formulae are identified up to logical equivalence, free variables appearing in a
formula should be defined as those appearing in every logically equivalent formula. Hence,
we define the set f v(ffi) of free variables occurring in a formula ffi by f v(ffi) = {v | ffi oe=Pfl

v.ffi}.

Definition 1 (Satisfiability of a formula). A formula ffi is satisfiable iff P fl f v(ffi).ffi.
Given two formula, ffi1 and ffi2, we say that ffi2 approximates ffi1, denoted by ffi1 !P ffi2, iff
ffi1 P ffi2.

268 CHIN AND KHOO

A constraint as defined in Figure 3 can be a formula, ffi, or a parameterized formula of
the form flSv1 * * * vk.ffi . flS provides a binding mechanism that binds variables in f v(ffi).
When the free variables refer to the sizes of the input and output of an expression, the
corresponding parameterized formula can be interpreted as constraints on the sizes of the
input and output of that expression. So, flS v1 * * * vk.ffi can be presented in different forms.
For instance, we use the notation [v1] \Xi  [v2, . . . , vk] : ffi (this is called the relational form
in the Omega's terminology) to indicate that ffi is the constraint between the input of size v1
and the output of sizes v2, . . . , vk. On the other hand, [v1, . . . , vk] : ffi (this is called the set
form) denotes a general constraint between the sizes v1, . . . , vk, without any differentiation
between input and output. In general, this presentation can be written as X \Xi  Y : ffi,
where X and Y are (possibly empty) sets of size variables. Lastly, we note that while
these presentations may be used with different intentions, they all have equivalent logical
semantics, which is exactly that of ffi.

We use the terminology "formula", "term", and "constraint" interchangeably, when it
is clear from the context. Also, we use the notions [v1, . . . , vk] : ffi (and its variants) and
flSv1 * * * vk : ffi interchangeably.

Given two constraints fi = flS X.ffi and j = flSY.ffl. Then fi \Psi  j is meaningful when
X = Y , and the result is flS X.ffi \Psi  ffl. We define fi fi j similarly.

Given a formula ffi, ffi[ffl1/y1, . . . , ffln/yn] denotes the substitution of ffl1, . . . , ffln for the
free variables y1, . . . , yn in ffi. For a parameterized formula, substitution applies to its
parameters as well. Substitution is written in a manner similar to function application.
Thus, given fi = [v1, . . . , vk] : ffi, we write fi(n1, . . . , nk) to denote ffi[n1/v1, . . . , nk/vk].

5. Type system
We assume that the input program to our sized-type system is well-typed (under the Milner-
Damas type system). Furthermore, the type of each expression in the program is available
during size computation.

A sized type is a pair containing an annotated type and a constraint. The precise set of
annotated type expressions is shown in Figure 4.

We use the notations [*] and (*, . . . , *) for type constructors of lists and tuples, respectively.
Size variables are attached to type constructors, such as [Intv]w, where w refers to the length
of a list, and v refers to the possible integer values of list elements. [Int] is referred to as
the underlying type of this list, and is described by underlining the annotated type; i.e.,
[Intv]w = [Int]. Size variables are used to formulate the size constraint of an expression.

v, i ^ V " {'} -- Size Variables
t ^ TVar -- Type Variables

D ^ TypeCon -- Type Constructors
` ^ TypeSch -- Type Schemes

::= ffit.` | '
' ^ Type -- Type

::= t | ' \Xi  ' | Dv'1 . . . 'k

Figure 4. Annotated type expressions.

CALCULATING SIZED TYPES 269

The symbol ' denotes the absence of size information. Thus, the domain D' denotes
the absence of size information for the constructor D. Moreover, ' is also used when the
size information is too "complicated" to compute, such as expressing multiplication of two
size variables. While size variables may appear in the constraint, ' may not.

Type variables convey no size information, since we do not know the actual type yet.
Consequently, our type system does not calculate any relationship among polymorphicallytyped parameters. We leave this interesting issue as one of the future works.

5.1. The meaning of size
The term size can have various meanings. In Xi and Pfenning's work on dependent type
system [30], users can specify a specific meaning of size through a size annotation on the
data declaration. To specify the rules for size computation, we require an understanding of
what sizes mean. While our work can possibly be extended to support user-specified size
annotation, we shall for simplicity assume default interpretation of sizes for data types of
interest. In this paper, we focus on deriving the size of lists declared as [t] = Nil + (t : [t]).
A shorthand notation for a finite list is of the form [d1, . . . , dk], where d1, . . . , dk are
elements of some type. Following is the default interpretation of sizes of the relevant data
types.

1. The size of an integer is the integer itself.
2. The size of a recursive data consists of its maximum depth ( i.e., the number of nestings).

Thus, the size of a list is its length, and the size of a binary tree is its depth. Also, the
size of an array1 is simply the array size.
3. We assign distinct size value to different data of a non-recursive enumerated type. In

the case of boolean type, we arbitrarily choose 0 as the size of False, and 1 for True.
Similarly, in Section 2.3, we declare a data type Cmp = LESS + EQUAL + MORE. We
will assign 0, 1, and 2 to the size of LESS, EQUAL, and MORE respectively.
4. The size of a function is a relation between its input and output sizes.

Note that we provide separate size information for different data constructors in an enumerated data type. In contrast to the work of sized typing by Hughes, Pareto, and Sabry [16],
this treatment of enumerated type enables us to infer more accurate size information of an
expression whose value depends on enumerated data.

Formally, we define the size through the following function S.

Definition 2 (Size function S). Function S is a strict function that takes in a data with its
annotated type, and returns a formula describing the size of the data.

S :: (U * TypeSch) \Xi  PS

(, :: ` ) = FalseS
(n :: Intv) = chkv(v = n)S
(Nil :: [' ]v) = (chkv(v = 0)) ffi ' ^ Type

270 CHIN AND KHOO

S([d1, . . . , dk] :: [' ]v) = \Lambda (chkv(v = k)) \Psi 

k\Xi 

j=1

(S(d j :: ' ))\Pi 

S((d1, d2) :: ('1, '2)') = (S(d1 :: '1)) \Psi  (S(d2 :: '2))S

(False :: Boolv) = (chkv(v = 0))S
(True :: Boolv) = (chkv(v = 1))S
( f :: '1 \Xi  '2) = ffi

where ffi = \Sigma {ffii | ffii ^ P and ffi(d1 :: '1) : ffi(d2 :: '2) :

(d2 = f d1) implies

((S(d1 :: '1)) \Psi  (S(d2 :: '2))) P ffii }S
(d :: ffit.` ) = S(d :: ` )S
(d :: t) = True

chkv :: P \Xi  P
chkv ffi = if (v = ') then True else ffi

In the above definition, function chkv checks if the variable v is ', and provides no information (i.e., True) when that is true. Notice also that we do not assign size information
for tuple construction. If we were to assign a size for a pair, it would be 2. Since the type
system only accepts well-typed program, it is not useful to know the sizes of tuples.

The size of a function is defined as the strongest constraint which can be satisfied by all
pairs of the function's input- and output-sizes.

We treat TypeSch as a discrete domain. The fact that S is monotone can be obtained by
structural induction. We omit the proof here.

Lemma 1. Function S is continuous.
Proof: We show the case when the first argument to S is an element of a chain of functions.
Consider a chain of functions { fi :: '1 \Xi  '2}i>0 such that for each i, S( fi :: '1 \Xi  '2) = ffii .
Let f# = OE { fi :: '1 \Xi  '2}. We wish to show that S( f# :: '1 \Xi  '2) = \Upsilon Pi>0{ffii } =\Phi 

i>0{ffii }.For any d ::

'1 and any d\Pi  :: '2, if d\Pi  = f# d, then there exists a function f j for some j

such that d\Pi  = f j d. By the definition of S, we thus have:

(d\Pi  = f# d) implies (d\Pi  = f j d) for some j

implies ((S(d :: '1)) \Psi  (S(d\Pi  :: '2))) P ffi j

P \Xi 

i>0{

ffii }.

We next show that \Phi i>0{ffii } is the desired strongest constraint. Given any constraint ffl
such that for any d :: '1 and any d\Pi  :: '2, d\Pi  = f# d implies (S(d :: '1)) \Psi  (S(d\Pi  :: '2))  ffl.
By monotonicity of S, we must have ffii !P ffl for all ffii = S( fi :: '1 \Xi  '2). Thus,
(\Phi i>0 ffii ) !P ffl. \Delta 

CALCULATING SIZED TYPES 271
We further extend on the definition of S to work on products of denotation values. That is,

S(d1 :: '1, . . . , dk :: 'k) =

k\Psi 

i=1 S

(di , 'i ).

Corollary 2. In the extended definition, S is continuous on all its data arguments.
Proof: By Lemma 1 and the distributivity of \Psi  over fi in the algebra P. \Delta 

Intuitively, if a value, such as an integer, is not ,, then it has a size, and applying S to this
value should produce a formula that is satisfiable (since this data is itself an evidence for
the satisfiability of the formula). However, this intuition does not extend straightforwardly
to a function. It is possible to define a function that never terminates. In this case, the size
of this function should be equivalent to False. Connecting this intuition with the definition
of satisfiability in Definition 1, we have the following tautology:

Property 1. For any d :: U with annotated type ' . Suppose that ffi = S(d :: ' ) and d oe= ,.
Then either fl f v(ffi).ffi =P True or fl f v(ffi).ffi =P False.

We capture two kinds of size information: the size relation between a function's input and
output; and the recursive-call invariant about the input to nested recursive calls. Although
there is no need to capture the output size of a function in this invariant, the output size of
an auxiliary function is still required for inter-procedural analysis. Therefore, we compute
the size relation between a function's input and output, before computing recursive-call
invariant.

5.2. Type rules
Our type system specifies the size relation of every expression in the language. This relation
relates the size of (possible values bound to) free variables occurring in an expression to
the expression's output size. Specifically, in the type system, we associate each expression
with an environment (a type assumption) and a sized type.

The subtyping relation for our type system is simply the approximation relation in our
constraint system:

(`, ffi) ffi !P ffl

(`, ffl)

Figures 5 and 6 list the inference rules for inferring the sized type of an expression. To
avoid cluttering of symbols, we include a "where"-clause beneath a rule whenever necessary.
The "where"-clause defines new symbols to be used in either the premises or the conclusion.

A type usually has an inherent size constraint associated with it. For instance, if the
(annotated) type is of the form [t]i , then it is true that (chki (i >= 0)) (chk is defined in
Definition 2). That is, if i ^ V, the length of a list must be non-negative. Next, for Bool j ,
we have (chk j ( j = 0 fi j = 1)). The inherent constraint associated with an annotated type
` is denoted by I(` ).

272 CHIN AND KHOO

^ " {x :: (`, ffi)}  x :: (`, ffi) (Var)

 n :: (Inti , ffi) (Con:Int)
where ffi = chki (i = n); new i

 Nil :: ([']i, ffi) (Con:List1)
where ffi = chki (i = 0); new i

^  e1 :: (', ffi1) ^  e2 :: (['\Pi ]j, ffi2)

^  (e1 : e2) :: (['\Pi \Pi ]a, ffi) (Con:List2)

where ffi = chka (chk j (a = j + 1)) \Psi 

ffi3 \Psi  ffi2 \Psi  ffi1
ffi3 = (eqC '\Pi \Pi  ') fi (eqC '\Pi \Pi  '\Pi )
['\Pi \Pi ]a = *(['\Pi ]j )

^ " {x :: ('1, ffi1)}  e :: ('2, ffi2)

^  (\Sigma x. e) :: ('1 \Xi  '2, ffi\Pi 2) --------- (Abs)
where ffi1 = I('1 \Xi  '2)

ffi\Pi 2 = flX . ffi2 \Psi  ffi1 \Psi  C^

X = _(ffi2) i (_('1 \Xi  '2) " _(^))

^  e1 :: ('1 \Xi  ', ffi1) ^  e2 :: ('2, ffi2)CT

(^, ('1, ffi1), ('2, ffi2))
^  (e1 e2) :: (', ffi1 \Psi  (eqC '2 '1) \Psi  ffi2) (App)

Figure 5. Rules for checking sized type--Part I.

Some operations are used in the type rules to manipulate size variables. They are:
- Size-variable renaming (*): This is an overloaded operation serving various purposes:

1. Applying * to an annotated type (or type scheme), ' (or ` ), returns a new annotated

type with the same underlying type, but new free size variables. Note that renaming'

returns ', and renaming a variable in V returns a new variable in V.
2. Applying * to a sized type (`, ffi) results in a consistent renaming of size variables

occurring in ` . Thus, *([Inti ] j , j >= 1 \Psi  i > 1) = ([Intk]l , l >= 1 \Psi  k > 1), for some
new variables k and l.

- Size-variable listing (_): This takes in either an annotated type ` or a formula ffi, and

returns the set of free size variables (from V) occurring in ` or ffi. Thus, _([Inti ] j ) = {i, j}.
In the case of formulae, _ works just like function f v defined in Section 4. We also
extend _ to take in a type assumption as argument: Given ^ = {xi :: (`i , ffii )}ni=1, we
define _(^) = \Omega ni=1{_(`i ) " _(ffii )}.
- Size-variable matching (eqC): Given two annotated types, `1 and `2, with identical

underlying type. If _(`1) % _(`2) = &, then eqC produces a conjunction of equalities
that identify the corresponding free size variables between `1 and `2. For instance,
eqC[Inti ] j [Intk]l = chki (chkk(i = k)) \Psi  chk j (chkl( j = l)).

CALCULATING SIZED TYPES 273

^  e1 :: (`, ffi1) ^ " ^1  e2 :: (', ffi)

^  (let x = e1 in e2) :: (', ffi \Psi  C^1) (Let)

where ^1 = {x :: (`, flX.ffi1)}

X = _(ffi1) i (_(`) " _(^))

^ " {x :: (', fi)}  e1 :: (', fi)
^ " {x :: (ffit.', fi)}  e2 :: (', ffi2)

^  (letrec x = e1 in e2) :: (', ffi2) (Rec)

^  e0 :: ('0, ffi0)
Pat (Pi, '0) :: ^i ffii ^ {1, . . . , n}

^ " ^i  ei :: ('i, ffii ) ffii ^ {1, . . . , n}
^  case e0 of ---- (Case)

(ci x1i . . . xmi \Xi  ei)ni=1 :: ('\Pi , flX . ffi)

where X = \Omega ni=1{_(ffi\Pi i) i (_('0) " _(^))}

ffi = ffi0 \Psi  \Phi i ((eqC '\Pi  'i ) \Psi  ffi\Pi i )
'\Pi  = *'1; ffi\Pi i = ffii \Psi  C^i

^  e :: ('1 \Xi  '2, ffi)
^  e :: ('1 \Xi  '2, flS X . ffi)--- (Bind)where X =

_('1 \Xi  '2)

^  e :: ('1 \Xi  '2, flS X . ffi)

^  e :: ('\Pi 1 \Xi  '\Pi 2, ffi\Pi ) ---------------- (Unbind)
where ('\Pi 1 \Xi  '\Pi 2, ffi\Pi ) = *('1 \Xi  '2, ffi)

Figure 6. Rules for checking sized type--Part II.

A type assumption ^ binds program variables to their sized types. We write C^ to denote the collection of all constraints occurring in ^. For example, given an assumption
^ = {b :: (Booli , i = 0 fi i = 1), xs :: ([a] j , j >= 0)}, we have C^ = {i = 0 fi i = 1, j >= 0}. We
call this set of constraint the contextual constraint. When the context is clear, we also useC

^ to denote the conjunction of all these constraints. ^ is consistent iff C^ is satisfiable.New and unique size variables (from V) are generated during size inference. This is

achieved via either the application of function new or via size-variable renaming. These
size variables are associated with some (sub-)expressions. As the focus of type inference
shifts from one expression to its enclosing expression, those size variables associated with
the sub-expression become "irrelevant", and need to be "localized", so that type inference
will only deal with those size variables associated with the enclosing expression. This
localization of size variable is achieved via existential quantification.

There are three places in the type rules where irrelevant size variables are localized. They
appear respectively in the rules (Abs), (Let) and (Case).

Now we describe type rules in detail, bearing in mind that the usual (un-annotated) types
for expressions have been provided, and the rules are used to infer constraints about the size.

The rules for variables and integer constants are obvious. There are two rules for constructing a list: For constructing an empty list, we set the length of the list to be 0. For
adding an element argument to the beginning of a list argument, we infer that the length of
the new list is one element longer than the list argument. Furthermore, the size of the list
element is a disjunction of the size of the element argument and the size of elements in the

274 CHIN AND KHOO
list argument. This disjunction is achieved by equating the size variables of the resulting
list element to either the size variables of the element argument or that of the elements of
the list argument.

In the rule for abstraction, we assume that the parameter is associated with its inherent
constraint, and infer the sized type of the abstraction body. The result of inference is denoted
by ffi2. As the abstraction may be applied later on in some other context, we make ffi2 to be
context-independent by (1) including (via conjunction) the contextual constraint to obtain
the constraint ffi2 \Psi  ffi1 \Psi  C^, and (2) localizing those "irrelevant" size variables occurring
in ffi2 through existential quantification. This set of quantified size variables is denoted by
the symbol X.

The "irrelevant" size variables occurring in ffi2 are generated when the size of the abstraction body is inferred; they will not be referenced outside the abstraction body, and therefore
need not remain free. For example, if a list is constructed and returned from the abstraction
body, size variables will be generated during the list construction. Once constructed, only
those size variables associated with the entire list is of relevant outside the abstraction, but
not the size variables generated during the construction of this list.

The rule (App) specifies that the constraint for a function application is formed by
combining the abstraction constraint and the argument constraint. The combination is
accomplished by identifying the size variables of the abstraction input and the argument.
The term CT (^, ('1, ffi1), ('2, ffi2)) is a consistency test. This ensures that the size of the
actual argument applied satisfies the sized constraint stipulated by the function's formal. It
is defined as follows:

CT (^, ('1, ffi1), ('2, ffi2)) = ffl2 !P ffl1

where ffl1 = flX.(ffi1 \Psi  (eqC _('1) Z))

ffl2 = flY.(ffi2 \Psi  C^ \Psi  (eqC _('2) Z ))

X = _(ffi1); Y = _(ffi2); Z = _(*('1))

For a let-expression, we first infer the sized type (`, ffi1) of its local declaration. We then
existentially quantify the "irrelevant" size variables occurring in ffi1, and use the result to
infer the sized type of the body. Finally, the fact that the size of the body relies on the
knowledge about the size of the local declaration is made explicit by including the local
constraints in the result.

Just like the case for inferring the size of an abstraction, the "irrelevant" size variables
refer to those generated in ffi1, and will not be used outside the local declaration.

In the rule (Rec), we infer the sized type of a local recursive function. This is accomplished via fixed-point computation, which will be elaborated in Section 7. We then use
the result about the local declaration to infer ffi2, the size of the letrec-body. In contrast to
the rule (Let), it is not necessary to include the size of local declaration in the final result,
because this size information will be included when the local function is used in the body.

We discuss the inference of case-expressions in the following 4 paragraphs. To begin
with, we infer the sized type of the test; the inference result is then used in the inference
of the branches. At each branch, we build a local assumption, and then infer the size of
this branch under this assumption. The final size of a branch is the conjunction of the size
inferred and the constraints in the assumption.

CALCULATING SIZED TYPES 275

The constraints produced at all branches are collected via disjunction to obtain a constraint for the case-expression. Now, the size variables generated at each of the branches
become irrelevant, and we therefore existentially quantify them. These variables include
those generated from the patterns, as well as those generated when inferring the branches.

Note that the contextual constraint in each branch are included in the final result. This is
useful as pattern matching reveals some information about the test. For example, consider
the following expression:

case i of 0 \Xi  1

(n + 1) \Xi  2

Conventional type inference will tell us the i :: Int. However, the pattern used in the case
tells us that i is expected to be non-negative. This information is captured in the local
environment of the second branch, which states that the size of i is one larger than that of n,
and the size of n is non-negative. However, this fact is not used by the expression in any of
the branches, and therefore it will only be captured by including the contextual constraint.

We require that patterns in a case-expression be mutually exclusive and exhaustive, and
employ a set of inference rules called Pat to compute the sized types of each pattern.
Figure 7 shows the rules for pattern matching. The function new Var introduces a new
variable from Var, which is then bound to a size type to form a type assumption.

Going back to the discussion of the type rules, we now discuss the last two rules shown
in Figure 6. The binding rule (Bind) is only applied to the constraint of \Sigma -abstraction.
It parameterizes the constraint with the size variables associated with the annotated type
of the lambda abstraction. On the other hand, the unbinding rule (Unbind) undoes the
job of (Bind), and instantiates (with new size variables) a parameterized constraint into
a Presburger formula for further manipulation. This rule is used only when the associated
lambda expression is applied.

Note that the binding rule applies to all \Sigma -abstractions encountered, and the unbinding rule applies to all function applications. This is different from the use of generalization/instantiation rules used in polymorphic type inference, and the reason for being
different is because each function application will have different sets of arguments and
result, and these need to be captured by different sets of size variables.

Since both (Bind) and (Unbind) are used in fixed places of a program during sized-type
inference, our rules can be made syntax-directed.

5.3. Soundness
Soundness of the sized-type system is defined with respect to the semantics function E
defined in Section 3 and the size function S given in Definition 2.

Definition 3 (Satisfiability). Given a denotational value d of an annotated type ` , we say
d satisfies a constraint ffi under ` if S(d :: ` ) !P flX.ffi, where X = _(ffi) i _(` ).

Lemma 3. Satisfiability is admissible.

276 CHIN AND KHOO

Pat (True, Booli) :: (PAT : Bool1){ x ::

(Booli, chki (i = 1)) }

where newVar x

Pat (False, Booli ) :: (PAT : Bool2){ x ::

(Booli, chki (i = 0)) }

where newVar x

Pat (x, ') :: { x :: (', True) } (PAT : Var)
Pat (Nil, [']i) :: { x :: ([']i , chki (i = 0)) } (PAT : List1)where newVar x

Pat (P1, '\Pi ) :: ^1
Pat (P2, ['\Pi \Pi ]j) :: ^2

Pat (P1 : P2, [']i ) :: { x :: ([']i , ffi) } (PAT : List2)where

ffi = (chki (chk j ( j + 1 = i)))\Psi 

(((eqC''\Pi ) \Psi  C^1) fi

((eqC''\Pi \Pi ) \Psi  C^2))
'\Pi  = *(')
['\Pi \Pi ]j = *(['\Pi ]i) ; newVar x

Pat (0, Inti ) :: { x :: (Inti , chki (i = 0)) } (PAT : Int1)where newVar x

Pat (n, Intj ) :: ^
Pat (n + 1, Inti ) :: { x :: (Inti , ffi) } (PAT : Int2)

where ffi = chki (chk j (( j + 1 = i) \Psi 

( j >= 0))) \Psi  C^ ;
Int j = *(Inti ); newVar x

Figure 7. Type rules for pattern matching.
Proof: By the continuity of function S (Lemma 1). \Delta 
We also extend the satisfiability to work with parameterized formula, instead of formula.
Given fi = flS X.ffi, we say "d satisfies fi under ` " if d satisfies ffi under ` .

When an assumption set ^ is given, and ffi may contain some "free variables" which are
further constrained by this assumption, we need to extend the satisfiability definition to ^.

Given an assumption set ^ and a denotational environment \Omega , we say \Omega  satisfies ^ if

S(d1 :: `1, . . . , dn :: `n) !P flX.C^

where X = _(C^) i \Lambda 

nff

i=1{

_(`i )}\Pi ;

\Omega  = {xi :: di }ni=1; ^ = {xi :: (`i , ffii )}ni=1.
Corollary 4. Satisfiability over an assumption set is admissible.

CALCULATING SIZED TYPES 277
Proof: By Corollary 2 and the continuity of an assumption set. \Delta 
Lemma 5 (Soundness of pattern matching). If Pat (P, ' ) :: ^, then for any denotational
value d of annotated type ', if d matches the pattern P, then S(d :: ' ) !P flX.C^ where

X = _(C^) i _(' ).

Proof: The proof is by induction on the height of the type derivation. We prove the case of
pattern matching a list. Cases of pattern matching other constructs can be proven similarly.

We show the soundness of rule (PAT : List1), let d be a denotation of an annotated type
[' ]i . Suppose that d pattern matches Nil, then d must be (the denotational value) Nil. We
have ^ = {x :: ([' ]i , chki (i = 0))}. So, C^ = chki (i = 0). Now, S(Nil :: [' ]i ) = chki (i = 0)=

P C^. Since _(C^) i _(' ) = &, we arrive at the proof.To prove the soundness of the rule

(PAT : List2), let d be a denotation of an annotated

type [' ]i . Suppose that d pattern matches the pattern (P1 : P2), then d must be a non-empty
list. Let d = [d1, . . . , dn] for some n > 0, then d1 matches P1, and [d2, . . . , dn] matches P2.
Assuming that Pat (P1, ' \Pi ) :: ^1 and Pat (P2, [' \Pi \Pi ] j ) :: ^2. From the induction hypothesis,
we have

S(d1 :: ' \Pi ) !P fl X.C^1 where X = _fiC^1fl i _(' \Pi )
andS

([d2, . . . , dn] :: [' \Pi \Pi ] j ) = (chk j ( j = n i 1)) \Psi 

n\Xi 

k=2

(S(dk :: ' \Pi \Pi )) !P fl Y.C^2

where Y = _fiC^2fl i _(' \Pi \Pi ).
So, S([d1, . . . , dn] :: [' ]i )

= chki (i = n) \Psi 

n\Xi 

k=1 S

(dk :: ' )

=P fl j.chki (chk j ( j + 1 = i)) \Psi  chk j ( j = n i 1) \Psi \Lambda 

S(d1 :: ' ) fi

n\Xi 

k=2S

(dk :: ' )\Pi 

=P fl V.chki (chk j ( j + 1 = i))\Psi \Lambda 

((eqC ' ' \Pi ) \Psi  chk j ( j = n i 1) \Psi  S(d1 :: ' \Pi )) fi\Lambda 

(eqC ' ' \Pi \Pi ) \Psi  chk j ( j = n i 1) \Psi 

n\Xi 

k=2S

(dk :: ' \Pi \Pi )\Pi \Pi 

where V = _(' \Pi ) " _(' \Pi \Pi ) " { j}!
P fl V.chki (chk j ( j + 1 = i)) \Psi  fifi(eqC ' ' \Pi ) \Psi  fl X.C^1fl fifi

(eqC ' ' \Pi \Pi ) \Psi  fl Y.C^2flfl
where V = _(' \Pi ) " _(' \Pi \Pi ) " { j}

X = _fiC^1fl i _(' \Pi ); Y = _fiC^2fl i _(' \Pi \Pi )

278 CHIN AND KHOO

!P (Since variables in X, Y do not occur anywhere else in the formula)fl

Z .chki (chk j ( j + 1 = i)) \Psi  fifi(eqC ' ' \Pi ) \Psi  C^1fl fifi

(eqC ' ' \Pi \Pi ) \Psi  C^2flfl
where Z = V " X " Y ; V = _(' \Pi ) " _(' \Pi \Pi ) " { j}

X = _fiC^1fl i _(' \Pi ); Y = _fiC^2fl i _(' \Pi \Pi )=
P (Since V " X " Y = _(C^) i _(' ))fl

Z .C^

where Z = _(C^) i _(' )

^ = {x :: ([' ]i , ffi)}

ffi = chki (chk j ( j + 1 = i)) \Psi  fifi(eqC ' ' \Pi ) \Psi  C^1flfifi

(eqC ' ' \Pi \Pi ) \Psi  C^2flfl \Delta 

Theorem 6 (Soundness). Given an expression e of type ` , for some ` . If  e :: (`, ffi),
then for any monomorphic instance of e with denotation d, d satisfies ffi under ` .

Proof: In the proof, we assume that the denotation of the expression is not ,. If it were,

, we would have S(,) = False, and the result would hold vacuously.

The proof is by induction on the height of the type derivation. The induction hypothesis
is as follows:

If ^  e :: (`, ffi), then for all \Omega  such that \Omega  satisfies ^ and E[[e]]\Omega  oe= ,,S

(E[[e]]\Omega  :: `, d1 :: `1, . . . , dn :: `n) !P flX. ffi \Psi  C^,
where \Omega  = {xi :: di }ni=1, and

X = _(ffi \Psi  C^) i \Lambda _(' ) "

nff

i=1{

_(`i )}\Pi .

The case of inferring the size of self-recursive function requires proving that the validity
of fixed-point computation, and we leave it until Section 7 (Theorem 7). The proofs for all
the other cases are standard. Here, we present the let case in detail:

Following the notation used in the type rule for let, assume ^  (let x = e1 in e2) :: (', ffi \Psi C

^1) and \Omega  satisfies ^, for some \Omega  = {xi :: di }ni=1. We wish to show that:

S(E[[(let x = e1 in e2)]] \Omega  :: ', d1 :: `1, . . . , dn :: `n)!

P flX. ffi \Psi  C^1 \Psi  C^ (*)

where X = _fiffi \Psi  C^1 \Psi  C^fl i \Lambda _(' ) "

nff

i=1 {

_(`i )}\Pi 

Equivalently,

S(E[[e2]]\Omega [u/x] :: ', d1 :: `1, . . . , dn :: `n) !P flX. ffi \Psi  C^1 \Psi  C^

where u = E[[e1]]\Omega 

CALCULATING SIZED TYPES 279

X = _fiffi \Psi  C^1 \Psi  C^fl i \Lambda _(' ) "

nff

i=1 {

_(`i )}\Pi 

\Omega  satisfies ^*

(induction hypothesis for ^  e1 :: (`, ffi1))S

(E[[e1]]\Omega  :: `, d1 :: `1, . . . , dn :: `n) !P flY. ffi1 \Psi  C^

where Y = _(ffi1 \Psi  C^) i \Lambda _(` ) "

nff

i=1 {

_(`i )}\Pi 

= \Lambda Since _(^) = _(C^) "

nff

i=1 {

_(`i )}\Pi 

* (_(ffi1) " _(^)) i \Lambda _(' ) "

nff

i=1 {

_(`i )}\Pi 

* \Lambda Since \Lambda 

nff

i=1 {

_(`i )}\Pi  ' _(^), and therefore X1 ' Y \Pi 

S(E[[e1]]\Omega  :: `, d1 :: `1, . . . , dn :: `n) !P flY1.flX1. ffi1 \Psi  C^

where Y1 = Y i X1; X1 = _(ffi1) i (_(` ) " _(^))

Now, Y1 = Y i X1

= \Lambda (_(ffi1) " _(^)) i \Lambda _(' ) "

nff

i=1 {

_(`i )}\Pi \Pi  i X1

= \Lambda Since _(^) = _(C^) "

nff

i=1 {

_(`i )}\Pi 

*\Lambda (_(ffi1) " _(C^)) i \Lambda _(' ) "

nff

i=1 {

_(`i )}\Pi \Pi  i X1

= (Since _(C^) % X1 = &)

* ((_(flX.ffi1) " _(C^)) i \Lambda _(' ) "

nff

i=1 {

_(`i )}\Pi \Pi 

Hence, we have

S(E[[e1]]\Omega  :: `, d1 :: `1, . . . , dn :: `n) !P flY1.(flX1. ffi1) \Psi  C^

where Y1 = \Lambda (_(flX.ffi1) " _(C^)) i \Lambda _(' ) "

nff

i=1 {

_(`i )}\Pi \Pi 

* (Definition of satisfiability)

\Omega [u/x] satisfies (^ " ^1) where ^1 = {x :: (`, flX1.ffi1)} and

u = E[[e1]]\Omega 

280 CHIN AND KHOO

* (induction hypothesis for ^ " ^1  e2 :: (', ffi))S

(E[[e2]]\Omega [u/x] :: ', u :: `, d1 :: `1, . . . , dn :: `n)!

P flZ . ffi \Psi  C^ " ^1

where Z = _fiffi \Psi  C^"^1fl i \Lambda _(' ) " _(` ) "

nff

i=1 {

_(`i )}\Pi 

( (Associativity of Conjunction)S

(E[[e2]]\Omega [u/x] :: ', u :: `, d1 :: `1, . . . , dn :: `n)!

P flZ . ffi \Psi  C^1 \Psi  C^
where Z = _fiffi \Psi  C^1 \Psi  C^fl

i\Lambda _(' ) " _(` ) "

nff

i=1 {

_(`i )}\Pi 

( (Definition of S on n-tuple)S

(u :: ` ) \Psi  S(E[[e2]]\Omega [u/x] :: ', d1 :: `1, . . . , dn :: `n)!

P flZ . ffi \Psi  C^1 \Psi  C^*
(Existential introduction)fl

X.(S(u :: ` ) \Psi  S(E[[e2]]\Omega [u/x] :: ', d1 :: `1, . . . , dn :: `n))!

P flX .flZ. ffi \Psi  C^1 \Psi  C^
where X = _(` )

( \Lambda since X % _\Lambda 

nff

i=1

`i \Pi  = &\Pi 

(flX.S(u :: ` )) \Psi  S(E[[e2]]\Omega [u/x] :: ', d1 :: `1, . . . , dn :: `n)!

P flX.flZ .ffi \Psi  C^1 \Psi  C^

By Property 1, we have either flX.S(u :: ` ) = False or flX.S(u :: ` ) = True. In the former
case, the above inequality is vacuously true, and we have attained the proof. In the latter
case, we get

S(E[[e2]]\Omega [u/x] :: ', d1 :: `1, . . . , dn :: `n)!

P flX.flZ. ffi \Psi  C^1 \Psi  C^=
P flW.ffi \Psi  C^1 \Psi  C^ where W = Z " X
Lastly, W = Z " X

= \Lambda _fiffi \Psi  C^1 \Psi  C^fl i \Lambda _(' ) " _(` ) "

nff

i=1 {

_(`i )}\Pi \Pi  " _(` )

= _fiffi \Psi  C^1 \Psi  C^fl i \Lambda _(' ) "

nff

i=1 {

_(`i )}\Pi 

Hence, we obtain the inequality expressed in (*). \Delta 

CALCULATING SIZED TYPES 281
6. Size inference for non-recursive functions
We show how type inference is done for the following non-recursive function:

f b xs = case b of False \Xi  xs

True \Xi  append xs xs

The function f defined above depends on append. The sized type of append is assumed
known.

The annotated type of f is Booli \Xi  [a] j \Xi  [a]k, for some size variables i, j, k ^ V,
and type variable a. Consider the process of inferring the type of the right-hand side of the
definition of f . The assumption at this point consists of:

^ = { b :: (Booli , i = 0 fi i = 1), xs :: ([a] j , j >= 0),

append :: ([t]m \Xi  [t]n \Xi  [t]p,

flS m, n, p .(m >= 0 \Psi  n >= 0 \Psi  p = m + n)) }

In the False branch, we construct an assumption, ^1 = {nv1 :: (Booli , i = 0)} from its
pattern (i.e., False). nv1 is a new variable name used to refer to this pattern. The size
constraint for this branch is now the constraint of xs: ^ " ^1  xs :: ([a] j , j >= 0).

In the True branch, we obtain the local assumption ^2 = {nv2 :: (Booli , i = 1)}. Here,
we encounter the application of append. First, we instantiate the sized type for append and
unbind its size variables, as follows:

append :: ([a]m1 \Xi  [a]n1 \Xi  [a]p1, m1 >= 0 \Psi  n1 >= 0 \Psi  p1 = m1 + n1)
We begin by deducing the sized type for (append xs). Here, we need to perform a consistency
test for the application argument. This is validated with the help of the Omega Calculator.

Generating constraints for function application. The constraint for (append xs), after
renaming the size variables of append, is

(fl m1.(m1 >= 0 \Psi  n1 >= 0 \Psi  p1 = m1 + n1) \Psi  m1 = j) \Psi  j >= 0
Next, we deduce the constraint for ((append xs) xs). After consistency test, the constraint is:

(fl n1.(fl m1.(m1 >= 0 \Psi  n1 >= 0 \Psi  p1 = m1 + n1)\Psi 

m1 = j) \Psi  j >= 0 \Psi  n1 = j) \Psi  j >= 0 (ffl2)

Generating constraints for case-expression. To combine both the True and False branches,
we introduce a common new size variable s for both branches and obtain the following
constraint for the case expression:

fl p1.(s = j \Psi  j >= 0 \Psi  i = 0) fi (s = p1 \Psi  ffl2 \Psi  i = 1) (ffl3)

282 CHIN AND KHOO
As s captures the size of both branches, which is the result of evaluating the function f ,
we equate it to k. Lastly, we introduce into ffl3 the inherent constraints of the parameters,
and bind those local size variables existentially, before passing the constraint to the Omega
Calculator for simplification. The final constraint for f is:

flS i, j, k.(i = 0 \Psi  j >= 0 \Psi  k = j) fi (i = 1 \Psi  j >= 0 \Psi  k = j + j)
Remark. Note that the use of type variables for a polymorphic function does not incur a
size constraint. This greatly simplifies the type-inference process, as it removes the need to
compute any size constraint about the list elements. In the case of monomorphic function
over lists or nested lists, we will, in principle, wish to compute both the sizes of the lists and
their elements/nested lists. This may result in time-consuming computation. In practice,
we can choose to assign a size variable from V to the list constructor, but assign ' to its
substructure--the list elements or nested lists. This assignment can either be passed as a
parameter to our type system or be annotated by the user on their programs. During type
inference, any term in BExp that involves ' will be replaced by True. While the resulting
sized type is still safe, this will effectively eliminate size-type inference for substructures,
and can help simplify the formula to be fed to the Omega Calculator. Consequently, the
cost of conducting type inference can be controlled by the "substructure depth" that is to
be analyzed.

7. Size inference for recursive functions
In the type rule for (Rec), we infer the sized type of a recursive function by computing a
safe approximation of the fixed point of the sized type of the function body. This involves
two steps:

1. Deriving the constraint associated with the recursive function body.
2. Computing a safe approximation of the fixed point of the constraint derived in the first

step.

7.1. Deriving constraint of recursive function body
The objective of the first step is to calculate the parent/child relationship of all the recursive
calls as exhibited in the function body. For example, consider performing size inference
on the definition of append in Section 1. We refer the left-hand side of the definition,
append xs ys, as the parent call, and the recursive call append xs\Pi  ys in the right-hand side
of the definition as the child call. The parent/child relationship is thus the change in both
the argument sizes and the size of the application result.

To derive this relation, we create unique size variables for the recursive function at each
of its recursive-call sites. At each of these call sites, we derive a constraint that captures the
call context. Again, consider the definition of append. Let the annotated type of append be
[t]m \Xi  [t]n \Xi  [t]p. During the inference of the recursive call to append, we instantiate the
annotated type of append to [t]i1 \Xi  [t]i2 \Xi  [t]i3, where i1, i2 and i3 are new size variables.

CALCULATING SIZED TYPES 283
A constraint associated with the function body can then be inferred by our type rules, and
simplified by the Omega Calculator to the following disjunctive normal form:

(m = 0 \Psi  n >= 0 \Psi  n = p) fi
(m = i1 + 1 \Psi  i1 >= 0 \Psi  i2 = n \Psi  n >= 0 \Psi  p = i3 + 1 \Psi  i3 >= 0)

Intuitively, this constraint captures the result of a single-step unfolding of a recursive call.
We therefore call it single-step constraint. It consists of two components, separated by a
disjunctive operator (fi):

1. Recursive-call constraint: This expresses the relationship between newly instantiated

size variables of the function at its recursive-call sites and those of its parameters and
results.
2. Termination constraint: This expresses the termination condition of the recursive call.

For the above single-step constraint, the recursive-call constraint is as follows:

U = [m, n, p] \Xi  [i1, i2, i3] : m = i1 + 1 \Psi  i1 >= 0 \Psi  i2 = n \Psi 

n >= 0 \Psi  p = i3 + 1 \Psi  i3 >= 0

whereas the following is the termination constraint:

BI = [m, n, p] : m = 0 \Psi  n >= 0 \Psi  n = p
How do we derive these two pieces of information? From the single-step constraint, the
termination constraint can be retrieved from those disjuncts that do not involve instantiated
size variables of recursive calls, and the recursive-call constraint is retrieved from disjuncts
that depend on those instantiated size variables. It is clear that the recursive-call constraint
contains the parent/child relationship of interest.

In general, our method for deriving parent/child relationship (and recursive-call constraint) works in the presence of partial applications of recursive functions, and multiple
recursive calls. To illustrate this, consider the following variant of binomial function bin\Pi :

bin\Pi  :: Intn \Xi  Intk \Xi  Intp
bin\Pi  n k = case n of 0 \Xi  1

m + 1 \Xi  if k <= 0 ffl k >= n then 1

else let f = bin\Pi  m

in ( f (k i 1)) + ( f k)

Syntactically, (bin\Pi  m) appears to be the only recursive call in the definition. Semantically,
however, both ( f (k i 1)) and ( f k) are real recursive calls the sizes of which are used to
develop the parent/child relation.

We shall demonstrate that, by consistently assigning new size variables to all functions
involved--either directly or indirectly--in recursive applications, and keeping track of the

284 CHIN AND KHOO
final sets of size variables used in the final (fully-applied) calls, we can derive the desired
relationship.

Let's assume that, during size inference, the annotated type of bin\Pi  as appeared in the
type assumption is Int a \Xi  Int b \Xi  Int c. During the inference of the size of the call bin\Pi  m,
size variables of bin\Pi  are renamed (according to the type rules) to Inta1 \Xi  Intb1 \Xi  Intc1.
Assuming that the size variable of m is denoted by m too, the result of inferring the size of
bin\Pi  m, and subsequently f , is as follows:

(bin\Pi  m) :: (Int b1 \Xi  Int c1, m = a1)

f :: (Int b1 \Xi  Int c1, flS b1.flS c1.(m = a1))

Next, for each application of f in the let-body, we instantiate f with new size variables.
For f (k i 1), we have the annotated type of f being Int b2 \Xi  Int c2; for f k, we have
Int b3 \Xi  Int c3. Consequently, we obtain

(k i 1) :: (Int c2, m = a1 \Psi  k i 1 = b2)

f k :: (Int c3, m = a1 \Psi  k = b3)

Notice that size variables of bin\Pi  have been consistently replaced everytime bin\Pi  is either
directly or indirectly applied. This systematic replacement of size variables for bin\Pi  can be
described using a tree structure as follows:

Int a \Xi  Int b \Xi  Int c

renamed to Int a1 \Xi  Int b1 \Xi  Int c1 at call (bin\Pi  m)

renamed to Int b2 \Xi  Int c2 at call ( f (k i 1))
renamed to Int b3 \Xi  Int c3 at call ( f k)

From this tree structure, we can obtain the size variables used by those final recursive calls
of bin\Pi . The annotated types associated with these size variables are underlined. Hence,
the size variables used by f (k i1) are a1, b2, and c2, and that used by f k are a1, b3
and c3. In deriving a single-step constraint, we do not existentially quantify these size
variables. The constraint resulted from inferring the size of the right-hand side of bin\Pi  (and
after simplification) is:

fl m : (n = 0 \Psi  p = 1) fi

(n > 0 \Psi  m + 1 = n \Psi 

((k <= 0 fi k >= n) \Psi  p = 1) fi
((k > 0 \Psi  k < n) \Psi  (m = a1 \Psi  p = c2 + c3 \Psi 

(k i 1 = b2 \Psi  k = b3)))

Now that the single-step constraint for bin\Pi  has been derived, we need to retrieve the
recursive-call constraint. First, as mentioned earlier, we delete all termination constraints

CALCULATING SIZED TYPES 285
from the single-step constraint. This yields:

fl m : (n > 0 \Psi  m + 1 = n \Psi 

((k > 0 \Psi  k < n) \Psi  (m = a1 \Psi  p = c2 + c3 \Psi 

(k i 1 = b2 \Psi  k = b3)))

Next, as this constraint includes constraints of multiple recursive calls, we need to determine
the relationship between the parent call and each individual recursive call, before combining
them to form the final recursive-call constraint.

To do that, we create as many copies of the above constraint as there are final recursive
calls. For each copy, we capture the constraint relating the parent call and a single recursive
child call. This is achieved by keeping the size variables of both the parent call and that
child call, and existentially quantifying the size variables of all other recursive calls. Let

S1 and S2 be these two constraints (for the two recursive child calls of bin\Pi ). We thus have:

S1 = fl b3, c3 : fl m :

(n > 0 \Psi  m + 1 = n \Psi 

((k > 0 \Psi  k < n) \Psi  (m = a1 \Psi  p = c2 + c3 \Psi 

(k i 1 = b2 \Psi  k = b3)))
S2 = fl b2, c2 : fl m :

(n > 0 \Psi  m + 1 = n \Psi 

((k > 0 \Psi  k < n) \Psi  (m = a1 \Psi  p = c2 + c3 \Psi 

(k i 1 = b2 \Psi  k = b3)))

Finally, we form the recursive-call constraint by taking the union of the parent/child relationship of all recursive calls:

Sbin\Pi  = S1 fi S2

7.2. Computing a fixed point of a constraint
Once the recursive-call constraint is derived, the next step is to compute a safe approximation
of the fixed point of the sized type of the recursive function. The least fixed point of a formula
is also called its transitive closure, borrowing the terminology from Kelly et al. [19]. We
shall use the terms least fixed point and transitive closure interchangeably.

Before defining transitive closure of a formula, we define the formula-composition operation as follows:

Definition 4 (Formula-composition operation). Let fi = X \Xi  Y : ffi and j = Z \Xi  W : ffl be
two constraints written in relation form. Suppose that X, Y , Z and W are all sets of identical
cardinality, then

fi ) j def= X \Xi  W : (ffi \Psi  ffl \Psi  (eqC Y Z ))

286 CHIN AND KHOO
Definition 5 (Transitive closure of a formula). Let fi = X \Xi  Y : ffi be a constraint such
that the cardinality of the sets X and Y are identical. The transitive closure of fi is defined
as

fi+ = ffi

Pi>0

fii where fi1 = fi

fin+1 = fi ) fin
In principle, the closure operation provided by the Omega Calculator allows us to compute
the transitive closure of the size constraint. In practice, however, such operation may fall
short in producing the desired transitive closure. There are several reasons for this; one such
reason is that the transitive closure of a constraint may not be expressible as a Presburger
formula.

We thus devise a generalization technique to compute an upper bound of the transitive
closure. The result is a fixed point, but not necessarily the least, of the constraint. We shall
describe the generalization technique in Section 9. In this section, we discuss how a fixed
point is computed.

A fixed point is computed in two steps:

1. Calculating a generalized transitive constraint for the recursive-call constraint.
2. Incorporating termination constraints into the generalized transitive constraint.

We describe these two steps in detail next, using the append function as illustration.
7.2.1. Calculating a generalized transitive constraint. A generalized transitive constraint,
gtc for short, describes a relation (i.e., formula) that holds between a parent call and an arbitrarily nested recursive child call. Given a recursive-call constraint U , a natural candidate
for its gtc is the transitive closure of U , denoted by U +. It is a constraint that holds for
all recursive calls reachable from the initial call by unfolding, and is characterized by the
following important property (adapted from [19]):

Property 2 (Transitive closure). Given two constraints fi and j such that fi = X \Xi 
Y : ffi, j = X \Xi  Y : ffl and X,Y have identical cardinality. If j !P fi !P j+, then the
following holds: fi =P j+ if and only if fi ) j !P fi.

Semantically, U + is an invariant about size, capturing the constraint between the size of
an initial call and that of an arbitrary recursive call. This is built on the idea of unfolding
recursive calls an arbitrary number of times.

The Omega Calculator provides a closure operation for calculating the transitive closure
of a relation. We call this closure operation closure to avoid clashing of terminology. For
the case of append, the Omega calculation yields the following:

closure(U ) = [m, n, p] \Xi  [m1, n1, p1] :

(n1 = n) \Psi  (m + p1 = m1 + p)\Psi 

(0 <= m1 < m) \Psi  (0 <= n)

CALCULATING SIZED TYPES 287
By checking the result of closure(U ) against Property 2, we verify that closure(U ) =
U +.

Techniques for calculating transitive closure have been described in the work on the
Omega Calculator [19]. As the transitive-closure computation can be expensive, in practice,
it is possible to set a limit on the resources needed by the Omega Calculator to perform
transitive closure. Consequently, the closure operation may not produce the desired
result, and we need to approximate the transitive closure by its upper bound, which we call
generalized transitive constraint. This generalization technique will be described in detail
in Section 9. We denote the generalized transitive constraint of a recursive-call constraint
U by U ss.

7.2.2. Incorporating termination constraint. To completely capture the size of a function,
we need to include termination constraints into our final constraint. Let U ss = [a1, . . . , an]\Xi 

[a\Pi 1, . . . , a\Pi n] : ffi be the generalized transitive constraint, and BI = [a1, . . . , an] : ffl be the
termination constraint. There are two scenarios to consider:

1. When the arguments of the initial function call match a degenerate case; i.e., no recursive

call is ever invoked. This situation is already captured by the termination constraint. We
denote it by BI .
2. When the arguments of a nested recursive call match a degenerate case, and thus terminate

the recursive invocation. The constraint is expressed as follows:

BR = [a1, . . . , an] : flb1, . . . , bn. U ss(a1, . . . , an, b1, . . . , bn) \Psi  BI (b1, . . . , bn)

where U ss(a1, . . . , an, b1, . . . , bn) = ffi[b1/a\Pi 1, . . . , bn/a\Pi n].

By combining both scenarios (through disjunction), we are able to calculate the sized
type for recursive functions quite accurately. For the case of append, we have:

B = [m, n, p] : BI (m, n, p) fi BR(m, n, p)

where BI = [m, n, p] : (m = 0) \Psi  (n >= 0) \Psi  ( p = n)

BR = [m, n, p] : fl m1, n1, p1.

(n1 = n) \Psi  (m + p1 = m1 + p)\Psi 

(0 <= m1 < m) \Psi  (0 <= n)\Psi 
(m1 = 0) \Psi  (n1 >= 0) \Psi  ( p1 = n1)=
[m, n, p] : ((m = 0) \Psi  (n >= 0) \Psi  ( p = n))fi

((m >= 1) \Psi  (n >= 0) \Psi  ( p = m + n))

The result is already the sized type of append. But, it can be further simplified by computing its convex hull2: Hull B = [m, n, p] : m >= 0 \Psi  n >= 0 \Psi  p = m + n.

We now state the soundness of our size analysis for the case of recursive function definitions.
Theorem 7 (Soundness--recursive case). Given a recursive function definition x = M of
type ` . If we infer that M has sized type (`, fi), then for any monomorphic instance of M
with denotation d, d satisfies fi under ` .

288 CHIN AND KHOO
Proof: Let v1, . . . , vn be the size variables associated with the input and output of x. Let

BI = [v1, . . . , vn] : ffl be the termination constraint, R = [v1, . . . , vn] \Xi  [v\Pi 1, . . . , v\Pi n] : ffi be
the recursive-call constraint for M, and Rss be the generalized transitive constraint. We
assume that it can be shown that Rss is an upper bound to R+ (this will be proven in
Theorem 8 of Section 9).

First, we note that the constraint fi derived for the recursive function definition is of the
form:

fi = [v1, . . . , vn] : BI (v1, . . . , vn) fifl

(u1, . . . , un.Rss(v1, . . . , vn, u1, . . . , un) \Psi  BI (u1, . . . , un))
Let F be the functional for the recursive function x = M. Then, x is the least upper bound
of the series of functions: xi = \Upsilon ik=1 Fk(,).

Let R be the recursive-call constraint for M. We define a series of constraints as follows:

Si =

iffi

Pk=1

(BI (v1, . . . , vn) fi

fl(u1, . . . , un.Rk(v1, . . . , vn, u1, . . . , un) \Psi  BI (u1, . . . , un)))
with R1 = R and Rk+1 = R ) Rk (refer to Definitions 4 and 5). We can then show that,
by the structural induction hypothesis, x1 satisfies S1 under ` ; by the fixed-point induction
hypothesis, xi satisfies Si under ` for all i > 0. Now, by the admissibility of satisfiability
(Lemma 3), we must have x (= x#) satisfies S# under ` , where

S# = ffi

Pk>0

(BI (v1, . . . , vn) fi fl(u1, . . . , un.Rk(v1, . . . , vn, u1, . . . , un)

\Psi BI (u1, . . . , un)))=
P BI (v1, . . . , vn) fi

fl\Lambda u1, . . . , un.\Lambda ffi

Pk>0 R

k(v1, . . . , vn, u1, . . . , un)\Pi  \Psi  BI (u1, . . . , un)\Pi 

=P BI (v1, . . . , vn) fifl

(u1, . . . , un.R+(v1, . . . , vn, u1, . . . , un) \Psi  BI (u1, . . . , un))!

P BI (v1, . . . , vn) fifl

(u1, . . . , un.Rss(v1, . . . , vn, u1, . . . , un) \Psi  BI (u1, . . . , un))=

P fi

\Delta 

8. Recursive-call invariants
In the previous section, we show that the proposed type system can be used to infer (1)
the sized type of non-recursive functions; and (2) both the recursive-call constraints and
termination constraints of recursive functions. In this section, we show that this information
is adequate for the computation of call invariants of recursive functions.

Computing a recursive-call invariant is almost identical to computing a size relation: it
can be inferred by taking the transitive closure of the initial recursive-call constraint. In
computing a size relation, we take into consideration the output size, whereas in computing

CALCULATING SIZED TYPES 289
a recursive-call invariant, we require only the input relation. Even so, the size relation of
auxiliary calls may still be needed. Hence, it is natural that we compute the size relation
before the recursive-call invariant.

The actual computation is also almost identical to the computation for gtc in Section 7.2.1.
Since we do not need to determine the size relation of the final output and initial input here,
there is no need to incorporate termination constraints in our computation, and the output
size relation is ignored.

As an example, consider the variant of binomial function bin\Pi  defined in Section 7. To derive its recursive-call constraint for the purpose of computing the recursive-call invariant, we
modify the original recursive-call constraints S1 and S2 by "dropping" those computations
related to output size:

U1 = fl p, c2 : S1= fl

p, c2 : fl b3, c3 : flm :
(n > 0 \Psi  m + 1 = n \Psi 
((k > 0 \Psi  k < n) \Psi  (m = a1 \Psi  p = c2 + c3 \Psi 

(k i 1 = b2 \Psi  k = b3))))
U2 = fl p, c3 : S2= fl

p, c3 : fl b2, c2 : fl m :
(n > 0 \Psi  m + 1 = n \Psi 
((k > 0 \Psi  k < n) \Psi  (m = a1 \Psi  p = c2 + c3 \Psi 

(k i 1 = b2 \Psi  k = b3))))

The desired recursive-call constraint for bin\Pi  is then obtained by, again, the union of all
parent/child relationships. After simplification, we obtain the following result:

Ubin\Pi  = U1 fi U2=

[n, k] \Xi  [n\Pi , k\Pi ] : (n\Pi  = n i 1 \Psi  k\Pi  = k i 1 \Psi  1 <= k < n)fi

(n\Pi  = n i 1 \Psi  k\Pi  = k \Psi  1 <= k < n)

Next, we compute the generalized transitive constraint of Ubin:

U ssbin\Pi  = [n, k] \Xi  [n+, k+] :

(k+, 1 <= k < n) \Psi  (k+, 1 <= n+ < n) \Psi 
(k + n+ <= n + k+) \Psi  0 <= k+

The above result can be more succinctly expressed as follows:

U ssbin\Pi  = [n, k] \Xi  [n+, k+] :

max(1, k+) <= n+ < min(n + k+ i k, n)\Psi 

max(k + n+ i n, 0) <= k+ <= min(n+, k)

This result is the same as that obtained using the original definition of bin (Section 2).
Moreover, in their earlier work [4], Chin and Hagiya also managed to obtain the above

290 CHIN AND KHOO
constraint, but relied on several advanced techniques. In contrast, the Omega Calculator
offers tremendous assistance in deriving the invariant. This separation of concerns between
specification and constraint solving enables the analyzer (either the programmer or the
compiler) to concentrate on formulating the constraint, without being unduly concerned
with the operational detail of actually solving the problem.

9. Computing generalized transitive constraints
We have used the closure operator provided in the Omega Calculator to help compute
the transitive closure of the recursive-call constraint U . As noted by the developers of
the Omega Calculator [19], the transitive-closure computation becomes complicated in the
presence of multiple disjuncts. Thus, in practice, there are three possible results from such
calculation using the Omega Calculator:

1. The computation closure(U ) terminates and yields a Presburger formula. In this case,

we check whether the formula is indeed the transitive closure. The check is done using
Property 2 (Section 7.2.1).
2. The computation terminates and yields a constraint that contains the symbol UNKNOWN.3

Here, we replace these UNKNOWN by a safe approximation--the value True, resulting in
an upper bound of the original constraint. (This replacement is done by the operation
upper bound in the Omega Calculator.)
3. The computation aborts because the Omega Calculator is unable to determine the transitive closure.

In the first two cases, when upper bound(closure(U ))4 fails to produce U +, the
result is a lower bound of U + (see Section 6.4 of [18]). We propose to overcome this
deficiency through a generalization strategy, which attempts to compute an upper bound to
the transitive closure as our desired transitive constraint. Specifically, our tactic is to reduce
the number of disjuncts by combining related disjuncts into one, where appropriate.

In the last case, when the computation has been aborted, we reduce the complexity of
the constraint U by reducing the number of dependencies between size variables, thus
eliminating the constraints for some input/output sizes. Elimination of these constraints is
obtained by existentially quantifying the pertinent size variables.

Figure 8 provides an overview of the strategy for computing a generalized transitive
constraint. Basically, Steps 2.a to 2.c describe our first strategy, which reduces the number
of disjuncts; Step 2.d describes our second strategy, which reduces the number of sizevariable dependency.

Reducing the number of disjuncts. A well-known method for combining multiple disjuncts
is the convex hull computation. Computation of convex hull is described in Schrijver's
Theory of Linear and Integer Programming [25].

While this method can sometimes be computationally expensive, and its result too general,
we can reduce both its complexity and generality by an intelligent (and automatic) selection
of suitable disjuncts for combination. We illustrate this with a practical example.

CALCULATING SIZED TYPES 291

Given a constraint U:
Step 0. SecondTime ` False ;

originalU ` U;
Step 1. R ` upper bound(closure(U)).
Step 2. If (R is a formula) , then
Step 2.a. If R == U+, then Uss ` R, halt.
Step 2.b. If SecondTime then Goto Step 2.d.
Step 2.c. If the number of disjuncts in R can be

reduced, then
Step 2.c.1. U ` apply convex-hull operation

heuristically on R,
SecondTime ` True.
Goto Step 1.
Step 2.c.2. Else, Uss ` True, halt.
Step 2.d. Else, U ` originalU ;
Step 2.d.1. if the number of dependencies in U can be

reduced, then
Step 2.d.2. U ` reduce dependency of U,

originalU ` U.
SecondTime ` False.
Goto Step 1.
Step 2.d.3. Else Uss ` True, halt.

Figure 8. Strategy for computing generalized transitive constraints.

Consider the Ackermann function ack. Inferring its recursive-call constraint produces
the following constraint in disjunctive normal form:

[i, j] \Xi  [i\Pi , j\Pi ] : (1 + i\Pi  = i \Psi  j\Pi  = 1 \Psi  j = 0 \Psi  i > 0) fi

(i\Pi  = i \Psi  1 + j\Pi  = j \Psi  i > 0 \Psi  j > 0) fi
(1 + i\Pi  = i \Psi  i > 0 \Psi  j > 0)

The Omega Calculator fails to compute the transitive closure, but instead yields the
following lower bound constraint:

closure(U ) = [i, j] \Xi  [i1, j1] :

(0 <= j1 < j \Psi  1 <= i \Psi  i1 = i) fi (0 <= i1 < i \Psi  1 <= j)fi

(0 <= i1 < i \Psi  0 <= j \Psi  1 = j1) fi (0 <= i1 <= i i 2 \Psi  0 = j)

Our heuristic attempts to identify disjuncts that are "similar" in form for merging. Two
disjuncts are similar if each variable in each disjuncts does not depend on different variables.
The following table illustrates the "similarity" between the disjuncts of closure(U ) stated
above:

292 CHIN AND KHOO

disjuncts
disjunct 1 2, 3, and 4
LB UB LB UB

i i1 i1 i1 -

j j1 - - -
i1 i i - i

j1 - j - -

Legend: LB i Lower boundUB i Upper bound

Notice from the table that, the last three disjuncts of closure(U ) share some similarity:
all have their size variable i bounded below by some constant factor of i\Pi , and vice versa.
Consequently, we combine these disjuncts through a convex-hull operation. Together with
the first disjunct, we obtain the following more general constraint U \Pi :

U \Pi  = [i, j] \Xi  [i1, j1] : (0 <= j1 < j \Psi  1 <= i \Psi  i1 = i)fi

(0 <= i1 < i \Psi  0 <= j)

With the Omega Calculator, we can show that closure(U \Pi ) = U \Pi +. Theorem 8 shows
that U \Pi  is an upper bound of the transitive closure of U , and therefore U ss = U \Pi .

It is interesting to see that this invariant constraint can be used to prove the termination
of Ackermann function. This is in contrast with the method proposed by Hughes, Pareto
and Sabry [16], which requires rewriting the definition of Ackermann function.

What happens if closure(U \Pi ) above is still not a transitive closure? From the empirical
result, we employ the second tactic (reducing the number of dependencies), instead of
repeating the existing one. This is because the closure operation has already made an
attempt to perform the transitive-closure operation. If it is still not possible to obtain the
desired transitive closure or its upper bound, it is usually the case that the result has too
many disjuncts. This makes further reduction of disjunction time-consuming, with little
prospect for obtaining a sufficiently accurate upper bound later on.

Reducing the number of dependencies. The second tactic we adopt is to reduce the number
of dependencies between size variables by ignoring some of these size variables. Instead
of computing a constraint between these size variables, we quantify them existentially. A
common technique we use is to first existentially quantify the size variables associated with
the output of a function, and then existentially quantify the size variables associated with
one of the input.

As an example, consider the function look defined locally in bsearch (Section 2.3). The
usual calculation of the recursive-call invariant yields True, due to its inability to relate the
two inputs. However, by reducing the number of dependencies between the two inputs, we
are able to obtain the more precise recursive-call invariant shown in Section 2.3.

Theorem 8 (Upper bound to transitive closure). U ss is an upper bound of U +.

CALCULATING SIZED TYPES 293
Proof: When computing U ss, the generalization method applies when the closure(U )
either yields a lower bound of U + or the computation aborts. In the first case, we let R1 =
closure(U ), and we have R1 = \Upsilon Pnk=1U k, for some n > 1. Thus, U !P R1. Next, let

S be the result of the convex-hull computation over R1. By the definition of the convex
hull, we must have U !P R1 !P S. In the second case, we existentially quantify some size
variables, resulting in a constraint R2 such that U !P R2. Now, by the monotonicity of the
transitive-closure operator, U + !P S+ and U + !P R+2 . Thus, both S+ and R+2 are upper
bounds of U +. \Delta 

In summary, this technique has worked quite well in practice, and enabled us to generate
useful upper bounds of transitive closures.

Termination. Our method always terminates, because (1) calls to the Omega Calculator
always terminate, (2) Steps 2.a to 2.c are executed only twice, and (3) Step 2.d is iterated a
maximum of n times, where n is the number of size variables available in U .

10. Enhancements
In this section, we point to some enhancements that can be made to the type system.
Dealing with mutual recursion. Our system works well on self-recursive function. To
treat mutual recursion, we encode each set of mutual-recursive functions as a single selfrecursive function, and feed this function to our type system for calculation. As an example,
consider the following odd and even functions:

odd :: Int \Xi  Bool even :: Int \Xi  Bool
odd 0 = False even 0 = True
odd (n + 1) = even n even (n + 1) = odd n

These can be encoded as the following oe function:

oe :: (Booli , Int j ) \Xi  Boolk
oe (True, 0) = False
oe (True, n + 1) = oe (False, n)
oe (False, 0) = True
oe (False, n + 1) = oe (True, n)

Here, the argument to oe is a pair, the first component of which is a tag indicating the
original function that is being called. Thus, applying the function odd to a number n has
been encoded in oe with input (True, n), and applying even to n has been encoded by call
to oe with input (False, n). The sized type computed for oe is shown below; it actually

294 CHIN AND KHOO
calculates the results of both the encoded odd and even functions.

[i, j] \Xi  [k] :

((i = 1) \Psi  (k = 0) \Psi  fl(* : 2* = j \Psi  0 <= j)) fi
((i = 1) \Psi  (k = 1) \Psi  fl(* : 0 = 1 + j + 2* \Psi  1 <= j)) fi
((i = 0) \Psi  (k = 0) \Psi  fl(* : 0 = 1 + j + 2* \Psi  1 <= j)) fi
((i = 0) \Psi  (k = 1) \Psi  fl(* : 2* = j \Psi  0 <= j))

Handling polymorphic types. Since the type system ignores polymorphic data (the types
of which are represented by type variables), it does not incorporate any placeholder in
the size constraint of a polymorphic function to capture the manipulation of such data.
Consequently, when a polymorphic function is instantiated (i.e., it is applied), some size
information is lost during the calculation. For instance, consider the polymorphic projection function fst and snd, which respectively returns the first and second component of a
tupled argument. In order to make use of these functions, we require them to be declared
monomorphically. For the function fst :: (Inti , Int j ) \Xi  Intk, we then derive its size to be
[i, j] \Xi  [k] : i = k.

Note that this does not mean that our type system will be rendered useless when dealing
with polymorphic functions. In practice, it can compute useful size information about these
functions. The size information of the append function provided in Section 1 is one such
example.

We believe that the size of polymorphic data can be better handled by a separate set of
subtyping relations. We are currently working in this direction.

Polymorphic recursion. In inferring the size of recursive function, we systematically instantiate a recursive function with new size variables at each recursive call. This idea is
similar to the treatment of recursive function as polymorphic recursion. Notice that we
only introduce new size variables (and not new un-annotated types) at each recursive calls.
Instantiation at each recursive calls is necessary since different recursive calls can, and do
have different call contexts. Introducing new size variables at each call enables capturing
of these contextual constraints.

It has been shown by Henglein [12] that type inference algorithm for polymorphic recursion is incomplete. For our type system, our type inference algorithm is incomplete for
another reason: the limitation of the Omega's transitive-closure computation over multiple
disjuncts [19].

Higher-order functions. The type system handles a restricted form of higher-order functions. Specifically, it requires that the function body be available in order to infer the
size of a function. Consequently, while it is able to calculate rather accurately the size of
the value resulting from either full or partial application of an already-defined function, it
fails to compute much meaningful information about a function parameter (because of the
unknown function value.)

When performing inference on a function definition with function parameters, our type
system attempts to infer the sizes of these function parameter from their uses within the

CALCULATING SIZED TYPES 295
function definition. It does not extend this inference beyond the definition body, thus
limiting the inference to local (within the function body) constraint. One plausible solution
is to add the notion of higher-order constraint to capture the size of a higher-order function
parameter. A higher-order constraint can be applied when the actual function argument
is available. Manipulation of higher-order constraints is outside the scope of Presburger
formulae, and requires a more sophisticated solvers (or a combination of different solvers)
than the Omega Calculator. This work is currently in progress.

11. Related work
Our work has been inspired by the work on sized types by Hughes, Pareto and Sabry [16].
However, instead of performing type-checking (as they did), we have devised a fixed-point
method for safely inferring the sizes of recursive functions. Furthermore, by assigning a
logical constraint to each expression, we are able to express size information more accurately
in at least two ways: Firstly, their method determines only one bound of the size, fixing
the other bound at either 0 (for data types) or , (for co-data types). On the contrary,
we are able to infer tighter bounds by determining both the lower and upper bounds (for
instance, the recursive-call invariant for bin). Secondly, their method still falls into the
domain of independent attribute analysis. It is thus difficult for their system to express
direct relationship between two inputs (as in the case of lexicographic ordering for the two
inputs of Ackermann function). On the other hand, we have not yet developed inference
technique for handling lazy languages, and thus unable to prove certain correctness aspects
of reactive systems (for example, productivity).

Size information can also be captured using dependent types. This technique is advocated
by Xi and Pfenning [29, 30]. Their type-checking framework is parameterized with constraint domains, of which size information can be described by a domain of natural numbers
with linear equality and inequality constraints. While this has the advantage that analysis efficiency can be controlled by the sophistication of the constraint domain, it requires insightful
type annotations from programmers. Also, their system does not compute the recursive-call
invariants for recursive functions, which are very useful for determining local constraints
within functions that holds throughout all nested calls. As a result, most recursive functions
used by Xi and Pfenning have to be rewritten as local functions. This affects code reuse.

Research into the inference of linear constraints dates back to 1970's, with the seminal
work of Cousot and Halbwachs on imperative languages [7]. Halbwachs has applied linear constraints to synchronization analysis of reactive systems [10, 11]. In their work, a
constraint is described by a set of linear inequalities, and the technique relies on an effective widening operation to obtain fixed point of polyhedra. Our method differs from their
work in the following aspects: (1) Their fixed-point computation includes the base case
computation at each iteration, whereas we exclude that from the generalized-transitiveconstraint computation. Consequently, they can sometimes provide more accurate results,
should the base-case constraint be needed to help restrict the explosion of search space. (2)
Their method captures only input/output size relations, but not recursive-call invariants. We
have found recursive-call invariants to be useful in some applications (e.g., [5]). (3) The
use of polyhedra restricts the solution to linear inequalities, instead of general Presburger

296 CHIN AND KHOO
formulae. Consequently, their solution can sometimes by less accurate than ours, when
the precise solution needs to be expressed using existential quantification. (4) A polyhedra
alone cannot be used to express disjunctive constraints. Thus, it cannot be used to describe
the lexicographical ordering of input to ack function. Jeannet, Halbwachs and Raymond
[17] recently described a way to statically partition a linear relation to improve the precision
of their analysis. This new method is similar to the introduction of disjunction in a linear
relation; further investigation is needed for comparison.

Linear constraints have also received much attention in logic programming community,
particularly in the inference of argument sizes for the purpose of termination analysis of
logic programs [8]. One such approach is pioneered by Ullman and Van Gelder [28], which
has spurred substantial development [2, 3, 23, 26, 27]. In this approach, size information
is represented via a restricted form of predicate inequalities (usually without disjunction of
constraints). For example, constraints of the form ffli^I pi + ssp >= ffl j^J p j were used in
the Mercury system [27], where pk stands for the size of argument k of predicate p, I is
a subset of the set of the input argument positions of predicate p, and J is a subset of the
set of its output argument positions. The aim is to solve for the minimum integer value of
ssp, which serves as an indicator of the maximum possible growth of arguments. Due to the
nature of the termination analysis problem, these proposals for capturing size information
are more restrictive and less modular than sized typing: Firstly, only one bound is computed
instead of two. Secondly, size relationship between inputs are ignored. Lastly, separate size
information for nested data structures is not captured.

King et al. also look into the use of linear constraints in program analysis, both in
computing argument-size relationships [1] and in time-complexity analysis [20]. Their
work is in the context of constraint logic programs. The main purpose of this analysis is to
detect termination. The technique employed is the polyhedra analysis proposed by Cousot
and Halbwachs. While their method supports input/output relationships, it is in first-order
setting, and the lack of a type framework limits their analysis to unnested data structures.
They also do not attempt to capture recursive-call invariants in their analysis; neither do
they employ disjunction in their computation.

12. Conclusion
We have presented a sized typing, its formal rules and associated techniques to support
automatic inference. Our main contributions are:

- We have designed a sized typing that can cater to various optimizations and analyses. It

is powerful since it supports inter-procedural analysis (via size constraints) and captures
loop invariants across recursion (via invariant constraints).
- We have formulated an inference method for sized typing. To the best of our knowledge, this has not been achieved before. Previous works were limited to type-checking
frameworks that require insightful type annotations from programmers.
- A key result of this paper is a new fixed-point method to help infer suitable sized constraints

for recursive functions. We base this on the transitive-closure operation [24], augmented
with selective generalizations to achieve good results (Section 9).

CALCULATING SIZED TYPES 297

- We showed the soundness of our sized type system.
- We confirmed the feasibility of our inference method with the help of the linear arithmetic

constraint-solving capability of the Omega Calculator. Our approach directly benefits
from the practically useful performance of the Omega Calculator. Since it is relatively
easy to replace the constraint solver, our approach is also well-hedged for future improvement in constraint-solving technology.

Our preliminary implementation of sized type inference via the Omega Calculator has
gone very well. Based on our new inference method, we have systematically found
the sized types for a range of examples including those used in this paper. Experiments are being conducted currently to further improve the strategy for computing generalized transitive constraints, so that it can more accurately handle non-linear recursive
definitions.

On the theoretical front, it would be of interest to work on an extension to sized typing
that could better support lazy languages. Under lazy semantics, the size of partial data
structures (that contains ,) should ideally be distinguished from that of ,. The extent to
which this can be achieved will help determine both the accuracy and the utility of sized
typing in the lazy setting. One approach is to use data and co-data types to distinguish finite
from infinite data structures [16], but the onus is on the user to make the distinction.

Acknowledgments
We would like to thank Neil Jones for his critical review of the earlier part of the work, and
Hongwei Xi, Masami Hagiya, Luke Ong, Zhenjiang Hu, and Tyng-Ruey Chuang for their
insightful comments and valuable suggestions to the first draft of this paper. We also thank
the anonymous referees for their comments, some of which were both spirited and detailed,
and helped to bring the article to its present form. Last, but not least, we thank Dana N. Xu
for her help in constructing a prototype of the type system. This work was supported by
the research grants RP3972716 and RP3982693.

Appendix A: An example run of the Omega Calculator
# Omega Calculator [v1.1, Nov 96]:
# # Here, we attempt to find the recursive-call invariant
# # for Ackermann function
#
## Step 1 : Computet he recursive-call constraint
# U1 := {[i,j] -> [i1,j1] :
# exists (k,k1 :
# (i > 0 & j = 0 & i1 + 1 = i & j1 = 1 & k1 = k) or
# (i > 0 & j > 0 & i1 = i & j1+1 = j &
# exists (u : k1 = u)) or (i > 0 & j > 0 &
# exists (u : j1 = u) & i1+1 = i & k1 = k)) } ;
#

298 CHIN AND KHOO
## Step 2 : Compute its transitive closure
# Fst := U1+ ;
#
# Fst ;
{[i,j] -> [i,j1] : 0 <= j1 < j && 1 <= i} union

{[i,j] -> [i1,j1] : 0 <= i1 < i && 1 <= j} union
{[i,j] -> [i1,1] : 0 <= i1 < i && 0 <= j} union
{[i,0] -> [i1,j1] : 0 <= i1 <= i-2}
#
## Step 3 : Check for transitive closure validity fails:
# (Fst compose U1) subset Fst ;
False
## Step 4 : Constraint Generalization using Hull operation
# MFst := {[i,j] -> [i,j1] : 0 <= j1 < j && 1 <= i} union
# hull ({[i,j] -> [i1,j1] : 0 <= i1 < i && 1 <= j} union
# {[i,j] -> [i1,1] : 0 <= i1 < i && 0 <= j} union
# {[i,0] -> [i1,j1] : 0 <= i1 <= i-2}) ;
#
## Step 5 : Compute the transitive closure for MFst
# Snd := MFst+ ;
#
## Step 6 : Check for its validity succeeds.
# (Snd compose U1) subset Snd ;
True
#
# This is the final recursive-call invariant
# Snd ;
{[i,j] -> [i,j1] : 0 <= j1 < j && 1 <= i} union

{[i,j] -> [Out_1,j1] : 0 <= Out_1 < i && 0 <= j}

Notes
1. It is easy to handle arrays with explicit lower and upper bounds. Moreover, multi-dimensional arrays can also

be handled using tuples of size information. For simplicity, this paper does not include formal treatment of
arrays.
2. A convex hull of a constraint flSv1 * * * vn.ffi is the least upper bound of flSv1 * * * vn.ffi which is of the form

flS v1 * * * vn. \Sigma ki=1{a1i v1 + * * * + ani vn <= bi | a1i, . . . , ani, bi ^ Z}. We can use (Hull B) as the size relation
for append, without compromising the accuracy of the result, since it can automatically be checked that (Hull B)*

B.
3. The special constraint UNKNOWN represents one or more additional constraints that are not known to the Omega

Library. Such constraints can arise from the need to approximate during computation, such as the uses of
transitive closure. We refer the readers to the related paper [18] for detail discussion.
4. We note here that when closure(U) returns a formula containing no UNKNOWN symbol, we have upper

bound(closure(U)) = closure(U). Thus, we can always subject the result of a closure operation to
an upper bound operation.

CALCULATING SIZED TYPES 299
References

1. Benoy, F. and King, A. Inferring argument size relationships with CLP(R). In Logic Programming Synthesis

and Transformation. Springer-Verlag, August 1997.
2. Bossi, A., Cocco, N., and Fabris, M. Typed norms. In European Symposium on Programming '92. Lecture

Notes in Computer Science, Vol. 582, Springer-Verlag, 1992, pp. 73-92.
3. Brodsky, A. and Sagiv, Y. Inference of inequality constraints in logic programs. In Proceedings of the 10th

ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems. ACM Press, 1991, pp.
227-240.
4. Chin, W.N. and Hagiya, M. A bounds inference method for vector-based memoisation. In 2nd ACM Intl.

Conference on Functional Programming, Amsterdam, Holland, June 1997. ACM Press, pp. 176-187.
5. Chin, W.N., Khoo, S.C., and Xu, D.N. Deriving pre-conditions for array bound check elimination. In Proceedings of the Second Symposium on Programs as Data Objects, LNCS 2053, O. Danvy and A. Filinski
(Eds.). 2001, pp. 2-24.
6. Cooper, D.C. Programs for mechanical program verification. In Proceedings of the Sixth Annual Machine

Intelligence Workshop, Edinburgh, Machine Intelligence, B. Meltzer and D. Michie (Eds.). Vol. 6, Edinburgh
University Press, 1971, pp. 43-59.
7. Cousot, P. and Halbwachs, N. Automatic discovery of linear restraints among variables of a program. In

Symposium on Principles of Programming Languages, 1978, ACM Press, pp. 84-96.
8. De Schreye, D. and Decorte, S. Termination of logic programs: the never-ending story. Journal of Logic

Programming, 19/20 (1994) 199-260.
9. Glenstrup, A.J. and Jones, N.D. BTA algorithms to ensure termination of off-line partial evaluation. In Andrei

Ershov Second Int. Conference "Perspective of System Informatics". Lecture Notes in Computer Science,
Vol. 1110, Springer-Verlag, 1996.
10. Halbwachs, N. About synchronous programming and abstract interpretation. Science of Computer Programming, Special Issue on SAS'94, 31(1) (1998).
11. Halbwachs, N., Proy, Y.E., and Roumanoff, P. Verification of real-time systems using linear relation analysis.

Formal Methods in System Design, 11(2) (1997) 157-185.
12. Henglein, F. Type inference with polymorphic recursion. ACM Transactions on Programming Languages and

Systems, 15(2) (1993) 253-289.
13. Henkin, L., Monk, J.D., and Tarski, A. Cylindric Algebras: Part I. North-Holland, Amsterdam, 1971.
14. Hilbert, D. and Bernays, P. Grundlagen der Mathematik I. Springer-Verlag, Berlin, 1968.
15. Holst, C.K. Finiteness analysis. In 5th ACM Functional Programming Languages and Computer Architecture

Conference, Cambridge, Massachusetts, August 1991. Lecture Notes in Computer Science, Vol. 523, SpringerVerlag, pp. 473-495.
16. Hughes, J., Pareto, L., and Sabry, A. Proving the correctness of reactive systems using sized types. In 23rd

ACM Principles of Programming Languages Conference, January 1996. ACM Press, pp. 410-423.
17. Jeannet, B., Halbwachs, N., and Raymond, P. Dynamic partitioning in analyses of numerical properties.

In International Symposium on Static Analysis, SAS'99. Lecture Notes in Computer Science, Vol. 1694,
Springer-Verlag, Berlin, 1999, pp. 39-50.
18. Kelly, P., Maslov, V., Pugh, W., Rosser, E., Shpeisman, T., and Wonnacott, D. The Omega library version 1.1.0 interface guide. University of Maryland, College Park, November 1996. http://www.cs.
umd.edu/projects/omega
19. Kelly, P., Pugh, W., Rosser, E., and Shpeisman, T. Transitive closure of infinite graphs and its applications.

Technical Report, UMIACS-TR-95-48, University of Maryland, College Park, April 1994.
20. King, A., Shen, K., and Benoy, F. Lower-bound time-complexity analysis of logic programs. In International

Symposium on Logic Programming, November 1997, Jan Maluszynski (Ed.). MIT Press, Cambridge, MA,
pp. 261-276.
21. Nishimura, S. Parametric polymorphic type inference in constraint form. In Second JSSST Workshop on

Programming and Programming Languages, Japan, March 2000. pp. 38-49.
22. Odersky, M., Sulzmann, M., and Wehr, M. Type inference with constrained types. Theory and Practice of

Object Systems, 5(1) (1999) 35-55.
23. Plumer, L. Termination proofs for logic programs. In Lecture Notes in Artificial Intelligence, Vol. 446.

300 CHIN AND KHOO

Springer-Verlag, Berlin, 1990.
24. Pugh, W. The Omega test: a fast practical integer programming algorithm for dependence analysis. Communications of ACM, 8 (1992) 102-114.
25. Schrijver, A. Theory of Linear and Integer Programming. John Wiley & Sons, New York, 1986.
26. Sohn, K. and Van Gelder, A. Termination detection in logic programs using argument sizes (extended abstract).

In Proceedings of the 10th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,
1991, pp. 216-226.
27. Speirs, C., Somogyi, Z., and Sondergaard, H. Termination analysis of Mercury. In Fourth International

Symposium on Static Analysis, Paris, France, 1997. Lecture Notes in Computer Science, Vol. 1302, pp. 157-
171.
28. Ullman, J.D. and Van Gelder, A. Efficient tests for top-down termination of logical rules. Journal of ACM,

35(2) (1998) 345-373.
29. Xi, H. and Pfenning, F. Eliminating array bound checking through dependent types. In ACM Conference on

Programming Language Design and Implementation, June 1998. ACM Press, pp. 249-257.
30. Xi, H. and Pfenning, F. Dependent types in practical programming. In Symposium on Principles of Programming Languages, January 1999. ACM Press, pp. 214-227.