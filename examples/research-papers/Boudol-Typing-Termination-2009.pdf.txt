

Typing Termination

in a
Higher-Order Concurrent Imperative Language (*)

G'erard Boudol
INRIA Sophia Antipolis

Abstract. We propose means to predict termination in a higher-order imperative and concurrent language `a la ML. We follow and adapt the classical method for proving termination
in typed formalisms, namely the realizability technique. There is a specific difficulty with
higher-order state, which is that one cannot define a realizability interpretation simply by
induction on types, because applying a function may have side-effects at types not smaller
than the type of the function. Moreover, such higher-order side-effects may give rise to computations that diverge without resorting to explicit recursion. We overcome these difficulties
by introducing a type and effect system for our language that enforces a stratification of the
memory. The stratification prevents the circularities in the memory that may cause divergence, and allows us to define a realizability interpretation of the types and effects, which
we then use to establish that typable sequential programs in our system are guaranteed to
terminate, unless they use explicit recursion in a divergent way. We actually prove a more
general fairness property, that is, any typable thread yields the scheduler after some finite
computation. Our realizability interpretation also copes with dynamic thread creation.

1. Introduction
In this work we are concerned with the design of methods to ensure the termination of
programs. Termination is a topic which has received considerable attention, in logics and
computability theory, and then in computer science, and there is a huge literature about
it. A large part of the research activity in this area has focused on designing techniques for
establishing termination of recursive functions over various data structures, and this has
recently been extended to higher-order functional languages (see for instance [11, 18, 32]),
but, as far as I can see, nothing has ever been done to study termination in higher-order
imperative languages `a la ML or Scheme. Our work is a contribution in this direction.
More precisely, we aim at establishing the property that, in such a language, divergence
can only result from using programming constructs that explicitly allow looping (such as
recursion) - a property which seems to be a sensible "programming axiom."

This axiom is generally not true, however, because recursion may be encoded in two
ways in a ML-like, or rather, for that matter, a Scheme-like language. Indeed, it is wellknown that one can define a fixpoint combinator in the untyped (call-by-value) *-calculus.

(*) Work partially supported by the CRE FT-R&D no 46136511, and by the ANR-SETI-06-010 grant.

1

Moreover, as shown long ago by Landin [22], one can implement recursion by means of
circular higher-order references (this is indeed the way it is implemented), like in

\Upsilon  = (let f = (ref *xx) in f := *x((! f )x) ; ! f ) (1)

' rec f (x)(f x)

where we use ML's notations (ref V ) for creating a reference with initial value V , and ! u
for reading the value of the reference u. The well-known method to recover from the first
difficulty, disallowing the ability to derive fixpoint combinators, is to use a type system, but
this is not enough to ensure termination of non-recursive programs in an imperative and
functional language: in a simple type system, the expression \Upsilon  above has type (o/ ! o/ ) (it
can indeed be written in OCaml for instance), but it diverges when applied to any value.
To the best of my knowledge, nothing has ever been proposed to ensure termination in a
higher-order imperative language, thus disallowing implicit recursion via the store. In this
work we show that we can use a type and effect system [24] for this purpose. This is our
main technical contribution.

Among the arguments used to show termination in typed higher-order formalisms, the
realizability method is perhaps the best known, and certainly the most widely applicable.
The realizability technique consists in defining, by induction on the structure of types, 1 an
interpretation of types as sets of expressions, so that

1. the interpretation of a type only contains expressions enjoying the intended computational property (e.g. weak or strong normalizability);

2. typing is sound: a typed expression belongs to the interpretation of its type, or realizes its type.

The main ingredient in the definition of such an interpretation of types is that an expression
M realizes a functional type (o/ ! oe) if and only if its application (M N ) to any argument
N realizing o/ is an expression that realizes oe. A realizability interpretation is therefore
a special case of a "logical relation" [26, 29]. Such a realizability interpretation was first
introduced by Kleene for intuitionistic arithmetic [20], though not for the purpose of proving
termination. The technique was then used by Tait in [35], under the name of "convertibility"
(with no reference to Kleene's notion of realizability), to show (weak) normalizability in
the simply typed *-calculus, and subsequently by Girard (see [19]) with his "candidats
de r'eductibilit'e", and by Tait again [36] (who related it to Kleene's work) to show strong
normalizability in higher-order typed *-calculi. As a matter of fact, this technique seems
to apply to most type theories - see the textbooks [5, 19, 21]. It has also been used for
(functional fragments of) higher-order process calculi, and most notably the ss-calculus
[31, 39].

However, the realizability technique has not been previously used for higher-order imperative (and concurrent) languages: the work that is technically the closest to ours, and
which was our main source of inspiration, is the one by Pitts and Stark [28], who introduced
logical relations to provide means to prove observational equivalence of programs (not to
prove termination), but their language is restricted to offer only storable values of basic
types. The program of Example (1) shows the main difficulty in attempting to define a

1 A more elaborate definition has to be used in the case of recursive types, see [10].

2

realizability interpretation for higher-order imperative languages: to define the interpretation of a type o/ , one should have previously defined the interpretation of the types of values
stored in the memory that an expression of type o/ may manipulate, but these types have
no reason to be strictly smaller than o/ . As another example, unrelated to divergence, one
may imagine a first-order function, say from lists of integers to integers, that reads from
the memory (or import from a module) a second-order function like map, and uses it for
its own computations.

To preclude the circularities in the memory that may cause recursion-free divergence,
our type and effect system stratifies the memory into regions, in such a way that functional
values stored in a given region may only have a latent effect, such as reading or updating
a reference, in strictly "lower" regions, thus rejecting (1) for instance. This stratification
turns out to be also the key to defining a realizability interpretation, by a sophisticated
induction over types and effects. A language with regions usually features a region creation
construct, like (private ae M ), as in [24], or (letregion ae in M ) as in [37]. Although it
does not cause any particular difficulty, 2 we shall not consider this construct, in order to
keep the technical developments simple, but we notice 3 that in our setting this construct
would actually introduce a typed region, (letregion ae : o/ in M ), so that the stratification
corresponds to the fact that ae is not in scope when building the type o/ .

Using the realizability interpretation, we can show that typable sequential programs
written without recursion terminate, that is the "programming axiom" mentioned above.
We shall in fact prove a more general property, namely fairness in a concurrent extension
of the higher-order imperative core. More precisely, we show such a property in the setting of cooperative shared variable concurrency. In the cooperative programming model, a
thread decides, by means of specific instructions, like yield for instance, when to leave its
turn to another concurrent thread, and the scheduling is therefore distributed among the
components. This programming style, and in particular the event-driven model, has been
advocated [1, 6, 7, 27] as the best suited for the purpose of programming some new applications, like web servers, network games or large scale databases, that are open to many
simultaneous connections or requests, and are therefore inherently massively concurrent.

However, this model also has its drawbacks. In particular, if the active thread does
not cooperate, failing to yield the scheduler, then the model is broken, in the sense that no
other component will have a chance to execute. In other words, in cooperative programming,
programs must be cooperative, or fair, that is, they should be guaranteed to either terminate
or suspend themselves infinitely often. Failing to cooperate may happen for instance if the
active thread performs a blocking i/o, or runs into an error, or raises an uncaught exception,
or diverges. Then we have to avoid divergence in some way, while still being able to program
non-terminating applications - any server for instance should conceptually have an infinite
life duration, and should not be programmed to stop after a while. To this end we introduce
a specific construct for programming non-terminating processes, the semantics of which is
that a looping process suspends itself on each recursive call, and the fairness property
we show is that every "instant" - in the sense of synchronous or reactive programming
[9, 15, 25, 33] - is finite, if we start from a well-typed program.

2 The two constructs actually have different semantics, and in particular the letregion construct of [37]
involves region deallocation. For the type safety property in this case, see [14].

3 This observation was made by one of the referees.

3

The paper is organized as follows: we first define the syntax (Section 2) and operational
semantics (Section 3) of our core language, introducing a "yield-and-loop" construct for
programming non-terminating applications, and a new way of managing threads over an
ML-like language. Then (Section 4) we define our type and effect system, where the main
novelty is the region typing context, introducing a stratification into the memory: a region
is given a type that can only involve effects in regions previously supplied with a type.
Next we show our type safety result (Section 5). To this end we introduce a realizability
interpretation of the types and effects, and show that the type system is sound with respect
to this interpretation. We then briefly conclude.

Some related works. We already mentioned that [28] was our main source of inspiration.
This work has been slightly extended in [8], which deals with references that can hold values
of a basic type, or of a reference type. In [2] a logical relation is defined for proving program
equivalence in a language with recursive and quantified types, and the extension to mutable
references is indicated as future work. This paper relies on an indexed model of types
introduced in [4], which has been used in [3] to build a semantical model of reference types.
This model uses a stratification on types, based on the number of computing steps for which
an expression appears to belong to some type. Quoting [2], "this stratification is essential
for handling various circularities," yet it differs from the one we introduce here for dealing
with higher-order store. In [17], a syntactic notion of "imperative realizability" is defined
for an ML-like language, with some restrictions to preclude aliasing, but without taking
into account the fact that an expression of some type may have effects at bigger types, and
therefore this notion does not seem to be well-founded. Some other related works are [12,
23], that follow a denotational approach to higher-order store; in the former, in particular,
a parameterized semantical logical relation is introduced for proving program equivalence
in a higher-order imperative language with recursive types. For simplicity, we are using
here simple types, since we are mainly interested in the problem of defining a realizability
interpretation for a language with higher-order references, which looks independent of the
problem of extending the realizability technique to other enrichments to the simple types,
like with recursive or higher-order types, for which one may try to adapt the syntactical
techniques of [16] for instance. It should be pointed out that, (apart from [17]), none of the
above mentioned works aims at providing means to ensure a termination property.

2. Syntax
2.1 The language

Our core concurrent programming language is an ML-like language, that is a call-by-value *-
calculus extended with imperative constructs for creating, reading and updating references
in the memory, and enriched with concurrent programming constructs, including a threadspawning construct (thread M ), and a "yield-and-loop" value ryM , to be run by applying it
to a void value (). This is similar to a recursive function rec y()M , but we wish to stress the
fact that the semantics are quite different. An expression (ryM ()) represents a recursive
process which yields the scheduler, while unfolding a copy of M (where y is recursively
bound to ryM ) to be performed when the scheduler resumes it. This construct is useful
to build non-terminating processes, which should not hold the scheduler forever. This is
similar to the loop construct of Esterel [9], the body of which is supposed to "take time",
that is to yield the scheduler.

We assume given an infinite set Reg of region names (or simply regions), ranged over by

4

ae. We also assume given an infinite set Loc of abstract memory locations. 4 We let u, v : : :
range over Loc. What we call a reference in this paper is a pair (u; ae), that we will always
denote by uae, of a memory location and a region. The set Ref of references is therefore
Loc * Reg. We shall denote Loc * {ae} as Locae. Finally we assume given an infinite set Var
of variables, ranged over by f , g, x, y, z : : :. The syntax of our core language is as follows:

M; N : : : ::= V | (M N ) expressions

| (refaeM ) | (! M ) | (M := N )
| (thread M )
V; W : : : ::= x | *xM | rec f (x)M | uae | () | ryM values

We require reference creation (refaeM ) to occur in an explicitly given region ae, although
in a type and effect inference approach (see [34, 37]) this could perhaps be inferred. We
denote by Val the set of values. As usual, the variable x is bound in *xM , and similarly
for f and x in rec f (x)M , and for y in ryM . An expression is said to be closed if it
does not contain free variables. We shall use the standard notations for (*xM N ), namely
(let x = N in M ), and (N ; M ) when x is not free in M . We denote by {x 7!V }M the
capture-avoiding substitution of the value V for the free occurrences of the variable x in
M . We shall consider expressions up to ff-conversion, that is up to the renaming of bound
variables and regions.

2.2 Redexes, evaluation contexts, and configurations
The operational semantics mainly consists in reducing expressions into values, which, as
usual, means repeatedly reducing redexes (reducible expressions) 5 inside evaluation contexts
(see [38]). The redexes and evaluation contexts are given as follows:

U := (V W ) | (refae V ) | (! V ) | (V := W ) redexes

| (thread M )

E ::= [] | E[F] evaluation contexts

F := ([] N ) | (V []) | (refae[]) | (! []) frames

| ([] := N ) | (V := [])

Notice that our notion of evaluation context is slightly different from the usual one. We
write for instance [][([]N )][(V [])] what is usually written ((V [])N ). Indeed, an evaluation
context in our sense always has the form of a stack, namely [][F0] * * * [Fn]. As usual, we
denote by E[M ] the expression obtained by putting M into the context E. This is defined
as follows: [][

M ] = M

E[F][M ] = E[F[M ]]

where F[M ] is defined in the obvious way: ([]N )[M ] = (M N ), and so on. It is easy to
check the following property, by induction on the syntax:

4 This should rather be a run-time concept, but it will simplify the technical developments to include

memory addresses in the source language.

5 This terminology is slightly incorrect here since a "redex" in our sense may be irreducible, like (()

V )

or (() := V ). We will not have to explicitly consider faulty expressions, as in [38], and a redex here rather
is an expression that ought to be reduced.

5

Lemma 2.1. For any expression M either M is a value or there is a (unique) evaluation
context E and a redex U such that M = E[U ].

The operational semantics will be defined as a transition relation between configurations,
that involve in particular the current expression to evaluate, and a pool of threads waiting
for execution. In order to get a fair scheduling strategy, we split this pool of threads into
two parts, or more precisely two turns, that are multisets of expressions. Then a configuration is a tuple C = (ffi; M; T; S), where

* ffi is the memory,

* M is the currently evaluated expression (the active thread),

* T is the multiset of threads in the current turn of execution,

* S is the multiset of threads waiting for the next turn.

The memory ffi in a configuration is a mapping from a finite subset dom(ffi) of Ref to the
set Val of values, such that to each memory address is assigned only one region, that is

uae0 2 dom(ffi) & uae1 2 dom(ffi) ) ae0 = ae1
Then we could actually regard a memory as a mapping from a finite set of locations to
pairs made of a region and a value (where the region component is immutable). We define

im(ffi) =def { V | 9uae 2 dom(ffi): V = ffi(uae) }
As usual, we denote by ffi[uae := V ] the memory obtained from ffi by updating the value of the
reference uae (which is assumed to belong to the domain of ffi) by V . We shall suppose given
a function fresh from the set Pf (Ref ) of finite subsets of Ref to Loc such that fresh(R)ae 62 R
for all ae.

As regards multisets, our notations are as follows. Given a set X, a multiset over X
is a mapping E from X to the set N of non-negative integers, indicating the multiplicity
E(x) of an element. We denote by 0 the empty multiset, such that 0(x) = 0 for any x,
and by x the singleton multiset such that x(y) = (if y = x then 1 else 0). Multiset union
E + E0 is given by (E + E0)(x) = E(x) + E0(x). In the following we only consider multisets
of expressions, which are ranged over by S; T : : :

As usual, we shall only consider for execution well-formed configurations. Roughly
speaking, a configuration (ffi; M; T; S) is well-formed, written (ffi; M; T; S) wf , if and only
if all the references occurring in the configuration are bound to a value in the memory
(we omit the obvious formal definition). To evaluate a closed expression M of the source
language, that does not contain memory locations, we start from the initial configuration
(;; M; 0; 0). Such a configuration is obviously well-formed. We are now ready to define the
operational semantics of our language.

3. Operational semantics
3.1 The transition relation
In order to define the transition relation between configurations, we first define the sequential evaluation of expressions. This is given by an annotated transition relation

(ffi; M ) -!e (ffi0; M 0; T; S)

6

(ffi; E[(*xM V )]) -!; (ffi; E[{x 7!V }M ]; 0; 0)
(ffi; E[(rec f (x)M V )]) -!

\Phi  (ffi; E[{x 7!V }{f 7!rec f (x)M }M ]; 0; 0)

(ffi; E[(refaeV )]) --!{

ae} (ffi [ {u

ae 7!V }; E[uae]; 0; 0) u = fresh(dom(ffi))

(ffi; E[(! uae)]) --!{

ae} (ffi; E[V ]; 0; 0) V = ffi(u

ae)

(ffi; E[(uae := V )]) --!{

ae} (ffi[u

ae := V ]; E[()]; 0; 0)

(ffi; E[(thread M )]) -!; (ffi; E[()]; M; 0)

(ffi; E[(ryM V )]) -!; (ffi; (); 0; E[{y 7!ryM }M ])

Figure 1: Operational Semantics (Sequential)

where T and S are the multisets (which actually are either empty or a singleton) of threads
spawned at this step, for execution in the current and next turn respectively, and e is the
effect performed at this step. As usual, (imperative) effects record the regions in which
an expression may operate, either by creating, reading or updating a reference. We shall
also record as an effect the application of a recursive function, which we call unfolding, and
denote \Phi  (assuming that this symbol does not belong to Reg). This is reminiscent of the
fact that a recursive function is implemented by means of a circular reference (which does
not explicitly appear in our operational semantics). In what follows it will not be necessary
to distinguish different kinds of imperative effects, and therefore an effect is simply a (finite)
subset of Reg [ {\Phi }. We denote by Eff the set of effects, that is Eff = Pf (Reg [ {\Phi }). We
should point out here that the effect that we record in a transition is just an annotation,
decorating the transition: there is no constraint associated with this effect, which would
simply be ignored in an implementation. This annotation is, at each step, either empty or
a singleton (which we abusively write as its single element).

The sequential part of the operational semantics is given in Figure 1. This part is
quite standard, except as regards the looping construct ryM . An expression E[(ryM V )]
instantly terminates, returning (), while spawning as a thread the unfolding {y 7!ryM }M of
the loop, in its evaluation context E. One should notice that the thread E[{y 7!ryM }M ]
created by means of the looping construct is delayed to be executed at the next turn,
whereas with the construct (thread M ) the new thread M is to be executed during the
current turn (and moreover creating a new thread does not terminate the evaluation of the
active expression). Then in order to execute immediately (and recursively) some task M ,
one should rather use the following construct:

uyM =def {y 7!ryM }M
For instance one can define (loop M ) = uy(M ; (y())) where y is not free in M , that is
(loop M ) = (M ; ry(M ; (y()))), which repeatedly starts executing M until termination,
and then resumes at the next turn. Notice that if M does not terminate "instantaneously"
(according to the terminology of Esterel), that is, in the current turn, then it may take
several turns for (loop M ) to actually loop. To code a service (repeat M ) that has to execute
some task M at every turn, like continuously processing requests to a server for instance,
one would write:

(repeat M ) =def uy((thread y()) ; M )

Anticipating on the concurrency semantics, we can describe the behaviour of this expression

7

(ffi; M ) -!e (ffi0; M 0; T 0; S0)
(ffi; M; T; S) -!e (ffi0; M 0; T + T 0; S + S0) (Exec)

(ffi; V; N + T; S) -!; (ffi; N; T; S) (Sched 1)
(ffi; V; 0; M + T ) -!; (ffi; M; T; 0) (Sched 2)
Figure 2: Operational Semantics (Concurrent)
as follows: it spawns a new thread N = (ry((thread (y())) ; M )()) in the current turn of
execution, and starts M . Whenever the thread N comes to be executed, during the current
turn, a thread performing (repeat M ) is spawned for execution at the next turn. Notice that
if M does not terminate in the current turn, the two expressions (loop M ) and (repeat M )
are not equivalent.

Our concurrency semantics is defined in Figure 2, which we now comment. We see
from the (Exec) rule that the active expression keeps executing, possibly spawning new
threads, untill termination (we could also formulate this rule using a big-step sequential
semantics). When this expression is terminated, a scheduling operation occurs: if there is
some thread waiting for execution in the current turn, the value returned by the previously
active thread is discarded, and a thread currently waiting is non-deterministically elected
for becoming active, as stated by rule (Sched 1). Otherwise, by the rule (Sched 2), one
chooses to execute a thread that was waiting for the next turn, if any, and simultaneously
the other "next-turn" threads all become "current-turn" ones. If there is no waiting thread,
the execution stops. One should notice that the termination of the active thread may be
temporary. This is the case when the thread is actually performing a looping operation.
Indeed, if we define

yield = (ry()())

then the execution of a thread E[yield] stops, returning (), and will resume executing E[()]
at the next turn. That is, we have

(ffi; E[yield]; T; S) -!; (ffi; (); T; S + E[()])

3.2 Some definitions
Let us first define a kind of reflexive and transitive closure *-!e of the transition relation, as
follows:

C *-!; C

C -!e C00 *-!e

0 C

0

C *---!

e [ e0 C

0

In order to state our main result, we have to introduce some further notations and definitions. We shall denote by -!a the transition relation between configurations that only
involves the active expression, that is, the transition relation defined as -!, but without
using the rules (Sched 1) and (Sched 2). Similarly, we denote by -!c the transitions that

8

occur in the current turn of execution. This is defined as -!, but without using (Sched 2).
We shall denote *-!e a and *-!e c the relations defined in the same way as *-!e , based on -!a
and -!c respectively. The sequences of ! transitions can be decomposed into a sequence of
-!c transitions, then possibly an application of the (Sched 2) rule, then again a sequence
of -!c transitions, and so on. Following the terminology of synchronous or reactive programming [9, 15, 25, 33], a maximal sequence of -!c transitions may be called an instant.
Then a property we wish to ensure is that all the instants in the execution of a program
are finite. However, we shall guarantee this only in the case where there is no divergence
resulting from calling ordinary recursive functions. We regard this kind of divergence as
a programming error, assuming that recursive functions - like for instance sorting a list -
always terminate, but we do not address here the problem of ruling out such errors (see
[11, 18, 32] for some recent advances on this topic). Notice that it would be such an error
to indefinitely spawn new threads, like with

(rec f (x)(thread (f x))())
Then we define:
Definition (Reactivity) 3.1. A well-formed closed configuration C is reactive if for
any maximal sequence of -!c-transitions

C = (ffi0; M0; T0; S0) -!e

0

c * * * ---!

en-1

c (ffin; Mn; Tn; Sn) * * *

then either this sequence is infinite, with 8i 9j ? i: ej = {\Phi }, or 9k: Mk 2 Val & Tk = 0.
In order to show our fairness property, we need to take into account the possible interleavings
of threads in the current turn. We will have in particular to use the fact that if an expression
of the form (M N ) is fair in some sense, then both M and N terminate in the current turn,
including the execution of the threads that these subexpressions may (hereditarily) create.
Since we cannot assume that the threads spawned by M are executed before the ones
spawned by N , we have to take into account in the notion of fairness (of M ) the fact that
the created threads may start in the context of a memory that is not necessarily the result
of executing M or one of its "descendants." Then, given a set M of memories, we define a
transition relation -!c;M which is given as -!c, except that in the case where a scheduling

occurs, the new thread may be started in the context of any memory from M:

(ffi; M ) -!e (ffi0; M 0; T 0; S0)

(ffi; M; T; S) -!e c;M (ffi0; M 0; T + T 0; S + S0)

ffi0 2 M (ffi0; N; T; S) wf
(ffi; V; N + T; S) -!; c;M (ffi0; N; T; S)

We shall also use the relation *-!e c;M, defined in the same way as *-!e c. The following
definition is the crucial one, which will be used in the realizability interpretation. It states
the kind of termination property we are seeking: a program M is said to be fair with respect
to a set M of memories if, unless it calls some recursive functions that cause divergence,

9

all the execution sequences in the current turn starting from M and a memory from M
are finite, including the executions of the threads hereditarily created by M (and started
in the context of an arbitrary memory in M). Moreover these executions terminate in
a cooperative way, that is in a state where the current turn is terminated. The formal
statement is as follows:

Definition (Fairness) 3.2. Given a set M of memories, a closed expression M is fair
w.r.t. M, in notation M %M if and only if, for all ffi 2 M such that C = (ffi; M; 0; 0) is
well-formed, any maximal sequence of transitions

C = (ffi0; M0; T0; S0) -!e

0

c;M * * * ---!

en-1

c;M (ffin; Mn; Tn; Sn) * * *

starting from C is either infinite, with 8i 9j ? i: ej = {\Phi }, or there exists k such that
Mk 2 Val & Tk = 0.

This definition is extended to multisets of expressions, as follows: T %M if and only if
T = M +T 0 implies that the statement of the previous definition holds for C = (ffi; M; T 0; 0).

3.3 Some operational properties
It should be obvious that reduction preserves well-formedness, and that the following properties hold:

Remark 3.3. For any expression M , if (ffi; M; T; S) *-!e a (ffi0; M 0; T 0; S0) and uae 2 dom(ffi0)
with ae 62 e then uae 2 dom(ffi) and ffi0(uae) = ffi(uae).

Remark 3.4. If M %M and (ffi; M; 0; 0) *-!e a (ffi0; V; T; S) where (ffi; M; 0; 0) wf and ffi 2 M
then T %M.

We can show that the definition of fairness for multisets of expressions is compatible with
(finite) multiset union:

Lemma 3.5. If T and S are finite multisets of expressions, that is T = M1 + * * * + Mm and
S = N1 + * * * + Nn then T %M & S %M ) (T + S) %M

Proof (Sketch): for this proof we introduce a refinement of the transition relation -!c;M.
First, we consider configurations where with each expression is associated an index i with
1 6 i 6 m+n. The meaning is that an expression indexed by i originates in Mi if 1 6 i 6 m,
and from Nj if i = j + m with 1 6 j 6 n. We denote by M i the expression M decorated
with index i, and, for any multiset T of expressions, by T i the multiset of the expressions
of T decorated by i. Then the decorated transition relation is defined as follows, where \Theta 
denotes a multiset of decorated expressions, and 1 6 h 6 m + n:

(ffi; M ) -!e (ffi0; N; T; S0)

(ffi; M h; \Theta ; S) h-!e c;M (ffi0; N h; \Theta  + T h; S + S0)

ffi0 2 M (ffi0; M h; \Theta ; S) wf
(ffi; V k; M h + \Theta ; S) h-!; c;M (ffi0; M h; \Theta ; S)

10

It should be clear that if M1 + * * * + Mm + N1 + * * * + Nn = P + T then any sequence
of -!c;M transitions from (ffi; M; T; 0) wf can be lifted into a sequence of decorated -!c;M

transitions, where the steps decorated by i determine a sequence of -!c;M transitions from
(ffi; Mi; 0; 0) (if 1 6 i 6 m) or from (ffi; Nj ; 0; 0) (if i = j + m with 1 6 j 6 n). Then we use
the hypotheses T %M and S %M to conclude.

4. The type and effect system
4.1 Types
The types are

o/; oe; ` : : : 2 T ype ::= 1 | ` refae | (o/ e-! oe)
The type 1 is also denoted unit. As usual, in the functional types (o/ e-! oe) we record the
latent effect, that is the effect a value of this type may have when applied to an argument.
We define the size |o/ | and the set reg(o/ ) of regions that occur in a latent effect in o/ as
follows:

|1| = 0 reg(1) = ;
|` refae| = 1 + |`| reg(` refae) = reg(`)
|o/ e-! oe| = 1 + |o/ | + |oe| reg(o/ e-! oe) = reg(o/ ) [ (e " Reg) [ reg(oe)

We shall say that a type o/ is pure if it does not mention any imperative effect, that is
reg(o/ ) = ;.

In order to rule out from the memory the circularities that may cause divergence in
computations, we assign a type to each region, in such a way that the region cannot be
reached by using a value stored in that region. This is achieved, as in dependent type
systems [5], by introducing the notion of a well-formed type with respect to a type assignment to regions. A region typing context \Delta  is a sequence ae1 : `1; : : : ; aen : `n of assignments
of types to regions. We denote by dom(\Delta ) the set of regions where \Delta  is defined, that
is {ae1; : : : ; aen}. Then we define by simultaneous induction two predicates \Delta  `, for "the
context \Delta  is well-formed", and \Delta  ` o/ , for "the type o/ is well-formed in the context of \Delta ",
as follows:

; `

\Delta  ` `
\Delta ; ae : ` `

ae 62 dom(\Delta )

\Delta  `
\Delta  ` 1

\Delta  ` \Delta (ae) = `

\Delta  ` ` refae

\Delta  ` o/ \Delta  ` oe e " Reg ` dom(\Delta )

\Delta  ` (o/ e-! oe)
For any well-formed region typing context \Delta , we denote by ET (\Delta ) the set of pairs (e; o/ )
of an effect and a type such that e " Reg ` dom(\Delta ) and \Delta  ` o/ . One may observe that if
ae1 : `1; : : : ; aen : `n ` then i 6= j ) aei 6= aej . Moreover, it is easy to see that

\Delta  ` o/ ) reg(o/ ) ` dom(\Delta )
and therefore

\Delta  ` ` refae ) ae 62 reg(`) (2)

Indeed, it is easy to check that if \Delta 0; ae : `; \Delta 1 ` then reg(`) ` dom(\Delta 0) and ae 62 dom(\Delta 0).

11

\Delta  ` \Gamma  \Gamma (x) = o/

\Delta ; \Gamma  ` x : ;; o/

\Delta ; \Gamma ; x : o/ ` M : e; oe \Delta  ` (o/ e-! oe)

\Delta ; \Gamma  ` *xM : ;; (o/ e-! oe)

\Delta ; \Gamma ; x : o/; f : (o/ e

0-! oe) ` M : e; oe

\Delta ; \Gamma  ` rec f (x)M : ;; (o/ e

0-! oe) e

0 = {\Phi } [ e \Delta  ` \Gamma  \Delta (ae) = `

\Delta ; \Gamma  ` uae : ;; ` refae

\Delta  ` \Gamma 
\Delta ; \Gamma  ` () : ;; 1

\Delta ; \Gamma ; y : (1 e-! 1) ` M : e; 1

\Delta ; \Gamma  ` ryM : ;; (1 e-! 1)

\Delta ; \Gamma  ` M : e; (o/ e

00-! oe) \Delta ; \Gamma  ` N : e0; o/

\Delta ; \Gamma  ` (M N ) : e [ e0 [ e00; oe

\Delta ; \Gamma  ` M : e; ` \Delta (ae) = `
\Delta ; \Gamma  ` (refaeM ) : {ae} [ e; ` refae

\Delta ; \Gamma  ` M : e; ` refae
\Delta ; \Gamma  ` (! M ) : {ae} [ e; `

\Delta ; \Gamma  ` M : e; ` refae \Delta ; \Gamma  ` N : e0; `

\Delta ; \Gamma  ` (M := N ) : {ae} [ e [ e0; 1
\Delta ; \Gamma  ` M : e; 1 \Delta  `
\Delta ; \Gamma  ` (thread M ) : e; 1

\Delta  ` o/ \Delta ; \Gamma  ` M : e; oe

\Delta ; \Gamma ; x : o/ ` M : e; oe x 62 dom(\Gamma )

Figure 3: Type and Effect System

The important clause in the definition of well-formedness is the last one: to be wellformed in the context of \Delta , the type (o/ e-! oe) of a function with side-effects must be such
that all the regions involved in the latent effect e are already recorded in \Delta  (this is vacuously
true if there are no such regions, and in particular if the functional type is pure). This is the
way we will avoid "dangerous" circularities in the memory. For instance, if \Delta  ` (o/ e-! oe)
and ae 2 e, then the type (o/ e-! oe) refae is not well-formed in the context of \Delta , thanks to the
remark (2) above.

4.2 Typing
The judgements of the type and effect system for our language have the form \Delta ; \Gamma  ` M : e; o/ ,
where \Gamma  is a typing context in the usual sense, that is a mapping from a finite set dom(\Gamma ) of
variables to types. We omit this context when it has an empty domain, writing \Delta ; ` M : e; o/
in this case. We denote by \Gamma ; x : o/ the typing context which is defined as \Gamma , except for x, to
which is assigned the type o/ . We extend the well-formedness of types predicate to typing
contexts, as follows:

\Delta  ` \Gamma  ,def \Delta  ` & 8x 2 dom(\Gamma ): \Delta  ` \Gamma (x)
The rules of the type and effect system for expressions of the language are given in Figure
3. Most of the typing rules are standard, except for the fact that we check well-formedness
with respect to the region typing context \Delta . One can see that the expression \Upsilon  of Example
(1) in the Introduction is not typable, since to type it, the variable f should have type

(o/ {ae}--! oe)refae, but this type is not well-formed (the fact that \Upsilon  is not typable will be
a consequence of our type safety result). One may notice that some circularities in the

12

memory are still permitted by the type system. For instance, if y is of type (o/ ;-! o/ )refae
then the statement y := *x(*zxy) is typable, introducing a value for y in the memory that
contains the reference y itself. One can also see that some expressions that read above

their type are typable, like ((! uae)N ) where \Delta (ae) = (o/ ;-! oe) and N is of type o/ (a more
interesting example was suggested in the Introduction, namely that of a function from lists
of integers to integers that reads a map function from the memory). As a matter of fact,
it is always safe to read functions of a pure type from the memory - more generally, in an
enriched language, it would always be safe to store a pure value, which does not involve
latent side effects. Moreover, it is easy to see that our type system extends the usual simple
type system for pure functions. As usual, we have the following derived typing for sequential
composition:

\Delta ; \Gamma  ` M : e; o/ ffi; \Gamma  ` N : e0; oe

\Delta ; \Gamma  ` M ; N : e [ e0; oe
Then for instance we have \Delta ; \Gamma  ` M ; () : e; 1 if \Delta ; \Gamma  ` M : e; o/ for some o/ .

In order to state our type safety result, we extend the typing to configurations and,
first, to memories:

\Delta ; \Gamma  ` ffi ,def 8uae: uae 2 dom(ffi) ) ( ae 2 dom(\Delta ) &\Delta ; \Gamma  ` ffi(u

ae) : ;; \Delta (ae)

The typing, or more accurately the effect system, is also extended to multisets of expressions,
with judgements \Delta ; \Gamma  ` T : e, as follows:

\Delta  ` \Gamma 
\Delta ; \Gamma  ` 0 : ;

\Delta ; \Gamma  ` M : e; o/ \Delta ; \Gamma  ` T : e0

\Delta ; \Gamma  ` M + T : e [ e0
Then we define

\Delta ; \Gamma  ` (ffi; M; T; S) : e ,def 9eM ; eT ; eS: 8???!???:

\Delta ; \Gamma  ` ffi &
e = eM [ eT [ eS &
9o/: \Delta ; \Gamma  ` M : eM ; o/ &
\Delta ; \Gamma  ` T : eT & \Delta ; \Gamma  ` S : eS

4.3 Some properties
First we notice some obvious facts:
Remark 4.1.
(i) \Delta ; \Gamma  ` M : e; o/ ) \Delta  ` & (e; o/ ) 2 ET (\Delta )
(ii) \Delta ; \Gamma  ` M : e; o/ ) reg(M ) ` dom(\Delta )
(iii) V 2 Val & \Delta ; \Gamma  ` V : e; o/ ) e = ;
Now we show a subject reduction property, stating, roughly, that typability of configurations
is preserved along the computations. Since the proof of this property follows the usual steps
(see [38]), and is, for a large part, quite standard, we shall not give it in full details.

Proposition (Subject Reduction) 4.2. Let C be a well-formed configuration. If
\Delta ; \Gamma  ` C : e and C *-!

e0 C

0 then e0 ` e and \Delta 0; \Gamma  ` C0 : e00 for some e00 ` e and \Delta 0 ' \Delta .

13

Clearly it is enough to prove
Lemma 4.3. Let C = (ffi; M; T; S) be a well-formed configuration, such that \Delta ; \Gamma  ` C : e
and (ffi; M ) -!

e0 (ffi

0; M 0; T 0; S0). Then e0 ` e and there exist \Delta 0 ' \Delta  and e00 ` e such that

\Delta 0; \Gamma  ` (ffi0; M 0; T + T 0; S + S0) : e00.
Proof (Sketch): by definition of the typing of configurations, we have \Delta ; \Gamma  ` ffi(uae) :
;; \Delta (ae) for all uae 2 dom(ffi), \Delta ; \Gamma  ` M : eM ; o/ for some eM and o/ , and \Delta ; \Gamma  ` T : eT and
\Delta ; \Gamma  ` S : eS with e = eM [ eT [ eS. We sketch the proof of the lemma by cases on the
transition

(ffi; M ) -!

e0 (ffi

0; M 0; T 0; S0) (3)

where M = E[U ]. To deal with the evaluation context E, we use the following property -
similar to the Replacement Lemma in [38]:

Lemma (Replacement). If \Delta ; \Gamma  ` E[M ] : e; o/ then there exist e0 and oe such that
\Delta ; \Gamma  ` M : e0; oe with e0 ` e, and for any N if \Delta 0; \Gamma  ` N : e1; oe with \Delta  ` \Delta 0 and e1 ` e0
then there exist e0 ` e such that \Delta 0; \Gamma  ` E[N ] : e0; o/ .

(The proof is by induction on E, and by cases on F when E = E0[F].) Now we examine a
few cases regarding the reduction (3).

* If the transition is (ffi; E[(*xN V )]) -!; (ffi; E[{x 7!V }N ]; 0; 0), we have, using Remark
4.1(iii): .

..

\Delta ; \Gamma ; x : ` ` N : e0; oe \Delta  ` (` e0-! oe)

\Delta ; \Gamma  ` *xN : ;; (` e0-! oe)

...
\Delta ; \Gamma  ` V : ;; `
\Delta ; \Gamma  ` (*xN V ) : e0; oe

...

\Delta ; \Gamma  ` E[(*xN V )] : eM ; o/
Then we use

Lemma (Substitution). If \Delta ; \Gamma ; x : o/ ` M : e; oe and \Delta ; \Gamma  ` V : ;; o/ then \Delta ; \Gamma  `
{x 7!V }M : e; oe.

(see [38]) to conclude \Delta ; \Gamma  ` {x 7!V }N : e0; oe, and then use the Replacement Lemma. The
proof is similar for U = (rec f (x)M V ) and U = (ryM V ).

* If the transition is (ffi; E[(refaeV ))] -!

e0 (ffi [ {uae 7!V }; E[uae]; 0; 0) where e

0 = {ae} and

14

u = fresh(dom(ffi)), we have

...

\Delta ; \Gamma  ` V : ;; ` \Delta (ae) = `

\Delta ; \Gamma  ` (refaeV ) : {ae}; ` refae

...

\Delta ; \Gamma  ` E[(refaeV )] : eM ; ` refae
Since \Delta (ae) = ` ) \Delta ; \Gamma  ` uae : ;; ` refae, we conclude \Delta ; \Gamma  ` E[uae] : e0; ` refae with e0 ` eM
using the Replacement Lemma, and therefore \Delta ; \Gamma  ` (ffi[{uae 7!V }; E[uae]; T; S) : e0 [eT [eS .
The cases of U = (! uae) and U = (uae := V ) are similar. The other cases are left to the
reader.

5. The termination property
5.1 The realizability interpretation

In this section we define the realizability predicate which, given a well-formed region typing
context \Delta , states that an expression M realizes the effect e and the type o/ in the context
of \Delta , in notation \Delta  |= M : e; o/ . This is defined by induction on e and o/ , with respect to a
well-founded ordering that we now introduce. First, for each region typing \Delta  and type o/ ,
we define the set Reg\Delta (o/ ), which intuitively is the set of regions in dom(\Delta ) that are involved
in a proof that o/ is well-formed, in the case where \Delta  ` o/ . This includes in particular the
regions of Reg\Delta (`) whenever o/ is a functional type, and ` is the type assigned in \Delta  to a
region that occurs in the latent effect of o/ . Then, overloading the notation, we also define
Reg\Delta (ae) ` Reg for ae 2 Reg. The definition of Reg\Delta (o/ ) and Reg\Delta (ae) is by simultaneous
induction on (the length of) \Delta . For any given \Delta , Reg\Delta (o/ ) is defined by induction on o/ , in
a uniform way:

Reg\Delta (1) = ;
Reg\Delta (` refae) = Reg\Delta (`)

Reg\Delta (o/ e-! oe) = Reg\Delta (o/ ) [ Reg\Delta (oe) [ [

ae 2 e

Reg\Delta (ae)

Then Reg\Delta (ae) is given by:

Reg;(ae) = ;

Reg\Delta ;ae:`(ae0) = ( {

ae} [ Reg\Delta (`) if ae0 = ae

Reg\Delta (ae0) otherwise
The mapping Reg\Delta  is extended to sets of regions, and more generally to effects, as usual,
that is

Reg\Delta (e) = [

ae 2 e

Reg\Delta (ae)

It is easy to see that reg(o/ ) ` Reg\Delta (o/ ) ` dom(\Delta ) if \Delta  ` o/ , and that R ` Reg\Delta (R) if
R ` dom(\Delta ). Moreover, if o/ is pure, then Reg\Delta (o/ ) = ;. The following is an equally easy
but crucial remark:

15

Lemma 5.1. If \Delta  ` and ` = \Delta (ae), where ae 2 dom(\Delta ), then Reg\Delta (`) ae Reg\Delta (ae).
(The proof, by induction on \Delta , is trivial, since \Delta ; ae : ` ` implies ae 62 dom(\Delta ) and \Delta  ` `.)
The realizability interpretation is defined by induction on a strict ordering on the pairs
(e; o/ ), namely the lexicographic ordering on (Reg\Delta (e) [ Reg\Delta (o/ ); |o/ |). More precisely, we
define:

Definition (Effect and Type Strict Ordering) 5.2. Let \Delta  be a well-formed
region typing context. The relation OE\Delta  on ET (\Delta ) is defined as follows: (e; o/ ) OE\Delta  (e0; o/ 0) if
and only if

(i) Reg\Delta (e) [ Reg\Delta (o/ ) ae Reg\Delta (e0) [ Reg\Delta (o/ 0), or
(ii) Reg\Delta (e) [ Reg\Delta (o/ ) = Reg\Delta (e0) [ Reg\Delta (o/ 0) and |o/ | < |o/ 0|.
We notice two facts about this ordering:

1. for pure types, this ordering is the usual one, that is (;; o/ ) OE\Delta  (;; oe) if and only if

|o/ | < |oe|;

2. the pure types are always smaller than impure ones, that is (;; o/ ) OE\Delta  (;; oe) if

reg(o/ ) = ; 6= reg(oe).

The strict ordering OE\Delta  is well-founded, that is, there is no infinite sequence (en; o/n)n2N in
ET (\Delta ) such that (en+1; o/n+1) OE\Delta  (en; o/n) for all n. This allows us to use the principle
of noetherian induction with respect to this strict ordering, namely that if a subset X of
ET (\Delta ) has the property\Gamma 

8(e0; o/ 0): (e0; o/ 0) OE\Delta  (e; o/ ) ) (e0; o/ 0) 2 X\Delta  ) (e; o/ ) 2 X
then X = ET (\Delta ). Notice that, by the lemma above, we have in particular (;; `) OE\Delta  (e; o/ )
if ` 2 \Delta (e) = { \Delta (ae) | ae 2 e }.

Our realizability interpretation states that if an expression realizes a type, then in
particular it is fair in the context of suitable memories. As explained in the Introduction,
realizability has to be defined for the types of values that the expression may read or modify
in the memory, and this is what we mean by "suitable." The portion of the memory that
has to be "suitable" may be restricted to the regions where the expression may have a
side-effect (as anticipated by the type and effect system). In the following definition we
write \Delta  |= V : o/ for \Delta  |= V : ;; o/ , and we let, for e 2 Eff :

\Delta  |= ffi _ e ,def 8ae 2 e " dom(\Delta ): 8uae 2 dom(ffi): \Delta  |= ffi(uae) : \Delta (ae)
Clearly, \Delta  |= ffi _ e is vacuously true if e ` {\Phi }.
Definition (Realizability) 5.3. The closed expression M realizes e; o/ in the context of
\Delta , in notation \Delta  |= M : e; o/ , if and only if the following holds, where M = { ffi | \Delta  |= ffi _ e }:

(i) (e; o/ ) 2 ET (\Delta );
(ii) M %M;
(iii) if ffi 2 M is such that (ffi; M; 0; 0) wf then

(ffi; M; 0; 0) *-!

e0

c;M (ffi0; M 0; T; S) ) e0 ` e & ffi0 2 M

16

(iv) with the same hypothesis on (ffi; M; 0; 0) as in (iii), if (ffi; M; 0; 0) *-!

e0

a (ffi0; V; T; 0) then

(a) if o/ = 1 then V = (),
(b) if o/ = ` refae then V 2 Locae,

(c) if o/ = (` e

00-! oe) then 8W: \Delta  |= W : ` ) \Delta  |= (V W ) : e00; oe.

This is extended to open expressions as follows: if fv(M ) ` dom(\Gamma ) where \Gamma  = x1 :
o/1; : : : ; xn : o/n then \Delta ; \Gamma  |= M : e; o/ if and only if

8i 8Vi: \Delta  |= Vi : o/i ) \Delta  |= {x1 7!V1; : : : ; xn 7!Vn}M : e; o/
This definition is well-founded. Indeed, with the statement \Delta  |= ffi _ e the definition of
\Delta  |= M : e; o/ calls for \Delta  |= V : ;; ` where ` = \Delta (ae) for some ae in e (if there is any

such region), and we have seen that (;; `) OE\Delta  (e; o/ ) in this case. If o/ = (` e

00-! oe), the

definition calls for \Delta  |= W : ;; ` and \Delta  |= M : e00; oe. It is clear that, in this case,
(;; `) OE\Delta  (e; ` e

00-! oe) since Reg

\Delta (`) ` Reg\Delta (`

e00-! oe) and |`| < |` e00-! oe|. Moreover, since

Reg\Delta (e00) ` Reg\Delta (` e

00-! oe), it is obvious that (e00; oe) OE

\Delta  (e; `

e00-! oe).

Notice that the hypothesis of the item (iv) of the definition means in particular that
reducing M does not end up performing a call to a recursive process ryN . Then it is not
difficult to see that an expression of the form (ryM V ) realizes any type and effect, and

therefore that \Delta  |= ryM : e; (o/ e

0-! oe) for any \Delta , e etc. We observe that, restricted to

values, the realizability interpretation is quite standard (see [28]).
Remark 5.4. Let V be a closed value. Then \Delta  |= V : o/ if and only if \Delta  ` o/ and
(a) if o/ = 1 then V = (),
(b) if o/ = ` refae then V 2 Locae,
(c) if o/ = (` e-! oe) then 8W: \Delta  |= W : o/ ) \Delta  |= (V W ) : e; oe.
One can also see that as regards the functional fragment of the language, the realizability
interpretation is as one might expect.

5.2 Continuity
In the following we shall prove the soundness of the type system with respect to the realizability interpretation, that is, \Delta ; \Gamma  ` M : e; o/ implies \Delta ; \Gamma  |= M : e; o/ . The proof of
the soundness property proceeds by induction on the inference of the typing judgement
\Delta ; \Gamma  ` M : e; o/ . In the cases where this inference uses a typing assumption about variables,
namely in the cases of functional values, recursive or not, we see that we have to substitute
for the variables some values realizing appropriate types. However, in the case of recursive
functions, that is rec f (x)M (we have seen that the case of ryM is trivial), there is a circularity in this argument, since we would like to show that such an expression realizes its
type by substituting rec f (x)M for f in M , with the hypothesis that the substituted value,
that is rec f (x)M , realizes the same type. Then in this case we shall resort to a continuity
argument, as in [28]: we shall define, for each expression, a set of approximants, and we
shall show that if all approximants of an expression realize a given type, then the expression
realizes this type. Our approach however differs from the one of [28]: we introduce new

17

Appn(x) = {x}
Appn(*xM ) = { *xN | N 2 Appn(M ) }
Appn(rec f (x)M ) = { reckf (x)N | k 6 n & N 2 Appn(M ) }
Appn(recmf (x)M ) = { reckf (x)N | k 6 min(m; n) & N 2 Appn(M ) }

Appn(ryM ) = { ryN | N 2 Appn(M ) }

Appn(M N ) = { (M 0N 0) | M 0 2 Appn(M ) & N 0 2 Appn(N ) }

...

Figure 4: Approximants
auxiliary values in the language,

V ::= * * * | recnf (x)M
for any n 2 N, with the following operational semantics:

(ffi; E[(rec0f (x)M V )]) -!

\Phi  (ffi; E[(rec

0f (x)M V )]; 0; 0)

(ffi; E[(recn+1f (x)M V )]) -!

\Phi  (ffi; E[{x 7!V }{f 7!rec

nf (x)M }M ]; 0; 0)

and the typing

\Delta ; \Gamma ; x : o/; f : (o/ e

0-! oe) ` M : e; oe

\Delta ; \Gamma  ` recnf (x)M : ;; (o/ e

0-! oe) e

0 = {\Phi } [ e

We then define the notion of an approximant of an expression, which is obtained by replacing
each occurrence of rec in the expression with recn for some n. More precisely, we define

App(M ) =def [{ Appn(M ) | n 2 N }
where Appn(M ) is defined by structural induction on M , in an obvious way, see Figure 4. We
define on the extended language, with the auxiliary values recnf (x)M , the approximation
ordering v, as the precongruence generated by

recnf (x)M v rec f (x)M
recnf (x)M v recn+1f (x)M

It is easy to see that this is actually an ordering, that is, an antisymmetric relation. One
should notice that if M v N , then the expression M has exactly the same structure as N ,
where some nodes rec or recn are replaced with reck for some k (6 n). This will allow us to
establish quite easily a direct correspondence between the transitions of an expression and
those of its approximants, without having to reformulate the operational semantics, as this
is done in [28]. The approximation ordering is extended pointwise to memories, that is

ffi0 v ffi ,def ( dom(ffi

0) = dom(ffi) &

8uae 2 dom(ffi): ffi0(uae) v ffi(uae)
and to multisets of expressions, as the congruence with respect to multiset sum generated
by v. It should be clear that the following holds:

18

Remark 5.5. V v W & M v N ) {x 7!V }M v {x 7!W }N
One should also notice that

M 2 App(N ) ) M v N
M v N ) M 2 Val , N 2 Val
M v N ) \Delta ; \Gamma  ` M : e; o/ , \Delta ; \Gamma  ` N : e:o/

Moreover, if M0 2 App(N ) and M1 2 App(N ), then there exists M0 t M1 2 App(N ) which
is a least upper bound of M0 and M1.

Lemma 5.6. M 2 App({x 7!V }N ) ) 9W 2 App(V ) 9N 0 2 App(N ): M v {x 7!W }N 0
Proof: by induction on N .
Now we aim at showing that if M approximates N , and N converges, then M either diverges
or converges towards a value that approximates the one of N , and that if M converges,
then N converges too, towards a better value. A first step is:

Lemma 5.7. Let N be a closed expression.
(i) If (ffi; N ) -!e (ffi0; N 0; T; S) and M v N and ffi0 v ffi then either e = {\Phi } and (ffi0; M ) -!e
(ffi0; M; 0; 0), or there exist M 0 v N 0, ffi1 v ffi0, T 0 v T and S0 v S such that (ffi0; M ) -!e
(ffi1; M 0; T 0; S0).
(ii) If M v N and ffi0 v ffi and (ffi0; M ) -!e (ffi1; M 0; T; S) then either e = {\Phi } and
(ffi1; M 0; T; S) = (ffi0; M; 0; 0), or (ffi; N ) -!e (ffi0; N 0; T 0; S0) for some ffi0, N 0, T 0 and S0 such
that (ffi1; M 0; T; S) v (ffi0; N 0; T 0; S0).
Proof: we prove (i) by case on the transition (ffi; N ) -!e (ffi0; N 0; T; S). Let us just examine
the case N = E[(rec f (x)M V )]. In this case we have e = {\Phi }, ffi0 = ffi, T = 0 = S
and N 0 = E[{x 7!V }{f 7!rec f (x)M }M ], and either M = E0[(rec f (x)M 0V 0)] or M =
E0[(recnf (x)M 0V 0)] with E0 v E, M 0 v M and V 0 v V . In the second case, we have
(ffi0; M ) -!

\Phi  (ffi0; M; 0; 0) if n = 0, and otherwise

(ffi0; M ) -!

\Phi  (ffi0; E

0[{x 7!V 0}{f 7!rec f (x)M 0}M 0; 0; 0)

or

(ffi0; M ) -!

\Phi  (ffi0; E

0[{x 7!V 0}{f 7!rec f n-1(x)M 0}M 0; 0; 0)

and we use Remark 5.5 to conclude. The proof of (ii) is similar.
Let us denote by (ffi; M ) * the fact that there exists a sequence of transitions from (ffi; M; 0; 0)
of the following form:

(ffi; M; 0; 0) *-!e c (ffi0; M 0; T; S) -!

\Phi 

a (ffi0; M 0; T; S)

Then an obvious consequence of the previous lemma is:
Corollary 5.8. Let N be a closed expression and M a set of memories.
(i) If (ffi; N; 0; 0) *-!e c;M (ffi0; V; T; S) and M v N and ffi0 v ffi then either \Phi  2e and (ffi0; M ) *
or there exist W , ffi1, T 0 and S0 such that (ffi0; M; 0; 0) *-!e c;M (ffi1; W; T 0; S0) with W v V ,
ffi1 v ffi0 and T 0 v T .

19

(ii) If M v N and ffi0 v ffi and (ffi0; M; 0; 0) has a *-!e a transition (resp. a *-!e c;M transition)
to (ffi1; V; T; S) then there exist ffi0, W , T 0 and S0 such that (ffi; N; 0; 0) has a *-!e a transition
(resp. a *-!e c;M transition) to (ffi0; W; T 0; S0) with (ffi1; V; T; S) v (ffi0; W; T 0; S0).

as announced, and therefore
Corollary 5.9. Let N be a closed expression. Then for all M and M if M v N then
M %M , N %M.

Now we show that if N approximates M , and M realizes e and o/ , then N realizes the same
effect and type.

Proposition 5.10. Let M be a closed expression. If \Delta  |= M : e; o/ and N v M then
\Delta  |= N : e; o/ .

Proof: by induction on (e; o/ ), ordered by OE\Delta . The points (i) and (ii) of Definition 5.3 are
obvious, using Corollary 5.9. Let M = { ffi | \Delta  |= ffi _ e } and ffi 2 M. If

(ffi; N; 0; 0) *-!e c;M (ffi0; M; T; S)
then

(ffi; M; 0; 0) *-!e c;M (ffi00; N; T 0; S0)

with ffi0 v ffi00 by Lemma 5.7(ii). Since \Delta  |= M : e; o/ , we have ffi00 2 M, hence also ffi0 2 M by
induction hypothesis. This shows (iii) of Definition 5.3. Let us now show (iv). If

(ffi; N; 0; 0) *-!e a (ffi0; V; T; 0)
then by Corollary 5.8(ii) we have

(ffi; M; 0; 0) *-!e a (ffi00; V 0; T 0; 0)
with V v V 0. Since \Delta  |= M : e; o/ , we have \Delta  |= V 0 : o/ . We examine the possible cases.
If o/ = 1 or o/ = ` refae we have V 0 = () or V 0 2 Locae, hence V = V 0, and we are done in

these cases. Otherwise, that is if o/ = (` e

00-! oe), let W be such that \Delta  |= W : `. Then

\Delta  |= (V 0W ) : e00; oe, and therefore, since (V W ) v (V 0W ) we have \Delta  |= (V W ) : e00; oe by
induction hypothesis, because (e00; oe) OE\Delta  (e; o/ ).

The evaluation of an expression can always be approximated by the evaluation of an approximant of the expression, if we chose n large enough in approximating a sub-expression
rec f (x)N as recnf (x)N 0. In the following lemma the definition of approximants is extended
to evaluation contexts, and (pointwise) to memories.

Lemma 5.11. Let N be a closed expression. If (ffi; N ) -!e (ffi0; N 0; T; S) and (ffi00; M 0; T 0; S0) 2
App(ffi0; N 0; T; S) then there exist (ffi0; M ) 2 App(ffi; N ) such that (ffi0; M ) -!e (ffi1; M 00; T 00; S00)
with (ffi00; M 0; T 0; S0) v (ffi1; M 00; T 00; S00).

Proof (Sketch): by case on the transition (ffi; N ) -!e (ffi0; N 0; T; S). Let us just examine
the case N = E[(rec f (x)M V )]. In this case we have e = {\Phi }, ffi0 = ffi, T = 0 = S and
N 0 = E[{x 7!V }{y 7!rec f (x)M }M ]. By Lemma 5.6 there exist W 2 App(V ), E0 2 App(E),

20

N0; N1 2 App(M ) and n such that M 0 v E0[{x 7!W }{f 7!recnf (x)N0}N1], and we also
have M 0 v E0[{x 7!W }{f 7!recnf (x)N }N ] where N = N0 t N1. Then we may let N =
E0[(recn+1f (x)N W )] and ffi0 = ffi00 in this case. The other cases are left to the reader.

An obvious consequence of this lemma is:
Corollary (The Approximation Lemma) 5.12. Let N be a closed expression. Then
for any set M of memories, if (ffi; N; 0; 0) has a *-!e c;M transition (resp. a *-!e a transition) to

(ffi0; N 0; T; S), and if (ffi00; N 00; T 0; S0) 2 App(ffi0; N 0; T; S) then there exists (ffi0; M ) 2 App(ffi; N )
such that (ffi0; M; 0; 0) has a *-!e c;M transition (resp. a *-!e a transition) to (ffi1; M 0; T 00; S00) for

some ffi1, M 0, T 00 and S00 such that (ffi00; N 00; T 0; S0) v (ffi1; M 0; T 00; S00).

Finally we can prove the continuity of the realizability interpretation:
Theorem (Continuity) 5.13.\Gamma 

8N 2 App(M ): \Delta ; \Gamma  |= N : e; o/ \Delta  ) \Delta ; \Gamma  |= M : e; o/

Proof: by induction on (e; o/ ). Let \Gamma  = {x1 : o/1; : : : ; xn : o/n}, and let V1; : : : ; Vn be
any closed values such that \Delta  |= Vi : o/i for all i. We let M = {x1 7!V1; : : : ; xn 7!Vn}M .
Let us show that \Delta  |= M : e; o/ . Since App(M ) 6= ; we have (e; o/ ) 2 ET (\Delta ). Now let
M = { ffi | \Delta  |= ffi _ e }. The fact that M %M is a consequence of Corollary 5.9.

Now let ffi be such that (ffi; M ; 0; 0) is well-formed, and (ffi; M ; 0; 0) *-!

e0

c;M (ffi0; N; T; S),

and let (ffi00; N 0; T 0; S0) 2 App(ffi0; N; T; S). Then by Corollary 5.12 there exist (ffi0; N ) 2
App(ffi; M ) such that

(ffi0; N; 0; 0) *-!

e0

c;M (ffi1; M; T1; S1)

with (ffi00; N 0; T 0; S0) v (ffi1; M; T1; S1). Using Lemma 5.6 one can see that there exist M0 2
App(M ) and Wi 2 App(Vi) for each i such that, if we let M0 = {x1 7!W1; : : : ; xn 7!Wn}M0,
we have N v M0 2 App(M ). By Corollary 5.8(ii) we have

(ffi0; M0; 0; 0) *-!

e0

c;M (ffi01; N 00; T 01; S01)

with ffi1 v ffi01. By Proposition 5.10 we have \Delta  |= Wi : o/i for all i and therefore \Delta  |= M0 : e; o/ ,
hence ffi01 2 M, which implies ffi00 2 M by Proposition 5.10 again. By induction hypothesis,
we then have ffi0 2 M, and this shows (iii) of Definition 5.3.

Now assume that (ffi; M ; 0; 0) *-!

e0

a (ffi0; V; T; 0). To show (iv) of Definition 5.3, we have to

prove \Delta  |= V : o/ . Let V 0 2 App(V ). Then by Corollary 5.12 there exist (ffi0; N ) 2 App(ffi; M )
such that (ffi0; N; 0; 0) *-!

e0

a (ffi1; M; T1; S1) with V 0 v M and 0 v S1. Then M 2 Val and

S1 = 0, and therefore \Delta  |= M : o/ , hence also \Delta  |= V 0 : o/ by Proposition 5.10. If o/ = 1
or o/ = ` refae we have V 0 = () or V 0 2 Locae, hence V = V 0, and we are done in these

cases. Otherwise, that is if o/ = (` e

00-! oe), let W be such that \Delta  |= W : `. Then for any

W 0 2 App(W ) we have \Delta  |= W 0 : ` by Proposition 5.10 again, hence \Delta  |= (V 0W 0) : e00; oe,
and therefore by induction hypothesis \Delta  |= (V W ) : e00; oe for all W such that \Delta  |= W : `.
This shows \Delta  |= V : o/ in this case.

5.3 Soundness and type safety

21

In this section we establish the soundess of the type system with respect to the realizability
interpretation, namely that if an expression has effect e and type o/ in some context, then
in the same context it realizes e and o/ (see [5, 21], and also [26], where soundness is called
"the Basic Lemma"). We shall use a saturation property:

Lemma (Saturation) 5.14.
(i) \Delta ; \Gamma  |= {x 7!V }M : e; o/ ) \Delta ; \Gamma  |= (*xM V ) : e; o/
(ii) \Delta ; \Gamma  |= {x 7!V }{f 7!rec f (x)M }M : e; o/ ) \Delta ; \Gamma  |= (rec f (x)M V ) : e; o/
(ii) \Delta ; \Gamma  |= {x 7!V }{f 7!recmf (x)M }M : e; o/ ) \Delta ; \Gamma  |= (recm+1f (x)M V ) : e; o/
Proof: if \Gamma  = x1 : o/1; : : : ; xn : o/n let V1; : : : ; Vn be such that \Delta  |= Vi : o/i for all i, and let

N = {x1 7!V1; : : : ; xn 7!Vn}(*xM V )
N 0 = {x1 7!V1; : : : ; xn 7!Vn}{x 7!V }M

Since

(ffi; N; 0; 0)

+-!

e0 (ffi

0; N 00; T; S) , (ffi; N 0; 0; 0) *-!

e0 (ffi

0; N 00; T; S)

it is clear that \Delta  |= N : e; o/ if and only if \Delta  |= N 0 : e; o/ . The other cases are similar.
Proposition (Soundness) 5.15. \Delta ; \Gamma  ` M : e; o/ ) \Delta ; \Gamma  |= M : e; o/
Proof: first we notice that the point (i) of Definition 5.3 is a consequence of Remark 4.1(i).
Then we proceed by induction on the proof of \Delta ; \Gamma  ` M : e; o/ . This is trivial if M is a
variable, a reference uae or (), or if the last rule used in this proof is the weakening rule. We
now examine the other cases.

M = *xN. If M = *xN with e = ;, o/ = (` e

0-! oe) and \Delta ; \Gamma ; x : ` ` N : e0; oe,

let W 2 Val be such that \Delta  |= W : `. If \Gamma  = x1 : o/1; : : : ; xn : o/n and \Delta  |= Vi : o/i
for all i we have, by induction hypothesis, \Delta  |= {x1 7!V1; : : : ; xn 7!Vn}{x 7!W }N : e0; oe,
and therefore \Delta  |= ({x1 7!V1; : : : ; xn 7!Vn}M W ) : e0; oe by Lemma 5.14. This shows
\Delta ; \Gamma  |= M : e; o/ in this case, where the points (ii) and (iii) of Definition 5.3 are trivial
since {x1 7!V1; : : : ; xn 7!Vn}M 2 Val .

M = recm f (x)N. If M = recmf (x)N with e = ;, o/ = (` e

00-! oe) and \Delta ; \Gamma ; x : `; f : o/ `

N : e0; oe, where e00 = e [ {\Phi } and \Gamma  = x1 : o/1; : : : ; xn : o/n, we proceed by induction on m.
Let V1; : : : ; Vn and V be closed values such that \Delta  |= Vi : o/i for all i and \Delta  |= V : `. If we
let M = {x1 7!V1; : : : ; xn 7!Vn}M = recmf (x)N where N = {x1 7!V1; : : : ; xn 7!Vn}N , we
have, in the case where m = 0,

(ffi; (M V )) -!

\Phi  (ffi; (M V ); 0; 0)

for any ffi, and therefore \Delta  |= (M V ) : e00; oe, and this shows

\Delta ; \Gamma  |= rec0f (x)N : ;; (` e

00-! oe)

Now if m = k + 1 we have

(ffi; (M V )) -!

\Phi  (ffi; {x 7!V }{f 7!rec

kf (x)N }N ; 0; 0)

22

Since \Delta  |= reckf (x)N : o/ by induction hypothesis (on m), and \Delta ; \Gamma ; x : `; f : o/ |= N : e0; oe
by induction hypothesis (on the inference of the typing judgement \Delta ; \Gamma ; x : `; f : o/ ` N :
e0; oe), we have \Delta ; \Gamma  |= {x 7!V }{f 7!reckf (x)N }N : e0; oe, and we use Lemma 5.14 to conclude in this case.

M = rec f (x)N If M = rec f (x)N with e = ;, o/ = (` e

00-! oe) and \Delta ; \Gamma ; x : `; f : o/ ` N : e0; oe

where e00 = e0 [ {\Phi }, we have \Delta ; \Gamma  |= recmf (x)N : ;; o/ for all m, as we have just seen, and
therefore \Delta ; \Gamma  |= M 0 : ;; o/ for all M 0 2 App(M ) by Proposition 5.10, hence \Delta ; \Gamma  |= M : ;; o/
by Theorem 5.13.

M = ryN. If M = ryN then e = ;, o/ = (1 e

0-! 1) and \Delta ; \Gamma ; y : o/ ` N : e0; 1. If the typing

context \Gamma  is x1 : o/1; : : : ; xn : o/n, and \Delta  |= Vi : o/i for all i then {x1 7!V1; : : : ; xn 7!Vn}M 2Val ,
and therefore the points (ii) and (iii) of Definition 5.3 are trivial in this case. Let M 0 =
{x1 7!V1; : : : ; xn 7!Vn}M , and let W 2 Val be such that \Delta  |= W : 1, that is, W = (). Then

(ffi; (M 0W ); 0; 0) -!; (ffi; (); 0; N 0)

where N 0 = {x1 7!V1; : : : ; xn 7!Vn}{y 7!M }N , and therefore it is obvious that \Delta  |= (M 0W ) :
e0; 1 (the expression (M 0W ) actually realizes any type and effect). This shows \Delta ; \Gamma  |= M :
e; o/ .

Since the context \Gamma  plays no role in the other cases, for simplicity we shall assume for
the rest of the proof that this context is empty, and in particular that M is closed (the proof
for open terms is the same, except that it involves in each case appropriate substitutions
of values).

M = (M0N). If M = (M 0N ) with \Delta ; ` M 0 : e0; (` e2-! o/ ) and \Delta ; ` N : e1; ` where
e = e0 [ e1 [ e2, we first show (ii) and (iii) of Definition 5.3. Let M = { ffi | \Delta  |= ffi _ e }
and Mi = { ffi | \Delta  |= ffi _ ei }. Since e0 ` e we have M ` M0, and therefore, by induction
hypothesis, we have M 0 %M, and similarly N %M. Let ffi 2 M be such that C = (ffi; M; 0; 0)
is well-formed, and let us consider a maximal -!c;M-transition sequence from C:

C = (ffi0; M0; T0; S0) -!"

0

c;M * * * ---!

"n-1

c;M (ffin; Mn; Tn; Sn) * * * (4)

This sequence starts with a (possibly empty) sequence of computations of the function M 0,
that is, there is a (maximal) sequence of -!a-transitions

(ffi0; M 0; T0; S0) -!"

0

a * * * ---!

"m-1

a (ffim; Nm; Tm; Sm) * * * (5)

such that Mm = (NmN ). Since this is also a (possibly not maximal) sequence of -!c;Mtransitions, we see, using M 0 %M, that there are two cases:
(1) If this sequence is infinite, we have "j = {\Phi } for an infinite number of indices j, since
M 0 %M. Then we are done in this case with (ii) of Definition 5.3. Moreover, we have
ffin 2 M0 for all n, by induction hypothesis. Let ae 2 e - e0 and uae 2 dom(ffin). By induction
hypothesis we have ae 62 "i ` e0 for i 6 n. Then by Remark 3.3 we have uae 2 dom(ffi0) and
ffin(uae) = ffi0(uae), hence ffin 2 M. This shows (iii) of Definition 5.3 in this case, where (iv) is
vacuously true.

23

(2) Otherwise, the sequence (5) is finite, of length k, and Nk 2 Val . Then by induction
hypothesis "i ` e0 and ffii 2 M0 for i 6 k, and \Delta  |= Nk : (` e2-! o/ ). Then, as above, if
uae 2 dom(ffii) with ae 2 e - e0, we have uae 2 dom(ffi0) and ffii(uae) = ffi0(uae), and this shows
ffii 2 M for i 6 k. We distinguish two cases:

(2.1) If Sk 6= 0, then Nk = () and Sk = E[{y 7!ryM 00}M 00] with Nk-1 = E[ryM 00]. In
this case the sequence (4) above is

(ffi0; (M 0N ); T0; S0) -!"

0

a * * * ---!

"k-1

a (ffik; (); Tk; (E[{y 7!ryM 00}M 00]N ))

--!"

k

c;M * * * ---!

"n-1

c;M (ffin; Mn; Tn; Sn) * * *

in such a way that

(ffi0; M 0; T0; S0) -!"

0

c;M * * * ---!

"k-1

c;M (ffik; (); Tk; E[{y 7!ryM 00}M 00])

--!"

m

c;M * * * ---!

"n-1

c;M (ffin; Mn; Tn; Sn) * * *

is a maximal sequence of -!c;M-transitions starting from (ffi0; M 0; T0; S0), and we easily
conclude in this case, where (iv) is vacuously true, using the induction hypothesis, that is
\Delta  |= M 0 : e0; (` e2-! o/ ).

(2.2) If Sk = 0, the function M 0 in (M 0N ) evaluates to a value V = Nk such that
\Delta  |= V : (` e2-! o/ ), by induction hypothesis. Then the computation of (M N ) reduces
into (V N ), and continues with the evaluation of the argument N . That is, there exists a
(maximal) sequence of -!a-transitions

(ffik; N; 0; 0) --!"

k

a * * * ---!

"k+l

a (ffik+l; N 0l ; T 0l ; Sk+l) * * * (6)

such that Mk+i = (V N 0i ) and Tk+i = Tk + T 0i for all i. If this sequence is infinite, or ends
up with a yield operation (unfolding a recursive process), then we argue as above, using
the induction hypothesis \Delta  |= N : e1; `. Otherwise, there exists h such that N 0h is a value
W , and Si = 0 for i 6 k + h. Then we have \Delta  |= W : ` and ffii 2 M1 for k < i 6 k + l by
induction hypothesis. By the same argument as above, we have ffii 2 M for i 6 k + l (see
(2) above). In this case the sequence (4) is

(ffi0; (M 0N ); T0; S0) -!"

0

a * * * ---!

"k-1

a (ffik; (V N ); Tk; 0)

--!"

k

a * * * ---!

"k+h

a (ffik+h; (V W ); Tk + T 0h; 0) * * *

Since \Delta  |= V : (` e2-! o/ ) and \Delta  |= W : `, we have, by definition, \Delta  |= (V W ) : e2; o/ , and in
particular (V W ) %M. Then there is a maximal sequence of -!a-transitions

(ffik+h; (V W ); 0; 0) ---!"

k+h

a * * * --!

"n

a (ffin; N 00n-(k+h); T 00n-(k+h); Sn) * * *

such that Mn = N 00n-(k+h) and Tn = Tk + T 0h + T 00n-(k+h) for n ? k + h. If this computation
diverges, or ends up with a yield operation, we argue as above. If for some t we have
N 00t 2 Val with Sk+h+t = 0, then the computation (4) is

(ffi0; (M 0N ); T0; S0) -!"

0

a * * * ---!

"k-1

a (ffik; (V N ); Tk; 0)

--!"

k

a * * * -----!

"k+h-1

a (ffik+h; (V W ); Tk + T 0h; 0)

---!"

k+h

a * * * -!

"t

a (ffit; N 00t ; Tk + T 0h + T 00t ; 0) * * *

24

and the computation continues executing some thread from Tk +T 0h +T 00t . By Lemma 3.5 we
have Tk + T 0h + T 00t %M, and this concludes the proof of (ii) in this case. To see that ffii 2 M
holds for i > k + h + t, one observes that ffii is actually obtained in a sequence of -!c;Mtransitions starting from M 0, or from N , or from (V W ), and we use the induction hypotheses
\Delta  |= M 0 : e0; (` e2-! o/ ) and \Delta  |= N : e1; `, and the fact that \Delta  |= (V W ) : e2; o/ to conclude ffii 2M, thus showing (iii). Finally to show (iv) we use the fact that \Delta  |= (V W ) : e2; o/ .

M = (refaeN). If M = (refaeN ) with \Delta ; ` N : e0; ` and e = {ae} [ e0, where ` = \Delta (ae), we
have \Delta  |= N : e0; ` by induction hypothesis, and in particular N %M0 where M0 = { ffi | \Delta  |=
ffi _ e0 }. Let M = { ffi | \Delta  |= ffi _ e } and ffi 2 M be such that C = (ffi; M; 0; 0) is well-formed.
Let us consider a maximal sequence of -!c;M-transitions from this configuration:

C = (ffi0; M0; T0; S0) -!"

0

c;M * * * ---!

"n-1

c;M (ffin; Mn; Tn; Sn) * * * (7)

This sequence starts with a (possibly empty) sequence of computations of N , that is, there
is a (maximal) sequence of -!a-transitions

(ffi0; N; T0; S0) -!"

0

a * * * ---!

"m-1

a (ffim; Nm; Tm; Sm) * * *

such that Mm = (refaeNm). Since M ` M0 we have N %M, and therefore this sequence
either diverges, or ends up with a yield operation, or there exists k such that Nk is a value
V and Sk = 0. In this latter case, the sequence (7) is

(ffi; (refaeN ); 0; 0) -!"

0

a * * * ---!

"k-1

a (ffik; (refaeV ); Tk; 0)

-!ae a (ffik [ {uae 7!V }; uae; Tk; 0)

where u = fresh(dom(ffik)). By induction hypothesis, we have "i ` e0 and ffii 2 M0. Assuming that ae 62 e0, if vae 2 dom(ffii) we have vae 2 dom(ffi) and ffii(vae) = ffi(vae) by Remark 3.3,
hence ffii 2 M for all i 6 k. Moreover \Delta  |= V : ` by induction hypothesis, and therefore
ffik[{uae 7!V }2M. In this case as well as the others, we conclude as in the case of application.

M = (! N). If M = (! N ) with \Delta ; ` N : e0; o/ refae and e = {ae} [ e0, we have \Delta (ae) = o/ , and
\Delta  |= N : e0; o/ refae by induction hypothesis, hence N +M0 where M0 = { ffi | \Delta  |= ffi _ e0 },
hence also N +M where M = { ffi | \Delta  |= ffi _ e }. Given ffi 2 M such that C = (ffi; (! N ); 0; 0) is
well-formed, any maximal -!c;M-computation of C starts with a maximal -!a-computation

of (ffi; N; 0; 0). If this computation diverges, or end up with a yield operation, we argue as
in the previous cases. Otherwise, there is a value V such that

(ffi; N; 0; 0) *-!" a (ffi0; V; T; 0)

and therefore \Delta  |= V : o/ refae by induction hypothesis, that is, V is a reference uae. By
induction hypothesis, we also have ffi0 2M0. If ae 62 ", we have uae 2dom(ffi) and ffi0(uae) = ffi(uae),
hence \Delta  |= ffi0(uae) : o/ in this case since ffi 2 M. Otherwise, that is if ae 2 ", we have ae 2 e0 and
therefore \Delta  |= ffi0(uae) : o/ also in this case since ffi 2 M0. This shows (iv) of Definition 5.3 in
this case. The remaining points of Definition 5.3 are shown as above. The proof is similar
in the case where M = (M 0 := N ), and similar to the case of M = (M 0N ). The details are
left to the reader, as well as the case of M = (thread N ).

25

We can now prove our main result, namely a Type Safety theorem, which improves upon
the standard statement:

Theorem (Type Safety) 5.16. Let M be a closed typable expression that does not
contain any memory address. Then the configuration C = (;; M; 0; 0) is reactive. Moreover,
any configuration reachable from C is reactive.

Proof: we prove that if C = (ffi; M; T; S) is a closed, well-formed typable configuration
then

(i) C is reactive, and
(ii) any configuration reachable from C is reactive.
We first show (i). We have \Delta  ` ffi and \Delta ; ` M : eM ; o/ for some type o/ , \Delta ; ` T : eT and
\Delta ; ` S : eS with e = eM [ eT [ eS. Let M = { ffi0 | \Delta  |= ffi0 _ e }. Then ffi 2 M, and M %M,
and also N %M for any N in T by Proposition 5.15. Then M + T %M by Lemma 3.5. This
shows that C is reactive.

(ii) If C *-!

e0 C

0 then C0 is closed and well-formed, and by Proposition 4.2 C0 is typable, and

therefore reactive by (i).
An obvious consequence of this result is that typable closed expressions written without
recursion - either ordinary, that is rec f (x)M , or using the yield-and-loop construct ryM -
always terminate. This solves a problem raised in [13], regarding the typing of termination
leaks. More precisely, we have shown in [13] that the typing of secure information flow (see
[30] for a survey on this topic) may be largely improved if we know that some expressions
(specifically, the branches in a conditional branching) are terminating, but we left open the
problem of designing a typing technique to ensure termination. Our type and effect system
provides a solution: for the language of [13], a typable expression that does not exhibit an
unfolding effect \Phi  is guaranteed to terminate. The language of [13] did not contain the
construct ryM . To detect also the kind of non-termination introduced by this construct,
the typing system should involve a specific (latent) effect in the typing of of ryM , similar
to \Phi . We have not considered this refinement here, but it should be obvious that making
explicit the yield effect does not cause any difficulty.

6. Conclusion
We have proposed a way of ensuring termination in a higher-order imperative language, and
more generally to ensure a fairness property in a concurrent (cooperative) version of the
language. Our main technical contribution consists in designing a type and effect system
for our language, that supports an extension of the classical realizability technique to show
our termination property, namely fairness. We think that, for expressions not involving the
yield-and-loop construct, we could show termination also under a preemptive discipline,
that is for the standard interleaving semantics of threads.

Our study was limited to a very simple core language, and clearly it should be extended
to more realistic ones. The synchronization constructs of synchronous, or reactive programming for instance [15, 25, 33] should be added. We believe this should not cause any
difficulty. Indeed, the problem with termination in a concurrent higher-order imperative
language is in the interplay between functions and store, and between recursion and thread
creation.

26

Another topic that deserves to be investigated is whether the restriction imposed by
our stratification of the memory is acceptable in practice. We believe that the restriction
we have on the storable functional values is not too severe (in particular, any pure function
can be stored), but obviously our design for the type system needs to be extended, and
experimented on real applications, in order to assess more firmly this belief. We notice
also that our approach does not seem to preclude the use of cyclic data structures. In
OCaml for instance, one may define cyclic lists like - using standard notations for the list
constructors:

(let rec x = cons(1; x) in x)

Such a value, which is a list of integers, does not show any effect, and therefore it should
be possible to extend our language and type and effect system to deal with such circular
data structures.

Finally it would be interesting to see whether showing termination in a concurrent,
higher-order imperative language may have other applications than the one which motivated
our work (we have mentioned such an application in the previous section). For instance,
"transactional concurrency" relies, like cooperative concurrency, on an implicit fairness
assumption, namely that any transaction is supposed to terminate. It would therefore
be interesting to adapt our termination argument to this setting. One may also wonder
whether our realizabity interpretation could be generalized to logical relations (and to richer
notions of type), in order to prove program equivalences for instance.

References

[1] A. Adya, J. Howell, M. Theimer, W.J. Bolosky, H.R. Douceur, Cooperative

task management without manual stack management or, Event-driven programming
is not the opposite of threaded programming, Usenix ATC (2002).

[2] A. Ahmed, Step-indexed syntactic logical relations for recursive and quantified types,

ESOP'06, Lecture Notes in Comput. Sci. 3924 (2006) 69-83.

[3] A.J. Ahmed, A.W. Appel, R. Virga, A stratified semantics of general references

embeddable in higher-order logic, LICS'02 (2002) 75-86.

[4] A.W. Appel, D. McAllester, An indexed model of recursive types for foundational

proof-carrying code, ACM TOPLAS Vol. 23 No. 5 (2001) 657-683.

[5] H. Barendregt, Lambda Calculi with Types, in Handbook of Logic in Computer

Science, Vol. 2 (S. Abramsky, Dov M. Gabbay & T.S.E. Maibaum, Eds.), Oxford University Press (1992) 117-309.

[6] R. von Berhen, J. Condit, E. Brewer, Why events are a bad idea (for highconcurrency servers), Proceedings of HotOS IX (2003).

[7] R. von Berhen, J. Condit, F. Zhou, G.C. Necula, E. Brewer, Capriccio: scalable threads for Internet services, SOSP'03 (2003).

[8] N. Benton, B. Leperchey, Relational reasoning in a nominal semantics for storage,

TLCA'05, Lecture Notes in Comput. Sci. 3461 (2005) 86-101.

27

[9] G. Berry, G. Gonthier, The ESTEREL synchronous programming language: design, semantics, implementation, Sci. of Comput. Programming Vol. 19 (1992) 87-152.

[10] L. Birkedal, R. Harper, Relational interpretation of recursive types in an operational setting, Information and Computation Vol. 155 No. 1-2 (1999) 3-63.

[11] F. Blanqui, J.-P. Jouannaud, A. Rubio, Higher-order termination: from Kruskal

to computability, LPAR'06, Lecture Notes in Comput. Sci. 4246 (2006) 1-14.

[12] N. Bohr, L. Birkedal, Relational reasoning for recursive types and references,

APLAS'06, Lecture Notes in Comput. Sci. 4279 (2006) 79-96.

[13] G. Boudol, On typing information flow, Intern. Coll. on Theoretical Aspects of

Computing, Lecture Notes in Comput. Sci. 3722 (2005) 366-380.

[14] G. Boudol, Typing safe deallocation, ESOP'08, Lecture Notes in Comput. Sci. 4960

(2008) 116-130.

[15] F. Boussinot, FairThreads: mixing cooperative and preemptive threads in C, Concurrency and Computation: Practice and Experience, Vol. 18 (2006) 445-469.

[16] K. Crary, R. Harper, Syntactical logical relations for polymorphic and recursive

types, G. Plotkin's Festschrift, ENTCS Vol. 172 (2007) 259-299.

[17] J.-C. Filli^atre, Verification of non-functional programs using interpretations in

type theory, J. Functional Programming Vol. 13 No. 4 (2003) 709-745.

[18] J. Giesl, R. Thiemann, P. Schneider-Kamp, Proving and disproving termination

of higher-order functions, FROCOS'05, Lecture Notes in Comput. Sci. 3717 (2005)
216-231.

[19] J.-Y. Girard, Y. Lafont, P. Taylor, Proofs and Types, Cambridge Tracts in Theoretical Computer Science 7, Cambridge University Press (1989).

[20] S.C. Kleene, On the interpretation of intuitionistic number theory, J. of Symbolic

Logic, Vol. 10 (1945) 109-124.

[21] J.-L. Krivine, Lambda-Calcul: Types et Mod`eles, Masson, Paris (1990). English

translation "Lambda-Calculus, Types and Models", Ellis Horwood (1993).

[22] P.J. Landin, The mechanical evaluation of expressions, Computer Journal Vol. 6

(1964) 308-320.

[23] P.B. Levy, Possible world semantics for general storage in call-by-value, CSL'02,

Lecture Notes in Comput. Sci. 2471 (2002) 232-246.

[24] J.M. Lucassen, D.K. Gifford, Polymorphic effect systems, POPL'88 (1988) 47-57.
[25] L. Mandel, M. Pouzet, ReactiveML, a reactive extension to ML, PPDP'05 (2005)

82-93.

[26] J.C. Mitchell, Foundations for Programming Languages, MIT Press (1996).

28

[27] J. Ousterhout, Why threads are a bad idea (for most purposes), presentation given

at the 1996 Usenix ATC (1996).

[28] A. Pitts, I. Stark, Operational reasoning for functions with local state, in HigherOrder Operational Techniques in Semantics (A. Gordon and A. Pitts, Eds), Publications of the Newton Institute, Cambridge Univ. Press (1998) 227-273.

[29] G. Plotkin, Lambda-definability and logical relations, Memo SAI-RM-4, University

of Edinburgh (1973).

[30] A. Sabelfeld, A.C. Myers, Language-based information-flow security, IEEE J. on

Selected Areas in Communications Vol. 21 No. 1 (2003) 5-19.

[31] D. Sangiorgi, Termination of processes, Math. Struct. in Comp. Science Vol. 16

(2006) 1-39.

[32] D. Sereni, N. Jones, Termination analysis of higher-order functional programs,

APLAS'05, Lecture Notes in Comput. Sci. 3780 (2005) 281-297.

[33] M. Serrano, F. Boussinot, B. Serpette, Scheme fair threads, PPDP'04 (2004)

203-214.

[34] J.-P. Talpin, P. Jouvelot, The type and effect discipline, Information and Computation Vol. 111 (1994) 245-296.

[35] W.Tait, Intensional interpretations of functionals of finite type I, J. of Symbolic

Logic 32 (1967) 198-212.

[36] W.Tait, A realizability interpretation of the theory of species, Logic Colloquium,

Lecture Notes in Mathematics 453 (1975) 240-251.

[37] M. Tofte, J.-P. Talpin, Region-based memory management, Information and Computation Vol. 132, No. 2 (1997) 109-176.

[38] A. Wright, M. Felleisen, A syntactic approach to type soundness, Information

and Computation Vol. 115 No. 1 (1994) 38-94.

[39] N. Yoshida, M. Berger, K. Honda, Strong normalisation in the ss-calculus, Information and Computation Vol. 191 No. 2 (2004) 145-202.

29