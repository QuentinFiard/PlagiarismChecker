

Minimal State Graph Generation

\Lambda 

A. Bouajjani, J-C. Fernandez,

N. Halbwachs, P. Raymond
IMAG/LGI (U.A. CNRS 398)
B.P. 53 - 38041 Grenoble, France

C. Ratel
Merlin Gerin / SES
38050 Grenoble, France

May 4, 1994

Abstract
We address the problem of generating a minimal state graph from a program, without building the whole state graph. Minimality is considered here with respect to bisimulation. A
generation algorithm is derived and illustrated. Applications concern program verification
and control synthesis in reactive program compilation.

1 Introduction
This paper concerns the problem of explicitly building a state graph from a program, a formula
or any implicit expression of a transition system. Such state graphs are used in program verification ("model checking" [7, 14], behavioral equivalence [12]) and compiling (control structure
synthesis [1, 6]).

A crucial problem with state graph generation is the size of the graph, which can be prohibitive. This size can be large not only because of the intrinsic complexity of the program, but
also because the graph contains a lot of states which are in some sense equivalent. Of course, the
proliferation of equivalent states increases the number of states, but its consequences are even
worse concerning the number of transitions: if any state has n equivalent copies, the number of
transitions can be multiplied by n2.

To explain the problem, let us give a very simple example, which will be considered throughout
the paper. The following program could be a boolean abstraction of a more realistic program:

x := true; y := false; w:= true; read(a);
loop

write(x or y);
z := y; y := (x and w) or a;
x := not z; w := (not w and x) or y; read(a);
end;

Now, assume we want to prove some property about the output of this program (e.g., the
temporal formula 3:(x . y)). We consider the program as a transition system, whose states are
the values of the variables when the output is written. The standard model checking procedure

\Lambda This work was partially supported by ESPRIT Basic Research Action "SPEC"

1

Figure 1: The complete state graph of the example

Figure 2: The minimal state graph of the example
consists of building first the state graph of this transition system: There are 4 initial states,
depending on the initial values of a and z:

q0 = (x y z w a) ; q1 = (x y z w a) ; q2 = (x y z w a) ; q3 = (x y z w a)
These states, in which x . y is true, can lead to two other states:

q4 = (x y z w a) ; q5 = (x y z w a)
Continuing the simulation, we get a graph with 10 states and 20 transitions, represented by
Fig. 1. In two of these states (filled in grey on the figure), x . y is false. The point is that, as
we are only interested in the value of x . y, we could have considered only a graph with 5 states
and 7 transitions (represented by Fig. 2), which is equivalent to the former with respect to that
value -- in the sense that any property involving only x . y is true on one graph if and only if
it is true on the other.

Some solutions have been given to this problem, by applying reduction algorithms [11, 13, 9].
However, these algorithms can only be applied once the graph has been entirely generated. It is
often the case that a tremendous amount of time and memory is necessary to generate a graph,
which afterward reduces to a very simple one. It even happens that an infinite graph reduces to
a finite one. So, it would be interesting to reduce the graph during the generation, on one hand
to improve the performance of the generation, and on the other hand, to allow finite state graph
generation from infinite systems.

This paper presents and illustrates an algorithm performing this task, when the equivalence
considered on states is a bisimulation. The basis of our algorithm is twofold:

2

ffl It combines the construction of the graph of accessible states with its reduction by bisimulation. The termination of the algorithm imposes that the quotient of the whole space of
(accessible and inaccessible) states by the bisimulation be finite.

ffl It considers state classes instead of states. For this purpose, a formal calculus over state

classes is needed.

A typical case where the algorithm works well is when the state space is a boolean space
ftrue; falsegn. Then the termination condition is obviously satisfied, and state classes may be
handled as boolean formulas.

A first version of the algorithm has been published in [3]. In this paper, it will be formally
derived from a fixpoint characterization.

The paper is organized as follows: Section 2 describes the theoretical framework of the
problem; it shows that we have to combine the computations of a least fixpoint -- the set of
accessible states -- and a greatest fixpoint -- the bisimulation. From this formal setting, the
algorithm is derived in Section 3. It is illustrated by an example in Section 4. In conclusion,
we shall give some experimental results about the implementation of the algorithm, which has
been inserted in the new compiler of the language Lustre [10]. All the proofs are given in the
Appendix.

2 Theoretical Framework
2.1 Transition systems
In the following, we consider a transition system: S = (Q; !; qinit), where Q is a set of states,
!` Q \Theta  Q is a transition relation, and qinit is the initial state.

The pre- and post-condition functions, from 2Q to 2Q are defined as usual:

pre(X) = fq 2 Q j 9q0 2 X such that q ! q0g
post(X) = fq 2 Q j 9q0 2 X such that q0 ! qg

Transition systems are generally represented implicitly, for instance by programs. We shall
call a state graph of S any explicit representation of S. For instance, the "program"

n := 0 ; loop n := n+1 end
represents the transition system, whose states are memories M 2 (fng 7! ZZ ), whose initial
state is [0=n] (the function associating 0 with n) and whose transition relation is the function
*M:M [(M (n) + 1)=n]. Its (infinite) state graph is [0=n] ! [1=n] ! [2=n] ! \Delta  \Delta  \Delta 

2.2 Partitions, Partial Partitions
Throughout the paper, we consider partitions of non-empty subsets of Q instead of equivalence
relations on Q. Moreover, we are only interested in partial partitions containing accessible classes
(a class is accessible if it contains an accessible state, see formal definition in section 2.3). Let P
be the set of sets of pairwise disjoint non-empty subsets of Q or partial partitions of Q:

P = fss ` 2Q j 8X; Y 2 ss; X; Y 6= ; and X " Y 6= ; ) X = Y g
ss 2 P is a partition if and only if it covers Q (Q = [fX j X 2 ssg).

3

In the remainder of the paper, X; Y; Z; T; : : : will be metavariables for subsets of Q, ss for partial
partitions on Q and ae for partitions or partial partitions on Q.

We consider two orders:

ffl the usual inclusion relations over 2Q and 22

Q, which make them complete lattices, with

usual least upper bound ([) and greatest lower bound (") operators.
ffl the refinement relation, noted v, over P:

ss v ss0 iff 8X 2 ss; 9X0 2 ss0 such that X ` X0
With this order, P is a complete lattice, with greatest lower bound operator u and least
upper bound operator t, defined as follows:

u

i ssi = fT 6= ; j T = "i Xi and Xi 2 ssig

t

i ssi = ufss j 8i; ssi v ssg

with the infimum ; and supremum fQg. Notice that the sublattice of partitions is also a
complete lattice with this order, but with infimum ffqg j q 2 Qg.

We denote by [q]ss the class of the partial partition ss containing the state q, if such a class
exists. Let press; postss denote the pre- and post-condition functions corresponding to a partial
partition ss; these functions are overloaded

ffl from 2Q to P: press(X) = f[q]ss j q 2 pre(X)g ; postss(X) = f[q]ss j q 2 post(X)g
ffl from P to P: press(ss0) = [fpress(X) j X 2 ss0g ; postss(ss0) = [fpostss(X) j X 2 ss0g

2.3 Accessible states and bisimulations
Let F be a monotonic total function, either from 2Q to 2Q or from 22

Q to 22Q and let G be a

monotonic total function from P to P. We denote by

ffl _ss:F (ss) the least fixpoint of F with respect to the ordering `
ffl *ss:G(ss) the greatest fixpoint of G with respect to the ordering v

Accessibility: A state q is accessible in S if and only if either q = qinit or there exist
q1; q2; : : : ; qn in Q, such that qinit ! q1 ! : : : ! qn ! q. With the above notations, the
set Acc0 of accessible states of S is Acc0 = _X :(fqinit g [ post(X )).

More generally, if X and X0 belong to a partial partition ss, X 0 is said to be directly ss-accessible
from X (noted X ss!X0) iff there exists q 2 X; q0 2 X0, such that q ! q0. ss-accessibility is defined
to be the transitive closure of that direct ss-accessibility. We define Acc(ss) to be the set of classes
which are ss-accessible from [qinit]ss. We have

Acc(ss) = _ss0:(f[qinit]ssg [ postss(ss0))
Notice that the fact that a set X belongs to Acc(ss) doesn't mean that X contains an accessible
state.

4

Splitting: We define a hierarchy of (overloaded) splitting functions:

ffl from 2Q \Theta  2Q to P: 8X; Y 2 2Q; split(X; Y ) = fX " pre(Y )g [ fX n pre(Y )g. In other

words, this function splits its first argument into the set of states which respectively can
or cannot lead to the second argument.

ffl from 2Q \Theta  P to P: 8X 2 2Q; 8ss 2 P; split(X; ss) = ufsplit(X; Y ) j Y 2 ssg
ffl from P \Theta  P to P: 8ss; ss0 2 P; split(ss; ss0) = [fsplit(X; ss0) j X 2 ssg

Stability: X (respectively ss0) is said to be stable with respect to ss, if and only if
fXg = split(X; ss) (resp. ss0 = split(ss0; ss)). A partition ae is a bisimulation if and only if it
is stable with respect to itself. By extension, a partial partition ss is a bisimulation if and
only if it is stable with respect to itself and if all accessible states belong to a class of ss,i.e.
Acc0 ` [fX j X 2 ssg. The greatest bisimulation refining a given partition aeinit is the greatest
fixpoint (with respect to v) of the function *ae:(aeinit u split(ae; ae)).

The reduction of a transition system S = (Q; !; qinit) according to a bisimulation ss of Q, is
the transition system S=ss = fss; ss!; [qinit]ssg,where

ffl [qinit]ss is the class of the initial state in ss

ffl X ss!Y if and only if 9q 2 X; q0 2 Y such that q ! q0

Let us state some useful properties of split:
Proposition 1 (properties of split)

1. split(ae; ae) v ae

2. ae1 v ae2 =) split(ae1; ae) v split(ae2; ae)
3. ae1 v ae2 =) split(ae1; ae1) v split(ae2; ae2)
4. ae1 v ae2 =) split(Acc(ae1); ae1 ) v split(Acc(ae2); ae2 )

2.4 Reduction of a transition system
Our problem of explicit generation of the minimal transition system of S, with respect to the
greatest bisimulation included in a given equivalence aeinit, may be solved by two different strategies. For each of them, we propose a set of equations and the method for solving it.

Strategy 1: Accessible states followed by refinement

Let

F1 = *X:(fqinitg [ post(X))
G1 = *(ae; ss):(aeinit u split(ae u ss; ae u ss))

F1 is monotonic on 2Q. *ss:G1(ae; ss) is monotonic on P from proposition 1, part 3. The
first strategy consists of computing first the set of accessible states of S, that is the least
fixpoint Acc0 = _X :F1 (X ) in the lattice 2Q. Then, this set is refined according to the
greatest bisimulation included in aeinit, i.e., the greatest fixpoint ss = *ss:G1(fAcc0 g; ss) is
computed in P. The result is the quotient of S by the greatest bisimulation included in
aeinit. This is the classical method, which leads to state explosion if Acc0 is generated by
enumeration.

5

Strategy 2: Refinement followed by accessible classes

Let

G2 = *ae:(aeinit u split(ae; ae))

F2 = *(ss; ae):([qinit]ae [ postae(ss))

G2 is monotonic on P from proposition 1, part 1. *ss:F2(ss; ae) is monotonic on 22

Q. The

second strategy consists of refining first the whole set of states, without regards to accessibility, i.e., the greatest fixpoint ae = *ae:G2(ae) is computed first in P. Then, the set of
accessible classes, i.e., the least fixpoint Acc(ae) = _ss:F2 (ss; ae) is computed in the lattice
22

Q. So, the quotient Q=ae of the whole set of states by the bisimulation is computed first,

and then the set Acc(ae) of accessible states of the quotient system S=ae is explored. Here,
the drawback is that the bisimulation is uselessly built over unaccessible states.

So, it is interesting to look for intermediate methods, combining the exploration of accessible
states or classes with the computation of the bisimulation. First notice that the problem is not
trivial, since we have to combine the computation of least and greatest fixpoints. In the following
section, we shall describe an algorithm, derived from strategy 2.

We first state that both strategies lead to the same graph, as far as accessible states are
concerned:

Definition: Let S = (Q; !; qinit) and S0 = (Q0; !0; q0init) be two transition systems, AccS
and AccS0 be their respective sets of accessible states. S and S0 are said to be isomorphic
if and only if there exists a bijection f from AccS to AccS0 such that f (qinit) = q0init and
q1 ! q2 () f (q1) !0 f (q2).

Proposition 2 Let ss = *ss:G1(fAcc0 g; ss) and ae = *ae:G2 (ae). The two transition systems

S1 = (ss; ss\Gamma !; [qinit]ss) and S2 = (Acc(ae); ae\Gamma !; [qinit]ae)
are isomorphic.

This proposition results from the three following lemmas:
Lemma 1 If ae1 and ae2 are two bisimulations on Q such that ae1 u fAcc0 g = ae2 u fAcc0 g, then
S=ae1 and S=ae2 are isomorphic.

Lemma 2 The two following partitions of Acc0 are equal:

A = fAcc0 g u *ae:G1 (fAcc0g; ae) B = fAcc0 g u *ae:G2 (ae)

Lemma 3

1. For any ss; ae 2 P; fAcc0 g u split(ss; ae) = split(fAcc0 g u ss; ae)
2. For any ss; ae 2 P, split(fAcc0g u ss; ae) = split(fAcc0 g u ss; fAcc0g u ae)

6

3 Generation Algorithm
In this section, we shall derive a generation algorithm based on "stepwise refinement on accessible
classes" strategy described below, which is an intermediate method between the two previous
ones. The idea is twofold: On one hand, we only need to refine accessible classes, and on the
other hand, only accessible classes can involve such a refinement.

3.1 Basic solution
Stepwise refinement on accessible classes
Let

G3 = *(ss; ae):aeinit u split(ss; ae)
G3(Acc(ae); Acc(ae)) is monotonic in ae by proposition 1. The strategy consists in computing the
greatest fixpoint *ae:G3(Acc(ae); Acc(ae)), in the lattice P. Given an initial partition, we compute
the set of its accessible classes, and then we apply one step of refinement of this set with respect
to itself, and so on, until all the classes are stable. Thus, *ae:G3(Acc(ae); Acc(ae)) is the limit of
the sequence:

ae0 = aeinit aen+1 = aeinit u split(ssn; ssn)
where

ssn = Acc(aen)

The correctness of this strategy is expressed by the following proposition:
Proposition 3 Let ae0 = *ae:(aeinit u split(Acc(ae); Acc(ae))). Then ae0 = Acc(ae0) = Acc(ae).
The proof of this proposition is based on the following lemma:
Lemma 4 For any ae,

1. split(Acc(ae); ae) = split(Acc(ae); Acc(ae)).
2. Acc(aeinit u split(ae; ae)) = Acc(aeinit u split(Acc(ae); ae)).
We can derive a naive algorithm for computing the limit ae0:

ae = aeinit ; ss = Acc(ae);
while ae 6= split(ss; ss) do

ae = split(ss; ss);
ss = Acc(ae);
od

3.2 Optimizations
The algorithm derived so far presents some obvious weaknesses:

ffl At each step, the whole set of accessible classes is computed again. In the following, we

shall try to compute ssn+1 by updating ssn.

7

ffl In the same way, aen+1 can be derived from aen without considering again all of its classes.

When a class X is split during the computation of aen, one can remember that its predecessors are likely to be split at the next step.

ffl The fact that a class belongs to Acc(aen) does not imply that it contains an accessible state.

As a consequence, it may happen that a class is uselessly considered and split.

Let us consider the elementary split operation applied in updating the partition: Given a
partition ae and a set ss of accessible classes of ae, each class X in ss is replaced by split(X; ae).
Now, two cases can occur:

- either X is effectively split into several subclasses. Then, for deciding which of these

subclasses remain accessible, we have to examine the predecessor classes of X. On the
other hand, these predecessors are precisely the classes which may have to be split at the
next step (in the sense of the fixpoint computation sequence) because of the splitting of X.
So, we can first try to split the predecessor classes of X.

- or X is stable with respect to ae (fXg = split(X; ae)). Then, all the states in X lead exactly

to the same classes in ae. So, if X contains one accessible state q, all the classes accessible
from X are accessible from q, and thus contain one accessible state. So, it is a good point
to update the set ss of accessible classes.

The set oe of stable classes with respect to the current partition ae is interesting not only for
determining accessible classes. Accessible unstable classes (those in ss n oe) are those which are
likely to be split, and the knowledge of oe also provides a good termination criterion: when all
accessible classes are stable, the generation is complete. A class enters oe when it is found to be
stable when trying to split it; it must be removed from oe whenever one of its successor classes is
split into several subclasses.

Taking these remarks into account, let us define the sequences fae00ng; fss00ng and foeng as follows:

ae000 = aeinit ; oe0 = ;

ss0n = [qinit]ae00n oe0n = ;
ssp+1n = sspn [ postae00n (oepn)
oep+1n = oepn [ fX 2 ssp+1n j split(X; ae00n) = fXgg
ss00n = limp sspn

oen = limp oepn
ae00n+1 = ae00n u split(ss00n n oen; ae00n) [ oen [ (ae00n n ss00n)
At each step, ss00n contains all the classes accessible from a class of oen, which is the set of all
the classes of ss00n which are stable with respect to ae00n.

Proposition 4 If ss00n = oen then ae00n is the limit ae00 of the sequence fae00i g and Acc(ae00) = ae0.

8

We get the following generation algorithm, which is exactly the one presented in [3], and which
is proved correct in the Appendix:

ae = aeinit; ss = f[qinit]aeg; oe = ;;(1)
while ss 6= oe do(2)

choose X in ss n oe;(3)
let ss0 = split(X; ae);(4)
if ss0 = fXg then(5)

oe := oe [ fXg; ss := ss [ postae(X);(6)
else(7)

ss := ss n fXg;(8)
if 9Y 2 ss0 such that qinit 2 Y then ss := ss [ fY g;(9)
oe := oe n preae(X);(10)
ae := (ae n fXg) [ ss0;(11)
fi(12)
od(13)

4 Example
Let us consider come back to our example program:

x := true; y := false; w:= true; read(a);
loop

write(x or y);
z := y; y := (x and w) or a;
x := not z; w := (not w and x) or y; read(a);
end;

We start with the initial partition:

f(a; w; x; y; z) j x . y = trueg; f(a; w; x; y; z) j x . y = falseg
In the following, classes are represented by their characteristic formulas. The initial partition
will be noted:

aeinit : C1 = fx . yg C2 = f:x ^ :yg
Standard rules of weakest precondition provide the precondition of a class X, with respect to
the body of the loop:

pre(X) = (X # a)[((:w ^ x) . y) = w][:z = x][((x ^ w) . a) = y][y = z]
where X # a = 9a0X[a0 = a] = X[false = a] . X[true = a]
So, pre(C1) = f(x ^ w) . :y . ag ; pre(C2) = f(:x . :w) ^ y ^ :ag
The successive partitions built by the algorithm are illustrated on figure 3.

9

Step 1: The only reachable class is C1, since x is initially true. For splitting it, we compute:

C1 ^ pre(C1) = fx ^ (:y . w . a) . (y ^ a)g
C1 ^ :pre(C1) = f(:x . :w) ^ y ^ :ag
and

C1 ^ pre(C1) ^ pre(C2) = false
C1 ^ pre(C1) ^ :pre(C2) = C1 ^ pre(C1)
C1 ^ :pre(C1) ^ pre(C2) = C1 ^ :pre(C1)
C1 ^ :pre(C1) ^ :pre(C2) = false
So, C1 is split into:

C11 = fx ^ (:y . w . a) . (y ^ a)g
C12 = f(:x . :w) ^ y ^ :ag
and only C11 is accessible. We have:

pre(C11) = f(x ^ w) . :y . ag;
pre(C12) = f((x ^ w) . a) ^ yg

Step 2: For splitting C11, we compute:

C11 ^ pre(C11) = fx ^ (:y . w . a) . (y ^ a)g = C11
So, C11 is split into:

C111 = C11 ^ pre(C11) ^ pre(C12) = f((x ^ w) . a) ^ yg
C112 = C11 ^ pre(C11) ^ :pre(C12) = fx ^ :yg
and only C112 is accessible. We have:

pre(C111) = f(x ^ w) . ag
pre(C112) = f(:x . :w) ^ :y ^ :ag

Step 3: For splitting C112, we compute:

C112 ^ pre(C111) ^ pre(C112) = false
C112 ^ pre(C111) ^ :pre(C112) = x ^ :y ^ (w . a)
C112 ^ :pre(C111) ^ pre(C112) = x ^ :y ^ :w ^ :a
C112 ^ :pre(C111) ^ :pre(C112) = false
So, C112 is split into:

C1121 = C112 ^ pre(C111) ^ :pre(C112) = x ^ :y ^ (w . a)
C1122 = C112 ^ :pre(C111) ^ pre(C112) = x ^ :y ^ :w ^ :a
and only C1121 is accessible. We have:

pre(C1121) = f(:x . :w) ^ :y ^ :ag
pre(C1122) = f:x ^ :y ^ w ^ :ag

10

Step 4: When considering C1121, we find it stable; all of its states lead to C111 and only to
C111. So, C111 becomes accessible.

Step 5: C111 is found stable

C111 ` pre(C111) ^ pre(C12)
So, C12 becomes accessible.

Step 6: In turn, C12 is found stable, all of its states leading to C2, which becomes accessible.
Step 7: C2 is split into:

C21 = C2 ^ pre(C111) = :x ^ :y ^ a

C22 = C2 ^ pre(C1121) ^ pre(C1122) = f:x ^ :y ^ :a ^ wg
C23 = C2 ^ pre(C1121) ^ :pre(C1122) = f:x ^ :y ^ :a ^ :wg
So C12 is removed from stable classes. We have:

pre(C21) = pre(C23) = f(:x . :w) ^ y ^ :ag

pre(C22) = false
which means that C22 is accessible from nowhere.

Steps 8, 9, 10: C12 is considered again, and found stable, all of its states leading to both C21
and C23, which become accessible. C21 and C23 are also found stable, respectively leading to
C111 and C1121.

Since all accessible classes have been considered and found stable, the algorithm stops. Figure 4 shows the resulting graph, which is exactly the one considered in the introduction (Fig. 2).

This example shows several features of our algorithm:

ffl In such a boolean case, the symbolic computation of classes is fully automatic. As a matter

of fact, we have an implementation of the algorithm, which performs efficient boolean
computations by means of "binary decision diagrams" [4, 2].

ffl Only 7 classes have been identified, and 5 accessible classes have been considered for splitting. However, the usual enumeration algorithm (strategy 1) builds a state graph with 10
states shown in Figure 1.

ffl Compared with strategy 2, notice that two classes (C1122 and C22) have not been considered

for splitting, since they are not accessible from the initial state. In this example, these
classes happen to be stable; however, in the general case, building the quotient of the
whole set of states, without regards to accessibility, can give rise to many irrelevant classes.

11

C111

C12

C12C12C12
C12

C2C2

C2C2
C1122C1121

C112
C111
C12C11

C2
C1

Final result

Result of step 7Result of steps 4, 5, 6Result of step 3
Result of step 2Result of step 1Initial partition

C23 C22 C21

C21C22C23

Figure 3: The successive partitions built by the algorithm

12

:x ^ :y ^ :a ^ :w

:x ^ :y ^ a
(:x . :w) ^ y ^ :a((x ^ w) . a) ^ y
x ^ :y ^ (w . a)

C23

C21
C12C111C1121

Figure 4: The reduced graph of the example
5 Conclusion
We have formally derived an algorithm combining generation and reduction methods. In our
opinion, this algorithm is interesting for program verification: a state graph with several thousand (or even infinitely many) states may be reduced to one with a small number of states by
considering an equivalence relation.

Applying our algorithm to program verification appears very close to what is now called "symbolic model checking" [5]. For instance, consider the problem of proving that a state property
OE is true in any accessible state of a transition system. Applying our algorithm to this problem
consists in starting from the initial partition fOE; :OEg (verifying that the initial states belong to
OE), then refining the class OE into fOE ^ pre(:OE) ; OE ^ :pre(:OE)g (verifying that the initial states
belong to OE ^ :pre(:OE)) and so on, until getting only one stable class which characterizes all
the states which cannot lead to :OE. So, if OE is invariant, the resulting automaton has only one
state. In that case, the algorithm is exactly the backward construction of the "greatest invariant
included in OE".

Concerning other applications, the algorithm has been implemented in the new version of the
Lustre compiler [10]. Lustre [6] is a synchronous declarative language designed for programming reactive systems. Its compiler produces efficient code by synthesising the control structure
of the code as a finite automaton. Initially proposed for the Esterel language [1], this technique
consists of an exhaustive enumeration of the states of boolean memories involved in a program.
The first Lustre compiler performed this construction in a standard way, and often produced
huge automata with many equivalent states (states with identical code). Our algorithm has been
first designed in order to remedy this problem.

Of course, one must be able to compute the function pre and intersections of classes, and to
decide the inclusion of classes. Such a symbolic computation is achievable in the boolean case,
with reasonable average cost, by means of Binary Decision Diagrams [4, 8].

We have not presented complexity measures. In our opinion, a comparison of theoretical
complexities wouldn't be very meaningful, for the following reason: The main cost and the
main gain of our method come from symbolic computations. In the worst case, a symbolic
computation on a formula is as costly as the enumeration of the set of states it characterizes. So,
the main argument for our method concerns average performances, as for all symbolic methods

13

using Binary Decision Diagrams. Let us give some experimental results about the use of the
Lustre-V3 compiler: Considering the Lustre program corresponding to the simple example
presented in Section 4, and a program implementing a digital watch -- a very common example
of reactive program -- we give below

ffl the size of the state graph produced by the standard enumeration method, the time (on

SUN4) for generating it, and the time for reducing it by means of Aldebaran [9].

ffl The size and generation time of the state graph when our method is used.

Simple example Enumeration Minimization Minimal graphgeneration

Number of states 10 5 5
Number of transitions 20 7 7
Generation time (sec.) 0.5 0.2 0.5

Watch Program Enumeration Minimization Minimal graphgeneration

Number of states 81 41 41
Number of transitions 1163 342 342
Generation time (sec.) 8.1 33.4 6.8

References

[1] G. Berry and G. Gonthier. The Esterel synchronous programming language: Design, semantics, implementation. Science Of Computer Programming, 19(2):87-152, 1992.

[2] J.P. Billon and J.C. Madre. Original concepts of PRIAM, an industrial tool for efficient

formal verification of combinational circuits. In G. Milne, editor, IFIP WG 10.2 Int Working
Conference on the Fusion of Hardware Design and Verification, Glasgow, Scotland, July
1988. North Holland.

[3] A. Bouajjani, J.-C. Fernandez, and N. Halbwachs. Minimal model generation. In R. Kurshan, editor, International Workshop on Computer Aided Verification, Rutgers, June 1990.

[4] R. E. Bryant. Graph-based algorithms for boolean function manipulation. IEEE Transactions on Computers, C-35(8):677-692, 1986.

[5] J.R. Burch, E.M. Clarke, K.L. McMillan, D.L. Dill, and J. Hwang. Symbolic model checking: 1020 states and beyond. In Fifth IEEE Symposium on Logic in Computer Science,
Philadelphia, 1990.

[6] P. Caspi, D. Pilaud, N. Halbwachs, and J. Plaice. Lustre: a declarative language for

programming synchronous systems. In 14th ACM Symposium on Principles of Programming
Languages, Munchen, January 1987.

[7] E. M. Clarke, E. A. Emerson, and A. P. Sistla. Automatic verification of finite-state concurrent systems using temporal logic specifications. ACM TOPLAS, 8(2), 1986.

[8] O. Coudert, C. Berthet, and J. C. Madre. Verification of synchronous sequential machines

based on symbolic execution. In International Workshop on Automatic Verification Methods
for Finite State Systems, Grenoble. LNCS 407, Springer Verlag, 1989.

14

[9] J.-C. Fernandez. An implementation of an efficient algorithm for bisimulation equivalence.

Science of Computer Programming, 13(2-3), May 1990.

[10] N. Halbwachs, P. Raymond, and C. Ratel. Generating efficient code from data-flow programs. In Third International Symposium on Programming Language Implementation and
Logic Programming, Passau (Germany), August 1991.

[11] P. Kanellakis and S. Smolka. CCS expressions, finite state processes and three problems of

equivalence. In Proceedings ACM Symp. on Principles of Distribued Computing, 1983.

[12] R. Milner. A Calculus of Communicating Systems. LNCS 92, Springer Verlag, 1980.
[13] R. Paige and R. Tarjan. Three partition refinement algorithms. SIAM J. Comput., 16(6),

1987.

[14] J. L. Richier, C. Rodriguez, J. Sifakis, and J. Voiron. Verification in xesar of the sliding

window protocol. In IFIP WG-6.1 7th. International Conference on Protocol Specification,
Testing and Verification, Zurich, 1987. North Holland.

[15] J. Sifakis. A unified approach for studying the properties of transition systems. TCS, 3,

1982.

Appendix
Characterization of split

We denote by X the complement of a subset X of Q : X = Q n X. Let us first give a characterization of split using a predicate transformer gpre defined below, introduced in [15] and shown to

be "-continuous: (gpre(X) = pre(X)).

gpre(X) = fq 2 Q j 8q0:q ! q0 =) q0 2 Xg = pre(X)
Lemma 5 split(X; ae) = fT j T = X " "

Y 2sspre(Y ) " gpre( [Y 2ssY ) and ss ` ae and T 6= ;g

Proof: From the definition of split, we have:

split(X; ae) = u

Y 2aesplit(X; Y )

= u

Y 2aefX " pre(Y )g [ fX " gpre(Y )g

= fT j T = X " "

Y 2sspre(Y ) " "Z2aenss gpre(Z) and ss ` ae and T 6= ;g

= fT j T = X " "

Y 2sspre(Y ) " gpre( "Z2aenssZ) and ss ` ae and T 6= ;g

(by continuity of gpre)

= fT j T = X " "

Y 2sspre(Y ) " gpre( [Z2aenssZ) and ss ` ae and T 6= ;g

= fT j T = X " "

Y 2sspre(Y ) " gpre( [Z2ssZ) and ss ` ae and T 6= ;g

From this lemma, we deduce :

15

Corollary 1

1. split(ss; ae) = fT j T = X " "

Y 2ssaepre(Y ) " gpre( [Z2ssaeZ) j ssae ` ae and X 2 ss and T 6= ;g.

2. split(ss1 [ ss2; ae) = split(ss1; ae) [ split(ss2; ae).

Proof of Proposition 1
Part 1 is obvious.
Part 2 is a direct consequence of the corollary 1.
We now prove part 3 of proposition 1. Let X 2 split(ae1; ae1). Then, from lemma 5 above, there
exists X1 2 ae1 and ss1 ` ae1 such that

X = X1 " "

Y 2ss1pre(Y ) " gpre( [Y 2ss1Y ):

Let X2 2 ae2 and ss2 ` ae2 such that X1 ` X2 and ss2 = fY2 2 ae2 j 9Y1 2 ss1 such that Y1 ` Y2g.
Then

X1 " "

Y 2ss1pre(Y ) " gpre( [Y 2ss1Y ) `

Zz ""-- -
X2 " "

Y 2ss2pre(Y ) " gpre( [Y 2ss2 Y ) :

and Z 2 split(ae2; ae2).

To prove part 4, we state that ae1 v ae2 =) Acc(ae1 ) v Acc(ae2) and then apply part 3. If
[q]ae1 exists and ae1 v ae2, then [q]ae2 exists and [q]ae1 ` [q]ae2. Thus postae

1(X) v postae2 (X) andpost

ae1(ss) v postae2(ss). On the other hand, Acc(ae) = _ss:(f[qinit]aeg [ postae(ss)). Thus, Acc(ae1) vAcc(

ae2).

Proof of Lemma 1
Let us first point out that if ae is a bisimulation, then X 2 Acc(ae) if and only if X u Acc0 6= ;.
Let f = *X:[X " Acc0 ]ae2 .

ffl f is a bijection from Acc(ae1) to Acc(ae2 ):

Let X 2 Acc(ae1 ). Then X " Acc0 2 ae1 u fAcc0 g, so X " Acc0 2 ae2 u fAcc0 g and there
exists one and only one Y 2 ae2 such that X " Acc0 ` Y .
By symmetric reasoning, for all Y 2 Acc(ae2 ), there exists one and only one X 2 ae1 such
that [X " Acc0 ]ae2 = Y .

ffl Obviously f ([qinit]ae1) = [qinit]ae2.
ffl We show that f ffi postae1 ` postae2 ffi f , the reverse inclusion being obtained by symmetry.

Let X 2 Acc(ae1 ) and X0 2 postae1 (X). Let us show that f (X0) 2 postae2 (f (X)). Since
X 2 Acc(ae1 ) and X0 2 postae1(X), there exists q 2 X " Acc0 and q0 2 X0 such that q ! q0.
Since q is accessible, so is q0, and q0 2 X0 " Acc0 . So, q0 2 f (X0). On the other hand,
since q 2 X " Acc0 , q 2 f (X) and there exists Y 2 postae

2 (f (X)) such that q0 2 Y . So,q0 2 Y " f (X0), and f (X0) = Y 2 post

ae2 (f (X)).

16

Proof of Lemma 2

B = fAcc0 g u *ae:G2 (ae)

= fAcc0 g u u

i*0G

i
2 (fQg)

= u

i*0(fAcc0 g u G

i
2 (fQg))

= u

i*0G

i
2(fAcc0 g u fQg) since, from lemma 3

fAcc0 g u G2 (ae) = aeinit u split(fAcc0 g u ae; fAcc0g u ae)

= G2(fAcc0g u ae)

= *ae:G2(fAcc0g u ae)
= *ae:aeinit u split(fAcc0 g u ae; fAcc0g u ae)
= *ae:G1(fAcc0g; ae)
= fAcc0 g u *ae:G1 (fAcc0 g; ae) = A

Proof of Lemma 3
The proof is based on the corollary 1.

fAcc0 g u split(ss; ae) = fAcc0 g u fX " "

Y 2ssaepre(Y ) " gpre( [Z2ssaeZ ) j ssae ` ae and X 2 ssg

= fAcc0 " X " "

Y 2ssaepre(Y ) " gpre( [Z2ssaeZ ) j ssae ` ae and X 2 ssg

= fX0 " "

Y 2ssaepre(Y ) " gpre( [Z2ssaeZ) j ssae ` ae and X

0 2 ss u fAcc

0 gg

= split(fAcc0g u ss; ae)
which proves part 1. For 2, we have

split(fAcc0 g u ss; ae) = fAcc0 " X " "

Y 2ssaepre(Y ) " gpre( [Z2ssaeZ ) j ssae ` ae and X 2 ssg

Let us show

Acc0 " "

Y 2ssaepre(Y ) " gpre( [Z2ssaeZ ) = Acc

0 " "Y 2ss

aeufAcc0 gpre(Y ) " gpre( [Z2ssaeufAcc0 gZ )

which will provide the result. This is a consequence of the two following properties:

1 Acc0 " pre(Y ) = Acc0 " pre(Y " Acc0 )

' : follows by monotonicity of pre.
` : Let q 2 Acc0 " pre(Y ). Then there exists q0 2 Y such that q ! q0. Now, since q is
accessible, so is q0, and q0 2 Y " Acc0 . Thus, q 2 Acc0 " pre(Y " Acc0 ).

2 Acc0 " gpre(Z ) = Acc0 " gpre(Z " Acc0 )

' : follows by monotonicity of gpre.
` : Let q 2 Acc0 " gpre(Z ). Then for all q0 such that q ! q0, q0 2 Z. Now, since q is
accessible, so is q0, and q0 2 Z " Acc0 . Thus, q 2 Acc0 " gpre(Z " Acc0 ).

Then

Acc0 " "

Y 2ssaepre(Y ) " gpre( [Z2ssaeZ ) = Acc

0 " "Y 2ss

aepre(Y " Acc

0 ) " gpre( [Z2ss

aeZ " Acc

0 )

= Acc0 " "

Y 2ssaeufAcc0 gpre(Y ) " gpre( [Z2ssaeufAcc0 gZ )

17

Proof of proposition 3
From the definition of the bisimulation as a greatest fixpoint, we have ae = ufaeng, where ae0 = aeinit
and aen+1 = aeinit u split(aen; aen) = aen u split(aen; aen) = split(aen; aen). If Q=ae is finite, the sequence
faengn*0 converges after a finite number of steps.

Let us introduce the sequence fssn = Acc(aen)g, which converges to Acc(ae). Now, ae0 = ufae0ng,
where ae00 = aeinit, ae0n+1 = ae0n u split(ss0n; ss0n) and ss0n = Acc(ae0n). Let us show that, for any
n * 0; ss0n = ssn. It is true for n = 0. Assume ss0n = ssn, i.e., Acc(aen) = Acc(ae0n). Then,

Acc(aen+1 ) = Acc(aeinit u split(aen; aen))

= Acc(aeinit u split(Acc(aen); aen)) from lemma 4 (part 2 ) below
= Acc(aeinit u split(Acc(aen); Acc(aen))) from lemma 4 (part 1 )
= Acc(aeinit u split(Acc(ae0n); Acc(ae0n)))
= Acc(ae0n+1 )

We have Acc(ae0n) ` ae0n . Moreover, if ae0n+1 = ae0n, then ae0n = split(Acc(ae0n); Acc(ae0n)) v Acc(ae0n),
by proposition 1, part 1. Thus, ae0n = Acc(ae0n). We have also ae0n+2 = ae0n+1 = ae0n = ae0. Thus,
ae0 = Acc(ae0).

Proof of Lemma 4
The proof of part 1 is very similar to the one of Lemma 3 (part 2 ). For part 2, let ae1 =
aeinit u split(ae; ae), and let

ss = f "

Y 2ssaepre(Y ) " gpre( [Z2ssaeZ) j ssae ` aeg

Now, X 2 Acc(ae1 ) implies that there exist X1 2 aeinit, X2 2 ae, and X3 2 ss such that X =
X1 " X2 " X3. Since ae1 v ae, then Acc(ae1 ) v Acc(ae). Moreover, X ` X2, X2 2 ae and
X 2 Acc(ae1). Thus, X2 2 Acc(ae). So,

Acc(ae1 ) = Acc(fX1 " X2 " X3 j X1 2 aeinit ; X2 2 ae; X3 2 ssg)

= Acc(fX1 " X2 " X3 j X1 2 aeinit ; X2 2 Acc(ae); X3 2 ssg)
= Acc(aeinit u split(Acc(ae); ae))

Proof of proposition 4
The proof is based on the following properties:

1. ss00n ` Acc(ae00n).
2. ae0n v ae00n.
3. ss00n = oen =) ae00n = ae00n+1 and ss00n+1 = ssn = oen+1.
4. ss00n = oen =) ss00n = Acc(ae00n) = oen.
Let ae00 be the limit of fae00ng. From properties 3 & 4 and definition of ae00n, we can deduce that
Acc(ae00) is a fixpoint of G3. Since ae0 is the greatest fixpoint of G3, we have:

Acc(ae0) v Acc(ae00) v ae0:
On the other hand, Acc(ae0) = ae0, by proposition 3. Thus, ae0 = Acc(ae00). Let us now prove
properties 2-4. The first property is a consequence of the definitions of sequences fae00ng; fss00ng and
foeng.

18

property 2

By induction on n: First, we have ae00 = ae000. Let us recall the definitions of ae0n and ae00n.

ae0n+1 = ae0n u split(Acc(ae0n); Acc(ae0n))
ae00n+1 = ae00n u split(ss00n n oen; ae00n) [ oen [ (ae00n n ss00n)

Assume that ae0n v ae00n. We show that ae0n+1 v ae00n+1. We have:

ae0n+1 v ae00n u split(Acc(ae00n); Acc(ae00n)) (by monotonicity of Acc and split)

= ae00n u split(Acc(ae00n); ae00n) (by lemma 4 part 1)
= ae00n u split(ss00n; ae00n) [ ae00n u split(Acc(ae00n) n ss00n; ae00n) (by corollary 1, part 2)
v ae00n u split(ss00n; ae00n) [ (ae00n n ss00n) since,

split(Acc(ae00n) n ss00n; ae00n) ` split(Acc(ae00n); ae00n) n ss00n and

split(Acc(ae00n); ae00n) v ae00n :

= ae00n+1 since

split(ss00n; ae00n) = split(ss00n n oen; ae00n) [ split(oen; ae00n)

= split(ss00n n oen; ae00n) [ oen

property 3

We have, ae00n+1 = oen [ (ae00n n ss00n) = ae00n. To prove the second part, we perform an induction
on k: if ae00n+1 = ae00n then

sskn+1 = sskn
oekn+1 = oekn

property 4

If X 2 ss00n, then X 2 oen and postae00n (X) ` ss00n = oen

Proof of the algorithm
Partial correctness
Let ae000p ; ss000p and oe000p be sequences obtained after each step of iteration. We have:

ae0000 = aeinit ss0000 = f[qinit]ae000

0 g; oe

000
0 = ;

ae000p+1 = ae000p n fXg[split(X; ae000p )

oe000p+1 = if X = split(X; ae000p ) then oe000p [fXg else oe000p n preae000p (X)
ss000p+1 = if X = split(X; ae000p ) then ss000p [postae000p (X) else ss000p n fXg[postae000p (qinit)
Several sequences could be considered, corresponding to the choice of X (line (3) of the second
algorithm). But we show they have the same limit, Acc(ae00). Let be ae000 be the limit of fae000n g.
The proof is based on the following properties:

8fae000i g:8n:9k such that ae00k v ae000n ;

We perform an induction on n. We have ae000 = ae0000 . Let fae000i g be a sequence and let n; k
such that ae00k v ae000n . We perform a refinement step of the algorithm. Let X 2 ss000n n oen.
If X = split(X; ae000n ), then ae00k v ae000n+1 else ae00k+1 v ae000n+1. Finally, we can conclude that
Acc(ae00) v Acc(ae000).

19

ss000n = oe000n =) Acc(ae000n ) = ss000n ;

Let X in ss000n . Then X 2 oen and postae000p (X) ` oen. Thus, all the accessible states are stable.

ae000 is a fixpoint of *ae:G3(Acc(ae); Acc(ae));

It is a direct consequence of the previous property. Thus, Acc(ae000) v Acc(ae00), by proposition 3.

Then, Acc(ae000) = Acc(ae00).

Termination
From the assumption that Q=ae is finite, it is obvious that ae cannot be indefinitely refined. The
termination follows.

20