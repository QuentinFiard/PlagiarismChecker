

Information and Computation 179, 1-18 (2002)
doi:10.1006/inco.2001.2950

The Subtyping Problem for Second-Order Types Is Undecidable

Jerzy Tiuryn1 and Pawel\Theta  Urzyczyn2
Institute of Informatics, Warsaw University, Banacha 2, 02-097 Warsaw, Poland

E-mail: tiuryn@mimuw.edu.pl, urzy@mimuw.edu.pl

Received January 15, 1997; published online May 16, 2002

We prove that the subtyping problem induced by Mitchell's containment relation for second-order
polymorphic types is undecidable. It follows that type-checking is undecidable for the polymorphic
lambda-calculus extended by an appropriate subsumption rule. C\Theta  2002 Elsevier Science (USA)

1. INTRODUCTION
Among various forms of subtyping, one that seems to be quite fundamental for the analysis of
polymorphic languages is induced by the containment relation of Mitchell [6]. An expression of a
universal type, say \Lambda \Theta \Lambda  , can be understood as having all types that are polymorphic instances of \Lambda \Theta  \Lambda  .
If we choose the second-order polymorphic lambda-calculus as the basic language, then (taking into
account the possibility of polymorphic generalization) we may formalize the notion of containment as

\Lambda \Theta 1 * * * \Lambda \Theta n \Lambda  \Pi  \Lambda \Xi 1 * * * \Lambda \Xi m \Lambda  (\Pi 1/\Theta 1, . . . , \Pi n/\Theta n), (1)
where \Xi 1, . . . , \Xi m are not free in \Lambda \Theta 1 * * * \Lambda \Theta n \Lambda  . The containment relation \Pi  is a quasi-order (it is
reflexive and transitive) and the subsumption rule for \Pi 

E \Sigma  M : \Psi , \Psi  \Pi  \Lambda 

E \Sigma  M : \Lambda 

is admissible for type inference in (Curry style) second-order lambda-calculus (system F). However, it
lacks compositionality: no conclusion of the form \Lambda  \Pi  \Psi  can be derived from the \Pi -related properties of
the components of \Lambda  and \Psi  . For instance, one cannot have e.g. \Psi  \Upsilon  \Lambda  \Pi  \Psi  \Upsilon  \Lambda  \Phi  even if \Lambda  \Pi  \Lambda  \Phi . Thus,
one would like to postulate that \Pi  be extended to a subtyping relation \Psi , so that arrow is monotonic
with respect to \Psi  in the second argument and antimonotonic in the first argument That is,

\Lambda 1 \Upsilon  \Psi 1 \Psi  \Lambda 2 \Upsilon  \Psi 2, whenever \Lambda 2 \Psi  \Lambda 1 and \Psi 1 \Psi  \Psi 2. (2)
Of course, the extended relation should be monotonic with respect to quantification:

\Lambda \Theta  \Lambda  \Psi  \Lambda \Theta  \Lambda  \Phi , whenever \Lambda  \Psi  \Lambda  \Phi . (3)
In the presence of (3), instead of (1) in its full generality, it is enough to postulate two special cases:

\Lambda \Theta  \Lambda  \Psi  \Lambda  (\Pi /\Theta ), (4)
and

\Lambda  \Psi  \Lambda \Theta  \Lambda , whenever \Theta  does not occur free in \Lambda  . (5)
1 The first author is partly supported by NSF Grants CCR-9417382 and CCR-9304144, by ESPRIT BRA7232 GENTZEN,
and by KBN Grant 2 P301 031 06.

2 The second author is partly supported by NSF Grant CCR-9417382, by ESPRIT BRA7232 GENTZEN, and by KBN

Grant 2 P301 031 06.

1

0890-5401/02 $35.00
C\Theta  2002 Elsevier Science (USA)

All rights reserved.

2 TIURYN AND URZYCZYN
Another postulate, which has an obvious logical counterpart (quantifier distributivity over arrow) can
be, in a weak form, written as follows:

\Lambda \Theta (\Lambda  \Upsilon  \Psi  ) \Psi  \Lambda  \Upsilon  \Lambda \Theta  \Psi , provided \Theta  is not free in \Lambda . (6)
As we noted, this is a weak form of distributivity, and we choose it because it seems that its necessity
is more appealing to the intuition. However, the general form

\Lambda \Theta (\Lambda  \Upsilon  \Psi  ) \Psi  \Lambda \Theta  \Lambda  \Upsilon  \Lambda \Theta  \Psi  (7)
can be derived from the weak one with the help of the other postulates, provided we also require that \Psi 
be transitive. This has to be done explicitly, and thus \Psi  is defined as the least quasi-order satisfying all
the postulates (2), (4), (5), (3), and (6).

The relation \Psi  is a proper extension of \Pi , and the subsumption rule for \Psi 

E \Sigma  M : \Psi , \Psi  \Psi  \Lambda 

E \Sigma  M : \Lambda  (8)

is no longer admissible for system F. To see this, observe that, e.g., x : \Theta  \Upsilon  \Lambda \Xi  \Xi  \Sigma  x : \Theta  \Upsilon  \Xi  is not
derivable, while obviously we have \Theta  \Upsilon  \Lambda \Xi  \Xi  \Psi  \Theta  \Upsilon  \Xi . In fact, one can show a stronger property.
There are pure lambda terms which are typable in system F extended by rule (8), but untypable in
system F alone. One possible example is

\Omega a. [\Omega yz. a((\Omega v. v(\Omega x y. yay))y)(zy)(z(\Omega x. x x x))]Y Z ,
where Y is \Omega x. K (x x)(xaa) and Z is \Omega u. u(\Omega x y. a). The existence of such terms may also be derived
using the "constants-for-free" approach of Wells [10]. This means that subtyping provides for a nontrivial extension of system F. Let us call this extension F\Psi .

To avoid possible confusion, let us stress that our system F\Psi  has very little in common with the
system F<= of [8]. The latter is based on the notion of bounded quantification, i.e., allows for types of
the form (\Lambda \Theta  <= \Lambda  )\Psi  , and has no counterpart of our condition (1) which is related to type instantiation.
In particular, the main result of [8] and our Theorem 2.1 do not imply each other.

A fundamental property of subtyping is that it is, in a sense, equivalent to extensionality. More
precisely, let us write Ffi for system F extended by the following rule:

E \Sigma  M : \Psi , M \Upsilon fi N

E \Sigma  N : \Psi 

.

As shown in [6], systems F\Psi  and Ffi derive exactly the same type assignments. That is, fl \Sigma  M : \Psi  holds
in F\Psi  iff it holds in Ffi. This fact is equivalent to the following property of subtyping: an inequality
\Psi  \Psi  \Lambda  is true if and only if there exists a coercion ("retyping function" in the terminology of [6]) from
\Psi  to \Lambda  , i.e., a lambda term C, which can be assigned type \Psi  \Upsilon  \Lambda  in system F and which is fi-reducible
to identity (has no computational meaning).

Subtyping is also an important notion from the point of view of semantics. The completeness theorem
(see [6]) states that \Lambda  \Psi  \Psi  holds iff \Lambda  is a subtype, i.e., a sub-PER of \Psi  in all PER models. To be precise,
this completeness result is stated only implicitly in [6], not even using the name PER. Theorem 7 of [6]
states completeness for all the so-called simple inference models. On the other hand, a simple quotientset model defined in Section 4.4 is just a PER model. The last paragraph of Section 4.4 explains that
every simple inference model can be made into a simple quotient-set model. Therefore Theorem 7 can
be seen as stating completeness of the containment relation in all PER models.

As we said above, system F\Psi , or equivalently system Ffi, is a proper extension of F. Although the
type reconstruction problem

Given a term M, does there exist E and \Psi  such that E \Sigma  M : \Psi  ?

POLYMORPHIC SUBTYPING 3
for F is undecidable [10], the proof does not apply to F\Psi . Thus, decidability of type reconstruction
remained an open question for our extended system. A recent work of Jim [4] shows that type reconstruction for F\Psi  reduces to the subtyping problem:

Given \Lambda  and \Psi  ; decide whether \Lambda  \Psi  \Psi  holds.
Unfortunately, Jim's reduction does not help much. Our main Theorem 2.1 states that \Psi  is undecidable.
In addition, one can easily see that \Lambda  \Psi  \Psi  is equivalent to x : \Lambda  \Sigma  x : \Psi  being derivable in F\Psi . Thus,
the subtyping problem reduces to the type-checking problem

Given M, E and \Psi  ; does E \Sigma  M : \Psi  hold?
It follows that type-checking for F\Psi  is also undecidable. However, undecidability of subtyping does
not directly imply undecidability of type-reconstruction. This fact was shown by Wells [12] very soon
after our main result was announced. The work of Wells uses another proof of the undecidability of
subtyping [11] which is based on an entirely different approach.

The undecidability of subtyping should be contrasted with a result of [9], which states that the
equivalence relation ff generated by the quasi-order \Psi , called bicoercibility, is decidable. (The relation ff
is defined so that \Psi  ff \Lambda  holds iff \Psi  \Psi  \Lambda  and \Lambda  \Psi  \Psi  .) This contrast can be explained as follows: our
subtyping relation is close to being antisymmetric; i.e., elements of one equivalence class differ very
little. Indeed, the characterization of [9] implies that bicoercible types are of identical shapes, when
seen as binary trees, and no substantial instantiation steps are needed to obtain one from the other (see
the axioms (B1)-(B3) in Section 3 and the remark following Proposition 3.1).

One more remark is in order here. The condition (6) is not essential for our undecidability result,
because our reduction also applies to the appropriate restriction of the subtyping relation [2]. On the
other hand, the proof method of Wells [11], at the cost of applying (6), yields undecidability of \Psi 
restricted to cases when instantiations cannot be nested more than twice, while an unbounded nesting
is used in our approach. It remains an open question whether there is really a trade-off between these
two features.

It should be stressed, however, that undecidability of \Psi  is always obtained as a result of a special
kind of interplay between the instantiation property of (4) and antimonotonicity of \Upsilon  in (2). If the
instantiation is restricted to monomorphic types \Pi , i.e., types without any quantifiers in them, and the
distributivity axiom (6) is removed, then, as shown in [7], the system becomes decidable. Also, it is not
difficult to see that the whole system with \Upsilon  made monotone in both arguments is decidable. Indeed,
one can think of our postulates (4), (5), and (6) as rewrite rules oriented from left to right. The subtyping
problem now becomes the reachability problem for such a rewriting system. But applying our rules to
a type can only increase its size, and thus an exhaustive search over a finite space suffices to decide
reachability.

The proof of our main result consists of two steps. The first one is to design a machine-oriented
variant of the problem. In Section 5 we define a particular automaton, called a stack-register machine,
and we prove in Section 6 that the halting problem for stack-register machines effectively reduces to
the subtyping problem. For this we encode machine ID's as subtyping inequalities, and we prove that
a machine halts on a given ID iff the corresponding inequality is true.

The next step is to show in Section 7 that the halting problem for stack-register machines is undecidable. For this, we show how to simulate a two-counter automaton with the help of a stack-register machine, and we use the well-known fact that the halting problem for two-counter automata is undecidable.

The organization of the paper is as follows: Section 2 introduces the main definitions. Sections 3 and
4 contain some technical results needed in Section 6. Sections 5 to 7 are devoted to the main proof, as
explained above.

2. SECOND-ORDER SUBTYPING
We use the following syntax for second-order types

\Lambda  ::= \Theta  |(\Lambda  \Phi  \Upsilon  \Lambda  \Phi \Phi )|(\Lambda \Theta  \Lambda  ).

4 TIURYN AND URZYCZYN
Symbols \Theta , \Xi , ffi , possibly with indices, will be ranging over type variables. Symbols \Lambda , \Psi , \Pi , ffl, i will
be ranging over second-order types. In order to avoid too many parentheses we adopt the convention
that arrows associate to the right (i.e., \Lambda  \Upsilon  \Psi  \Upsilon  \Pi  means \Lambda  \Upsilon  (\Psi  \Upsilon  \Pi )), and that quantifiers have
higher priority than arrows (so that \Lambda \Theta  \Psi  \Upsilon  \Lambda  means (\Lambda \Theta  \Psi  ) \Upsilon  \Lambda  , rather than \Lambda \Theta  (\Psi  \Upsilon  \Lambda  )). Types
are taken with respect to alpha-conversion; that is, we identify types that differ only in names of their
bound variables.

We will write \Lambda  (\Psi /\Theta ) to indicate the result of substituting \Psi  for all free occurrences of \Theta  in \Lambda  --
bound variables in \Lambda  may have to be renamed in order to avoid capture of free variables of \Psi  .

In this paper we will also use the notation C[ ] to denote a context with at most a single hole [ ], and
we will write C[\Psi  ] for the result of filling the hole with \Psi  , where \Psi  may be a type or a context. This
should be understood as a literal substitution in which some of the free variables of \Psi  become captured
by quantifiers occurring in C[ ] (cf. Chapter 2 in [1]). This is not inconsistent with our alpha conversion
convention, because a context is not a type.

It is sometimes useful to view second-order types as binary trees such that:

* inner nodes are labelled \Upsilon ;*

leaves are labelled with type variables;*
every node is labelled by a finite (possibly empty) sequence of quantifiers \Lambda \Theta 1\Lambda \Theta 2 . . ..

A node in a type \Lambda  will be identified with a path (a sequence of 0's and 1's) leading from the root to
that node. Similarly, we can assign a path of 0's and 1's to a hole [ ] in a context C[ ]. The parity of a
path j is positive if the number of 0's in j is even; otherwise the parity of j is negative.

We say that \Pi  occurs as a subtype in \Lambda  along a path j if there is a context C[ ] with a single hole, the
node where [ ] occurs is j, and \Lambda  = C[\Pi ]. Note that, due to possible occurrences of quantifiers, there
may be more than one subtype occurring along the same path.

2.1. Mitchell's System
First we present Mitchell's system for polymorphic subtyping (see [6]). The original definition of \Psi 
consisted of containment axioms (1) and (7) and containment rules (2), (3) and transitivity. We prefer,
however, to split the general axiom (1) into three specific postulates to highlight the three different
basic forms of containment: identity, instantiation, and generalization. We also use the weak form of
quantifier distribution. It is left to the reader to verify that these two definitions are equivalent. Formally,
Mitchell's system derives formulas of the form \Lambda  \Psi  \Psi  , where \Lambda  and \Psi  are polymorphic types.

Axioms:
(refl) \Lambda  \Psi  \Lambda  ;
(inst) \Lambda \Theta  \Lambda  \Psi  \Lambda  (\Pi /\Theta );
(dummy) \Lambda  \Psi  \Lambda \Theta  \Lambda  , (\Theta  not free in \Lambda  );
(distr) \Lambda \Theta  (\Lambda  \Upsilon  \Psi  ) \Psi  \Lambda  \Upsilon  \Lambda \Theta  \Psi  , (\Theta  not free in \Lambda  ).
Rules:

(\Upsilon ) \Lambda  \Phi  \Psi  \Lambda  \Psi  \Psi  \Psi  \Phi \Lambda  \Upsilon  \Psi  \Psi  \Lambda  \Phi  \Upsilon  \Psi  \Phi  (\Lambda ) \Lambda  \Psi  \Psi \Lambda \Theta  \Lambda  \Psi  \Lambda \Theta . \Psi 

(trans) \Lambda  \Psi  \Pi  \Pi  \Psi  \Psi \Lambda  \Psi  \Psi  .
Our main result is as follows:

THEOREM 2.1. It is effectively unsolvable to decide, for given types \Lambda  and \Psi , whether the formula
\Lambda  \Psi  \Psi  can be derived in the above system.

POLYMORPHIC SUBTYPING 5
2.2. The Longo-Milsted-Soloviev System
The system which we are about to present is due to Longo, Milsted, and Soloviev (see [5]). It will
play an auxiliary role in our proof of the main result. The crucial notion of a weight of an inequality,
which we introduce in this section, will be used as a measure with respect to which we will carry
on the induction argument in the main technical lemma of our proof. This notion is directly related
to the system of Longo, Milsted, and Soloviev. Obviously, it can be introduced as well for any other
system which derives inequalities, including Mitchell's system of the previous section. The benefit of
working with the system of Longo, Milsted, and Soloviev is that it will be easier to control the weight
of inequalities in Sections 3 and 4.

The system is designed for deriving sequents of the form \Lambda  \Sigma  \Psi  , where \Lambda  and \Psi  are polymorphic
types.

Axiom: \Theta  \Sigma  \Theta 
Rules:

(\Upsilon ) \Lambda  \Phi  \Sigma  \Lambda  \Psi  \Sigma  \Psi  \Phi \Lambda  \Upsilon  \Psi  \Sigma  \Lambda  \Phi  \Upsilon  \Psi  \Phi 

(\Lambda -left) \Lambda  (ffl/\Theta ) \Sigma  \Psi \Lambda \Theta  \Lambda  \Sigma  \Psi 

(\Lambda n-right) \Lambda  \Sigma  \Psi 1 \Upsilon  * * * \Upsilon  \Psi n \Upsilon  \Pi \Lambda  \Sigma  \Psi 

1 \Upsilon  * * * \Upsilon  \Psi n \Upsilon  \Lambda \Theta  \Pi 

In the above rule we assume that n >= 0 and \Theta  does not occur free in \Lambda  , \Psi 1, . . . , \Psi n. (Originally, the
axiom scheme proposed in [5] was "\Lambda  \Sigma  \Lambda  ," for arbitrary \Lambda  . It is a routine exercise to see that our
weaker axiom, which we prefer for technical reasons, is sufficient.)

By \Lambda  \Sigma LMS \Psi  we denote derivability of the sequent \Lambda  \Sigma  \Psi  in the above system.
Strength of the above system lies in simplicity of its rules. In particular, as it follows from the next
result, the rule of transitivity is admissible in it. This can be seen as a form of cut elimination.

THEOREM 2.2 (Longo-Milsted-Soloviev [5]). For all types \Lambda  and \Psi ,

\Lambda  \Psi  \Psi  i \Lambda  \Sigma LMS \Psi ,
Next we introduce the notion of a weight of an inequality. We set the weight of (an instance of) the
(\Lambda -left) rule

(\Lambda -left) \Lambda  (ffl /\Theta ) \Sigma  \Psi \Lambda \Theta  \Lambda  \Sigma  \Psi 
as the number of \Upsilon 's in ffl (which we denote |ffl |), multiplied by the number of free occurrences of \Theta  in \Lambda  ,
i.e., it is the total number of \Upsilon 's eliminated by this rule. The weight of a derivation in \Sigma LMS is the sum
of the weights of all instances of (\Lambda -left) used in this derivation (or zero if (\Lambda -left) does not occur).
Finally, the weight of an inequality \Lambda  \Psi  \Psi  is the least weight of a derivation which derives \Lambda  \Sigma  \Psi  . The
weight is assigned only to inequalities which are true.

3. INVARIANCE OF THE WEIGHT UNDER BICOERCIONS
Recall that types \Psi  and \Lambda  are bicoercible (written \Psi  ff \Lambda  ) iff \Psi  \Psi  \Lambda  and \Lambda  \Psi  \Psi  . The goal of this section
is to establish invariance of the weight introduced in Section 2.2 under the bicoercibility relation. We
will need a series of lemmas which leads to this result. Throughout this section we denote by j the
type \Lambda \Theta  \Theta .

6 TIURYN AND URZYCZYN

Before we start let us recall the equational axiomatization of ff from [9]. This is the least congruence
(i.e., preserved by \Upsilon  and \Lambda ) which satisfies the following three axioms.

(B1) \Lambda \Theta \Lambda \Xi  \Lambda  ff \Lambda \Xi \Lambda \Theta  \Lambda 
(B2) \Lambda \Theta  (\Lambda  \Upsilon  \Psi  ) ff (\Lambda  \Upsilon  \Lambda \Theta  \Psi  ) (\Theta  `' F V (\Lambda  ))
(B3) \Lambda \Theta  \Lambda  ff \Lambda  (j/\Theta ) (all occurrences of \Theta  in \Lambda  are positive)

We will say that the inequality \Lambda  \Phi  \Psi  \Psi  \Phi  is obtained from \Lambda  \Psi  \Psi  by one application of (B1) if \Lambda  \Phi  or \Psi  \Phi 
is obtained from \Lambda  or \Psi  , respectively, by performing one application of (B1) treated as a rewrite rule
ordered from left to right. In a similar way we treat (B2) and (B3) as rewrite rules, ordered from left to
right.

We start with rearranging derivations. We say that a derivation in \Sigma LMS is in normal form iff a
conclusion of a (\Lambda i -right) rule is never a premise of (\Lambda -left), nor the right premise of (\Upsilon ).

LEMMA 3.1. If \Lambda  \Sigma  \Psi  is derivable in \Sigma LMS, then there is a derivation in a normal form of the same
sequent in \Sigma LMS with the same weight.

Proof. The result of applying first a (\Lambda i -right) rule and then a (\Lambda -left) rule is the same as first applying
the (\Lambda -left) rule and then applying the (\Lambda i -right) rule (but not conversely). A similar permutation is
possible if a conclusion of (\Lambda i -right) is the right premise of (\Upsilon ). It may happen that the variable which
gets bound by the (\Lambda i -right) rule occurs free in the left premise of (\Upsilon ). In that case we rename, using
a brand new variable, all occurrences of that variable in the sub-derivation whose conclusion is the
(\Lambda i -right) rule. Clearly, such a renaming does not change the weight. We leave the details for the reader.
\Theta 

LEMMA 3.2. Let \Lambda  \Sigma  \Pi 1 \Upsilon  * * * \Upsilon  \Pi k \Upsilon  \Lambda \Xi  \Psi  be derivable in \Sigma LMS and let \Xi  be not free in
\Lambda , \Pi 1, . . . , \Pi k. Then there is a normal derivation of \Lambda  \Sigma  \Pi 1 \Upsilon  * * * \Upsilon  \Pi k \Upsilon  \Psi  of the same weight.

Proof. An easy induction with respect to derivations. (In fact, every normal derivation of \Lambda  \Sigma  \Pi 1 \Upsilon * * * \Upsilon 

\Pi k \Upsilon  \Lambda \Xi  \Psi  must end with an application of (\Lambda k-right) introducing \Lambda \Xi . \Theta 

LEMMA 3.3. If \Lambda  \Phi  \Psi  \Psi  \Phi  is obtained from \Lambda  \Psi  \Psi  by one application of (B1), then the weight of \Lambda  \Phi  \Psi  \Psi  \Phi 
is equal to the weight of \Lambda  \Psi  \Psi  .

Proof. We proceed by induction on the length of the derivation of \Lambda  \Sigma  \Psi  in \Sigma LMS. Let us assume
that the derivation is in normal form, as described in Lemma 3.1. The case of the axiom and (\Upsilon ) is
obvious. Assume now that the last rule is (\Lambda -left). The only interesting case to consider here is when \Lambda 
is of the form \Lambda \Theta \Lambda \Xi  \Lambda 1 and \Lambda  \Phi  is \Lambda \Xi \Lambda \Theta  \Lambda 1. All other cases are handled in a routine way by the induction
hypothesis. Since the derivation is in normal form it follows that the last two rules are (\Lambda -left), one
which introduced the \Lambda \Xi  and another which introduced the \Lambda \Theta  in \Lambda  . Swapping these two rules results
in deriving \Lambda  \Phi  \Sigma  \Psi  . An easy calculation shows that the weight of this derivation remains unchanged.

Let the last rule be (\Lambda i -right). The only nontrivial case, when \Lambda  has the form \Pi 1 \Upsilon  * * * \Upsilon  \Pi i \Upsilon  \Lambda \Theta \Xi  \Psi  ,
follows easily from Lemma 3.2. This is because we can assume that the preceding step is also an
application of (\Lambda i -right), introducing \Lambda \Xi . \Theta 

LEMMA 3.4. If \Lambda  \Phi  \Psi  \Psi  \Phi  is obtained from \Lambda  \Psi  \Psi  by one application of (B2), then the weight of
\Lambda  \Phi  \Psi  \Psi  \Phi  is equal to the weight of \Lambda  \Psi  \Psi  .

Proof. First we prove that \Lambda  \Phi  \Psi  \Psi  \Phi  is of weight less than or equal to the weight of \Lambda  \Psi  \Psi  . Again
we proceed by induction on the length of derivation of \Lambda  \Sigma  \Psi  in \Sigma LMS. Assume that the derivation is
in normal form (see Lemma 3.1). We discuss here only the case when the last rule in the derivation is
(\Lambda -left). The remaining cases are easier and are left for the reader. Thus the last step of the derivation
looks as follows,

\Lambda 0(ffl/\Theta ) \Sigma  \Psi \Lambda 

\Theta  \Lambda 0 \Sigma  \Psi  ,

POLYMORPHIC SUBTYPING 7
and the only interesting case is when \Lambda 0 = \Lambda 1 \Upsilon  \Lambda 2, the variable \Theta  does not occur freely in \Lambda 1 and
\Lambda  \Phi  = \Lambda 1 \Upsilon  \Lambda \Theta  \Lambda 2. (The other cases are being dealt with by induction hypothesis.) Thus we have derived

\Lambda 1 \Upsilon  \Lambda 2(ffl/\Theta ) \Sigma  \Psi ,
and this is followed by (\Lambda -left). Since the derivation is in normal form, it follows that the previous step
must have been the (\Upsilon ) rule. Thus \Psi  = \Psi 1 \Upsilon  \Psi 2 and we get that \Psi 1 \Sigma  \Lambda 1 and \Lambda 2(ffl /\Theta ) \Sigma  \Psi 2 are derivable
in \Sigma LMS. Applying (\Lambda -left) and then (\Upsilon ) to the latter sequent we conclude that

\Lambda 1 \Upsilon  \Lambda \Theta  \Lambda 2 \Sigma  \Psi 
is derivable in \Sigma LMS and the weight of this derivation is the same as the weight of the original \Lambda  \Sigma  \Psi  .

Assume now that we have a derivation of \Lambda  \Phi  \Sigma  \Psi  \Phi  with a weight smaller than the weight of \Lambda  \Sigma  \Psi  ,
and choose \Lambda  , \Psi  , and \Lambda  \Phi  so that our derivation is the shortest possible normal derivation of this property.
There are two cases depending on whether (B2) was applied to \Lambda  or to \Psi  . In the latter case a contradiction
is derived easily with help of Lemma 3.2, so let us consider the former case. We have \Psi  \Phi  = \Psi  , and let
\Lambda  = \Lambda \Theta  (\Lambda 1 \Upsilon  \Lambda 2) and \Lambda  \Phi  = \Lambda 1 \Upsilon  \Lambda \Theta  \Lambda 2. Since our derivation is normal it must end with either an
application of (\Lambda i -right) or an application of (\Upsilon ). If the last rule is (\Lambda i -right), then by removing the last
step we obtain a "bad" derivation which is shorter than the original one. Thus the last rule must be (\Upsilon ),
with its right premise being obtained from an application of (\Lambda -left). That is, we have \Lambda \Theta  \Lambda 2 \Sigma  \Psi 2 derived
from \Lambda 2(\Pi /\Theta ) \Sigma  \Psi 2, for some \Pi , where \Psi  = \Psi 1 \Upsilon  \Psi 2. The other premise of (\Upsilon ) is \Psi 1 \Sigma  \Lambda 1. With no
change of weight we can permute these two rules to obtain \Lambda \Theta  (\Lambda 1 \Upsilon  \Lambda 2) \Sigma  \Psi  ; that is \Lambda  \Sigma  \Psi  . \Theta 

The next three lemmas are needed to establish the invariance of the weight under rewrites induced
by (B3).

LEMMA 3.5. If \Lambda  \Psi  \Psi , then for every variable \Theta , the weight of \Lambda  (j/\Theta ) \Psi  \Psi  (j/\Theta ) is not greater
than the weight of \Lambda  \Psi  \Psi  .

Proof. Induction on length of derivation of \Lambda  \Sigma  \Psi  in \Sigma LMS. We leave the routine details for the
reader. \Theta 

LEMMA 3.6. Let \Lambda , ffl, and \Psi  be types and assume that a variable \Theta  has n occurrences in \Lambda  . Let ^\Pi 
be a sequence of types and let ^\Xi  be a sequence of type variables of the same length as ^\Pi . Then

(i) If \Theta  occurs only positively in \Lambda  and \Lambda  (ffl/\Theta , ^\Pi / ^\Xi ) \Psi  \Psi , then \Lambda  (j/\Theta , ^\Pi / ^\Xi ) \Psi  \Psi  and the weight
of the latter inequality is not greater than the weight of the former inequality plus n * |ffl |.

(ii) If \Theta  occurs only negatively in \Lambda  and \Psi  \Psi  \Lambda  (ffl /\Theta , ^\Pi / ^\Xi ), then \Psi  \Psi  \Lambda  (j/\Theta , ^\Pi / ^\Xi ) and the weight
of the latter inequality is not greater than the weight of the former inequality plus n * |ffl |.

Proof. The proof is by mutual induction on \Lambda  . The induction hypothesis is that (i) and (ii) hold for
all possible substitutions ^\Pi / ^\Xi . The base step consists in considering two cases: when \Lambda  = \Theta  and when
\Lambda  = \Xi  `= \Theta . For the first case, use the easy fact that \Psi  \Psi  \Psi  is of weight zero; for the second case note
that ffi \Psi  \Psi  implies ffi = \Psi  if ffi is a variable.

The induction step is completely routine as well. For the case of \Upsilon  we mutually use (i) and (ii) as
induction hypotheses and for the case of \Lambda  it is essential that ^\Pi / ^\Xi  is arbitrary. We leave the details for
the reader. \Theta 

LEMMA 3.7. Let \Lambda  and \Psi  be types. Let ^\Pi  be a sequence of types and let ^\Xi  be a sequence of type
variables, of the same length as ^\Pi . Then

(i) If \Theta  occurs only negatively in \Lambda  and \Lambda  (j/\Theta , ^\Pi / ^\Xi ) \Psi  \Psi , then \Lambda  ( ^\Pi / ^\Xi ) \Psi  \Psi  and the weight of
the latter inequality is not greater than the weight of the former inequality.

(ii) If \Theta  occurs only positively in \Lambda  and \Psi  \Psi  \Lambda  (j/\Theta , ^\Pi / ^\Xi ), then \Psi  \Psi  \Lambda  ( ^\Pi / ^\Xi ) and the weight of
the latter inequality is not greater than the weight of the former inequality.

8 TIURYN AND URZYCZYN

Proof. The proof is by mutual induction on \Lambda  . Let us consider the base case \Lambda  = \Theta  and let us prove
(ii). Assume that \Psi  \Sigma LMS \Lambda \Theta  \Theta  and assume that the derivation is in normal form (see Lemma 3.1). It
follows that the last rule in this derivation must have been (\Lambda 0-right) and we have had \Psi  \Sigma  \Theta .

The rest of the proof is completely routine and we leave it for the reader. \Theta 

LEMMA 3.8. If \Lambda  \Phi  \Psi  \Psi  \Phi  is obtained from \Lambda  \Psi  \Psi  by one application of (B3), then the weight of
\Lambda  \Phi  \Psi  \Psi  \Phi  is equal to the weight of \Lambda  \Psi  \Psi  .

Proof. We proceed by induction on the length of derivation of \Lambda  \Sigma  \Psi  in \Sigma LMS, of the least possible
weight. Again, the base case and the case of rule(\Upsilon ) are routine. Consider the case when the last rule
in this derivation was (\Lambda -left). Then \Lambda  = \Lambda \Theta  \Lambda 1 and

\Lambda 1(ffl/\Theta ) \Sigma  \Psi \Lambda 

\Theta  \Lambda 1 \Sigma  \Psi  .

The nontrivial case is when \Lambda  \Phi  = \Lambda 1(j/\Theta ) and \Theta  occurs only positively in \Lambda 1. By Lemma 3.6(i) we
have \Lambda 1(j/\Theta ) \Psi  \Psi  and the weight of this inequality is not greater than the weight of \Lambda \Theta  \Lambda 1 \Psi  \Psi  .

Now suppose that the weight of \Lambda 1(j/\Theta ) \Psi  \Psi  is strictly less than the weight of \Lambda \Theta  \Lambda 1 \Psi  \Psi  . Then we
can take a derivation of the latter inequality and complete it with

\Lambda 1(j/\Theta ) \Sigma  \Psi \Lambda 

\Theta  \Lambda 1 \Sigma  \Psi 

to a derivation of a smaller weight, a contradiction.

Finally let us consider the case when the last rule was (\Lambda n-right). Then \Psi  = \Psi 1 \Upsilon  * * * \Upsilon  \Psi n \Upsilon  \Lambda \Theta  \Pi 
and the nontrivial case is when \Theta  occurs only positively in \Pi . We have

\Lambda  \Sigma  \Psi 1 \Upsilon  * * * \Upsilon  \Psi n \Upsilon  \Pi 
\Lambda  \Sigma  \Psi 1 \Upsilon  * * * \Upsilon  \Psi n \Upsilon  \Lambda \Theta  \Pi 

and \Theta  does not occur free in \Lambda , \Psi 1, . . . , \Psi n. By Lemma 3.5 we have

\Lambda  \Psi  \Psi 1 \Upsilon  * * * \Upsilon  \Psi n \Upsilon  \Pi (j/\Theta )
with weight not increased.

Now assume that the weight of \Lambda  \Psi  \Psi 1 \Upsilon  * * * \Upsilon  \Psi n \Upsilon  \Pi (j/\Theta ) is strictly smaller than the weight
of \Lambda  \Psi  \Psi 1 \Upsilon  * * * \Upsilon  \Psi n \Upsilon  \Lambda \Theta  \Pi . By Lemma 3.5(ii) we conclude that the weight of \Lambda  \Sigma  \Psi 1 \Upsilon  * * * \Upsilon 

\Psi n \Upsilon  \Pi  is strictly smaller than the weight of \Lambda  \Psi  \Psi 1 \Upsilon  * * * \Upsilon  \Psi n \Upsilon  \Lambda \Theta  \Pi , which is a contradiction.
This proves that both weights are equal. \Theta 

We conclude this section with the following invariance result.

PROPOSITION 3.1. If \Lambda  and \Lambda  \Phi  are bicoercible and \Psi  and \Psi  \Phi  are bicoercible, then the weight of \Lambda  \Psi  \Psi 
is equal to the weight of \Lambda  \Phi  \Psi  \Psi  \Phi , provided both inequalities hold.

Proof. Type \Lambda  can be rewritten into \Lambda  \Phi  through a sequence of atomic steps induced by the axioms
(B1), (B2), and (B3), viewed as left-to-right rules as well as right-to-left rules. By Lemmas 3.3, 3.4, and
3.8 all these steps do not change the weight of the inequalities. The same argument applies to \Psi  and \Psi  \Phi .
\Theta 

Let us observe the following consequence of Proposition 3.1. If \Psi  ff \Lambda  then the weight of \Psi  \Psi  \Lambda  is
the same as the weight of \Psi  \Psi  \Psi  , and it is easy to see that the latter equals zero. That is, inequalities
between bicoercible types are of weight zero (although the reader will easily find examples showing
that the converse does not hold).

POLYMORPHIC SUBTYPING 9
4. FORCING INEQUALITIES
In this section we prove several properties of subtyping, which will be used as an essential tool in the
proofs of Section 6.

LEMMA 4.1. If \Lambda 1 \Upsilon  \Lambda 2 \Psi  \Psi 1 \Upsilon  \Psi 2, then \Psi 1 \Psi  \Lambda 1 and \Lambda 2 \Psi  \Psi 2. Moreover the weight of the
inequalities in the conclusion is smaller than or equal the weight of \Lambda 1 \Upsilon  \Lambda 2 \Psi  \Psi 1 \Upsilon  \Psi 2.

Proof. Take the derivation of \Lambda 1 \Upsilon  \Lambda 2 \Sigma  \Psi 1 \Upsilon  \Psi 2 in \Sigma LMS. The proof is by induction on the number
of (\Lambda n-right) rules that end this derivation. If this number is 0, then the last rule is (\Upsilon ), and we are done.
Otherwise the last step in the derivation looks as follows.

(\Lambda n-right) \Lambda 1 \Upsilon  \Lambda 2 \Sigma  \Psi 1 \Upsilon  \Psi  \Phi 2\Lambda 

1 \Upsilon  \Lambda 2 \Sigma  \Psi 1 \Upsilon  \Psi 2 , (9)

where n > 0. By the induction hypothesis we obtain \Psi 1 \Psi  \Lambda 1 and \Lambda 2 \Psi  \Psi  \Phi 2. Thus \Lambda 2 \Sigma LMS \Psi  \Phi 2 holds. It
follows from (9) that

(\Lambda n*1-right) \Lambda 2 \Sigma  \Psi  \Phi 2\Lambda 

2 \Sigma  \Psi 2

is a legal derivation. Thus \Lambda 2 \Psi  \Psi 2. It is easy to see that the weight of \Psi 1 \Psi  \Lambda 1 and \Lambda 2 \Psi  \Psi 2 is not greater
than the weight of \Lambda 1 \Upsilon  \Lambda 2 \Psi  \Psi 1 \Upsilon  \Psi 2. \Theta 

LEMMA 4.2. Let \Lambda  \Psi  \Psi  and let \Pi  and i be subtypes in \Lambda  and \Psi , respectively, which occur along the
same path j. Assume that no free variable of \Pi  is bound in \Lambda  and no free variable of i is bound in \Psi  . If
j is positive, then \Pi  \Psi  i holds; otherwise, i \Psi  \Pi .

Proof. We prove the lemma by induction on the length of j, denoted |j|. If |j| = 0, then \Lambda  is of
the form

\Lambda  = \Lambda \Xi 1 * * * \Lambda \Xi n \Pi 
and similarly \Psi  is of the form

\Psi  = \Lambda ffi1 * * * \Lambda ffim i,
where the quantifiers \Lambda \Xi 1 * * * \Lambda \Xi n and \Lambda ffi1 * * * \Lambda ffim are dummies in \Lambda  and \Psi  , respectively. Thus \Psi  ff i
and \Lambda  ff \Pi . It follows that

\Pi  \Psi  i.
Let |j| > 0. Let us observe that we can assume without loss of generality that \Psi  does not start with a
quantifier. Indeed, it immediately follows from the following property. For all types ffl and \Psi  \Phi  and a type
variable \Theta  `' F V (ffl )

ffl \Psi  \Lambda \Theta  \Psi  \Phi  i ffl \Psi  \Psi  \Phi .
The proof of (_) follows from (inst) and (trans), while the proof of (*) follows from (\Lambda 0-right).

Thus we may assume that \Psi  is of the form \Psi 1 \Upsilon  \Psi 2. Let

\Lambda  = \Lambda \Theta 1 * * * \Lambda \Theta n (\Lambda 1 \Upsilon  \Lambda 2).
We prove the hypothesis by induction on n. For n = 0 the conclusion immediately follows from
Lemma 4.1 and the main induction hypothesis applied either to \Psi 1 \Psi  \Lambda 1, or to \Lambda 2 \Psi  \Psi 2, depending on
where the subterms occur in \Lambda  and \Psi  .

10 TIURYN AND URZYCZYN

Let n > 0 and let \Lambda  = \Lambda \Theta 1 \Lambda  \Phi . Consider a normal derivation of \Lambda \Theta 1 \Lambda  \Phi  \Sigma  \Psi 1 \Upsilon  \Psi 2 in \Sigma LMS. Since\Lambda 
\Theta 1 \Lambda  \Phi  \Sigma  \Psi 1 \Upsilon  \Psi 2 is not an axiom, it follows that the \Lambda \Theta 1 in \Lambda  has been introduced by (\Lambda -left) rule,
followed by a sequence of (\Lambda i -right) rules: (\Lambda i1-right) * * * (\Lambda im -right), with i j > 0, for j = 1, . . . , m
(where m can be equal to 0). We consider the last application of (\Lambda -left) rule and allow m = 0. Let

\Lambda  \Phi (ffl/\Theta 1) \Sigma  \Psi 1 \Upsilon  \Psi  \Phi 2\Lambda 

\Theta 1 \Lambda  \Phi  \Sigma  \Psi 1 \Upsilon  \Psi  \Phi 2 (\Lambda -left)

be the last application of (\Lambda -left) rule in this derivation.

Let us observe that the number of leading quantifiers in \Lambda  \Phi (ffl/\Theta 1) is smaller than n; thus we can apply
the induction hypothesis to \Lambda  \Phi (ffl/\Theta 1) \Psi  \Psi 1 \Upsilon  \Psi  \Phi 2. Assume first that the (\Lambda i -right) rules following the
last (\Lambda -left) rule do not introduce a quantifier below the point where i occurs in \Psi 1 \Upsilon  \Psi 2. Then i also

occurs in \Psi 1 \Upsilon  \Psi  \Phi 2 along j and, by the induction hypothesis, we can conclude that \Pi  \Psi  i or i \Psi  \Pi 
holds, depending on the parity of j. (This is because \Pi  having no free occurrence of \Theta 1 is not affected

by the instantiation and thus remains in \Lambda  (ffl/\Theta 1) along j.) Otherwise, let i\Phi  be a subtype of \Psi 1 \Upsilon  \Psi  \Phi 2
along j which is further transformed into i by the sequence of (\Lambda i -right) rules. It follows that j must
be of the form 1k, for some k >= 0. Thus, by the induction hypothesis we have

\Pi  \Psi  i\Phi .
Thus the sequent \Pi  \Sigma  i\Phi  is derivable in \Sigma LMS. Applying the rules (\Lambda i1*1-right), . . . , (\Lambda im*1-right) we

obtain

\Pi  \Sigma LMS i.
Hence \Pi  \Psi  i holds, as required. \Theta 

LEMMA 4.3. If \Pi  ff \Psi  then \Lambda  (\Pi /\Theta ) ff \Lambda  (\Psi /\Theta ), for all \Lambda  .
Proof. Obvious induction. \Theta 
LEMMA 4.4. Let \Lambda  and \Psi  be types satisfying the following two conditions.

(i) \Psi  contains no quantifier on paths of the form 1m.
(ii) There are two paths, positive j1 and negative j2, such that they lead in \Lambda  and in \Psi  to an
occurrence of the same free variable \Theta .

Then, for every type \Pi ,

\Lambda \Theta  \Lambda  \Psi  \Psi  (\Pi /\Theta ) i \Lambda  (\Pi /\Theta ) \Psi  \Psi  (\Pi /\Theta ).
If both these inequalities are true, then the weight of \Lambda  (\Pi /\Theta ) \Psi  \Psi  (\Pi /\Theta ) is less than or equal to the
weight of \Lambda \Theta  \Lambda  \Psi  \Psi  (\Pi /\Theta ). In addition, if the number of \Upsilon 's in \Pi  is larger than 0 (i.e., |\Pi | > 0), and
both these inequalities are true, then the weight of \Lambda  (\Pi /\Theta ) \Psi  \Psi  (\Pi /\Theta ) is strictly less than the weight of\Lambda 

\Theta  \Lambda  \Psi  \Psi  (\Pi /\Theta ).

Proof. Part (*) is obvious since \Lambda \Theta  \Lambda  \Psi  \Lambda  (\Pi /\Theta ). For the proof of (_) let us assume that \Lambda \Theta  \Lambda  \Psi 
\Psi  (\Pi /\Theta ). Consider a normal derivation of the sequent \Lambda \Theta  \Lambda  \Sigma  \Psi  (\Pi /\Theta ) in \Sigma LMS, and choose one with the
minimal weight, say n. Since \Lambda \Theta  \Lambda  \Sigma  \Psi  (\Pi /\Theta ) is not an axiom, it follows from (i) that the derivation
must end with (\Lambda -left). Thus there is a type ffl such that

\Lambda  (ffl/\Theta ) \Sigma LMS \Psi  (\Pi /\Theta ),
Therefore \Lambda  (ffl /\Theta ) \Psi  \Psi  (\Pi /\Theta ) and applying Lemma 4.2 twice to the paths j1 and j2, we obtain

ffl \Psi  \Pi  and \Pi  \Psi  ffl.

POLYMORPHIC SUBTYPING 11
Thus ffl and \Pi  are bicoercible and therefore \Lambda  (\Pi /\Theta ) \Psi  \Psi  (\Pi /\Theta ), by Lemma 4.3. Clearly \Lambda  (ffl/\Theta ) \Psi  \Psi  (\Pi /\Theta )
has weight less than or equal to n. By Proposition 3.1, \Lambda  (\Pi /\Theta ) \Psi  \Psi  (\Pi /\Theta ) also has weight less than or
equal to n.

In addition, if |\Pi | > 0 then |ffl | > 0 and the weight of \Lambda  (ffl/\Theta ) \Psi  \Psi  (\Pi /\Theta ) is strictly less than n. It follows
from Proposition 3.1 that the weight of \Lambda  (\Pi /\Theta ) \Psi  \Psi  (\Pi /\Theta ) is equal to the weight of \Lambda  (ffl/\Theta ) \Sigma  \Psi  (\Pi /\Theta );
i.e., it is less than n. \Theta 

5. THE STACK-REGISTER MACHINES
Below we describe a computing device, which we call a stack-register machine. The halting problem
for stack-register machines will be used as an intermediate step in our reduction. That is, we first reduce
the halting problem for two-counter machines to the halting problem for stack-register machines, and
then the latter will be reduced to the subtyping problem.

Informally speaking, a stack-register machine is a deterministic one-state device that has two main
registers, and a finite number of auxiliary registers r1, . . . , rn, where each register is capable of holding a
(nonempty) stack of instruction labels. At each step, the machine reads the top label from one of the main
registers (in an alternating manner) and modifies the auxiliary registers according to the appropriate
instruction. That is, the contents of every register ri are replaced by the contents of another register r j ,
with some (or none) labels added at the top of it. An exception is when the top label is the last one
so that the currently scanned main register R becomes empty after reading it. In this case, its contents
are restored by copying r1 onto R before executing the instruction. There is one conditon that must be
satisfied: if ' is a label of an instruction I , then all labels mentioned in I must be less than ' with respect
to a fixed partial order.

To describe our machines more formally, let us first assume that ,^, <=ss is a finite partially ordered
set. Elements of ^ are called instruction labels. An n-ary instruction over ^ is formally defined as a
tuple of pairs (, i1, w1 ss, . . . , , in, wn ss), where i1, . . . , in ' {1, . . . , n} and w1, . . . , wn are words over ^,
but one may prefer to see it as a (simultaneous) assignment command:

(r1, . . . , rn) := \Theta w1 * ri1, . . . , wn * rin \Lambda .
A label is said to occur in the instruction (, i1, w1 ss, . . . , , in, wn ss) iff it occurs in w1 * * * wn.

The result of performing such an instruction on a tuple of strings (v1, . . . , vn) is of course defined as
(w1 * vi1, . . . , wn * vin ). The intended meaning is as follows: suppose (v1, . . . , vn) are the current contents
of registers (r1, . . . , rn), respectively. Then, for each k, the word wkvik is assigned to the register rk.

An n-ary stack-register machine is defined as a quadruple of the form

M = ,^, <=, I, e ss,
where ,^, <= ss is a finite partially ordered set, the symbol e denotes a fixed minimal element of ^, called
an end label, and finally I is a function that assigns an n-ary instruction I(') over ^ to every label in
^ * {e} in such a way that '\Phi  < ' holds for all labels '\Phi  occurring in I(').

An instantaneous description (ID) of an n-ary stack-register machine M is an n + 2-tuple of nonempty
words (V1, V2, v1, . . . , vn). The vi 's are values of auxiliary registers, while V1 and V2 are values of
main registers, the first one being currently scanned. Given such an ID, the next ID is obtained as
follows:

* If V1 = e * V \Phi 1, then there is no next ID--the machine stops.*

If V1 = ' * V \Phi 1 with ' `= e, and (v\Phi 1, . . . , v\Phi n) is the result of performing the instruction I(')
on (v1, . . . , vn), then the next ID is (V2, V \Phi 1, v\Phi 1, . . . , v\Phi n), provided V \Phi 1 is not empty. (Note that the main
registers are switched.)*

If V1 = ' * V \Phi 1 with ' `= e, but V \Phi 1 is empty, then the next ID is (V2, v1, v\Phi 1, . . . , v\Phi n), where
v\Phi 1, . . . , v\Phi n are as above. (That is, the main register is first assigned the value of the auxiliary register r1,
and only then the instruction is normally executed.)

12 TIURYN AND URZYCZYN
The halting problem for stack-register machines is whether a given stack-register machine halts for a
given ID.

6. ENCODING OF A STACK-REGISTER MACHINE BY INEQUALITIES
Given a stack-register machine M = ,^, <=, I, e ss we will encode the halting problem of M by two
types \Lambda  and \Psi  , so that M halts iff \Lambda  \Psi  \Psi  holds.

First we encode finite sequences of instructions of M. For a word w ' ^* we define a type ^\Lambda w.
For this definition we fix two type variables \Xi  and ffi . These variables are going to occur freely in all
types ^\Lambda w, except for those w's which start with the end label e.

In order to properly define types ^\Lambda w, let us assume that ^ = {'1, . . . , 'r } is listed in such a way that
e = '1 and 'i < ' j implies i < j, for all i, j = 1, . . . r. The definition of ^\Lambda w is by induction with

respect to two parameters:

(1) the largest i such that 'i occurs in w (zero if none);
(2) the length of w.

First we encode the end label e by

^\Lambda e = \Lambda \Theta  \Theta 
and the empty word by

^\Lambda * = \Xi .
If ^\Lambda ' has been defined for all labels which occur in w, then we can define ^\Lambda w by induction on the length

of w.

^\Lambda '*w = ^\Lambda '( ^\Lambda w/\Xi ).
The encoding for other labels is as follows. Let I(') = (, i1, w1 ss, . . . , , in, wn ss). We define

^\Lambda ' = \Lambda \Theta 1 . . . \Lambda \Theta n \Xi (\Theta 1 \Upsilon  \Theta 1) \Upsilon  * * * \Upsilon  (\Theta n \Upsilon  \Theta n)

\Upsilon  \Xi \Theta  ^\Lambda w1\Theta \Theta i1/\Xi \Lambda  \Upsilon  ^\Lambda w1\Theta \Theta i1/\Xi \Lambda \Lambda  \Upsilon  * * * \Upsilon  \Theta  ^\Lambda wn \Theta \Theta in /\Xi \Lambda  \Upsilon  ^\Lambda wn \Theta \Theta in /\Xi \Lambda \Lambda 
\Upsilon  \Xi  \Upsilon  ffi \Pi  \Upsilon  ffi \Pi .

Note that, according to our restriction on machine instructions, the words w1, . . . , wn may only contain
labels less than ', and we can assume that ^\Lambda w are already defined.

Clearly, ^\Lambda w has at most one free occurrence of \Xi , and we can assume that there is no bound occurrence
of \Xi . We finally define for w `= * the type

\Lambda w = C[\Theta 1],
where C[ ] is a context such that ^\Lambda w = C[\Xi ]. Note that the variable \Theta 1 gets caught by a quantifier of ^\Lambda w.

The following property follows immediately from our definitions. Let w = w\Phi  * w\Phi \Phi  be such that e
does not occur in w\Phi  and w\Phi \Phi  `= *. Then

\Lambda w = ^\Lambda w\Phi  (\Lambda w\Phi \Phi /\Xi ). (10)
The proof of (10) is by induction on the length of w\Phi . We leave it for the reader.

POLYMORPHIC SUBTYPING 13
We also have for all w\Phi  and w\Phi \Phi ,

\Lambda w\Phi *e*w\Phi \Phi  = \Lambda w\Phi *e.
In particular, |\Lambda w| > 0 iff w does not start with e. We will use this observation in the proof of the
next result.

The reduction is based on the following fact.

PROPOSITION 6.1. M halts for an instantaneous description (V1, V2, v1, . . . , vn) iff

\Lambda V1 \Psi  \Theta \Lambda v1 \Upsilon  \Lambda v1\Lambda  \Upsilon  * * * \Upsilon  \Theta \Lambda vn \Upsilon  \Lambda vn \Lambda  \Upsilon  \Lambda V2 \Upsilon  ffi .
Proof. We prove the "only if" part by induction with respect to the length of computation and the
"if" part by induction with respect to the weight of the inequality.

If (V1, V2, v1, . . . , vn) is a terminal ID, i.e., if V1 = e * V \Phi 1, then \Lambda V1 = \Lambda \Theta  \Theta , and the inequality in the
conclusion of the proposition holds.

Assume that (V1, V2, v1, . . . , vn) is not a terminal ID, i.e., that V1 begins with a label ' `= e. LetI
(') = (, i1, w1 ss, . . . , , in, wn ss) and let (v\Phi 1, . . . , v\Phi n) be the result of performing the instruction I(') on
(v1, . . . , vn). Thus

v\Phi j = w j * vi j , for j = 1, . . . , n. (11)
For the induction to work we need the following two properties.

If V1 = ' * V \Phi 1 with V \Phi 1 `= *, then

\Lambda V1 \Psi  \Theta \Lambda v1 \Upsilon  \Lambda v1\Lambda  \Upsilon  * * * \Upsilon  \Theta \Lambda vn \Upsilon  \Lambda vn \Lambda  \Upsilon  \Lambda V2 \Upsilon  ffi

i \Lambda V2 \Psi  \Theta \Lambda v\Phi 1 \Upsilon  \Lambda v\Phi 1\Lambda  \Upsilon  * * * \Upsilon  \Theta \Lambda v\Phi n \Upsilon  \Lambda v\Phi n \Lambda  \Upsilon  \Lambda V\Phi 1 \Upsilon  ffi . (12)

If V1 = ', then

\Lambda V1 \Psi  \Theta \Lambda v1 \Upsilon  \Lambda v1\Lambda  \Upsilon  * * * \Upsilon  \Theta \Lambda vn \Upsilon  \Lambda vn \Lambda  \Upsilon  \Lambda V2 \Upsilon  ffi

i \Lambda V2 \Psi  \Theta \Lambda v\Phi 1 \Upsilon  \Lambda v\Phi 1\Lambda  \Upsilon  * * * \Upsilon  \Theta \Lambda v\Phi n \Upsilon  \Lambda v\Phi n \Lambda  \Upsilon  \Lambda v1 \Upsilon  ffi . (13)

We prove (12). Let V1 = ' * V \Phi 1 with V \Phi 1 `= *. By (10) we have

\Lambda V1 = \Lambda \Theta 1 * * * \Lambda \Theta n \Xi (\Theta 1 \Upsilon  \Theta 1) \Upsilon  * * * \Upsilon  (\Theta n \Upsilon  \Theta n)

\Upsilon  \Xi \Theta  ^\Lambda w1\Theta \Theta i1/\Xi \Lambda  \Upsilon  ^\Lambda w1\Theta \Theta i1/\Xi \Lambda \Lambda  \Upsilon  * * * \Upsilon  \Theta  ^\Lambda wn \Theta \Theta in /\Xi \Lambda  \Upsilon  ^\Lambda wn \Theta \Theta in /\Xi \Lambda \Lambda 
\Upsilon  \Lambda V\Phi 1 \Upsilon  ffi \Pi  \Upsilon  ffi \Pi .

Let i = (\Theta 1 \Upsilon  \Theta 1) \Upsilon  * * * \Upsilon  (\Theta n \Upsilon  \Theta n) \Upsilon  [( ^\Lambda w1(\Theta i1/\Xi ) \Upsilon  ^\Lambda w1(\Theta i1/\Xi )) \Upsilon  * * * \Upsilon  ( ^\Lambda wn (\Theta in /\Xi ) \Upsilon 

^\Lambda wn (\Theta in /\Xi )) \Upsilon  \Lambda V\Phi 1 \Upsilon  ffi ] \Upsilon  ffi . Applying n times Lemma 4.4, we have the following equivalence

\Lambda V1 \Psi  \Theta \Lambda v1 \Upsilon  \Lambda v1\Lambda  \Upsilon  * * * \Upsilon  \Theta \Lambda vn \Upsilon  \Lambda vn \Lambda  \Upsilon  \Lambda V2 \Upsilon  ffi

i S(i) \Psi  \Theta \Lambda v1 \Upsilon  \Lambda v1\Lambda  \Upsilon  * * * \Upsilon  \Theta \Lambda vn \Upsilon  \Lambda vn \Lambda  \Upsilon  \Lambda V2 \Upsilon  ffi , (14)

where S is a substitution which assigns to each \Theta i type \Lambda vi , for i = 1, . . . , n. Now, applying n + 1 times
Lemma 4.1, we obtain

S(i) \Psi  \Theta \Lambda v1 \Upsilon  \Lambda v1\Lambda  \Upsilon  * * * \Upsilon  \Theta \Lambda vn \Upsilon  \Lambda vn \Lambda  \Upsilon  \Lambda V2 \Upsilon  ffi

i \Lambda V2 \Psi  \Theta  ^\Lambda w1\Theta \Lambda vi1 /\Xi \Lambda  \Upsilon  ^\Lambda w1\Theta \Lambda vi1 /\Xi \Lambda \Lambda  \Upsilon  * * *

\Upsilon  \Theta  ^\Lambda wn \Theta \Lambda vin /\Xi \Lambda  \Upsilon  ^\Lambda wn \Theta \Lambda vin /\Xi \Lambda \Lambda  \Upsilon  \Lambda V\Phi 1 \Upsilon  ffi . (15)

14 TIURYN AND URZYCZYN
By (10), (11), (14) and (15) we obtain (12). The proof of (13) is very similar. The difference is that,
instead of \Lambda V\Phi 1 , we now have \Lambda v1 at the appropriate place at the right-hand side of (15), because of the
bound occurrence of \Theta 1 at the end of \Lambda V1. We leave the details for the reader.

Clearly, the weights of the right-hand sides of (12) and (13) are not greater than the weight of the
respective left-hand sides. Now, if at least one vi does not start with e, then |\Lambda vi| > 0 and by Lemma 4.4,

in both cases the weight of the right-hand side is strictly smaller than the weight of the left-hand side.

Now, using (12) and (13) the proof of the "only if" part follows by a completely routine induction
on the length of computation.

We prove the "if" part by induction on the weight of the inequality. Assume that

\Lambda V1 \Psi  \Theta \Lambda v1 \Upsilon  \Lambda v1\Lambda  \Upsilon  * * * \Upsilon  \Theta \Lambda vn \Upsilon  \Lambda vn \Lambda  \Upsilon  \Lambda V2 \Upsilon  ffi
holds and let the next ID be (U1, U2, u1, . . . , un). Using (12) and (13) we obtain that

\Lambda U1 \Psi  \Theta \Lambda u1 \Upsilon  \Lambda u1\Lambda  \Upsilon  * * * \Upsilon  \Theta \Lambda un \Upsilon  \Lambda un \Lambda  \Upsilon  \Lambda U2 \Upsilon  ffi
must also hold. In addition, if at least one vi does not start with e then the weight of the latter inequality
is strictly less than that of the former. Thus, by induction hypothesis, we are done. On the other hand,
if all vi 's start with e, then we perform the computation steps of M until it either pushes on one of
the auxiliary stacks a symbol different than e, in which case we can apply the previous reasoning, or
else it terminates by eventually copying the first auxiliary register to V1. Hence, in either case M will
terminate. This completes the proof of Proposition 6.1. \Theta 

7. PROGRAMMING WITH STACK-REGISTER MACHINES
We want to prove that the halting problem for stack-register machines is undecidable. For this,
we show how to simulate the behaviour of a two-counter automaton with the help of a stack-register
machine. Before we begin our main construction, let us first make a few observations which will simplify
the consideration.

Let us start with the following remark. We defined machine ID's as tuples (V1, V2, v1, . . . , vn) with
alternating main registers and the current register listed first. This is convenient in Section 6, but for our
present goal we would prefer to have fixed main registers (each given a name) and a flag showing which
one is currently scanned. Formally, this would mean that an ID of a stack-register machine should now
be a tuple (\Theta , V1, V2, v1, . . . , vn), where \Theta  ' {1, 2}. The new ID of the form (1, V1, V2, v1, . . . , vn) is
understood as the old (V1, V2, v1, . . . , vn), while (2, V1, V2, v1, . . . , vn) replaces (V2, V1, v1, . . . , vn).
There is one subtlety here: each old ID of the form (V1, V2, v1, . . . , vn) corresponds to two new ID's,
namely (1, V1, V2, v1, . . . , vn) and (2, V2, V1, v1, . . . , vn). This is, however, not essential--the reader
can easily check that the new approach is equivalent to the old one, after the obvious modification of
the "next ID" function.

The above suggests one more convention: since we can distinguish the two main registers, we do not
have to read their contents from the same auxiliary register. Instead, we could assume that there are two
registers r1 and r2, such that r1 is copied onto the first main register whenever the latter becomes empty
and r2 is used in the same way for the second main register. One possible way to simulate this by an
ordinary stack-register machine may be as follows: the contents of both the main registers are always
read from a new auxilary register r0, but the latter is assigned the same value as r1 in every even step

and the same value as r2 in every odd step. This is not as simple as it can appear at first, because we
have to distinguish between odd steps (reading from the first main register) and the even steps (reading
from the second one). Fortunately, the machine to be constructed below will have a nice property: the
sets of labels occurring in the two main registers will be disjoint, and in this case the above trick does
not cause any difficulty. (But, of course, a simulation is possible in the general case, e.g., by using two
different copies of the alphabet, and two different sets of registers.)

Another convention that may simplify the construction is to present the instruction of a machine in
the form of a sequence of assignment instructions rather than one simultaneous assignment. Of course,

POLYMORPHIC SUBTYPING 15
we mention only relevant assignments, skipping those of the form ri := ri . Fortunately, below we can
always choose the order of assignments so that there is no confusion possible.

The last observation is as follows. With no loss of generality one can assume that an instruction may
assign an arbitrary fixed string to a register, i.e., that instructions may involve assignments of the form
r j := w. To simulate such instructions one needs to keep an extra register rw for every such string w.

This register is assigned the value w in the initial ID and is never changed. Note that w does not itself
occur in the actual assignment r j := rw. Thus, the symbols occurring in w are not counted as occurring
in the instruction, and their positions in the partial order <= do not matter.

After this preparation, let us turn to our main subject. Recall that a two-counter automaton has a finite
set of states, some of them final, and in each state it performs one of the following actions:

* Increase a counter by 1; go to another state;*

Decrease a counter by 1; go to another state;*
Test a counter for zero; go to another state, depending on the result.

The halting problem for two-counter automata (given an automaton, an initial state, and the initial values
of the counters, decide if the machine reaches a final state) is well known to be undecidable (see [3]).
Thus, it suffices for our needs to prove the following:

LEMMA 7.1. The halting problem for two-counter automata is effectively reducible to the halting
problem for stack-register machines.

The proof of this lemma covers the rest of the section. Assume that a two-counter automaton A is given
and that it has only one final state. A configuration of the automaton can be seen as a triple (q, n, m),
where q is a current state, and n and m are the values of the first and second counter, respectively. We
construct a stack-register machine M to simulate A.

The general idea of the simulation is as follows. The first main register, called State, always holds
only one label representing a state of A. There are five labels: q0, q1, q2, q3, q4, for each state q of A.
This is because M makes a sequence of steps for each single step of A, and we want to distinguish
different phases of this process. The values of the counters are stored on two auxiliary registers c1
and c2 so that a number n is coded as a string of the form 1 . . . 1$, with n occurrences of "1." Each
operation on a counter, say c1, is simulated by first copying its value to the second main register, called
Counter, erasing c1, and then rebuilding it to the appropriate new value.

The label alphabet ^ of our machine will consist of two parts: labels used for the State register
will not occur in the Counter register and conversely.

* Labels q0, q1, q2, q3, q4, where q is a state of A, will be used by the State register.*

Labels B (for "begin"), E (for "end"), and the labels 1, $ for coding numbers, will occur in the
Counter register.

The partial order on the set of labels is such that q3 > 1 and q4 > 1 holds for all q, and all other
elements are incomparable. This means that 1 is the only label that can be pushed to the stacks and that
it can only happen when the machine scans labels q3 or q4. The end label is q f0 , where q f is the final
state of A.

Our machine has eight auxiliary registers: next-State, next-Counter, c1, c2, new-0, new1, alt-new-0, alt-new-1. These registers are used as follows:

* The value of next-Counter is read to Counter, whenever the latter becomes empty.*

Similarly, next-State always holds the next value for State.*
Registers c1, c2 store the values of counters.*
The role of the other four registers is to store information on the next state of the automatonA
. That is, when A is in state q and will move to state p after completing a counter operation, then
the desired values of new-0 and new-1 are p0 and p1, respectively. Registers alt-new-0 and
alt-new-1 are necessary in case of a test for zero, since there are two possibilities for the next state
of A.

16 TIURYN AND URZYCZYN
For an arbitrary n, let n stand for the string 1n$. An ID of A of the form (q, n, m) will be represented
by an ID of M, such that the values of registers are as follows,

State = q0, Counter = B, c1 = n, c2 = m, next-State = q1,
and that State is currently scanned. The values of other registers do not matter. Note that a final ID
of A is only represented by final ID's of M. Our goal is to construct M so that (*) below implies (**),
for all triples (q, n, m) and ( p, n\Phi , m\Phi ).

(*) The automaton A moves from (q, n, m) to ( p, n\Phi , m\Phi ) in one step;
(**) The machine M, when started in an ID representing (q, n, m), after a sequence of moves

reaches an ID representing ( p, n\Phi , m\Phi ).

The behaviour of M obeys the alternating pattern of passing control from one main register to the
other. That is, we have two processes, which are, to some extent, independent from each other (but
communicating with the help of the common auxiliary registers). Let us begin with explaining the
actions associated to the Counter register. The contents of Counter will always be either a numeral
n = 1n$ or a single symbol E or B.

If the symbol on the top of Counter is B then the machine executes the instruction I(B), which
consists of only one assignment (i.e., other registers remain unchanged):

next-Counter := E.
Of course, the symbol B is removed from the stack, and it is normally the last symbol. This means that
the previous contents of next-Counter is copied to Counter before the above assignment takes
place.

The instruction I(1) to be performed when "1" is read from Counter is:

new-0 := alt-new-0; new-1 := alt-new-1.
We explain the meaning of this instruction later, and now we just note that "1" will normally not occur
at the bottom of the stack and that reading it amounts to decreasing the stored number by 1.

If the symbol $ is read from Counter, it will normally be the last symbol, so the new value of
Counter is read from next-Counter, and then there are the assignments:

next-State := new-0; next-Counter := B.
Finally, if the symbol on the top of Counter is E, then the assignment to be performed is:

next-State := new-1.
Suppose now that the machine is started in an ID representing (q, n, m) with the value of nextCounter set to a numeral n = 1n$. It should be easy to see that the contents of Counter change as
follows: first B is replaced by n. This string is being read symbol by symbol until the stack is empty.
The next symbol read from next-Counter is E and then again B.

Now we describe the instructions associated to labels occurring on the State register and their
meaning. Suppose for the beginning that the action performed by A in state q is to decrease the value
of the first counter and to change the state to p. The instruction I(q0) can be written as the following
sequence of assignments:

next-Counter := c1;
c1 := 0;
next-State := q2;
new-0 := p0; alt-new-0 := p0;
new-1 := p1; alt-new-1 := p1.

The other instructions related to q are much simpler:

POLYMORPHIC SUBTYPING 17

* I(q1) is "next-State := q3";* I

(q2) is "next-State := q4";* I
(q3) is empty;* I
(q4) is "c1 := 1 * c1".

Assume again that the machine is started in an ID representing (q, n, m). Since State = q0, the value
of next-Counter is set to n. Thus, according to what we observed before, it takes 2n + 6 steps to
reach another ID of M satisfying Counter = B, and such that State is scanned. The contents of
Counter during these 2n + 6 steps changes as follows:

B \Upsilon  B \Upsilon  n \Upsilon  n \Upsilon  n * 1 \Upsilon  n * 1 \Upsilon  * * * \Upsilon  1 \Upsilon  1 \Upsilon  $ \Upsilon  $ \Upsilon  E \Upsilon  E \Upsilon  B.
The underlined symbols refer to the ID's in which Counter is being read.

Consider now the behaviour of the registers new-0 and alt-new-0 during these 2n + 6 steps. They
are both set to p0 at the beginning, and no further instruction can change it. Thus, executing I($) will
result in next-State = p0. We conclude that at the last step, the value of State is p0. Similarly,
new-1 and alt-new-1 are assigned p1, and thus at the last moment next-State = p1.

In order to show that the final ID of our 2n + 6 steps does represent the configuration ( p, n, m) of A,
it remains to check that c1 = n * 1 (as it should be obvious that the contents of c2 remain unchanged).
For this it suffices to note that we have the following sequence of labels occurring on the State register
(scanned labels are underlined).

q0 \Upsilon  q1 \Upsilon  q1 \Upsilon  q2 \Upsilon  q2 \Upsilon  q3 \Upsilon  q3 \Upsilon  q4 \Upsilon  q4 \Upsilon  * * * \Upsilon  q4 \Upsilon  q4 \Upsilon  q4 \Upsilon  p0
with I(q4) executed exactly n * 1 times. (If n = 0 or n = 1 then I(q4) is never executed.)

If the action performed by A in state q is to increase the first counter rather than to decrease it,
the instructions I(q1), I(q2), and I(q4) are defined as in the previous case. The instruction I(q3) is
"c1 := 1 * c1"; that is, it is identical to I(q4) (we keep a different label just for uniformity). Finally,I

(q0) differs from the previous case in that it contains the assignment "c1 := 1" instead of "c1 := 0."
It is left to the reader to check that after 2n + 6 steps we again obtain a desired ID.

The last case is a test for zero. Assume that A changes its state from q to p if the first counter is zero,
and to s otherwise. Then we define I(q0) as follows:

next-Counter := c1;
c1 := 0;
next-State := q2;
new-0 := p0; alt-new-0 := s0;
new-1 := p1; alt-new-1 := s1;

and I(qi ), for i = 1, 2, 3, 4, are the same as for the instructions increasing the first counter (i.e., q3
adds 1 to c1). Since this time c1 begins with zero and is increased n times, its final value is the same.
However, the final contents of State and next-State will now depend on whether n is zero or not.
Indeed, the values p0 and p1 will remain in registers new-0 and new-1 only if I(1) is never executed,
i.e., if n = 0. The details are left for the reader.

Of course, instructions related to c2 are defined analogously, and the instructions I(q fi ), for i > 0,
may be arbitrary, because these labels will never show up on the main registers. The reader may now
easily conclude that the condition (*) implies (**), for every configuration of A and every ID of M
representing it. Since both machines are deterministic, it follows that we have the equivalence of the
following two conditions:

(*) The automaton A terminates when started in (q, n, m);
(**) The machine M terminates when started in an ID representing (q, n, m).

Thus, in order to answer the question "does A halt if started in the configuration (q, n, m)" it is enough
to take any ID of M that represents (q, n, m) and ask if M halts when started in this ID. This concludes
the proof of Lemma 7.1 and of Theorem 2.1.

18 TIURYN AND URZYCZYN

ACKNOWLEDGMENT
The authors thank Damian Niwi'nski for pointing out a gap in an earlier version of this proof.

REFERENCES
1. Barendregt, H. P. (1984), "The Lambda Calculus: Its Syntax and Semantics," North-Holland, Amsterdam.
2. Chrza,szcz, J. (1998), Polymorphic subtyping without distibutivity, in "Proc. Mathematical Foundations of Computer Science"

(L. Brim, J. Gruska, and J. Zlatuska, Eds.), Lecture Notes in Computer Science, Vol. 1450, pp. 346-355, Springer-Verlag,
Berlin.
3. Hopcroft, J. E., and Ullman, J. D. (1979), "Introduction to Automata Theory, Languages and Computation," Addison-Wesley,

Reading, MA.
4. Jim, T. (1995), System F plus subsumption reduces to Mitchell's subtyping relation, manuscript.
5. Longo, G., Milsted, K., and Soloviev, S. (1995), A logic of subtyping, in "Proc. 10th IEEE Symp. Logic in Computer Science,

San Diego," pp. 292-299.
6. Mitchell, J. C. (1988), Polymorphic type inference and containment, Inform. and Comput. 76, 211-249.
7. Odersky, M., and L"aufer, K. (1995), Putting type annotations to work, presented at the Newton Institute Workshop Advances

in Type Systems for Computing, August 1995.
8. Pierce, B. C. (1994), Bounded quantification is undecidable, Inform. and Comput. 112, 131-165.
9. Tiuryn, J. (1995), Equational axiomatization of bicoercibility for polymorphic types, in "Proc. Conf. Foundations of Software

Technology and Teoretical Computer Science'95" (P. S. Thiagarajan, Ed.), Lecture Notes in Computer Science, Vol. 1026,
pp. 166-179, Springer-Verlag, Berlin.
10. Wells, J. B. (1999), Typability and type checking in the second-order \Omega -calculus calculus are equivalent and undecidable,

Ann. Pure Appl. Logic 98, 111-156. (Preliminary version in "Proc. 9th IEEE Symposium on Logic in Computer Science,
Paris, France, 1994," pp. 176-185.)
11. Wells, J. B. (1995), "The Undecidability of Mitchell's Subtyping Relationship," Technical Report 95-019, Computer Science

Department, Boston University.
12. Wells, J. B. (1996), "Typability is Undecidable for F+Eta," Technical Report 96-022, Computer Science Department, Boston

University.