

June 11, 1998
SRC

Research
Report 158

A Type System for Java Bytecode Subroutines

Raymie Stata and Mart'in Abadi

d i g i t a l
Systems Research Center
130 Lytton Avenue
Palo Alto, California 94301

http://www.research.digital.com/SRC/

Systems Research Center
The charter of SRC is to advance both the state of knowledge and the state of the art
in computer systems. From our establishment in 1984, we have performed basic and
applied research to support Digital's business objectives. Our current work includes
exploring distributed personal computing on multiple platforms, networking, programming technology, system modelling and management techniques, and selected
applications.

Our strategy is to test the technical and practical value of our ideas by building
hardware and software prototypes and using them as daily tools. Interesting systems
are too complex to be evaluated solely in the abstract; extended use allows us to
investigate their properties in depth. This experience is useful in the short term in
refining our designs, and invaluable in the long term in advancing our knowledge.
Most of the major advances in information systems have come through this strategy,
including personal computing, distributed systems, and the Internet.

We also perform complementary work of a more mathematical flavor. Some of it
is in established fields of theoretical computer science, such as the analysis of algorithms, computational geometry, and logics of programming. Other work explores
new ground motivated by problems that arise in our systems research.

We have a strong commitment to communicating our results; exposing and testing
our ideas in the research and development communities leads to improved understanding. Our research report series supplements publication in professional journals and conferences. We seek users for our prototype systems among those with
whom we have common interests, and we encourage collaboration with university
researchers.

A Type System for Java Bytecode Subroutines
Raymie Stata and Mart'in Abadi
June 11, 1998

This paper will appear in the ACM Transactions on Programming Languages and
Systems. A preliminary version appeared in the Proceedings of the 25th ACM
Symposium on Principles of Programming Languages.

cflACM 1998
All rights reserved. Published by permission. Permission to make digital or hard
copies of part or all of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first page. Copyrights
for components of this work owned by others than ACM, Inc. must be honored.
Abstracting with credit is permitted. To copy otherwise, to republish, to post on
servers, or to redistribute to lists, requires prior specific permission and/or a fee.
Request permission from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or
permissions@acm.org.

Abstract
Java is typically compiled into an intermediate language, JVML, that is interpreted
by the Java Virtual Machine. Because mobile JVML code is not always trusted,
a bytecode verifier enforces static constraints that prevent various dynamic errors.
Given the importance of the bytecode verifier for security, its current descriptions
are inadequate. This paper proposes using typing rules to describe the bytecode
verifier because they are more precise than prose, clearer than code, and easier to
reason about than either.

JVML has a subroutine construct which is used for the compilation of Java's tryfinally statement. Subroutines are a major source of complexity for the bytecode
verifier because they are not obviously last-in/first-out and because they require a
kind of polymorphism. Focusing on subroutines, we isolate an interesting, small
subset of JVML. We give typing rules for this subset and prove their correctness.
Our type system constitutes a sound basis for bytecode verification and a rational
reconstruction of a delicate part of Sun's bytecode verifier.

Contents
1 Bytecode verification and typing rules 1
2 Overview of JVML subroutines and our type system 4
3 Syntax and informal semantics of JVML0 7
4 Semantics without subroutines 8

4.1 Dynamic semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.2 Static semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.3 One-step soundness theorem . . . . . . . . . . . . . . . . . . . . . . . 12

5 Structured semantics 12

5.1 Dynamic semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5.2 Static semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.3 One-step soundness theorem . . . . . . . . . . . . . . . . . . . . . . . 16

6 Stackless semantics 18

6.1 Dynamic semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
6.2 Static semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
6.3 One-step soundness theorem . . . . . . . . . . . . . . . . . . . . . . . 22

7 Main soundness theorem 25
8 Sun's rules 25

8.1 Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
8.2 Technical differences . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

9 Other related work 27
10 Conclusions 28
Appendix 28
A Proof of soundness for the structured semantics 28

A.1 Preservation of WFCallStack . . . . . . . . . . . . . . . . . . . . . . 28
A.2 Lemmas about types . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
A.3 Preservation of stack typing . . . . . . . . . . . . . . . . . . . . . . . 34
A.4 Preservation of local variable typing . . . . . . . . . . . . . . . . . . 35

B Proof of soundness for the stackless semantics 36

B.1 Analogous steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
B.2 Preservation of Consistent . . . . . . . . . . . . . . . . . . . . . . . . 39

B.2.1 Miscellaneous lemmas . . . . . . . . . . . . . . . . . . . . . . 40
B.2.2 Preservation of WFCallStack2 . . . . . . . . . . . . . . . . . . 44
B.2.3 Preservation of GoodStackRets . . . . . . . . . . . . . . . . . 47
B.2.4 Preservation of GoodFrameRets . . . . . . . . . . . . . . . . . 50

C Proof of main soundness theorem 54

C.1 Making progress . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
C.2 Chained soundness theorem . . . . . . . . . . . . . . . . . . . . . . . 57

Acknowledgements 58
References 61

1 Bytecode verification and typing rules
The Java language is typically compiled into an intermediate language that is interpreted by the Java Virtual Machine (VM) [LY96]. This intermediate language, which
we call JVML, is an object-oriented language similar to Java. Its features include
packages, classes with single inheritance, and interfaces with multiple inheritance.
However, unlike method bodies in Java, method bodies in JVML are sequences of
bytecode instructions. These instructions are fairly high-level but, compared to the
structured statements used in Java, they are more compact and easier to interpret.

JVML code is often shipped across networks to Java VMs embedded in web
browsers and other applications. Mobile JVML code is not always trusted by the
VM that receives it. Therefore, a bytecode verifier enforces static constraints on
mobile JVML code. These constraints rule out type errors (such as dereferencing an
integer), access control violations (such as accessing a private method from outside
its class), object initialization failures (such as accessing a newly allocated object
before its constructor has been called), and other dynamic errors.

Figure 1 illustrates how bytecode verification fits into the larger picture of Java
security. The figure represents trusted and untrusted code. At the base of the
trusted code is the Java VM itself--including the bytecode verifier--plus the operating system, which provides access to privileged resources. On top of this base layer
is the Java library, which provides controlled access to those privileged resources.
Java security depends on the VM correctly interpreting JVML code, which in turn
depends on the verifier rejecting illegal JVML code. If the verifier were broken but
the rest of the VM assumed it was correct, then JVML code could behave in ways
not anticipated by the Java library, circumventing the library's access controls.

For example, the current implementation of the library contains private methods
that access privileged resources without performing access control checks. This implementation depends on the verifier preventing untrusted code from calling those
private methods. But if the verifier were broken, an attacker might be allowed to
treat an instance of a library class as an instance of a class defined by the attacker.
In this situation, the class defined by the attacker could be used as a back door. At
runtime, most VMs use dispatch-table indices to name methods. Successful invocation of a method depends only on this index and on passing the right number and
type of arguments to the method. If the verifier allowed the attacker to treat an
instance of the library class as an instance of the back-door class, then the attacker
could call the instance's methods, including the private ones that do not perform
access control checks.

As this example shows, a successful attack requires more than a broken verifier
that accepts rogue JVML code. If, in our example, a VM had some runtime mechanism for preventing the use of an instance of an unexpected class, then the attack

1

Trusted code
Untrustred code

Hostile JVML code

Librarylevel

Baselevel
Java library (includingreference monitors)

C libraries/syscalls forprotected resources Java VM(including bytecode verifier)

Figure 1: The Java VM and security
would fail. However, this mechanism might be expensive. Thus, the purpose of
the verifier is to permit implementations of JVML that are fast without sacrificing
safety. The primary customer of the verifier is the rest of the VM implementation:
instruction interpretation code, JIT-compiler routines, and other code that assumes
that illegal input code is filtered out by the verifier.

Given its importance for security, current descriptions of the verifier are deficient.
The official specification of the verifier is a prose description [LY96]. Although good
by the standards of prose, this description is ambiguous, imprecise, and hard to reason about. In addition to this specification, Sun distributes what could be considered
a reference implementation of the verifier. As a description, this implementation is
precise, but it is hard to understand and, like the prose description, is hard to reason
about. Furthermore, the implementation disagrees with the prose description.

This paper proposes using typing rules to describe the verifier. Typing rules
are more precise than prose, easier to understand than code, and they can be manipulated formally. Such rules would give implementors of the verifier a systematic
framework on which to base their code, increasing confidence in its correctness. Such
rules would also give implementors of the rest of the VM an unambiguous statement
of what they can and cannot assume about legal JVML code.

From a typing perspective, JVML is interesting in at least two respects:

* In JVML, a location can hold different types of values at different program

points. This flexibility allows locations to be reused aggressively, allowing
interpreters to save space. Thus, JVML contrasts with most typed languages,
in which a location has only one type throughout its scope.

* JVML has subroutines to help compilers generate compact code, for example

2

for Java try-finally statements. JVML subroutines are subsequences of the
larger sequence of bytecode instructions that make up a method's body. The
JVML instruction jsr jumps to the start of one of these subsequences, and the
JVML instruction ret returns from one. Subroutines introduce two significant
challenges to the design of the bytecode verifier: the verifier should ensure that
ret is used in a well-structured manner, and it should support a certain kind
of polymorphism. We describe these challenges in more detail in Section 2.

This paper addresses these typing problems. It defines a small subset of JVML,
called JVML0, presents a type system and a dynamic semantics for JVML0, and
proves the soundness of the type system with respect to the dynamic semantics.
JVML0 includes only 9 instructions, and ignores objects and several other features
of JVML. This restricted scope allows us to focus on the challenges introduced by
subroutines. Thus, our type system provides a precise, sound approach to bytecode
verification, and a rational reconstruction of a delicate part of Sun's bytecode verifier.

We present the type system in three stages:

1. The first stage is a simplified version for a subset of JVML0 that excludes jsr

and ret. This simplified version provides an introduction to our notation and
approach, and illustrates how we give different types to locations at different
program points.

2. The next stage considers all of JVML0 but uses a structured semantics for jsr

and ret. This structured semantics has an explicit subroutine call stack for
ensuring that subroutines are called on a last-in, first-out basis. In the context
of this structured semantics, we show how to achieve the polymorphism desired
for subroutines.

3. The last stage uses a stackless semantics for jsr and ret in which return addresses are stored in random-access memory. The stackless semantics is closer
to Sun's. It admits more efficient implementations, but it does not dynamically
enforce a last-in, first-out discipline on calls to subroutines. Because such a
discipline is needed for type safety, we show how to enforce it statically.

The next section describes JVML subroutines in more detail and summarizes
our type system. Section 3 gives the syntax and an informal semantics for JVML0.
Sections 4-6 present our type system in the three stages outlined above. Section 7
states and proves the main soundness theorem for our type system. Sections 8 and 9
discuss related work, including Sun's bytecode verifier. Section 8 also considers how
our type system could be extended to the full JVML. Section 10 concludes. Several
appendices contain proofs.

3

2 Overview of JVML subroutines and our type system
JVML subroutines are subsequences of a method's larger sequence of instructions;
they behave like miniature procedures within a method body. Subroutines are used
for compiling Java's try-finally statement.

Consider, for example, the Java method named bar at the top of Figure 2.
The try body can terminate in one of three ways: immediately when i does not
equal 3, with an exception raised by the call to foo, or with an execution of the
return statement. In all cases, the compiler must guarantee that the code in the
finally block is executed when the try body terminates. Instead of replicating the
finally code at each escape point, the compiler can put the finally code in a JVML
subroutine. Compiled code for an escape from a try body executes a jsr to the
subroutine containing the finally code.

Figure 2 illustrates the use of subroutines for compiling try-finally statements.
It contains a possible result of compiling the method bar into JVML, putting the
finally code in a subroutine in lines 13-16.

Figure 2 also introduces some of JVML's runtime structures. JVML bytecode
instructions read and write three memory regions. The first region is an object heap
shared by all method activations; the heap does not play a part in the example
code of Figure 2. The other two regions are private to each activation of a method.
The first of these private regions is the operand stack, which is intended to be used
on a short-term basis in the evaluation of expressions. For example, the instruction
iconst 3 pushes the integer constant 3 onto this stack, while ireturn terminates the
current method and returns the integer at the top of the stack. The second private
region is a set of locations known as local variables, which are intended to be used on
a longer-term basis to hold values across expressions and statements (but not across
method activations). Local variables are not operated on directly. Rather, values in
local variables are pushed onto the stack and values from the stack are popped into
local variables via the load and store instructions respectively. For example, the
instruction aload 0 pushes the object reference in local variable 0 onto the operand
stack, while the instruction istore 2 pops the top value off the operand stack and
saves it in local variable 2.

Subroutines pose two challenges to the design of a type system for JVML:

* Polymorphism. Subroutines are polymorphic over the types of the locations

they do not touch. For example, consider how variable 2 is used in Figure 2. At
the jsr on line 7, variable 2 contains an integer and is assumed to contain an
integer when the subroutine returns. At the jsr on line 18, variable 2 contains
a pointer to an exception object and is assumed to contain a pointer to an
exception object when the subroutine returns. Inside a subroutine, the type

4

int bar(int i) {try {

if (i == 3) return this.foo();} finally {
this.ladida();}
return i;}

01 iload 1 // Push i02

iconst 3 // Push 303
if icmpne 10 // Goto 10 if i does not equal 3
// Then case of if statement04
aload 0 // Push this05
invokevirtual foo // Call this.foo06
istore 2 // Save result of this.foo()07
jsr 13 // Do finally block before returning08
iload 2 // Recall result from this.foo()09
ireturn // Return result of this.foo()
// Else case of if statement10
jsr 13 // Do finally block before leaving try
// Return statement following try statement11
iload 1 // Push i12
ireturn // Return i
// finally block13
astore 3 // Save return address in variable 314
aload 0 // Push this15
invokevirtual ladida // Call this.ladida()16
ret 3 // Return to address saved on line 13
// Exception handler for try body17
astore 2 // Save exception18
jsr 13 // Do finally block19
aload 2 // Recall exception20
athrow // Rethrow exception
// Exception handler for finally body21
athrow // Rethrow exception

Exception table (maps regions of code to their exception handlers):Region Target

1-12 1713-16 21

Figure 2: Example compilation of try-finally into JVML

5

// Assume variable 0 contains pointer to this01
iconst 1 // Push 102
istore 1 // Initialize variable 1 to 103
jsr 13 // Call subroutine04
aload 0 // Push this05
iconst 0 // Push 006
putfield x // Set this.x to 007
iconst 1 // Push 108
istore 0 // Set variable 0 to 109
iconst 0 // Push 010
istore 1 // Set variable 1 to 011
jsr 13 // Call subroutine12
return // End of method
// Top of subroutine13
iload 1 // Push variable 114
iconst 0 // Push 015
if icmpeq 18 // Goto 18 if variable 1 equals 016
astore 2 // Save return address in variable 217
ret 2 // Return to return address in variable 218
pop // Throw out current return address19
ret 2 // Return to old address saved in variable 2

Figure 3: Illegal JVML code
of a location such as variable 2 can depend on the call site of the subroutine.
(Subroutines are not parametric over the types of the locations they touch; the
polymorphism of JVML is thus weaker than that of ML.)

* Last-in, first-out behavior. In most languages, when a return statement in

procedure P is executed, the dynamic semantics guarantees that control will
return to the point from which P was most recently called. The same is not
true of JVML. The ret instruction takes a variable as a parameter and jumps
to whatever address that variable contains. This semantics means that, unless
adequate precautions are taken, the ret instruction can transfer control to
almost anywhere.

Using ret to jump to arbitrary places in a program is inimical to static typing,
especially in the presence of polymorphism. For example, consider the illegal JVML
method body in Figure 3. This method body has two calls to the subroutine that
starts at line 13, which is polymorphic over variable 0. The first time this subroutine
is called, it stores the return address into variable 2 and returns. The second time
this subroutine is called, it returns to the old return address stored away in variable 2.

6

This causes control to return to line 4, at which point the code assumes that variable 0
contains a pointer. However, between the first and second calls to the subroutine, an
integer has been stored into variable 0. Thus, the code in Figure 3 will dereference
an integer.

Our type system allows polymorphic subroutines and enforces last-in, first-out
behavior. It consists of rules that relate a program (a sequence of bytecode instructions) to static information about types and subroutines. This information maps
each memory location of the VM to a type at each program point, identifies the
instructions that make up subroutines, indicates the variables over which subroutines are polymorphic, and gives static approximations to the dynamic subroutine
call stack.

Our type system guarantees the following properties for well-typed programs:

* Type safety. An instruction will never be given an operand stack with too few

values in it, or with values of the wrong type.

* Program counter safety. Execution will not jump to undefined addresses.

* Bounded operand stack. The size of the operand stack will never grow beyond

a static bound.

3 Syntax and informal semantics of JVML0
In JVML0, our restricted version of JVML, a program is a sequence of instructions:

P ::= instruction*
We treat programs as partial maps from addresses to instructions. We write Addr
for the set of all addresses. Addresses are very much like positive integers, and we use
the constant 1 and the function + on addresses. However, to provide more structure
to our semantics, we treat numbers and addresses as separate sets. When P is a
program, we write Dom(P ) for the domain of P (its set of addresses); P [i] is defined
only for i 2 Dom(P ). We assume that 1 2 Dom(P ) for every program P .

In JVML0, there are no classes, methods, or objects. There is no object heap,
but there is an operand stack and a set of local variables. We write Var for the set
of names of local variables. (In JVML, these names are actually natural numbers,
but we do not require this.) Local variables and the operand stack both contain
values. A value is either an integer or an address.

7

JVML0 has only 9 instructions:

instruction ::= inc | pop | push0|

load x | store x|
if L|
jsr L | ret x|
halt

where x ranges over Var, and L ranges over Addr. Informally, these instructions
behave as follows:

* The inc instruction increments the value at the top of the operand stack if

that value is an integer. The pop instruction pops the top off the operand
stack. The push0 instruction pushes the integer 0 onto the operand stack.

* The load x instruction pushes the current value of local variable x onto the

operand stack. The store x instruction pops the top value off the operand
stack and stores it into local variable x.

* The if L instruction pops the top value off the operand stack and either falls

through when that value is the integer 0 or jumps to L otherwise.

* At address p, the jsr L instruction jumps to address L and pushes return

address p + 1 onto the operand stack. The ret x instruction jumps to the
address stored in x.

* The halt instruction halts execution.

4 Semantics without subroutines
This section introduces our approach and some notation. It presents static and
dynamic semantics for the subset of JVML0 that excludes jsr and ret.

We use (partial) maps extensively throughout this paper. When g is a map,
Dom(g) is the domain of g; for x 2 Dom(g), g[x] is the value of g at x, and g[x 7! v]
is the map with the same domain as g defined by the following equation, for all
y 2 Dom(g):

(g[x 7! v])[y] = ( g[y] x 6= yv x = y

That is, g[x 7! v] has the same value as g at all points except x, where g[x 7! v] has
value v. We define equality on maps as follows:

f = g j Dom(f ) = Dom(g) ^ 8x 2 Dom(f ). f [x] = g[x]

8

That is, f = g exactly when f and g have the same domains and have equal values
at all points.

We often use maps with domain Addr. We call those maps vectors. When F is
a vector and i is an address, we may write F i instead of F [i].

We also use strings. The constant ffl denotes the empty string. If s is a string,
then v * s denotes the string obtained by prepending v to s. Sometimes we treat
strings as maps from indices to integers. Then Dom(s) is the set of indices of s, and
s[i] is the ith element of s from the right. Concatenation adds a new index for the
new element but does not change the indices of the old elements; that is:

8s, i, n. i 2 Dom(s) ) i 2 Dom(n * s) ^ (n * s)[i] = s[i]

4.1 Dynamic semantics
We model a state of an execution as a tuple hpc, f, si, where pc is the program
counter, f is the current state of local variables, and s is the current state of the
operand stack.

* The program counter pc is an address, that is, an element of Addr.

* The current state of local variables f is a total map from Var to the set of

values.

* The current state of the stack s is a string of values.
All executions start from states of the form h1, f, ffli, where f is arbitrary and where
ffl represents the empty stack.

Figure 4 contains a small-step operational semantics for all instructions other
than jsr and ret. This semantics relies on the judgment

P ` hpc, f, si ! hpc0, f 0, s0i
which means that program P can, in one step, go from state hpc, f, si to stateh

pc0, f 0, s0i. By definition, the judgment P ` hpc, f, si ! hpc0, f 0, s0i holds only
when P is a program and hpc, f, si and hpc0, f 0, s0i are states. In Figure 4 (and in
the rest of this paper), n matches only integers while v matches any value. Thus,
for example, the pattern "n * s" represents a non-empty stack whose top element
is an integer. Note that there is no rule for halt--execution stops when a halt
instruction is reached. Execution may also stop as the result of a dynamic error, for
example attempting to execute a pop instruction with an empty stack.

9

P [pc] = inc
P ` hpc, f, n * si ! hpc + 1, f, (n + 1) * si

P [pc] = pop
P ` hpc, f, v * si ! hpc + 1, f, si

P [pc] = push0
P ` hpc, f, si ! hpc + 1, f, 0 * si

P [pc] = load x
P ` hpc, f, si ! hpc + 1, f, f[x] * si

P [pc] = store x
P ` hpc, f, v * si ! hpc + 1, f[x 7! v], si

P [pc] = if L
P ` hpc, f, 0 * si ! hpc + 1, f, si

P [pc] = if L

n 6= 0
P ` hpc, f, n * si ! hL, f, si

Figure 4: Dynamic semantics without jsr and ret
4.2 Static semantics
Our static semantics employs the following typing rules for values:

v is a value

v : Top

n is an integer

n : Int

K, L are addresses
K : (ret-from L)

The type Top includes all values. The type Int is the type of integers. Types of the
form (ret-from L) include all addresses. However, the typing rules for programs
of Sections 5 and 6 make a more restricted use of address types, preserving strong
invariants. As the syntax (ret-from L) suggests, we use L as the name for the
subroutine that starts at address L, and use (ret-from L) as the type of return
addresses generated when L is called. Collectively, we refer to the types Top, Int,
and (ret-from L) as value types.

Types are extended to stacks as follows:

(Empty hypothesis)

ffl : ffl

v : T s : ff

v * s : T * ff

where T is a value type and ff is a string of value types. The empty stack is typed
by the empty string; a stack with n values is typed by a string with n types, each
type correctly typing the corresponding value in the string of values.

A program is well-typed if there exist a vector F of maps from variables to types
and a vector S of strings of types satisfying the judgment:

F , S ` P

10

P [i] = inc
F i+1 = F i
Si+1 = Si = Int * ff

i + 1 2 Dom(P )

F , S, i ` P

P [i] = if L
F i+1 = F L = F i
Si = Int * Si+1 = Int * SL

i + 1 2 Dom(P )

L 2 Dom(P )

F , S, i ` P

P [i] = pop
F i+1 = F i
Si = T * Si+1
i + 1 2 Dom(P )

F , S, i ` P

P [i] = push0

F i+1 = F i
Si+1 = Int * Si
i + 1 2 Dom(P )

F , S, i ` P

P [i] = load x

x 2 Dom(Fi)

F i+1 = F i
Si+1 = F i[x] * Si

i + 1 2 Dom(P )

F , S, i ` P

P [i] = store x

x 2 Dom(Fi)
F i+1 = F i[x 7! T ]

Si = T * Si+1
i + 1 2 Dom(P )

F , S, i ` P

P [i] = halt

F , S, i ` P

Figure 5: Static semantics without jsr and ret
The vectors F and S contain static information about the local variables and operand
stack, respectively. The map F i assigns types to local variables at program point i.
The string Si gives the types of the values in the operand stack at program point i.
(Hence the size of Si gives the number of values in the operand stack.) For notational
convenience, the vectors F and S are defined on all of Addr even though P is not;
F j and Sj are dummy values for out-of-bounds j.

We have one rule for proving F , S ` P :

F 1 = E

S1 = ffl8
i 2 Dom(P ). F , S, i ` P

F , S ` P

where E is the map that maps all variables to Top and ffl is (as usual) the empty
string. The first two hypotheses are initial conditions; the third is a local judgment
applied to each program point. Figure 5 has rules for the local judgment F , S, i ` P .

11

These rules constrain F and S at point i by referring to F j and Sj for all points j
that are control-flow successors of i.

4.3 One-step soundness theorem
The rules above are sound in that any step from a well-typed state leads to another
well-typed state:

Theorem 1 For all P , F , and S such that F , S ` P :

8pc, f, s, pc0, s0, f 0.^

P ` hpc, f, si ! hpc0, f 0, s0i^ 8

y. f [y] : Fpc[y]^
s : Spc)
s0 : Spc0 ^ 8y. f 0[y] : F pc0[y]

We do not prove this theorem, but we do prove similar theorems for programs including jsr and ret. (See the appendices.)

5 Structured semantics
This section shows how to handle jsr and ret with the kind of polymorphism
described in Section 2. To isolate the problem of polymorphism from the problem of
ensuring that subroutines are used in a well-structured manner, this section presents
what we call the structured semantics for JVML0. This semantics is structured
in that it defines the semantics of jsr and ret in terms of an explicit subroutine
call stack. This section shows how to achieve polymorphism in the context of the
structured semantics. The next section shows how to ensure that subroutines are
used in a well-structured manner in the absence of an explicit call stack.

5.1 Dynamic semantics
In the structured semantics, we augment the state of an execution to include a
subroutine call stack. This call stack holds the return addresses of subroutines that
have been called but have not yet returned. We model this call stack as a string ae
of addresses. Thus, the state of an execution is now a four-tuple hpc, f, s, aei.

Figure 6 defines the structured dynamic semantics of JVML0. The rules of
Figure 6 use the subroutine call stack to communicate return addresses from jsr
instructions to the corresponding ret instructions. Although ret takes an operand
x, the structured dynamic semantics ignores this operand; similarly, the structured
dynamic semantics of jsr pushes the return address onto the operand stack as well as

12

P [pc] = inc
P `s hpc, f, n * s, aei ! hpc + 1, f, (n + 1) * s, aei

P [pc] = pop
P `s hpc, f, v * s, aei ! hpc + 1, f, s, aei

P [pc] = push0
P `s hpc, f, s, aei ! hpc + 1, f, 0 * s, aei

P [pc] = load x
P `s hpc, f, s, aei ! hpc + 1, f, f[x] * s, aei

P [pc] = store x
P `s hpc, f, v * s, aei ! hpc + 1, f[x 7! v], s, aei

P [pc] = if L
P `s hpc, f, 0 * s, aei ! hpc + 1, f, s, aei

P [pc] = if L

n 6= 0
P `s hpc, f, n * s, aei ! hL, f, s, aei

P [pc] = jsr L
P `s hpc, f, s, aei ! hL, f, (pc + 1) * s, (pc + 1) * aei

P [pc] = ret x
P `s hpc, f, s, pc0 * aei ! hpc0, f, s, aei

Figure 6: Structured dynamic semantics
onto the subroutine call stack. These definitions enable us to transfer the properties
of the structured semantics of this section to the stackless semantics of the next
section.

5.2 Static semantics
The structured static semantics relies on a new typing judgment:

F , S `s P

13

This judgment is defined by the rule:

F 1 = E

S1 = ffl
R1 = {}8
i 2 Dom(P ). R, i ` P labeled8

i 2 Dom(P ). F , S, i `s P

F , S `s P

The new, auxiliary judgment

R, i ` P labeled
is used to define what it means to be "inside" a subroutine. Unlike in most languages, where procedures are demarcated syntactically, in JVML0 and JVML the
instructions making up a subroutine are identified by constraints in the static semantics. For the instruction at address i, Ri is a subroutine label that identifies the
subroutine to which the instruction belongs. These labels take the form of either
the empty set or a singleton set consisting of an address. If an instruction's label
is the empty set, then the instruction belongs to the top level of the program. If
an instruction's label is the singleton set {L}, then the instruction belongs to the
subroutine that starts at address L. Figure 7 contains rules for labeling subroutines.
These rules do not permit subroutines to have multiple entry points, but they do
permit multiple exits.

For some programs, more than one R may satisfy both R1 = {} and the constraints of Figure 7 because the labeling of unreachable code is not unique. It is
convenient to assume a canonical R for each program P , when one exists. (The
particular choice of R does not matter.) We write RP for this canonical R, and RP,i
for the value of RP at address i.

Much as in Section 5, the judgment

F , S, i `s P
imposes local constraints near program point i. For the instructions considered in
Section 5 (that is, for all instructions but jsr and ret), the rules are the same as in
Figure 5. Figure 8 contains rules for jsr and ret.

The elements of F need not be defined on all variables. For an address i inside
a subroutine, the domain of F i includes only the variables that can be read and
written inside that subroutine. The subroutine is polymorphic over variables outside
Dom(F i).

Figure 9 gives an example of polymorphism. (Simpler examples could be found
if the typing rules allowed a more liberal use of Top.) The code, in the rightmost

14

P [i] 2 {inc, pop, push0, load x, store x}

Ri+1 = Ri
R, i ` P labeled

P [i] = if L
Ri+1 = RL = Ri

R, i ` P labeled

P [i] = jsr L

Ri+1 = Ri

RL = {L}
R, i ` P labeled

P [i] 2 {halt, ret x}

R, i ` P labeled

Figure 7: Rules labeling instructions with subroutines

P [i] = jsr LDom(

F i+1) = Dom(F i)Dom(

F L) ` Dom(F i)8
y 2 Dom(F i)\Dom(F L). F i+1[y] = F i[y]8

y 2 Dom(F L). F L[y] = F i[y]

SL = (ret-from L) * Si

i + 1 2 Dom(P )

L 2 Dom(P )

F , S, i `s P

P [i] = ret x

RP,i = {L}

8j. P [j] = jsr L ) ` ^ 8y 2 Dom(F i). F j+1[y] = F i[y]^ S

j+1 = Si '

F , S, i `s P

Figure 8: Structured static semantics for jsr and ret

15

F i[0] F i[1] Si i P [i]
Top Top ffl 01 push0
Top Top Int * ffl 02 store 1
Top Int ffl 03 jsr 6
(ret-from 6) Int ffl 04 jsr 11
(ret-from 11) (ret-from 15) Int * ffl 05 halt
Top Int (ret-from 6) * ffl 06 store 0
(ret-from 6) Int ffl 07 load 1
(ret-from 6) Int Int * ffl 08 jsr 15
(ret-from 6) (ret-from 15) Int * ffl 09 store 1
(ret-from 6) Int ffl 10 ret 0
(ret-from 6) Int (ret-from 11) * ffl 11 store 0
(ret-from 11) Int ffl 12 load 1
(ret-from 11) Int Int * ffl 13 jsr 15
(ret-from 11) (ret-from 15) Int * ffl 14 ret 0
undefined Int (ret-from 15) * Int * ffl 15 store 1undefined

(ret-from 15) Int * ffl 16 incundefined
(ret-from 15) Int * ffl 17 ret 1

Figure 9: Example of typing
column of Figure 9, computes the number 2 and leaves it at the top of the stack.
The code contains a main body and three subroutines, which we have separated with
horizontal lines. The subroutine that starts at line 6 increments the value in variable
0. The subroutine that starts at line 11 also increments the value in variable 0 but
leaves its result on the stack. These subroutines are implemented in terms of the
one that starts at line 15, which increments the value at the top of the stack. This
last subroutine is polymorphic over variable 0.

5.3 One-step soundness theorem
To state the one-step soundness theorem for the structured semantics, two definitions
are needed.

In the previous section, where programs do not have subroutines, the type of
local variable x at program point pc is denoted by F pc[x]. However, for program
points inside subroutines, this definition is not quite adequate. For x 2 Dom(F pc),
the type of x is still F pc[x]. For other x, the type of x depends on where execution
has come from. A subroutine is polymorphic over local variables that are not in
Dom(F pc): different call sites of the subroutine may have values of different types
in those local variables. Therefore, we construct an assignment of types to local

16

variables that takes into account where execution has come from. The expression:

F(F , pc, ae)[x]
denotes the type assigned to local variable x when execution reaches point pc with
a subroutine call stack ae. The type F(F , pc, ae)[x] is defined by the rules:

x 2 Dom(F pc)F
(F , pc, ae)[x] = F pc[x] (tt0)

x 62 Dom(F pc)F

(F , p, ae)[x] = TF
(F , pc, p * ae)[x] = T

(tt1)

Note that F (F , pc, ae)[x] is not defined when x is not in Dom(F pc) or any of the
Dom(F p) for the p's in ae. However, the theorems and lemmas in the following
subsections consider only situations in which F is defined.

The second auxiliary definition for our theorem concerns an invariant on the
structure of the subroutine call stack. The theorem of the previous section says
that, when a program takes a step from a well-typed state, it reaches a well-typed
state. With subroutines, we need to assume that the subroutine call stack of the
starting state is well-formed. For example, if the address p is in the call stack,
then it must be the case that the instruction at p - 1 is a jsr instruction. The
new theorem says that if the program takes a step from a well-typed state with a
well-formed subroutine call stack, then it reaches a well-typed state with a wellformed subroutine call stack. The following judgment expresses what we mean by a
well-formed subroutine call stack:

WFCallStack(P, F , pc, ae)
This judgment is defined by the rules:

Dom(F pc) = Var
WFCallStack(P, F , pc, ffl) (wf0)

P [p - 1] = jsr L

L 2 RP,pc
Dom(F pc) ` Dom(F p)
WFCallStack(P, F , p, ae)
WFCallStack(P, F , pc, p * ae)

(wf1)

Informally, a subroutine call stack ae is well-formed when:

17

* given a return address p in ae, the instruction just before p is a jsr instruction

that calls the routine from which p returns;

* the current program counter pc and all return addresses in ae have the correct

subroutine label associated with them;

* the return address at the bottom of the call stack is for code that can access

all local variables;

* any variable that can be read and written by a callee can also be read and

written by its caller.

With these two definitions, we can state a one-step soundness theorem for the
structured semantics. The proof of this theorem is in Appendix A.

Theorem 2 For all P , F , and S such that F , S `s P :

8pc, f, s, ae, pc0, f 0, s0, ae0.^

P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i^
WFCallStack(P, F , pc, ae)^ 8

y. 9T. F(F , pc, ae)[y] = T ^ f [y] : T^
s : Spc)

^ WFCallStack(P, F , pc0, ae0)^ 8

y. 9T 0. F (F , pc0, ae0)[y] = T 0 ^ f 0[y] : T 0^
s0 : Spc0

6 Stackless semantics
The remaining problem is to eliminate the explicit subroutine call stack of the previous section, using instead the operand stack and local variables to communicate
return addresses from a jsr to a ret. As discussed in Section 2, when the semantics
of jsr and ret are defined in terms of the operand stack and local variables, uncontrolled use of ret combined with the polymorphism of subroutines is inimical to type
safety. This section presents a static semantics that rules out problematic uses of
ret. This static semantics restricts programs to operate as if an explicit subroutine
call stack like the one from the previous section were present. In fact, the soundness
argument for the stackless semantics relies on a simulation between the structured
semantics and the stackless semantics.

The static and dynamic semantics described in this section are closest to typical
implementations of JVML. Thus, we consider these the official semantics of JVML0.

18

P [pc] = jsr L
P ` hpc, f, si ! hL, f, (pc + 1) * si

P [pc] = ret x
P ` hpc, f, si ! hf [x], f, si

Figure 10: Stackless dynamic semantics for jsr and ret
6.1 Dynamic semantics
The stackless dynamic semantics consists of the rules of Figure 4 plus the rules
for jsr and ret of Figure 10. The jsr instruction pushes the return address onto
the operand stack. To use this return address, a program must first pop it into
a local variable and then reference that local variable in a ret instruction. The
ret x instruction extracts an address from local variable x; if x does not contain an
address, then the program gets stuck (because hf [x], f, si is not a state).

6.2 Static semantics
To define the stackless static semantics, we revise the rule for F , S ` P of Section 4.
The new rule is: F

1 = E
S1 = ffl

C1 = ffl8
i 2 Dom(P ). C, i ` P strongly labeled8

i 2 Dom(P ). F , S, i ` P

F , S ` P

The new, auxiliary judgment

C, i ` P strongly labeled
constrains C to be an approximation of the subroutine call stack. Each element of C
is a string of subroutine labels. For each address i, the string Ci is a linearization of
the subroutine call graph to i. Of course, such a linearization of the subroutine call
graph exists only when the call graph is acyclic, that is, when subroutines do not
recurse. (We believe that we can prove our theorems while allowing recursion, but
disallowing recursion simplifies our proofs and agrees with Sun's specification [LY96,
p. 124].) Figure 11 contains the rules for this new judgment; note that a subsequence
is not necessarily a consecutive substring. Figure 12 shows an application of the rules

19

P [i] 2 {inc, pop, push0, load x, store x}

Ci+1 = Ci
C, i ` P strongly labeled

P [i] = if L
Ci+1 = CL = Ci
C, i ` P strongly labeled

P [i] = jsr L

L 62 Ci
Ci+1 = Ci

CL = L * c
Ci is a subsequence of c
C, i ` P strongly labeled

P [i] 2 {halt, ret x}
C, i ` P strongly labeled

Figure 11: Rules approximating the subroutine call stack at each instruction

Ci i P [i]

ffl 01 push0
ffl 02 store 1
ffl 03 jsr 6
ffl 04 jsr 11
ffl 05 halt
6 * ffl 06 store 06 *

ffl 07 load 16 *
ffl 08 jsr 156 *
ffl 09 store 16 *
ffl 10 ret 0
11 * ffl 11 store 011 *

ffl 12 load 111 *
ffl 13 jsr 1511 *
ffl 14 ret 0
15 * 11 * 6 * ffl 15 store 115 * 11 * 6 *

ffl 16 inc15 * 11 * 6 *
ffl 17 ret 1

Figure 12: Example of C labeling

20

P [i] = jsr LDom(

F i+1) = Dom(F i)Dom(

F L) ` Dom(F i)8
y 2 Dom(F i)\Dom(F L). F i+1[y] = F i[y]8

y 2 Dom(F L). F L[y] = F i[y]

SL = (ret-from L) * Si

(ret-from L) 62 Si8
y 2 Dom(F L). F L[y] 6= (ret-from L)

i + 1 2 Dom(P )

L 2 Dom(P )

F , S, i ` P

P [i] = ret x

RP,i = {L}
x 2 Dom(F i)
F i[x] = (ret-from L)

8j. P [j] = jsr L ) ` ^ 8y 2 Dom(F i). F j+1[y] = F i[y]^ S

j+1 = Si '

F , S, i ` P

Figure 13: Stackless static semantics for jsr and ret
of Figure 11 to the code from Figure 9. In this example, the order of 6 and 11 could
be reversed in C15, C16, and C17.

As with R, more than one C may satisfy both C1 = ffl and the constraints in
Figure 11. We assume a canonical C for each program P , when one exists. We write
CP for this canonical C, and CP,i for the value of CP at address i. Programs that
satisfy the constraints in Figure 11 also satisfy the constraints in Figure 7; we define
RP from CP as follows:

RP,i = ( {} when CP,i = ffl{L} when C

P,i = L * c for some c

On the other hand, programs with recursive subroutines may satisfy the constraints
in Figure 7 but never those in Figure 11.

Figure 13 contains the rules that define F , S, i ` P for jsr and ret; rules for
other instructions are in Figure 5. The rule for jsr L assigns the type (ret-from L)
to the return address pushed onto the operand stack. This type will propagate to any
location into which this return address is stored, and it is checked by the following

21

hypotheses in the rule for ret:

x 2 Dom(F i)
F i[x] = (ret-from L)

Typing return addresses helps ensure that the return address used by a subroutine L
is a return address for L, not for some other subroutine. By itself, ensuring that the
return address used by a subroutine L is a return address for L does not guarantee
last-in, first-out behavior. One also has to ensure that the only return address for L
available inside L is the most recent return address, not one tucked away during a
previous invocation. This is achieved by the following hypotheses in the rule for jsr:

(ret-from L) 62 Si8
y 2 Dom(F L). F L[y] 6= (ret-from L)

These hypotheses guarantee that the only value of type (ret-from L) available
inside L is the most recent value of this type pushed by the jsr instruction. (These
hypotheses might be redundant for reachable code; we include them because our
rules apply also to unreachable code.) Except for the lines discussed above, the
rules for jsr and ret are the same as those of the structured static semantics. As
an example, we may simply reuse the one of Figure 9, since the typing given there
remains valid in the stackless static semantics.

6.3 One-step soundness theorem
The one-step soundness theorem for the stackless semantics requires some auxiliary
definitions. We develop those definitions top-down, after stating the theorem. The
proof of the theorem is in Appendix B.

Theorem 3 For all P , F , and S such that F , S ` P :

8pc, f, s, ae, pc0, f 0, s0.^

P ` hpc, f, si ! hpc0, f 0, s0i^
Consistent(P, F , S, pc, f, s, ae)^ 8

y. 9T. F(F , pc, ae)[y] = T ^ f [y] : T^
s : Spc) 9
ae0.^
Consistent(P, F , S, pc0, f 0, s0, ae0)^ 8

y. 9T 0. F (F , pc0, ae0)[y] = T 0 ^ f 0[y] : T 0^
s0 : Spc0

In this theorem, the judgment Consistent performs several functions:

22

* Like WFCallStack in the one-step soundness theorem for the structured semantics, Consistent implies an invariant on the structure of the subroutine call
stack.

* In the stackless semantics, the subroutine call stack is not explicit in the dynamic state. The judgment Consistent relates an implicit subroutine call stack
(ae or ae0) to the state of an execution.

* The implicit subroutine call stacks of the stackless semantics are closely related

to the explicit subroutine call stacks of the structured semantics. In particular,
the following implication holds:

8pc, f, s, ae, pc0, f 0, s0.^

P ` hpc, f, si ! hpc0, f 0, s0i^
Consistent(P, F , S, pc, f, s, ae)) 9
ae0. P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i

Thus, Consistent helps us relate the structured and the stackless semantics.
The judgment Consistent is the conjunction of four other judgments:

Consistent(P, F , S, pc, f, s, ae) j^

WFCallStack(P, F , pc, ae)^
WFCallStack2(P, pc, \Lambda P (ae), ae)^
GoodStackRets(Spc, s, \Lambda P (ae), ae)^
GoodFrameRets(F , pc, {}, f, \Lambda P (ae), ae)

The first judgment is the judgment WFCallStack from the previous section. The
next judgment adds more constraints on the subroutine call stack. The last two
judgments constrain the values of return addresses in s (the operand stack) and f
(local variables) to match those found in ae (the implicit subroutine call stack).

The auxiliary function \Lambda  returns the sequence of caller labels associated with a
sequence of return addresses. It is a partial function defined by two rules:

(Empty hypothesis)

\Lambda P (ffl) = ffl (l0)

P [p - 1] = jsr L

\Lambda P (ae0) = *0
\Lambda P (p * ae0) = L * *0

(l1)

According to this definition, \Lambda P (ae) is the string of labels of subroutines that have
been called in ae. The function \Lambda P (ae) is used only when P is well-typed and
WFCallStack holds, in which case \Lambda P (ae) is always defined.

23

The extensions of WFCallStack made by WFCallStack2 are defined by two rules:

CP,pc = ffl
WFCallStack2(P, pc, ffl, ffl) (wf20)

L 62 *0
L * *0 is a subsequence of CP,pc

WFCallStack2(P, p, *0, ae0)
WFCallStack2(P, pc, L * *0, p * ae0)

(wf21)

These rules ensure that no recursive calls have occurred. They also ensure that
the dynamic subroutine call stack is consistent with the static approximation of the
subroutine call stack.

The judgment GoodStackRets constrains the values of return addresses in the
operand stack:

8j, L. j 2 Dom(ff) ^ ff[j] = (ret-from L) ) GoodRet(L, s[j], *, ae)

GoodStackRets(ff, s, *, ae) (gsr0)

(Empty hypothesis)

GoodRet(K, p, ffl, ffl) (gr0)

(Empty hypothesis)
GoodRet(K, p, K * *, p * ae) (gr1)

K0 6= K
GoodRet(K, p, *0, ae0)
GoodRet(K, p, K0 * *0, p0 * ae0)

(gr2)

In these definitions, we introduce GoodRet as an auxiliary judgment. These definitions say that, when a subroutine L has been called but has not returned, the
values of return addresses for L found in s (the operand stack) agree with the return
address for L in ae (the implicit subroutine call stack).

Similarly to the judgment GoodStackRets, the judgment GoodFrameRets constrains the values of return addresses in local variables:

(Empty hypothesis)
GoodFrameRets(F , pc, u, f, ffl, ffl) (gfr0)

8y, L.  ^ y 2 (Dom(F pc)\u)^ F pc[y] = (ret-from L) ! ) GoodRet(L, f [y], K * *0, p * ae0)

GoodFrameRets(F , p, Dom(F pc), f, *0, ae0)

GoodFrameRets(F , pc, u, f, K * *0, p * ae0)

(gfr1)

24

Because subroutines may not be able to read and write all variables, GoodFrameRets
cannot constrain all variables simultaneously in the way that GoodStackRets constrains all elements of the operand stack. Instead, the rules for GoodFrameRets
must work inductively on the subroutine call stack, looking at the return addresses
available to each subroutine in turn.

7 Main soundness theorem
Our main soundness theorem is:
Theorem 4 For all P , F , and S such that F , S ` P :

8pc, f0, f, s. ^

P ` h1, f0, ffli !* hpc, f, si^ 6 9

pc0, f 0, s0. P ` hpc, f, si ! hpc0, f 0, s0i !)

P [pc] = halt ^ s : Spc

This theorem says that if a computation stops, then it stops because it has reached
a halt instruction, not because the program counter has gone out of bounds or
because a precondition of an instruction does not hold. This theorem also says that
the operand stack is well-typed when a computation stops. This last condition is
important because, when a JVML method returns, its return value is on the operand
stack.

The proof of the main soundness theorem is in Appendix C.

8 Sun's rules
Sun has published two descriptions of the bytecode verifier, a prose specification
and a reference implementation. This section compares our rules with both of these
descriptions.

8.1 Scope
While our rules simply check static information, Sun's bytecode verifier infers that
information. Inference may be important in practice, but only checking is crucial
for type safety (and for security). It is therefore reasonable to study checking apart
from inference.

JVML has around 200 instructions, while JVML0 has only 9. A rigorous treatment of most of the remaining JVML instructions should pose only minor problems.

25

In particular, many of these instructions are for well understood, arithmetic operations; small difficulties may arise because of their exceptions and other idiosyncrasies.
The other instructions (around 20) concern objects and concurrency. Their rigorous treatment would require significant additions to our semantics--for example, a
model of the heap. Fortunately, some of these additions are well understood in the
context of higher-level, typed languages. As discussed below, Freund and Mitchell
are currently extending our rules to a large subset of JVML.

8.2 Technical differences
Our rules differ from Sun's reference implementation in the handling of recursive
subroutines. Sun's specification disallows recursive subroutines, as do our rules, but
Sun's reference implementation allows recursion in certain cases. We believe that
recursion is sound in the sense that it does not introduce security holes. However,
recursion is an unnecessary complication since it is not useful for compiling Java.
Therefore, we believe that the specification should continue to disallow recursion and
that the reference implementation should be corrected.

Our rules deviate from Sun's specification and reference implementation in a few
respects.

* Sun's rules forbid load x when x is uninitialized or holds a return address.

Our rules are more general without compromising soundness.

* Sun's rules allow at most one ret instruction per subroutine, while our rules

allow an arbitrary number.

* According to Sun's rules, "each instruction keeps track of the list of jsr targets

needed to reach that instruction" [LY96, p. 135]. Using this information, Sun's
rules allow a ret to return not only from the most recent call but also from calls
further up the subroutine call stack. Adding this flexibility to our rules should
not be difficult, but it would complicate the structured dynamic semantics and
would require additional information in the static semantics.

Finally, our rules differ from Sun's reference implementation on a couple of other
points. Sun's specification is ambiguous on these points and, therefore, does not
provide guidance.

* Sun's reference implementation does not constrain unreachable code. Our rules

put constraints on all code. Changing our rules to ignore unreachable code
would not require fundamental changes.

26

* When it comes to identifying what subroutine an instruction belongs to, our

rules are more restrictive than the rules implicit in Sun's reference implementation. The flexibility of Sun's reference implementation is important for compiling finally clauses that can throw exceptions. Changing our rules to capture
Sun's approach would not be difficult, but changing our soundness proof to
support this approach may be.

9 Other related work
In addition to Sun's, there exist several implementations of the bytecode verifier.
Only recently has there been any systematic attempt to understand all these implementations. In particular, the Kimera project has tested several implementations,
pointing out some mistakes and discrepancies [SMB97]. We take a complementary
approach, based on rigorous reasoning rather than on testing. Both rigorous reasoning and testing may affect our confidence in bytecode verification. While testing
does not provide an adequate replacement for precise specifications and proofs, it is
a cost-effective way to find certain flaws and oddities.

More broadly, there have been several other implementations of the Java VM. Of
particular interest is a partial implementation developed at Computational Logic,
Inc. [Coh97]. This implementation is defensive, in the sense that it includes strong
(and expensive) dynamic checks that remove the need for bytecode verification. The
implementation is written in a formal language, and is intended as a model rather
than for production use. Ultimately, one may hope to prove that the defensive
implementation is equivalent to an aggressive implementation plus a sound bytecode
verifier (perhaps one based on our rules).

There have also been typed intermediate languages other than JVML. Several
have been developed for ML and Haskell [TIC97]. Here we discuss the TIL intermediate languages [Mor95, MTC+96] as representative examples. The TIL intermediate languages provide static guarantees similar to those of JVML. Although
these languages have sophisticated type systems, they do not include an analogue
to JVML subroutines; instead, they include constructs as high-level as Java's tryfinally statement. Therefore, the main problems addressed in this paper do not
arise in the context of TIL.

Finally, the literature contains many proofs of type soundness for higher-level
languages, and in particular proofs for a fragment of Java [DE97, Nv98, Sym97].
Those proofs have not had to deal with JVML peculiarities (in particular, with
subroutines); nevertheless, their techniques may be helpful in extending our work to
the full JVML.

In summary, there has not been much work closely related to ours. We do not

27

find this surprising, given that the handling of subroutines is one of the most original
parts of the bytecode verifier; it was not derived from prior papers or systems [Yel97].
However, interest in the formal treatment of bytecode verification is mounting; several approaches are currently being pursued [FM98, Gol97, HT98, Qia98, Sar97].
Goldberg, Qian, and Saraswat all develop other formal frameworks for bytecode verification, basing them on constraints and dataflow analysis; their work is rather broad
and not focused on subroutines. Hagiya and Tozawa generalize our rules for subroutines. Building on our type system, Freund and Mitchell study object initialization
and its problematic interaction with subroutines; work is under way on a subset of
JVML that includes objects, classes, constructors, interfaces, and exceptions.

10 Conclusions
The bytecode verifier is an important part of the Java VM; through static checks, it
helps reconcile safety with efficiency. Common descriptions of the bytecode verifier
are ambiguous and contradictory. This paper suggests the use of a type system as
an alternative to those descriptions. It explores the viability of this suggestion by
developing a sound type system for a subset of JVML. This subset, despite its small
size, is interesting because it includes JVML subroutines, a source of substantial
difficulty in the design of a type system.

Our results so far support the hypothesis that a type system is a good way to
describe the bytecode verifier. Significant problems remain in scaling up to the full
JVML, such as handling objects and concurrency. However, we believe that these
problems will be no harder than those posed by subroutines, and that a complete
type system for JVML could be both tractable and useful.

Appendix
A Proof of soundness for the structured semantics
First, we prove that a step of execution preserves WFCallStack. Next, we prove some
lemmas about types. Finally, we prove the soundness theorem for the structured
semantics (Theorem 2), first by showing that well-typing of the stack is preserved
and then by showing that well-typing of local variables is preserved.

A.1 Preservation of WFCallStack
The following lemma states certain insensitivities of WFCallStack to the exact value
of pc:

28

Lemma 1 For all P , F , and S such that F , S `s P :

8pc, pc0, ae.^

WFCallStack(P, F , pc, ae)^
RP,pc0 = RP,pc^
Dom(F pc0) = Dom(F pc))
WFCallStack(P, F , pc0, ae)

Proof Assume that P , F , S, pc, pc0, and ae satisfy the hypotheses of the lemma.
We do a case split on ae:

1. Case: ae = ffl. In this case, the assumption WFCallStack(P, F , pc, ae) must

be true by rule (wf0), so Dom(F pc) = Var. Given this and the assumption
that Dom(F pc0) = Dom(F pc), it follows that Dom(F pc0) = Var. Thus, by
rule (wf0), we can conclude that WFCallStack(P, F , pc0, ae).

2. Case: ae 6= ffl. In this case, we know that ae = p * ae0 for some p and ae0. Also, the

assumption WFCallStack(P, F , pc, ae) must be true by rule (wf1), so, for some
L, we know that

P [p - 1] = jsr L

L 2 RP,pc
Dom(F pc) ` Dom(F p)
WFCallStack(P, F , p, ae0)

Substituting RP,pc0 for RP,pc and Dom(F pc0) for Dom(F pc), we have all we
need to establish WFCallStack(P, F , pc0, ae) by rule (wf1).

2

Now we show that the structured dynamic semantics preserves WFCallStack:
Restatement of part of Theorem 2 For all P , F , and S such that F , S `s P :

8pc, f, s, ae, pc0, f 0, s0, ae0.^

P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i^
WFCallStack(P, F , pc, ae))
WFCallStack(P, F , pc0, ae0)

Proof Assume that P , F , S, pc, f , s, ae, pc0, f 0, s0, and ae0 satisfy the hypotheses
of the lemma. The assumption that a step of execution is possible starting fromh

pc, f, s, aei implies that P [pc] is defined; it also implies that P [pc] 6= halt. We do
a case split on the instruction possible at P [pc], proceeding in three cases:

29

1. The first case is instructions that do not affect the subroutine call stack: inc,

if, load, store, pop, and push0. For these instructions, their structured
dynamic semantics implies that ae0 = ae and their structured static semantics
implies that RP,pc = RP,pc0 and Dom(F pc) = Dom(F pc0). Given these equalities, we can conclude WFCallStack(P, F , pc0, ae0) by Lemma 1.

2. The second case is P [pc] = jsr K for some K. In this case, the structured

dynamic semantics implies that ae0 = (pc + 1) * ae, so we want to show that:

WFCallStack(P, F , K, (pc + 1) * ae)
To prove this judgment using rule (wf1), it suffices to show:

(a) that P [(pc + 1) - 1] = jsr K, which we are assuming;
(b) that K 2 RP,K, which is true because RP,K = {K} by the structured

static semantics of jsr (specifically, line 3 of the jsr rule in Figure 7);

(c) that Dom(F K ) ` Dom(F (pc+1)), which also follows from the structured

static semantics of jsr K (specifically, lines 2 and 3 of the jsr rule in
Figure 8);

(d) that WFCallStack(P, F , pc + 1, ae). We are assuming that

WFCallStack(P, F , pc, ae)
From the structured static semantics of jsr K we know that RP,(pc+1) =
RP,pc (line 2, jsr rule, Figure 7) and Dom(F (pc+1)) = Dom(F pc) (line 2,
jsr rule, Figure 8). Under these conditions,

WFCallStack(P, F , pc + 1, ae)
follows from Lemma 1.

3. The third case is P [pc] = ret x for some x. The assumption that a step of

execution is possible starting from hpc, f, s, aei implies that ae is not empty. If
ae is not empty, then by the structured dynamic semantics of ret we know that
ae = pc0 * ae0. Given the assumption that

WFCallStack(P, F , pc, ae)
by substitution we can conclude that

WFCallStack(P, F , pc, pc0 * ae0)
This can be true only by rule (wf1), so

WFCallStack(P, F , pc0, ae0)
must be true.

2

30

A.2 Lemmas about types
A few lemmas about types are needed. The first lemma helps us reason about the
types of locations at program points that dynamically follow a ret instruction:

Lemma 2 For all P , F , and S such that F , S `s P :8

pc, f, s, p, ae0, x.^

WFCallStack(P, F , pc, p * ae0)^
P [pc] = ret x)

^ 8y 2 Dom(F pc). F p[y] = F pc[y]^

Sp = Spc

Proof Assume that P , F , S, pc, f , s, p, ae0, and x satisfy the hypotheses of the
lemma. The assumption WFCallStack(P, F , pc, p * ae0) must hold by rule (wf1), so
there exists an L such that L 2 RP,pc and P [p - 1] = jsr L. Given such an L, the
conjuncts of the conclusion are instantiations of the quantified term found in the
structured static semantics of ret (Figure 8). 2

The following lemma states an insensitivity of F to the exact value of pc:
Lemma 3 8

F , ae, y, T, pc, pc0.^

Dom(F pc0) = Dom(F pc)^
y 2 Dom(F pc) ) F pc0[y] = F pc[y]^ F

(F , pc, ae)[y] = T) F
(F , pc0, ae)[y] = T

Proof Assume that F , ae, y, T , pc, and pc0 satisfy the hypotheses of the lemma.

If y 2 Dom(F pc), then the assumption that F (F , pc, ae)[y] = T must be true
by rule (tt0), so we can conclude that T = F pc[y]. Given y 2 Dom(F pc), by
assumption F pc0[y] = F pc[y], thus F pc0[y] = T . Therefore, by rule (tt0)

F (F , pc0, ae)[y] = T
If y 62 Dom(F pc), then F(F , pc, ae)[y] = T must be true by rule (tt1), so we can
conclude, for some p and ae0, that ae = p * ae0 and that

F (F , p, ae0)[y] = T (1)
Given the assumptions that y 62 Dom(F pc) and Dom(F pc) = Dom(F pc0), it follows
that y 62 Dom(F pc0). Given this and (1),

F (F , pc0, ae)[y] = T
follows from rule (tt1). 2

31

The next two lemmas say that F does not change as a result of executing a jsr
or ret instruction. The lemma for jsr is:

Lemma 4 For all P , F , and S such that F , S `s P :

8pc, f, ae, L, y, T.^

P [pc] = jsr L^ F

(F , pc, ae)[y] = T) F
(F , L, (pc + 1) * ae)[y] = T

Proof Assume that P , F , S, pc, f , ae, L, y, and T satisfy the hypotheses of the
lemma. We do a case split on y:

* Case: y 2 Dom(F L), that is, y is one of the variables accessible in the subroutine being called. By the structured static semantics of jsr (line 3 of jsr
rule in Figure 8), Dom(F L) ` Dom(F pc), so y is an element of Dom(F pc) as
well. Given that y 2 Dom(F pc), the assumption that

F(F , pc, ae)[y] = T
could be true only by rule (tt0), so we know that T = F pc[y]. From y 2
Dom(F L) and the structured static semantics of jsr (Figure 8, line 5) it follows
that F L[y] = F pc[y], so T = F L[y]. From y 2 Dom(F L) and T = F L[y], it
follows from rule (tt0) that

F(F , L, (pc + 1) * ae)[y] = T

* Case: y 62 Dom(F L). We know that

Dom(F pc) = Dom(F pc+1)
y 2 Dom(F pc) ) F pc[y] = F pc+1[y]F

(F , pc, ae)[y] = T

The first two lines follow from the structured static semantics of jsr (Figure 8,
lines 2 and 4); the last we are assuming. From Lemma 3 it follows that

F(F , pc + 1, ae)[y] = T
Since y 62 Dom(F L), it follows from rule (tt1) that

F(F , L, (pc + 1) * ae)[y] = T

2
32

The next lemma says for ret what the previous lemma says for jsr. Here,
however, we need to assume that we have a well-formed subroutine call stack.

Lemma 5 For all P , F , and S such that F , S `s P :

8pc, f, p, ae0, x, y, T.^

WFCallStack(P, F , pc, p * ae0)^
P [pc] = ret x^ F

(F , pc, p * ae0)[y] = T) F
(F , p, ae0)[y] = T

Proof Assume that P , F , S, pc, f , p, ae0, x, y, and T satisfy the hypotheses of the
lemma. We do a case split on y:

* Case: y 2 Dom(F pc). Given the WFCallStack assumption plus the assumptions that P [pc] = ret x and y 2 Dom(F pc), we can conclude that
F p[y] = F pc[y] by Lemma 2. Given that y 2 Dom(F pc), the assumption that

F(F , pc, p * ae0)[y] = T
must hold by rule (tt0), so T = F pc[y] and thus T = F p[y]. The WFCallStack
assumption could be true only by rule (wf1), so Dom(F pc) ` Dom(F p) and
y 2 Dom(F p). Given T = F p[y] and y 2 Dom(F p):

F (F , p, ae0)[y] = T
follows from rule (tt0).

* Case: y 62 Dom(F pc). Given y 62 Dom(F pc), the assumption that

F(F , pc, p * ae0)[y] = T
could be true only by rule (tt1), so we can conclude

F (F , p, ae0)[y] = T

2
The final lemma says that F is defined when WFCallStack holds:
Lemma 6 For all P , F , and S such that F , S ` P :

8y, ae, pc.

WFCallStack(P, F , pc, ae)) 9

T. F(F , pc, ae)[y] = T

33

Proof This lemma holds because WFCallStack ensures that Dom(F p) = Var for
some p in ae, so we are guaranteed that rule (tt0) will apply for at least this p.

More formally, assume an arbitrary y in Var and P , F , S such that F , S ` P .
We proceed by induction on ae:

* In the base case, ae = ffl. Assume pc such that WFCallStack(P, F , pc, ae). Because ae = ffl, this WFCallStack assumption could be true only by rule (wf0),
so Dom(F pc) = Var and thus y 2 Dom(F pc). Therefore, by rule (tt0),F

(F , pc, ae)[y] = F pc[y].

* In the inductive step, ae = p * ae0 for some p and ae0. Assume pc such that

WFCallStack(P, F , pc, ae).

If y 2 Dom(F pc), then we can conclude F(F , pc, ae)[y] = F pc[y] by rule (tt0).
If y 62 Dom(F pc), then we need to use the induction hypothesis:

8q. WFCallStack(P, F , q, ae0) ) 9T. F(F , q, ae0)[y] = T
Because ae is not empty, the assumption WFCallStack(P, F , pc, ae) could be true
only by rule (wf1), so WFCallStack(P, F , p, ae0). This matches the antecedent of
the induction hypothesis, so we can conclude that F (F , p, ae0)[y] = T for some
T . Thus, by rule (tt1), we can conclude that F (F , pc, ae)[y] = T .

2

A.3 Preservation of stack typing
We now turn our attention back to the soundness theorem. This subsection shows
that the typing of the operand stack is preserved; the next shows that the typing of
variables is preserved.

Restatement of part of Theorem 2 For all P , F , and S such that F , S `s P :

8pc, f, s, ae, pc0, f 0, s0, ae0.^

P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i^
WFCallStack(P, F , pc, ae)^ 8

y. 9T. F(F , pc, ae)[y] = T ^ f [y] : T^
s : Spc)
s0 : Spc0

Proof Assume that P , F , S, pc, f , s, ae, pc0, f 0, s0, and ae0 satisfy the hypotheses
of the lemma. The assumption that a step of execution is possible starting fromh

pc, f, s, aei implies that P [pc] is defined; it also implies that P [pc] 6= halt. We do
a case split on the instruction possible at P [pc]:

34

1. The first case is ret. We know from the structured dynamic semantics of ret

that s0 = s and ae = pc0 * ae0. Given the latter equality and the WFCallStack
assumption, Spc0 = Spc follows from Lemma 2. Given Spc0 = Spc, s0 = s, and
the assumption that s : Spc, it follows that s0 : Spc0.

2. The second case is inc, pop, push0, if, jsr, and store.

Consider inc, for example. We know from the structured dynamic semantics
of inc that s = n * s00 for some integer n and value string s00, and that s0 =
(n + 1) * s00. We know from the structured static semantics of inc that

Spc = Spc0 = Int * ff
for some type string ff. Given s : Spc, s = n * s00, and Spc = Int * ff, it follows
that s00 : ff. Given s00 : ff, s0 = (n + 1) * s00, and Spc0 = Int * ff, it follows that
s0 : Spc0.

As another example, consider store x. We know from the structured dynamic
semantics of store that s = v * s0 for some v, and we know from the structured
static semantics that Spc = T * Spc0 for some T . Given these equations and
the assumption that s : Spc, it follows that s0 : Spc0.

The proofs for the other instructions in this category are along similar lines.

3. The last case is load x. In this case, we use the assumptions to infer that f [x] :

T for some T such that F (F , pc, ae)[x] = T . The structured static semantics for
load says that x 2 Dom(F pc), so F(F , pc, ae)[x] = T must be true by rule (tt0),
so that T = F pc[x]. We know from the structured static semantics of load x
that Spc0 = F pc[x] * Spc, so Spc0 = T * Spc. We know from the structured
dynamic semantics of load x that s0 = f [x] * s. Given f [x] : T , s : Spc,
s0 = f [x] * s, and Spc0 = T * Spc, it follows that s0 : Spc0.

2

A.4 Preservation of local variable typing
Restatement of part of Theorem 2 For all P , F , and S such that F , S `s P :

8pc, f, s, ae, pc0, f 0, s0, ae0, y.^

P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i^
WFCallStack(P, F , pc, ae)^
s : Spc^ 9

T. F (F , pc, ae)[y] = T ^ f [y] : T) 9
T 0. F(F , pc0, ae0)[y] = T 0 ^ f 0[y] : T 0

35

This statement is stronger than that in Section 5.3 in that the quantifier for y has
been moved to the top level.

Proof Assume that P , F , S, pc, f , s, ae, pc0, f 0, s0, ae0, and y satisfy the hypotheses
of the lemma. Let T be a type such that F(F , pc, ae)[y] = T and f [y] : T ; by assumption, such a T exists. The assumption that a step of execution is possible starting
from hpc, f, s, aei implies that P [pc] is defined; it also implies that P [pc] 6= halt. We
do a case split on the instruction possible at P [pc]:

1. The first case is instructions that do not modify y or ae: inc, if, load,

pop, push0, and also store x for x 6= y. For these instructions, it follows
from their structured static semantics that Dom(F pc0) = Dom(F pc) and that
F pc0[y] = F pc[y] when y 2 Dom(F pc). It follows from their structured dynamic semantics that ae0 = ae. Thus, by Lemma 3, F(F , pc0, ae0)[y] = T . What
remains to be shown is f 0[y] : T . It follows from the structured dynamic semantics of these instructions that f [y] = f 0[y]; from f [y] : T and f [y] = f 0[y]
it follows that f 0[y] : T .

2. The next case is store y. By the structured static semantics of store y,

y 2 Dom(F pc0). Thus, by rule (tt0), F (F , pc0, ae0)[y] = F pc0[y]. What remains
to be shown is f 0[y] : F pc0[y]. The structured static semantics for store y
says that Spc = F pc0[y] * Spc0; the structured dynamic semantics for store y
says that s = f [y] * Spc0. Given these equations and the assumption s : Spc, it
follows that f 0[y] : F pc0[y].

3. The last case is jsr L and ret x. We can conclude that F (F , pc0, ae0)[y] = T

by Lemma 4 when P [i] is jsr L and by Lemma 5 when P [i] is ret x. What
remains to be shown is f 0[y] : T . From the structured dynamic semantics of
these instructions we know that f 0 = f . Given f [y] : T and f [y] = f 0[y], it
follows that f 0[y] : T .

2

B Proof of soundness for the stackless semantics
This section proceeds roughly top-down, first stating a proposition and two lemmas,
next using these to prove the soundness theorem for the stackless semantics, then
proving the two lemmas.

If F and S are a typing for P in the stackless semantics, then they are a typing
for P in the structured semantics:

Proposition 1 For all P , F , S, if F , S ` P , then F , S `s P .

36

The proofs below implicitly use this proposition when they apply lemmas and theorems that have F , S `s P in their hypotheses.

The first lemma establishes a simulation between the structured semantics and
the stackless semantics:

Lemma 7 For all P , F , and S such that F , S ` P :

8pc, f, s, ae, pc0, f 0, s0.^

P ` hpc, f, si ! hpc0, f 0, s0i^
Consistent(P, F , S, pc, f, s, ae)) 9
ae0. P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i

The second lemma states that Consistent is preserved by the structured dynamic
semantics:

Lemma 8 For all P , F , and S such that F , S ` P :

8pc, f, s, ae, pc0, f 0, s0, ae0.^

P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i^
Consistent(P, F , S, pc, f, s, ae))
Consistent(P, F , S0, pc0, f 0, s0, ae0)

Given these two lemmas and the soundness theorem for the structured semantics,
the soundness theorem of the stackless semantics follows directly.

Restatement of Theorem 3 For all P , F , and S such that F , S ` P :

8pc, f, s, ae, pc0, f 0, s0.^

P ` hpc, f, si ! hpc0, f 0, s0i^
Consistent(P, F , S, pc, f, s, ae)^ 8

y. 9T. F(F , pc, ae)[y] = T ^ f [y] : T^
s : Spc) 9
ae0.^
Consistent(P, F , S, pc0, f 0, s0, ae0)^ 8

y. 9T 0. F (F , pc0, ae0)[y] = T 0 ^ f 0[y] : T 0^
s0 : Spc0

Proof Assume that P , F , S, pc, f , s, ae, pc0, f 0, and s0 satisfy the hypotheses of
the theorem. Use Lemma 7 to pick ae0 such that

P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i
The first conjunct of the conclusion follows from our selection for ae0 and Lemma 8.
The last two conjuncts follow from our selection for ae0 and Theorem 2. 2

37

B.1 Analogous steps
Restatement of Lemma 7 For all P , F , and S such that F , S ` P :

8pc, f, s, ae, pc0, f 0, s0.^

P ` hpc, f, si ! hpc0, f 0, s0i^
Consistent(P, F , S, pc, f, s, ae)) 9
ae0. P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i

Proof Assume that P , F , S, pc, f , s, ae, pc0, f 0, and s0 satisfy the hypotheses
of the lemma. The assumption that a step of execution is possible starting fromh

pc, f, si implies that P [pc] is defined; it also implies that P [pc] 6= halt. We do a
case split on the instruction possible at P [pc]:

* For all instructions except jsr and ret, pick ae0 = ae. A comparison of the

structured and stackless dynamics in Figure 10 and Figure 6 shows that the
conclusion of our lemma follows directly from our assumptions.

* For jsr L, pick ae0 = (pc + 1) * ae. Again, a comparison of Figures 10 and 6

shows that the conclusion of our lemma follows from our assumptions in this
case as well.

* The case for the ret x instruction is the only complicated one.

First, we show (by contradiction) that our assumptions imply that ae has at
least one element. If ae had no elements, then the WFCallStack2 part of the
Consistent assumption would have to be true by rule (wf20). This would
require that CP,pc be empty, but the stackless static semantics for ret x (line 2,
ret rule, Figure 13) together with the relationship between RP and CP imply
that CP,pc must have at least one element. Thus, WFCallStack2 cannot hold
by rule (wf20). Instead, it must hold by rule (wf21), so ae must have at least
one element.

Therefore, ae = p * ae00 for some p and ae00. We let ae0 = ae00. To prove that this
selection of ae0 satisfies the conclusion, we need to show that:

P `s hpc, f, s, aei ! hpc0, f 0, s0, ae00i
In order to establish this judgment, we must show that pc0 = p. By the
stackless dynamic semantics of ret x, pc0 = f [x], so this reduces to showing
that f [x] = p.

Because ae has at least one element, the WFCallStack assumption must be true
by rule (wf1). From this we can infer that, for some L, P [p - 1] = jsr L and
L 2 RP,pc.

38

- Given P [p - 1] = jsr L and ae = p * ae0, we know by rule (l1) that

\Lambda P (ae) = L * *0 for some *0. (As mentioned in Section 6.3, \Lambda P (ae) is always
defined for well-typed programs when WFCallStack holds.)

- Given that L 2 RP,pc, we know by the stackless static semantics of ret x

that F pc[x] = (ret-from L) (line 4, ret rule, Figure 13). We also know
from the stackless static semantics of ret x that x 2 Dom(F pc) (line 3).

Because ae is not empty, the GoodFrameRets component of the Consistent assumption must be true by rule (gfr1). Given this, x 2 Dom(F pc), and

F pc[x] = (ret-from L)
it must be the case that

GoodRet(L, f [x], \Lambda P (ae), p * ae0)
We know that \Lambda P (ae) = L * *0, so by substitution:

GoodRet(L, f [x], L * *0, p * ae0)
This could be true only by rule (gr1), so we can conclude that f [x] = p.

2

B.2 Preservation of Consistent
Expanding the definition of Consistent, we want to prove the following lemma:
Restatement of Lemma 8 For all P , F , and S such that F , S ` P :

8pc, f, s, ae, pc0, f 0, s0, ae0.^

P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i^
WFCallStack(P, F , pc, ae)^
WFCallStack2(P, pc, \Lambda P (ae), ae)^
GoodStackRets(Spc, s, \Lambda P (ae), ae)^
GoodFrameRets(F , pc, {}, f, \Lambda P (ae), ae))

^ WFCallStack(P, F , pc0, ae0)^

WFCallStack2(P, pc0, \Lambda P (ae0), ae0)^
GoodStackRets(Spc0, s0, \Lambda P (ae0), ae0)^
GoodFrameRets(F , pc0, {}, f 0, \Lambda P (ae), ae0)

We prove this by assuming the hypotheses and proving separately each conjunct
of the conclusion. We prove the first conjunct (preservation of WFCallStack) in
Section A.1. We prove the remaining conjuncts below after we state and prove some
miscellaneous lemmas.

39

B.2.1 Miscellaneous lemmas
The following lemma describes how a program step can change \Lambda P (ae):
Lemma 9 For all P , F , and S such that F , S ` P :

8pc, f, s, ae, pc0, f 0, s0, ae0.^

P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i^
WFCallStack(P, F , pc, ae))

^ 8L. (P [pc] = jsr L ) \Lambda P (ae0) = L * \Lambda P (ae))^ 8

x. (P [pc] = ret x ) 9L. L * \Lambda P (ae0) = \Lambda P (ae))

Proof Assume that P , F , S, pc, f , s, ae, pc0, f 0, s0, and ae0 satisfy the hypotheses
of the lemma. The assumption that a step of execution is possible starting fromh

pc, f, s, aei implies that P [pc] is defined; it also implies that P [pc] 6= halt. We do
a case split on the instruction possible at P [pc]:

* jsr L: From the structured dynamic semantics of jsr L, ae0 = (pc + 1) * ae. By

assumption, P [pc] = jsr L. Thus, by rule (l1), \Lambda P (ae0) = L * \Lambda P (ae).

* ret x: From the structured dynamic semantics of ret x, ae = pc0 * ae0. Thus, the

WFCallStack assumption must be true by rule (wf1), so P [pc0 - 1] = jsr L for
some L. Thus, by rule (l1), \Lambda P (ae) = L * \Lambda P (ae0).

2

Lemma 10 For all P , pc, *, and ae:

WFCallStack2(P, pc, *, ae) ) ae and * have the same length
This can be proven by induction on the derivation of WFCallStack2(P, pc, *, ae).
Lemma 11 For all K, p, *, ae:

K 62 * ^ * and ae have the same length)

GoodRet(K, p, *, ae)

This lemma can be proven by induction on * using rule (gr0) in the base case and
rule (gr2) in the inductive step.

The following lemma states an insensitivity of GoodRet:

40

Lemma 12

8P, F , pc, p, ae0, L, *0, v, K.^

WFCallStack2(P, pc, L * *0, p * ae0)^
GoodRet(K, v, L * *0, p * ae0))
GoodRet(K, v, *0, ae0)

Proof Assume P , F , pc, p, ae0, L, *0, v, and K satisfy the hypotheses of our
lemma. The WFCallStack2 assumption implies that ae0 and *0 have the same length
(by Lemma 10). We proceed with a case split on ae0:

* When ae0 is empty, so is *0, thus GoodRet(K, v, *0, ae0) follows from rule (gr0).

* When ae0 is not empty, we proceed with a case split on K. When K 6= L, the

GoodRet assumption must be true by rule (gr2), so

GoodRet(K, v, *0, ae0)
The next case is K = L. Because ae0 is not empty, the WFCallStack2 assumption must be true by rule (wf21), so L 62 *0 and thus K 62 *0. Given this and
the fact that *0 and ae0 have the same length, we can conclude by Lemma 11
that

GoodRet(K, v, *0, ae0)

2
The next lemma is used to prove the following lemma:
Lemma 13

8ae, P, F , pc, u, f, f 0, x, v.^

WFCallStack(P, F , pc, ae)^
u ` Dom(F pc)^
x 2 u^
f 0 = f [x 7! v]^
GoodFrameRets(F , pc, u, f, \Lambda P (ae), ae))
GoodFrameRets(F , pc, u, f 0, \Lambda P (ae), ae)

Proof We proceed by induction on ae:

* In the base case, ae = ffl. Pick arbitrary P , F , pc, u, f , f 0, x, and v. In this

case, GoodFrameRets(F , pc0, u, f 0, \Lambda P (ae), ae) follows from rule (gfr0).

41

* In the inductive step, ae = p * ae0 for some p and ae0. Pick arbitrary P , F , pc, u, f ,

f 0, x, and v. Assume all hypotheses of Lemma 13 hold. The GoodFrameRets
assumption could be true only by rule (gfr1), so

8y, L.  ^ y 2 (Dom(F pc)\u)^ F pc[y] = (ret-from L) ! ) GoodRet(L, f [y], \Lambda P (ae), ae) (2)
and also

GoodFrameRets(F , p, Dom(F pc), f, \Lambda P (ae0), ae0) (3)

First, we want to show that

8y, L.  ^ y 2 (Dom(F pc)\u)^ F pc[y] = (ret-from L) ! ) GoodRet(L, f 0[y], \Lambda P (ae), ae) (4)
Assume y and L such that y is in Dom(F pc)\u and F pc[y] = (ret-from L).
Because x 2 u, it must be that y 6= x, so by our assumption relating f to f 0,
f [y] = f 0[y]. Given this equality, GoodRet(L, f 0[y], \Lambda P (ae), ae) follows from (2).

Next, we want to show that

GoodFrameRets(F , p, Dom(F pc), f 0, \Lambda P (ae0), ae0) (5)
We prove this using the induction hypothesis, which we instantiate as follows:

^ WFCallStack(P, F , p, ae0)^

Dom(F pc) ` Dom(F p)^
x 2 Dom(F pc)^
f 0 = f [x 7! v]^
GoodFrameRets(F , p, Dom(F pc), f, \Lambda P (ae0), ae0))
GoodFrameRets(F , p, Dom(F pc), f 0, \Lambda P (ae0), ae0)

The first two antecedents of this instantiation of the induction hypothesis follow from the fact that the WFCallStack assumption must be true by rule (wf1).
The next antecedent follows from the assumptions that x 2 u and u `
Dom(F pc). The fourth antecedent we are assuming, and the last is (3). Thus,
we can use the induction hypothesis to conclude (5).

Given (4) and (5),

GoodFrameRets(F , pc, u, f 0, \Lambda P (ae), ae)
follows from rule (gfr1).

2

42

The final lemma in this set states an insensitivity of GoodFrameRets:
Lemma 148

P, F , pc, f, ae, pc0, f 0, x, v, T.^

WFCallStack(P, F , pc, ae)^
x 2 Dom(F pc)^
f 0 = f [x 7! v]^
F pc0 = F pc[x 7! T ]^ 8

K. T = (ret-from K) ) GoodRet(K, v, \Lambda P (ae), ae)^
GoodFrameRets(F , pc, {}, f, \Lambda P (ae), ae))
GoodFrameRets(F , pc0, {}, f 0, \Lambda P (ae), ae)

Proof Pick arbitrary P , F , pc, f , ae, pc0, f 0, x, v, and T . We proceed by a case
split on ae:

* The first case is ae = ffl. In this case, GoodFrameRets(F , pc0, {}, f 0, \Lambda P (ae), ae)

follows from rule (gfr0).

* The second case is ae = p * ae0 for some p and ae0. Assume all hypotheses

of Lemma 14 hold. The GoodFrameRets assumption could be true only by
rule (gfr1), so

8y, L.  ^ y 2 (Dom(F pc))^ F pc[y] = (ret-from L) ! ) GoodRet(L, f [y], \Lambda P (ae), ae) (6)
and also

GoodFrameRets(F , p, Dom(F pc), f, \Lambda P (ae0), ae0) (7)

First, we want to show that

8y, L.  ^ y 2 (Dom(F pc0))^ F pc0[y] = (ret-from L) ! ) GoodRet(L, f 0[y], \Lambda P (ae), ae) (8)
By assumption, Dom(F pc0) = Dom(F pc), so (8) is equivalent to

8y, L.  ^ y 2 (Dom(F pc))^ F pc0[y] = (ret-from L) ! ) GoodRet(L, f 0[y], \Lambda P (ae), ae)
Assume y and L such that y is in Dom(F pc) and F pc0[y] = (ret-from L).
For y 6= x, GoodRet(L, f 0[y], \Lambda P (ae), ae) follows from (6) and our assumptions
relating f to f 0 and F pc to F pc0. For y = x, GoodRet(L, f 0[y], \Lambda P (ae), ae) follows
from the assumption

8K. T = (ret-from K) ) GoodRet(K, v, \Lambda P (ae), ae)

43

Next, we want to show that

GoodFrameRets(F , p, Dom(F pc0), f 0, \Lambda P (ae0), ae0) (9)
By assumption, Dom(F pc) = Dom(F pc0), so (9) is equivalent to

GoodFrameRets(F , p, Dom(F pc), f 0, \Lambda P (ae0), ae0)
We prove this using Lemma 13. Our assumptions imply the following conclusions: WFCallStack(P, F , p, ae0)

Dom(F pc) ` Dom(F p)
x 2 Dom(F pc)
f 0 = f [x 7! v]
GoodFrameRets(F , p, Dom(F pc), f, \Lambda P (ae0), ae0)

The first two conclusions follow from the fact that ae is not empty and thus
the WFCallStack assumption can only be true by rule (wf1). The next two we
are assuming directly. The last conclusion is (7). These conclusions are the
antecedents of Lemma 13, so (9) follows.

Given (8) and (9),

GoodFrameRets(F , pc0, {}, f 0, \Lambda P (ae), ae)
follows from rule (gfr1).

2

B.2.2 Preservation of WFCallStack2
Restatement of Part of Lemma 8 For all P , F , and S such that F , S ` P :

8pc, f, s, ae, pc0, f 0, s0, ae0.^

P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i^
WFCallStack(P, F , pc, ae)^
WFCallStack2(P, pc, \Lambda P (ae), ae))
WFCallStack2(P, pc0, \Lambda P (ae0), ae0)

This restatement omits unneeded conjuncts in the hypotheses.
Proof Assume that P , F , S, pc, f , s, ae, pc0, f 0, s0, and ae0 satisfy the hypotheses
of the lemma. The assumption that a step of execution is possible starting fromh

pc, f, s, aei implies that P [pc] is defined; it also implies that P [pc] 6= halt. We do
a case split on the instruction possible at P [pc]:

44

* For all instructions except jsr and ret, we have ae0 = ae (by the structured dynamic semantics), so \Lambda P (ae0) = \Lambda P (ae) (by substitution). Also, by the stackless
static semantics of these instructions, CP,pc0 = CP,pc. Given these equations,

WFCallStack2(P, pc0, \Lambda P (ae0), ae0)
follows from the WFCallStack2 assumption, whether that assumption is true
by rule (wf20) or rule (wf21).

* In the case of jsr L, we have L = pc0 (by the structured dynamic semantics).

We first prove that the WFCallStack2 assumption implies that

\Lambda P (ae) is a subsequence of CP,pc (10)
If ae 6= ffl, then WFCallStack2 must hold by rule (wf21), so (10) must be true
in this case. If ae = ffl, then \Lambda P (ae) = ffl (by rule (l0)) and CP,pc = ffl (because
WFCallStack2 must hold by rule (wf20)), so (10) must be true in this case as
well.

We use rule (wf21) to show that WFCallStack2(P, pc0, \Lambda P (ae0), ae0) is true. According to this rule, it suffices to establish the following three intermediate
results:

1. L 62 \Lambda P (ae). This follows from (10) and L 62 CP,pc, which follows from the

stackless static semantics of jsr L (line 2, jsr rule, Figure 11).

2. \Lambda P (ae0) is a subsequence of CP,pc0. From Lemma 9 we know that \Lambda P (ae0) =

L * \Lambda P (ae). Together with (10), this implies that

\Lambda P (ae0) is a subsequence of L * CP,pc (11)
From the stackless static semantics of jsr L, we know that

L * CP,pc is a subsequence of CP,pc0 (12)
(lines 4 and 5, jsr rule, Figure 11). Given (11) and (12), we can conclude
that

\Lambda P (ae0) is a subsequence of CP,pc0

3. WFCallStack2(P, pc + 1, \Lambda P (ae), ae). We prove this in two cases, the first

when the WFCallStack2 assumption is true by rule (wf20), the second
when the WFCallStack2 assumption is true by rule (wf21). In both of
these cases we have CP,pc+1 = CP,pc, which follows from the stackless
static semantics of jsr L (line 3, jsr rule, Figure 11).

45

If the WFCallStack2 assumption is true by rule (wf20), then ae, \Lambda P (ae),
and CP,pc must be empty. Given CP,pc+1 = CP,pc, CP,pc+1 must be
empty too. Thus,

WFCallStack2(P, pc + 1, \Lambda P (ae), ae)
follows from rule (wf20).
If the WFCallStack2 assumption is true by rule (wf21), then there exist
p, ae00, K, and *00 such that

ae = p * ae00
\Lambda P (ae) = K * *00

K 62 *00
K * *00 is a subsequence of CP,pc

WFCallStack2(P, p, *00, ae00)

Because CP,pc+1 = CP,pc, K **00 is a subsequence of CP,pc+1 as well. Since
K 62 *00, K**00 is a subsequence of CP,pc+1, and WFCallStack2(P, p, *00, ae00),

WFCallStack2(P, pc + 1, K * *00, p * ae00)
follows from rule (wf21). Since ae = p * ae00 and \Lambda P (ae) = K * *00, we obtain

WFCallStack2(P, pc + 1, \Lambda P (ae), ae)

* In the case of ret x, we know from Lemma 9 and from the structured dynamic

semantics that \Lambda 

P (ae) = L * \Lambda P (ae0)

ae = pc0 * ae0

for some L. Applying these equations to the WFCallStack2 assumption, we
can conclude that

WFCallStack2(P, pc, L * \Lambda P (ae0), pc0 * ae0)
This judgment can be true only by rule (wf21), so we can conclude

WFCallStack2(P, pc0, \Lambda P (ae0), ae0)

2

46

B.2.3 Preservation of GoodStackRets
Restatement of Part of Lemma 8 For all P , F , and S such that F , S ` P :

8pc, f, s, ae, pc0, f 0, s0, ae0.^

P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i^
WFCallStack(P, F , pc, ae)^
WFCallStack2(P, pc, \Lambda P (ae), ae)^
GoodFrameRets(F , pc, {}, f, \Lambda P (ae), ae)

^ 8j, L.  ^ j 2 Dom(Spc)^ Spc[j] = (ret-from L) ! ) GoodRet(L, s[j], \Lambda P (ae), ae)

) 8j, L.  ^ j 2 Dom(Spc0)^ Spc0[j] = (ret-from L) ! ) GoodRet(L, s0[j], \Lambda P (ae0), ae0)
Again, this restatement omits unneeded conjuncts in the hypotheses. Also, for convenience, this restatement expands the definition of GoodStackRets.

Proof Assume that P , F , S, pc, f , s, ae, pc0, f 0, s0, and ae0 satisfy the hypotheses
of the lemma. The assumption that a step of execution is possible starting fromh

pc, f, s, aei implies that P [pc] is defined; it also implies that P [pc] 6= halt. We do
a case split on the instruction possible at P [pc]:

* First we look at all instructions except jsr and ret. For these instructions,

ae = ae0 by the structured dynamic semantics of the individual instructions; also,
by substitution, \Lambda P (ae) = \Lambda P (ae0) as well.

Assume j and L such that j 2 Dom(Spc0) and Spc0[j] = (ret-from L); we
need to show that

GoodRet(L, s0[j], \Lambda P (ae0), ae0)

This follows from rule (gr0) when ae0 = ffl. Assume that ae0 6= ffl and proceed
with a case split on j:

- The first case is stack slots that are unchanged, that is, stack slots that

are not written by the instruction but rather are carried over. The exact
set is going to vary by instruction, but values of j in this set will satisfy
the following:

j 2 Dom(Spc) ^ Spc0[j] = Spc[j] ^ s0[j] = s[j]
For these stack elements, if Spc0[j] equals (ret-from L), then Spc[j] also
equals (ret-from L). This implies GoodRet(L, s[j], \Lambda P (ae), ae), which by
substitution means GoodRet(L, s0[j], \Lambda P (ae0), ae0).

47

- The second case is stack slots that are written by the instruction. It turns

out that this case applies only to load for the following reasons:*

The instructions pop, if, and store do not write the stack, so this
second case does not apply to these instructions.*

For inc and push0, if j is the element written (that is, the top element of s0), we know from the stackless static semantics that the
type Spc0[j] is Int, which violates our assumption that Spc0[j] equals
(ret-from L), so this second case does not apply to these instructions either.

For load x, we know from the stackless static semantics of load x that
x 2 Dom(F pc) and F pc[x] = Spc0[j]. Given the assumption that ae0 6=
ffl, the GoodFrameRets assumption must be true by rule (gfr1). The
antecedents of this rule and the assumptions that x 2 Dom(F pc) and
Spc0[j] = F pc[x] = (ret-from L) imply that

GoodRet(L, f [x], \Lambda P (ae), ae)
By the structured dynamic semantics of load x, s0[j] = f [x] and ae0 = ae,
so by substitution

GoodRet(L, s0[j], \Lambda P (ae0), ae0)

* For jsr and ret, assume j and L such that j 2 Dom(Spc0) and Spc0[j] =

(ret-from L); we need to show that

GoodRet(L, s0[j], \Lambda P (ae0), ae0) (13)
But first we prove a helpful fact. For both jsr and ret, when j 2 Dom(Spc) we
can assume that Spc[j] = Spc0[j]. For jsr, this follows from the stackless static
semantics (line 6, jsr rule, Figure 13). For ret, this follows from Lemma 2;
to apply Lemma 2, we need that ae = pc0 * ae0, which follows from the structured
dynamic semantics of ret.

Given Spc[j] = Spc0[j] for j 2 Dom(Spc), Spc0[j] = (ret-from L), and the
GoodStackRets assumption, we can conclude:

j 2 Dom(Spc) ) GoodRet(L, s[j], \Lambda P (ae), ae)
Given this and the fact that s[j] = s0[j] for j 2 Dom(Spc) (which follows from
the structured dynamic semantics of jsr and ret), we can conclude:

j 2 Dom(Spc) ) GoodRet(L, s0[j], \Lambda P (ae), ae) (14)

Now we return to proving (13) for the cases of interest:

48

- In the case of jsr K, we do a case split on j:

1. Case: j 62 Dom(Spc), that is, j is the index of the top of the stack.

In this case we know from the stackless static semantics of jsr K that
K = L (line 6, jsr rule, Figure 13), we know from the structured
dynamic semantics that s0[j] = pc + 1 and ae0 = (pc + 1) * ae, and we
know by Lemma 9 that \Lambda P (ae0) = L * \Lambda P (ae). Given these equations,
(13) follows by rule (gr1).

2. Case: j 2 Dom(Spc). In this case, we know from the stackless static

semantics of jsr K that K 6= L (line 7, jsr rule, Figure 13). We
know from the structured dynamic semantics that ae0 = (pc + 1) * ae,
and we know from Lemma 9 that \Lambda P (ae0) = K * \Lambda P (ae). Given these
equations and (14), (13) follows from rule (gr2).

- In the case of ret, we know from Lemma 9 and the structured dynamic

semantics that: \Lambda 

P (ae) = K * \Lambda P (ae0)

ae = pc0 * ae0

for some K. Given these equations, we prove (13) by cases on L:

1. Case: K 6= L. By the stackless static semantics, Dom(Spc0) is equal

to Dom(Spc), so j 2 Dom(Spc). Given this, the conclusion of (14)
must be true. Given \Lambda P (ae) = L * \Lambda P (ae0) and K 6= L, the conclusion
of (14) can be true only by rule (gr2), so we can conclude

GoodRet(K, s0[j], \Lambda P (ae0), ae0)
2. Case: K = L. From the WFCallStack2 assumption and Lemma 10

we know that \Lambda P (ae) and ae have the same length; given this, \Lambda P (ae0)
and ae0 must also have the same length. The assumption that a
step of execution is possible starting from hpc, f, s, aei implies that
ae is not empty, thus the WFCallStack2 assumption must be true by
rule (wf21), so L 62 \Lambda P (ae0). Thus, (13) follows from Lemma 11.

2

49

B.2.4 Preservation of GoodFrameRets
Restatement of Part of Lemma 8 For all P , F , and S such that F , S ` P :

8pc, f, s, ae, pc0, f 0, s0, ae0.^

P `s hpc, f, s, aei ! hpc0, f 0, s0, ae0i^
WFCallStack(P, F , pc, ae)^
WFCallStack2(P, pc, \Lambda P (ae), ae)^
GoodStackRets(Spc, s, \Lambda P (ae), ae)^
GoodFrameRets(F , pc, {}, f, \Lambda P (ae), ae))
GoodFrameRets(F , pc0, {}, f 0, \Lambda P (ae0), ae0)

This time, we need all conjuncts in the hypotheses.
Proof Assume that P , F , S, pc, f , s, ae, pc0, f 0, s0, and ae0 satisfy the hypotheses
of the lemma. The assumption that a step of execution is possible starting fromh

pc, f, s, aei implies that P [pc] is defined; it also implies that P [pc] 6= halt.

When ae0 = ffl, we have \Lambda P (ae0) = ffl by rule (l0). Thus, we can conclude

GoodFrameRets(F , pc0, {}, f 0, \Lambda P (ae0), ae0)
by rule (gfr0).

When ae0 is not empty, we do a case split on the instruction possible at P [pc]:

* The first case is inc, pop, push0, if, and load x for any x. It follows from the

stackless static semantics of these instructions that Dom(F pc) = Dom(F pc0)
and, for all y in Dom(F pc), F pc[y] = F pc0[y]. It follows from the structured
dynamic semantics of these instructions that f = f 0 and ae = ae0.

Because ae is not empty, we can assume that ae0 = p * ae00 for some p and ae00. The
GoodFrameRets assumption must be true by rule (gfr1), so we can conclude
that

^ 8y, L.  ^ y 2 Dom(F pc)^ F pc[y] = (ret-from L) ! ) GoodRet(L, f [y], \Lambda P (ae), ae)
^ GoodFrameRets(F , p, Dom(F pc), f, \Lambda P (ae00), ae00)

Using the equalities f = f 0, ae = ae0, Dom(F pc) = Dom(F pc0) and, for y in
Dom(F pc), F pc[y] = F pc0[y], we can conclude:

^ 8y, L.  ^ y 2 Dom(F pc0)^ F pc0[y] = (ret-from L) ! ) GoodRet(L, f 0[y], \Lambda P (ae0), ae0)
^ GoodFrameRets(F , p, Dom(F pc0), f 0, \Lambda P (ae00), ae00)

50

From this we can conclude

GoodFrameRets(F , pc0, {}, f 0, \Lambda P (ae0), ae0)
by rule (gfr1).

* The next case is store x. Let v and T satisfy s = v * s0 and Spc = T * Spc0.

We know these exist by the structured dynamic semantics and stackless static
semantics of store x, respectively. We want to apply Lemma 14 to conclude:

GoodFrameRets(F , pc0, {}, f 0, \Lambda P (ae), ae)
To do so, we must discharge the antecedents of Lemma 14. The first antecedent, WFCallStack, we are assuming. The second and third antecedents
follow from the stackless static semantics and the structured dynamic semantics
of store x, respectively. The fourth antecedent follows from the stackless static
semantics. The fifth and sixth antecedents follow from the GoodStackRets
and GoodFrameRets assumptions, respectively. Thus, we can indeed apply
Lemma 14 to conclude

GoodFrameRets(F , pc0, {}, f 0, \Lambda P (ae), ae)
The structured dynamic semantics of store x implies that ae = ae0. Substituting, we get

GoodFrameRets(F , pc0, {}, f 0, \Lambda P (ae0), ae0)

* The next case is jsr K. From Lemma 9 and the structured dynamic semantics

of jsr K we know that \Lambda 

P (ae0) = K * \Lambda P (ae)

ae0 = (pc + 1) * ae

f 0 = f
pc0 = K

From the stackless static semantics we know that

Dom(F K ) ` Dom(F pc)
Dom(F pc+1) = Dom(F pc)

We want to prove:

GoodFrameRets(F , pc0, {}, f 0, \Lambda P (ae0), ae0)
Substituting, this is equivalent to

GoodFrameRets(F , K, {}, f, K * \Lambda P (ae), (pc + 1) * ae)

51

To establish this via rule (gfr1), it suffices to show that

8y, L.  ^ y 2 Dom(F K )^ F

K [y] = (ret-from L) !) GoodRet(

L, f [y], K * \Lambda P (ae), (pc + 1) * ae)

(15)

and also that

GoodFrameRets(F , pc + 1, Dom(F K ), f, \Lambda P (ae), ae) (16)

To prove (15), we assume y and L such that y is in Dom(F K ) and F K[y] =
(ret-from L) and show that

GoodRet(L, f [y], K * \Lambda P (ae), (pc + 1) * ae) (17)
To show this, first we show that

GoodRet(L, f [y], \Lambda P (ae), ae) (18)
If ae is empty, then (18) follows from rule (gr0). If ae is not empty, then the
GoodFrameRets assumption must hold by rule (gfr1). Given the first line of
rule (gfr1) and the fact that y 2 Dom(F K ) and F K [y] = (ret-from L), (18)
follows. We know K 6= L from the stackless static semantics of jsr K (line 8,
Figure 13). Given this and (18), we can conclude (17) by rule (gr2).

Next, we need to prove (16). If ae is empty, then (16) follows from rule (gfr0).
Otherwise, we assume that ae = p * ae00 for some p and ae00. Given ae = p * ae00, to
establish (16) via rule (gfr1), it suffices to show that

8y, L.  ^ y 2 (Dom(F pc+1)\Dom(F K ))^ F pc

+1[y] = (ret-from L) ! ) GoodRet(L, f [y], \Lambda P (ae), ae)(19)

and also that

GoodFrameRets(F , p, Dom(F pc+1), f, \Lambda P (ae00), ae00) (20)
Because ae 6= ffl, the GoodFrameRets assumption must be true by rule (gfr1),
so we can conclude that

8y, L.  ^ y 2 Dom(F pc)^ F pc[y] = (ret-from L) ! ) GoodRet(L, f [y], \Lambda P (ae), ae) (21)
and also that

GoodFrameRets(F , p, Dom(F pc), f, \Lambda P (ae00), ae00) (22)

52

To prove (20), we simply observe that, because Dom(F pc+1) = Dom(F pc),
(22) is equivalent to (20).

To prove (19), we assume y and L such that y is in Dom(F pc+1)\Dom(F K )
and F pc+1[y] equals (ret-from L) and show that

GoodRet(L, f [y], \Lambda P (ae), ae) (23)
From the stackless static semantics of jsr K (line 4, jsr rule, Figure 13),
because y is in Dom(F pc+1)\Dom(F K ), we can conclude that F pc[y] equals
F pc+1[y] which in turn equals (ret-from L). Also, given Dom(F pc+1) =
Dom(F pc), we can conclude that y 2 Dom(F pc). Given (21), y 2 Dom(F pc),
and F pc[y] = (ret-from L), we can conclude (23).

* The last case is ret x. From Lemma 9 and the structured dynamic semantics

of ret x we know that, for some K:

\Lambda P (ae) = K * \Lambda P (ae0)

ae = pc0 * ae0

f 0 = f

We want to prove:

GoodFrameRets(F , pc0, {}, f 0, \Lambda P (ae0), ae0)
If ae0 is empty, then this follows from rule (gfr0).
Otherwise, we have that ae0 = p * ae00 for some p and ae00. We are assuming that

GoodFrameRets(F , pc, {}, f, \Lambda P (ae), ae)
Applying the equations above, this is equivalent to

GoodFrameRets(F , pc, {}, f 0, K * \Lambda P (ae0), pc0 * ae0)
This must be true by rule (gfr1), so we can conclude that

8y, L.  ^ y 2 Dom(F pc)^ F pc[y] = (ret-from L) !

) GoodRet(L, f 0[y], K * \Lambda P (ae0), pc0 * ae0)

(24)

and also that

GoodFrameRets(F , pc0, Dom(F pc), f 0, \Lambda P (ae0), ae0) (25)

53

Because ae0 = p * ae00, (25) must be true by rule (gfr1), so

8y, L.  ^ y 2 (Dom(F pc0)\Dom(F pc))^ F pc0[y] = (ret-from L) ! ) GoodRet(L, f 0[y], \Lambda P (ae0), ae0)

(26)
and also

GoodFrameRets(F , p, Dom(F pc0), f 0, \Lambda P (ae00), ae00) (27)

Because ae 6= ffl, the WFCallStack assumption must be true by rule (wf1), so
P [pc0 - 1] = jsr M for M such that RP,pc = {M }. Given this, by the stackless
static semantics of ret x (lines 2 and 5, ret rule, Figure 13), we know that,
for y in Dom(F pc), F pc0[y] = F pc[y]. Combining this with (24), we get

8y, L.  ^ y 2 Dom(F pc)^ F pc0[y] = (ret-from L) !

) GoodRet(L, f 0[y], K * \Lambda P (ae0), pc0 * ae0)
Using Lemma 12 and the WFCallStack2 assumption, we can conclude

8y, L.  ^ y 2 Dom(F pc)^ F pc0[y] = (ret-from L) ! ) GoodRet(L, f 0[y], \Lambda P (ae0), ae0) (28)
Combining (26) and (28), we can conclude

8y, L.  ^ y 2 Dom(F pc0)^ F pc0[y] = (ret-from L) ! ) GoodRet(L, f 0[y], \Lambda P (ae0), ae0) (29)

Given (29) and (27), we can use rule (gfr1) to conclude

GoodFrameRets(F , pc0, {}, f 0, \Lambda P (ae0), ae0)

2

C Proof of main soundness theorem
To prove Theorem 4, our main soundness theorem, we use two lemmas. This section
first states these lemmas, proves the main soundness theorem, then proves the two
lemmas.

The first lemma says that well-typed programs get "stuck" only at halt instructions:

54

Lemma 15 For all P , F , and S such that F , S ` P :

8pc, f, s, ae.^

Consistent(P, F , S, pc, f, s, ae)^ 8

y. 9T. F(F , pc, ae)[y] = T ^ f [y] : T^
s : Spc^
pc 2 Dom(P )^
P [pc] 6= halt) 9
pc0, f 0, s0. P ` hpc, f, si ! hpc0, f 0, s0i

The next lemma takes invariants stated about individual steps of execution and
extends them over multi-step execution sequences:

Lemma 16 For all P , F , and S such that F , S ` P :

8pc, f0, f, s.

P ` h1, f0, ffli !* hpc, f, si) 9

ae.^

Consistent(P, F , S, pc, f, s, ae)^ 8

y. 9T. F(F , pc, ae)[y] = T ^ f [y] : T^
s : Spc^
pc 2 Dom(P )

We use these lemmas to prove the main soundness theorem:
Restatement of main soundness theorem For all P , F , and S such that
F , S ` P : 8pc

, f0, f, s.

P ` h1, f0, ffli !* hpc, f, si^ 6 9

pc0, f 0, s0. P ` hpc, f, si ! hpc0, f 0, s0i !)

P [pc] = halt ^ s : Spc

Proof Assume that P , F , S, pc, f , and s satisfy the hypotheses of this theorem.
Under this assumption, the second conjunct of the conclusion (s : Spc) follows
directly from Lemma 16.

We prove the first conjunct of the conclusion by contradiction. Assume that
P [pc] 6= halt. Our previous assumptions about P , F , S, pc, f , and s imply the
hypotheses of Lemma 16, so we can conclude that there exists ae such that P , F ,
S, pc, f , s, and ae satisfy all hypotheses of Lemma 15. However, the conclusion of
Lemma 15 contradicts our assumption that

6 9pc0, f 0, s0. P ` hpc, f, si ! hpc0, f 0, s0i
Thus, we are forced to conclude that P [pc] = halt. 2

55

C.1 Making progress
Lemma 15 says that a well-typed program does not get stuck unless it hits a halt
instruction:

Restatement of Lemma 15 For all P , F , and S such that F , S ` P :

8pc, f, s, ae.^

Consistent(P, F , S, pc, f, s, ae)^ 8

y. 9T. F(F , pc, ae)[y] = T ^ f [y] : T^
s : Spc^
pc 2 Dom(P )^
P [pc] 6= halt) 9
pc0, f 0, s0. P ` hpc, f, si ! hpc0, f 0, s0i

Proof Assume that P , F , S, pc, f , s, and ae satisfy the hypotheses of the lemma.
We proceed with a case split on the instruction at P [pc], constructing pc0, f 0, and s0
satisfying the stackless dynamic semantics of the instruction at P [pc].

* For load x, push0, and jsr L, progress can always be made. For load x and

push0, s0 = s, f 0 = f , and pc0 = pc + 1. For jsr L, s0 = (L + 1) * s, f 0 = f ,
and pc0 = pc + 1.

* For inc, if L, pop, and store x, progress can be made when the stack has at

least one value of the appropriate type in it. The assumption that s : Spc plus
the static constraints on Spc for each instruction ensure that the stack does
indeed have an appropriately typed value on top. For all instructions, assume
s = v * s00 for some v and s00. For inc, if L, and pop, we take f 0 = f ; for
store x, we take f 0 = f [x 7! v]. For if L, pop, and store x, we take s0 = s00.
For inc, v must be some integer n, and we take s0 = (n + 1) * s00. For inc, pop,
and store x, pc0 = pc + 1. For if L, v must be some integer n, and we take
pc0 = pc + 1 if n is zero and we take pc0 = L for other values of n.

* For ret x, progress can be made in states where f [x] is an address. The

assumption that 9

T. F(F , pc, ae)[x] = T ^ f [x] : T

and the stackless static semantics of ret x imply that f [x] does contain an
address. Thus, progress is possible, and we take pc0 = f [x], f 0 = f , and s0 = s.

2

56

C.2 Chained soundness theorem
Before proving Lemma 16, we state and prove one more invariant about individual
steps of execution:

Lemma 17 For all P , F , and S such that F , S ` P :

8pc, f, s, ae, pc0, f 0, s0.^

P ` hpc, f, si ! hpc0, f 0, s0i^
Consistent(P, F , S, pc, f, s, ae))
pc0 2 Dom(P )

Proof Assume that P , F , S, pc, f , s, and ae satisfy the hypotheses of the lemma.
The assumption that a step of execution is possible starting from hpc, f, si implies
that P [pc] is defined; it also implies that P [pc] 6= halt. We do a case split on the
instruction possible at P [pc]:

* For pop, push0, inc, load, and store, pc0 = pc + 1, and pc + 1 is constrained

by the stackless static semantics of these instructions to be in Dom(P ).

* For if L, pc0 equals either pc + 1 or L, depending on which way the branch

goes. Both pc + 1 and L are constrained by the stackless static semantics of
if L to be in Dom(P ).

* For jsr L, pc0 = L, and L is constrained by the stackless static semantics of

jsr L to be in Dom(P ).

* For ret x, we first show that our assumptions imply that ae has at least one

element. If ae had no elements, then the WFCallStack2 part of the Consistent
assumption would have to be true by rule (wf20). This would require that CP,pc
be empty, but the stackless static semantics for ret x implies that CP,pc must
have at least one element. Thus, WFCallStack2 cannot hold by rule (wf20).
Instead, it must hold by rule (wf21), so ae must have at least one element.

Let ae = p * ae0 for some p and ae0. Using Lemma 7, we have that pc0 = p. The
WFCallStack part of the Consistent assumption must be true by rule (wf1), so
P [p - 1] = jsr L for some L. From the stackless static semantics of jsr L we
have that (i+1) 2 Dom(P ) for all i such that P [i] = jsr L. Thus, substituting
p - 1 for i, we can conclude that p, and thus pc0, is in Dom(P ).

2

Lemma 16 applies Lemma 17 and Theorem 3 to multi-step executions starting
from the initial state:

57

Restatement of Lemma 16 For all P , F , and S such that F , S ` P :

8pc, f0, f, s.

P ` h1, f0, ffli !* hpc, f, si) 9

ae.^
Consistent(P, F , S, pc, f, s, ae)^ 8

y. 9T. F(F , pc, ae)[y] = T ^ f [y] : T^
s : Spc^
pc 2 Dom(P )

Proof Assume P , F , and S such that F , S ` P and proceed by induction on the
number of execution steps in P ` h1, f0, ffli !* hpc, f, si:

* In the base case, hpc, f, si equals h1, f0, ffli, that is, no steps of execution are

taken. In Section 3, it is assumed that P [1] is defined for all programs, so we
can conclude pc 2 Dom(P ). We pick ae = ffl and observe that:

^ Consistent(P, F , S, 1, f0, ffl, ffl)^ 8

y. 9T. F(F , 1, ffl)[y] = T ^ f0[y] : T^
ffl : S1

Given the values of F 1, S1, and CP,1 it is not hard to check this.

* In the inductive step, let hpcn, fn, sni be a state such that

P ` h1, f0, ffli !* hpcn, fn, sni ! hpc, f, si
By induction, we know that there exists a aen such that:

^ Consistent(P, F , S, pcn, fn, sn, aen)^ 8

y. 9T. F(F , pcn, aen)[y] = T ^ fn[y] : T^
sn : Spcn^
pcn 2 Dom(P )

Our assumptions and these conjuncts satisfy the hypotheses of Theorem 3 and
Lemma 17, so we can conclude that

^ Consistent(P, F , S, pc, f, s, ae)^ 8

y. 9T. F(F , pc, ae)[y] = T ^ f [y] : T^
s : Spc^
pc 2 Dom(P )

2

58

Acknowledgements
We thank Luca Cardelli, Drew Dean, Sophia Drossopoulou, Stephen Freund, Cynthia Hibbard, Mark Lillibridge, Greg Morrisett, George Necula, Frank Yellin, and
anonymous referees for useful information and suggestions.

5960
References
[Coh97] Richard M. Cohen. Defensive Java Virtual Machine version 0.5 alpha

release. Web pages at http://www.cli.com/, May 1997.

[DE97] Sophia Drossopoulou and Susan Eisenbach. Java is type safe--probably.

In Proceedings of ECOOP '97, pages 389-418, June 1997.

[FM98] Stephen N. Freund and John C. Mitchell. A type system for object

initialization in the Java bytecode language. Technical Report STAN-CSTN-98-62, Department of Computer Science, Stanford University, April
1998. To appear in Proceedings of OOPSLA '98.

[Gol97] Allen Goldberg. A specification of Java loading and bytecode verification. To appear in Proceedings of the Fifth ACM Conference
on Computer and Communications Security, 1998; Web page at
http://www.kestrel.edu/~goldberg/Bytecode.html, 1997.

[HT98] Masami Hagiya and Akihiko Tozawa. On a new method for dataflow

analysis of Java Virtual Machine subroutines. On the Web at http://
nicosia.is.s.u-tokyo.ac.jp/members/hagiya.html; a preliminary
version appeared in SIG-Notes, PRO-17-3, Information Processing Society of Japan, pages 13-18, 1998.

[LY96] Tim Lindholm and Frank Yellin. The Java Virtual Machine Specification.

Addison-Wesley, 1996.

[Mor95] Greg Morrisett. Compiling with Types. PhD thesis, Carnegie Mellon

University, 1995.

[MTC+96] G. Morrisett, D. Tarditi, P. Cheng, C. Stone, R. Harper, and P. Lee. The

TIL/ML compiler: Performance and safety through types. In Workshop
on Compiler Support for Systems Software, 1996.

[Nv98] Tobias Nipkow and David von Oheimb. Javalight is type-safe--definitely.

In Proceedings of the 25th ACM Symposium on Principles of Programming Languages, pages 161-170, January 1998.

[Qia98] Zhenyu Qian. A formal specification of Java Virtual Machine instructions

for objects, methods and subroutines. In Jim Alves-Foss, editor, Formal
Syntax and Semantics of JavaTM. Springer-Verlag, 1998. To appear.

[Sar97] Vijay Saraswat. The Java bytecode verification problem. Web page at

http://www.research.att.com/~vj/main.html, 1997.

61

[SMB97] Emin G"un Sirer, Sean McDirmid, and Brian Bershad. Kimera: A

Java system security architecture. Web pages at http://kimera.cs
.washington.edu/, 1997.

[Sym97] Don Syme. Proving Java type soundness. Technical Report 427, University of Cambridge Computer Laboratory, June 1997.

[TIC97] ACM SIGPLAN Workshop on Types in Compilation (TIC97). 1997.
[Yel97] Frank Yellin. Private communication. March 1997.

62