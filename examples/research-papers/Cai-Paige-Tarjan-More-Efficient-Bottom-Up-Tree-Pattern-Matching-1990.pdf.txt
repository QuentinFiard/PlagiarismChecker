

More Efficient  Bottom-Up Tree Pattern  Matching 

J. Cai and R. Paige 1  and Dept. of Computer  Science 

NYU/Courant Institute New  York,  NY 10012 

R. Tarjan  2 Dept. of Computer  Science  and  NEC  Research  Institute 

Princeton University  4  Independence  Way Princeton, NJ 08540  Princeton, NJ 08544 

ABSTRACT 
Pattern  matching  in  trees  is  fundamental  to  a  variety  of  programming 
language systems.  However, progress has been slow in satisfying a pressing need 

for general purpose pattern matching algorithms that are efficient in both time and 
space,  We  offer  asymptotic  improvements  in  both  time  and  space  to  Chase's 
bottom-up algorithm for  pattern preprocessing.  Our preprocessing algorithm has 

the additional advantage of being incremental  with respect to pattem additions  and 
deletions.  We  show  how  to  modify  our  algorithm  using  a  new  decomposition 
method to obtain a space/time tradeoff.  Finally, we trade a log factor in time for a 

linear space bottom-up pattern matching algorithm that handles a wide subclass of 

Hoffmann and O'Donnell's Simple Patterns. 

I.  Introduction 

Pattem  Matching  in  trees  is  fundamental  to  term  rewriting  systems  [11],  transformational 
programming systems  [4,7,18,22],  program editing  and  development systems [6,13],  code gen- 

erator  generators  [9, 17],  theorem  provers/14],  logic  programming  optimizers  that  attempt  to 
replace unification  with  matching/16],  and  compilers  for ML[21],  Haskell[12],  and  a  variety of 
functional  languages  with  equational  function  definitions.  However, this  problem  seems  to  be 
extremely difficult.  The best known  space-efficient top-down algorithm to locate  all occurrences 
of a  pattem tree  of size t  in  a  tree of size  n  takes  O(nl'75polylog(l))  time,  a  recent  result due  to 

Kosaraju [15],  which is barely better than the naive O(nl) algorithm.  Bottom-up pattern matching 

seems to be even more difficult  than top-down matching and is of special practical importance.  In 
a  seminal paper Hoffmann and  O'Donnell presented bottom-up tree  pattem matching  algorithms 
that were highly efficient in time but required  excessive space [10] both in theory and practice (see 

Chase's empirical data  [5]).  Hoffmann and  O'DonneU's work has stimulated  a  number of papers 
offering heuristic  space improvements [2,  3, 5,19],  and  Chase's method has  aroused considerable 

attention  [5].  However,  none  of  these  papers  offer  theoretical  improvements  or  promising 
space/time tradeoffs. 

In this paper we present two new theoretical  results in bottom-up tree matching. 

1. Part of this  work was done  while Paige was  a summer faculty at IBM T, J.  watson  Research  Center.  This  work  is  also 
partly  based on research suptx~ed by the  Office of Naval  Research  under  Contract  No.  N00014-8%0461. 2.  Research at Prlnceson  University  partially  supported by  DIMACS  (Center for Discrete  Mathematics  and  Theoretical 

Computer  Science),  a  National  Science  Foundation  Science  and  Technology  Center,  grant  NSF-STC88-09648,  and the 

Office of Naval Research, contract N00014-S7-K-0467. 

73 
I.  At the end of his CAAP '88 paper [3] Burghardt called for an algorithm that could prepro- 
cess  patte:m  trees  incrementally  as  worthwhile  future  research.  Such an algorithm  is  needed  in the 
RAFFS  U'ansformational  programming  system  [4],  because  incrementally  modifying  systems  of 

rewrite rules is a frequent activity, and preprocessing full sets of pattems is highly expensive. 

In this  paper we present a modification to Chase's  algorithm so that its  costliest task, prepro- 
cessing, can  be  achieved incrementally  with  respect to additions  and deletions  of patterns.  When 
our algorithm is applied  repeatedly to solve nonincremental preprocessing by adding one pattern  at 

a  time  starting  from  the  empty  set,  it  runs  asymptotically  better  in  time  and  space  than  Chase's 
algorithm. 

2.  In bottom-up  pattern  matching,  the  main  difficulty  that  sorely  needs  to  be  overcome  is 
space  utilization.  We  present  an  algorithm  for  a  subclass  of Hoffmann  and  O'Donnell's  Simple 
Patterns that runs in O(/) space overall and O(log/)  time per step, where l is a parameter related  to 
the number of input patterns.  Previous bounds due to Hoffmann and O'DonneU are 0(l 2) time  and 

space  for an  algorithm  tailored  to  binary  Simple  Patterns  (which  our  subclass  properly  includes) 
and  O(l k ~ + l )   space  with  O(1)  step  time  for an  algorithm  handling  all  Simple  Patterns,  where 
kmax is the maximum  arity of a pattern.  Thus,  we offer a  quadratic  space improvement over the 
latter  algorithm  for binary  patterns  and  even  more  dramatic  improvement  for patterns  of greater 

arity.  Our space compression is obtained by applying persistent data structures in a new  way. 

2.  Bottom-up pattern matching 

Hoffmann and O'Donnell [10] define patterns inductively as follows: 

Definition:  Given  an  alphabet  of one distinguished  variable  v  and  function  symbols f  with fixed arity A(f), then the  set of patterns  is the smallest  set of terms that include (i)  v,  (ii) constant c 

ifc is a fimction symbol with arity 0, and (iii) f  (p 1,  "  " ",Pk),  which we call an f-pattern,  if f  is  a function symbol of arity k and p l,  "  " ",Pk  are pattems. 

They also define pattern matching as follows: 

Definition:  Pattern  Pl  is  said  to  be  more  general  than pattem P2,  denoted  by Pl  >P2,  iff either (i)  p  1 is v, or (ii) p  1 is f  (Xl,  " " ,   xk), P2  is f  (y 1,  " " ,   Yk) and xi > Yi for i  =  1, . . - ,   k. 

If p  1 -> P  2, we also say that p  1 matches p  2 or that  [p 1, P  2]  is  a match.  The  set of subexpres- 
sions, or subpattems,  o f p   is denoted by sub (p).  A  slightly more general  form of the main pattern 
matching problem considered by Hoffmann and O'Donnell is: 

Problem(Multi-pattem  matching): Given a set P  of pattems  and a pattem t called the subject, find the  set MP TM(t)=  {[p,  q]: p  ~ P,  q  ~ sub(t)  1  p  > q}  of all  patterns  in P  matching  subpat- 

tems of t. 

As  the  preceding  notation  suggests,  bottom-up  solutions  presented  by  Hoffmann  and 
O'Donnell  and  Chase  treat  the  set P  of patterns  as fixed and the  subject  t (which for them has  no 
variables)  as the  only parameter  that can vary.  In a bottom-up  strategy to  solve the  multi-pattern 
matching problem,  a complete set MPTM (q)  of matches is found for each subpattem  q of t without 

reference to any subpattem of t that properly encloses q. 

In order to explain  these  algorithms,  we need to  first present  a few  definitions  and  notational conventions. 

Detinition: I f P  is a set of patterns,  then thepattern  forest PF of P  is the set of subpatterns  of all the patterns in P. 

74 
Definition:  If PF  is  the  pattern  forest  for  a  set P  of pattems  and  t  is the  subject,  then  the match  set MS (t) for t is defined by the rule MS (t) =  {q  ~  PF  I  q  > t}. 
The  main  idea underlying  Hoffmann and  O'Donnell's bottom-up  algorithm  is  the  following 
equivalent recursive definition: 

MS(v) = {v} 
MS(c) =  {v}, when constant c ~I PF 

{v,c}, when constant c  ~ PF 
(1)  M S(f   (fl,  " '',   tk)) =  {f(ql,  " " ,   qk) ~  PF  I qi  E  MS(ti),i  =  1,  " " ,   k} U  {v} 
After determining  match sets for constants and variable  occurring in the subject t, the main task of 
Hoffmann and  O'Donnell's bottom-up  algorithm  is  to  identify  the  match  set  for each  subpattern 
f ( t l ,   ""  *,  tk)  of  t  based  on  the  match  sets  for  tl,  i  = 1,  . . . ,   k.  This  is  achieved  by  solving 

expression (1), which  we call the Basic  Bottom-Up  Step. 

Consider a multi-pattern matching problem instance with pattern set P, pattern  forest PF,  and 
subject t.  We will use the following parameters throughout this paper: 

n  = length of t m  = number of match sets for P 

l =   IPFI o  =  I MPTM (t) 1 
kmax  = maximum arity of any function appearing in PF 
In order to compute Step (1) and print the set M S( f (t l ,   " " ,   tk)) n  P of patterns  that match 
f  (tl,  " " ,   tk)  in  time  O(k  +  IM S(f   (tl,  . '. ,   t k ) )n P   1),  Hoffmann  and  O'Donnell  preprocess 

the patterns in P  to 

i.  encode  each  pattern  in  PF  as  a  distinct  integer  from  1 to  l,  and  represent  patterns  as trees  in the obvious way (implemented  in compressed form as dags); 

ii.  compute all match sets,  and encode each such set as a distinct integer from  1 to m; iii.  compute  the  subset of pattems  in P  belonNng  to the  ith match  set for i  =  l ....  m; 
iv.  compute  a  transition  function zf   for every  k-ary  function  symbol f  occurring in  P  so that zf(msl,  " " ,   msk)  = M S (f   (t 1,  . . . ,   tk)  whenever msj  = MS(tj)  f or j  =  1 ....  k;  % 

=  {v}, and  %  {v,  c} if  c  is  any  constant  appearing  in  PF;  transition  maps  zf  are implemented  as  multi-dimensional  arrays  accessed  using  integer  encodings  of match 

sets. 
After  preprocessing  the  patterns  in  P,  Hoffmann  and  O'Donnelt's  algorithm  solves  the 
multi-pattem  matching  problem  by repeatedly  solving Step  (1) from innermost  to outermost  sub- 
pattern  of t.  Their  worst case  time  is  O (n + o)  after preprocessing P.  The  transition  table  zy for 
each k-ary   function symbol f  appearing  in PF  uses  ~(m k) space,  where the  number m  of match 

sets  can  be  ~2(2t),  which  is  expensive  in  practice.  Their  rough  bound  on  preprocessing  time  is 

O (12m/~Urc). 

Several  approaches  seem  reasonable  to  overcome the  large  preprocessing  and  space  costs. 
Chase  [5] saves  space  in the  transition  function by eliminating  some  redundancy.  Hoffmann and 
O'Donnell restrict the class of patterns to the Simple  Patterns  for which m is always small - essen- 
tiany  O (I).  For Simple  Patterns  that  are further restricted  to have  arity less  than or equal  to two, 

Hoffmann and O'DonneU give an algorithm where match sets can be avoided entirely. 

In the next  section we show how to make  Chase's preprocessing  algorithm  incremental  and 
asymptotically better  in  both  time  and  space.  We  also  present  a  general problem  decomposition 
technique that allows the algorithm to be tailored according to a space/time tradeoff.  After that,  we 

show  how  to  improve  the  space  and  time  for  Hoffmann  and  O'Donnell's  algorithm  for  binary 

75 
Simple Patterns  and how to extend the algorithm to a wide subclass of Simple Patterns with unres- 
tricted arity. 

3.  Incremental preprocessing 

Chase  was  able  to  improve  Hoffmann  and  O'Dounell's  method  by  exploiting  the  deeper 
structure  of  the  pattem  set  P  to  reduce  the  size  of  transition  functions[5].  Chase's  heuristic 
preserves the O (1) per step matching time, 

Let PF be the pattern forest for P,  and assume that it contains variable v.  For each function f 
appearing in PF, define projection H} =  {ci:f (c 1, "'"  ,ck) ~  PF}  to be the set of patterns appear- 
ing as the  i ~h parameter of some f-pattern in PF. Chase made the crucial observation that the basic 
Bottom-Up Step (1) could be rewritten equivalently as 

(2)  M S ( f ( t  1,  . . . ,   tk)) =  {f(cl,  * '" ,ck) E  PF  I cl ~  MS(tl) n  FI}, i  =  1 .. k}  u {v} 
and  that  the  size  o f  the  finite  function  (i.e., number  of pairs  stored  in  its  graph  representation) 
defined by the rule Of(MS(t1) c~ FI),  . . . ,   MS(tk) ch II~) = M S ~ ( t l ,   - " ,   tk)) must be no greater 
than Hoffmann  and  O'Donnell's transition function "c/.  The  essential idea may be  simply put:  for 

any two  finite functions f  and g  where f i s   defined by the rule f(h(x))  = g(x),  we know that  It  <  Igl 
as long as h is not one-to-one.  Chase also provided extensive empirical evidence to show that Of is 
much smaller than "of in practice. 

Chase's  bottom-up  step  involves  two  substeps.  First each  Hoffmann  and  O'Donnell  match 
set  MS(ti)  is  turned  into  the  smaller  Chase  match  set  lzil(MS(ti))=MS(ti)n  H~) for  i  =  1 ....  k. 
Next,  Chase's  transition  function  0f  is  used  to  obtain  the  Hoffmann  and  O'Donnell  match  set 

O/(Ix)(MS (tl)) .....  ~t~(MS (tk))).  Chase's  implementation uses  integer encodings for both kinds of 
match sets. 

We  will give an abstract algorithm that incrementally constructs functions  IX and  0  and  runs 
asymptotically faster than  Chase's  algorithm.  Since our algorithm  is specified in terms of set and 
map  operations, it is useful to  discuss some  notations  and  implementation details.  In addition to 

standard mathematical notations it will sometimes be convenient to use certain unconventional dic- 
tions.  Expression A with  x  abbreviates set element  addition A  u {x}  (where  in  this  context A  is 
interpreted  as the  empty  set if it is undefined),  and  assignment A  op := x  abbreviates A  := A  op x. 
If f  is a binary relation, then domain f =   {x: Ix,y] c  f},  range f =   {y:  Ix, y] e  f}, f(x)  denotes func- 
tion  application (undefined  if f  is  multivalued  at  x  or  if x  ~/  domain  JO, and f{x}  denotes  mul- 
tivalued map application with value  {y: Ix,y]  ~ f}. 

By  a  Set  Encoding  Structure  (abbr.  SE-Structure)  we  mean  a  triple  (U,  A,  Q)  with  finite 
universe  U, primary  set A  c  2 u,  and  secondary  set  Q  c U,  SE-structures  support  the  following 

five operations: 

1.  (create)  Add a new set {z} to A, and possibly add z to Q, where z  ~  U, i.e., 

A with:=  {z} 

Q with:= z 

2.  (replace)  Replace a  e  A by new set a with z, which is denoted by, 

a  with:= z 
3.  (add) Add new set a  with z to A, and perhaps add z to Q, where a  e  A and z  e U; that is, 

A with:= a  with z 
4.  (query)  Retrieve set a  n Q, where a  e A. 

76 
5.  (index)  Retrieve set  {a e  A  I  c  e a}, where c  E U. 

We  will implement  SE-stmctures using the  following data  structure  called an SE-Tree  (see 
Fig.  t).  Each set ax belonging to primary set A  is associated with a unique node x  in the SE-tree; 

that is, x  'encodes' ax. Each node x in the tree will be uniquely associated with a set ax ~  U, which 
may  or may  not  belong to A.  If ax  and  ay  are sets associated with tree nodes x  and y,  then x  is a 
descendent of y  in the tree only if ay c a x .   Each node is implemented by a record with  five fields: 

a  right sybling pointer,  a leftmost child pointer,  a  Q-list pointer to  a  subset of Q,  a  pointer to  the 
nearest  ancestor  with  a  nonempty  Q-list,  and  a  membership  bit  indicating  whether  the  node 

corresponds to  a  set belonging to A  or not.  For each node x,  which  represents  set ax,  Q-lists for 
nodes along the path from x to the root are mutually disjoint, and their union  stores the  set ax n  Q. 

Sets  U  and  Q  are  implemented  by  a  list  of  records.  The  record  corresponding to  each  element 
c  e  U has a bit indicating membership in Q  and a pointer to a list (called the c-list) of tree nodes x 
closest to the root such that the associated set ax contains c. 

A-  right  left  Q-list member  sibling  child  ancestor  Q-  numeric list  code  U  Q  U-list 

/  c  0/1 /  / 

Fig.1.  SE-structure(U, A, Q) 
The  create  operation (A with:=  {z};  Q with:=  z) is  implemented  by  adding  a  new  tree  root 
with empty sybling, child, and Q-list ancestor pointers, membership bit on, and Q-list containing z 

if it belongs to Q  and empty if not.  We also add a pointer to the newly created record in the z-list. 
This operation takes O(1) time. 

Implementation of replace a  with:=  z requires two  cases to be considered.  In the  first case, 
called a  nondestructive  replace, the tree node x associated with a  has a nonnuU child pointer.  Then 

(i) unset the membership bit in x and create a new tree node y as a child of x, (ii) if the Q-list in x is 
nonempty, then make the Q-list ancestor in y point to x; otherwise, make it point to the same record 
that the Q-list ancestor in x points to, and (iii) set the member bit in y.  In the second case, where x 
has  no  children,  we  reuse  x  to  represent  the  new  set  a  with  z.  In  this  case,  called  a 

destructive  replace,  we  assume that nodes x  and y  are the  same.  In either case, if z belongs  to  Q, 
add z to the Q-list fory.  Finally, add y to the z-list.  This operation takes O(1) time. 

To implement addA  with:= a  with z we letx be the tree node associated with set a.  Create a 
new  tree  node  y  associated  with  set  a  with  z,  and  make  y  a child  of x.  If  the  Q-list  in  x  is 

nonempty, then make the Q-list ancestor in y point to x; otherwise, make it point to the same record 
that the Q-list ancestor in x points to, and set the member bit in y. Ifz belongs to Q,  add z to the Q- 
list fory.  Finally, add y to the z-list.  This operation takes O(1) time. 

Operation  query  a  c~ Q  is  implemented  as  follows.  If x  is  the  tree  node  associated with  a, 
then  retrieve the elements in each Q-list along the path starting from x  following Q-list ancestors. 

77 
The Q-lists along this path are disjoint.  This operation takes O(la n  QI) time. 

Finally, SE-trees support a straightforward implementation of index  {a  ~  A  I  c  E a}.  Form a 
list of records x, where set ax belongs to A, occurring in subtrees rooted in nodes  contained in the 

c-list. This operation takes O(l{a e  A Ic  e  a}l) time, because the number of nodes x  in these sub- 
trees  sucla that  ax  4  A must  be  strictly less  than  the  number  of leaves in  these  subtrees  (and  all 
leaves represent sets belonging to A). 

In order to  analyze the complexity of SE-trees, we  give the  following definitions.  For each 
node x in an SE-tree, define path (x) to be the set of nodes in the tree path from the root to x. Define 
weight(x)  to  be  the  number  of  elements  u ~   U  whose  u-list  contains  x.  Define 
Wn(A) =  ~,  weight (x) to be the total weight of all the nodes in the tree that implements set 

.x is a  tree node A.  Letting  des(x)  denote  the  number  of  tree  descendents  of  x,  we  can  define 

Wp(A) =  ~.  des(x)*weight(x) to  be  the  sum  of the  weights  of every tree  path.  Clearly, 

x  is a  tree  node IA  1  <Wn(A)  <Wp(A) <  2  ~  l  a I.  Usually, Wn(A) is much smaller than Wp(A). 

a~A 
The: total space required by an SE-tree is O (Wn(A)), and  any sequence  of h  of the  first three 
operations above takes O(h) time and space. Note that  a naive representation of the set A will take 

0  (Wp(A)) space. 

We will consider useful variants of SE-structures that  require minor alteration to the preced- 
ing implementation and do not affect the stated complexities.  A Simple SE-structure is one with no 
secondary set.  A  numeric  SE-structure  is one  in  which the  set elements  of the  primary  set A  are 
identified! by natural numbers  1 ..... IAI (cf Fig. 1). Numeric SE-structures have special importance in 
connection with our second abstract datatype described next. 

The main abstract datatype used in our pattem matching algorithm is the SE-Map,  which is a 
partial' function f: A---~B from  a domain set A  to a range set B,  where A  and B  are the primary sets 
of two SE-structures. SE-maps support the following two map operations: 

1.  (modify  range) Given a  set A  and  an element z, where A  ~ A, and  z does  not belong  to 

any set in B, add z to f  (x) for each x belonging to A.  This operation is denoted by, 

(for x  ~ A) 

f(x) with:= z 
end 
2.  (modify  domain) Given a set x in the domain o f f   and  an element z, form  a  new  domain 

set x with z and map it under f t o   the old image f  (x).  This operation is denoted by, 

J(x with z) := fix) 

Our basic implementation of SE-maps f: A ~ B   uses  SE-tree implementations for A  and B  as 
described above.  In  addition,  for each pair  [a  ,b]  belonging to  map f,  if x  and y  are the  records 
associated with sets a  and b, then x  stores  a pointer to y,  and y  stores the  size of the preimage set 
f-1 {b}.  If A is part of a numeric SE-structure, it is sometimes useful to implement domain f a s   an 

array accessed using the numeric  code of an A  element as shown  in Fig.2.  We  also make use  of a 
multi--dimensional  SE-map  in  which  the  domain  is  the  cartesian product  of primary  sets  of  SE- 

structures. 

To implement modify  range, (for x  ~ A) f(x) with:= z end,  we search through records associ- 
ated with elements of A, and handle these elements according to three different Cases.  (1)  If there 
are elements of A not belonging to the domain off, we augment B with a new set {z} using a create 

78 
operation and  add the appropriate record pointers and counts. 

domain f 

array 
i is numeric code forx e  A 

preimage range f  count  B-fields 
Y  I I/'-~ (y}l 
Fig.2.  SE-map f: A  --+ B 
(2)  For each  range  element y  ~ ~A]  whose preimage  is  entirely contained  in A,  we  simply  add  z 
destructively to y using a replace operation.  Nothing more is necessary, since B is modified impli- 
citly. (3)  For each element  y  e f[A] not handled in case (2),  we execute an add operation B with:= 
y  with  z,  relink  each element  in A n f   -1 {y} to the new  set y  with  z, and modify preimage  counts. 

The total cost of this operation is O(IAI). 

The implementation  of modify  domain f(x with  z) := f(x)  depends  on whether or not set x  is 
modified  destructively  using  a  replace  operation  to  obtain  the  new  set x  with  z.  If x  is  modified 
destructively,  then the  implementation  is  vacuous,  since  all  operations including  the  modification 
to A are implicit.  However, ifx is modified nondestructively, then we need to link the new domain 

element x with  z to the old range element f(x)  and  increment the preimage  reference  count,  which 
takes O(1) time, 

By an easy counting  argument using  the  preceding  analysis,  we obtain  the  following result, 
which is central to the analysis of our incremental preprocessing algorithm. 

LEMMA 1.  Any  sequence of intermixed  modify  range  and modify  domain  operations  takes 
0  (Wn(A)+Wn(B)) time and space, where A  and B are at theirfinal values. 

Let F  be the set of function symbols appearing in PF. For each function f e   F, let A (f)  be its 
arity.  Let  F  be  the  set  of Hoffmann  and  O'Donnen  match  sets.  From  the  above  discussion,  we 
know that the following equations hold: 

F =   {{v,  s  }  :  s  e  P  F  I  s is  a  leaf}  u  u  /  { range O  f  :  f  E  F I A ( f ) > 0 }  
H~=  {ci:  f (c 1,  "" , ck )   c  PF} 
Ix}=  {[m, m  nI-llf]:m  ~  F} 0s= 

{[tm, . . . . .   re, l,  m]: ml  ~  range IX},  " . ' ,   mk  e range  IX} } 
where m  = {f(cl,  "'"  ,ck) e  e F   Icie  mi,i=l ..... k}  u  {v} 

Because the preceding equations contain a cyclic dependency in which F  depends on both PF and 
0,  IX depends  on F,  and 0 depends  on IX and PF, it would seem that a costly fixed  point iteration  is 
needed  to  maintain  these  equations  when  PF  is  modified.  Fortunately,  this  can  be  avoided  with 

careful scheduling. 

The  algorithm  also  depends  on  a  careful  logical  organization  of the  data  into  SE-structures 
and  SE-maps.  Let  C} represent  Chase  match  sets  f o r f e   F  and  i=1 .....  A(.t). Then (PF,  F, P) is  a 
numeric  SE-structure  and  (PF,  C},  .)  are Simple numeric SE-structures  for f  ~  F and  i =1 ..... A(f). 
We  also  have  the  following  SE-maps:  IX}:  F--+C}  for  f  E F,  i=1 ..... A(3'),  and  0]: 

C}x...*C}(r)--~  F  for fEF.  Fig.3  describes  the  data  structures  used  to  access  the  main  SE- 

79 
structures 

and SE-maps shown in FigA. 

<  IPFI  :" F  arity  H  Ix  0  HI 

- I   k  f  . . , . .   k  \  i 
/  '  ~  "--------'-~ domain  IXi ~  *  " ~'FI~ 

|  ~  array of bit vectors numeric  ~  (j is numeric code for q  ~ PF) 
\  F  code  children  P  PF-list  *  ~  <  k  :" 

i 

q  ~  j  0/I  ....  PF  |  [ .-7  /  I  ,FI  / . -   J 

. . > < - - - - - - - - "   array of range gy-lists c /   q-list  . ~ /   q-list 
F  range  IX~ 

Fig.3,  Core data structure 
It is useful to explain our incremental algorithm in terms of three cases. 
(case  1)  Assume~  first of all,  that  the  set  of patterns P  is  set to  empty.  It  is  also  convenient  to 

assume that pattern forest PF always contains v.  Then we can initialize variables F, I7, Ix, and 0  as 
follows: 

PF := {v} 

r:={{v}) :~:= {I 

I~:= l} o~ := (vl 
Next,  suppose that P  is augmented  by a  new pattem p.  In order to reestablish PF, we add to 
PF those  subpattems  of p  not  already in  PF  in  an  innermost-to-outermost order.  Because  of the 

order in  which  updates  are  scheduled,  we  know  that  immediately before  a  subpattem  q  of p  is 

added to PF, either q is a leaf or all the subpattems of q except for q itself are already in PF.  More 
importantly we know that q is not the subpattem of any other pattern belonging to PF. 

(case 2)  Suppose PF is augmented with a constant symbol c.  In this case, we can maintain the sys- 
tem of equations by executing the following code just before the modification PF with:= c: 

Fwith:=  {v} with c 
COMMENT:  Perform a modify  domain operation on [tt~ 

(for [j,f ,m ]  ~ Ix{{v}}) 

~t~({v}with c) := ~t~({v}) 
,end 
O~ := {v,c} 
TO implement the  loop  efficiently, for each  match  set m  ~  F we  maintain  a  single doubly linked 

list threading each occurrence of m within domain  Ix} f o rf  ~  F,i =1 ..... A ff). 

80 
(case  3)  The  third  and  more  difficult  case  to  consider  is  when  PF  is  augmented  with  pattern 
f  (tl,  *  *  * ,tk), where  k>0.  Below we describe how to propagate modifications  to each of the  vari- 

ables  F,  H,  Ix, and  0  separately.  Recall  that each  of the  sets  F  and  range  It}, f  e F,  i  =  1..A (f), 
will be implemented as SE-trees. 

t k 

numeric SE-tree(PF,  F, P) 

domain  Ixf  t 
i  .... 

<  I1-1 

array of SE-maps 

0: 
/ 
k-dimentional SE-map (ith coordinate 
accessed using numeric code from range  ~t}) 

*  / / ~ / ~ h a s e   Codes 

numeric simple SE-tree(PF,  range  g),. 
Fig.4.  Data structure for Of and ~t~ 
1.  Modify Hf before the modification PF with:= f  (t 1,  "  ' *, tk): 

(for j  = 1,.., to) 

if tj *  l-I} then 

H~ with:: t  i 
end 
end 
2.  Perform  a  modify  range  operation  on  IX~ immediately  prior  to  the  modification  Yl~ 

with:= tj of step  1: 

(form ~  F  I tie  ra) 

~t~m) with:= tj 
end 
As  discussed  in  SE-tree  operation  5,  we  can  use  the  tj-list  to  retrieve  the  set 

{m  e  F  t tj ~  m}. 
3.  Perform  a  modify  domain  operation  on  Of prior  to  the  modification  txj.(m)  with:=  tj  of 

step 2 if the modify  range  in step 2 was nondestructive: 

(for [m 1,  "-  ,mj .....  ink]  e domain Of I  m  s  =/.t~(rn)  ) 

0/(rnl,  -..,  rnj with tj,  ...,  rot)  := 0/.(ral,  ..-,  m/,  ---,  ink) 
end 
Here 0f(ml,  " " ,   mj with  tj,  . . . ,   mD  = Of(m 1,  " "",  m j,  . . . ,   mk), because the pattern 
f  (t l,  " " ,   tk)  has  not  yet  been  added  to  PF,  and  so  no  f-pattern  in  PF  has  ty  as  its 

jth  child.  We  can  speedup  the  search  by  using  the  index  {[mi,[ml,  . . . .   mk]]: 

[ml . . . . .   mk]  e  Of}.  Maintaining  all k such indexes  for f  along with Of does not change 
the overall asymptotic time or space. 

81 
4.  Perform  a  modify  range  operation  on  Of  just  before  the  modification  P F  

with:=f  (tt,  "'"  ,tk) and  after the preceding three steps: 

(forml  ~  range  ~} . . . . .   rn  k  ~ range  ~t~ I  t  1  ~  m  l .. ...   tk~mk) 

If [rnl . . . . .   mk] ~ domain 0c then 01(rnl . . . . .   rak)  := {v} 

end 

Oi(m  1 ... ..   rn D wlth:= f  (t 1  , ... ,tk) 
end 

It  is  important  to  observe  that  range  Ix.t} is  nonempty  for  i=1 ..... k  because  of  steps  1,  2, 

and  3.  Again,  we  can  use  the  tj-list  to  search through  the  sets  {mj  ~  range  IXJ~I tj  ~  mj} 
instead of the potentially much  larger sets range  ~} ,j =1 .... k. However,  this  step contains 

a  new  operation  to create a  k-tuple  [m 1 . . . . .   mk]  and locate  it in the  domain of Of.  Hash- 
ing  is  a  practical  solution  that  preserves  the  Lemma  1  space  complexity  but  makes  the 
time  randomized.  This  would  also  make  the  Bottom-Up  Step  O(1)  randomized  time. 

Our  current  implementation  uses  this  approach.  Another  way  of  preserving  space  com- 
plexity  at the  expense  of time  is  to  use  a  balanced  search  tree;  e.g.,  a  red/black tree  [23]. 

Access  time  is  then  O(Iog  Idomain  0fl),  and  so  is  the  Bottom-Up  Step.  Like  Chase  we 
can  also  use  a  large table  to  store  Of,  which  doubles  its  size  and  reorganizes  whenever  it 
overflows.  If each  new  array is  allocated  in unit  time  using  the  solution  to  exercise  2.12 
of Aho,  Hopcroft,  and  Ullman's  book[l],  then  the Lemma   1 time  complexity  is preserved, 
but the  run-time  space requirements  for Of are increased  to be the  same  as  Chase. 
5.  Modify  F  prior  to  the  modification  0y(ml . . . . .   mk)  wi t h: = f  (t l,  *  * *, tk)  of  step  4  if the 

modify  range  operation of step 4  was nondestructive: 

F with:= 0/(m 1,  "",  m~) with f  (t 1,  '",  tk ) 
Since f  (t 1,  "  " ",tk)  is  a  new  subpattem,  no  other  subpattem  in  P F  has f  (t 1,  "  " ",tk)  as  a 
subpattem.  Thus  no  further modification is needed  for FI. 
6.  Perform  a  modify  domain  operation  on  Ix  just  before  the  modification  F 

with:=  0f(ml,  " " ,   m/c) with f  (t l,  "'"  ,tk) of step 5: 

(for [j,g,m] ~  ~t{0/(ml, ..-,  mk}) 

1-tJ(0/(ml,  " ' ,   ink) with f  ( tl ,. ..  ,tk)  ) := }.t~(0flrn~, ---, ink)) end 

Observe  that  within  the  preceding  code  ~tJ(0~ml,  - . -,   mk)  with  f (t l,'",t k) )  = 
IxJ(0~m 1,  " " ,   ink)), b e c a us e f (t l ,   . . .   ,tk) d  I7~.  The  implementation  is the same  as in 
case 2. 

Now  we  compare  the time  and  space complexity  of Chase's algorithm  and  our algorithm. 
THEOREM  1. 

1.  For each  m  ~ domain  IX}, where f  c F, j  =1..4  ff),  Chase's  algorithm  computes  Ix}(m) in 
~(min(  I m t ,   I FI}I )) time,  whereas  our algorithm  takes  O  (  I g}(m) I) time. 

2.  For  each  [m l,  " ' " ,   rnk]  in  domain  Of,  Chase's  algorithm  computes  0f(ml,  " " ,   rnk)  in 
[2(min( IPF  I, I  m  1  x  *  *  *  x rn k  I )) time,  whereas  our algorithm takes  O  (  I 0f(m 1,  '  " ",  rnk) I  ) time. 

3.  We  use  O (Wn(l"))  auxiliary  space  to  represent  the  set  F,  whereas  Chase  uses  f2(Wp(I-)) 
space. 

4.  To  represent  the  range  of  ~tJ~, we  use  O(Wn(range  IX~)) auxiliary  space,  whereas  Chase 
uses  f2(Wp(range  IX})) space. 

82 
Proof  Sketch)  In both  algorithms,  the time  complexity  is dominated  by the  time  needed  to 
construct  the tables  Ix} and  0/,  where f E   F  and j  =I..A (f). 

1.  For  each  m  E domain  ~},  Chase's  algorithm  computes  tt~(m)  by  intersecting  m  and  rl}, 
and thus  takes f~(min( I  m I,  I H~I))  time.  By Lemma  1, we spend  O (I lab(m) I) time to establish the 
value of Ix~m). 

2.  For  each  [m l,  " " ,   mk]  in  domain  0f,  Chase's  algorithm  computes  0/(m l,  ""  ',   mk)  by 
evaluating  the  set  {f  (c l,  *  " ",  cD  ~  PF  I [c 1,  "  " ",  ck] ~  m  i  *  "  "  "  * mk } naively  and  thus  takes 
f~(min( I PF   I, I  m  1  *  ""  '  * mk I)) time.  In our algorithm,  the  initial value  Of(m 1,  "  " ",  mk)  is  {  V  } 
by  default.  Then  it  gets  new  values  in  step  3  by  copying,  and  increases  one  element  at  a  time  in 
step  4.  Thus  we  spend  O (10/(m l,  "  ",  mk) l)  time  to  establish  the  value  of  0f(mt,  * " ,   ink). 
Usually 0f(m 1,  "  " ",ink)  is much  smaller than either P F  or m  1  *  ""  "  * ink. 

3 and 4. Follows from Lemma  1. 
We  briefly mention  that deleting  patterns  from  P  can  be handled  much  like  pattern  addition, 
except  that  scheduling  pattern  deletion  from  PF  is  in  an  outermost-to-innermost  subexpression 
order.  Further,  a  pattern  is deleted  from  PF  only  if its parent  is not  in PF.  The  deletion  algorithm 
follows  the  same  logic  as  the  addition  algorithm  but  in  a  backwards  order  to  undo  the  effect  of 

addition.  Details will be provided in a fuller version of the paper. 

4.  Space/Time  tradeoff 

In Chase's  algorithm,  for each  function  symbol f  e  F of arity k, the  space  required  for the  Of 
table  could  be  f~(2lk).  Here  we  give  a  method  that  decomposes  0f  into p  tables  with  worst  case 
overall space O (p (21k/P)) but leads to time O (p) to solve the Basic Bottom-Up  Step. 

Let P F  be partitioned  into p  disjoint equal  size sets PF 1 ..... PFp,  and consider equations, 

1-l),j =  (ci: f  (Cl,'' ',C k)  ~  PFj} 

i  _  C~ H},j]: m  e F} ~y ,j -   {Ira,  m 

0/,j  =  {[[ml . . . . .   mk],  m]:  ml  e  range  IX),j,  . . . ,   m~  ~ range  IX~.j } 

where  m  = {f (c l ,   "'"  ,ck) e  PFj  IciE  mi,i=l  ..... k}  u  {v} 

If MSi  =  O:,i(g~,i(MS  (tl))  .....  Ix~,i(MS (tk))),  then  we  can  compute  disjoint unions  MS  ~  (tl ..... tk)) 
=MS1  u  .  .  .  u MSp  in O(p)  time. 

Consider the  space  required by  this  approach.  If r},j  =  I range  IX} j  I, then  riy,j =  O (2 ~  n k.:J) = 
O (2 IPF:I )  =  O (2t/p),  and  10:,jl  =  O(rle,j*...*r~,j)  =  0 (2tk/P).  Thus,  the  total  space  storing  the p 
match  tables  for  function  symbol  f  is  0  (p (2tk/P)),  which  for p  >  1 is  asymptotically  better  than 

Chase's  algorithm  in the  worst case.  i 

The  space  required  by  each  Ix table  is  always  I  F I.  Thus  the  total  space  for  the  tables  Ixf,j, 
i =l..k,  j  = 1..p  is  now  pk  I  r I,  and  the  total  space  for  the  Ix and  0  tables  for  function  symbol f  is lk 

O(pk  IFi+p(2lk/e)).  When  P  = log(k IF t ) '  we  obtain  the  approximate  minimum 

tk 2 
O (i o g( kl F i )   IFI). 

To  further  reduce  the  size  of  Ix tables,  we  can  split  each  IX},j table  into  p  subtables  IX~j, i 
t=l..p,  with  domain  g,i::j  =  { xnP Ft:xeF }.  Then  for  xe   F,  we  have  I.t/,j(x)  = 

il Ix].j(x  n  PF1)  u  "'"  L) Ix~]Pj(x  n PFp)},  which can be computed  i np  time.  This  increases the  time 

per step to  O (p2). 

83 
The total  size  of the p  subtables  is  now bounded  by  O (p2t/P),  and  the  total  space  for the  l.t 
and  0 tables  for function f i s   O(kp22 t/p +p21~p).  Since  this  approach is meaningful only for step 
time complexities better than 0  (l), i.e., p  =  0 (ll/2), the best upper bound we can get in this case is 

0(ll/22ckY2)  for  some  constant  c.  This  result  also  indicates  that  this  approach  is  useful  only 
when  IFI  >> 2/1/2. 

In a  practical  implementation  it  is  not necessary for PF to  be  partitioned  into  disjoint  equal 
size subsets.  For example,  we can let PF 1 be the  set of patterns  that are not children  of any pat- 
tern,  PF i  be  the  set  of  children  of  patterns  in  PFI_ t  not  contained  in  PFj,  where 

i  = 1..maximum  height  of patterns, j  <  i. Then the tables  I.t,~) can be omitted for t  > j - l .   Alterna- 
tively,  we can let PF  i  be the  set of all  children  of patterns  in PFi_  1  . Now the  size of each subset 
may  gi~ow, but  the  tables  Ix~,tj can be  omitted  for all  t *:j-1.  It is  an  interesting  question  how to 

find  a partition of PF that minimizes the table  size for a  fixed per step time bound. 

5.  Match  set elimination 

Hoffmann  and  O'Donnell  [10]  considered  two  subclasses  of patterns  for  which  the  prepro- 
cessing and space costs for bottom-up multi-pattern matching are greatly reduced. 

Definition:  A  set P  of patterns  is Simple  if for every two distinct  patterns p,  q  * PF, either 
(1) p  < q, (2) q  < p, or (3) ~] subject t  I  t  <  q and t  < p. 

For  Simple  Patterns  P  Hoffmann  and  O'Donnell  observed  that  the  partial  ordering  (PF,  <) 
could be represented  by a directed tree (called a subsumption tree)  with v at the root (assuming that 

v occurs in P).  Each match set equals  the  set of patterns  along some path  in the  subsumption  tree 
from  a  node to  the  root.  And every path  from  a  node to  the  root detemaines  a  match  set.  Thus, 
there  are only l  match  sets,  and  each one  can be represented  by its minimum  pattern.  For a  func- 
tion f o r   arity k, the transition table Zy uses O (l k) space,  a great improvement over the general case 
but  still  expensive.  Hoffmann  and  O'Donnell  also  argue  that  most  sets  of  patterns  they  have 
encountered in rewriting systems are Simple or can be turned into equivalent Simple sets. 

Hoffmann  and  O'Donnell  also  looked  at  a  subclass  of binary  Simple  Patterns;  i.e.,  Simple 
Patterns  in which the maximum arity of any function is two.  Although greatly restricted,  this class 

is interesting,  because conventional  arithmetic  and operations in combinatory logic have  afity less 
than or equal to two.  Also, Hoffmann and  O'Donnell showed that naive transformation of patterns 

with arity greater than two into binary form sometimes but not always preserves the Simple Pattern 
property.  For  binary  Simple  Patterns  they  gave  an  algorithm  requiring  no  transition  tables,  but 
uses  O (/2)  space,  O (/h 2)  preprocessing  time  (h is  the  longest  path  in  the  subsumption  tree),  and 

O (h 2) time instead  of an O (1) time for Step (1). 

We will give a bottom-up algorithm  for binary  Simple Patterns  (which extends  to a subclass 
of Simple  Pattems  with  arbitrary  arity)  with  O (1) space  and  O (log  l)  time per  step.  Our Prepro- 
cessing time  is the  same  as that of Hoffmann and  O'Donnell.  The  algorithm makes use of persis- 
tant search trees  [20], and we expect it to be fast in practice. 

Let PF be the pattern forest for the set P  of pattems,  and let T be its  subsumption tree.  Recall 
that  for Simple  Patterns  each match  set can be  represented  by the unique minimum  pattern  in the 
set.  Ifpl  represents  the  match set  for subpattern  ti  of the  subject,  i  =  1  ..  k, then  the match set for 
f  (tl,  *  * ",  tk) is represented by the pattern determined by the following formula: 

(New Bottom-Up Step): 

(3)  m i n / ( { v } u { f ( q l ,   " " , q k ) * P F   l qi >p l ,  i = l . . k } )  
We call pattern f  Co l,  *  * *, Pk) the search argument for Step (3). 

84 
Consider any binary function f  appearing in PF, and let f  (p 1, P2) be the search argument for 
Step (3).  (We will not discuss unary patterns  and constants, which are simpler subcases.)  We want 
to analyze (i) the  worst case cost of performing Step (3);  and (ii)  the  auxiliary  space while  execut- 
ing Step (3). 

An important observation is that,  unlike  patterns P  l and p2,  search argument f  (P l, P2) may 
not belong to the subsumption tree T!  Consequently, if we let  1 denote the unique maximum pat- 

tern,  and if we define relation R  =  {Ix, y]:f(x,  y) ~  PF} u  {[1,1]}, then  we can replace  Step (3) 

for search argument f  (p 1, P  2) more conveniently by, 

(4)  rain/  {[x, y]  e  R  I  x >pl  andy  >p2} 
Expression  (4) can be  computed  by locating  the  pair  of nearest  ancestors belonging  to R  of 
nodes p  1 and P2  with respect to subsumption tree T.  This  characterization  is meaningful because 
of the following proposition. 

Proposition:  If/x l, y 1] and [x2,  Y2] are any two pairs in R and X  l  < x2, then y  2  c  Y l. 

Proof  Otherwise,  P  would  not be  Simple;  i.e.,  we  would  have f ( x l ,   Y2)  < f ( x l ,   Yl)  and f(X l, 

Y2) < f   (x2, Y2). 

In order to  compute  (4)  efficiently,  the  difficulties  of two  dimensional  ancestor testing  and 
searching  within  partially  ordered  sets  need  to  be  overcome.  This  is  done  by  reducing  the  two 

dimensional  nearest  ancestor  search  in  tree  T  to  single  dimensional  searching  through  a  totally 
ordered set.  The essential idea is presented just below. 

Let R {x} denote the set  {y: Ix, y] ~  R}, and let  domain  R denote the set  {x :[x,y ]z  R}.  For 
each xe  domain R, define set S (x) =  uyz~R {y}; for each z  ~  S (x) define witness 

w  (x, z) = minimum y  _  x such that [y,  z  ]  ~  R 
Then we can compute (4) by performing these two queries: 

(5)  i.  qo=mln  /{x ~  domainR  I x>-pl} ii.  q2 = m i n / { y   ~  S(qo)I  y  >P2} 

If either qo or q2  equals  1, then v is the answer; otherwise, we obtain f  (w (q o,  q2),  q2)- 

The  two  queries  (5)  reduce computation  (4)  to  finding  single  dimensional  nearest  ancestors 
and  computing and storing sets S (x). Nearest  ancestors in trees  can be computed efficiently based 
on the following idea. Letpre (i) and des (i) be the preorder number and descendent count of node i 
in tree  T.  Then node i is an ancestor of node j  iffpre (i) <- pre (i) < pre (i) +  des (i);  also, if i and k 
are both ancestors of j, then i is nearer to j  than  k iffpre (i) > pre (k). 

Let Q be any subset of the nodes in T. Then for any node p  in T, we can compute 

(6)  min/{x ~  Q  I  x > p }  
whenever  a  solution  exists  by  finding  the  node  i  in  Q  with  maximum  pre(i)  such  that 
pre (i)<pre(p)  < pre (i)+ des (i). To  facilitate  this  computation  we can  preprocess  Q  as  follows. 

For all  i  in Q  define  function find(pre (i))= i and find(pre (i)+ des (i))= j  such that pre 0')  is  the 
maximum  for  which  pre(j)'<_pre(i)+ des(i)<pre(j)+des(j)  and  j  ~ Q.  Hence,  (6)  can  be 

solved by computing find(x), where x is the greatest element in domain find such that x  <_pre (p). 

We  can  store  domain  find  as either  a  red/black  tree  [8,23]  or  Willard's  variant  of the  Van 
Emde  Boas  priority  queue/24, 25]  and  obtain  the  following  time/space  bounds.  Both  data  struc- 
tures  use  space  O ( I Q I ) .   Computing  query  (6)  costs  O(loglQ  1)  with  red/black  trees,  and 

O (loglogl) with priority queues (where l is the number of nodes in 7"). 

85 
Based on the preceding analysis,  we can perform query (5),  (i)  with O (l)  space overall  if we 
store all of the domains of relations R for each binary function f  appearing in T either as red/black 
trees or Van Erode Boas priority  queues.  Query time  is  O (fog/) using  red/black  trees,  O (loglogl) 

with priority queues. 

For query (5), (ii) we can store all  of the sets S(qo) and their witnesses using a minor variant 
of the persistent  search tree of Samak and Tarjan  [20].  Recall that a persistent search tree can store 

a  sequence  To,  T1,  * "",  Tr  of sets,  where  To  is  empty  and  Ti  is  formed  from  Ti-1  by  element 
addition  or deletion  for i  =  1,  *  * *,  r.  The data  structure takes  up  O (r)  space and can support the 
nearest neighbor operationpred(i,  x) = max  /  {y  ~ Ti  I  y  < x}  in O (tog r) worst case time. 

In our  application  the  sequence  of sets  is  obtained  by  traversing  the  subsumption  tree  T  in 
preorder,  adding R{x}  as we arrive  at node x from its  parent,  and  deleting  R{x}  when we go back 
from  x  to  its  parent.  Hence,  the  sets  S(x)  for x  in  domain  R  are  included  as  a  subsequence. 
Witnesses  are stored using stacks inside  the  search tree.  Since each set R{x}  is  added  and  deleted 

once in forming the  sequence, the  size r  of our sequence is just  1R I, which is  also  the number  of 
distinct  patterns  with  root f  appearing  in  PF.  Thus,  query  (5),  (ii)  can  be  computed  in  O (fog/) 
time,  and  the  cumulative  space  for  storing  persistent  search  trees  for  all  the  binary  functions  f 

appearing in PF is just O (1).  Thus, we have 

THEOREM 2.  Step  (3) can be computed for  binary  Simple Patterns in  0 (log/)  time  and 0  (l) space. 

Extending the preceding idea to functions of arbitrary arity is straightforward. 

Definition:  A  k-ary function symbol f  is Very Simple  if there  exists  a  k-permutation  g  such that  for  i=1 .....  k-1 and  every  two  distinct  f  pattems  f(xl,...,x~)  and  f(Yl,...,Yk),  XSj-->Yg i  j=l,..,i 

implies xg~+ t  c Ygi+l. 

Any Very  Simple  function f  in  a  Simple  pattern  forest  can  be  handled  without  a  transition 
map.  Our algorithm  runs  in  step  time  0  (kmax log/)  and  total  auxiliary  space 0  (kmax l)  for all 

Simple  functions together, where kmax is the greatest arity of any Very Simple  function appearing 
in PF. 

6.  Conclusion 

We  believe  that  a  deeper  analysis  and  exploitation  of the  structure  of pattern  matching  can 
lead to further  algorithmic  improvements.  In a subsequent  paper we will  report how to extend the 

algorithms presented  here to a more complex pattern language, which is used  to perform semantic 

analysis within RAPTS. 

Acknowledgements  We  are  grateful  for  stimulating  discussions  about  pattern  matching  with 
David  Chase, Chris  Hoffmann, and Ken Perry.  We also thank the CAAP referees  for helpful com- ments. 

~References 

1.  Aho, A., Hoperoft, J., and Ullman, J., Design and Analysis of Computer Algorithms, Addison-Wesley, 1974. 
2.  Borsfler, J., Moncke, U., and Wilhelm. R., Table  Compression for Tree  Automata, Lehrstuhl fur Informatik IlL Universitat des Saarlandes,  1987. 

3.  Burghardt, J.,  "A Tree Pattern Matching Algorithm with Reasonable Space Requirements," in Proc.  CAAP  '88, ed. M. Daudet and M. Nivat,  Lecture Notes in Computer Science, vol. 299, pp. 1-15, Springer-Verlag, 1988. 
4.  Cai, J. and Paige, R., "'The RAPTS Transformational System -  A Proposal For Demonstration," in ESOP "90 Sys- tems Exhibition, May 1990. 

86 
5.  Chase, D,  "An improvement  to  bottom-up  tree  pattern matching," in Proc.  Fourteenth  Annual ACM Symposium on  Principles of Programming Languages, pp. 168-177,  January,  1987. 

6.  Donzeau-Gouge, V., Huet,  G.,  Kahn,  G., and Lang,  B.,  "Programming  environments  based on structured  Editors: the  Mentor Experience," in Interactive  Programming Enviromnents, ed.  D.  Barstow,  H,  Shrobe, and E.  San- 

dewall,  McGraw-Hill,  1984. 
7.  Givler,  J. and Kieburtz,  R.,  "Schema Recognition  for  Program  Transformations," in ACM Symposium  on  LISP and  Functional Programming, pp.  74-85,  Aug,  1984. 

8.  Guibas,  L.  and  Sedgewick,  R., "A dichromatic  framework for balanced  trees,"  in Proc. 19th IEEE FOCS, pp. 157-184,  1978. 
9.  Hatcher,  P. and Christopher,  T., "High-Quality Code Generation  Via Botlom-Up  Tree  Pattern  Matching," in Proceedings13th ACM Symposium on Princ~les  of Programming  Languages,  pp. 119-130,  Jan,  1986, 

10.  Hoffmann,  C. and O'Donnen, 1., "Pattern Matching in Trees," JACM, vol.  29, no. 1, pp.  68-95,  Jan,  1982. 

11.  Hoffmann,  C.  and  O'DonneU,  M.,  "Programming with Equations,"  ACM TOPLAS, vol.  4,  no. 1, pp.  83-112,  Jan., 1982. 

12.  Hudak, P., "Conception, Evolution, mad Application of Functional  Programming  Languages," ACM Computing Survey, vol.  21,  no.  3,  pp.  359-411,  Sep.  1989. 

13.  Huet,  O. and Lang, B.,  "Proving and Applying  Program  Transformations  Expressed with Second-Order  Pat- terns," Acta  lnformatica, vol.  11,  pp.  31-55,  1978. 
14.  Knuth,  D.  and  Bendix,  P., "Simple Word  Problems  in  Universal  Algebras,"  in  Computational  Problems  in Abstract  Algebra, ed.  Leech,  J., pp. 263-297,  Pergamon  Press,  1970. 

15.  Kosaraju, S., "Efficient Tree Pattern Matching," in Proc.  FOCS  '89, Oct.,  1989. 
16.  Maluszynski,  J. and Komorowski,  H.  J.,  "Unification-flee  execution of logic programs," 1EEE  Proceedings of symposium  on  logic  programming, Boston,  1985. 

17.  Pelegri-Llopart,  E.  and  Graham, S.,  "Optimal Code  Generation  for  Expression  Trees: An Application  of BURS Theory,"  in Proceedings 15th ACM Symposium  on  Principles of Programming  Languages, pp.  294-308,  Jan, 

1988. 
18.  Pfenning, F. and Elliott,  C.,  "Higher-Order  Abstract Syntax," in Proceedings S1GPLAN  '88  Conf.  on  Prog.  Lang. Design  and Implementation, pp. 199-208,  June,  1988. 

19.  Purdom, P. and Brown, C., "Fast Many-to-one  Matching Algorithm," in Proc.  RTA  '85, ed. J.-P.  Jouannaud,  Lee- ture  Notes in Computer  Science, voL 202,  pp.  407-416,  Springer-Verlag,  1985. 
20.  Samak,  N. and Tarjan,  R.,  "Planar Point  Location  Using  Persistent  Search Trees,"  CACM, vol.  29,  no.  7,  pp. 

669-679,  July,  1986. 

21.  Sethi, R., Programming  Languages:  Concepts  and Constructs, Addison-Wesley,  1989. 

22.  Standish,  T.,  Kibler, D.,  and  Neighbors,  J., "The Irvine  Program  Transformation  Catalogue,"  Univ. of Cal.  at Irvine,Dept. of Information  and  Computer  Science,  Jan,  1976. 

23.  Tarjan,  R., Data  Structures and Network Algorithms, SIAM,  1984. 
24.  Van  Erode  Boas,  P., "Preserving order in a  Forest in Less Than Logarithmic  Time  and  Linear Space," IPL, vol. 

6,  pp.  80-82,  1977. 

25.  Willard,  D.,  "Log-Logarithmic  Worst-Case  Range  Queries  are  Possible in Space O(N)," IPL, voL 17, pp.  81-89, 

1983. 