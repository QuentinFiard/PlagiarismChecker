

Semantic Types: A Fresh Look at the Ideal Model for Types

J'er^ome Vouillon Paul-Andr'e Melli`es

CNRS and Denis Diderot University{
Jerome.Vouillon,Paul-Andre.Mellies}@pps.jussieu.fr

Abstract
We present a generalization of the ideal model for recursive
polymorphic types. Types are defined as sets of terms instead of sets of elements of a semantic domain. Our proof
of the existence of types (computed by fixpoint of a typing operator) does not rely on metric properties, but on the
fact that the identity is the limit of a sequence of projection
terms. This establishes a connection with the work of Pitts
on relational properties of domains. This also suggests that
ideals are better understood as closed sets of terms defined
by orthogonality with respect to a set of contexts.

Categories and Subject Descriptors
F.3.3 [Logics and Meanings of Programs]: Studies of
Program Constructs--Type structure; F.3.2 [Logics and
Meanings of Programs]: Semantics of Programming
Languages--Operational semantics; D.3.1 [Programming
Languages]: Formal Definitions and Theory--Semantics;
D.3.2 [Programming Languages]: Language Classifications--Applicative (functional) languages

General Terms
Languages, Theory

Keywords
Recursive types, polymorphism, subtyping, realizability,
ideal model, inductive/coinductive principle

1 Introduction
The typing and subtyping rules for powerful type systems
can be rather complex and unexpected, due to interactions

Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first page.
To copy otherwise, to republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee.
POPL'04, January 14-16, 2004, Venezia, Italia.
Copyright 2004 ACM 1-58113-729-X/04/0001 ...$5.00

between the several kinds of types. The first author experienced this with XDuce [26], which combines union types
and product types. A clear semantics of subtyping appears
to be really helpful in those situations [15, 21, 27]. Ideally,
it should be set-theoretic. In the case of XDuce, subtyping
is simply defined as the inclusion of types as sets of values.
This is quite handy: union types and product types are then
directly interpreted as set-theoretic union and product.

Unfortunately, the semantic approach becomes complicated,
but hopefully still intuitive, in presence of recursive types.
Indeed, in this case, the denotation of a type cannot be defined by induction on its structure. Instead, it is generally
defined as a fixpoint of some operator. The existence of
such a fixpoint is not immediate: the operator is usually not
monotone, since function types are contravariant on the left
and covariant on the right.

MacQueen, Plotkin, and Sethi [31] proved the existence of
such a fixpoint when the denotations of types is taken to be
ideals, that is, sets of elements of a semantic domain enjoying
additional properties. This is fine, but not entirely satisfactory. First, domain theory is complex, especially if one wants
to account for non-determinism. Second, there is often a mismatch between the operational semantics of a language and
its denotational semantics, which can have an impact on the
properties of types. It seems therefore important to recast
this model inside operational semantics. Some major steps
have been taken in this direction [1, 5, 12, 13, 14]. We carry
on, and provide here an alternative perspective, halfway between the work of Krivine on realizability [16] and the work
of Pitts on logical relations [34, 35].

Our goal is to classify terms according to their types. We
expect two properties of types: first, if a term has a type,
then it evaluates without error; second, it must be possible
to build more complex typed terms by assembling smaller
typed terms according to simple (typing) rules. For instance:

Appe : !! " ! e! : !!

ee! : !

Fste : ! * !!

fste : !

Snde : ! * !!

snd e : !!
Let "(!) be the denotation of type !, that is the set of terms of
type !. Let S be the set of terms that evaluates without error.
The rules above suggest to have the following inclusions.

"(!! " !) $ {e % S |&e! % "(!!).ee! % "(!)}"(! * !!) $ {e %

S |fste % "(!) ' snd e % "(!!)}

These inclusions ensure the soundness of the typing rules
above. In order to reason about types, it is important to have
a more precise characterization of types. It seems therefore
natural to replace these inclusions by an equality. We can
then view " as a fixpoint of an operator F such that:F

(")(!! " !) = {e % S |&e! % "(!!).ee! % "(!)}F(")(! * !!) = {e %

S |fste % "(!) ' snd e % "(!!)}

This operator is neither monotone nor anti-monotone, as can
be seen on the first equation: the set F(")(!! " !) gets larger
when the set "(!!) gets smaller and when the set "(!) gets
larger. As we shall see, it turns out to be useful refine the
picture, and to define an operator F with two arguments,
monotone on its first argument and anti-monotone on its
second argument:F

("+,"-)(!! " !) = {e % S |&e! % "-(!!).ee! % "+(!)}F("+

,"-)(! * !!) = {e % S |fst e % "+(!) ' snd e % "+(!!)}

Still, the existence of " such that F(",") = " is not clear as
the Knaster-Tarski fixpoint theorem does not apply.

This article is organized as follows. We recall some lattice
theory (section 2) and provide general conditions implying
the existence of a fixpoint of the typing operator F above
(section 3). Then, we specify a type system and its denotation (section 4), from which we deduce the soundness of our
subtyping system (section 5). The closure operator involved
in these constructions is nicely expressed by orthogonality
between terms and contexts (section 6). We illustrate our
theory by a call-by-value calculus (section 7) and its typing
rules (section 8). We finish by mentioning limitations of our
approach (section 9), presenting related work (section 10),
and pointing out future directions (section 11).

2 Mathematical Background
Complete Lattice A complete lattice is a partially ordered
set such that any subset X of elements has a least upper
bound sup X and a greatest lower bound infX.

Limit We define the limit inferior and the limit superior
of a sequence (xn)n%N of elements of a complete lattice, by:liminfx

n = supn%N infn!>=nxn!
limsup xn = infn%N supn!>=nxn!

When the two limits coincide, we say that the sequence converges and we define the limit of the sequence by:limx

n = liminfxn = limsup xn
All increasing and decreasing sequences converge. The limit
operator is order preserving: if xn * yn for all n % N, thenlimxn * limyn

. The Sandwich Theorem holds: if a sequence(zn)n%

N is such that xn * zn * yn for all n % N, where (xn)n%Nand (yn)n%N are two sequences converging to a same limit,

then (zn)n%N converges to this very limit.
Closure A closure operator over a partially ordered set(X

,*) is a function which associates to each element x of X
an element x of X such that:

* x * x (extensive);* x

= x (idempotent);

* if x * x!, then x * x! (monotone).
An element x is closed when x = x. The greatest lower bound
of a family of closed elements is closed. The closed elements
of a complete lattice form themselves a complete lattice.

Galois Connection Let (X,*) and (Y,+) be two partially
ordered sets. A Galois connection is a pair of two functionsf : X " Y

and g : Y " X such that for all x % X and y % Y ,x * g(y)
iff f (x) + y. A Galois connection ( f ,g) induces a
closure operator g , f over the partially ordered set (X,*).

Polarity A binary relation R between two sets A and B
induces a Galois connection, called polarity, between the two
power sets (P(A),$) and (P(B),-). The components ( f ,g)
of this Galois connection are defined by:

f (U) = {y % B|&x % U.(x,y) % R} for U $ Ag(V ) = {x % A|&y % V

.(x,y) % R} for V $ B

3 Fixpoints: Existence and Properties
In this section, we specify some conditions ensuring the definition of the denotation "#($) of a family of types $ as a
fixpoint of an operator F. We also state some interesting
properties of this fixpoint.

3.1 Assumptions on the Calculus
The calculus is composed of a set of terms e. Before section 7, we don't actually assume that these are syntactic
terms. They could just as well be, for instance, the elements
of a semantic domain.

A closure operator associates to a set of terms E another set
of terms E. The intuition is that E is the set of terms that
cannot be distinguished from the terms E: a term in E will
evaluate without error in any context where all terms in E
would evaluate without error. A possible definition for the
closure operator is proposed in section 6.

We distinguish a set S of safe terms (which, intuitively, evaluate without error) and a set B of bottom terms (which loop).
These sets satisfy the following properties:

* S is closed;*

B is closed;*
B $ S.

There exist a family ( )n of approximation operators (n % N),
from terms to terms, with the following properties:

* if e % E, then (e)n % E;

* if (e)n % E for all n % N, then e % E;

* if e % S, then (e)0 % B.

An intuition is that (e)n behaves just like e until a nested
interaction of depth n is reached, then loops; and that a
term e is the "limit" of its approximations (e)n.

In section 7, we will fully specify a calculus and provide a
corresponding definition for the sets S and B, and for the
approximations operators ( )n.

3.2 Semantic Candidates
We now define more precisely the domain of the operator F.
We assume given a set of types $. The operator F takes as
arguments two semantic candidates and returns a semantic
candidate. A semantic candidate " is a function from types
to sets of terms which satisfies the following property:

B $ "($) $ S.
In other words, a well-typed term does not fail, and a term
that loops can be given any type.

The semantic candidates are naturally equipped with a complete lattice structure induced by the point-wise order +:"

+ "! iff &$."($) $ "!($).
The closure operator on set of types induces a closure operator on semantic candidates, defined by:"

($) = "($).

3.3 Assumptions on the Operator F
We now state some properties of the operator F ensuring
that it has a fixpoint.

We define a family of relations !n by:

E !n E! iff for all e in E we have (e)n % E!.
In other words, E !n E! iff the image of E by ( )n is included
in E!. We write "!n "! for the point-wise extension of the relation !n to semantic candidates. We state a few interesting
properties of the relations !n that we will use in section 3.4.

Remark 1 (Some properties of the relations !n).
The following properties hold:

* E !0 B if E $ S;

* if E1 $ E!1, E!1 !n E!2, and E!2 $ E2, then E1 !n E2;

* if E !n E! for all n % N, then E $ E!.
Proof. The two first properties are easy to check. We prove
the third one. Suppose that e % E. By assumption, (e)n % E!

for all n % N. So, e % E!. We conclude that E $ E!.

The operator F satisfies the three following properties:

* F is monotone on its first argument and anti-monotone

on its second argument:

if "1 + "!1 and "!2 + "2, then F("1,"2) +F("!1

,"!2);* F

preserves closure:

if " and "! are closed, then F(","!) is also
closed;* F

is "contractive":

if " and "! are closed and " !n "!, thenF("

,"!) !n+1 F("!,").

The last property plays the same technical role as contractivity in [31] without requiring a metric space.

3.4 Semantic Typing
We can now state the existence of a fixpoint. Actually, it
turns out that there exists a best fixpoint, in the sense that
it is the closure of any other fixpoint, hence the greatest
one. We will only consider this particular fixpoint "#, the
semantic typing, in the remainder of this paper.

Theorem 2 (Greatest Fixpoint). The operator F has
a closed fixpoint "#. This fixpoint is the closure of any other
fixpoint of F.

Proof. Preliminary Let "B be the least semantic candidate (which associates B to each type) and "S be the greatest
semantic candidate (which associates S to each type). We
define two sequences of semantic candidates ("-n )n%N and("+n )n%

N by: "-

0 = "B "-n+1 = F("-n ,"+n )"+0 = "

S "+n+1 = F("+n ,"-n )

The sequence ("-n )n%N is increasing and the sequence("+n )n%

N is decreasing. Besides, for all n, we have "-n + "+n .So, both sequences are converging to some semantic candidate "-# and "+# and we have the following ordering:"

-0 + ...+"-n +...+"-# +"+# +...+"+n + ...+"+0 .

Furthermore, the semantic candidates "-n and "+n are closed
for all n % N, as F preserves closure. The semantic candidate"+#

is also closed, as it is the greatest lower bound of a set
of closed semantic candidates. The semantic candidate "-#,
on the other hand, need not be closed.

Any fixpoint of F is between "-# and "+#. Indeed, we can
prove by induction that a fixpoint " satisfies "-n + " + "+n
for all n. This suggests to compare more precisely these two
semantic candidates.

A closed fixpoint candidate We are going to prove that"

+# = "-#. This implies that if there is a closed fixpoint, it

must be "+#.

First, we prove by induction that "+n !n "-n . This relation
holds for n = 0: "

+0 ($) !0 B = "-0 ($).

Let us assume that it holds for some n. Then, it holds forn + 1

as F is contractive:"

+n+1 = F("+n ,"-n ) !n+1 F("-n ,"+n ) = "-n+1.

So, we have "+# + "+n , "+n !n "-n , and "-n + "-#. Hence, "+# !n"-

#. This is true for all n % N, so "+# " "-#.

On the other hand, we have "-# + "+# and "+# is closed. Therefore, "-# + "+#.

Existence of the closed fixpoint We now check that "+#
is indeed a fixpoint, that is, "+# = F("+#,"+#).

We remark that for all semantic candidate ", if "-# + " +"+#

, then "-# + F(",") + "+#. Indeed, if "-# + " + "+#, then"-

n + " + "+n for all n and thus "-n+1 + F(",") + "+n+1 for all n.Hence,

"-# + F("+#,"+#) + "+#.

Then, as F("+#,"+#) is closed (F preserve closure),

"+# = "-# + F("+#,"+#) + "+#
as wanted.
Greatest fixpoint Let " be a fixpoint of F. We showed
in the preliminary of this proof that "-# + " + "+#. Hence,"

= "+#.

3.5 A Reasoning Principle
In order to reason about types, it is not sufficient to characterize the semantic typing "# as a fixpoint of F. It is also
important to have some kind of induction principle, which
takes advantage of the uniqueness of "#. We are going to
state such a principle. We start by remarking that "# can
also defined by iteration.

Lemma 3 (Iterations). Any sequence ("n)n%N of closed
semantic candidates such that "n+1 = F("n,"n) converges to
the semantic typing "# in the lattice of closed semantic candidates.

Proof. We consider the sequences ("-n )n%N and ("+n )n%N defined during the proof of theorem 2. Both sequences converge
to "# in the lattice of closed semantic candidates. Furthermore, for all n % N, we have "-n + "n + "+n . So, by the Sandwich theorem, the sequence ("n)n%N converges to "#.

The reasoning principle is a direct corollary of this lemma.
We say that a semantic candidate " is constant if for all
types $ and $! we have "($) = "($!).

Theorem 4 (Reasoning Principle). Let P be a predicate on closed semantic candidates such that:

* if " is constant, then " satisfies P;

* if " satisfies P, then so does F(",");

* if a sequence ("n)n%N converges in the lattice of closed

semantic candidates, and all "n satisfies P, then their
limit also satisfies P.

Then, the semantic typing "# satisfies P.

Proof. We consider the sequence ("n)n%N defined by "0 ="

B and "n+1 = F("n,"n). By lemma 3, this sequence con-verges to "#. All "n satisfies P by the two first hypotheses.

So, their limit "+# also satisfies P, by the third hypothesis.

4 Types
4.1 Syntax of Types
Types are defined in two steps. First, we define inductively
finite patterns called type patterns. Then, these patterns
are assembled coinductively [8, 19, 22] into possibly infinite
trees called types. This two-step construction rules out illdefined types, such as ! = ! .! because ! .! is not a pattern.
Indeed, any occurrence of a type in a pattern is below a
constructor " or *.

The different type constructions are standard. See section 4.3 for a precise description of their meaning. We assume given a set of type variables % and a single constant &.
Given a set of types !, we define type patterns t inductively
by the grammar below.

t ::= & constant| ! * !

pair type| ! " !
function type| %

type variable| /
top type| t 't
intersection type| &%
.t universal quantification| 0

bottom type| t .t
union type| 1%
.t existential quantification

We write t(!1,...,!k) when the pattern t has leaves !1, . . . ,!k

, where each !i occurs linearly in t. The finite patterns t
are assembled coinductively as follows:

! ::= t(!1,...,!k) coinductively.
By coinduction, every type ! is of the form t(!1,...,!k). So,
we can reason inductively on the structure of type patterns,
then coinductively on the structure of types. This turns out
to be very convenient. Another point is that all the constructions ", *, ', . . . , on type patterns lift in the obvious
way to constructions on types. This enables to write types
like !1 " !2, !1 ' !2 or &%.!.

Types are considered modulo renaming of their bound variables. This does not contradict the coinductive definition of
types on the alphabet of patterns since, in fact, %-conversion
is only a handy presentation of de Bruijn indices. Note also
that we don't assume types to be regular: types may have
an infinite number of distinct subtrees.

Because of free type variables, we cannot directly associate
a set of terms to a type !. Indeed, we need to provide an
interpretation for these variables. We use an environment'

, a function from type variables to set of terms such that
for all variable %, the set '(%) is closed and the inclusions
B $ '(%) $ S hold. Then, a "type" $ of section 3 is actually
a pair, written (!)', of a type ! and an environment '.

4.2 Language Refinements
In order to give a meaning to types, we need to refine our
definition of the language: the language has functions, pairs
and a constant. More precisely, we must specify the destructors corresponding to these constructions: the application,
the pair projections fst and snd, and a function isconst
that test whether its argument is the constant &.

So, we have an application function which associates to two
terms e and e! another term noted ee!, and three functions
fst, snd and isconst from terms to terms. We don't assume
that any of theses function is a term.

Let f be a function from terms to terms. We say that f is
strict if f (B) $ B (or, equivalently, B $ f -1(B)). We say

that f is continuous if, for all closed set of terms E, the setf -1(

E) is closed.

The functions fst, snd, and isconst are strict and continuous. If e! % S, then the function which associate to a terme

the term ee! is strict and continuous. The approximation
operators ( )n are continuous. (One can prove that they are
also strict.)

The function fst, snd and the application satisfy some "commutation" properties with respect to the approximation operators:

* if (fste)n % E, then fst (e)n+1 % E;

* if (snde)n % E, then snd (e)n+1 % E;

* if (e(e!)n)n % E, then (e)n+1 e! % E.

4.3 Definition of the Type Operator
We discuss informally what properties of the semantic typing "# we would like to have. Since most properties can
be interpreted as (reversible) elimination rules, we take the
freedom to write them as such.

* A term e has type & iff it is safe and it can safely be

applied to the function isconst:( 2 e : &

( 2 isconst e : /

* A term e has type !1 *!2 iff it is safe and it yields a term

of type !1 when applied to fst and a term of type !2
when applied to snd:( 2 e : !

1 * !2( 2
fst e : !1 ( 2 e : !1 * !2( 2 snd e : !2

* A term e has type !2 " !1 iff it is safe and it yields a

term of type !1 whenever applied to a term of type !2:( 2 e

1 : !2 " !1 ( 2 e2 : !2( 2 e1 e2 : !1

* A term e has type % in the environment ' iff it is included in '(%).*

A term e has type / iff it is safe (/ is the greatest type).*
A term e has type !1 ' !2 iff it both has types !1 and !2:( 2 e : !

1 ' !2( 2 e : !1 ( 2 e : !1 ' !2( 2 e : !2

* A term e has type &%.! in the environment ' iff it has

type ! in any environments derived from ' by binding

the variable % to a closed set of term E containing B
and contained in S: ( 2 e : &%

.!( 2 e : !
[!!/%]

* A term e has type 0 iff it is a bottom term (0 is the

least type).*

A term e has type !1 . !2 iff it is in the closure of the
elements of type either !1 or !2. Intuitively, the need of
a closure comes from the following typing rule:( 2 e

1 : !1 . !2 ( 2 e2 : !1 " ! ( 2 e2 : !2 " !( 2 e2 e1 : !

This is a Curry-style counterpart (the terms do not contain any type) to the more usual typing rule [33]:

( 2 e1 : !1 . !2 (;x : !1 2 e2 : ! (;x : !2 2 e2 : !( 2

case e1 of x 3 e2 : !

The rule can be read informally as: a term e1 has type!1 . !2

if it can be plugged in any "context" that accept
any expression of type !1 or !2. The closure operator is
meant to express this kind of contextual property. One
may have expected that a term e has type !1 . !2 if it
has either !1 or !2. But this is clearly not the right
definition in the presence of non-determinism.*

A term e has type 1%.! in the environment ' iff it is
in the closure of elements of type ! in any environment
derived from ' by binding the variable % to a closed set

of terms E containing B and contained in S:(

2 e1 : 1%.!1 (;% 2 e2 : !1 " ! % 4% FV(!)( 2 e

2 e1 : !
The reason for the closure is similar as for union types.

The operator F associates to a pair ("+,"-) of semantic
candidates a semantic candidate " defined inductively on
type patterns as follows:

"!(&)'" = {e % S |isconst e % S}"!(!1 * !2)'" =

{e % S |fst e % "+ !(!1)'" ' snd e % "+ !(!2)'"}"!(!2 " !1)'" =
{e % S |&e! % "- !(!2)'" .ee! % "+ !(!1)'"}"!(%)'" = '(%)
"!(/)'" = S"!(t1 't2)'" = "!(t1)'" 5 "!(t2)'"

"!(&%.t)'" = TB$E$S "#(t)'[%6"E]$"!

(0)'" = B"!
(t1 .t2)'" = "!(t1)'" 7 "!(t2)'""!

(1%.t)'" = SB$E$S "#(t)'[%6"E]$

In order to prove the expected properties of F, it can be
useful to define it by the following equivalent, but more algebraic, equalities:

"!(&)'" = S 5 isconst-1 (S)"!(!1 * !2)'" =

S 5 fst-1 !"+ !(!1)'"" 5 snd-1 !"+ !(!2)'"""!(!2 " !1)'" =

S 5 Te!%"-((!2)'){e|ee! % "+ !(!1)'"}

4.4 Properties of the Operator
We now prove that the operator F indeed defines a best
fixpoint "#, by checking all our assumptions of section 3.3.

Lemma 5 (Well-defined). The function " = F("+,"-)
is a semantic candidate.

Proof sketch. It is clear that "((t)') $ S for all t and '.
We show that B $ "((t)') by induction on the type patternt

, using the strictness of isconst, fst and snd and the leftstrictness of application.

Lemma 6 (Monotone). If "1 + "!1 and "!2 + "2, thenF("1

,"2) + F("!1,"!2).

Proof sketch. Let " = F("1,"2) and "! = F("!1,"!2). We
assume "1 + "!1 and "!2 + "2, and prove "((t)') $ "!((t)') by
induction on the type pattern t. All cases are immediate.

Lemma 7 (Closed). If "+ is closed, then F("+,"-) is
closed.

Proof sketch. We show that F("+,"-)((t)') is closed by
induction on the type pattern t. We use the stability of
the closure by intersection, the continuity of fst, snd and
isconst, and the left-continuity of application.

Remark 8 (Additional properties of !n). The
following properties hold:

* E !n E;

* if E1 !n E!1 and E2 !n E!2, then (E1 7 E!1) !n (E!1 7 E!2);

* if E1 !n E!1 and E2 !n E!2, then (E1 5 E!1) !n (E!1 5 E!2);

* if E !n E!, then E !n E!.
The second and third properties can be generalized to infinite
unions and intersections.

Proof. All but the last property are easy to check. Let us
consider the last property. The assumption can be restated
as: E is included in the inverse image of E! by ( )n. So, by

monotony, E is also included in the inverse image of E! by( )n

. As ( )n is continuous, this inverse image is closed. So,

it contains the closure of E. That is, E !n E! as wanted.

Lemma 9 (Contractive). If " !n "! and "! is closed,
then F(","!) !n+1 F("!,").

Proof sketch. We show that F(","!)(t)!n+1 F("!,")(t) by
induction on the type pattern t. We only consider the most
interesting case: t = !2 " !1.

Let e % F(","!)(!2 " !1)'. We want to show that (e)n+1 %F

("!,")(!2 " !1)'. First, as e % S and S is closed, we
have (e)n+1 % S. Let e! % "((!2)'). By assumption, (e!)n %"!((!2)')

. Therefore, e(e!)n % "((!1)'). By assumption again,
(e(e!)n)n % "!((!1)'). By commutation, as the semantic candidate "! is closed, (e)n+1 e! % "!((!1)') as wanted.

5 Subtyping Rules
We have now made enough assumption about the language
and the types to prove the soundness of the subtyping rules
given in figure 1. These rules are standard. The set FV(!) is
the set of free variables of !.

These rules may be interpreted inductively or coinductively.
Soundness is obvious in the inductive case, since each rule
is immediately true when taken in isolation. But we adopt
here the coinductive interpretation [8, 22], which captures
more subtyping relations.

Const& <: & Pair!1 <: !!1 !2 <: !!2!

1 * !2 <: !!1 * !!2

Fun!!2

<: !2 !1 <: !!1!

2 " !1 <: !!2 " !!1

Var% <: %

Top! <: / Inter-R! <: !1 ! <: !2!

<: !1 ' !2

Inter-L1!1 <: !

!1 ' !2 <: !
Inter-L2!2 <: !

!1 ' !2 <: !

All-R!

<: !! % 4% FV(!)!

<: &%.!!

Bot0 <: ! Union-L!1 <: ! !2 <: !!

1 . !2 <: !

Union-R1! <: !1

! <: !1 . !2
Union-R2! <: !2

! <: !1 . !2

Exists-L!!

<: ! % 4% FV(!)1%

.!! <: !

Figure 1. Subtyping rules

We write [[!]]' for "#((!)'), where "# is the closed fixpoint
of the operator F.

Theorem 10 (Soundness of the subtyping rules).
If ! <: !!, then for all environment ' we have [[!]]' $ [[!!]]'.

Proof sketch. This result is proved by applying the Reasoning Principle (theorem 4). In order to use this theorem,
we first need to define the predicate P: for all type patterns t
and t!, for all environment ', if t <: t!, then "((t)') $ "((t!)').
We then need to check that each hypothesis of theorem 4
holds. Most hypothesis are immediate. We prove:

if " is closed and satisfies P, then so does F(",")
by induction on the type patterns t and t!, and by case on
the last rule of a derivation of t <: t!.

6 Orthogonality
We show how an appropriate choice of the closure operator
implies immediately the continuity properties of section 4.2.

We define the closure operator using a polarity. Suppose
we have a set of "coterms", together with an orthogonality
relation e 0 c. The relation induces a polarity between terms
and coterms, and therefore a closure relation on terms. The
first component of the polarity associates to a set of terms E
a set of coterms E0 = {c|&e % E.e 0 c}; the second component

associates to a set of coterms C a set of terms C0 = {e|&c %
C.e 0 c}.

We now need to choose a set of coterms and an orthogonality
relation. The key ingredient for this choice is the following
lemma about adjunction. Let f be a function from terms to
terms, and g be a function from coterms to coterms. We say
that g is an adjoint of f ifff

(e) 0 c 8 e 0 g(c).

Abs)x.e 9 )x.e

Appe 9 )x

.e1 e! 9 v2e1[v2

/x] 9 %vee! 9 %v

Paire1 9 v1 e2 9 v2

(e1,e2) 9 (v1,v2)

Const& 9 & Case-Conste1 9 & e2 9 %v

case e1 e2 e3 e4 9 %v

Case-Paire1 9 (v1,v2) e3 9 %v

case e1 e2 e3 e4 9 %v
Case-Abse1 9 )x.e e4 9 %v

case e1 e2 e3 e4 9 %v

Fste 9 (v1,v2)

fst e 9 v1

Snde 9 (v1,v2)

snde 9 v1

Para-Lefte1 9 %v

e1:e2 9 %v

Para-Righte2 9 %v

e1:e2 9 %v

Var-Errorx 9

error

App-Error-1e 9 %v %v 4= )x.e

ee! 9 error
App-Error-2e 9 )x

.e1 e! 9 erroree! 9

error

Pair-Error-1e1 9

error

(e1,e2) 9 error

Pair-Error-2e1 9 v1 e2 9

error

(e1,e2) 9 error

Case-Errore1 9

error

case e1 e2 e3 e4 9 error

Fst-Errore 9 %v %v 4= (v1,v2)

fst e 9 error
Snd-Errore 9 %v %v 4= (v1,v2)

snd e 9 error
Figure 2. Reduction rules

Lemma 11. If a function f has an adjoint g, then it is continuous.

Proof. Let f be a function with an adjoint g. Let E be a
closed set of terms. The following propositions are equivalent: e % f -1

(E) f (e) % E&c %
E0. f (e) 0 c &c % E0.e 0 g(c)&c % g

(E0).e 0 c e % (g(E0))0

Therefore, f -1(E) = (g(E0))0 is closed.

We therefore choose the coterms and the orthogonality relation so that we have an adjoint for fst, snd, isconst,
the application and the approximation operators ( )n. This
yields naturally to the use of contexts:C ::

= [ ] hole| Ce

application| eC
application|
fst C first projection|
snd C second projection|
isconst C constant tester| (C)n

approximation

Indeed, let us use the following definition of orthogonality:e 0 C

iff C[e] % S.
Then, all functions expected to be continuous clearly have
an adjoint (for instance, fste 0 C iff e 0 C[fst[ ]]). Besides,

S is closed: S = {[ ]}0.

It is interesting to notice that the specification of union types
in section 4.3:"!

(t1 .t2)'" = "!(t1)'" 7 "!(t2)'"
can be rewritten equivalently as:

"!(t1 .t2)'"0 = "!(t1)'"0 5 "!(t2)'"0
which is dual to the specification of intersection types:"!

(t1 't2)'" = "!(t1)'" 5 "!(t2)'"
A similar duality also holds for existential and universal
quantification.

Interpreting types as closed sets of terms defined by orthogonality ensures that types are maximal, in the sense that, if
we take a term e which is not of some type !, then we can
find a context C accepting any term of type ! but rejecting e.

7 A Simple Call-by-value Calculus
In order to verify that the assumptions made in the previous sections make sense, we consider a simple call-by-value
calculus and prove that it satisfies these assumptions.

7.1 Definition
The terms of the calculus are given by the grammar below.
We assume an infinite set of variable x. There is a single
constant &.

e ::= x variable| )x

.e abstraction| ee

application| &
constant| (e
,e) pair|
case e e e e case operator|
fst e first projection|
snd e second projection| e:e

non-deterministic choice

We give a big step semantics to the language. Indeed, we
don't need to capture diverging behaviors: terms that diverge are well-typed. The values are abstractions, pairs, the
constant and the error:

v ::= )x.e | (v,v) | &%v ::= v |

error

Terms and values are considered modulo renaming of bound
variables.

The convergence relation e 9 %v is defined by the inductive
rules of figure 2. We say that a term e diverge if there is no
value %v such that e 9 %v.

7.2 Defining the Semantic Typing
We now show that this calculus satisfies all assumptions of
sections 3.1 and 4.2. We first define safe terms and bottom
terms. The safe terms are the terms that do not converge to
error.

S = {e|~(e 9 error)}
The bottom terms are the terms that diverge.

B = {e|~(1%v.e 9 %v)}
The closure operator is defined by orthogonality using the
definition of safe terms and the contexts of section 6. We can
then check that the assumption on S and B of section 3.1
are satisfied.

Lemma 12 (Properties of S and B). The sets S and B
are closed, and the set B is a subset of S.

Proof. It is clear that B $ S and S is closed (S = ([ ])0).
Let us prove that B is closed. Let e in B. We have fst [ ] %
B0 and [ ]& % B0. Therefore, by definition of the closure
operator, we have both fst e % S and e& % S. This is only
possible if e diverges.

We now define a number of useful terms:

diverge = ()x.x x)()x.xx)
fail = &&
isconst = )x.case x diverge error error*0 = )x

.diverge*n

+1 = )x.case xx

(*n (fst x),*n (snd x)))y

.*n (x (*n y))

The term diverge is an element of B. The term fail is not
an element of S (that is, fail 9 error). The term isconst
diverges when applied to the constant, and fails when applied
to any other value. We define the approximation operators( )n

by (e)n = *n e.

We can then check that the continuity and strictness assumptions of section 4.2 are satisfied.

Lemma 13 (Continuity and Strictness). The operators fst, snd, isconst and ( )n are strict and continuous.
If e! % S, then the function which associate to a term e the
term ee! is also strict and continuous.

Proof sketch. Continuity is clear by lemma 11. Strictness is also easy. For instance, if e % B, then we can prove
by contradiction that fst e % B. Indeed, there would otherwise exist a value %v such that fste 9 %v. We can easily see
that this is not the possible by case on a derivation of this
proposition.

We now prove the commutation properties of the approximation operators (section 4.2). The proof relies on the following
lemma.

Lemma 14. Suppose that, for all %v, if e! 9 %v then e 9 %v. Then,
if e % E then e! % E.

Proof sketch. Let us assume that e % E. Let C in E0.
We want to prove that e! 0 C, that is, C[e!] % S, or ~(C[e!] 9

e #n e! n <= n!e

#n (e!)n! )x

.e #n )x.e! n <= n!)x
.e #n )y.(()x.e!)(y)n!)n! x

#n x

e #n e!)x
.e #n )x.e! e1

#n e!1 e2 #n e!2e

1 e2 #n e!1 e!2 &

#n &

e1 #n e!1e2

#n e!2

(e1,e2) #n (e!1,e!2) e

#n e!
fst e #n fst e! e

#n e!
snd e #n snde!

e1 #n e!1 e2 #n e!2e3

#n e!3 e4 #n e!4

case e1 e2 e3 e4 #n case e!1 e!2 e!3 e!4

e1 #n e!1 e2 #n e!2e

1:e2 #n e!1:e!2

error #n error

Figure 3. Annotation of terms and values

error). On the other hand, we have ~(C[e] 9 error). So, it
is sufficient to prove that if C[e] 9 error then C[e!] 9 error.

We prove by induction on contexts the more general property
that, for all context C, if C[e!] 9 %v then C[e] 9 %v.

Lemma 15 (Commutation). The following properties of
the approximation operator hold:

* if (fst e)n % E, then fst(e)n+1 % E;

* if (snd e)n % E, then snd(e)n+1 % E;

* if (e(e!)n)n % E, then (e)n+1 e! % E.
Proof sketch. By application of lemma 14. For instance,
for the first property, we prove by case on a derivation of
fst (e)n+1 9 %v that if fst (e)n+1 9 %v then (fst e)n 9 %v.

We finish by checking the other properties of the approximation operator (section 3.1).

Lemma 16 (Approximation). The following properties
of the approximation operators hold:

* if e % E, then (e)n % E

* if (e)n % E for all n % N, then e % E.

* if e % S, then (e)0 % B;
The proof of the two first properties require some work. Intuitively, we show that if we insert approximation operators
into a term, then we get less errors. Conversely, if a term
fails, it still fails when the rank of the added approximation
operators is high enough. To be precise, we define an annotation relation e #n e! stating that the term e! is the term e
with some inserted approximation operators of rank at leastn

(figure 3) and prove the two lemmas below.

Lemma 17. If e! 9 %v! and e #0 e!, then there exists a value%v

such that e 9 %v and %v #0 %v!.

Var-Access((x) = !

( 2 x : !

App( 2 e1 : !2 " !1

( 2 e2 : !2( 2 e

1 e2 : !1

Abs(;x : !2 2 e : !1

( 2 )x.e : !2 " !1

Const( 2 & : & Pair( 2 e1 : !1 ( 2 e2 : !2( 2

(e1,e2) : !1 * !2

Fst( 2 e : !1 * !2

( 2 fst e : !1

Snd( 2 e : !1 * !2

( 2 snde : !2

Para( 2 e1 : ! ( 2 e2 : !

( 2 e1:e2 : !

Union-Elim( 2 e1 : !1 . !2

( 2 e2 : !1 " !( 2 e2 : !2 " !

( 2 e2 e1 : !

Exists-Elim( 2 e1 : 1%.!1

(;% 2 e2 : !1 " !% 4% FV(!)

( 2 e2 e1 : !

Exists-Intro( 2 e : ![!!

/%]( 2 e : 1%

.!

Inter-Intro( 2 e : !1 ( 2 e : !2

( 2 e : !1 ' !2
All-Intro(;% 2 e : !

( 2 e : &%.!

All-Elim( 2 e : &%

.!!( 2 e : !!

[!!!/%]

Sub( 2 e : !! !!

<: !( 2 e : !

Figure 4. Typing rules
Proof sketch. The proof is by induction on a derivation
of e! 9 %v!.

Lemma 18. If e 9 %v, then there exists n % N such that, for
all n! % N, if e #n+n! e! then there exists a value %v! such thate! 9 %v!

and %v #n! %v!.

Proof sketch. The proof is by induction on a derivation ofe 9 %v

. The integer n depends both on the size of the derivation
of e 9 %v and on the size of each intermediate value.

We can now finish the proofs.
Proof sketch of lemma 16.

* Let us assume that (e)n 4% E and show that e 4% E. We

know that there exists C % E0 such that the relation(e)n 0 C

does not hold. Therefore, by definition of 0
and S, C[(e)n] 9 error. On the other hand, we haveC[e]

#0 C[(e)n]. So, by lemma 17, there exists a value%v
such that C[e] 9 %v and %v #0 error. By inspection of
the definition of the annotation relation, we see that%v =

error. Therefore, the relation e 0 C does not hold:e 4%
E as wanted.

* The proof of the second property is similar, but relies

on lemma 18 instead of lemma 17.

* By inspection of the reduction rules of figure 2, it is

clear that if e % S, then (e)0 % B.

8 Typing Rules
We define some typing rules for the calculus of section 7 and
prove their soundness. The typing judgments J are defined
by the following grammar:J ::

= 2 e : ! | x : !;J | %;J
The constructions x : !;J and %;J respectively bind x and % in
the judgment J. Judgment are considered modulo renaming
of bound variables. A typing environment is a context in
with one can plug a judgment to form another judgment:( ::

= | x : !;( | %;(
We note ((x) the type associated to the variable x in the
typing environment (. The typing rules are given in figure 4.

In order to prove the soundness of the typing rules, we interpret the judgments as logical propositions:

[[2 e : !]]' = e % [[!]]'[[x : !;J]]' = &e! % [[!]]'

.[[J[e!/x]]]'[[%;J]]' = &
E.B $ E $ S 3 [[J]]'[%6"E]

We write [[!]]' for "#((!)'). The substitution J[e!/x] is the
usual capture-avoiding substitution.

We make use of the following lemma in order to prove the
soundness of the typing rules Exists-Intro and All-Elim.

Lemma 19 (Type substitution). The following equality
hold:

[[![!!/%]]]' = [[!]]'[%6"[[!!]]']

Proof sketch. We simultaneously prove by induction onn

that for all type !, we have [[![!!/%]]]' !n [[!]]'[%6"[[!!]]'] and

[[!]]'[%6"[[!!]]'] !n [[![!!/%]]]'.

Theorem 20 (Soundness of the typing rules). If 2e : !

, then e % S.

Proof sketch. We prove that if ( 2 e : ! then for all environment ' the proposition [[( 2 e : !]]' holds, by considering
each typing rule in turn. Then, if 2 e : !, we have e % [[!]]' $ S
for any environment '.

9 Limits of the Approach
9.1 Proof Techniques
It seems natural to replace the rules Exists-Intro and AllElim by subtyping rules:

!!! <: ![!!/%]!!!

<: 1%.! !

[!!/%] <: !!!&%

.! <: !!!

These rules would be obviously sound (by lemma 19) if our
subtyping relation were defined inductively. Unfortunately,
in our coinductive system, we are not able to extend the
proof of our soundness theorem 10 to accommodate these two
subtyping rules. Indeed, the current proof is by induction on

type patterns, but the type pattern t[!!/%] is not necessarily
"smaller" than the type patterns 1%.t and &%.t.

Another point is that the proof of lemma 19 relies on a very
precise knowledge of the structure of the semantic typing (it
makes uses of the !n relations). We don't know how to avoid
this.

9.2 Approximation Operators
Our results rely deeply on the approximation operators ( )n.
However, these approximation operators do not exist for arbitrary calculi.

For instance, suppose that we replace the two reduction rules
App-Error-1 and App-Error-2 by the rules below:

e 9 erroree! 9

error e 9 v e

! 9 error
ee! 9 error

e 9 v e! 9 v!v 4= )x

.eee! 9

error

In other words, we don't require anymore that the first term
of an application is a function, as long as the second term
diverges. Then, the assumption:

if (e(e!)n)n % E, then (e)n+1 e! % E.
does not hold for E = S, since the term (&(&)0)0 diverges,
while the term (&)1 & converges to error.

Similarly, if we extend the calculus with coinductive values,
such as a value v0 such that v0 = (v0,v0), then the assumption:

if (e)n % E for all n % N, then e % E
does not hold for E = B and e = v0, since (v0)n diverges for
all n, while v0 is a value.

In the first case, a workaround is to modify the definition of
the operator F for function types to:

"!(!2 " !1)'" ={e %

S |isfun e % S ' &e! % "- !(!2)'" .ee! % "+ !(!1)'"}

where isfun = )x.case x error error diverge. That is, we
force terms of type !2 " !1 to reduce to function values. In
that way, we only need the following weaker assumption in
our proof:

if isfune % S and (e(e!)n)n % E, then (e)n+1 e! % E.
But this workaround is annoying because it rejects the equality 0 " 0 = / which is intuitively appealing in this modified
calculus. Indeed, the application of any term of type / to a
term of type 0 diverges.

In fact, there seems to be no way out: defining a family of
approximation operators necessarily modifies either the semantics of the calculus or of the typing. This approach is
taken, for instance, by Dami [13, 14] who adds the approximation operators directly in the language. In the calculus
with coinductive values mentioned above, this results in an
altered semantics where new values are admitted like a pairv1 = (+

,+), where fst v1 diverges (these values are introduced to approximate coinductive values such as v0). But,
then, the type 0 * 0 is inhabited by non-diverging terms

such as v1, while we would expect to have 0 * 0 <: 0 in a
call-by-value language with coinductive values.

10 Related Work
Models of Recursive Types
MacQueen, Plotkin, and Sethi [31] define the ideal model for
types by interpreting types as ideals of a domain V, which
verifies the isomorphism:

V != T + N + (V " V) + (V * V) + (V + V) + {error}0.
That is, V is isomorphic to the coalesced sum of the truth
values T, the integers N, the continuous functions from V toV

, the product of V with itself, the sum of V with itself, and
a value error.

An ideal I is a non-empty set of elements of V which is downward closed and closed under least upper bounds of increasing sequences. The ideals are actually the closed sets of the
Scott topology (but the empty set). Our framework can be
instantiated by taking S = V \ {error} so that these closed
sets coincide with our closed sets of section 6. Indeed, a
closed set I of the Scott topology is closed according to our
definition: there exists a continuous function f from V to V
such that f e

= 0 if e % If e =

error otherwise,

and therefore I = { f [ ]}0. Conversely, one can check that
our closed sets are downward closed and closed under least
upper bounds of increasing sequences.

The three authors specify a metric on the ideals of V, such
that the set of ideals is a complete metric space. The denotation of a recursive type is computed as the fixpoint of an
operator from ideals to ideals which is shown to be contractive. In our case, this metric can be defined using the !n
relations: d(I,J) = 0 if I = J; otherwise, d(I,J) = 2-n where n
is the greatest integer such that I !n J and J !n I. The equivalent of our iteration principle (lemma 3) is proved using the
Banach fixpoint theorem.

Chroboczek [12] introduces a metric in the same spirit to
analyze recursive types in game semantics.

Damm [15] applies the ideal model to study the subtyping
of union and intersection types and to prove the soundness
and completeness of a subtyping algorithm.

Cartwright [11] defines types semantically as intervals, that
is, as pairs of two ideals (I,J) such that I $ J. Such a typing
provides two pieces of information at the same time: every
term e % I has type (I,J), and every term of type (I,J) is element of J. The interesting point about intervals is that an
operator F from ideals to ideals which is monotone in its first
argument and anti-monotone in its second argument induces
a monotone operator from intervals to intervals: this operator associate to an interval (I,J) the interval (F(I,J),F(J,I)).
Then, by the Knaster-Tarski fixpoint theorem, this monotone operator has a least fixpoint (this correspond to the
beginning of our proof of theorem 2). Unfortunately, the
two bounds of the fixpoint do not a priori coincide. At this
point, Cartwright uses the same metric space argument as

in the ideal model to establish that the bounds coincide in
the case of contractive operators.

In domain theory, a solution V of a recursive equationV = G(V)

is usually obtained by bi-limit [3] of the sequence
defined inductively by V0 = {0} and Vn+1 = G(Vn). We can
consider that Vn $ V (the domain Vn is isomorphic to a subset of V) and define a family of projections *n from V toVn

+1. These projections corresponds to our approximationoperators. They are central to most works derived from the

ideal model. Only Appel and McAllester [5] take a different
approach: they give a small-step semantics to their calculus,
and limit the number of steps to get an approximation of the
behavior of a term.

Pitts [34] is interested in solving general recursive equations
on domains. He applies a similar technique as Cartwright,
inspired by the work of Freyd [20] on algebraically compact categories, but notices that the coincidence of the two
bounds of the limit interval is a consequence of a so-called
minimal invariant property, corresponding to the fact that
the identity is the limit of the projections. Our proof of
theorem 2 is based on this very idea.

Dami [14, 13] uses a notion of labelled reduction, inspired by
the calculi by L'evy [30], Hyland [28], and Wadsworth [42].
The disadvantage of instrumenting the semantics this way is
that it modifies the properties of the initial calculus. Abadi,
Pierce and Plotkin[1] realized that the projections can be defined in some well-chosen syntactic calculi, and use this idea
to define a faithful semantics of types. This idea of denotable
projections appears to be fruitful. It was taken up successfully by Smith, Mason, and Talcott [32, 37], and Birkedal
and Harper [6] to transfer approximation techniques from
denotational semantics to operational semantics.

In the works of Appel and McAllester [5] and of Dami [14,
13], the semantic typing "# does not satisfy the fixpoint
property ("# = F("#)), but only a post-fixpoint property
("# $ F("#)) corresponding to soundness. The semantic
typing is defined as the greatest lower bound "# = inf("n)
of a decreasing sequence of semantic candidates "n defined
inductively. The advantage of this approach is that it is easier to define "#. But this semantic typing "# is a priori not
canonical: it may depend on the way the sequence ("n) is
defined. Consequently, if one wants to prove a given property on types, one has to work on the sequence "n, instead
of on its limit "#.

Orthogonality, Continuity
The notion of polarities [18] was introduced by Birkhoff [7].
The notion of closure by orthogonality appeared recently in
proof theory, independently in Krivine-style realizability [16]
and in Girard's Ludics [23]. Pitts [35] uses a similar notion of
closure to express logical relations in operational semantics.
But his typed framework remains to be connected to the
untyped framework favored by proof theoreticians.

The notion of continuity can be extended from topology to
sets X equipped with a "closure" operator c which here is
simply a function from P(X) to P(X), without any other

assumption: f : X " Y is continuous if c( f -1(B)) $ f -1(c(B))
for all B $ Y [38, 39]. In the case of closure operators as they

are defined in section 2, this definition is equivalent to our
definition of continuity (section 4.2).

11 Future Work
This article is only a first step in a wider program. We
thought it was important to start with the "elementary" case
of types interpreted as set of terms (that is, unary relations
on terms). The next step will be to study PER models and
logical relations [2, 4, 9, 10, 35, 36] in the same way.

We assume that the calculus is extensional, and therefore has
no side-effect. In particular, some of our typing rules are not
sound [17] in presence of references. It will be interesting to
see what happen in a calculus with side-effect. A starting
point may be different work on reasoning about effects [25,
36, 40, 41].

12 References

[1] M. Abadi, B. Pierce, and G. Plotkin. Faithful ideal

models for recursive polymorphic types. International
Journal of Foundations of Computer Science, 2(1):1-
21, Mar. 1991. Summary in Fourth Annual Symposium
on Logic in Computer Science, June, 1989.

[2] M. Abadi and G. Plotkin. A per model of polymorphism

and recursive types. In J. Mitchell, editor, 5th Annual
IEEE Symposium on Logic in Computer Science, pages
355-365. IEEE Computer Society Press, 1990.

[3] S. Abramsky and A. Jung. Domain theory. In S. Abramsky, D. M. Gabbay, and T. S. E. Maibaum, editors,
Handbook of Logic in Computer Science, volume 3,
pages 1-168. Clarendon Press, 1994.

[4] R. Amadio. Recursion over realizability structures. Information and Computation, 91:55-85, 1991.

[5] A. W. Appel and D. McAllester. An indexed model

of recursive types for foundational proof-carrying code.
ACM Transactions on Programming Languages and
Systems (TOPLAS), 23(5):657-683, 2001.

[6] L. Birkedal and R. Harper. Constructing interpretations of recursives types in an operational setting. Information and Computation, 155:3-63, 1999.

[7] G. Birkhoff. Lattice theory, volume 25 of Colloquium

Publications. American Mathematical Society, 1940.

[8] M. Brandt and F. Henglein. Coinductive axiomatization

of recursive type equality and subtyping. In R. Hindley,
editor, Proc. 3d Int'l Conf. on Typed Lambda Calculi
and Applications (TLCA), Nancy, France, April 2-4,
1997, volume 1210 of Lecture Notes in Computer Science (LNCS), pages 63-81. Springer-Verlag, April 1997.

[9] K. Bruce and J. Mitchell. PER models of subtyping, recursive types and higher-order polymorphism. In Proc.
19th ACM Symp. on Principles of Programming, pages
316-327, 1992.

[10] F. Cardone. Relational semantics for recursive types

and bounded quantification. In Proceedings of the
Sixteenth International Colloquium on Automata, Languages, and Programming, volume 372 of Lecture Notes
in Computer Science, pages 164-178, Stresa, Italy, July
1989. Springer-Verlag.

[11] R. Cartwright. Types as intervals. In Proceedings of

the 12th ACM SIGACT-SIGPLAN symposium on Principles of programming languages, pages 22-36. ACM
Press, 1985.

[12] J. Chroboczek. Subtyping recursive games. In Proceedings of the Fifth International Conference on Typed
Lambda Calculi and Applications (TLCA'01)., Krak'ow,
Poland, May 2001.

[13] L. Dami. Labelled reductions, runtime errors and operational subsumption. In P. Degano, R. Gorrieri, and
A. Marchetti-Spaccamela, editors, ICALP, volume 1256
of Lecture Notes in Computer Science, pages 782-793.
Springer, 1997.

[14] L. Dami. Operational subsumption, an ideal model of

subtyping. In A. D. Gordon, A. M. Pitts, and C. Talcott, editors, HOOTS II Second Workshop on HigherOrder Operational Techniques in Semantics, volume 10
of Electronic Notes in Theoretical Computer Science.
Elsevier Science Publishers, 2000.

[15] F. Damm. Subtyping with union types, intersection

types and recursive types II. Research Report 2259,
INRIA Rennes, May 1994.

[16] V. Danos and J.-L. Krivine. Disjunctive tautologies

and synchronisation schemes. In Computer Science
Logic'00, volume 1862 of Lecture Notes in Computer
Science, pages 292-301. Springer, 2000.

[17] R. Davies and F. Pfenning. Intersection types and computational effects. In ICFP '00 [29], pages 198-208.

[18] M. Ern'e, J. Koslowski, A. Melton, and G. E. Strecker.

A primer on Galois connections. In A. R. Todd, editor, Papers on general topology and applications (Madison, WI, 1991), volume 704 of Annals of the New York
Academy of Sciences, pages 103-125, New York, 1993.
New York Acad. Sci.

[19] M. P. Fiore. A coinduction principle for recursive data

types based on bisimulation. Information and Computation, 127(2):186-198, 1996.

[20] P. J. Freyd. Algebraically complete categories. In

A. Carboni, M. C. Pedicchio, and G. Rosolini, editors,
Proceedings of the 1990 Como Category Theory Conference, volume 1488 of Lecture Notes in Mathematics,
pages 95-104. Springer-Verlag, 1991.

[21] A. Frisch, G. Castagna, and V. Benzaken. Semantic

subtyping. In 17th IEEE Symposium on Logic in Computer Science, pages 137-146. IEEE Computer Society
Press, 2002.

[22] V. Gapeyev, M. Levin, and B. Pierce. Recursive subtyping revealed. In ICFP '00 [29]. To appear in Journal
of Functional Programming.

[23] J.-Y. Girard. Locus solum: From the rules of logic to

the logic of rules. Mathematical Structures in Computer
Science, 11(3):301-506, June 2001.

[24] A. D. Gordon and A. M. Pitts, editors. Higher Order Operational Techniques in Semantics. Publications
of the Newton Institute. Cambridge University Press,
1998.

[25] J. Goubault-Larrecq, S. Lasota, and D. Nowak. Logical relations for monadic types. In Proceedings of

the 11th Annual Conference of the European Association for Computer Science Logic (CSL'02), volume 2471
of Lecture Notes in Computer Science, pages 553-568.
Springer-Verlag, Sept. 2002.

[26] H. Hosoya and B. C. Pierce. XDuce: A statically typed

XML processing language. ACM Transactions on Internet Technology, 2002. Submitted for publication.

[27] H. Hosoya, J. Vouillon, and B. C. Pierce. Regular expression types for XML. ACM Transactions on Programming Languages and Systems (TOPLAS). To appear; short version in ICFP 2000.

[28] J. M. E. Hyland. A syntactic characterization of the

equality in some models of the )-calculus. Journal of
the London Mathematical Society, 12(2):361-370, 1976.

[29] Proceedings of the the Fifth ACM SIGPLAN International Conference on Functional Programming
(ICFP'00), Montr'eal, Canada, Sept. 2000. ACM Press.

[30] J.-J. L'evy. An algebraic interpretation of the

lambda beta K-calculus; and an application of a labelled lambda-calculus. Theoretical Computer Science,
2(1):97-114, June 1976.

[31] D. MacQueen, G. Plotkin, and R. Sethi. An ideal model

for recursive polymorphic types. Information and Control, 71(1-2):95-130, 1986.

[32] I. A. Mason, S. F. Smith, and C. L. Talcott. From

operational semantics to domain theory. Information
and Computation, 128(1):26-47, 1996.

[33] B. C. Pierce. Programming with intersection types,

union types, and polymorphism. Technical Report
CMU-CS-91-106, Carnegie Mellon University, Feb.
1991.

[34] A. M. Pitts. Relational properties of domains. Information and Computation, 127:66-90, 1996.

[35] A. M. Pitts. Parametric polymorphism and operational

equivalence. Mathematical Structures in computer Science, 10:321-359, 2000.

[36] A. M. Pitts and I. D. B. Stark. Operational reasoning

for functions with local state. In Gordon and Pitts [24],
pages 227-273.

[37] S. F. Smith. The coverage of operational semantics. In

Gordon and Pitts [24], pages 307-346.

[38] B. M. R. Stadler and P. F. Stadler. Basic properties of

closure spaces, 2002. Supplemental material for [39].

[39] B. M. R. Stadler and P. F. Stadler. Generalized topological spaces in evolutionary theory and combinatorial chemistry. J. Chem. Inf. Comput. Sci., 42:577-585,
2002. Proceedings MCC 2001, Dubrovnik.

[40] I. Stark. Names, equations, relations: practical

ways to reason about new. Fundamenta Informaticae,
33(4):369-396, 1998.

[41] C. Talcott. Reasoning about functions with effects. In

Gordon and Pitts [24], pages 347-390.

[42] C. P. Wadsworth. The relation between computational

and denotational properties for Scott's D#-models of
the lambda-calculus. SIAM Journal on Computing,
5(3):488-521, Sept. 1976.