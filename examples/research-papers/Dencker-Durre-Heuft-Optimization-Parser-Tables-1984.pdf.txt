

Optimization  of  Parser  Tables  for  Portable Compilers 
PETER DENCKER, KARL DORRE, and JOHANNES  HEUFT 
Universit~t Karlsruhe 

Six  methods  for  parser  table  compression  are  compared.  The  investigations  are  focused  on  four methods  that  allow  the  access  of table  entries  with  a  constant  number  of  index  operations.  The 
advantage of these methods is that the access to the compressed tables can be programmed efficiently in  portable  high-level languages  like  Pascal  or  FORTRAN.  The  results  are  related  to  two  simple 
methods  based  on  list  searching.  Experimental  results  on  eleven different grammars  show that,  on the average, a  method based on graph  coloring turns  out best. 

Categories  and  Subject  Descriptors:  D.2.7  [Software  Engineering]:  Distribution  and  Mainte- nance-portability;  D.3.4  [Pro gramming  Languages]:  Processors--compilers; parsing; translator 
writing systems  and  compiler generators; E.1  [Data]:  Data  Structures;  E.2  [Data]:  Data  Storage Representations;  G.2.2  [Discrete  Mathematics]:  Graph  Theory--graph  algorithms 

General Terms: Algorithms, Experimentation,  Languages,  Performance 
Additional Key Words and  Phrases:  Graph  coloring, sparse  matrices, table compression 

1.  INTRODUCTION 
LR-parsers  are  currently  the  standard  choice  of  compiler  designers  for  the syntactical  analysis  of programs.  These  parsers  are  controlled  by  tables,  which 

in  their  original  form are too  large  for practical  use  in  compilers.  We  survey  and compare  six  general  compression  methods  for  sparse  tables.  They  have  been 
implemented  and  applied  to  parser  tables  generated  from  grammars  of  eleven different  languages.  We  have  focused  on  methods  that  particularly  allow  the 
efficient  usage  of  the  compressed  parser  tables  in  portable  compilers.  Here, portability  means  that the compiler  is completely written in a portable high-level 
language  like  Pascal  or  FORTRAN  [36].  The  methods  are  compared  for  their algorithmic  complexity,  their  compression  potential,  and  their  induced  table- 
access  overhead. Most  of  the  grammars  were  taken  from  real  compiler  construction  projects 
without change. These  grammars  include Ada 1 [1], AL  [23, 33],  BALG  [19], LEX 

1 Ada is a  registered trademark  of the U.S. Department  of Defense. 

Authors'  present  addresses:  P.  Dencker,  Systeam  KG  Dr.  Winterstein,  Am  Entenfang  10,  7500 Karlsruhe  21,  West  Germany;  K.  Ddrre,  Institut  fLir Informatik  I, Universit/it Karlsruhe,  Kaiserstr. 

12,  D-7500  Karlsruhe  1,  West  Germany;  J.  Heuft,  Gesellschaft  flit  Mathematik  und  Datenverar- beitung mbH,  Schloss Bir!inghoven, D-5105  St. Augustin  1, West Germany. 
Permission to copy without fee all or part  of this material  is granted provided that the copies are not made or distributed  for  direct commercial advantage,  the ACM copyright  notice and the  title of the 
publication and  its date  appear,  and  notice is given that  copying is by permission of the Association for  Computing  Machinery.  To  copy  otherwise,  or  to  republish,  requires  a  fee  and/or  specific 
permission. (C) 1984 ACM 0164-0925/84/1000-0546  $00.75 

ACM Transactions  on Programming Languages and Systems, Vol. 6, No. 4, October 1984, Pages 546-572. 

Optimization of Parser Tables for Portable Compilers  *  547 
[18],  LIS  [22],  MIN IL EX   [30],  and  Pascal  [24].  The  other  grammars  ALGOLW [40],  ALGOL60  [34],  Euler  [41],  and  X P L   [32]  have  been  chosen  for  reference 
purposes.  All grammars  are published  in  [10]. Because  many  entries  in  LR-Parser  tables  are  identical  (i.e.,  error  entries)  or 

even  insignificant  (i.e.,  "don't  cares"),  the  tables  can  easily  be  compressed  to about  20 percent  of their  original  size  or  less  ([4],  6.8)  by storing  them  in  lists. 
The  well-known  YACC  (yet  another  compiler-compiler)  [26],  for  instance,  uses such a  technique  (see  Section  2.5)  where the  information  retrieval  requires  a  list 

search. The  disadvantage  of list  storing  techniques  is that  a  list  search  is required  for 
information  retrieval.  This  search  may  become  more  expensive  than  the  access by index  operations  if (machine-dependent)  list  search  instructions  are  unavail- 
able.  Binary  search  would be useful  only  if the  lists  are  long  enough  to  gain  an appropriate  trade-off. Fels' results  [16]  for a  SIEMENS  7.755 show that  a binary 
search  beats  a  linear  search  only  for  more  than  16  elements  per  list.  Our experimental  results  show that,  on  the  average,  this  boundary  is  exceeded only 
in two cases. To substantiate  this,  investigations  of the dynamic access frequency would be necessary.  Even when  such instructions  like IBM's  "translate  and test" 
are  exploited,  the  search  criterion  is  restricted  in  its  size  to  8 bits.  For  example, the  technique  stated by Pager  [35]  restricts  the  number  of symbols and  pseudo- 
symbols to  256,  so that  for languages  like Ada  no parser  could be  generated.  For these  reasons  we  have  been  looking  for  compression  methods  without  such 
limitations  and  with  fast  constant-access  time. Since  1977  we  have  gained  practical  experience  with  two  methods  based  on 
graph  coloring  [13]  and  line  elimination  [6].  They  are  part  of  our  LALR(1)- Parser-Generator  [9],  which  has  proved  its  practicability  and  portability  in 
various academic and industrial  compiler projects running  on different machines. These  two  methods  combine  the  properties  of  fast  table  access  and  machine 

independence  for  the  parsers  generated.  They  borrow  from  Joliat  [27,  28]  the idea of error  matrix  factoring.  Joliat's  method is based on automata theory. After 
factoring  out  a  Boolean  error  matrix,  he  interprets  the  rest  of the  parser  tables in  the  same  way  as  those  for  an  incompletely  specified  finite-state  automaton, 
where  the  original  error  entries  represent  "don't  care"  conditions.  His  method keeps the  parser  tables  in  matrix  form.  The  information  retrieval  requires  some 

simple  index  operations.  These  index  operations  are  basic  to  many  common programming  and  machine  languages. 

The  methods  described  in  Section  2  ignore  the  semantics  of the  parser  table entries.  Therefore  optimization  methods  based  on  the  semantics  of the  entries 
m a y   be  applied  independently.  However  it  should  be  noted  that  it  is  just  this kind  of optimization  that  gives  our  methods  their  excellent  space  performance. 

A  method that  exploits the  special  structure  of LALR(1) parser  tables  and  some tuning  possibilities  of the  different  methods  are  discussed in  Section  3  in  detail. 

In  Section  3.7  we  give  an  overview  of  sophisticated  extensions  with  which  we have  not  experimented.  The  theoretical  and  experimental  results  are  presented 
and  interpreted  in  Section  4. In  Section  1.1  we give the  notation  and  an  example  of the  kind  of parser  table 
we have used.  In  Section  1.2 we introduce  the  error  matrix,  which  is essential  for the  use of sparse  matrix  optimization  methods  to parser  tables. 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October  1984. 

548  *  P  Dencker, K  DQrre, and J  Heuft 

Figure 1 

J. 1 
1 2 
3  -3 4  -1 
5 6 
7 8  -2  

T-table  N-table 

+ *   (  )  id  Z  E  T  F 2  3  4  5  6  1  2  3  4 

2  *7 2  ,7 
-3  6  -3 7 

7  *6 2  *7 

2  *7 -2  6  -2 

1  4  3  *5 5  3  *5 

.4 8  .5 

1 1  Notation  and  Example 
We  take  the  notation  from  Anderson,  Eve,  and  H o m i n g   [3]  with  the  following 
convenient  additions: 

VT,  denotes  the  union  of the  terminal  vocabulary  VT with  the  e n d m a r k e r / ,  
V'  denotes  the  union  of  VT; with  the  nontermin al  vocabulary  VN, 
Q  denotes the  finite  set  of states  of a  given parser. 

T h e   T-table  and N-table of [2, 3 (4.2)] can now be defined as two partial mappings: 

T-table:  Q  x  VT,  ~  {error}  U  {shift}  x  Q  U 

{reduce,  shift-reduce}  x  P 
N-table:  Q  *  VN --*  {shift}  X  Q  U 

{shift-reduce}  x  P. 

Figure  1  shows  the  LALR(1)-parser  tables  for the  augmented  example  gramm ar 
G with  productions 

I:  Z - - *   E 
2:  E - - *   E  +  T  3:  E - - *   T 
4:  T---,  T  .  F  5:  T---*  F 
6:  F - - *   (E)  7:  F  ~  id 

Te rm in ati on   of the  parser  is  achieved in  the  example  by  detecting  a  shift-action 

into  the  first  state,  t h a t   is, the  start  state  of the  parser. 

denote  a  shift  action  into  state  q, 
denotes  a  shift-reduce  with  production p, 
denotes  a  reduce  with  production p, 
denotes  an  error  entry, 
....  denotes  an  insignificant  entry. 

Contrary to  [2,  3]  we  employ the  "scan-production"  n um be r  en try  for bo th  T- 
table  and  N-table  and  refer to  it  as  "shift-reduce" because  it  is  a  concatenation 

of a  shift-  and  a  reduce-action.  I n s i g n i f i c a n t   entries  are  those  t hat   can  never 
be  accessed  during  a  parse.  A  definition  for  insignificant  entries  in  the  T-table 

may  be  found  in  [9  (3.4.2)].  In  the  following,  however,  we  treat  the m  as  error 
entries because we have  not as yet found an efficient algorithm to compute them. 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

Unsigned 

integers  q " . p "  

,,_p- 

Optimization of Parser Tables for Portable Compilers  *  549 
_L  +  *  (  )  id 

1  2  3 45   6 

1  1  1  0  1  0 

1  1  1  0  1  0 
0  0  0  1  0  1 
0  0  1  1  1  1 

1  0  1  1  0  1 
1  1  1  0  1  0 

1  1  1  0  1  0 
0  0  0  1  0  1 

Fig.  2.  The  error  matrix to the T-table  of 
Figure  I 

1,2  Error  Matrix  Factoring 
In contrast to an N-table, a T-table  is not sparse by nature. It contains  many  
error entries (see Figure 1). To make  optimization  methods  for sparse matrices 
applicable, we have to make  it look sparse. To this end we may factor out of the 
T-table the most frequent entry, as was done by Joliat [27, 28]. Obviously this is 
the  error  entry. The  resulting binary matrix is  called the  error  matrix.  Figure  2 

shows it for the T-table of Figure 1. Now the error entries in the T-table become insignificant if the  T-table  is  only accessed when  the  error  matrix  indicates no 

error  entry.  Hence  the  T-table  has  become  sparse.  Note  that  in  the  following 

sections we refer to the error matrix as the  negated sigmap. 

2.  TABLE  COMPRESSION  SCHEMES 
Typical parser  tables  are  either  sparse,  that  is, they have only a  few significant entries,  or  they  may  become  sparse  when  factoring  out  the  error  entries  as 

described above. Thus, all methods for compressing sparse tables  are  applicable. As entries  of parser  tables  are  not  modified during parsing,  we  do  not  consider 

methods  allowing  insertion  and  deletion  of  entries.  In  the  next  sections  we consider for compression a  table T  defined as 

T:ARRAY [1 ..  m, 1 .. n] OF data. 

We  give  short  descriptions  of  the  six  methods  compared  in  the  following 
sections.  The  first  four  methods  are  called i n d e x   access  methods  because  the access is done by index operations. The other two methods are called list s e a r c h  

methods because the  access is performed by searching in lists. 

2.1  Graph  Coloring  Scheme (GCS) 
In this scheme  [13,  39]  we use the  fact that one row can be  merged with another 
if both do not have different significant values in any column position. Thus we are looking for a partition of all rows into classes such that the rows in each class 

do not collide and can be  merged.  Such a  row partition with a  minimal number of classes  represents  an  optimal  compression  of all  table  rows.  This  problem, 

however,  is  equivalent to  the  problem  of looking for the  minimal  coloring of a 
graph by the construction below. We consider a graph where each vertex uniquely represents a table row. Vertices 

are  adjacent  if  and  only  if  the  respective  rows  collide,  that  is,  have  different significant values  in  at  least  one  column  position.  Two  vertices  of this  graph 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

550  *  P. Dencker, K. D0rre, and J. Heuft 

having the  same color  in  a  vertex coloring represent  two  noncolliding rows  and 
thus can be merged to a single row. For the coloring approximate algorithms may 
be used (see Section 3).  Reducing T by merging all rows of each color class yields 
a  (gr * n)-table  T '   where g,  <_ m  is  the  number  of colors  used  for  coloring the graph representing the rows of T. 

Usually the  compressed  table  will  also  be  sparse,  and  therefore  the  columns 
may be merged in the same manner giving a  table 

value: ARRAY [1 .. gr, 1.. gel OF data 

(where gc -  n  is  the  number  of colors  used  for coloring the  graph  representing the columns of table  T '). 

If the  environment guarantees  that  only significant entries  are  accessed,  the above structure is sufficient for information retrieval. Otherwise, we need 

* sigmap: ARRAY [1 ..  m, 1..  n] OF boolean. 

For parser tables this sigmap  corresponds to the  negated  error matrix described in Section 1.2. 

In any case, two vectors 
rowmap: ARRAY [1 ..  m] OF 1 .. gr 
and 
eolumnmap:ARRAY [1 .. n] OF 1 .. gc 

are  necessary for representing the  mapping from the  rows and columns of table 

T to the  rows and columns of the table  value.  The value of a  table entry T[i, j] is obtained by 

IF sigmap[i, j] THEN value[rowmap[i], columnmap[j]] ELSE insignificant. 
If only significant values are accessed there is no need for s i g m a p  and the access is done by 
value[rowmap[i], columnmap[j]]. 
The row-merging method reported in  [27,  28]  is similar to this scheme; however, 
the  connection  with  graph  coloring,  which  opens  the  field  for  application  of different heuristic graph coloring algorithms, is not mentioned there. 

2.2  Line Elimination  Scheme (LES) 
LR-parser  tables  (as  many precedence  tables)  usually contain  many  rows  and columns in  which all significant entries  have the  same  value.  Therefore we  can 

apply a  method which Bell  [6]  originally suggested for precedence tables and in which this  characteristic  is  exploited.  The  compression  is  done by  scanning all 
rows  of T,  therein  eliminating each  row  i where  all significant entries  have  the same value vi; then the  same is done for all columns by eliminating each column 
j  with  single  significant value  vj.  This  value  vi (respectively,  vj)  is  entered  in  a vector 

r:ARRAY [1 ..  m] OF data  (respectively, c:ARRAY [1 .. n] OF data) 
ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October  1984. 

Optimization of Parser Tables for Portable Compilers  *  551 
by 

r[i]  -- vi  (respectively, e[j] := vj). 

In  case  a  column  has  been  eliminated,  some  other  rows  may  also  be  eliminated 
in a  second scan of the  remaining  rows, and a  second column scan  may also bring 
new  column  elimination.  This  procedure  is  applied  alternatively  to  rows  and 
columns  until  no  further  lines  can  be  eliminated.  If such  an  elimination  occurs 
in  the  sth  scan  of rows  (respectively, columns)  the  nu mb er   s  of scans  is  entered 
in  a  decision vector dr(respectively, dc)  by 

dr[i] := s  (respectively, dc[j]  := s). 

For  each  row  i  (respectively,  each  column j )  th at  could  not  be  eliminated,  the 
number of the very last scan sm~x is entered in the decision vector d r   (respectively, 
dc),  and  the  mapping  to  row  i'  (respectively, column j ' )   of the  compressed  lr.lc 
table  v a l u e   is  entered  in  vector r  (respectively, c).  T he   four vectors  dr,  dc,  r,  c 

represent the  mappings 

dr:  [1  ..  m] --* [1  ..  Sma=], dc:  [1 ..  n]  --* [1  ..  sm,,], 

r:  [1  ..  m]  --*  [1  ../~]  U  data, c:  [1  ..  n]  ~  [1 ../c]  U  data. 

If  arbitrary  entries  are  accessed,  the  same  binary  matrix  s i g m a p   as  in  the 
scheme  above  is  needed.  A  table  entry  T[i, j]  is  obtained by 

IF sigmap  [i,j] THEN 

IF dr[i] <  dc[j] THEN r[i] 
ELSE IF dr[i] >  dc[j] 

THEN c[j] ELSE value[r[/], c[j]] 
ELSE in sign ifica nt 

As  in  the  Graph  Coloring  Scheme,  s i g m a p   and  the  first  IF  state ment   are  not 
needed if we  know  th at   only significant  entries  are  accessed. 

2.3  Row  Displacement Scheme (RDS) 
This  method  was  suggested  by  several  authors  [4,  42].  T h e   essential  idea  is  to 
merge  all  rows  of table  T  into  a  single  vector 

value: ARRAY [1 ..  h] OF data 

in  such  a  way  t ha t  one  row  at  a  time  is  embedded  into  v a l u e .   This  is  done  by 
shifting  the  row  from  index  1  to  higher  indices  until  all  its  significant  elements 
are  overlapping only insignificant  entries  in  v a l u e .   For  retrieval  of information, 
vectors 

rowindex:ARRAY  [1 ..  h] OF 0 ..  m 

and 
rowpointer:ARRAY  [1 ..  m] OF 0 ..  h - n  

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

552  *  P. Dencker, K. D0rre, and J. Heuft 
are  needed,  where  r o w i n d e x [ h ]   contains  a  row  num ber  i, if value[k]  contains  a 
significant  element  of  row  i.  r o w p o i n t e r [ i ]   points  to  the  position  in  v a l u e ,  
which  the  zeroth  element  T[i,  0]  of  row  i  would  have.  T hu s  the  access  to  an 
arbitrary  element  T[i, j]  m ay be  realized by 

IF rowindex[rowpointer[i]  + j]  (  )  / THEN insignificant 

ELSE value[rowpointer[i]  + j]. 

If only significant  entries  are  accessed,  this  procedure  is  simplified to 
value[rowpointer[i]  + j] 

and  the  vector r o w i n d e x   is  not  necessary. 

2.4  Significant  Distance  Scheme  (SDS) 

In this  method  [5]  the  heading  and  trailing  insignificant  elements  of each  row in 

T  are  ignored,  and  the  remaining  intervals  from  the  first  to  the  last  significant 
elements  are  placed  in  a  vector 

value:ARRAY  [1 ..  h] OF data 

in a  linear sequence row after row. For retrieval of an entry T [i, j] three additional 
vectors 

ro w po i n te r   : ARRAY  [1  ..  m]  OF  - n   ..  h - 1 ,  first,  last  : ARRAY  [1  ..  m]  OF  0  ..  n 

are  introduced.  In  first[i]  the  columnindex  of  the  first  and  in  last[i]  the 
columnindex  of the  last  significant  element  of row  i  is  entered,  r o w p o i n t e r [ i ]  
points  to  t ha t  position  in  v a l u e   t hat   the  zeroth  element  T  [i,  0]  of row  i  would 

have.  T hu s  the  access  is done  by 

IF (j <  first[i])  or (j >  last[i]) THEN insignificant 

ELSE value[rowpointer[i]  + j]. 

In  general,  this  basic  version  of  the  me tho d  does  not  save  very  much   space. 
Therefore Beach  suggests  an  additional  heuristic  for gaining better  compression 
by producing  a  special  column permutation.  In  this  case  an  additional  vector 

columnperm: ARRAY  [1 ..  n] OF  1 ..  n 

is  needed  and  in  the  access  j  has  to  be  replaced  by  c o l u m n p e r m [ j ] .   If  only 
significant  entries  are  accessed  the  vectors  f i r s t   and  l a s t   are  superfluous  and T[i, j] 

is  accessed by 

value  [rowpointer[i]  + j]. 

2.5  Row  Column  Scheme  (RCS) 
A  commonly  employed  compression  m et ho d  using  list  structure  is  the  Row 

Column  Scheme  [37].  In  this  scheme,  the  storage  of  insignificant  elements  is 

completely avoided by the  help  of three  vectors 

value  : ARRAY  [1  ..  h]  OF  data. columnindex  : ARRAY  [1  ..  h]  OF  1 ..  n. 

rowpointer  : A R R A Y [ 1 . . m +   1]OF  1 . . h +   1. 
ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October  1984. 

Optimization of Parser Tables for Portable Compilers  *  553 
Scanning  the  table  T  row  by  row,  the  vector  v a l u e   is  filled  with  significant elements  T [i, j]  only. The  column  index j  is entered in the  respective position  of 
eo l u m n i n d ex .  When  a  new  row  i  is  started,  the  index  of the  next  free  element of v a l u e   is entered  in  ro wp oi n ter[i ].  When  a  table  element  T[i, j] is looked up, 
the  index j  is  searched  in  e o l u m n i n d e x   from  position  r o w p o i n t e r[i ]   to  r o w -  p o i n t e r [ / +   1]  -  1.  In  the  case in  which j  is  found  in  position  k, value[k]  is the 
value  of  T[i,  j];  otherwise  T[i,  j]  is  insignificant.  One  should  note  that  this method  cannot  be  simplified  when  accessing  only  significant  elements.  Sophis- 

ticated extensions  of this  basic  method  are  discussed in  Section  3.7. 

2.6  Suppressed  Zero Scheme  (SZS) 
This  method  is  often  used  to  compress  text  files.  The  compression  is  done  by scanning  to  the  table  T  row  by  row  and  sequentially  entering  the  significant 

values in  a  vector 

tablevector: ARRAY [1 ..  2 * h + 1] OF integer. 

The  number  of  consecutive  insignificant  elements  before  the  first  or  after  the last significant  element, or between two significant elements,  of a row are entered 

in  t a b l e v e c t o r   before,  after,  or  between  these  elements,  together  with  a  label. marking  the  insignificance.  For  faster access it  is advisable to use  a vector 

rowpointer: ARRAY [1 ..  m+ 1] OF 1 ..  2 * h+ 1 
similar  to the  Row Column  Scheme. 

3.  HEURISTICS AND TUNING 

In  this  Section  we  discuss  the  heuristics  that  have  been  applied  additionally  in order  to  improve  the  methods.  In  Section  3.7  an  overview  of  some  possible 

extensions  is presented. 

3.1  Code  Reduction Scheme  (CRS) 
Whereas  the  methods  described  in  Section  2  aim  at  reducing  the  size  of  the tables,  the  Code Reduction  Scheme  aims  at  reducing  the  size of entries. 

It  is  a  divide-and-conquer  method  for  data.  The  basic  idea  is  to  divide  the information  of a  table  entry  into  a  large  base  information  that  is common  to  all 
entries  of  a  single  column  and  a  small  displacement  information  that  is  just enough  to  distinguish  different  entries  in  that  column.  The  displacement  infor- 
mation  is  left  in  the  table,  whereas  the  base  information  is placed  in  a  separate vector  (one  for  each  column).  As  a  highly  desirable  side  effect,  the  rows  and 
columns  of the  tables  show  more  uniformity  (see  Figure  3).  Our  experimental results  show that  including  this  scheme yields better preconditions  for the  Graph 

Coloring Scheme and  the  Line Elimination  Scheme than  leaving it out.  It is only in this  method  that  we use the  information  in the  contents  of the  parser  tables. 

The  tables  generated  by  an  LALR(1)-parser-generator  have  the  following properties: 

(1)  For the  set of states  Q, a  partition  Q/V'  may be defined by 

Qa =  {q'  IT-table(q,  a)  =  (shift,  q'),  q  E  Q} Va  E  Vw, 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

554  *  P  Dencker, K  DOrre, and J. Heuft 
and 
where 

QT  QN 

0  5 67   0  0  0  1 3  0 

PT  PN 

2  2 22   2 3  0 0 0 0  

PLP 

0  1 
0  2 
0  3 1  4 

2  5 
3  6 
3  7 
3  8 

PL 
5 67  

1  2  3 

1 
- I   1 - I  - I   1 

- I  - I  1 

4  5  6  1 23  

1  *1  1 22  

*1 

1 
1  2 

- i  
- i  
1  *I 
1  *I 
1  *I 

*I 
*i *2 
*I 

Figure 3 
Q~ =  [q'  I N-table(q,  a)  =  (shift, q'),  q E  Q} Va E  VN 

Q~ n  Qb =  0 ,  a  ~  b,  a,  b  E  V'. 
(2)  For the  set of productions P,  a  similar partition  P / V '   may be  defined by 

Pa  =  {P I T-table(q,  a)  =  (shift-reduce, p),  q E  Q} Va E  VT. 
and 

where 

P~ =  {p [ N-table(q,  a)  =  (shift-reduce, p),  q E  Q} Va E  VN 

P~ n  Pb =  O,  a  c  b,  a, b E  V'. 
(a)  First,  because  of  Property  (1)  we  may  renumber  the  states  consistently such that  for all 

a  E  V'   the  states  of Qa have consecutive numbers. For retrieval of information we need two mappings 

QT : { VT '- - >Q O  { 0} --* minQ~  -  1 
Q N : { 2   -*   Q  u  {0} --~ rain Qa -  1 
where 

min  Qa =  { 1  in[q[q  E  Qa}  if  Q a ~ O ,  otherwise. 

ACM Transactions on Programming  Languages and Systems, Vol. 6, No. 4, October 1984. 

Optimization  of Parser Tables for Portable Compilers  *  555 
Now we can  replace  state  number  q in  a  table  column  a  by a  displacement 

dq=  {q  -  QT(a),  a  E  VT', QN(a),  a  *  VN. 

Thus,  all  displacements  in  a  column  a  *  V'  are  in the  range  1  . ..  I Qa I. This  normalization  of displacement  entries  increases  the  probability  of iden- 
tical  entries  in  different  columns  and  decreases  the  number  of bits  necessary to store  such  entries. 

(b)  The  same  technique  may  be  applied  to  production  numbers  using  the partition  P / V '   instead  of partition  Q/V'.   Here  we have  the  two mappings 

p T:{ V T ' -* P U   { 0} --* minPa  -  1 

p N : { ? " - > P U {   0} --* minPa  -  1 
where 

~min{p  IP  E  P~}  if  P ,   ~  O minPa  [1  otherwise 

and  the  displacement  of a  production  number p  is given by 

/9--  Pw(a),  a  E  VT,, d~ =  PN(a),  a  E  VN. 

(C)  The  reduce  entries  in  the  T-table  do  not  fit  into  such  a  classification.  We treat  them  as  follows:  For  each  row  q  of  the  T-table  we  construct  a  list  PLq 
(production  list)  consisting  of  those  production  numbers  that  have  reduction entries  in  the  row  and  replace  the  production  numbers  in  the  row by respective 
indices jp  in  PLq.  Now we concatenate  all  lists  into  one  vector PL.  For  retrieval of information  a  vector P L P   (production  list pointer)  contains  the  start  position 

(minus  one)  of each  list PLq in  PL,  such that 

PL  [PLP[q]  + jp] 
yields p. 

The  following example  may be helpful  in  understanding  the  essential  ideas  of the  Code Reduction  Scheme. 

Let  G  again  be  our  example  grammar  with  the  generated  LALR(1)-parser tables  as  in  Figure  1.  Then  Figure  4  shows  the  tables  after  state  (QR)  and 
production  (PR)  renumbering  before  the  mapping  onto  the  displacements  is performed.  Note  that  the  vectors  PR   and  QR do not  add  to  the  storage  require- 

ment  if all  production  and  state  information  is  renumbered  consistently.  Figure 3 shows the  tables  after  application  of the  Code Reduction  Scheme. 

Part  (a)  of the  Code  Reduction  Scheme  was  first  mentioned  by Anderson  [2]. We  added  the  steps  (b)  and  (c)  to  get  smaller  numbers  all  over the  table.  When 
applying this  scheme, we are  able to store the  entries  of the  tables  in  1 byte even 

ACM Transactions  on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

556  *  P. Dencker, K. DOrre, and J. Heuft 

PR:  1 2 3 4 5 6 7  

5 6 7 2 1 3 4  

QR 

1  1 8  2 

5  3 3  4 
2  5 7  6 
6  7 4  8 

1  2 3 4   5 

6 -5  6 
-6  -6  7 -7  -7  7 

8  *3 

-6 -7 
8 8 
8 

6  1  2 
*4  1  3 

*4 *4 
*4  2 

3  4 
5  *1 

4  .1 *2 

5  *1 

Figure 4 
in the  case of the  larger grammars.  Note t h at   there  could be considerable  savings 
with  bit  storage  techniques  in  the  case  of the  smaller  grammars  in  a  trade-off of 
portability  and  fast  access  (compare  with  [8]).  Anderson,  Eve,  and  H o m i n g   [3] 

rejected  metho d  (a)  in  favor  of anot her  state  pe rmut atio n  allowing  the  deletion 
of empty  rows  in the  N-table.  T h e y  argue  th at   for a  large  n um ber   of states  there 
is  no  entry  in  the  N-table.  But  we  have  observed  th at  this  n um ber   decreases 
considerably  if  LR(0)-reduce  states  are  removed  and  replaced  by  shift-reduce 
transitions. 

3.2  Graph  Coloring  Heuristics 
T h e   principles  of  the  Graph  Coloring  Scheme  are  independent  of  the  actual 

coloring of the  respective  graph.  Usually,  a  minimal  coloring cannot  be  achieved 
because of the high  algorithmic complexity of the problem, which  is NP-com plete 

[29,  17]. Therefore we have used the  following approximate algorithm,  which  can 
be  obtained  from  a  backtrack  algorithm  by  suppressing  the  backtracking  and 

which  proved  slightly  better  in  experimental  results  t h a n   other  approximate 

algorithms  [11]: 

wh ile  there is a  noncolored vertex in G loop  Choose a  noncolored vertex  v that  is  blocked for a  maximum  number  of 

colors (blockings caused by already colored adjacent vertices); Color vertex v with the least possible color; 
repeat 

Unfo rtun ately   the  approximation  m ay  be  very  bad  in  special  cases.  This  is 
comm on  to  all  approximate  coloring schemes  and  hence  c ann ot   be  improved by 

switching  to  ano ther   m et ho d  [25].  For  each  nu mb er   g  there  are  g-chromatic 
graphs  (i.e.,  graphs  colorable  with  g  but  not  with  g  -  1  colors),  for  which  the 
algorithm  m ay use  a  nu mb er ga of colors for coloring t h a t   is  linearly bounded  on 
the  vertex nu mb er   n,  th a t   is, ga =  O (n)  [14]. 

For  instance,  let  us  consider  a  graph  constructed  from the  complete  3-partite 
graph  with  3p  vertices  and  the  three  independent  vertex  sets  {al . . . . .   ap},  {bl, 

. . . .   bp}, and  {cl,  . . . ,   cp} by deleting  all  edges  (bi,  ci)  joining  bi and  ci for  1  _< i  ___ 
p  and  all  edges  {ai,  bj}  and  {ai,  cj}  for  I  ~  i  _< p  and  1 < j   _  p.  This  graph  m ay be 

colored by  our  algorithm  in  the  vertex  sequence  alblc~a2  * * *  apbpcp,  and  the n  i t  
ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

Optimization of Parser Tables for Portable Compilers  *  557 

1 2 3 4 5 6  
e t 1 2 3 4 5 4  
sigmap 
1  2  3  4  5  6  eq  sigmap' 

0 0 0 1 0 1   1  0 ~ 0 01 0  
0  0  0  1 0   1  1  1  1  1 0 1 

1  1 1   0  1  0  2  1 1 0  0  0 
1  1  0  0  0  0  ~  3  0  1  0  0  1 
0 1 0 0 1 0   4 
0 0 0 1 0 1   1 
0 0 0 1 0 1   1 

1 1 1 0 1 0   2 

Fig. 5.  The  compression of sigmap corresponding to  the 
error  matrix  of  Fig.  2  with  the  help  of  two  vectors  et 
and eq. 

will use p  +  1 colors by assigning  the  colors  1, 2, 2,  1, 3, 3 , . . . ,   1, p  +  1, p  +  I  to the  vertices  in the  said sequence. 

Preordering  the  vertices  by  decreasing  degree,  as  proposed  in  [7],  will  not change  the  behavior  [21],  as  each  vertex  can  be  joined  with  an  appropriate 
number  of  1-degree  vertices  producing  the  above  order  al blc l  . . .   apbpcp  when applying  the  preordering  rule.  Also,  experimental  results  using  random  graphs 
with  up to  1000 vertices  [11]  showed no  significant  change  of the  approximation behavior when these  additional  heuristics  were  applied. 

In the  same way the  conjecture stated in  [7]--that  the  algorithm  with  decreas- ing  degree  ordering  will  "color  a  clique  with  an  almost  maximum  dimension 
first"--should  be  treated  cautiously,  because  an  infinite  number  of  counterex- amples  exists.  For  instance,  for  any  number p  _  3  a  graph  may  be  constructed 
with  2p  +  1 vertices containing  a  vertex with p  adjacent  vertices of degree  1 and joined  to  one  vertex  of  a  complete  graph  with p  vertices;  obviously,  a  2-clique 
will be colored first,  whereas  there  is also a p-clique. Nevertheless, despite this  bad behavior in some cases, the  average behavior for 

random  graphs  has  been  shown  to  be  quite  good for  such  approximate  coloring schemes  [20]. An even better result is achieved when coloring the collision graphs 
of the  investigated  parser  tables.  We  know  this  from  the  fact  that  these  graphs have  complete  subgraphs  with  nearly  as  many  vertices  as  the  number  of colors 
used  in  the  produced  colorings.  Therefore,  the  algorithm  can  be  recommended for use  in  the  Graph  Coloring  Scheme. 

3.3  Bir~ary Table Compression 

In the Graph Coloring Scheme and the Line Elimination Scheme a binary table 
sigmap is used for accessing the T-table. Since the compression may often save 

more  than  90 percent of the original storage requirement,  this binary table 
sigmap makes up an essential part of the total storage need. Fortunately sigmap 

has a quite uniform structure, which we can compress by merging identical rows 
and columns as shown in Figure 5. Joliat proposed this technique in [27]. Table I  gives the  improvement  for the  different  grammars. 

ACM Transactions  on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

558  *  P  Dencker, K  DOrre, and J  Heuft 

Table  I.  Binary  Matrix  Optimization 
I  sigmap  I  storage  I  opl  I  slorage  I 
language  I  table  I  need  I  table  I  need  I 

I  size  I  I  size  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I AD~  1381  *  96  I  4572  1137  *  72  I  1710  I 

I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I 
AL  1365  *  1361  6205  I121  *  69  I  1590  I I  I  I  I  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I 
6LGOLN  1128  *  67  :  1152  I  63  *  42  I  573  I I  I  I  I  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I 
6LGOL60  1129  *  58  I  1032  I  63  *  45  I  565  I 

I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I 
B6LG  I486  *  136I  8262  1151  *  73  I  2132  I I  I  I  I  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I EULER  I  62  *  75  I  620  I  34  *  32  I  273  I 

I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I 
LEX  1134  *  73  I  1340  I  62  *  46  I  579  I I  I  I  I  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I 
LIS  1330  *  I041  4290  1141  *  76  I  1844  I I  I  I  I  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I 
MINILEX  I  89  *  41  I  534  I  35  *  30  I  270  I I  I  I  I  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I 
F6SC6L  I209  *  62  I  1672  I  75  *  41  I  721  I I  I  I  I  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I XPL  90  *  46  I  540  I  44  *  28  I  312  I 

I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

For "sigmap and sigmap' the equation 

sigmap[q,  a]  =  sigmap'  [eq[q],  et[a]] 
holds  for all  q E  Q and  a  E  VT'. Note.  If we knew  the  insignificant  entries  in  the  original  T-table  (see  Section 

1.1) we could incorporate  them in s i g m a p   and apply one of the table compression 
methods  to it  as well. 

3 4   Row  Displacement  Heuristics 

Parser  tables  have  2-20  percent  significant  entries  for the  grammars  considered (see Table  III).  A  little  calculation  shows that  in  the  Row Displacement  Scheme 

it  is  advisable  to  replace  the  mapping  vector r o w i n d e x   used  for the  T-table  by 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

Optimization of Parser Tables for Portable Compilers  *  559 
the binary table  s i g m a p   as  needed in the  Graph  Coloring Scheme and the  Line Elimination  Scheme  (see Table  V).  But  by this  trade-off the  better  access-time 
behavior  of the  Row  Displacement  Scheme compared with  the  Graph  Coloring Scheme is  lost  (see  Figure  6).  In  our  implementation  of the  Row  Displacement 

Scheme we  incorporated  the  "first  fit  decreasing"  heuristic,  which was  already suggested in  [42]  and  analyzed in  [38]. 

3 5   Combination  of Compression  Methods 
As  the  resulting  tables  v a l u e   from  the  Graph  Coloring  Scheme  and  the  Line Elimination  Scheme may still have insignificant entries, a  further application  of 

one of the table  compression schemes may be  considered. Evidently, the  combi- nations  of  the  Line  Elimination  Scheme  with  itself  and  the  Graph  Coloring 

Scheme with itself will cause no improvement. It can be shown that a combination of the Graph Coloring Scheme with the Line Elimination Scheme will never give 
better  results  than  the  combination  of the  Line  Elimination  Scheme  with  the Graph Coloring Scheme if we have a  minimal coloring of the respective collision 

graphs.  Since the value table of the Graph  Coloring Scheme is not sparse enough to  allow  a  compression  by  the  Row  Displacement  Scheme  or  the  Significant 
Distance  Scheme,  only the  combinations  of the  Line  Elimination  Scheme with the  Graph  Coloring  Scheme  and  the  Line  Elimination  Scheme  with  the  Row 
Displacement Scheme are left for experimental considerations. Note  that  in  an  application  of  one  of the  combinations  to  the  T-table,  the 
binary  table  s i g m a p   used  in  the  Line  Elimination  Scheme  is  sufficient  for retrieval, and therefore the binary table  s i g m a p   in the  Graph  Coloring Scheme 

and  the  vector  r o w i n d e x   in  the  Row  Displacement  Scheme  are  superfluous. Additionally, we can combine the two mappings r o w m a p   and c o l u m n m a p  used 
in the Graph Coloring Scheme with the mappings r  and c in the Line Elimination Scheme. A similar concatenation can be applied to the mappings r o w p o i n t e r   in 
the Row Displacement Scheme and r  in the  Line Elimination  Scheme. These  savings  are  not  possible  in  a  combination  of  the  Line  Elimination 

Scheme with  the  Significant Distance  Scheme, and  experiments on  this  combi- nation  do  not  seem worthwhile.  In  practice the  combination  of the  Line  Elimi- 
nation  Scheme  with  the  Graph  Coloring  Scheme  has  another  advantage.  It  is much faster than the Graph Coloring Scheme alone (see Section 4.1) because the 

Graph Coloring Scheme has  only to be applied to the already compressed tables. 
3 6   Table Transposition 

Since  all  compression  methods  considered  give  the  parser  tables  an  arbitrary orientation  in  rows  and  columns,  we  can  apply  the  methods  to  the  transposed 

tables  as  well.  Evidently  we  get  no  different  results  for  the  Line  Elimination Scheme. For the  Row Column  Scheme and the  Suppressed Zero Scheme experi- 

ments with the transposed tables  never showed better results. 
3 7   Sophisticated  Extensions 
The  methods  mentioned  in  this  section  are  usually  found  in  the  literature  as improvements over the  basic  Row  Column  Scheme.  We  have  not  implemented 

these  methods  since their  efficient usage  is  limited to  machine-specific instruc- tions,  for  example,  bit-field  and  character-search  instructions.  The  trade-off 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

560  *  P. Dencker, K. D0rre, and J. Heuft 

between  saved  storage  and  efficient  access  needs  to  be  considered  anew  for 

different machines.  We  think  that  this  conflicts with  the  idea of portability. One  of these  methods  is to extract  default  values  from the  tables  as  suggested 

in  some  publications  [3,  4,  35].  For  the  nonterminal  part  of the  tables,  this  is easily  done  by  computing  for  each  nonterminal  the  most  frequent  entry  and 
eliminating  this  entry  in  the  column  of this  nonterminal.  Since  there  can  be  no error  when  accessing  the  nonterminal  table,  the  eliminated  entry  is  used  as 
default whenever the  searched entry  was  not  found. The  same  method  may  be  applied  to  the  terminal  table  whenever  there  is  a 
possibility of detecting  an  error  correctly  (see  [4]).  Obviously, the  methods  using the  binary  table  s i g m a p   have  this  property. 

Another  possibility is the  extraction  of default  values  of each  state.  Note  that in  the  terminal  table  the  default  action  for the  states  is  almost  always the  error 
entry,  if the  error  matrix  is not  factored out before extracting  the  defaults. To  save  superfluous  bits,  some  authors  propose  bit  ~torage  techniques  (e.g., 

[8]):  For  every vector  and  table  their  largest  entry  is  computed  and  the  vector (respectively, table)  is restored in a  smaller data structure where each entry needs 
as  many  bits  as  the  largest  entry  does.  However,  when  looking  for  a  fast  table access it  is  advisable to  use 8-bit  or  16-bit units  on  a  machine  where  these  units 
are  directly addressable. When  the  table  structures  are  examined,  it  is  observed that  there  are  groups 

of rows whose members have almost the  same structure.  Some of these rows have identical  structure  and  may  be  merged  easily when  doing  list  compression:  The 
starting  value  of the  identical  row,  which  is  already  stored  in  the  vector  v al u e, is assigned to the  corresponding field in the  vector r o w p o i n t e r .   If there  are rows 
with  only  a  few  different  significant  entries  saving  additional  space  may  be achieved  by  merging  the  identical  tails  of  the  rows.  Such  tails  are  merged  by 

compressing  one  row completely and  replacing  the  tail  of the  others  by a  link  to the  former.  If,  in  connection  with  default  values,  difficulties  arise  the  default 
treatment  may be incorporated  into  the  link  [35]. Joliat  [27]  proposes  a  strength  reduction  by linearizing  the  table  and  precom- 
puting  the  start  indices  of each  row,  thus  replacing  an  index  calculation  by  an additional  vector and  a  vector access.  If only significant  entries  are accessed, the 
leading  and  trailing  insignificant  entries  of  each  row  may  be  omitted.  The precomputed  indices then  point  to the  virtual  start  address  of the  corresponding 
row.  This  is  just  the  idea  of  the  simple  Significant  Distance  Scheme  that  is applied  here  to the  compressed tables. 

4.  RESULTS 

In this Section we give a short review on the behavior of the methods with respect to  the  complexity  of  the  compression  procedures,  the  amount  of  time  used  for 

access,  and  the  storage  needed  for  the  compressed  data  structure,  and  then provide  the experimental  results on the  storage requirements. 

4.1  Complexity  of Compression  Procedures 

Since the graph coloring problem is NP-complete, in practice only an approximate coloring algorithm  is possible.  When  using  the  algorithms  with  time  complexity 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October  1984. 

Optimization of Parser Tables for Portable Compilers  *  561 
O(n 2) given in Section 3.2, the generation of the graph from the table will be the 
most  complex part  of the  Graph  Coloring Scheme.  Therefore,  we  have  a  worst case complexity 

O ( n  *  m 2, m  *  n 2) and a best case complexity O ( m  2,  n2). 
We obtain the same worst case complexity for the Line Elimination Scheme if 
exactly one  row  (respectively,  one  column),  is  eliminated  in  each  scan.  In  the 
best case we have O ( m   *  n). In  the  Row  Displacement  Scheme,  the  worst  case  occurs  if none  of the  rows 

can  be  embedded  into  the  prefix  of  the  vector  v a l u e   constructed  from  the 
previous rows.  For each position  i in this prefix it may be  necessary to compare 

a  section of the prefix bounded by  i and n  +  i -  1 with the  actual row. Thus we have, 

O ( m  2  *  n 2)  comparisons  in  the  worst  case.  The  best  case  complexity is 
O ( m   *  n). The  simplest  version  of  the  Significant  Distance  Scheme  has  complexity 

O ( m   *  n).  The  additional  heuristic  reordering  of the  columns  suggested in  [5] has worst and best case complexity 

O ( m  2 *  n2). 
When the different schemes are applied to different grammars, our experimen- 
tal results show that the Line Elimination Scheme is by far the fastest, since the 
best  case  O ( m   *  n)  will  usually  apply.  In  spite  of having  a  higher  worst  case complexity,  the  Row  Displacement  Scheme  turns  out  faster  than  the  Graph 

Coloring Scheme, since the  Row Displacement Scheme approaches its best  case 
O ( m   *  n)  better  than  the  Graph  Coloring  Scheme  approaches  its  best  case O ( m  2, n2). 

In  fact,  the  Row  Displacement  Scheme  is  3  times,  and  the  Graph  Coloring 
Scheme  about  10  times  slower than  the  Line  Elimination Scheme  measured in CPU time on a  SIEMENS  7.760  using Pascal. Because the best case complexity 

of the  Significant Distance  Scheme with the  additional heuristics is O ( m  2 *  n2), 
the  execution  time  of the  generation  cannot  be  put  into  linear  relation  to  the 

other schemes, taking into account the size of the considered grammars. For the smallest grammar, the execution time was 50 times slower than the  Line Elimi- 

nation  Scheme.  In  the  case  of the  five  largest  grammars,  the  execution  time 
exceeded our resources  (more than  2 hours of CPU  time). 

4.2  Efficiency of Access  Procedures 
The  execution  time  needed  to  access  an  entry  in  a  compressed  table  mainly depends on the number of vector and matrix accesses. Additional time is needed 

for looking up  a  single bit instead of an addressable unit. Figure 6 gives an overview of the retrieval time measured in terms of accesses. 
For  each  of the  four  index  access  methods,  the  first  line  shows  the  number  of these  accesses  when  looking up  arbitrary  entries,  and  the  second  line  contains 
the number of accesses if only significant entries are examined. The column "Bit access  necessary"  indicates  whether  an  access  to  the  compressed  binary  table 
s i g m a p '   is  necessary  or  not  (see  Section  3.3).  Column  "Number  of  matrix accesses"  includes one  access  to  the  s i g m a p '   in  all  cases  where  a  bit  access  is 

necessary. The  last  column summarizes the  number  of conditions that  have  to be evaluated for the  access to an entry. For the  Significant Distance Scheme the 
access costs are given for the version with column permutation. 

For  the  Code  Reduction  Scheme,  one  add  operation  and  one-  (respectively, two-) vector accesses  have to be  added in all of the  four schemes. Obviously the 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

562  *  P. Dencker,  K. D0rre, and J. Heuft 
GCS 

Arbitrary 

Significant 

LES 

Arbitrary 

Number  of 

vector 
accesses 

2o r 4 

2 

Number  of 

matrix 
accesses 

l or 2 

1 

Bit access 

necessary 

Yes 
No 

Number  of 

conditions 

2-8  1 or  2  Yes  1-3 
Significant  3-6  0 or  1  No  1 or 2 

J  I  I  I RDS 

Arbitrary  2 or 4  0  No  1 

Significant  2  0  No  0 

I  J  I  I SDS 

Arbitrary  2 or 5  0  No  2 Significant  3  0  No  0 

Fig. 6.  Table access costs. 
Row Displacement  Scheme  guarantees  the  fastest  access followed closely by the Significant  Distance  Scheme. 

Detailed  considerations  [12]  show  that  the  Graph  Coloring  Scheme  is  3  to  4 times  slower  when  accessing  an  arbitrary  element  calculated  for  a  SIEMENS 
7.755  using  FO R TR A N   and  a  UNIVAC  1108 using  FO R T R A N   and  Assembler. This  result  is mainly due to the bad bit instruction  performance  of the  machines 
used. 

For  the  list  methods,  the  Row  Column  Scheme  and  the  Significant  Distance Scheme,  the  access  time  is  dependent  on  the  number  of significant  entries  per 

row.  In  the  worst  case,  the  Suppressed  Zero  Scheme  needs  twice  as  many comparisons  as the  Row Column  Scheme  (see Section  4.4 and  Tables  IV and  V). 
In  the  best case the  same  number  of comparisons  is  needed.  The  average  search list  length  in  a  running  compiler  depends  heavily  on  the  language  and  on  the 
manner  in  which  the  tables  are  constructed.  Therefore,  benchmarking  (see  [27] for XPL)  seems useful only for each  specific environment. 

4.3  Theoretical Storage  Requirements 
In  general,  the  minimal  storage  requirements  for  the  tables  will  differ  for  the different  compression  methods.  This  minimum  will  be  reached  for  the  Line 

Elimination  Scheme  and  for  the  two  list  search  methods  Row  Column  Scheme and  Suppressed  Zero  Scheme  in  each  application,  whereas  the  other  three 
methods  will  produce  results  not  necessarily  equal  to  their  minima,  depending on  the  order  in  which  the  rows  and  columns  are  treated.  The  generation  of the 

respective  minimum  of  each  of  these  methods  is  an  NP-complete  problem: Determinating  a  minimal  graph  coloring  is proved to  be NP-complete  [29],  and 
gaining  the  minimum  compression of the  Row Displacement  Scheme is also NP- complete  [15]. A  method  similar  to that  used in  [15]  will yield the  NP-complete- 
ness of the  Significant  Distance  Scheme. 

Evidently,  the  methods  described  are  approximation  methods,  and  there  is ample  room  for  additional  heuristics,  as  described in  Section  3.  In  certain  cases 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

Optimization of Parser Tables for Portable Compilers  *  563 
these heuristics may generate results arbitrarily far from the minimum. Experi- 

mental results on very sparse random tables (densities from 0.1 to 1 percent) 
under the stipulation that arbitrary elements are accessed [12] showed that the 

R o w   Displacement Scheme  is superior to the Graph  Coloring Scheme  with a 
factor decreasing with increasing density. In this interval of density, the R o w  

Column Scheme had results equal to the R o w  Displacement Scheme. In fact the 
storage need for the R o w  Column Scheme is a lower bound for the storage need 
of the R o w  Displacement Scheme in the version using rowindex. It is possible 
that the Graph Coloring Scheme and the R o w  Displacement Scheme yield similar 

results for the higher densities that parser tables have. The  Suppressed Zero 

Scheme has in the worst case the same storage need as the R o w  Column Scheme, 
but  less than  the  R o w   Column  Scheme  when  there are many  consecutive significant entries  in  T. 

4.4  Experimental  Results 
To compare the schemes presented and the combination of the Line Elimination 

Scheme with the Graph Coloring Scheme and the Line Elimination Scheme with 
the R o w  Displacement Scheme discussed in the preceding Section, we chose 11 

available LALR(1)-grammars  and applied the different methods to their parser 
tables. In Table II we give the characteristics of the grammars and in Table III 

the characteristics of the corresponding unoptimized parser tables. 

The experimental optimization, results are shown in Tables IV and V  and the 
comparison of the best results for each method  in Tables  VI and VII. The 
columns "storage need" in Tables IV and V include all access vectors, and Table 
V includes in addition the optimized sigmap where applicable. 

The results are expressed in bytes assuming a byte machine, that is, integer 
values in the range of 0 to 255 are stored in 1 byte, and values in the range of 0 
to 2 TM -  1 are stored in a 2-byte unit. Furthermore the number of bytes used for 

a row of a binary table with b bits will be (b +  7) div 8. In the presentation of 
the algorithms we have used ranges of integers with lower bound 1 for simpler 

notation; however, in our implementation we have used lower bound 0. 

The Code Reduction Scheme was also applied to all results shown in Tables 
IV-VII. O n  the one hand it adds to the access time of the tables (see Section 
4.2), but on the other hand it appears to cut the space requirements of the tables 
by half. 

From  Tables VI and VII the Graph  Coloring Scheme  emerges as the best 
compression method on the average. Even when we look at the results that lie 

20 percent above the minimum, for the T-table only the combination of the Line 

Elimination Scheme with the Graph Coloring Scheme is competitive with the 
Graph Coloring Scheme, whereas for the N-table we get a slightly scattered field 
led by the Graph Coloring Scheme. 

Those cases where the compressed tables could be generated by using the 
Significant Distance Scheme  (see Section 4.1) behave comparable to the R o w  
Displacement Scheme  for the N-table and slightly better for the T-table. The 

Line Elimination  Scheme  alone trails the other methods  in most  cases. In 
combination with the Graph Coloring Scheme it does not effect an improvement 
over the Graph Coloring Scheme alone, whereas in combination with the R o w  
Displacement Scheme there generally is a slight improvement. This improvement 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October  1984. 

564  P. Dencker, K. D0rre, and J. Heuft 

Table If.  Grammar  Characteristics 
I  I  I  Irighthandl 
language  I  product-lterminalsl  nonter-  I  side  I I  tions  I  I min~ls  I  length  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I ADA  I  370  I  96  I  170  I  782 

I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I AL  I  306  I  236  I  115  I  702  I 

I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I ALGOLN  I  146  I  67  I  69  I  272  I 

I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I ALGOL60  I  158  I  58  I  76  I  287  I 

I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I BALG  I  406  I  136  I  172  I  1045  I 

I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I 
EULER  I  125  I  75  I  50  I  196  I I  I  I  I  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I LEX  202  73  109  880  I 

I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I LIS  I  328  I  104  I  129  I  706  I 

I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I MINILEX  I  81  I  41  I  37  I  175  I 

I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I PASCAL  I  274  I  62  I  166  I  470  I 

I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I XPL  I  108  I  46  I  Sl  I  212  I 

I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

for the combination of the Line Elimination Scheme with the Row Displacement Scheme is  due to  the deletion of significant entries from the tables by the  Line 
Elimination Scheme (see Tables IV and V). The list-searching methods Row Column Scheme and Suppressed Zero Scheme 
are not competitive with the above methods (even in combination with the Code Reduction Scheme). Comparison with published results on list searching methods 

(see Figure 7)  leads us to the conjecture that  the  Row Column Scheme and the Suppressed Zero Scheme are  handicapped by the particular factorization of the 
parser tables  that  we have used.  In the  direct comparison of both  methods, the Suppressed Zero Scheme requires less storage. The  reason for this  is that  there 

are some consecutive significant entries in the parser tables.  On the other hand, the Suppressed Zero Scheme requires more comparisons of keys for access in the 

ACM  Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

Optimization  of Parser Tables for Portable Compilers 
Table  III.  Characteristics  of the  Nonoptimized  Parser  Tables 

*  565 

I  I  T-table  I  N-lable  I language  I  slates  I  storage  I  signlf.  I  X  of  sigl  storage  I  empty  I  signif.  I  x  of  sigl 
I  I  need  I  entries  I  entries  I  need  I  cogs  I  entries  I  entries  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  ,I  I AD~  I  381  73152  I  2325  6.36  ~I  129540  I  154  1925  2.97  ~I 

I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I  I  I  I AL 

I  365  I  99280  I  4193  I  8,45  Xl  83950  1  191  I  1716  I  4.09  g; I  I  I  I  I  I  I  I  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I  I  I  I ~LGOIAJ 

I  128  I  17152  I  1123  I  13.09  gl  17664  I  60  I  831  I  9.41  ~I I  I  I  I  I  I  I  I  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I ~LOGOL60  I  129  14964  I  928  I  12.40  Xl  19608  52  768  7.83  ~I 

I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I  I  I  I 
8~LG  I  486  I  132192  I  4326  I  6.55  Xl  167184  I  221  I  2098  I  2.51  Xl I  I  I  I  I  I  I  I  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I  I  I EULER  I  62  I  9300  I  1377  I  29.61  Xl  3100  I  22  I  726  23.42  gl 

I  I  I  I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I LEX  I  134  I  19564  I  1797  18.37  ~I  29212  I  51  1432  9.80  gl 

I  I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I  I  I LIS  I  330  68640  I  2408  I  7.02  ~I  85140  I  153  I  1527  I  3.59  gl 

I  I  I  I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I  I  I  I HINTT-EX  I  89  I  7298  I  762  I  20.61  Xl  3293  I  44  I  262  I  7.96  %1 

t ,  I  I  I  I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I P~SCAL  209  1  25916  1156  8,92  Xl  69388  77  794  2.29  Xl 

I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I  I  I  I XPL  I  90  I  8280  I  472  I  11.40  gl  4590  I  42  I  398  I  8.67  ~I 

I  I  I  I  I  I  I  I  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

worst  case,  as  can  be  seen  from  the  lengths  of  both  search  lists  (the  last  two columns  of Tables  IV and V). 

The  methods  Line  Elimination  Scheme  and  Row  Displacement  Scheme  are sensitive  to  the  different  structure  of the  T-table  and  the  N-table,  whereas  the 
Graph  Coloring  Scheme  and  the  Significant  Distance  Scheme  show  the  same optimization behavior for both tables.  For methods involving the Graph Coloring 

Scheme,  the  heuristics  for coloring columns before  rows is generally  ahead of the opposite order  heuristics.  Contrary to this  the  Row Displacement  Scheme  shows 
better  results  when  merging  rows  rather  than  columns.  The  only  exception appears  in  the  N-table  of XPL. 

Replacing  vector  r o w i n d e x   by  matrix  s i g m a p   in  the  Row  Displacement Scheme  results  in  saving  25-60  percent  of storage.  This  saving  is especially  high 
in  those  cases  where  the  elements  of the  vector  r o w i n d e x   have  to  be  stored  in more than  1 byte. 

The  fact  that  the  Graph  Coloring  Scheme  saves  more  space  than  the  Row Displacement  Scheme  is  contrary  to  the  results  for  very  sparse  random  tables 

ACM Transactions on Programming Languages and Systems, VoL 6, No.  4, October 1984. 

b~O~  ~r 

m g  - 

0 ~ ,   ~ 
.~-~_, _ _ _  

m +i..,  i o~Ii 

@  I 
g 

~;~  - - -  

g 
~.~  r, ~  m 

* ~  (R)(R)  ~ 

* ~r,  .q.  * ~"~  wr'~ 

e,l 

co ~P 
~m 

g 

"1 

~ r   ""  

~  g 

N ~ u  

g 
m~ 

,r,..  ~r,  a  ,  r',..  o 

O~  ugqp e,l,~ 
~'0  O0 

- - N u  

----u 

~r~ 
cOr~ 
mm 

~m 

CO0 
I" 

mm 

s 

I 
B 

.+,.+ 

,..+~ ~.,-+.  +m 

*  .~ 
cO 

o~ 
,'o~ 
or'3  o 

~  ""  ~  +;,.,+ 

8 

o 

m~++.. 

m 

m U] ~  "Cl 

g 
,-,o~ 

~ - -  

g 

g 

g 

g~ 

~.~ 

m~ 

p~l p.. 

g 
g 

~P,m. 

'PIp.,+ +,,+ Q~ 

,,,,., ,'m 

I~,I. 

p++.~ 
~u-J 

I  * 
'~'~0 

p..,~ 

g 

~ g 

O3 

O0 
~0 ~r 

_ N _  
=mm 

,p..~ 

g 

m 
N 

~ w  

m  m 
__~  __~ 

e< 

+,,+ ,,~. 
m ~  

e,l 

g 

.1 

0 0  

r. 
P+) 0"+ 

,,,.i U,,. I 

'+,,0 -- 
,,,.+ + 

~ m  

o ~  
e,,i ,~ 

,ve,  l 

0 
~0o 
e+,l~ 

m: 

m 

O~ 

m 

O~ 

O~ 

,-+ .++ 
~,+.+ * .+ ,.++ 

i++ 

t~ 

<"+  +,,,+ o~  ~ 
~  p,.,  (R) "= 

~  ,..~ r...) (,.) 

+..+ #--, #--, 

568  *  P. Dencker, K. DQrre, and J. Heuft 

Table VI.  Comparison of Results Concerning N-tables 
I  C~S  I  LF.~  I  RD~  I  SDS  I  LES&GCS  I  LES&RDS  I  I~CS  I  SZS  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I  I  I  I 
ADA  I  1.01  I  2.79  I  1.33  I  I  I  1.03  I  1.00  I  1.63  I  1.21  I I  2.4  Z  I  6.5  Z  I  3.1  ,z  I  I  2.4  Z  I  2.3  %  I  3,8  Z  I  2,8  Z  I 

:  I  I  I  I  I  I  I  I AL  I  1.00  I  1.49  I  1.29  I  /  I  1.04  I  1.18  I  1.56  I  1.13  I 
I  3.4  %  I  5.0  Z  I  4.3  %  I  I  3.5  Z  I  4.0  Z  l  5.2  lI  I  3.8  %  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   .  . . . . . . . . . . . . .  

I  I  I  I  I  I  I ALGOLW  I  1.01  1.32  ~..21  I  1.39  I  1.08  I  1.00  t  1.64  I  1.16  I 

I  7.1  Z  9.3  %  8.6  Z  I  9.8  Z  I  7.6  Z  I  7.1  Z  I  11.6  %  I  8.2  Z 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  '  '  I  I  ' ALGOL60  I  1.00  1  1.87  I  1.36  I  1.53  I  1.12  I  1.20  I  1.74  I  1.28  I 

I  5.7  Z  I  10.7  Z  '  7.8  Z  I  8.7  Z  I  6.4  Z  I  6.9  Z  I  9.9  %  I  7.3  %  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  :  I  I  I  I  I 
BALG  I  1.00  I  3.98  I  1.48  I  I  I  1.02  :  1.05  I  1.78  I  1.43 I  1.9  %  I  7.4  Z  I  2.7  Z  I  I  1.9  X  I  1.9  %  I  3.3  Z  I  2.6  % 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I  I  I  I EULER  I  1.39  I  I .18  I  2.08  I  2.21  I  1.18  I  1.00  I  3.31  I  2.07  I 

I  22.8  Z  I  19.4  %  I  34.0  k  I  36.2  Z  I  19.4  Z  I  16.4  Z  I  54.0  %  I  33.8  %  I 
I  I  I  I  I  I  I  I  I L, EX  I  1.04  I  1.79  I  2.12  I  2.01  ]  1.00  I  1.13  I  2.22  I  1.56  I 
I  4.8  %  I  8.2  %  I  9.7  %  I  9.2  %  I  4.6  %  I  5.2  %  I  11.5  Z  I  8.1  %  I 

,'  I  I  I  I  I  I LIS  I  1.00  I  2.77  I  1.99  I  I  I  1.10  I  1.22  I  2.21 
I  2.1%  I  5.8  %  I  4.2  %  I  ]  2.3  %  I  2.6  %  I  4.7  %  1  .84 3.9  % 
I  I  I  I  I  I  I  I MINILEX  I  1.00  I  1.55  I  1.08  I  1.30  1.14  I  1.10  i  1.50  I  1.13 
I  15.8  %  I  24.4  %  I  17.0  %  I  20.4  %  18.0  1~  I  17.4  %  1  23.6  %  I  17.8  Z  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  i  I  I  I  I  I  I  I PASCAL  I  1.00  I  1.96  I  1.45  I  I  I  1.26  l  I  .31  I  1.87  I  1.43  I 

I  1.8  1I  l  3.5  %  I  2.6  k  I  I  2.2  II  I  2.4  %  I  3.4  %  I  2.6  Z  I 

I  I  I  I  1  I  I XPL  I  1.00  1.37  I  1.54  I  1.36  1.02.  I  1.00  I  1.78  I  1.37  I 
I  13.2  %  18.2  %  I  20.4  %  I  18.0  %  13.4  %  I  13.2  %  I  23.4  X  I  18.0  %  l 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Average  I 

I  I  I  I  I  I  I factor  I  1.04  I  2.01  I  1.54  I  1.63  I  1.09  I  2.11  I  1.93  I  1.42  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

Note: 
For  each method the best result is drawn  from  Table  IV. 
For  each  grammar the results of the  different methods are given by the  factor 

storage  need  for  the  actual  method 

storage  need  for  the  best  method 

and  in the second line by the percentage  of the  remaining storage need. 

(see Section 4.3).  Reasons for this behavior are the higher densities of the parser tables  and  the  special  table  structure  which  supports  the  ability  of the  Graph 
Coloring Scheme to overlap coincident significant entries. 
4.5  Comparison with Published Results 

In  Figure  7  we  oppose  the  results  on  five  grammars  found  in  [3,  8,  27,  31,  37] with  our results  using the  Graph  Coloring  Scheme. From  this  figure we can  see 

that  the  Graph  Coloring  Scheme is  competitive  in  its  storage  requirement with the published list-searching methods.  It is superior in that  it allows the access to 
the  compressed tables  to  be  programmed  efficiently in  portable  high-level  lan- guages like Pascal  or FORTRAN. 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

Optimization  of Parser Tables for Portable Compilers 
Table VII.  Comparison of Results Concerning T-tables 

*  569 

I  GCS  '  LES  I  RDS  I  SDS  I  LES&GCS  I  LES&RDS  I  RCS  [  SZS  ; 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I  I  I  I l%De,  I  I .00 -  I  4.90  I  I .49  I  I  I  'I .00  I  I .45  I  1- .45  I  I .18  I 

I  6.1  X:  I  30.4  Y.  I  9.2  X  I  I  6.2  Y.  I  8.9  Y.  I  8.9  Y.  I  7.3  7.  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   ~. . . . . . . . .  

I  I  I  I  I  I  I  I  I PIT.  I  1.00  I  7.39  I  2  .77  I  /  I  !  ,08  I  2  .30  I  2  .80  I  3  .85  I 

I  4..I  :~  I  30.2  Y.  I  "-I.3  :4  I  I  4.4  :4  I  9.4  Y.  I  10.2  Y.  I  7.6  Y.  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I hLGOLN  I  1- .00  3.90  2.20  1.26  I  .07  I  2.13  2.09  1.6"L 

I  7.8  ~:  30.4  :4  17.1  :4  9.9  :4  8.4  :4  I  16.7  :4  16.3  ~:  12  ,5  ?. 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

ALGOL60  I  I .00  I  3.51  I  I .79  I  I .42  I  I .11  I  I .69  I  I .73  I  I .38 1  9.3  Y.  I  32.6  :~  I  '1,6.6  Y,  I  13.2  Y.  I  10,3  *  I  15.7  :~  I  16  I  Y,  I  12.9  Y. 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I  I  I  I BI%LG  I  1- .00  I  4.74  I  2.0S  I  I  I  I .04  I  2.16  I  I .80  I  1- .59  I 

I  6.0  :~  I  28.8  X  I  10.3  :4  I  I  6,2  :4  I  1_0.6  Z  I  9.6  :~  I  8.0  ~  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I EULER  I .00  1_ .99  I  2.37  1_ .55  1_ .05  I  1.  ,43  I  3 .~=8  I  2.29 

9.6  Z  1_9.1_  Y.  I  22.8  :4  14.9  :4  I_0.7  X  I  I-3,7  '/,  I 38.4  Z  I  22.0  :4 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

'  I  '  I  .  , L]~X  I  '1..00  I  3.22  I  2.69  I  2.06  I  1- .09  I  1. ,72  I  2 .67  I  1- .87  I 

I  8.0  :4  I  25.8  ~:  I  21.6  ~  I  1-6.5  ~  I  8.7  Y.  I  13,8  Y.  I  21.4  :4  I  15.0  Y.  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I  I  I LIS  I  1..DO  I  4.74  I  1- .66  I  /  I  I  .01  I  1,88  I  I  .52  I  1.34 

I  6.1  :4  I  29.0  Y.  I  10.1  ~  I  I  6.2  :c  I  11,5  :4  I  9.3  :4  I  8.2  Y. 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I HINILEX  1- .04  2.57  I  I .88  I  1- .63  I  1- .00  I .49  2 .06  I .49 

1,3.0  X  32.1  :4  I  23.5  :4  I  20.4  :4  I  12.5  Y.  '18,6  X  25.8  Y.  18.6 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

l  I  I  I  I  I  I  I  I PhSCPtL  I  ~..OO  I  3.06  I  1- .73  I  /  I  I  .10  I  1- .48  I  1- .72  I  1  .41-  I 

I  7.3  ~  I  22.2  :4  I  1-2.6  :4  I  I  8.0  :4  I  10.7  Y.  I  12.5  Y.  I  1(].3  Z  I 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

I  I  I  I  I  I  I  I XI=L  I  ~, .O0  I  2  .23  I  ~..46  I  1- .45  l  1- .0"1  I  1- /33  I  1- ,50  I  1- A8 

I  10.7  X  I  28.9  ~  I  1_5.7  Y.  t  '15.5  ~  I  1-0.8  :4  I  1-4.3  :4  I  16.1  :~  I  12.4 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

.~verege  I  I  I  I  I  I  I  I  I factor  I  1- .O0  I  3.84  I  2  .01  I  1- .56  I  '1.06  I  1..73  I  2  .07  I  '1.56  I 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

Note: 
For each method the best result is drawn from Table V. 
For each grammar the results of the different methods are given by the factor 

storage  need  for  the  actual  method 

storage  need  for the  best  method 

and in the second line by the percentage of the remaining storage need. 

Reference 

[30] [8] 

[3] [35] 
[26] 
GCS 

ALGOLW 

2434 1689 
2593 

ALGOL60 

2821 

2512 

Euler 

1606 

1401 

XPL 

1250 

1182 
1836 
1493 

Pascal 

m 
1741 

3135 
Fig. 7.  Storage  comparison  with published  results (in bytes). For the N-table  of Euler LES  &  RDS 
was counted. 

ACM  Transactions on Programming  Languages and Systems, Vol. 6, No. 4, October 1984. 

570  *  P. Dencker, K. D0rre, and J. Heuft 
5.  CONCLUSION 
We   have presented six compression methods for sparse tables and investigated 
their applicability to parser tables. Four of them are index access methods, which 
are suitable for portable compilers. The  other two are list-searching methods, 
which have been included for their good reputation in storage optimization. In 

our experimental work they served as a reference for the comparison of the index 
access methods. 

From  our  experimental  results we  can  conclude  that the  Graph  Coloring 
Scheme turned out best. Therefore we recommend this method for use in parser 
generating systems. During the development phase of a compiler where the parser 
generator is used frequently, it is advisable to choose the combination of the Line 
Elimination Scheme with the Graph Coloring Scheme because it is an order of 

magnitude  faster than  the  Graph  Coloring  Scheme  alone. For example,  the 
generation of the Ada tables with the Graph Coloring Scheme and column-row 
coloring took 500 C P U   seconds on a S I EM E N S  7.760 in contrast to 55 CP U  
seconds for the same tables using the Line Elimination Scheme with the Graph 
Coloring Scheme  and column-row  coloring. The compressed tables need 7569 
bytes of storage using the Graph Coloring Scheme and 7658 bytes using the Line 

Elimination Scheme with the Graph Coloring Scheme. For X P L  the correspond- 
ing time and space requirements were 17 C PU   seconds versus 1.9 C PU   seconds and 1493 bytes versus 1513 bytes. 

Contrary to the development phase of a compiler, in a production compiler the generation time  for the  tables  is  negligible  compared to  the  access time  to  the 
table entries, which is twice as fast for the  Graph  Coloring Scheme than for the Line Elimination Scheme in combination with the Graph Coloring Scheme. 

Whether it is useful to add one of the sophisticated extensions to the schemes depends on the weight of three different aims: portability, fast access, and saved 
storage. It is our opinion that the Graph Coloring Scheme alone or in combination with the Line Elimination Scheme best meets these requirements. 

ACKNOWLEDGMENTS 
We  wish to thank the referees for their helpful criticism and G. Goos, H.  Mfiller, J.  R6hrich,  F.-P.  Schmidt-Lademann,  Y.  Shimamoto,  and  G.  Wrightson  for 

valuable discussions and comments. 

REFERENCES 

1. Reference Manual for The Ada Programming Language.  Proposed Standard Document. U.S. 

Department of Defense, July 1980. 
2. ANDERSON,  T.  Syntactic analysis of LR(K)  languages. Ph.D. dissertation, Univ. of Newcastle 

upon Tyne, England, 1972. 
3. ANDERSON, T., EvE, J., AND HORNING, J.J.  Efficient LR(1) parsers. Act. Inf. 2 (1973), 12-39. 
4. AHO, A.V., AND ULLMAN, J.D.  Principles of Compiler Design. Addison-Wesley,  Reading, Mass., 

1977. 
5. BEACH, B.  Storage organization for a type of sparse matrix. In Proceedings of the 5th South- eastern Conference on Combinatorics, Graph Theory and Computing, 

(Feb. 25-Mar. 1, Winnipeg). 
Florida Atlantic Univ., Boca Raton, 1974, pp. 245-252. 
6. BELL, J.R.  A  compression method for compiler precedence tables. In Proceedings of the IFIP Congress  74, 

Booklet  2, (Aug., Stockholm).  Elsevier North-Holland,  New  York,  1974, pp. 
359-362. 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

Optimization of Parser Tables for Portable Compilers  *  571 
7.  BRI~LAZ, D.  New methods  to  color the vertices  of a  graph.  Commun.  ACM 22,  4  (Apr.  1979), 251-256. 
8.  BURGER, W.F.  A  parser  generation  tool  for  micro  computers.  COMPSAC  3  (Nov.  1979), 631-634. 
9.  DENCKER, P.  Ein neues LALR-System.  Diploma  thesis,  Fak.  f. Informatik, Univ. Karlsruhe, Karlsruhe, W. Germany, 1977. 
10.  DENCKER, P., DORRE, K., AND HEUFT, J.  Optimization of parser tables for portable compilers. Int. Bet.  22/81, Fak. f. Informatik, Univ. Karlsruhe, Karlsruhe, W. Germany, 1977. 
11.  Dt~RRE, K.  Approximate  Algorithms for Coloring Large Graphs.  In Datenstrukturen, Graphen, Algorithmen,  J. Miihlbacher, Ed. Hanser Verlag, Munich, 1978, pp.  192-203. 
12.  Dt~RRE, K., AND FELS, G.  Efficiency of Sparse  Matrix Storage  Techniques. In Discrete  Struc- tures and Algorithms,  U. Pape, Ed. Hanser Verlag, Munich, 1980, pp. 209-221. 
13.  DORRE, K., GOOS, G., AND SCHMITT, A.  Minimal representation of LR-parsers.  Unpublished Manuscript, Fak. f. Informatik, Univ. Karlsruhe, Karlsruhe, W. Germany, 1976. 
14.  Dt~RRE, K.,  HEUFT, J.,  AND M(~LLER, H.  Worst and best case  behaviour of an approximate graph coloring algorithm.  In Proceedings  o[ the 7th Conference  on Graphtheoretic  Concepts  (WG 

81), (Linz, Austria, June 15-17). Hanser Verlag, Munich, 1982, pp. 339-348. 15.  EVEN, S.,  LICHTENSTEIN, D.I.,  AND SHILOAH, Y.  Remarks  on  Zeigler's  method  for  matrix 

compression. Unpublished manuscript, 1977. 16.  FELS, G.  Effizienzuntersuchungen von Speicherungsmethoden  fiir d/innbesetzte Matrizen.  Di- 
ploma thesis, Fak. f. Informatik, Univ. Karlsruhe, Karlsruhe, W. Germany, 1979. 17.  GAREY, M.R., AND JOHNSON, D.S.  Computers  and Intractibility.  Pitman, San Francisco,  1979. 
18.  Goos,  G.  Die Programmiersprache  LEX.  Int. Ber.  1/75, Fak. f. Informatik, Univ. Karlsruhe, Karlsruhe,  W. Germany, 1975. 
19.  Goos, G.  Die Programmiersprache  BALG, vorl~iufige Fassung. Int. Bet. 6/75, Fak. f. Informatik, 

Univ. Karlsruhe, Karlsruhe, W. Germany, 1975. 20.  GRIMMET, G.R., AND MCDIARMID, G.J.H.  On colouring random graphs. Math. Proc. Cambridge 

Phil.  Soc.  77 (1975), 313-324. 21.  HEUFT, J.  Untersuchungen fiber das  Approximationsverhalten von Graphenfiirbungsalgorith- 

men. Diploma thesis,  Fak. f. Informatik, Univ. Karlsruhe, Karlsruhe, W. Germany, 1981. 22.  ICHBIAH, J.D.  LIS Reference Manual. UB D Dv WS SP31, Siemens AG Miinchen 70, 1978. 
23.  JAKOB, W.  Entwurf und Implementierung eines Compilers fiir eine h6here Manipulatorsprache zur  Programmierung eines  Roboters.  Diploma  thesis,  Fak.  f.  Informatik,  Univ.  Karlsruhe, 

Karlsruhe, W. Germany, 1980. 24.  JENSEN, K., AND WIRTH, N.  PASCAL User Manual and Report.  Springer Verlag, New York, 

1978. 25.  JOHNSON, D.S.  Worst case behavior of graph coloring algorithms.  In Proceedings  o/the 5th S. 
E.  Conference  on  Combinatorics,  Graph  Theory,  and  Computing,  (Feb.  25-Mar.  1,  Winnipeg). Florida Atlantic Univ., Boca Raton, 1974, pp. 513-527. 
26.  JOHNSON, S.C.  YACC--Yet another compiler  compiler.  CSTR32,  Bell Laboratories,  Murray Hill, N.J.,  1975. 
27.  JOLIAT, M.L.  On the reduced matrix representation of LR(K) parser tables. Tech. Rep. CSRG- 28, Univ. of Toronto, Canada,  1973. 
28.  JOLIAT, M.L.  Practical minimisation of LR(k) parser tables. In Proceedings of the IFIP Congress 1974, (Aug. Stockholm).  Elsevier North-Holland, New York, pp. 376-380. 

29.  KARP, R.M.  Reducibility  among combinatorial problems.  In Complexity  of Computer Compu- tations,  Miller, R.E., and Thatcher, J.W. Eds. Plenum Press,  New York, 1972, pp. 85-103. 
30.  KASTEN, U.  Die Beispielprogrammiersprache  MINILEX. Internal working paper, Fak. f. Infor- matik, Univ. Karlsruhe, Karlsruhe, W. Germany, 1978. 
31.  LALONDE, W.R.  An efficient  LALR parser generator.  Tech.  Rep. CSRG-2, Univ. of Toronto, Canada,  1971. 

32.  MCKEEMAN, W.M.,  HORNING, J.J.,  NELSON, E.C.,  AND WORTMAN, D.B.  The XPL compiler generator  system.  In  Proceedings  o[ AFIPS  Fall  Joint  Computer  Conference,  (San  Francisco, 

Calif., Dec. 9-11), vol. 33. AFIPS Press,  Reston, Va., pp. 617-636. 33.  MUJTABE, S.,  AND  GOLDMAN, R.  AL  user's  manual.  Tech.  Rep.  Stan-Cf-79-718,  Stanford 
Artificial Intelligence Laboratory,  Stanford,  Calif., 1979. 34.  NAUR, P.  (EDITOR)  Revised  report  on the  algorithmic  language  ALGOL60. Numer.  Math.  4 

(1963), 420-452. 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 

572  *  P. Dencker, K. D0rre, and J. Heuft 
35. PAGER, D.  Eliminating unit productions from LR parsers. Act. In[. 9 (1977), 31-59. 
36. POOLE,  P.C.  Portable and Adaptable Compilers. Lecture Notes in Computer  Science, vol. 21. 

Springer Verlag, New York, 1974. 
37. TEWARSON,  R.  Row column permutation of sparse matrices. Comp. J. 10 (1967/68), 300-305. 
38. TARJAN,  R.E., AND  YAO,  A.C.  Storing a sparse table. Commun.  ACM  22, 11  (Nov.  1979), 

606-611. 
39. SCHMITT,  A.  Minimizing  storage space of sparse matrices by graph coloring algorithms. In Graphs, Data Structures, Algorithms. 

Nagl, M., and Scheider, H.-J., Eds. Hanser Verlag, Munich, 
1979, pp. 157-168. 
40. WIRTH, N., AND HOARE, C.A.R.  A contribution to the development of ALGOL.  Commun. ACM 

9, 6 (June 1966), 413-431. 
41. WIRTH, N., AND WEBER,  H.  Euler: A  generalization of Algol, and its formal definition: Part I. Commun. ACM 

9, 1 (Jan. 1966), 13-23. 
42. ZEIGLER, S.F.  Smaller faster table driven parser. Unpublished manuscript, Madison Academic 

Computing Center, Univ. of Wisconsin, Madison, 1977. 

Received April 1982; revised February and October 1983; accepted December  1983 

ACM Transactions on Programming Languages and Systems, Vol. 6, No. 4, October 1984. 