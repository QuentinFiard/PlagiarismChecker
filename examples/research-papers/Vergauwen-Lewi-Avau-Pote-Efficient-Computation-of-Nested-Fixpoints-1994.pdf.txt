

Efficient  Computation  of  Ne s t e d  Fix-Points, with  Applications  to  Model  Checking 

B.  Vergauwen,  J.  Lewi,  I.  Avau,  A.  Potfi 
K.U.Leuven,  Department  of Computer  Science, 

Celestijnenlaan  200A,  3001  Leuven,  Belgium 

A b s t r a c t .   The paper  presents  a general algorithm  for computing  nested 
fix-points  over  complete  lattices  of finite  height.  The  method  presented 

relies  on  techniques  familiar  from  the  realm  of functional  programming languages,  such  as  e.g.  lazy  evaluation.  The  algorithm  is  constructed  in 

a  stepwise  fashion:  We  start  with  a  schema  based  on  some  simple  facts 
of fix-point  theory.  As  such  this  schema  is  easily  seen  to  be  correct.  It is,  however,  rather  inefficient.  We  then  trace  the  sources  of inefficiency 

and  refine  the  basic  schema  resulting  in  a  correct  and  more  efficient 
algorithm.  After  presenting  the  general  algorithm,  we apply it,  by means 
of illustration,  to  the  field  of model  checking. 

1  P r e l i m i n a r i e s :   B l o c k s  
Assume  a  set  l/  of  values  and  a  partial  order  relation  E  on  P  such  that 

s  =  (V, E)  forms  a  complete  lattice  of finite  height 1.  The  b o t t o m   element  of 
s  is  denoted  by  _1_, the  top  element  by  T.  Also  assume  a  (countable)  set  X  of 
variables. 

E n v i r o n m e n t s   An  envirortment  0  : X ~  l / m a p s   variables  into  values.  Th e  set 
of environments  is  denoted  by  O.  Updating  of environments  is  as  foltows: Define O[z ~  v] 

as  the  environment  0'  that  agrees  with  0  except  that  z  is  u pdated   to  v, 
i.e.,  O'(z)  =  v  and  0'(y)  =  0(y)  for  y  #  z.  Furthermore  for  p :   ( z l , . . . ,   z l }   --* 1/ 

define  O[p] as  the  environment  0[Zl  ~  p(zl)]  . ..  [z,~ --* p(z,~)].  The  partial  order 

E  can  be  extended  pointwise  towards  environments  in the  usual  manner:  01  E  02 
i~T ex(~)  E  e~(z)  for  every  z  ~  x. 

E q u a t i o n s   An   equa~iort is  of the  form  (z u__ f)  where  ~  E  X,  ~  E  {O,  |  and 
f  :  O  ~  1/is  a  mon oton e  (isotone)  function,  i.e,  f(01)  E  f(02)  for  every  01,  02 
such  th at  01  E  02.  Let  E  =  (z  = ~  f)  be  an  equation.  Given  an  environment  0  for 
interpreting  variables  other  than  z,  one  can  solve  E  for  z.  A  value  v  E  1/  is  a solution 

of E  iff v  =  f(O[z  H  v]).  As  f  is  monotone,  E  has  at  least  one  solution. 
There  even  is  a  unique  least  solution  as  well  as  a  unique  greatest  solution  [T]. 

The  role  of a  is  precisely  to  specify  which  of both   extremal  solutions  is  meant:  A 

O  label  denotes  the  least  solution,  a  @ label  denotes  the  greatest  solution.  In this 

1 I.e.,  there  do  not  exist  infinite  sequences  of the  form  vl  t-  v2  E  va  E  ...  or  vl  -7 

v2  -3 va  ~  ... 

166 
paper  we  are  not  interested  in  solving  single  equations  but  in  nested  equations. 
The  term  Block is  used  to  refer  to  a  list  of nested  equations. 

B l o c k s   The  set  B  of Blocks  is  defined  inductively  as  the  least  set  such  that: 

-  n i l   6  B,  a n d / v ( n i l )   =  0  (lv for !eft hand  side  variables) 
-  Let  S  6  B  and  let  E  =  (z  ~  f)   be  an  equation  such  that  z  r  lv(S).  Then 

E  fi9 B  is  also  in  B,  and  lv(E  * B)  =  {z} U lv(B). 

Note  that  all  left  hand  side  variables  of  a  Block  are  required  to  be  distinct. 
This  is  for  technical  reasons  only.  A  Block  can  be  seen  as  a  recursive  definition 
for  its  left  hand  side  variables.  The  order  of the  equations  determines  the  rela- 

tive  importance  of variables.  The  role  of the  @/@  labels  is  as  explained  above. 

Formally  the  semantics  of Blocks  is  defined  by  the  following semantic  function 

[ - I I - ] : ( B x e )   -4  e :  

-  Enil  II 6]  =  0 
-  [ < ~   :>~  II o] =  ~B  II O[z H  #,f'2]] where , n  is the least fix-point of ~  =  ~,, e ~:.:([B II o[~ .-. ,,] ]I) 

-  I[<~ ~  :> oB   II o] =  lIB II o[~ ~ ,~ ] ] where ~,n is the greatest ~ix-point of ~  =  :~,, e V.:([B  II 0[~ ~  ,,]]) 

Note  that  the  above  definition  is  sound:  As  the  right  hand  side  functions  of 
Blocks  are  monotone, functional  1"2 is  also  monotone and  hence  unique  extremal 
fix-points  of 1"2 do  exist  IT]. 

The  following definitions  will  prove  useful  later  on.  For  B  6  B  and  z  6  lv(B) 
define: 

-  eq(B, z)  as  the  equation  of B  having  z  as  its  left  hand  side  variable,  i.e., 

eq(((z  ~  f )  *  B ' ) , z )   =  <z  ~  f),  and  eq((<y  ~  f)  *  B'),z)  =  eq(B',z)  if 

-  BY  as  the  Block  obtained  from  B  by  stripping  off top-equations  until  an 

equation  carrying  a  different label is encountered,  i.e., n i l   V =  (E * n*  = 

~  ~  al  a2 nil, (<~1  f1>'(~2  f2)'B')V  ---- ((z2  f2).B')V, and ((~i =  fl).(z~  -- 

f2)  fi9 S')V   =  (z2  ~  f2>*B'  ift~l  #or2. 
-  tv(B)  as  the  set  of toplevel variables  of B,  i.e., tv(S)  =  Iv(B)  \  lv(BV). -  nd(B,  z) 

as  the  nesting  depth  of z  in  B  (top-level variables  have  a  nesting 
depth  of 1),  i.e.,  nd(S,  z)  --- 1 if z  6  ~v(B),  and  nd(S,  z)  --  1 + nd(BV,  ~)  if z  ~  tv(S). 

-  B \  $  as  the  Block obtained from B  by deleting the  equation associated  with 

z,  i.e.,  ((z  ~  f)  fi9 B')  \  z  =  B',  and  (<y--a  f > .   B')  \  z  =  (y =~ f)  fi9 (B'  \ ~ )   if y # ~ .  

167 
2  Goals  and  O ve r v i e w  

The  primary  goal  of  this  paper  is  to  construct  an  efficient,  general  algorithm 

SOLVE(B,0, z)  for  computing  component  [ B   ]l 0~(z).  We  shall  concentrate  on sparse 

Blocks,  i.e.  Blocks  whose  right  hand  side  functions  are  sparse.  Sparseness is  a  rather  qualitative  notion:  A  function  f  :  O  -~  Y  is  said  to  be  sparse  if for 

'most'  environments  0  only  a  'small'  fragment  of 0  is actually  needed  in  order  to 
compute  f(O). The  construction  of  SOLVE proceeds  in  a  stepwise  fashion:  In  section  3  we  first 

give  the  basic  schema  for  SOLVE. This  basic  schema  is  obviously  correct  (by 
construction)  but  it  is rather  inefficient.  In  sections  4 and  5 we then  subsequently refine  this  schema  resulting  in  a  correct  and  efficient  algorithm.  In  section  6  the 

algorithm  is  applied,  by  means  of illustration,  to  the  field  of model  checking. 

3  Basic  Schem a  for  SOLVE 
The  following  simple  facts  form  the  basis  for  computing  [ B   II 0](~). 

(1)  V B 6 B , O 6 0 ,   z 6 X   :  z ~ l v ( B )   ~  [BllO~(z)=O(=) 
(2)  Let  B  6  B  and  let  zl,  z~  6  Iv(B)  such that  rid(B, zl)  =  rid(B, z~).  Let  B'  be 

the  Block  obtained  from  B  by  switching  equations  eq(B, zl)  and  eq(B, z2). 
Then  liB  11 0]  =  [ B '   11 0~  for  any  0  6  tO. (3)  V B 6 B , O e O ,   z e l v ( B )   :  [B  IlO~=[B\z  I[O[z~[B IlO](z)]l 

Based  upon  the  above  facts,  [ B   ]] 8~($)  can  be  computed  along  the  following 
lines: If z   ff lv(B)  then  obviously, by fact  (1),  ~B  II O~(z)  =  O(z). Ifz   6  Iv(B)  and nd(B, 

z)  =  1 then  z  is a  top-level  variable  of B.  Let  eq(B, z)  =  (z  ~- f).  Assume 
that  (r  =  O.  From  fact  (2)  together  with  the  definition  rule  for  [ i  11 -]  it  then 
immediately  follows that  I S   tl 0 ~(z)  =  ]~12, where  12 =  Av  6  ];.f([  B  \  z  I] 0[z  ~-* v]~).  As  the  lattice  s  is  of  finite  height,  gf2  is  effectively  computed  as  the 

limit  point  of the  chain  I2~  E  121(_s  E  122(1 )  _  123(A_)  E  ...  where D~  =  y  and  f2i+l(y)  =  O(12i(y)).  In  case  c~ =  @ we compute  u12  as  the  limit 
point  of  the  chain  12~  _~  OI(T)  _  122(T) ~_  123(T) _  ... Finally  if z  6  Iv(B) 

but  nd(B, z)  >  1  then  we  first  compute  the  values  of  the  top-level 
variables  of  B  (as  explained  above),  we  then  eliminate  these  variables  from  B using  elimination  fact  (3)  and  then  compute  the  z-component  of the  simplified 

Block. 
The  basic  schema  for  SOLVE is  listed  below.  We  assume  a  function  EVAL for 

evaluating  right  hand  side  functions  of  Blocks.  (Right  hand  side  functions  are assumed  to  be  computable  in  finite  time). 

168 
function  SOLVEI(B, O, z)  =  aolvol(B, O, z) -- 

return  IB  II 0](~ ) 

function  solvel(B,  O, z)  = --  return  ~ B  I[ O](z) 
begin 

case l  fi9 r  Z~(B)  : return  0(~) 

fi9 fi9 lv(B)  and  nd(B,z)  =  1: 

Let eq(B, z)  =  (z  -~ f) v~  :=  if  a  =  6)  then  _L else  T  fi 

loop 

-- compute f([[B \ z  II 0[~ ~  ~o~,35) 0'  :=  Ay 6  X.solve,(n \  z, O[z ~  v,~,], y) 

v.~.~ := EYAL(f, 0') 
if v~.  =  v~.~ then  exit loop ii 
endloop return  you, 

p := Ay 6  tv(B).,olvel(B,O,y) 
endcase end 

T h e o r e m   SOLVEI(B, O, z)  correctly  computes  ~B  H 0~(z) 
The  nice thing about  SOLVE1 is that  it is extremely simple because  it  was directly 
derived  from  a  few  simple facts.  On  the  other  hand  it  is  impractical  because  it 

is  also  extremely  inefficient,  the  worst-case  running  time  being  exponential  in 

I/v(B)  I. There  are  two  main  sources  of inefficiency: 

1.  Needless  eomputagion  :  As  Blocks  are  sparse,  lots  of  information  may  be 

computed  that  is  actually never  used. 2.  Needless  re-computation 

:  The  same/related  information is  computed  more 
than  once. 

In  the  next  section we refine SOLVE by eliminating needless  computations.  Need- 
less  re-computation  is addressed  in  section  5. 

4  A v o i d i n g   N e e d l e s s   C o m p u t a t i o n  
As  Blocks  are  assumed  to  be  sparse,  lots  of information  that  is  computed  by 

solve1  may turn  out not to be needed  at  all. The  key idea to avoid such needless 
computation is to postpone the computation of information until this information is  actually  needed  (i.e.  call  by  need).  In  this  way no  time  is  wasted  computing 

'irrelevant' information. There  are  two points in s o l v e l   where postponing comes 
into play. We discuss  each  in  turn. 

169 
L a z y   e v a l u a t i o n   o f  f(O').  Consider  the  following code  fragment  of solve1: 

--  compute  f([B  \  z  II o[~  ~+ ~,=4 ]1) o' 

:=  A~  ~  X.,ol~o,(B  \  ,,0[~   ~+ ~~ ~,,,.~ 

:=  XWL(/,  o') 

--  ~,~.~  =  .f([B  \  fi9  li  0[~  ~  ,,o,~.] ]) 

As  f  is  sparse,  in  general  only  a  small  fragment  of  8 ~ will  actually  be  needed 
in  order  to  compute  f(O I).  Hence,  instead  of first  computing  all  components  of 

Y  and  thereafter  evaluating f(O'),  it  seems  worth-while  to  integrate  both  steps, 
i.e., to evaluate f(O') in a  lazy (demand driven)  way. In order to do this, function 
EVAL is  slightly extended  in  the  following way:  We  allow  the  second  argument 

of EVAL to  be  a  pargial environment  p  : Dorn(p) --~ Y  with  Dora(p) C  X. If the 
values  of variables  from  Dorn(p)  suffice  to  evaluate  f,  then  EVAL(f, p)  returns 
the  desired  value;  Otherwise  a  variable  y  6  (X \  Dam(p))  is returned,  indicating 
that  the  value  of y  is  needed  in  order  to  evaluate  f.  Lazy evaluation of f(O')  is 

now implemented as follows (e denotes the empty environment, i.e. Dora(e) =  @): 

--lazy  computation of f ( [ B   \  z  II e[~ ~  ~=~,3 ]) p :=  ~  ~  :=  EVtL(.f,  p) 

while  r  ~  12 do 
od 

D e l a y e d   c o m p u t a t i o n   o f p .   Consider the following code fragment of solvel: 
(*)  p := Ay 6  tv(B).solvol(B,O,y)  solvol(BV,O[p],z) 
It  makes little  sense  to  compute  all components  of p  a  priori.  Instead  we  would 

like to  delay the  computation of p-components  until they are  actually needed  by 

solvel(BV,  O[p], z).  It  is  however  unclear  at  what  points  p-components  may be 
needed  during  execution  of  solvel(BV,  O[p], z),  if they  will  ever  be  needed  at 
all.  Therefore  in  order  to  delay  the  computation  of p,  we  have  to  keep  track  of 
the  cor~gez$ in which components of p  should be  computed when needed  later on. 
To keep  track of this context information, we will use extended  environments (~) 

instead  of ordinary  environments.  Extended  environments  differ from  ordinary 
environments in that  a variable may also be bound to a  cont.ez~ (closure) which is 
just  a  tuple consisting of a  Block together  with, again, an extended environment. 

D e f i n i t i o n   The  set  E  of extended  environments is  the  smallest  set  such  that 

-  ec_s, 
-  Let  (  E  ~,  ~  6  X, v  6  Y.  Then  ([z  ~-* v]  is  also  in  .T, 

170 
-  Let f l , f 2   e  ~ , z   E X,B  E  B.  Then  fl[~  ~-~ (B,~2)]  is  also in  ~. 
N o t e   Because  of the  possible  nesting  of equations,  delayed computations  can 
be  nested  too. This is why in fl[Z ~  (S, f2)]  we must allow ~2 to be an  extended 
environment  and  not just  an  ordinary  environment. 

Using extended  (~) instead of ordinary environments  (0),  code fragment (,) be- 
comes: 

solve~ (B V, ~[~v(B) ~-~ (B, ~)], z) 
Eliminating needless computation as explained above, function SOLVE1 is refined 
into  the  function  SOLVE2 listed  below. 

N o t a t i o n   Wc  shall  simply  write  [ B   H ~]  instead  of  [ B   H ~],  where  ~  is  the ordinary  environment  'equivalent'  with  f,  i.e.,  0  =  6,  f[z  ~  v]  =  ~[z  ~  v],  and 
~1[~  ~  (B,~,)]  =  ~ [ ~   ~  [B  II ~  ](~)]. 

f unetlo n  SOLVr~(B,  8, .)  =  . o l v . , ( B ,   O, .)  --  return  [ B   fl e l ( .)  
function ,olvo=(B, ~, z)  = --  re tu rn   [B  II ~](z) 

begi n 

case z  ~_ Iv(B)  and ~(z)  e  l~:  re tu rn   ~(z) 

z  *  Iv(B)  and ~(z) _-- (Ba, ~d):  solve2(Ba, ~d, z)  --  compute delayed value z  e  Iv(B)  and rid(B, z)  =  1 : 

Let  eq(B, z) =  (z ~ f> 
v=~   :=  i fu   =  @  t hen  *  else  T  fi loop 

--  lazy computation of f ( ~ B   \  z  II ~[z ~-+ v=~,,] ]) 

p : = ~  r:=ZVAL(Lp) while  r  r  ~  do 

od 
V~e,.#  :=  T 
-- v,.,.~, =  f([[B \ z  II ~[~ ~  ~0=-]]) if ~=~r =  v~.~ then exit Ioop 11 

endloop 

ret urn  v=u~ z  E  Iv(B)  and rid(B, z)  >  1 : solve2(BV, ~[tv(B)  ~  (B, ~)], z) 

endease end 

Not e  We   obviously don't want to compute the delayed value [[ Bd  [I ~d ](z) anew 
each  time  it  is  needed.  An  actual  implementation should  first  check  whether  or 

171 
not  this  delayed  value  has  already  been  computed  previously.  If so,  then  this 
previously computed  value  is  returned  instead  of calling  solve2(Bd, ~ ,   ~). 

The  following definition  and  fact  are  useful  for proving  termination  of SOLVE2. 

Definition  Let  B  fi9 B  and  let  O fi9 ~.  Then  define the closure  cl(B, O) C  B  *  E 
as  the  least  set  such  that: 

-  (B,  O)  e  cl(B,  0), 

-  Let  (B', ~')  fi9 cl(B, 0),  let  z  fi9 tv(B')  and  let  v  fi9 V.  Then  (B'  \  z, ~'[~  ~-* v]) 

is  also  in  cl(B, 0). 
-  Let  ( S',~')  fi9 cl(S,8).  Then  (B'V,~'[~v(B')  ~-* (B',~')])  is  also  in  cl(B,8). 

It  is  clear  that  (B',~')  E  cl(B,  8)  for  any  call  solv e2(B', ~', _)  that  is  entered 

during  execution  of SOLVE~(B, 8, _). 

F a c t   (4)  Let  (B1, ~1)  E cl(B, 8)  and  let f l ( z )   =  ( B 2 ,~ ).   Then: 

{y  ~  lv(B2)1~2(Y)  E F}  =  (y  ~  lv(B1)I~I(Y)  E ]/  and  nd(B,y)  <  nd(B,z)} 
T h e o r e m   SOLVE2(B, O, z)  correctly computes  ~B  II/~]](z) 

The  advantage  of postponed  evaluation  is  that  only  information  that  is  really 
needed is computed.  As such  SOLVE2 has the  same  worst-case time complexity as 

SOLVEI, i.e.,  exponential in  fly(B)  I. For practical applications,  however,  SOLVE2 
is  expected  to  perform  significantly better  than  SOLVEI, especially  if Blocks  are 
really sparse. 

5  Avoiding  Needless  Re-Computation 
The reason for the bad worst-case time complexity of SOLVE2 is that  each  solve~- 
call  is  treated  isolated,  i.e.,  there  is  no  re-use  of information  between  solve2- 
calls. 2 Take for example call s o l v e 2 ( B \ z ,  ~[~  ~-+ Your], y) in the body of the inner 

while-loop.  This  call  may  be  executed  for  different  values  of y  and  your.  There 
is,  however,  not  the  slightest  re-use  of information  between  any  two  such  calls. 
The  goal of this  section is precisely to refine  (extend)  function  solve2  such  that, 
apart  from  returning  ~B  II ~ ( z ) ,   it  also  returns  some  additional  information. 
This  additional  information should  be  use~l,  i.e.,  it  should  allow a  considerable 
speed-up  for  subsequent  solve2-calls,  and  it  should  he  e~sy  io  compule.  The 

2 Function solve2 may even be called more than once with exactly the same argument values.  This,  of course,  could easily  be  alleviated  by dynamically  keeping  track  of a 

table  in  which  all  results  computed  so far  are  stored,  and  then  performing  a  look- up  before  actually  computing  a  component.  This  would  guarantee  that  the  same 
information  is  computed  at  most  once.  However,  it  would  not  improve  upon  the worst case behaviour  as the size of this  table  may grow exponentially in the  number 

of variables. 

172 
additional  information that  we  will  consider  takes  the  form  of lower  and  upper 

bounds.  We  first  indicate  how  knowledge  of  such  lower/upper  bounds  can  be 
exploited  to  speed  up  the  computation  process.  Then  we  discuss  some  (simple) 
rules  that  can  be  used  to  compute  lower/upper  bounds. 

U s i n g   u p p e r / l o w e r   b o u n d s .   The  cornerstone  of SOLVE is iterative extremal 
fix-point computation.  For e x a m p l e / ~   is computed  as the  limit point  of the  in- creasing  sequence  n 0 ( *   E  n ~(*  ___ ~ ( *   E  n s ( *   ___ ....  Now  if it  is  known 

in  advance  that  vz  (resp.  v~,) is  a  lower  (resp.  upper)  bound  for  ~ ,   then  the 
following more  refined schema can  be  used:  Iteration  starts  at  vl  and  terminates 

when  a  fix-point  of  ~  is  reached  or  when  vu  is  reached,  whatever  occurs  first. 

Hence  knowledge  of upper/lower  bounds  allows  to  reduce  the  number  of itera- 
tion  steps  needed  to  compute  extremal  fix-points. 

--  v~ E  #~  E: v,~ 

loop if v,,~  ---- v,~ the n  exit  loop  fi 

if v~,  =  v,~,  t h en   exit  loop  fi 
endloop 

The  next  definition  is  just  to  make  lower/upper  bound  information  of  Blocks 
more  explicit. 

D e f i n i t i o n   Let  s  L / 6   ~  such  that  s  E / / .   Then  define  the  Block  B Ls 

nil[Z :,//]  -- n i l   and  ((z  ~  .f)  fi9 B')[AU]  =  (s  L  f') fi9 ( B' [ s 
where  f '   :  AO.(f(O)  U f.(z))  ~ ll(z). 

O b s e r v a t i o n   Let  fi9 E  lv(B).  Then  s  _  [B[r.,U 1  II 0](~)  E  u(~) 
C o m p u t i n g   l o w e r / u p p e r   b o u n d s .   The  computation of lower/upper  bound 
information is  based  upon  elimination fact  (3)  together  with the  following facts: 

(5)  V B ~ B ,   Ox,O2~O  : OxE02  =:=> [ B I I O ~ E E BI I 0 2 ~  (6)  V B EB , 0 ,s   : L ~ [ B I I O ~ E U   ==>  I[BII0~:I[BL~:,Ul  II0]] 

Elimination fact  (3) comes into play upon  exit from the  loop-construct in  solve2 
because at that point we have that vo~,, =  [B  II ~('). Hence, by fact (3), lower 
and upp~  bounds obtained for [B \ fi9 II ~[~ ~  ~,,]~ can be fully re-used for the subsequent computation of I[ B  II ~" If-components. 

The  monotonicity fact  (5)  allows a  partial  re-use  of information in  the  following 
way:  In  a  =  O  (resp.  @)  then  the  successive  values  obtained  by  variable  vc~,~ 
form  an  increasing  (resp.  decreasing)  chain  vo, vl, v~, ....  Hence  if G --  O  (resp. 

173 
@) then  lower  (resp.  upper)  bound  information  obtained  for  [ B   \  m II  ~  ~] 1 
can  be  re-used  later  on  when  computing  components  of I S   \  z  II f[m ~  vj] 1 with j > i .  

Fact  (6)  comes  into  play  when  computing  several  different  components  of  the 
same  Block.  This  is  for  example  the  case  inside  the  while-loop  of  solve2  where 
different  components  of ~ B \ z   II f[m ~-* v=~,~] 1 are  computed  for the  same  value of 

your. To  see  how fact  (6)  can  speed  up  the  computation  process,  assume  that  we 
have just  computed  [ B  IIf l(z) fi9 As  a  result  of this  computation  we also  obtained 
lower bounds  L  and  upper  bounds  H  such  that  s  E_ [ B  II  E  H.  Hence,  by fact 

(6),  ~ B  II ~ 1  =  [ S  [L, H]  IIf 1- I.e.,  the  computation  of subsequent  components 
of [ B   II f  l  is  reduced  to  computing  components  of [ B  [s  H]  II f  1,  which  can  be done  more  efficient  as  the  'range'  of the  right  hand  sides  of  B [L:, H]  is  smaller 

compared  to  right  hand  sides  of B. 

N o t e   1  Fact  (6)  is a  generalization  of fact  (3).  This  generalization  in  necessary 
because  fact  (3)  only  allows  to  eliminate  a  variable  m from  B  on  condition  that 

B  I101(m)  is  known  exactly.  However, as  a  result  of applying  monotonicity  fact (5),  we  do  not  always  know  the  exact  value  of components.  Unlike  fact  (3),  fact 

(6)  allows  to  simplify  Blocks  if only  partial information  on  components  (under 
the  form  of lower/upper  bounds)  is  available. 

N o t e   2  The  only  point  where  lower/upper  bound  information  is  actually gemer=ted is  upon  exit  from  the  loop-construct  in  solve2.  There  we  have  that 
vo~,  =  EB  II  ~1(~)  and  hence  vo~,  acts  as  a  lower  and  an  upper  bound  for 

~B  II ~ ~(=)o  Facts  (a)  and  (5)  only  allow to  p ~   (p~op=g~e)  lower/upper  bound 
information  from  E Bm II em I  to  [B2  II e~ 1 where  (B~, e~)  and  (B~, e2)  are  different but  related. 

Incorporating  lower/upper  bound  information  as explained  above, function  SOLVE2 
is  refined  into  a  function  SOLVEs (see  appendix).  Thanks  to  the  re-use  of infor- 
mation,  the  average  run  time  behaviour  of  SOLVEs is  significantly  better  than 
for  SOLVE2. Under  the  assumption  that  delayed  values  are  never  needed  (i.e.  the 
second  case-arm  of  solve3  is  never  applicable),  we  can  even  prove  that  SOLVE3 
runs  in time  polynomial  in  ] lv(B) I and  only exponential  in  the  alternation  depth where  h a(ni l)  :  0  and    a(B)  =  1 +    a(SV)  i f .   4  n i l .   The  above  as- 

sumption is trivially  satisfied  in  case  B  is alternation  free,  i.e.  ad(B) =  1.  Hence 

S0LVE3 runs  in  time  polynomial  in  ]lv(B) I for  alternation  free  Blocks. 

6  A p p l i c a t i o n   :  M o d e l   C h e c k i n g  
In  this  section  we  apply  algorithm  SOLVE to  the  field  of automated  system  ver- 
ification,  in  casu  model  checking.  We  briefly  sketch  how  SOLVE can  be  turned into  a  local  model  checker,  i.e.  an  algorithm  for  automatically  deciding  whether 

a  concurrent  system  satisfies  a  desired  property. 

174 
M o d e l i n g   B e h a v i o u r   .  We  use  (labeled)  finite  state  machines for  modeling 
the  operational  behavior  of  (concurrent)  systems.  A  finite  state  machine  (fsm) 
is  a  triple  A4  -  (S, .4, ---*) where  S  is  a  finite  set  of system  states,  A  is  a  finite 
set  of actions  that  the  system  is  capable  of performing,  and  --*  C  S  *  ,4  *  S 
is  the  transition  relation,  representing  the  state  transitions  resulting  from  the 
execution of actions. We shall write  s  -~  #  instead of (s, a, #)  E-~.  Hence  s  2,  # 

expresses  that  the  system may perform  action  a  when  in state  s  and  in doing so 
it  moves to  state  #. 

S p e c i f y i n g   S y s t e m   P r o p e r t i e s   .  Various  temporal  and  modal  logics  have 
been  proposed  for  describing  system  properties.  One  particular  expressive  logic 
is the  propositional  modal mu-calculus  [K].  This  calculus  allows a  wide range  of 

system  properties  to  be  expressed,  including  liveness,  safety and  fairness  prop- 
erties.  Examples of properties  expressible  as  modal mu-formulas are:  'eventually 
action  a  will  happen',  'it  is  always  possible  to  perform  action  a,  'action  a  can 

happen  infinitely  often',  etc.  The  expressive  power  of  the  mu-calculus  mainly 
stems  from the  possibility of nested  (alternating)  fix-points.  Lots of other  logics 
have  uniform encodings  in  the  mu-calculus. 

Fix  a  fsm  A.t  =  ($, .4,-4).  Define  B , ~   as  the  set  of  Blocks  over  the  lattice 

(2 s,  C_) with  right  hand  side  functions of the  form: ~ 

I  I  I 
where  Y  ranges  over  2 x,  m over  X and  a  over  ~4.  Furthermore,  for  any  Q  c_  S, 

s'  implies  s'  E  Q}. 

Note  that Bm~  grosso  modo  corresponds  to  the  modal  mu-calculus.  It  is  fairly 
straightforward  to  transform  modal  mu-formulas  into  Blocks  of  Bmu.  Hence 

B,~,  can  be  used  for specifying properties  of A4,  in  the  same  way as  modal  mu- 
formulae.  As  an  illustration  consider  the  modal  mu-formula vml.(/~m2.([a] m l  A 

[b] m2)) with  A  =  {a, b}.  This  formula, taken  from  [C],  states  that  it  is  always 
the  case  that  an  action  a  is  infinitely often  possible,  assuming  that  A,4  has  no 
deadlock  states.  The  zl-component  of  the  following Block  corresponds  to  the 

above  modal  mu-formula: 

where  B  _--  _0  [b]  . n i l  

More  formally  properties  of  A4  can  be  specified  by  a  triple  (B, 6, m)  where 
B  E  Bm~,  m E  Iv(B)  indicates the  component of B  we are  actually interested  in, 
and  #  : X -*  2 s  gives an  interpretation  for  the  free  variables  of B,  i.e.,  variables 
not  in  Iv(B)  that  occur  in  the  right  hand  sides  of B.  (Free  variables  of B  play 
the  same  role  as  atomic propositions  of modal mu-formulas). 

3 We use normal forms. Furthermore  U~  --- @ and  ['1~ =  "-q. 

175 
SOLVE as  a  ( lo ca l)   m o d e l   c h e c k e r .   Given a  system A4  and  a  property  spec- 
ification  (S, 8, z)  for  M ,   we  can  use  algorithm  SOLVE to  compute  [ B   II 0](z). 

This  type  of  model  checking  is  usually  referred  to  as  global mode]  checking 

[EL,CKS,CES,AC,CS1,  CS2,LP,A1,VL1]  because  all  states  of  A//  satisfying  a 
specified property  are  computed.  This  application  of SOLVE is  however not  very 

interesting.  The  reason  is  that  the  Blocks  involved  in  most  property  specifica- 
tions  are  not  really sparse. In  practice,  however,  one  is  not  so  much  interested  in  all  states  that  satisfy  a 

given  property,  but  only  in  a  few  states,  typically  the  initial  system  states.  It 
then  seems  overwhelming to  compute the  set  of states  satisfying a  property just 
to  check  whether  the  state  of interest  is  in  this  set.  This  idea  is  central  to  the 

development  of  local model  checkers  [A1,C,SW,W,VL2].  Given  a  system  A//,  a 
state  s  of A//  and  a  property  specification  (B, 8, z),  a  local  model  checker  will 
try  to  check  whether  s  E  [ B   II 8](z)   without  having  to  compute  the  complete 
set  [ S  II 8](~).  The  rest  of this  section  sketches  how such  a  local model checker 
for B , ~   can  be  obtained  using  SOLVE. 

The  key  idea  is  to  'merge'  A//and  B  in  order  to  obtain  an  'equivalent'  boolean 

Block.  A  boolean  Block  is  a  Block  over the  lattice  ( { f a l s e ,  t r u e } , - < / w i t h   the 
usual  ordering  f a l s e   <  t r u e .   More  precisely  define  a  translation  T r   from  Brnu 

into  boolean  Blocks  as  follows: 

Tr(nil) = nil  Tr((z ~  f)eB') =  (z.sl ~  f*sl)s...e(z.sn  E_ f.s.,)eTr(B') 
where  S  =  { S l , . . . ,   s,~} and  f  * s  is  defined  as  follows: 

(~o. U,~Y o(~) ), s = ~o'. V.~Y o'(z.s) (~o. N.~Y o(~) ), 

~  = ~o'. A.~Y o'(~.~) ( ~o. (~) o(~) ), s = ~o'. V,,~ I,:.,' o'(~.s') 

(~o. [~] o(~) ), s = ~o'. A,,~, ,_~,, o'(~.s') 
It  can  now  be  proved  that  Block  B  is  equivalent  with  T r ( B )   in  the  following 
sense: 

w  e  x, vs  fi9 s  :  s  fi9 [ B   II o](,0  ~  [ ~ ( B )   It O']l(~,.s)  =  t r u e  
where  e'  =  ~( . .s ) .   if  s  fi9  e ( z)   then  t ~ e  else  f a ls e.  

Hence  in  order  to  check  wether  a  state  s is  in  [ B   II 0](x),  we  simply  compute 

[ T r ( B )   II 0'](z.s)  using  SOLVE. In  this  way  SOLVE is  turned  into  a  local  model 
checker for  Bin=. 

This  use  of  SOLVE is  much  more  interesting  because,  as  opposed  to  B,  Block 

T r ( B )   usually  exhibits  a  high  degree  of sparseness.  There  is 

-  Semantic  sparseness:  To evaluate  a  boolean  expression  usually  only the  val- 

ues for  a  subset  of variables  occurring  in  that  expression  are  needed, 

4 Note that  variables  of Tr(B)  are of the form  z.s,  where z  E I  and  s  E 8. 

V~  =  f a l s e   and  AO  =  true. 

176 
-  Syntactic  sparseness:  A  right  hand  side  of  Tr(B)  usually  only  contains  a 

small number  of boolean  variables.  This  is  a  result  of the  fact  that  for most application  systems  the  number  of transitions  leaving  a  state  is  small com- 

pared  to  number  of  states.  I.e.  [  ---+  ]  is  of the  order  O([ 8  [),  rather  than O(ISl  )- 

N o t e   There  is no need  to explic~tly store  and construct  Tr(B)  f~om B  in order to  compute  T  (B)  II e'  Instead  we  use  the  rules  for  Hence  we  only 

have  to  store  B  together  with the  fragment  of A//that  has  been  explored  so far. 

7  D i s c u s s i o n  
We  have  presented  a  new  local  algorithm  for  computing  nested  fix-points  over 
complete  lattices  of finite height.  Several  improvements  of the  basic  ~[gorithm 

have been  proposed.  As  an illustration, we sketched how the  proposed  Mgorithm 
could be  used  as  a local model checker for  (essentially) modal mu-formulas. Lots 
of other  application  areas  can  be  thought  of, e.g.  data  flow analysis. 

Local model checkers  for the  modal mu-calculus  have  been  presented,  under  the 
form  of tableau  systems,  by  [SW,C]  and,  under  the  form  of a rewrite  relation, 

by  [W].  A  fundamental  difference  between  SOLVE and  [SW,C,W]  is  that  SOLVE 

is  based  upon  lazy fix-poin%  ~pproximation,  whereas  [SW,C,W]  are  based  upon 

an  implicit  use  of fix-point  induction.  Nevertheless,  there  seems  to  be  some  in- 
teresting  similarities.  For  example  in  [SW]  definition  lists  are  used  and  fresh 
constants  are  introduced for renaming recursive  propositions  each  time they are 
unwound.  The  role  of this  definition  list  and  constants  seems  to  be  similar  to 
our  closures:  Both  serve  to  save  the  necessary  context  information  needed  to 
compute  delayed  values  when  they  are  needed  later  on.  In  [C]  hypotheses  sets 
are  used.  This seems to  be  yet ~nother  equivalent  way for representing  closures. 

The  set  H(B~,(I)  of  hypotheses  associated  with  a  closure  (B1,~1)  6  el(B,8) 
is  roughly  given  by  {(m,~l(m))]m  E  lv(B)andre  ~  Iv(B1)and~l(m)  6  1)}. 

Note  that  knowledge of H(B1, ~1) is  sufficient to  construct  (B1, ~1)  from  (B, 8). 
Hence  SOLVE can  be  rewritten  solely in  terms  of hypotheses  sets.  Furthermore 
let  ~l(m)  --  (B2, ~2).  Then  rephrasing  fact  (4)  in  terms  of hypotheses  sets  yields: H(B2, ~2)  =  H(B1, ~1) \ 

{(Y, v)  6  H(B1, ~1) I nd(B, z)  <  nd(B, y)},  explaining 
in  a  way  the  removal of hypotheses  in  rules  RT,R8  and  DR3  of [C]. 

Thanks  to  the  re-use  of information, the  average  run  time  behaviour  of SOLVEs 
is  significantly  better  than  for  SOLVE2. For  the  special  case  where  B  is  alter- 
nation  free,  i.e.  ad(B)  --  1,  s0aVEs  even  runs  in  time  polynomial  in  Ilv(B)[. In  ILl]  Larsen  also  gives  a  polynomial time  local  model  checker  for  alternation 

free  boolean  Blocks.  The  improvement  of  [L1] over  [L2] is  essentially  based  on 
facts  (3)  and  (5).  These  two facts  indeed suffice in the  hoolean case.  However for 

lattices  other  than  the  boolean  tattice~ fact  (6)  is  also needed~  because  fact  (6) 

allows  to  simplify Blocks  when  only partial  information on  component  values is 

known. 

177 
Despite  the  re-use  of  information,  the  worst-case  behaviour  of  SOLVE3  is  still 
exponential  in  [lv(B)  lif ad(B)  >  1. The  reason  is that  the re-use of information 
doesn't  work for delayed values. Intuitively, this  can be seen as follows: ,Consider 
the  recursive  call  solv e3((B?) Ls  ~[tv(B)  ~  (B L s  5/7, ~)], z).  in  function solve3.  If the  value  of a  variable  y  6  tv(B)  turns  out  to  be  needed  during  ex- 

ecution  of  this  call,  then  the  computation  of  ~  is  temporarily  suspended  and 

the  delayed  value  [Y [s  U]  H (~(Y)  is  computed.  Hence  at  this  point  we  are 
computing  different  components  (~  and  y)  of the  same  Block  I S  [s  U]  II ~ ]Y 

without  there  being any re-use of information.  This  causes the  exponential  blow- up.  Local  algorithms  that  are  polynomial  in  lip(B)  t  and  only  exponential  in 

ad(B)  are  contained  in  [X]  for  the  full  mu-calculus,  and  in  [A2] for  Blocks  of 
alternation  depth  2.  We believe that  it  should  be  possible to  refine  SOLVE3 into a  polynomial  time  algorithm.  This  could  probably  be  done  along  the  follow- 

ing  lines:  If during  execution  of solve3((BV) [s  U], ([tv(B)  ~-+ (B [s  U], ~)], ~) the  value  of  a  variable  y  6  tv(B)  is  needed,  then  we  do  not  compute  this 
value,  because  this  caused  the  exponential  blow-up.  Instead  we  assume  a  value s  for  y  if  y  is  a  O  variable,  and  a  value  U(y)  if  y  is  a  @ variable.  When 

solves((BV) [s  U], ([tv(B)  ~-* (B [s  ~)], z)  then  returns,  we check  whether 
the  assumption  for  y  was justified.  If  not  then  z  has  to  be  recomputed  using the  correct  value  for  y.  The  point  is  that  in  such  a  modified  schema  there  is 

an  optimal re-use  of information:  The  lower/upper  bounds information  obtained 
by  solve3((SV)Ls  )  ~-*  (S[s  can  be  fully  re-used  when 

checking  the  y-assumption,  and  the  information  obtained  from  checking  this  y- 
assumption  can  be fully re-used  when  re-computing  z  with  the  correct  y-value. 
The  above ideas  are  of course  very informal  and  need  be  further  explored.  This is an  interesting  topic  for future  research. 

R e f e r e n c e s  

[A1]  Andersen,  H.  R.:  Model  Checking  and  Boolean  Graphs,  ESOP'92,  LNCS 582,  1992 
[A2]  Andersen,  I-I. R.  :  Verification  of  Temporal Properties  of  Concurrent  Sys- tems,  PhD  thesis,  Aarhus  University,  DAIMI  PB  -  445,  1993 
[AC]  Arnold,  A.,  Crubille,  P.:  A  linear  algorithm  ~o solve  fized-points  equations on  transition  systems,  Information  Processing  Letters,  vol.29,  57-66,  1988 
[CES]  Clarke,  E.M., Emerson,  E.A., Sistla, A.P.: Automatic  verification  of finite- state  concurrent  systems  using  ~emporal  logic specifications,  ACM  Transac- 

tions  on  Progr.  Languages  and  Systems,  Vol.8,  No.  2,  pp.  244-263,  April 

1986 
[C]  Cleaveland,  R.:  Tableau-based  model checking  in the propositional mu calculus, Acta  Informatica,  1990 

[CKS]  Cleaveland,  R.,  Klein,  M.,  Steffen,  B.:  Faster  Model  Checking  for  the Modal  MuCaleulus, 

CAV'92,  LNCS  663 

178 
[CS1]  Cleaveland, R.,  Steffen,  B.:  Computing  Behavioural  Relations,  Logically, ICALP  91, pp.  127-138, LNCS  510 

[C S 2]  Cleaveland, R. and Steffen, B.: A  Linear- Time Model- Checking Algorithm for  the  Alternation-Free  Modal  Mu-Calculus,  CAV'91,  LNCS  575, 1992 
[EL]  Emerson,  E.A.,  Lei,  C.-L.:  Efficient  model  checking  in  fragments  of  the propositional  #-calculus,  LICS,  267-278,  1986 
[K]  Kozen,  D.:  Results  on  the  propositional  mu-calculus,  TCS  17,  1983 
[L1]  Larsen,  K.G.:  Efficient  Local  Correctness  Checking,  CAV'92,  LNCS  663 
[L2]  Larsen,  K.G.: Proof systems for Hennessy-Milner  logic wi~h recursion,  CAAP, 1988, see also TCS,  72,  1990 

[LP]  Lichtenstein, O., Pnueli, A.:  Checking  that finite  state  concurrent programs satisfy  their  linear  specification,  (Proc.)  12th  ACM  annual  Symposium on 

Principles of Programming Languages, pp.  97-107,  1985 

[SW]  Stifling,  C.,  Walker,  D.:  Local  model  checking  in  the  modal  mu-calculus, TCS,  October  1991, see also  LNCS  351, 369-383, CAAP  1989 

[S]  Stifling,  C.:  Modal  and  Temporal  Logics,  in  Handbook  of  Logic  in  Com- puter  Sciences,  Volume 2.  Edited  by  S.  Abramsky,  M.  Gabbay  and  T.S.E. 

Maibaum; Oxford  Science Publications,  1992 

IT]  Tarski,  A.:  A  Lattice-Theoretical  Fizpoint  Theorem  and  its  Applications, Pacific  Journal  of Mathematics,  5: 285-309,  1955 

[VL1]  Vergauwen,  B.,  Lewi, J.:  A  linear  algorithm  for solving fized points  equa- tions  on  transition  systems,  CAAP'92,  LNCS  581, 322-341 
[VL2]  Vergauwen,  B.,  Lewi,  J.:  A  Linear  Local  Model  Checking  Algorithm  for CTL,  CONCUR'93,  LNCS  715 
[W]  Winskel,  G.:  A  note  on  model  checking  the  modal  nu-calculus,  ICALP, LNCS  372, 1989, see also TCS  83,  1991 
[X]  Xinxin,  L.:  Specification  and  Decomposition  in  Concurrency,  PhD  thesis, Aalborg University,  1992, R  92-2005 

179 
A p p e n d i x  

f un c t i o n  SOLVK~(B, O, z)  = 
b e g i n  (v,  _, _):=  solv.s(BLA~ e X._L, AU ~  X.T],O,  z) 

return 
end 

function  solvea(B [s  U], ~, .)  = 

--  return  (v ....  s   such  that 
--  ~0~,  =  [B [AUl  [[ ~ll(z) 
--  s 1 6 3   and  [B[L,U]  II ~ = [B Ls   Ila] b e g i n  

case I z  r  Z~(B) and  ~(z)  e  V:  re tu r n  (~(z),  C, U) 

D *  r  h,(B)  ana  ~(z)  -  (Bd[ZZd, U,d, ~,,)  : (,,, _, _) :=  .o~,,.,(B4&,  u 4,   ~,  .) 

r e t ur n   (v, L, ld) D fi9 ~  Z,,(B)  . a   ,.,Z(B,.)  =  ~  : 

Let  eq(B, z)  =  (z ~ f) 

t ~   t ~  ve~,  :=  if  u  =  @ t h e n   ] s  [ else  [/d(z)  [ fi 

I  I vb~a  :=  if  a  =  @ then/d(m)  else  s  fi 

t s  .'   : =.1 l oop 
--  Ir(B  \  =)LL,  ul  II ,~[*  ,--, .,,o.,,,.]n =  ll(B  \  ~,)LZ:', u'l  II ,@:  '-'  ".,o,,,.]11 

if  yea,-  ----- vb.a  t h e n   exit  l oo p  fi 

--  lazy  computation  of I ( [ ( B   \  .)LZ:',U'l  II a[* ,--, ,,o,,,] ]) p 

:=  e  r  :=  EVIL(f,  p) while  r  ~  ];  do 

( , , , V ~ ) : =   so~v.~(  (B  \  ~)LZ:', U'l ,  ~[.  ,--. ,,o...], ~) 
p  :=  p[r  ~  v] r 

:=  EVAL(.f,  p) od 

--  v,~..  =  f([[(B  \  :,)L,C',U'l  II a[,  ,--, ,,o,,,]n) 
l v.e~  :=  if  a  =  @ t h e n   v.e~  U re,,,, else  v~..  n  v,~.  fi ] 

if  vcu~  =  V~e~  t h e n   exit  l oo p  fl 

i [if  ~  =  0  t he n/ . / '   :---//else  s   :=  s  fi[ 

e n d l o op  
r e t u r n   (~ ....  I Z:'[z  ~  v~,.],U'[z  ~  ~0,~.]  ) 
D *  ~  a,(B)  and  ,~d(B, =)  >  ~  : ,,o~,,~((B  V) [L, UT, ~[t,,(B)  ,--, (B [Z:, Ul, ~)], *) 

e n d e a s e  e n d 