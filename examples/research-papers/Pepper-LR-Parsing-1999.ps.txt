

LR Parsing

=
Grammar Transformation + LL Parsing

Making LR Parsing More Understandable And More Efficient

Peter Pepper

No. 99 - 5

April, 1999

Abstract
The paper has three aims. Its primary focus is a derivation method which is -- in contrast
to many of the classical presentations in the literature -- easy to comprehend and thus easy
to adapt to different needs. Secondly, it presents an improved LR parser which has the power
of LR parsing, but (almost) the efficiency of LALR parsing. Finally, it elucidates the strong
conceptual relationships that actually exist between LL and LR parsing.

It is also briefly shown that the flexibility and easy adaptability of our techniques open up the
possibility for new applications. As an example, we outline how type-based overload resolution
(and thus type analysis) for arbitrary mixfix operators can be implemented in the framework of
a polymorphic functional language.

Contents
1 Introduction 3
2 Basic Concepts: Grammars and Transformations 6

2.1 Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 Parse Trees and Postfix Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.3 Adding Rule Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.4 Continuations "(Lookahead") . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.5 Equivalence-Preserving Transformations on Grammars . . . . . . . . . . . . . . . 11

I Towards LR(k) Grammars 15
3 Normal Forms for Grammars 16

3.1 First Normal Form: Bounded Production Length . . . . . . . . . . . . . . . . . . 16
3.2 Second Normal Form: Leading Terminals . . . . . . . . . . . . . . . . . . . . . . 18
3.3 Third Normal Form: Unique Shifting . . . . . . . . . . . . . . . . . . . . . . . . . 20

4 Main Theorem: 3NF + LL(k) = LR(k) 22

II From Grammars To Parsers 28
5 Emulated Versus Classical LR Parsing 29

5.1 A Simple (Top-Down) Parsing Scheme . . . . . . . . . . . . . . . . . . . . . . . . 29
5.2 "Emulated" LR Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
5.3 Towards "True" LR Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

6 Termination of the 3NF Construction 34

6.1 What Is The Problem? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
6.2 A Simple Solution: Backtracking . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
6.3 Another Simple Solution: Lazy Grammar Transformation . . . . . . . . . . . . . 36
6.4 A Solution Based on Equivalence Classes . . . . . . . . . . . . . . . . . . . . . . 37

7 Relationship to Classical LR-Style Parser Generation 40

1

7.1 The Classical "Sets-of-Items" Construction: LR(0) Parser . . . . . . . . . . . . . 40
7.2 Stronger Than LALR(k) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
7.3 Less Expensive Than Classical LR(1): Empirical Results . . . . . . . . . . . . . . 44

III Variations, Observations and Applications 47
8 Facts About Grammars and Languages 48
9 Variations 50

9.1 Variations on the Grammar Presentation . . . . . . . . . . . . . . . . . . . . . . . 50

9.1.1 Features From EBNF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
9.1.2 Action-less Productions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
9.1.3 Precedence And Associativity . . . . . . . . . . . . . . . . . . . . . . . . . 52
9.2 Variations on the Grammar Transformation Process . . . . . . . . . . . . . . . . 53

9.2.1 Optimizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
9.2.2 Incremental Parser Generation . . . . . . . . . . . . . . . . . . . . . . . . 54
9.2.3 Why Simple Left-Recursion Elimination Fails . . . . . . . . . . . . . . . . 55
9.3 Variations on the Parsing Process . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

9.3.1 k-Lookahead or Backtrack? . . . . . . . . . . . . . . . . . . . . . . . . . . 58
9.3.2 Multi-Pass Parsing Versus Generalized LR Parsing . . . . . . . . . . . . . 59
9.3.3 Error Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
9.3.4 Integrating Context-Sensitive Aspects . . . . . . . . . . . . . . . . . . . . 62

10 Sketch of a Non-standard Application:

Type-Dependent Overload Resolution and Mixfix Operators 63

11 Conclusion 65
A Appendix: More About (Functional) Parsing 67

2

Chapter 1
Introduction
The area of parsing is undoubtedly one of the best researched areas in computer science.1 One
might therefore wonder whether there are any new insights left worth presenting in a paper. To
make this clear from the beginning: our paper, too, will not reveal new parsing techniques which
have never been heard of before. Rather, our studies remain within the classical (and highly
successful) realm of LL and LR parsing. Nonetheless, there are still a number of contributions
made in this paper:

1. First and foremost, we present a new derivation process for LR-style parsing which has

several advantages:

ffl It is much simpler and thus much easier to comprehend (and verify) than the traditional ways of presenting LR parsing.2 The main reason is that the entire development
takes place within a single formalism rather than switching back and forth between
several mechanisms (grammars, automata, tables, programs, : : : ).

ffl This simplicity and clarity makes the approach very flexible and thus facilitates the

construction of variations (e.g. incremental or parallel parsers and parser generators)
as well as integration into different environments.

ffl It is easy to obtain a parser which combines LR-style efficiency with the full backtrack

power needed to cope with (those parts of) languages that do not satisfy the necessary
prerequisites. This is sometimes referred to as generalized LR parsing [36, 31].

2. Equally importantly, our approach unifies the paradigms of LL(k) and LR(k) parsing.

That is, we have one derivation method that simultaneously generates an LL(k) and an
LR(k) parser.3

3. As a somewhat surprising result, our method generates a parser that combines the power

of LR parsing with the efficiency of LALR parsing. That is, the parser is LR(1), but the
size of its tables is much closer to that of an LALR(1) parser than to that of traditional
LR(1) parsers.

4. Our approach trivially includes sentential-form parsing and is thus suited for tools such

as syntax-directed editors, integrated software development environments, transformation

1An extensive bibliography is given by Nijholdt [20].
2This claim is confirmed by several years of teaching the method to students; they now find LR parsing no

more complex than LL parsing.

3As a matter of fact, our protoypical implementation has a command line option that is only checked at a

handfull of points in the program in order to output either an LL(1) or an LR(1) parser.

3

systems and the like.4
5. The implementation is easily adaptable to an incremental parser generation, which is also

an important prerequisite for many applications.

6. We obtain informative insights into the relationship between different parsing paradigms,

showing, for example, why there is actually not much difference between LL and LR
parsing,5 and why LR parsing is more powerful than LL parsing.

7. The flexibility opens up new application domains, such as the use of LR-style parsing for

type analysis and overload resolution in polymorphic languages.

As regards the comprehensibility, every mathematician is familiar with the uneasy feeling caused
by certain proofs: you can follow each and every step, but at the end you still wonder: How do
all these steps lead to the result? What is the idea behind the proof? In short, why did the proof
work? To many people, the LR-parsing techniques presented in articles and textbooks fall into
the same category. On a step-by-step basis, the ffs, fis, and fls somehow fit together, but the
big picture does not emerge.

This effect became particularly noticeable when I tried to derive a parallel parser. It was by
no means clear where and which changes had to be applied to the standard table-generating
procedures in order to achieve the necessary adaptions. A similar problem arose when we tried
to allow the user of our functional language Opal [11, 26] to define arbitrary mixfix operators (in
connection with liberal rules for overloading and polymorphism): the analysis of such programs
led to the need for incremental parser generation and "fuzzy" nonterminals. I thus decided to
reconsider the whole derivation process for LR-style parsing.

The guiding principle of the design is to make the proofs so straightforward and simple that
they are even amenable to automatic verification tools. Thus, the derivation is based solely
on equivalence-preserving transformations of grammars. In my opinion, this is a much more
transparent concept than the traditional approach based on "sets of LR(0) items" with their
rather odd use of "dotted production rules" and "spontaneous" and "propagated" lookahead
symbols.

A note on the literature. The topic of parsing has been the subject of intensive research over
several decades, so it would be completely futile to strive for a list of references which comes
even close to completeness. On the other hand, the topic is extremely well covered in standard
textbooks. Thus, we basically content ourselves with these textbooks and only add individual
references where they specifically address issues we cover.6

Following the introduction of basic terminology in Section 2, the paper has three main parts:

Part I (Sections 3-4) presents our approach to LR-style parsing, which may be paraphrased

as follows. Traditionally, LR parsers are obtained by applying a complex generation algorithm to the original grammar. We apply a simple generation algorithm to a transformed
grammar.. This leads to our Main Theorem 4.0.2, which can be visualized as follows:

G G0

LR(k) iff LL(k)

transform

4In their recent paper [38] Wagner and Graham stress the importance of sentential-form parsing. There it can
also be seen how much more complex this extension is for the traditional parsing techniques.

5This observation also has been made -- albeit in another setting -- by others [31].

6In order not to interrupt the flow of the presentation we defer most of the references to the footnotes. Unless

one is specifically interested in the literature, one may therefore skip all footnotes from now on.

4

That is, the original grammar G is LR(k) iff the transformed grammar G0 is LL(k). This
way of proceeding is beneficial, because the transformation is very simple as well.

Part II (Sections 5-7) considers the actual parsing process induced by our transformed grammars. It is shown that our approach captures the "conceptual essence" of LR parsing in
the sense that it makes the same decisions and thus has the same expressive power as the
classical approaches. But we use different implementation techniques and therefore should
rather speak of an "emulation" of LR parsing by a top-down parser (actually by an LL(k)
parser). On this basis we study the relationship between our approach and the traditional
way of proceeding in LR and LALR parsers.

Part III (Sections 8-10) presents some extensions and non-standard applications. These include a short discussion of generalized LR parsing, that is, LR parsing for ambiguous
grammars, and of error handling.

The ideas underlying this paper have been used over a number of years in student courses at
the Technical University of Berlin [23]. Our experience shows that the approach appears to be
considerably easier to comprehend than traditional methods. I therefore decided to pursue the
subject further in order to make the overall principle even easier to understand. The result of
this effort is reported in the following.

5

Chapter 2
Basic Concepts:
Grammars and Transformations

We assume that the reader is familiar with the basic notions of formal languages. Therefore, we
list only very briefly our notational conventions here.

2.1 Grammars
The most fundamental notions that we need in our discussion are those of "grammars" and their
"derivations" and "reductions".

Definition 2.1.1 (Grammar) A grammar is a four-tuple G = (T ; N ; S; P), where T is the
set of terminal symbols, N is the set of nonterminal symbols, S 2 N is the start symbol, and
P is the set of production rules, which are of the form A ! u with A 2 N and u 2 (T [ N )\Lambda .
As usual, we require that N " T = ;. By V = N [ T we denote the set of all symbols. \Lambda 

Definition 2.1.2 (Derivation, reduction) A derivation of a string s is a sequence of derivation
steps, beginning with the start symbol S and ending with the string s. In each step, denoted
si ) si+1, some nonterminal in the intermediate string si is replaced by one of its right-hand
sides in the grammar G, thus yielding si+1. We usually denote the derives relation as S

\Lambda ) s:

(When needed, the notion of derivation can also be extended from the start symbol S to arbitrary
nonterminals.) A reduction is the converse of a derivation.1 \Lambda 

Definition 2.1.3 (Language) The language Lt(G) of a given grammar G is the set of all
terminal strings derivable from the start symbol S, that is, Lt(G) = fu 2 T \Lambda  j S

\Lambda ) ug. The

strings u 2 Lt(G) are called sentences (or words).
As a straightforward generalization one also considers sentential forms u 2 V \Lambda , which may
contain terminals as well as nonterminals;2 the language of sentential forms is denoted by L(G).
\Lambda 

1In [29], these two concepts are formalized as dual forms of grammars: generative and analytic.
2This is not only necessary for LR-style parsing, but is also useful e.g. in connection with certain tools such

as syntax-directed editors, incremental editors [38] or program transformation systems, where one works with
so-called program schemes. Such schemes are nothing but sentential forms.

6

To illustrate our concepts we use the following grammar3 as a running example throughout most
of the paper. It generates sentences of the kind "\Lambda  \Lambda  \Lambda  i = \Lambda  \Lambda  i", with arbitrarily many stars.

S ! L = R

R
L ! * R

i
R ! L

2.2 Parse Trees and Postfix Notation
The essence of derivations and reductions is captured in so-called parse trees: Parse tree (a) in
Figure 2.1 represents the full derivation of the sentence "\Lambda  \Lambda  i = i", while the tree (b) represents
a partial derivation, that is, a sentential form.

S
L

R

L

R R

L L
\Lambda  \Lambda  i = i

S
L

R R

L L
\Lambda  \Lambda  R = i

(a) full tree (sentence) (b) intermediate tree (sentential form)

Figure 2.1: Variations of parse trees

Note: For the subsequent specifications it will be convenient to employ the "apply-to-all" operator that is well-known from functional programming: the application of a function f to all
elements of a set S is denoted by

f \Lambda  S def= f f (x) j x 2 S g

Definition 2.2.1 (Parse trees) The parse trees, in short trees, for a grammar G are defined
as the set T (G) = f t j t is a tree fulfilling condition (\Lambda )g (see also [32])

(\Lambda ) Each subtree t0 contained in t has the following property: if we take the root of t0 as lefthand side and the sequence of the roots of the subtrees of t0 as right-hand side, then we
obtain a production of G. Formally:

(root(t0) ! root \Lambda  subtrees(t0)) 2 P
using obvious functions root and subtrees. \Lambda 

The following properties are trivial consequences of this definition (see also Figure 2.1).
Lemma 2.2.1 The leaves of t 2 T (G), read from left to right, yield a sentence or sentential
form s 2 L(G). Formally: leaves \Lambda  T (G) = L(G), with the obvious function leaves.

3This example is taken from the standard textbook [1] because it is one of the smallest grammars illustrating
the difference between SLR and LALR parsers.

7

Lemma 2.2.2 The redices occurring in the reduction process correspond to complete subtrees
of height 1.

2.3 Adding Rule Numbers
Note: There is a technique that is helpful in many circumstances; it is based on the concept of
so-called null nonterminals: one can add to the grammar new nonterminals that derive the
empty string ", since this does not change the language. These additional nonterminals can then
be used for various purposes: they may carry semantic actions or they may act as "assertions"
that guide reductions or enable correctness proofs. We employ this technique for two purposes:
In this section we use it for integrating the tree generation more homgeneously into the parsing
process. And in the next section we use it for coping more easily with the idea of "follow sets".

The standard notion of production rules transfers too little information into the parse trees.
Let us therefore briefly redesign our grammars and parse trees. First of all, we add production
numbers4 to our grammar, as illustrated by the modified Grammar G0 in Table 2.1.

Grammar G0
S ! L = R 1

R 2
L ! * R 3

i 4
R ! L 5

Table 2.1: Grammar with rule annotations
Now we no longer label the inner nodes of the parse tree using the nonterminals, rather use
the numbers i of the corresponding production rules. The result is illustrated by the tree (b)
in Figure 2.2 (where we have additionally attributed the nodes by the nonterminals as their
"types"). Note that this is actually a better representation than the traditional form (a), since
it contains more information. The nonterminals can be trivially retrieved from the rule numbers,
but the rule numbers also differentiate between the different productions for the nonterminal.5

However, the crucial point is that this representation not only conveys more information than the
classical parse trees, it also conveys essential information: if we carry out a postorder traversal
of the tree (b), we obtain the following string:

\Lambda  \Lambda  i 4 5 3 5 3 = i 4 5 1

The symbols i therefore have a dual nature. On the one hand, they are considered to be the
empty string; hence, they are "invisible". On the other hand, they tell us which reductions are
to be applied where. To see this, consider the two corresponding derivations of our running
example in the two variants of the grammar.

Original Grammar: \Lambda  \Lambda  i = i
Modified Grammar: \Lambda  \Lambda  i 4 5 3 5 3 = i 4 5 1

4I was told that a numbering of this kind was also used by Don Knuth in an early paper on parsing; unfortunately, I have not been able to track this paper down.

5Incidentally, this illustrates very clearly the close connection between term algebras over a signature \Sigma  and

abstract syntax trees for grammars, as has already been discussed in great detail in [35]: our numbers correspond
to the function symbols of the signature ("named production rules").

8

S
L

R

L

R R

L L
\Lambda  \Lambda  i = i

1 S
3 L

5 R
3 L

5 R 5 R
4 L 4 L
\Lambda  \Lambda  i = i
(a) original tree (b) tree with rule numbers

Figure 2.2: Parse trees with numbers

The first view considers the i as null nonterminals, the second one as some set A of "pseudo
terminals", called actions.

Lemma 2.3.1 Consider a grammar G = (T ; N ; S; P) and its two number-augmented variants
G0 = (T ; N [ A; S; P0), where A is the set of actions viewed as null nonterminals, and G00 =
(T [ A; N ; S; P0), where the actions from A are viewed as pseudo terminals.

ffl The languages coincide up to a filtering of the actions i :

Lt(G) = Lt(G0) = filter \Lambda  Lt(G00)
L(G) = filter \Lambda  L(G0) = filter \Lambda  L(G00) where filter eliminates the symbols i

ffl The strings of the extended grammar G00 are in a one-to-one correspondence to the parse

trees for the original grammar: L(G00) = postorder \Lambda  T (G)

Proof : Obvious from the definitions (by a trivial induction on the height of the trees). \Lambda 
As a matter of fact, this extension conveys even more information. If the numbers in a G00 string
are used -- from left to right -- to determine the succession of reduction rules (and the positions
where they are applied), then we obtain an LR parsing of the sentence. This will become evident
in the course of the paper. Actually the relationship is even more elucidating (which we mention
here without proof):

ffl the postorder traversal of the tree corresponds to LR parsing;
ffl the preorder traversal corresponds to LL parsing.

Effect on grammar transformations: The addition of these numbers has essential advantages for
our approach: we can now freely transform our grammars (see Section 2.5). Since the numbers
(which determine the tree generation) have become part of the rules, they are kept in the proper
places under all transformations.

Based on the well-known fact that the postorder traversals are an isomorphic representation of the
corresponding trees, we will base most of our subsequent discussions on these number-augmented
grammars.

9

2.4 Continuations "(Lookahead")
In many parsing situations - both in LL- and LR-parsing - we need to know, which symbols
can possibly follow after some point (usually after an action symbol i ) that we have reached.
Traditionally one uses the concept of "follow sets" for providing this knowledge. However, the
whole treatment becomes much smoother when we integrate this knowledge more homogeneously
into our approach, again using suitable null nonterminals: The continuation symbol A
intuitively stands for "anything that can follow after the nonterminal A in any sentential form".

Definition 2.4.1 For any nonterminal A we define its continuation language as the set

L( A ) def= f w j S

\Lambda ) vAw g

For the start symbol S we employ the convention that L( S ) contains the special "end-of-input"
symbol `"', that is, " 2 L( S ). \Lambda 

The occurrence of a continuation symbol in some derivation of the kind

S

\Lambda ) u A v

is to be considered as an assertion that the following substring v is contained in L( A ). Such
an assertion is at certain points in the parsing process needed to disambiguate conflicting productions (as we will see later on).

The integration of these continuations has to tackle two problems: (1) Where shall continuation
symbols be attached to the grammar? (2) How are the continuation languages described? The
first aspect is trivial: We simply append the pertinent continuation symbol A to each righthand side of the nonterminal A. This is admissible, since we consider A as a null nonterminal.
This extension is illustrated for our running example in the left part of Table 2.2.

Grammar G0
S ! L = R 1 S

R 2 S
L ! * R 3 L

i 4 L
R ! L 5 R

Continuation Grammar

S "
L = R S

R
R S

L

Table 2.2: Grammar with continuations
The second aspect is not very hard either (as is illustrated by the right part of Table 2.2):
The continuation grammar can be directly read off the given grammar. This is entailed by the
following observation: consider a production

A ! u1Bu2
and a derivation S

\Lambda ) vAw ) vu

1Bu2w. This entails the relationship L( B ) ' L(u2 A ), which

is equivalent to the production

B u2 A

We use a different derivation symbol ` ' for the description of the continuation grammar in
order not to confuse the two aspects of the continuation symbols: (a) they act as null nonterminals in regular derivations; (b) they act as assertions representing continuation languages.
When computing these continuation languages, we have to treat the A as "normal" nonterminals defined by the continuation grammar. To symbolize this change of view, we use the symbol
`

\Lambda  ' for such derivations.

10

To obtain the continuation grammar we essentially have to scan the extended grammar for
all occurrences of the nonterminals A 2 N in order to obtain the definitions of the A . The
following principles then determine the continuation grammar:

1. If G contains the rule B ! v A w B , then the continuation grammar contains the rule

A w B .

2. Action symbols i are ignored. (They are null nonterminals and thus do not contribute

any visible symbols.)

3. Trivial rules of the kind A A are eliminated. (They do not contribute to the

continuation language.)

4. For the axiom S the rule S " is added. (When we address LR(k) grammars, we

have to add k such `end-of-input' symbols, i.e. S "k, in order to avoid undefined
lookaheads.6)

The right half of Table 2.2 shows the continuation grammar for our running example. In principle, this grammar could be further simplified (using the transformations to be presented in a
moment). But in general we will only need to extract very little information from it. Therefore
such a simplification is in general not worth the effort.

The following remarks should further elucidate our use of these continuation symbols:

ffl They mainly serve the purpose of simplifying and unifying some of the later considerations:

In connection with LR(k) parsing we will encounter references to "the first k elements of
the string s generated by ff". In these situations it will sometimes require continuation
symbols for ff to generate a sufficiently long string at all.

ffl The occurrence of a continuation symbol in some string, e.g. u A v, is an assertion meaning "v (or an extension vw of it) is in the continuation language L( A )". Consequently, a
string like u A v B can usually be shortened to u v B , since the information conveyed
by A is a weaker assertion than that provided by B .

Since the continuation symbols A are uniquely determined by the action symbols i , we will
usually omit them unless they are explicitly needed.

2.5 Equivalence-Preserving Transformations on Grammars
As mentioned in the introduction, the essence of our approach lies in suitable transformations
of the grammars. These transformations are all composed of a few very elementary rules that
we present in the sequel. (The fact that these rules are so trivial is the clue to the simplicity,
comprehensibility and flexibility of our approach.)

Definition 2.5.1 (Equivalence of grammars) We call two grammars G1 and G2 equivalent if
they generate the same language, i.e. Lt(G1) = Lt(G2). \Lambda 

Note that this definition implies that the two grammars have the same set T of terminal symbols.
But the nonterminals and the productions may be different.

In the course of the paper we will also encounter a generalized notion of "equivalence" (which
actually isn't an equivalence relation and therefore needs another name):

6This has already been observed by Knuth in his original paper [18].

11

Definition 2.5.2 (Quasi-equivalence) We say that the grammar G0 models the grammar G, if
Lt(G0) = L(G); that is, the sentences of G0 are the sentential forms of G. We sometimes also
say that the two grammars are quasi-equivalent.

Now we introduce a few elementary operations that produce equivalent grammars.
Definition 2.5.3 (Unfolding, folding) Let a grammar G be given which contains two productions of the form given below (where u, v or w may be empty). Then unfolding of B yields a
new production (and thus a new grammar G0)

A ! uBw
B ! v 

A ! uvw
B ! v (unfolding)

If the production (A ! uBw) is kept, then the new grammar also retains the same sentential
forms. The converse transformation is called folding.7 \Lambda 

Definition 2.5.4 (Deletion, addition) Let G be a grammar. We call a nonterminal B nonreachable if there is no derivation S

\Lambda ) u such that u contains the symbol B. If B is nonreachable, we

may delete any production of the kind B ! v. Conversely, we may add arbitrary productions
for nonreachable nonterminals.

Of course, we can also add or delete null nonterminals, that is, nonterminals that only derive
the empty string. \Lambda 

Definition 2.5.5 (Left factoring) Let a grammar G be given that contains one or more productions of the form shown below (where some of the wi may be empty). Then left factoring,
short factoring, extracts the common part v into a new production using a new nonterminal
Z.

A ! vw1

...

A ! vwn



A ! vZ
Z ! w1

...

Z ! wn

(left factoring)

\Lambda 
For top-down parsers the presence of left-recursive productions is disastruous. We can eliminate
a left-recursive production by introducing a new nonterminal with suitable production rules.
Alternatively, we can also use the Kleene star. The traditional form of this transformation is
the following:

A ! A u
A ! v  A ! v u

\Lambda   A ! v ZZ ! u Z

Z ! "

However, in connection with LR(k) parsing the generated "-production may cause problems.
Therefore we use a slightly more complex form of left-recursion elimination. This extended
form considers not only the definition of the nonterminal A, but also all its application points
B ! w1Aw2. Let us take the Kleene star version of the above transformation and unfold it. This
leads to B ! w1 vu\Lambda  w2. If we now reduce this Kleene star to normal productions, we obtain the
following rule.

7One only has to prohibit the pathological situation of folding the right-hand side of a production with itself.
That is, B ! u must not be converted into B ! B.

12

Definition 2.5.6 (Left-recursion elimination) We can replace left-recursive symbols at their
application points by the following rule:

B ! w1 A w2
A ! A u
A ! v



B ! w1 v Z

Z ! w2
Z ! u Z

(left-recursion removal)

This way, the left-recursion of A is replaced by the right-recursion of Z. \Lambda 
The decisive advantage of this form is that it does not generate "-productions. Moreover, as will
be seen later on, this retains the LR(k) property also in those cases, where there is a conflict
between w2 and u. (In these cases the simple variant of left-recursion removal would fail.) The
price to be paid is that we need a new Z for each application of A.

Sometimes we want to keep the nonterminal A (for example, because we need all sentential
forms, not just the sentences). Then we can use the following variant of the transformation:

B ! w1 A w2
A ! A u
A ! v



B ! w1 A Z
A ! v
Z ! w2
Z ! u Z

However, this variant can only be used, when all occurrences of A are transformed simultaneously
(since the productions of A are actually changed).

The correctness of our whole approach is based on the following obvious property:

Lemma 2.5.1 The above operations -- viz. unfolding, folding, factoring, addition, deletion and
left-recursion elimination -- generate equivalent grammars: Lt(G0) = Lt(G).

The addition of rule numbers as decribed in Section 2.3 becomes particularly important in
connection with these transformations. This shows very clearly e.g. in the rule for recursion
removal:

B ! w1 A w2 1
A ! A u 2
A ! v 3



B ! w1 v 3 Z

Z ! w2 1
Z ! u 2 Z

Without the rule numbers the transformed grammar would lead to a completely different kind
of parse tree; but the rule numbers - which represent tree constructors - are transformed accordingly with the rest of the productions such that the original tree productions are kept in
place. (The string for the extended grammar G0 remains unaffected by the transformations, and
this string is the postfix representation of the original tree.)

Lemma 2.5.2 Since we only work with number-augmented grammars G, the above transformations also retain the generated trees: T (G0) = T (G).

A similar observation holds for the continuation symbols A . According to the principles laid
out in Section 2.4 they are integrated into all transformations without problems. For example,
suppose that in the left-recursion removal all right-hand sides end in continuations. Then we
obtain:

B ! w1 A w2 1 B
A ! A u 2 A
A ! v 3 A



B ! w1 v 3 Z

Z ! w2 1 B
Z ! u 2 Z

13

Note that the continuations after 2 and 3 are no longer needed, because they are now followed
by a true string, which conveys more information than the assertion A .

This example also illustrates why it is usually the easiest way of proceeding to first apply all
transformations to a grammar without continuations and afterwards add the (uniquely determined) continuations to all those productions that end in action symbols.

14
Part I
Towards LR(k) Grammars

15

Chapter 3
Normal Forms for Grammars
The main idea of our approach is to transform a given grammar into an equivalent form that
is, however, better suited for parsing. Incidentally, the forms into which we want to bring
our grammars are very close to well-known normal forms: the Chomsky normal form and the
Greibach normal form.

We split the overall transformation into three consecutive subtransformations, which bring the
grammar closer and closer to the intended final form. All transformations are solely based
on the rules from Section 2.5, thus immediately guaranteeing the correctness of the overall
transformation process.

3.1 First Normal Form: Bounded Production Length
The first transformation is a trivial customization; it is inspired by the well-known Chomsky
normal form.1

The idea. LR-style parsing owes its speed to some extent to the fact that it utilizes an induced
regular grammar. Therefore, our first goal is to bring our grammar a bit closer to this form. In
order to generate the envisaged normal form, we introduce auxiliary nonterminals and production
rules so that we finally obtain a grammar in which the right-hand sides of all productions have
at most length 2. We also consider the variant, where continuation symbols are admitted.

Definition 3.1.1 (First normal form / Chomsky normal form2) A grammar G is said to be in
first normal form, short: 1NF, if every production is of one of the three forms

X ! Y Z or
X ! Y i or
X ! i

respectively

X ! Y Z or
X ! Y i A or
X ! i A

with X 2 N [ Z, Z 2 Z, Y 2 N [ T . Note that we constrain the symbols Z on the second
position of the right-hand sides to a special set Z of newly introduced auxiliary nonterminals.
\Lambda 

The following construction transforms a given grammar into its 1NF. We formulate it for the
variant without continuation symbols.

1This normal form is also the basis of the Cocke-Younger-Kasami algorithm [16, 40], which -- like LR-style
parsing -- is a bottom-up parser.

2Actually, this is not quite the Chomsky normal form, because there we had to convert productions A ! bZ

i

with terminal b 2 T into A ! BZi and B ! b. This, however, would not be suited for our later design.

16

Construction 3.1.1 (First normal form) Apply the following process as long as possible: For
any production

X ! Y1 \Delta  \Delta  \Delta  Yn (n * 3)
introduce a new nonterminal Zi and transform the production into the two productions

X ! Y1 Zi Zi ! Y2 \Delta  \Delta  \Delta  Yn .
For the start symbol S we introduce a special "start" production

Z0 ! S 0
The action 0 indicates the successful parse of the whole string. \Lambda 

For our running example (see Table 2.1), Construction 3.1.1 leads to the following grammar.

S ! L Z1

R 2
L ! * Z3

i 4
R ! L 5

Z0 ! S 0
Z1 ! = Z2

Z3 ! R 3

Z2 ! R 1

For reasons of easier readability in some of the later stages, we rearrange the layout of the
grammar a bit, leading to the version in Table 3.1, which will serve as our reference point in the
following.

Grammar G1
Z0 ! S 0 Z1 ! = Z2

Z2 ! R 1

Z3 ! R 3

S ! L Z1

R 2

L ! * Z3

i 4

R ! L 5

Table 3.1: First normal form (rearranged layout)
Theorem 3.1.1 (Correctness of the 1NF construction) Construction 3.1.1 transforms a given
grammar G = (T ; N ; S; P) into an equivalent grammar G0 = (T ; N [Z; Z0; P0), which is in 1NF.

Proof : Trivial: we only use equivalence-preserving transformations. Moreover, the construction
obviously terminates, since the right-hand sides become stricly shorter. Since every production
in G ends with a symbol i , the 1NF form follows immediately. \Lambda 

Discussion. There is an obvious relationship between the size of the given grammar and the
number of new nonterminals required for the 1NF. Here, the size of the grammar is the sum of
the lengths of all its right-hand sides (not counting the action symbols i ).

Lemma 3.1.1 The 1NF construction introduces auxiliary nonterminals Z0, : : : , Zn\Gamma k, where
n is the size of the grammar G and k is the number of the productions in G (= the number of
actions i ).

This observation is interesting, because it is shown in [5] that for an LL(1) grammar this size
coincides with the number of states of the corresponding LALR(1) grammar (which is n + 1).
This will turn out to be a much deeper connection in Section 7.1.

17

3.2 Second Normal Form: Leading Terminals
LR-style parsing also obtains its efficiency to a great extent from the fact that decisions between
conflicting reductions can be made by only inspecting the first input symbol(s). In LL parsers
the same principle is embodied in the so-called first (and follow) sets. This motivates our second
customization of the grammar (the crucial step), which is inspired by the well-known Greibach
normal form.

The idea. All nonterminals which start a rule have to be unfolded because we ultimately want
to arrive at a grammar where all rules start with a terminal symbol t or an action symbol i .
Again, we have to distinguish the variants with and without continuation symbols. So we strive
for productions of the kind

Z ! t \Delta  \Delta  \Delta  Zi \Delta  \Delta  \Delta  j \Delta  \Delta  \Delta 
Z ! i \Delta  \Delta  \Delta  Zi \Delta  \Delta  \Delta  j \Delta  \Delta  \Delta  or also

Z ! t \Delta  \Delta  \Delta  Zi \Delta  \Delta  \Delta  j \Delta  \Delta  \Delta  k A
Z ! i \Delta  \Delta  \Delta  Zi \Delta  \Delta  \Delta  j \Delta  \Delta  \Delta  k A

where the \Delta  \Delta  \Delta  indicate arbitrary mixtures of Zi and/or i . This is formalized in the following
definition.

Definition 3.2.1 (Second normal form / Greibach normal form3) A grammar G is said to be
in second normal form, short: 2NF, if every production is of one of the two forms

Z ! t z j z j * 1
Z ! i z j z j * 0

with Z 2 Z, t 2 T (or t 2 T [ N ), z 2 (Z [ A)\Lambda . If we also admit continuation symbols, the
strings z that end in an action symbol i are followed by a continuation symbol A . \Lambda 

Note that as a byproduct the so-called "first sets" used in many of the traditional approaches
are now immediately evident from the grammar.

Example: In our grammar G1 (see Table 3.1), we start by unfolding all applications of R. (In
principle, the order in which the nonterminals are unfolded does not matter, but for reasons of
efficiency some orders are better than others.) Note that R can be eliminated, since it is no
longer needed.

Z0 ! S 0 Z1 ! = Z2

Z2 ! L 5 1

Z3 ! L 5 3

S ! L Z1

L 5 2

L ! * Z3

i 4

At this point we should apply left factoring to the two productions for the nonterminal S. This
leads to the introduction of an auxiliary nonterminal Z4:

S ! L Z4 Z4 ! Z1

5 2

Since we do not want rules that have Zi's as their leftmost symbol, we unfold Z1 immediately.
(Z1 could now be eliminated, because it is no longer reachable).

S ! L Z4 Z4 ! = Z2

5 2

We call this transformation full left factoring ; that is, left factoring with subsequent unfolding
of the leading Zi (if such Zi are generated). The analogous principle is used for full leftrecursion removal.

3Actually only the first kind of production is admitted in the true Greibach normal form, but our -- unavoidable

-- generalization is close enough.

18

Now we have to unfold the next nonterminals, that is, L and S. Since this does not entail a
need for further left factoring, we are done (in this example). The result is shown in Table 3.2.

Grammar G2
Z0 ! * Z3 Z4 0

i 4 Z4 0

Z2 ! * Z3 5 1

i 4 5 1

Z3 ! * Z3 5 3

i 4 5 3
Z4 ! = Z2

5 2

Table 3.2: Second normal form

Including Sentential Forms. The extension to sentential forms can be achieved by a minor
adaption of our 2NF construction: after unfolding a nonterminal we simply keep the pertinent
production instead of eliminating it.4 With this modification our running example from Table 3.2
obtains the form presented in Table 3.3.

Grammar G2a
Z0 ! S 0

R 2 0
L Z4 0
* Z3 Z4 0
i 4 Z4 0

Z2 ! R 1

L 5 1
* Z3 5 1
i 4 5 1

Z3 ! R 3

L 5 3
* Z3 5 3
i 4 5 3
Z4 ! = Z2

5 2

Table 3.3: Second normal form with sentential forms
In this grammar the former nonterminals S, L and R are treated like terminals (since there no
longer exist productions for them). Hence, the language generated by this grammar comprises
all sentential forms of the language of our original grammar.

As illustrated by the above example, we apply the following construction. However, by contrast
to the example, we do not apply left factoring yet.

Construction 3.2.1 (Second normal form) Let G be a grammar in 1NF. Then do the following
as often as possible:

Pick some nonterminal A 2 N (this means that A occurs as leftmost symbol in at least one
production).

1. If A is left-recursive, apply "full left-recursion elimination".
2. Unfold all occurrences of A in the grammar. (This is feasible due to the absence of leftrecursive productions.)

3. Eliminate the productions for A from the grammar (since it has become unreachable).

We can employ this construction in two different variants, depending on whether we want only
terminal sentences or all sentential forms:

4These additional productions correspond to the so-called goto table in traditional LR-style parsers, whereas
our original productions correspond to the action table (see Section 7.1).

19

Variant a: Eliminate the productions that contain occurrences of A.
Variant b: Keep the productions.

\Lambda 
Theorem 3.2.1 (Correctness of the 2NF construction) Variant a of Construction 3.2.1 transforms a given grammar G = (T ; N [ Z; Z0; P), which is in 1NF, into an equivalent grammar
G0 = (T ; Z; Z0; P0), which is in 2NF.

Proof : The proof requires the verification of three facts: equivalence, the 2NF property, and
termination.

Equivalence is trivial, since we only apply equivalence-preserving transformations.
The 2NF property requires two observations: (1) In each step, the complete unfolding lets the
pertinent nonterminal A vanish from the grammar. So, ultimately, there will be only nonterminals Z 2 Z left. (2) The rules invariantly have the required forms: initially, this is true, because
G is in 1NF (and 1NF is a special case of 2NF). The unfolding obviously retains the patterns.

Termination is easily seen: In each major step one nonterminal A 2 N vanishes. And each
nonterminal A has only finitely many occurrences that need to be unfolded or considered in
left-recursion elimination. \Lambda 

Corollary 3.2.1 Variant b of Construction 3.2.1 yields a grammar G0 = (T [N ; Z; Z0; P0) that
models G (in the sense of Def. 2.5.2), i.e. it generates all sentential forms of G: L(G) = Lt(G0).

Remark : Our auxiliary nonterminals Zi 2 Z essentially correspond to the LR(0) items from
traditional approaches. (This will be discussed in greater depth in Section 5.2.)

3.3 Third Normal Form: Unique Shifting
A major deficiency of the above 2NF construction is the omission of left factoring. Left factoring
is not mandatory for our normal forms. Therefore the correctness theorem works out nicely. But
it is crucial for efficiency both of the construction itself and the resulting parser. As a matter of
fact, it will turn out to be the key issue for establishing the LR(k) property. However, in certain
pathological cases the left factoring process may run into termination problems. Therefore it is
technically simpler to separate the presentation of this aspect from the rest of the normalization
process. But in practice it should be amalgamated with the other transformations in order to
increase the efficiency of the overall process.

In our running example we have used left factorings (plus subsequent unfoldings) such as

S ! L Z1

L 5 2

 S ! L Z4

Z4 ! Z1

5 2

 S ! L Z4

Z4 ! = Z2

5 2

before eliminating the nonterminal L through unfolding. This is obviously reasonable, since it
avoids the duplication of all L-productions in the right-hand sides of S, which would otherwise
lead to

S ! * Z3 Z1

i 4 Z1
* Z3 5 2
i 4 5 2

20

Moreover, since left factoring is an equivalence-preserving transformation, there seems to be no
reason for not integrating it into the process from the very beginning.

Yet, there is one subtle problem: The process may not terminate. Even though this can only
happen in a few pathological cases, it is nonetheless unpleasant. However, in order not to
interrupt the derivation process at this point, we defer the discussion of this termination problem
(to Section 6) and proceed with the normalizing transformations.

Definition 3.3.1 (Third normal form) A grammar G is said to be in third normal form,
short: 3NF, if it is in 2NF and there are no two productions

Z ! x u

x v

the right-hand sides of which start with the same symbol.

Except for the aforementioned rare termination problem this normal form can obviously be
obtained by the following principle:

ffl Apply left factoring wherever possible.
However, we can improve the efficiency by delaying the left factorings as long as possible. This
may be called "lazy left factoring ".

Construction 3.3.1 (Third normal form) Modify the 2NF construction 3.2.1 by adding the
following transformation before step (1):

0. Apply full left factoring to all occurrences of the nonterminal A.
Finally, when all nonterminals have been unfolded, apply full left factoring to all terminals in
all productions. \Lambda 

Note: if we would always apply left factoring as soon as possible, we could create unnecessarily
many additional Zi symbols. Consider a situation like

Zi ! i : : :

A : : :
B : : :

A ! i : : :
B ! i : : :
Due to the successive unfolding of A and B we will create two auxiliary Z-symbols instead of the
one that suffices, if we factorize all three occurrences of i in one sweep. So we wait with the left
factoring as long as possible. However, we must not forget to do it at the latest before unfolding
a nonterminal A 2 N in the 2NF construction (as has been demonstrated for the nonterminal
L in our running example).

Theorem 3.3.1 (Correctness of the 3NF construction) For any given grammar G, Construction 3.3.1 yields an equivalent grammar G0, which is in 3NF - provided that the construction
terminates.

Proof. Follows directly from the correctness of the 2NF construction and the fact that left
factoring is an equivalence-preserving transformation. \Lambda 

21

Chapter 4
Main Theorem: 3NF + LL(k) =
LR(k)

So far our derivation has led to a construction that transforms a given grammar G into an
equivalent grammar G0. Moreover, we have argued on intuitive grounds that this transformed
grammar is better suited for parsing purposes, because it has the LR(k) property. Now we have
to make this informal claim and argument precise. This is done in our main theorem.

Theorem 4.0.2 (Main theorem) Consider a grammar G and its transformed 3NF version
G0. The original grammar G is LR(k) iff the transformed grammar G0 is LL(k).

The proof of this theorem also sheds light on the close relationship between the LR(k) and LL(k)
property. Recall that the only difference between 2NF and 3NF is that the former does not yet
include left factoring. It turns out that this is also the characterisitc difference between LL- and
LR-style parsers.

Corollary 4.0.1 Consider a grammar G without left recursion and its transformed 2NF version
G0. If G0 is "conflict-free" (in the sense of Def. 4.0.5 below), then G is LL(k).

The remainder of this section is devoted to the proof of Theorem 4.0.2. Yet, there is an unfortunate difficulty: we can find several different definitions of "LR parser" in the literature.1 This
forces us to first choose one of these definitions, before we can prove any equivalence at all. In
such a situation it is advisable to employ standard textbooks as the point of reference. Unfortunately, there still is a choice: one class of books treats the problem from the point of view of
practical compiler construction; the main reference here is, of course, the famous "dragon book"
[1] (to which most other books refer anyhow, e.g. [2, 39]). The other class takes the viewpoint
of formal-language theory [19, 29, 32]; here the choice is not so obvious, but we basically employ
[29]. Knuth in his original paper [18] actually addresses both variants.

There is a second problem that we have to take into consideration: as was mentioned above, our
construction does - in its current form - not necessarily terminate. In order not to obfuscate
the following discussion we ignore this problem for the moment. (The formulation " : : : its
transformed 3NF version G0 " in the above theorem entails the successful completion of the
construction.)

1It is telling that DeRemer and Pennello [8] point out in connection with LALR parsing that several papers
actually define what they call "nqlalr(1)", that is, "not quite lalr(1)". It also comes as a surprise that some of
the books and papers in the literature do not make any attempt to establish the equivalence of their definitions.

22

Before we go into the details of the proof we repeat the outcome of the normalization process
for our running example. Table 3.3 already contains the essence of the 3NF - albeit without the
continuation symbols. Since we have to refer to them in the course of the proof, it is now time
to explicitly show them, which is done in Table 4.1.

Grammar G3
Z0 ! S 0 S

R 2 0 S
L Z4 0 S
* Z3 Z4 0 S
i 4 Z4 0 S

Z2 ! R 1 S

L 5 1 S
* Z3 5 1 S
i 4 5 1 S

Z3 ! R 3 L

L 5 3 L
* Z3 5 3 L
i 4 5 3 L
Z4 ! = Z2

5 2 S
S " L Z4 S

S

Table 4.1: Third normal form with continuations
As can be seen here, the information S " is mandatory for disambiguating the two productions for Z4.

Proof of the Main Theorem
Remark: This is the point, where the straightforward simplicity of our approach ends, because
now we have to link it to the traditional notions and notations.

Most books and papers follow - in more or less adapted notations - the original definitions
given by Knuth in his seminal paper [18] (see e.g. [19, 29, 39]). We summarize these concepts
to the extent needed for our proof. The focus of our attention are the points, where reductions
can take place, that is, the occurrences of right-hand sides of productions (A ! ff) in the given
strings.

Definition 4.0.2 (Handle, viable prefix) Consider a rightmost derivation that ends with an
application of the production (A ! ff):

S

\Lambda ) v A w ) v ff w with w 2 T \Lambda 

Then ff is called the handle of the string v ff w. Moreover, any prefix of v ff is called a viable
prefix. \Lambda 

Remark 1 : Since LR parsing is only concerned with rightmost derivations, we presume this
property for all derivations in the remainder of this section without mentioning it explictily
every time anew.

Remark 2 : The introduction of the action symbols i in our approach yield a nice characterization of the handles: Any handle ff ends with an action symbol i . (Unfortunately this is only a
conceptual characterization, since the i are "invisible", i.e. not present in the input string.)

The quest for efficient parsing suffers from a fundamental problem: at any given point in time we
have in general only seen an initial fragment of the given string; that is, we have to deal with a
limited horizon. Therefore we are interested in languages, where the next action - i.e. the handle
- can always be uniquely determined in spite of this limited horizon. The most important class

23

of this kind are the LR(k) grammars: here the horizon extends k symbols beyond the handle.2
Similar to the functions for left contexts in [2] or [19] we can introduce a corresponding function
that makes this horizon property more explicit:

Definition 4.0.3 (Horizon) We define the k-horizon of a production (A ! ff) by the following function:

horizonk(A ! ff) = f v ff u j S

\Lambda ) v A u w; juj = k g

\Lambda 

Let the "starts" relation s1 v s2 express the fact that the string s1 is a prefix of the string s2,
that is, s2 = s1u for some suitable (possibly empty) u. Extend this relation to sets of strings
by letting A v B express the fact that some string in A is a prefix of some string in B, that is,
9 a 2 A; b 2 B : a v b. Then the LR(k) property can be expressed as

Definition 4.0.4 (LR(k) grammar) A grammar G is an LR(k) grammar, if the following
condition holds:

horizonk(A ! ff) v horizonk(B ! fi) ) (A ! ff) = (B ! fi)
\Lambda 

This means: whenever there are two strings with rightmost derivations of the form

S

\Lambda ) v

1 A u w1 ) v1 ff u w1 (u w1 2 T

\Lambda ; juj = k)

S

\Lambda ) v

2 B w2 ) v2 fi w2 (w2 2 T

\Lambda )

with different productions (A ! ff) 6= (B ! fi), then

v1 ff u 6v v2 fi w2
Put into other words: if we have a string s1 with a handle ff, and if we consider a horizon that
extends k symbols beyond the handle, then any other string s2 with a different handle has to
differ from s1 already within the horizon.

Example: To see the meaning of these definitions we consider one of the simplest possible
counterexamples (see Table 4.2).3

Grammar C1
S ! a S a 1

a 2

Table 4.2: A simple non-LR(k) grammar

Consider two different strings and their rightmost derivations:

S

\Lambda ) a a S a a ) a a a a a (= s

1)

S

\Lambda ) a a a S a a a ) a a a a a a a (= s

2)

2In order to avoid problems at the right end of the input strings, we have to provide at least k "end-of-file"
symbols `"' as continuation S of the start symbol (as it was already done by Knuth).

3This example is a simplified variant of examples given e.g. in [29, 30]. It is nice, because it provides a

counterexample without being ambiguous.

24

This example shows that the grammar cannot be LR(1). Consider the string s1. The handle ends
after the third symbol, the LR(1)-horizon therefore comprises the first four symbols. However,
these first four symbols are identical to those of s2, which has a different handle. (In this example
it happens to be the same production, but at a differrent point.) Actually, the example even
shows that the grammar is not LR(2). And by using arbitrarily long strings it can be easily
seen that it actually is not LR(k) for any k.

Impact on our construction. The transformation of the above grammar generates the version
in Table 4.3. Here we have a so-called shift/reduce conflict for Z3, because S contains the

Grammar C2
Z0 ! S 0 S

a Z3 0 S

Z1 ! S Z2

a Z3 Z2
Z2 ! a 1 S

Z3 ! S Z2

a Z3 Z2

2 S
S "

a S

Table 4.3: The transformed non-LR(k) grammar

lookahead symbol a. Hence, the grammar is not LR(1). As a matter of fact, the continuation
of the shift production and the continuation of the reduce production both generate arbitrarily
long sequences `aaa : : : ', which demonstrates that the grammar is not LR(k) for any k. \Lambda 

The notion of "conflict" used in the example has to be made precise. (Recall that `

\Lambda  ' denotes

the derivation relation, where the continuation symbols A actually generate the continuation
strings.)

Definition 4.0.5 (Conflict) Let G be a grammar in 3NF. A nonterminal Z has a shift/reduce
conflict of length k, if it has two productions of the kind (Z ! t z1) and (Z ! i z2) such that
z1

\Lambda  uw

1 : : : and z2

\Lambda  tuw

2 with juj = k \Gamma  1.

Z has a reduce/reduce conflict of length k, if it has two productions of the kind (Z ! i z1)
and (Z ! j z2) such that z1

\Lambda  uw

1 and z2

\Lambda  uw

2 with juj = k. \Lambda 

This concept coincides - for the special case of grammars in 3NF - with the classical notion of
strong LL(k) grammar (as e.g. formulated in [2] or [19]).

Lemma 4.0.1 If a 3NF grammar G has no k-conflicts, it is a strong LL(k) grammar. \Lambda 
Now we are ready to prove our main theorem.
Proof of Main Theorem 4.0.2. We want to show that the original grammar G is LR(k) iff
the transformed 3NF grammar G0 is LL(k), i.e. has no k-conflicts.

From the correctness theorems for our normal forms we know that G and G0 are "quasi-equivalent",
that is, L(G) = Lt(G0). Based on this fact we show the two directions of the theorem separately.

1. G0 is conflict-free ) G is LR(k) .

This is equivalent to the negated form "G is not LR(k) ) G0 has k-conflicts", which we
will actually use. If G is not LR(k), then we have two rightmost derivations

S

\Lambda ) v

1 ff i u1 w1

S

\Lambda ) v

2 fi j u2 w2

sucht that 8!:

i 6= j
v1ffu1 = v2fiu2
ju1j = k

(\Lambda )

25

(Actually, for large fi the horizon may also end inside of fi; but this does not influence
the argument. W.l.o.g. we also assume that v2 is at least as long as v1.) Due to the
aforementioned equivalence there is a corresponding derivation in G0

Z0

\Lambda ) v

1ffZiz ) v1ff i z

0z \Lambda ) v1ff i u1w1

with a production (Zi ! i z0) and z0z

\Lambda ) u

1w1.

Since we are talking about rightmost derivations, i is the leftmost action symbol. Due
to the left factorings that entail the uniqueness property of the 3NF, the derivation of v1ff
in G0 is uniquely determined. Hence, Zi is the first point, where a choice may occur and
we have v1 = v2. We distinguish two cases:

(a) jffj = jfij.

Then (\Lambda ) together with v1 = v2 entails also ff = fi and u1 = u2. Hence there must be
two productions

Zi ! i z0

j z00

such that we obtain

Z0

\Lambda ) v

1ffZiz ) v

1ff i z

0z \Lambda ) v1ff i u1w1

) v1ff j z00z \Lambda ) v1ff i u1w2

This entails z0

\Lambda  u

1w1 and z

00 \Lambda  u1w2 with ju1j = k. Therefore Zi has a reduce/reduce conflict.
(b) jffj 6= jfij.

W.l.o.g. we take jffj ! jfij. By the uniqueness of the 3NF (\Lambda ) entails fi = fftu and
u1 = tuu2 with suitable t 2 V; u 2 V \Lambda . By the same argument as above we obtain the
need for two productions

Zi ! i z0

t z00

such that z0

\Lambda  u

1w1 and z

00 \Lambda  uu2w2 with ju1j = k. This is a shift/reduce conflict.

2. G is LR(k) ) G0 is conflict-free.

This is equivalent to the negated form "G0 has k-conflicts ) G is not LR(k) ", which we
will actually use. We consider the case of a reduce/reduce conflict:

Zi ! i z0 with z0

\Lambda  uw

1;

j z00 with z00

\Lambda  uw

2 juj = k

As above, consider two derivations which exhibit this conflict:

Z0

\Lambda ) v

1ffZiz ) v

1ff i z

0z \Lambda ) v1ff i uw1

) v1ff j z00z \Lambda ) v1ff i uw2

Without loss of generality we can assume that i is the leftmost action symbol in the
string. (If it were not, we could use the corresponding sentential form, where all action
symbols further left have been reduced to their corresponding nonterminals.) Due to the

26

3NF property, v1ff is uniquely determined. Because of the equivalence of the grammars
we also have corresponding derivations in G:

S

\Lambda ) v

1ff i uw1

S

\Lambda ) v

1ff j uw2 with juj = k

which violates the LR(k) condition.
The case of a shift/reduce conflict is shown analogously.

This concludes the proof of our Main Theorem. \Lambda 

27
Part II
From Grammars To Parsers

28

Chapter 5
Emulated Versus
Classical LR Parsing

Our first observation is that our transformed grammars induce parsing algorithms that behave
like classical LR(k) parsers in the sense that they make the same decisions and thus have the
same expressive power. However, they are programmed differently: While the induced parsers
of our grammars are realized in a recursive-descent style, the traditional LR(k) parsers are
table-driven, based on the paradigm of stack automata. In the following we want to study this
relationship more closely and show, how our approach can also be used to produce classical LR
parsers.

5.1 A Simple (Top-Down) Parsing Scheme
For the following discussion we have to refer to the parser that is induced by a grammar. To
make these references sufficiently precise we briefly sketch one way of obtaining such parsers.
Even though our approach can be used for parsers written (or generated) in any language, we
feel that the paradigm of functional programming provides by far the best and most elegant
framework for expressing the pertinent concepts. Therefore, we work with a recursive-descent
parser based on higher-order functions.1

The programming technique is based on the philosophy that the grammar already is the parser.
Basically, every nonterminal becomes a function, and its productions provide the corresponding
function definitions.2 This is illustrated for our standard grammar in Table 5.1. As can be seen
here, we merely introduce a few connectors between the symbols in the grammar, but otherwise
leave the structure of the grammar completely unchanged. The sequential composition `A; B'
stands for "first apply parser A and then apply parser B", and the choice `AjB' for "apply parser
A or parser B". In addition, we need a lifting operator `!' to convert terminal symbols into
parsers. At the end of each production, we add an action actioni which takes care of the tree
constructor i .

Since this is not a paper on functional programming, we defer the actual code of these higher1That higher-order functions are an elegant means for directly implementing parsers from given grammars has
been legendary in the functional-programming community for quite a while. We have actually used this principle
successfully in the teaching of parsing concepts to students for many years [23]. Detailed descriptions of such
techniques are given e.g. in [13, 14, 12, 22] (where further references can be found as well).

2We essentially use the notation of the functional language Opal [24, 10, 11, 26], slightly enriched in order

to increase flexibility. But it should be noted that the code would look essentially the same in other functional
languages such as Ml, Haskell and Miranda.

29

fun S L R : Parser
def S == (L ; " = "! ; R ; action1)

j (R ; action2)
def L == (" \Lambda  "! ; R ; action3)

j ("i"! ; action4)
def R == (L ; action5)

-- S ! L = R 1
-- S ! R 2
-- L ! * R 3
-- L ! i 4
-- R ! L 5

Table 5.1: Grammar G as a functional program

order functions to the Appendix (even though it's less than 20 lines). Suffice it to list the
functionalities:

fun ; : Parser \Theta  Parser ! Parser
fun j : Parser \Theta  Parser ! Parser
fun ! : String ! Parser

-- sequential composition
-- alternative parsers (not commutative)
-- lifting of tokens

with

type Parser == Tree \Theta  String ! Tree \Theta  String

It should be mentioned that the equivalence-preserving grammar transformations considered in
Section 2.5 are also valid in the functional interpretation:

Lemma 5.1.1 The operations unfolding, folding, factoring, addition and deletion are equivalence-preserving transformations for the functions associated with the grammar.

Note: since left-recursive grammars are undefined in the functional interpretation, a similar
result cannot hold for them. On the contrary, it is only through left-recursion elimination that
the functional interpretation makes any sense.

Even though the above parsing scheme uses a recursive-descent strategy and contains backtracking, it works very well and without any reservations in our framework, since left recursion
has been eliminated and backtracking due to "shift/shift" conflicts has been minimized by left
factoring.

5.2 "Emulated" LR Parsing
When we apply the above parser generation to a 3NF grammar, we obtain what may be called
"emulated LR parsing". That is, the parser has LR(k) power, because it makes the same
decisions and performs the same actions as a traditional LR(k) parser, but it is programmed in
the style of a recursive-descent parser. But it is a relatively straightforward exercise to derive
from this parser a "classical LR parser".

Consider the induced parser for the 3NF of our running example (see Table 4.1). If we parse
the simple string 00 \Lambda  i = i "00 then we obtain the following process. (Recall that a function
composition like Z3; Z4 applies from left to right, i.e. first Z3 and then Z4. By `a::L' we denote
the prepending of the token a to the list L. And we write i instead of actioni to increase

30

readability.)

Z0(00\Lambda  i = i "00)
= \Lambda  :: (Z3; Z4; 0 )(00i = i "00)
= \Lambda  :: i :: ( 4 ; 5 ; 3 ; Z4; 0 )(00= i "00)
= \Lambda  :: i :: 4 :: 5 :: 3 :: (Z4; 0 )(00= i "00)
= \Lambda  :: i :: 4 :: 5 :: 3 :: = :: (Z2; 0 )(00i "00)
= \Lambda  :: i :: 4 :: 5 :: 3 :: = :: i :: ( 4 ; 5 ; 1 ; 0 )(00"00)
= \Lambda  :: i :: 4 :: 5 :: 3 :: = :: i :: 4 :: 5 :: 1 :: 0

As can be seen here, the function compositions of the Zi and j act essentially like a stack. This
becomes even more evident, if we introduce a more compact notation for representing the above
calculation process:

\Theta Z

0\Lambda  \Lambda  24

0
Z4
Z335 i

26666
4

0
Z4

3
5
4

37777
5

4

2664

0

Z4

3
5

3775

5 24

0
Z4

3 35 3 ^

0
Z4* = ^

0
Z2* i

2664

0

1
5
4

3775

4 24

0
1
5 35 5 ^

0
1 * 1 \Theta  0 \Lambda  0 (\Lambda )

We can give this notation two kinds of meaning:

ffl We can consider it as a shorthand for describing the above functional computation process.

In this interpretation we simply record at every point in the string the (composed) function
that is applied at this point to the reststring.

ffl Or we can consider it as a shorthand for some classical mathematical formula, where [ff]x[fi]

means ff

\Lambda ) xfi. This view provides a nice calculus for doing correctness calculations.

In any case this representation provides the formal basis, on which we can argue about the
dynamic execution of the induced parser.

Definition 5.2.1 (Parsing process) We call a term such as (\Lambda ) above a parsing process:
between any two elements of the string the (composed) parsing function ' that is applied at this
point to the reststring is recorded in the form ['].

5.3 Towards "True" LR Parsing
The transition from emulated to classical LR parsing is based on a simple observation, which is
hinted at in (\Lambda ) by the gray coloring.

Lemma 5.3.1 In a parsing process like (\Lambda ) above only the bottom elements of the composed
functions are actually needed. All other functions are redundant.

Sketch of proof : This is shown relatively easily: as an intermediate step for the better understanding of this effect consider a variant, where we actually apply the reductions i rather than
only recording them. For example, the production L ! i 4 leads to the following reduction:

\Theta Z

0\Lambda  \Lambda  24

0
Z4
Z335 i

26666
4

0
Z4

3
5
4

37777
5

4

2664

0

Z4

3
5

3775

: : :  \Theta Z0\Lambda  \Lambda  24

0
Z4
Z335 L

2664

0

Z4

3
5

3775

: : : (\Lambda 1)
31

By inspecting the 3NF in Table 4.1 we can see that the production Z3 ! L 5 3 also yields the
action 5 (and even 3 ) that becomes the next bottom element of the function composition.
That is, the information above 4 is not needed for deducing that the next bottom element is

5 .

To continue the example, suppose we have also applied the next reduction 5 and proceed to

3 . That is, we have the situation:

\Theta Z

0\Lambda  \Lambda  24

0
Z4
Z335 R 24

0
Z4

3 35 3 ^

0
Z4* : : :  \Theta Z0\Lambda  L ^

0
Z4* : : : (\Lambda 2)

Again the 3NF grammar provides through its production Z0 ! L Z4 0 all necessary information
to deduce the next bottom function.

Of course, it is not necessary to actually perform the reductions as it has been done above for
didactic purposes. To see this consider the following stage of the process (\Lambda ):\Theta 

Z0\Lambda  \Lambda  "

...
Z3# i "

...
4 # 4 "

...
5 # 5 "

...
3 # 3 "

...
?# = : : : (z)

Reduction 3 tells us (see Table 2.1) to go two elements back (not counting 3 itself). In doing
so we encounter 5 , which recursively forces us to go one element back. Analogously the 4
brings us beyond the `i'. The second element requested by 3 then is the `\Lambda '. So we end at [Z0].
Since the reduction 3 generates an `L', the production Z0 ! L Z4 : : : in the 3NF yields [Z4] as
the next bottom element. This is formally expressed in the following lemma, which also entails
the proof of Lemma 5.3.1. \Lambda 

Lemma 5.3.2 Consider a production A ! ff i from the original grammar. The handle ff can
occur in the parsing process in a form like (where we omit the intermediate functions)

: : : "

...
Zi# ff i "

...
Zj# : : :

iff the 3NF contains the production Zi ! AZj : : : . Analogously for i instead of Zj .
The proof follows directly from the construction of the 3NF (as illustrated by the above examples). \Lambda 

Note 1 : This way of proceeding gives us complete freedom to either do the reductions right
away or to delay them until some better suited time -- a freedom that we wouldn't like to lose.

Note 2 : The recursive backward search can, of course, be shortened by using an appropriate data
structure (actually a stack) for "memoization". However, since this paper is not about programming techniques we defer this technical amelioration to the Appendix. It should be mentioned,
however, that this implementation leads to a program that directly mimics the classical stack
automata used in the literature.

Modification of the 3NF Construction. When we do no longer look at the higher Zi and

i in the stacks, then there is no reason to keep them in the grammar. Hence we could shorten
the grammar in Table 4.1 to the form presented in Table 5.2 by cutting the right-hand sides after
the first Zi or i . Note that such a cutting would be the first time in this paper that we apply a
transformation that is not equivalence-preserving, but derives its correctness from an according

32

Grammar GLR
Z0 ! S 0

R 2
L Z4
* Z3
i 4

Z2 ! R 1

L 5
* Z3
i 4

Z3 ! R 3

L 5
* Z3
i 4
Z4 ! = Z2

5 S

Table 5.2: Traditional LR-style grammar

alteration of the corresponding algorithm. Actually, this grammar describes the language of the
so-called viable prefixes. However, in our further proceeding this cutting is actually not helpful
(except for the above reference to the notion of "viable prefix" found in the literature).

33

Chapter 6
Termination of the 3NF
Construction

We have essentially completed the presentation of our construction and its correctness - except
for the unpleasant termination problem. Up to now we have allowed ourselves the comfort
of simply ignoring this problem. And there are good reasons, why this neglection could be a
reasonable way of proceeding. From a pragmatic viewpoint1 the nontermination problem may
actually not be worth the effort of a special treatment:

ffl The nontermination does not endanger the correctness of our approach, but at most its

efficiency. This is a very strong reason.

ffl There are several possibilities for overcoming the nontermination problem (which we will

discuss in a moment). This is a strong reason as well.

ffl The nontermination happens only in very few and quite pathological situations (as will be

illustrated below). But this is a weak reason.

ffl We note in passing that for an LL(k) grammar the 2NF construction suffices (and this one

always terminates). However, this criterium is weak in the context of LR parsing.

6.1 What Is The Problem?
Before we discuss the solutions, we want to see more clearly where the problem actually arises.
The following example illustrates (by way of the simplest possible grammar) how a transformation process may run into infinite cycling.

Example: Consider the toy grammar in Table 6.1. In this grammar, the nonterminal A generates
an odd number of stars, while the nonterminal B generates an even number of stars.

When we transform this grammar into its 1NF, we arrive at the following intermediate form:

1There is also a nice gedanken experiment from a theoretical viewpoint: the following considerations - in
particular the possibility for a "lazy grammar transformation" - also allow us to perceive infinite grammars. The
left factoring transforms a given grammar into a new one, which we can consider to be "larger" in a suitably
chosen ordering (induced by unfolding). This enables the classical method of ideal completion etc., which makes
the notion of infinite grammars well-defined. On the basis of these grammars all our theorems hold without any
reservations.

34

Grammar L
S ! A 1

B 2

A ! * B 3 B ! * A 4

i 5

Table 6.1: A pathological counterexample

S ! A 1

B 2
Z0 ! S 0

A ! * Z1
Z1 ! B 3

B ! * Z2

i 5
Z2 ! A 4

Now we move towards the 2NF by unfolding the nonterminals S, A and B:

Z0 ! : : :

* Z1 1 0
* Z2 2 0

: : :

Z1 ! B 3

* Z2 3
i 5 3

Z2 ! A 4

* Z1 4

Left factoring of Z0 leads to the situation

Z0 ! : : :

* Z3

: : :

Z3 ! Z1 1 0

Z2 2 0

Now we have to unfold Z1 and Z2 (in order to re-establish the 2NF form) leading to

Z3 ! : : :

* Z2 3 1 0
* Z1 4 2 0

: : :

This requires another left factoring:

Z3 ! : : :

* Z4

: : :

Z4 ! Z2 3 1 0

Z1 4 2 0

Unfolding in turn leads to

Z4 ! : : :

* Z1 4 3 1 0
* Z2 3 4 2 0

: : :

Evidently, Z4 repeats the problem that we just "solved" for Z3 - albeit with longer right-hand
sides. Hence we have a sequence of growing elements Z0, Z3, Z4, : : : and thus the process will
never terminate. \Lambda 

The problem with this example clearly lies in the fact that the auxiliary nonterminals Z1 and Z2
occur in a combined left factoring situation. This very strange setting makes it plausible that
the situation is indeed pathological and will virtually never happen in practice. Nevertheless,
since the above grammar is LR(1) - actually even SLR(1) - we have to accept the fact that our
construction does not always work -- if we include left factoring.

As illustrated by the above example (and shown by the normal-form theorems), the termination
problem may only arise through iterated full left factoring. The clue to all solutions lies in the
following fact.

Fact: We can stop the grammar transformation process

35

at any point in time without losing correctness.
By stopping the process we obtain partly non-normalized grammars, that is, grammars which
are "amost" in 3NF (except for a very few productions).

Definition 6.1.1 We say that a grammar is almost in 3NF, if it is in 2NF and only a few
productions violate the uniqueness requirement of the 3NF.

An equally useful variant of this definition could be given by admitting a few productions which
violate the 2NF property by still having productions of the form (Zi ! Zj : : : ), i.e. the leading
Zj has not yet been unfolded.

The only problem is to decide, when the factor-unfold process should be stopped, that is, when
we should content ourselves with an almost-3NF grammar. Fortunatley, there is a very simple
criterium for this decision: we initially do the full left factorings for the "block" of all Zi
productions generated by the 2NF. This leads in general to a further block of new Zj productions;
these in turn call for more left factorings, and so forth. That is, we obtain a series of blocks
of new Zi productions. Our empirical data indicates that these consecutive blocks should be
strictly decreasing in size. Otherwise the construction is very likely to be infinite. (As a matter
of fact, this was the case in all our experiments.)

Note that the danger of prematurely stopping the transformation process due to the coarse
block-size criterium only concerns the efficiency of the resulting parser, not its correctness!

Construction 6.1.1 (Modified 3NF Construction) We alter the 3NF Construction 3.3.1 such
that it performs the left factorings in the blockwise manner described above. We stop the factorunfold process as soon as the blocks are no longer strictly decreasing.

Lemma 6.1.1 The modified 3NF construction always terminates and generates a 3NF grammar
or an almost-3NF grammar.

The proof is trivial. In practice, the result will mostly be in 3NF. But in a few pathological
cases (as illustrated by the above example) some non-3NF productions will remain. But they
still are 2NF.

In the following Sections 6.2 - 6.4 we consider three ways of dealing with these almost-3NF
grammars.

6.2 A Simple Solution: Backtracking
If we have an almost-3NF grammar containing unnormalized productions such as

Z4 ! Z2 3 1 0

Z1 4 2 0

in the above example we can still apply all our parsing schemes from Section 5. The only effect
now is that the backtracking facility built into this programm will actually occur in the parsing
process. This is "only" a question of efficiency, not of correctness. (In Section 9.3 we will briefly
sketch implementation techniques that make this backtracking even negligeable in "99%" of all
practical cases.)

6.3 Another Simple Solution: Lazy Grammar Transformation
We consider again an almost-3NF grammar G. But if we now encounter a production like

36

Z4 ! Z2 3 1 0

Z1 4 2 0

we do not apply backtracking in the parser. Rather, we perform one further unfolding/factorization step to the grammar, leading to a modified grammar G0. Then we continue the parsing
with G0. We call this technique lazy grammar transformation.2

Since G0 allows us to proceed at least one symbol ahead in the input string, the number of
possibly needed transformation steps is bound by the length of the input string. This guarantees
termination.

We note in passing that this also allows a nice adaption to incremental parser generation [28].

6.4 A Solution Based on Equivalence Classes
The two solutions presented above both have the same disadvantage: they burden the runtime
of the parser. Therefore we now consider another solution that is slightly more intricate but
only burdens the parser generator. Moreover, it is closer to classical LR-style parsing.

Unfortunately this solution has a major drawback as well: it only works for the classical LR
parsing of Section 5.3 and not for the emulated LR parsing of Section 5.2. So our freedom of
choosing the one or the other algorithmic realization gets lost.3

The clue to the solution is the observation (expressed in Lemma 5.3.1) that in a parsing process
only the bottom elements are needed. Or, put into other words, all elements after the first Zi
or j in a production can be ignored. This allows us to cut the infinite unfold-factor cycles.

To understand the underlying idea let us consider a phenotypical production that requires left
factoring. That is, we resume the example from Section 6.1, but now in a more schematic form:
we are in a situation, where the original 3NF construction stops with an almost-3NF grammar.
This means that we have a few unnormalized productions of the following kind. (To make the
explanation of our arguments easier we prefer to use letters Yi instead of Zi for the new symbols.)

Z ! t Z1 z1

t Z2 z2
t i z3

: : :

 Z ! t Y1

: : :
Y1 ! Z1 z1

Z2 z2

i z3

The zi stand for sequences of Zj and j possibly with trailing continuations A . The resulting
new production calls for immediate unfolding, where the only problem arises, when two identical
terminals are created by Z1 and Z2, necessitating a further left factoring. For illustration
purposes let us assume that we do not yet run into the cycle.

Y1 ! Z1 z1

Z2 z2

i z3

 Y1 ! t0 Z3 z4 z1

: : :
t0 Z4 z5 z2

: : :

i z3

 Y1 ! t0 Y2

: : :
: : :

i z3
Y2 ! Z3 z4 z1

Z4 z5 z2

2In accordance with the popular terminology of Java we might also baptize this as "just-in-time transformation".

3Note that traditional LR parsing only realizes one of these possibilities anyhow.

37

Now we have to unfold the productions for Y2, where the need for left-facorization may arise
again. Let us assume that this is the point, where the cycle occurs.

Y2 ! Z3 z4 z1

Z4 z5 z2

 Y2 ! t00 Z1 z7 z4 z1

: : :
t00 Z2 z8 z5 z2

: : :
t00 i z9 z5 z2

: : :

 Y2 ! t00 Y3

: : :
: : :
: : :
Y3 ! Z1 z7 z4 z1

Z2 z8 z5 z2

i z9 z5 z2

The left factoring of t00 leads to Y3, which is a repetition of the situation of Y1, albeit with longer
trailers. Processes of this kind are the (only) source of nontermination.

But the classical LR parser has a decisive feature that distinguishes it from the emulated LR
parser: in the productions of Y1, Y2 and Y3 anything beyond the leading Zis and j s will be cut
off. We have indicated this by the light printing of z3z1, z4z2 etc. Therefore we may consider
two productions as being equivalent, whenever they are equal after this cutting.

Definition 6.4.1 (Core, equivalence) In a production (A ! Ziv) the part Zi is called the
core and v the extension. Analogously for (A ! i v).

We call two nonterminals equivalent, if the cores of their productions are identical. \Lambda 

In our schematic example we have the cores Y1 b=fZ1; Z2; i g, Y2 b=fZ3; Z4g and Y3 b=fZ1; Z2; i g,
which shows the equivalence of Y1 and Y3.

So we may push the modified 3NF construction a bit further: we continue the unfolding/factorization process except for those Yi that are equivalent to an existing Yk. This way the brute-force
stopping criterium from Section 6.1 (i.e. the blocks are not strictly decreasing in size) is extended
by the finer criterium that equivalent productions are not treated anew.4

As a matter of fact, we could refine this even further: only those equivalences are dangerous
that lead to a cycle. In our above example we have this situation

Y1 Y2 Y3
But when the equivalence only generates a dag-like situation it is harmless:

Y2 Y3 Y4 Y5 Y6
Y1

Y7 Y8 Y9 Y10

equivalent

Here we may continue the transformation of Y8. Even though this creates additional nonterminals Yi it has the great advantage that we keep the freedom of choosing among all parsing
algorithms.

The thus extended transformation process has two possible outcomes: either it fully succeeds
with a 3NF grammar or it still ends with an almost-3NF grammar. But in this grammar each
unnormalized nonterminal Yi is known to be equivalent to an existing (normalized) one - and
it is known to be indeed responsible for a cycle.

Construction 6.4.1 (Modified 3NF construction - first attempt) We modify the 3NF construction as follows: when the left factoring would lead to a new nonterminal Yj that is equivalent to
an existing Yi, then we use the old Yi instead. \Lambda 

4We could use this finer criterium from the very beginning, but it it is much more costly than the block-size
criterium and it is only needed in a few pathological cases.

38

Lemma 6.4.1 With the above modification the construction always terminates.
Proof. We are given a set of Zi productions resulting from the 2NF and start the process of left
factoring. Since there are only finitely many Zi and j , the number of sets over these Zi and j
and thus the number of possible new Yk is also finite. \Lambda 

We now have guaranteed termination for our construction. And also correctness is still guarantteed - provided that we use the classical LR parsing variant, which only looks at the bottom
elements in the parsing process (see Section 5.3). However, we may lose the LR(k) property. The
reason is that our above notion of equivalence does not take the lookahead into consideration.

To formulate this definition the notion of k-frontier is convenient: the k-frontier of a string v
is the set of the prefixes of length k of all strings derivabel from v:

frontierk(v) = f u j v

\Lambda  u w; juj = k g

(We use the derivation `

\Lambda  ', since we may have to employ continuation symbols in order to

achieve the required length k.)

Definition 6.4.2 (k-equivalence) Consider two equivalent nonterminals Y1 and Y2 (i.e. they
have the same core). Y1 and Y2 are called k-equivalent if each pair of corresponding productions

Y1 ! : : :

Z z1

: : :

Y2 ! : : :

Z z2

: : :

generates the same prefixes of length k: frontierk(Z z1) = frontierk(Z z2).

Theorem 6.4.1 (Adaption of Main Theorem) If we use k-equivalence in the modified 3NF
construction above, then the Main Theorem 4.0.2 still holds.

Proof : We only need to show that the identification of k-equivalent nonterminals does not
introduce additional conflicts. This can be seen in our schematic example above. Consider Y3
that - without the identification with Y1 - would have to be transformed further into

Y3 ! t0 Y4

: : :
: : :

i z9 z5 z2

Y4 ! Z3 z4 z7 z4 z1

Z4 z5 z8 z5 z2

Since Y1 and Y3 are k-equivalent, Y3 has a shift/reduce or reduce/reduce conflict of length k if
and only if Y1 has the same conflict. \Lambda 

Note: The emulated LR parser also uses the extension parts in the parsing process. Therefore
Y1 and Y3 could not be identified in this setting. But for the classical LR parser the extension
parts are only needed for the k-lookahead in the case of conflicts. And this is respected by the
notion of k-equivalence.

39

Chapter 7
Relationship to Classical LR-Style
Parser Generation

Our Main Theorem 4.0.2 demonstrates that the result of our 3NF construction is an LR(k)
grammar. Now we want to study, how the construction itself relates to the classical ways of
obtaining LR-style parsers. This will provide further insights into both our method and the
traditional techniques. Moreover, it will turn out (somewhat surprisingly) that - and why - our
approach combines the power of LR(k) parsers with the efficiency of LALR(k) parsers - at least
to some extent.

7.1 The Classical "Sets-of-Items" Construction: LR(0) Parser
The classical construction for LR(k) parsers is the "sets-of-items" construction as described
in the standard textbook [1]. It appears worthwhile to study the connections between this
traditional construction and our method more deeply. Roughly speaking, we have the following
correspondences:

ffl The 1NF construction produces the so-called "kernel items".
ffl The 2NF construction corresponds to the "closure forming".
ffl The 3NF construction generates the so-called "sets of items". More precisely, this holds for

the 3NF construction modulo the core-based equivalence relations of Def. 6.4.1 and 6.4.2.

In order to ease understanding we illustrate the comparison by a classical example1 (see [1],
p. 222): Table 7.1 presents the well-known grammar for arithmetical expressions.

Proposition. Consider the Grammar of Table 7.1 and its LR(0) items (as given in [1], p. 225).
There is a one-to-one correspondence between the kernel items and the Zi and i in our 1NF.
More precisely, the Zi and i represent exactly the parts following the dot.

Proof : The lemma can be shown by a simple induction. Due to its straightforward simplicity
we do not present this induction here formally but rather illustrate it on the basis of the 1NF
of the above grammar K0 (see Table 7.2).

1Unfortunately we cannot take our running example here, because we need "serious left recursion" in order to
demonstrate some complexities.

40

Grammar K0
S ! E 1
E ! E + T 2

T 3
T ! T * F 4

F 5
F ! ( E ) 6

i 7

Table 7.1: A left-recursive grammar

Grammar K1
S ! E 1
E ! E Z1

T 3
T ! T Z3

F 5
F ! ( Z5

i 7

Z0 ! S 0
Z1 ! + Z2

Z3 ! * Z4
Z5 ! E Z6

Z2 ! T 2
Z4 ! F 4
Z6 ! ) 6

Table 7.2: First normal form
The "canonical collection of sets of LR(0) items" in [1] is based on two functions, closure and
goto. We start the process by forming the closure of the productions for S (where a `\Delta ' is added
in front of the right-hand sides). The kernel items are the original productions, the nonkernel
items are added by the closure forming: the productions of all nonterminals that are directly
preceded by a `\Delta ' are added (recursively, if necessary) to the set. (Evidently, this corresponds to
the unfolding process in our 2NF construction.)

I0 : S ! \Delta E Z0 kernel item

E ! \Delta E + T
E ! \Delta T
T ! \Delta T \Lambda  F nonkernel items
T ! \Delta F
F ! \Delta (E)
F ! \Delta i

At the right of each kernel item we point out its corresponding Zi or j from the above 1NF.
Now the operation goto(I0; A) is applied to the set I0 and every nonterminal A that is preceded
by a dot: this operation shifts the `\Delta ' over the nonterminal A and thus generates the kernel items
of the next set. If necessary, the closure operation is then applied to these kernel items. The
final result of this process is shown in Table 7.3.

From this illustration the above lemma is evident: the shifting of the dot in the goto operation
corresponds to the introduction of the new Zi in the 1NF construction. Therefore the kernel
items indeed are in a one-to-one correspondence to the Zi and i . \Lambda 

As mentioned above, the 2NF construction corresponds to the closure forming.
Finally, the sets with several kernel items are those points, where our 3NF construction performs
the left-factorization. This essentially yields the sets of (kernel) items.

41

I0 = S ! \Delta E Z0

E ! \Delta E + T
E ! \Delta T
T ! \Delta T \Lambda  F
T ! \Delta F
F ! \Delta (E)
F ! \Delta i

I1 = goto(I0; E)

S ! E\Delta  1
E ! E \Delta  +T Z1

I2 = goto(I0; T )

E ! T \Delta  3
T ! T \Delta  \Lambda F Z3

I3 = goto(I0; F )

T ! F \Delta  5

I4 = goto(I0; `(')

F ! (\Delta E) Z5
E ! \Delta E + T
E ! \Delta T
T ! \Delta T \Lambda  F
T ! \Delta F
F ! \Delta (E)
F ! \Delta i

I5 = goto(I0; i)

F ! i\Delta  7

I6 = goto(I1; +)

E ! E + \Delta T Z2
T ! \Delta T \Lambda  F
T ! \Delta F
F ! \Delta (E)
F ! \Delta i

I7 = goto(I2; \Lambda )

T ! T \Lambda  \Delta F Z4
F ! \Delta (E)
F ! \Delta i

I8 = goto(I4; E)

F ! (E\Delta ) Z6
E ! E \Delta  +T Z1

I9 = goto(I6; T )

E ! E + T \Delta  2
T ! T \Delta  \Lambda F Z3

I10 = goto(I7; F )

T ! T \Lambda  F \Delta  4

I11 = goto(I8; `)')

F ! (E)\Delta  6

Table 7.3: The sets of items of grammar K0
Proposition. The sets of items Ik are in a one-to-one correspondence to the equivalence classes
of the 3NF according to the core-based equivalence relation of Def. 6.4.1 and 6.4.2.

Proof : We illustrate the proof again in terms of the above grammar. For example, in our 3NF
construction we have - among others - the following situations (see also Table 7.4):

Z0 ! : : :

T 3 1 0
T Z3 3 1 0
: : :

Z5 ! : : :

T 3 Z6
T Z3 3 Z6

: : :

By left-factorization we obtain the new Zi-productions

Z8 ! 3 1 0

Z3 3 1 0

Z11 ! 3 Z6

Z3 3 Z6

which then undergo the usual unfolding and (if necessary) iterated factorization process leading
to the 3NF. However, since Z8 and Z11 have the same core, they are equivalent and thus belong
to the same set of items. This set is determined by the core (in this case I2). \Lambda 

For example, in our above grammar the left factorizations yields new Zi with the following
cores: Z7 ^=f 1 ; Z1g, Z8 ^=f 3 ; Z3g, Z9 ^=f 2 ; Z3g, Z10 ^=fZ6; Z1g and Z11 ^=f 3 ; Z3g. So we obtain
the according correspondences I1 ^=Z7, I2 ^=fZ8; Z11g, I8 ^=Z10 and I9 ^=Z9.

Remark 1 : This reference to the core-based equivalence classes demonstrates that the traditional
LR-style parsing indeed corresponds to what we baptized classical LR parsing.

42

Grammar K2
Z0 ! S 0

E Z7
T Z8
F 5 3 1 0
( Z5 5 3 1 0
i 7 5 3 1 0
Z1 ! + Z2

Z2 ! T Z9

F 5 2
( Z5 5 2
i 7 5 2
Z3 ! * Z4
Z4 ! F 4

( Z5 4
i 7 4

Z5 ! E Z10

T Z11
F 5 3 Z6
( Z5 5 3 Z6
i 7 5 3 Z6
Z6 ! ) 6

Z7 ! 1 0 S

+ Z2 1 0
Z8 ! 3 1 0 S

* Z4 3 1 0

Z9 ! 2 E

* Z4 2

Z10 ! ) 6

+ Z2 Z6
Z11 ! 3 Z6

* Z4 3 Z6

Table 7.4: Second normal form

Remark 2 : The plain notion of equivalence leads to the sets of LR(0) items, that is, to an LR(0)
parser. If we use k-equivalence instead, we obtain an LR(k) parser. The relationship of this fact
to the traditional techniques will be discussed in the following.

7.2 Stronger Than LALR(k)
It is known from literature that LALR(k) is as strong as LR(k) with respect to shift/reduce
conflicts. (It may, however, perform a few more reductions before it realizes a conflict.) But it
is strictly weaker with respect to reduce/reduce conflicts. Since we have already shown that our
method leads to LR(k) parsing, we have the following corollary to Theorem 4.0.2:

Corollary 7.2.1 The parsing induced by the 3NF is strictly stronger than LALR(k) parsing.
But we also want to illustrate this property by a concrete example in order to gain further
insights into the reasons for this greater power. In the literature one can find various little
grammars that demonstrate the superiority of LR(1); as usual we pick here an example from [1]
(p. 238), which is given in Table 7.5.2

Grammar H0
S ! a A d 1

a B e 2
b A e 3
b B d 4

A ! c 5 B ! c 6

Table 7.5: An LR(1) but non-LALR(1) grammar
As usual we derive the 2NF of this grammar, which in particular creates the following two
productions

2Incidentally, Beatty [5] (p.1019) uses this grammar as an example for an LL(k) grammar, which is not
LALR(k) for any k.

43

Z1 ! A Z2

B Z3
c 5 Z2
c 6 Z3

Z4 ! A Z5

B Z6
c 5 Z5
c 6 Z6

This calls for left factorings of c, which create Z7 and Z8. The final result is presented in
Table 7.6.

Grammar H3
Z0 ! S 0

a Z1 0
b Z4 0

Z1 ! A Z2

B Z3
c Z7
Z2 ! d 1
Z3 ! e 2
Z7 ! 5 Z2

6 Z3

Z4 ! A Z5

B Z6
c Z8
Z5 ! e 3
Z6 ! d 4
Z8 ! 5 Z5

6 Z6

Table 7.6: Third normal form

The potential reduce/reduce conflicts in Z7 and Z8 are resolved here by inspecting the onesymbol lookahead of the productions Z2 and Z3 in the first case and Z5 and Z6 in the second
case. Since this conflict is not resolved by the LALR(1) parser (see e.g. [1], p. 238), this
demonstrates that our parser is strictly stronger.

The explanation is simple: The LALR technique puts Zis with the same core into an equivalence
class and merges their extensions. In the above example this applies to Z7 and Z8, which leads
to a situation of the form

Z7=8 ! 5 fZ2, Z5g

6 fZ3, Z6g

The resulting union does no longer disambiguate the two reductions.
Note: In this example our method successfully terminates already in the basic 3NF construction
and therefore never has to look at the cores and their induced equivalences. But even if we
would use that criterium from the very beginning (which is admissible but inefficient), we would
have to employ the notion of 1-equivalence in order to obtain an LR(1) parser. Under this
equivalence Z7 and Z8 still would not be identified!

7.3 Less Expensive Than Classical LR(1): Empirical Results
The big disadvantage of LR(1) parsers is the huge size of their tables. Since their superiority
over LALR(1) parsing only shows in a few relatively pathological cases, this size prohibits their
practical use. We claim that our approach actually produces (much?) smaller tables than LR(1)
parsers, even though in general they are not quite as small as LALR(1) tables.

It is well-known that the difference in sizes between LALR(1) tables and LR(1) tables can be
orders of magnitude (a few hundred as opposed to several thousands). This naturally raises the
question, whether the size of our grammars is closer to the one or to the other.

An open problem. Unfortunately we do not have a formal proof that gives a precise cost
analysis, but there are two reasons to conclude that the size will be closer to that of an LALR(1)
parser: the first one is some "higher-level reasoning" (a weak one) and the second one is evidence
gained from experimental test data (a strong one).

44

(1) Plausibility. The classical construction of LR(1) parsers essentially duplicates all productions (i.e. sets of items) for all possible follow symbols. It is this duplication that causes
the explosive growth in size. By contrast, our 3NF construction works with continuation symbols that represent continuation languages, i.e. whole groups of follow symbols. Hence, the
duplication only happens for these groups and not for each symbol in each group.

This is essentially the same principle as that used for the equivalence classes of LALR(1) parsers.
However, our approach is not quite as coarse as the LALR approach and only equivates cores
to an extent that does not violate the LR(k) property.

Example: Again, we illustrate this claim by a concrete example. In [1] (p. 231-236) the grammar
from Table 7.7 is used to illustrate the derivation of an LR(1) parser.

Grammar I0
S ! C C 1
C ! a C 2

b 3

Table 7.7: Initial grammar

Again we generate the 3NF of this grammar, which is presented in Table 7.8.

Grammar I3
Z0 ! S 0

C Z1 0
a Z2 Z1 0
b 3 Z1 0

Z1 ! C 1

a Z2 1
b 3 1
Z2 ! C 2

a Z2 2
b 3 2

Table 7.8: Second normal form

This grammar has 7 "states" (three symbols Zi and four actions i ), whereas the corresponding
LR(1) grammar has 10 "states" Ij (see [1], p. 235). This shows that our approach actually
produces smaller tables - at least in one example. \Lambda 

This illustrates the "meta-level reasoning" that is based on the following observations:

ffl First, recall from Lemma 3.1.1 that the number of symbols Zi and i in the 1NF is equal

to the size of the original grammar G, where the "size" s of a grammar is the sum of the
lengths of its right-hand sides. (Moreover, this is also the number of kernel items in the
sets of LR(0) items.)

ffl The 2NF does not get much larger than the 1NF: most of our transformations are unfoldings, which leave the number of the Zis invariant. The only increase could occur through
left-recursion removals. But these are rare - and if we use the classical LR parsing variant,
they are not needed at all.

ffl Only the left factorings in the transition to the 3NF introduce new Zjs. This should,

however, not do too much harm:

- Left factorings are not too frequent.

45

- In many cases the left factoring entails an unfolding by which some other Zk vanishes,

thus leaving the overall count invariant. As a matter of fact, only when left factoring
becomes necessary due to the unfolding of two different nonterminals and when the
following Zk are also needed in productions elsewhere, the number increases.

It is telling that in all of our examples the 3NF is equal or even smaller than the 1NF! So we may
expect the number of Zis of our 3NF to be in the order of the size s of the original grammar G.
By contrast, the LR(1) tables are in the order t \Lambda  s, where t is the number of terminal symbols in
the grammar and s is again the size of G. This is based on the reasoning that only left factorings
lead to a reduction of the sets of items Ij and that every LR(0) set is duplicated for all its follow
symbols.

(2) Empirical evidence. The above "meta-level reasoning" needs backing by empirical measurements; these are presented in Figure 7.1. We have used three major grammars: The c

Grammar (nonterminals / productions)
2946
524

715
281

1244
222

pascal (130 / 237)
c (72 / 203)
opal (52 / 104)

Figure 7.1: Comparison of table sizes
grammar as specified in the standard book [17], the pascal grammar in its original version
published in [15] and a simplified version of our revised language opal 2ff [11].

The dark shaded areas represent the number of states generated by the traditional LR(1) method
(as described in [1] p. 232), whereas the light shaded areas represent the number of states
generated by our method. As can be seen from these figures, our method creates tables that are
by a factor 3 - 5 smaller than the traditional tables.

These empirical results back the above "meta-level reasoning".

46

Part III
Variations, Observations and Applications

47

Chapter 8
Facts About Grammars and
Languages

Even though it is not part of our derivation, we note in passing that our approach also provides
nice insights into general properties of grammars and their languages. For example, the following
facts are known from the literature (see e.g. [29, 19, 2]):

Proposition. The following properties hold for the various grammar classes:

1. Every LL(k) grammar is also an LR(k) grammar.
2. For a given grammar G it is decidable, whether it is an LR(k) grammar for given k.

Analogously for LL(k) grammars.

3. Every LR(k) grammar is unambiguous. Analogously for LL(k) grammars.
4. There are LR(1) grammars which are not SLR(k) or LALR(k) for any k.

The first property is evident from our construction, since the k symbols in LL(k) refer to the
handle plus some lookahead (which therefore has length ^ k), whereas the k in LR(k) refers
only to the lookahead. Actually, we can see quite clearly here, why LR-style parsing is more
powerful than LL-style parsing: suppose a nonterminal A has two productions of the form

A ! B : : :
A ! C : : :

In such a situation a recursive-descent parser has to choose which of the two productions to
take. Suppose that the two nonterminals B and C have productions

B ! t : : :

...

C ! t : : :

...

with the same leading terminal t. (This means that the language does not have the LL(1)
property.) In this case -- which may be baptized a shift/shift conflict -- the recursivedescent parser for A cannot make the choice by simply looking at the next token: it needs

48

backtracking. At such points our construction delays the need for making the decision by
applying left factoring. And this delaying of decisions makes the LR paradigm more powerful.

This coincides with the observation (see Section 2.3) that LL parsing corresponds to a preorder
traversal of the tree. This means that the parser has to decide, whether to insert an (invisible)
action symbol i , before it has seen the handle ff, whereas in LR parsing the decision is taken
after having seen the handle.

Properties (2) and (3) follow directly from the fact that our construction works. The last
property has actually been shown by the example in Section 7.2.

These properties show, how strong the LR grammars are. But for the weaker SLR(k) and
LALR(k) grammars the relationships are more complicated.

Proposition. For grammars with "-productions the LL(1) and SLR(1) classes are incomparable
[2, 5] But for "-free grammars the following properties are shown in [5]:

1. If G is LL(1) then it is also LR(0).
2. There are LL(k) grammars which are not LALR(k) for any k * 2
3. For p-reduced grammars (i.e., grammars without null nonterminals) the following property

holds:

If G is LL(1) then it is also LALR(1).

The first property follows directly from the fact that we need no left factoring. This and the
absence of "-productions guarantees that no i occurs as first element of a right-hand side.
Hence the grammar is LR(0). The same kind of reasoning also shows the last property. The
second property is actually shown by the grammar in Section 7.2.

Proposition. The following properties hold for the various language classes:

1. Every LR(k) language is also an LR(1) language.
2. Every LR(k) language actually is also an LR(0) language (see [19] p. 143).
3. Every LR(k) language is SLR(1).
4. Every LL(1) language is LR(1) and thus also SLR(1).

Note that we now talk about the induced languages, not about the grammars themselves.
Sketch of proof. Since we are no longer interested in the (grammar-dependent) trees but only
in the pure strings, we can ignore the actions i . This means that we can continue the left
factoring process beyond these i , which is guaranteed to terminate at some point due to the
LR(k) property. The thus constructed grammar clearly is an LR(0) grammar. \Lambda 

These short sketches shall suffice to indicate that our modified approach not only yields useful
techniques for practical parser generation, but also provides insights into theoretical questions.

49

Chapter 9
Variations
In the previous sections we have presented an approach that is more flexible and easier to
comprehend than traditional LR-style parsers, but besides that exhibits the same expressive
power. Now we want to show, how this increased flexibility can be utilized to ease certain
features (such as error handling) and to increase the power (such as adaption to non-LR(1)
languages).

9.1 Variations on the Grammar Presentation
Throughout the paper we have used a very puristic presentation of the grammars: all productions
obey the classical form of context-free grammars and are teminated by action symbols. But in
practice we need more notational comfort and flexibility.

9.1.1 Features From EBNF
In practical grammars one wants to have the convenience of the so-called EBNF form. Its
features can be trivially translated into elementary forms of productions. As a representative
example we consider the Kleene star. The question here is, whether opting for a left- or a
right-recursive solution makes any difference.

left-recursive solution

A ! u\Lambda  v 1  A ! B v 1 A

B ! B u 2 B

3 B

 A ! 3 Z

Z ! u 2 Z

v 1 A

right-recursive solution

A ! u\Lambda  v 1  A ! C v 1 A

C ! u C 2 C

3 C

This shows that both solutions are equally good : in the left-recursive solution problems arise,
when u Z and v A , that is, u+v A and v A exhibit a k-conflict. In the right-recursive solution
such problems occur, when u C C and C , that is, u+ C and C exhibit a k-conflict. Since

C = v A these two situations are identical.

The same considerations apply to the case of u+ and other features such as optional terms or
repetitions with separators.

50

9.1.2 Action-less Productions
We have used the action symbols i throughout this paper as a central means for organizing the
overall transformation, parsing and tree-formation process. However, there are situations when
one actually wants to do without these action symbols. And this can be done without problems
- at least if some precautions are observed.

The major reason, why one may want to omit action symbols, is nicely illustrated by the
classical grammar for arithmetic expressions (see Table 7.1). We can write it in modified form
as in Table 9.1, where we retain the numbering from the old version.

Grammar Ka
S ! E
E ! E + T 2

T
T ! T * F 4

F
F ! ( E ) 6

i 7

Table 9.1: A left-recursive grammar

Here we have the action symbols only in those places, where we actually want to perform
semantic actions. So we spare a number of pure adaption actions. This illustrates the main
reason for omitting action symbols: it is often desirable to introduce auxiliary nonterminals into
a grammar only for modularization purposes. These nonterminals do not play a role in the
abstract syntax tree but only make the grammar "better": it may express precedences, avoid
ambiguities or simply be more readable. This typically happens in two kinds of situations:

1. A nonterminal with many variants can be made more readable by making each right-hand

side into a separate production with a new nonterminal. This does not cause problems as
can can be seen from an application of A in a production such as Z ! A i :

A ! B

C
B ! u 1
C ! v 2

Z ! A i  Z ! A i

B i
u 1 i
C i
v 2 i

The effect here is that the nonterminal A does not show up in the abstract syntax tree
(because there is no action that creates it), but only B and C. Therefore the production
Z ! A i can be omitted.

2. There is also the converse situation, where one wants to have the nonterminal A in the

syntax tree, but not the variants B and C:

A ! B 1

C 2
B ! u
C ! v

Z ! A i  Z ! A i

B 1 i
u 1 i
C 2 i
v 2 i

The effect here is that the nonterminals B and C do not show up in the abstract syntax
tree, but only A. Therefore the pertinent right-hand sides B 1 i and C 2 i can be
eliminated.

51

These schematic examples illustrate that our techniques go through without problems in such
situations. However, one should be careful, when recursive productions are involved. To see this
consider the following productions and their transformed form.

A ! B 1
B ! B u

v

 A ! v Z

Z ! u Z

1

Here it is almost impossible to give any useful code for the semantic action 1 , because all
information about how many u's there are has been lost.

9.1.3 Precedence And Associativity
Almost every language encounters the problem that the standard operators in arithmetic and
logical expressions shall be written in infix notation and obey certain precedence and associativity
rules. This complicates the grammar considerably. Moreover, it makes it quite different from the
actually intended abstract syntax tree. (In Section 10 we will show how to handle applications,
where the programmer is allowed to invent his own infix operators.)

The most famous example for such a grammar is probably that for arithmetic expressions given
in Table 9.2. It is charming, because it expresses the essence of the envisaged abstract syntax
tree. And it is only because of precedence rules that we have to switch to the much less obvious
grammar used in practice (see Table 7.1). We will now sketch, how such an ambiguous grammar
can actually be used for parsing, if we add some additional information. All we need to say is that
the operator `\Lambda ' binds stronger than `+', and that both shall be treated as being left-associative.

Grammar M0
S ! E 1
E ! E + E 2 [left-assoc, \Lambda  O/ +]

E * E 3 [left-assoc]
i 4

Table 9.2: A highly ambiguous grammar

The 3NF construction for this grammar yields the grammar in Table 9.3.

Grammar M1
Z0 ! S 0

E Z6
i 4 Z6
Z6 ! 1 S

+ Z2 Z6
* Z4 Z6

Z2 ! E Z7

i 4 Z7

Z7 ! 2 E

+ Z2 Z7
* Z4 Z7

Z4 ! E Z8

i 4 Z8

Z8 ! 3 E

+ Z2 Z8
* Z4 Z8
S " E Z6

Z7
Z8

Table 9.3: The 3NF of M0

This is obviously highly ambiguous, since E leads to +, \Lambda  and " as possible first symbols.
But now we want to employ the information that `\Lambda ' has a higher precedence than `+' and that
they shall both be left-associative. (We only consider a 1-symbol lookahead.)

52

ffl The precedence information says that no reduction of E + E 2 may take place, when the

next symbol is `\Lambda '. So the production for Z7 is modified to (Z7 ! 2 E1 ), where E1 is E
minus the symbol `\Lambda '.

ffl The precedence information also says that the reduction E \Lambda  E 3 must take place, when

the next symbol is `+'. That is, in Z8 the reduction has precedence over the shift of `+'.
This means that we have to eliminate the production (Z8 ! + Z2Z8).

ffl The left-associativity says that a shift/reduce conflict is to be resolved in favor of the

reduction. This means that the two productions (Z7 ! + Z2Z7) and (Z8 ! \Lambda  Z4Z8) have
to be eliminated from the grammar.

ffl If we would have right-associativity of, say `+', then we would have to take the symbol `+'

out of E1 as well. That is, we would have to act as if the precedence + O/ + would hold.

The results of these considerations are presented in Table 9.4. As can be seen immediately, this

Grammar M2
Z0 ! S 0

E Z6
i 4 Z6
Z6 ! 1 S

+ Z2 Z6
* Z4 Z6

Z2 ! E Z7

i 4 Z7

Z7 ! 2 E1

* Z4 Z7

Z4 ! E Z8

i 4 Z8

Z8 ! 3 E

S " E1 "

+

E "

+
*

Table 9.4: The 3NF of M0 after disambiguation

grammar has no conflicts!
So we have here a systematic process by which we can disambiguate grammars based on associativity and precedence information. This is a very convenient feature:

ffl It makes the specification of many grammars much simpler, in particular for people who

are not so familiar with the art of syntax specification.

ffl It is mandatory in applications, where programmers shall be able to define their own infixor mixfix operators (see Section 10).

9.2 Variations on the Grammar Transformation Process
The description of our transformation process in Section 3 was very concise. Now we want to
point out a few possibilities for optimizations and extensions. Moreover, we want to give an
a-posteriori motivation for certain design choices.

9.2.1 Optimizations
In order not to interrupt the presentation of our approach in Section 3, we have not mentioned
two straightforward means for improving the efficiency:

53

ffl Chain productions. Our construction generates as a byproduct an optimization that --

according to [39] -- may speed up the final LR-style parser considerably. As can be seen
e.g. in Table 3.3 we obtain at various places sequences of actions such as Z3 ! i 4 5 3 .
These sequences result from so-called chain productions.

In non-optimized LR parsing the above sequence ultimately leads to three individual state
transistions plus the corresponding reductions, whereas in the optimized version it is handled by one state transition and one (combined) reduction. This latter variant is generated
automatically in our approach.

ffl Optimization. In Construction 3.2.1 we allow an arbitrary order in which the nonterminals

are unfolded (and thus eliminated). However, by using a specific ordering we can improve
the process such that it will need fewer left factorings and thus produces a smaller result
grammar.

When determining the order, we should start with the "topmost" nonterminals. In this
context, a nonterminal A is "above" a nonterminal B if B occurs (as first symbol) in a
rule for A. That is,

A "is above" B if there is a rule A ! B : : :
This was the reason why we chose the nonterminal R before L in our earlier example.
"Lazy" left factoring is what makes it better: we collect all pertinent right-hand sides
before we treat them together in a single transformation.

In connection with left-recursive productions we can apply the classical algorithm for
finding the maximal strongly connected components of a directed graph.

9.2.2 Incremental Parser Generation
For many applications (see Section 10) it would be helpful, if we could build up our parsers
incrementally.1 Our 3NF construction provides this facility relatively easily. We distinguish two
cases:

1. Suppose we want to add a new nonterminal B and its productions. Since this nonterminal

would be unreachable, we have to extend at least one of the existing nonterminals, say A,
by the new production (A ! B i ).

This situation is easily handled by our construction, provided that no left recursion is
introduced.

ffl The 1NF construction for B is independent from the existing grammar (except for

determining, which new Zis are available).

ffl The 2NF construction proceeds through the nonterminals from N successively anyway. So we simply treat B as the last nonterminal to be unfolded.

The only modification is that we must remember the productions for the other nonterminals, because we have to unfold them in the right-hand sides of B, before we
unfold B in A.
1From the literature it is known that this can be done within the LR paradigm [7], but that it is simpler within
the LL paradigm [3].

54

2. The story becomes more intricate, if we admit more complex extensions of an existing

nonterminal A. Most of these extensions can be reduced to the situation (1) above, with
one exception: if the addition leads to (direct or indirect) left recursion, we have to be
careful. To see the underlying principle let us consider the simplest possible situation.
Suppose that we have the grammar of Table 9.5.

Grammar N0
S ! A 1
A ! x 2

Table 9.5: A trivial nonrecursive grammar

The 3NF of this grammar is given in Table 9.6.

Grammar N1
Z0 ! S 0

A 1 0
x 2 1 0

Table 9.6: The 3NF of N0

Now assume that we want to add the production

A ! A y 3
If we add this production to the original grammar N0 and then apply the 3NF construction,
we obtain the grammar in Table 9.7. If we now compare the grammars in Table 9.6 and

Grammar N2
Z0 ! S 0

A Z1
x 2 Z1

Z1 ! 1 0

y 3 Z1

Table 9.7: The 3NF of the modified grammar N0
Table 9.7, we find the following correspondences:

ffl Z1 is defined exactly as required by the left-recursion removal in Def. 2.5.6 - applied

to the 3NF grammar in Table 9.6!

ffl The only complication is that we have to replace the trailer 1 0 by Z1 also in all

those productions of Z0 that have been obtained by unfolding A. But this is obviously
doable with little effort.

These short considerations demonstrate that the 3NF construction indeed can be adapted to
incremental parser generation.

9.2.3 Why Simple Left-Recursion Elimination Fails
In Section 2.5 we have opted against the traditional form of left-recursion elimination that is
mostly used in the literature and chosen a slightly more complex version instead (see Def. 2.5.6).
Now we want to demonstrate that this choice is indeed mandatory.

55

1. The simple form does not guarantee the LR(k) property.
2. The simple form only works for the classical but not for the emulated LR parsing algorithm.

Aspect (1) can be seen from the following counterexample:

S ! A b 1 S A ! A b 2 A

a 3 A

According to Def. 4.0.4 this grammar is LR(1), since S = f " g and A = f b g. If we transform
the grammar using the simple recursion removal, we obtain

Z0 ! S 0 S

A Z1 0 S
a 3 Z3 Z1 0 S

Z1 ! b 1 S
Z2 ! b 2 A

Z3 ! b 2 Z3 A

" A

Hence, Z3 exhibits a conflict. By contrast, the complex variant of recursion removal generates
the grammar

Z0 ! S 0 S

a 3 Z3 0 S

Z1 ! b 1 S
Z2 ! b 2 A
Z3 ! b Z4 S

Z4 ! 1 S

2 Z3 S

Z4 avoids the conflict. The explanation is simple: The symbol b follows A in the recursive case
and in some application position. Hence, the simple variant cannot decide, when the recursion is
finished (i.e., when the "-production has to be used). The complex variant resolves this problem
by way of left factoring in Z3, which leads to the conflict-free Z4.

Aspect (2) can also be seen from a little counterexample. In the classical-LR variant we could
actually do without the left-recursion removal. However, by keeping it we retain the option to
switch between the different parsing methods at will.

Example: Consider a maximally simplified variant of the classical grammar for arithmetic expressions. This grammar is shown in Table 9.8.

Grammar E0
S ! E 1
E ! E + i 2

i 3

Table 9.8: A left-recursive grammar

(a) The 3NF is shown in Table 9.9. (The intermediate Z1 is no longer reachable and has therefore
been eliminated.)

Grammar E2
Z0 ! S 0

E Z3 0
i 3 Z3 0

Z2 ! i 2
Z3 ! 1 S

+ Z2 Z3

Table 9.9: Second normal form (using left-recursion removal)

56

If we apply any of the two parsing methods to the sample string "i + i "" we obtain the following
derivation (where the classical LR parsing ignores the upper elements):

\Theta Z

0\Lambda  i 24

0
Z3

3 35 3 ^

0
Z3* + 24

0
Z3
Z235 i 24

0
Z3

2 35 2 ^

0
Z3* ^

0
1 * 1 \Theta  0 \Lambda  0 "

(b) On the other hand, if we apply a variant of the 2NF construction without left-recursion
removal, we obtain the grammar in Table 9.10. Note that Z3 now comes from the left factoring
of

Z0 ! S 0

E 1 0
E Z1 1 0
i 3 1 0

Grammar E2a
Z0 ! S 0

E Z3
i 3 1 0

Z2 ! i 2
Z3 ! 1 0 S

+ Z2 1 0

Table 9.10: Second normal form (without left-recursion removal)
This variant leads essentially to the same classical LR parsing process as the grammar in Table 9.9. However, if we use this grammar with emulated LR parsing, it would not work, since
the process would get stuck immediately:\Theta 

Z0\Lambda  i 24

0
1
3 35 3 ^

0
1 * 1 \Theta  0 \Lambda  0 + i "

This demonstrates that the variant with recursion removal is superior in the sense that it enables
all parsing methods, whereas the variant without recursion removal only works for one of the
variants.

9.3 Variations on the Parsing Process
Not only the presentation of the grammars and the transformation process of the grammars -
that is, the parser generation - allow variations. Also the parsing algorithms can be implemented
in different ways and be adapted to changing requirements. Most of these variations have to do
in one way or another with "ambiguity".

1. When a grammar is LR(k) with k * 2 we may consider using backtracking instead of

k-lookahead.

2. For truly ambiguous grammars we need backtracking.
3. Errors are in some sense the worst form of ambiguity, since we do not even know, where

in the string we should restart with what Zi.

All these problems can essentially be handled with the same techniques. We demonstrate these
for the case (1) and only sketch the pertinent adaptions for the other two cases.

57

9.3.1 k-Lookahead or Backtrack?
It is well-known that LR(k) parsers are efficient for the case k = 1 but may become expensive
for k ? 1. To see how this affects our approach we consider an example that is derived from the
(slightly modified) syntax of import declarations in Opal [10]:

import A only a1 a2 a3

B only b1 b2

When we encounter the identifier B, it is not yet clear, whether this is the next identifier on the
only-list of A or the name of the new import. The essence of this situation is captured by the
grammar of Table 9.11.

Grammar I0
S ! L 1
L ! I L 2

I 3
I ! i o N 4
N ! i N 5

i 6

Table 9.11: A Non-LR(1) Grammar

The 3NF is presented in Table 9.12. (Note that Z1 and Z4 are not reachable and therfore have
been eliminated.)

Grammar I3
Z0 ! S 0

L 1 0
I Z5 1 0
i Z2 Z5 1 0

Z2 ! o Z3
Z3 ! N 4

i Z6 4

Z5 ! L 2

I Z5 2
i Z2 Z5 2

3 L
Z6 ! N 5

i Z6 5

6 N
S "
L S

I Z5 S

Z5 L

N I

Table 9.12: Third normal form
As can be seen here, the continuation information suffices for Z5 to disambiguate all productions,
but in the rules for Z6 the shift/reduce conflict still cannot be resolved, since i is also a leading
symbol of the continuation N .

However, when we look further ahead then we find that in the production Z6 ! iZ6 5 the next
terminal symbol is again `i' or `" ', whereas in the second production Z6 ! 6 N the next
terminal symbol is `o', since this is the first symbol generated by Z2. Hence, with two symbols
lookahead the conflict is resolved: We have an LR(2) parser. This shows the following property.

Proposition. Our method automatically generates LR(k) parsers (for arbitrary k) without any
additional effort! \Lambda 

All we have to do is to analyze the continuation grammar further ahead. This is easy, since
its description already exists in the 3NF. But above all, this additional effort only has to be

58

invested, if a 1-conflict is actually encountered! This property contrasts our approach nicely
from the traditional way of proceeding.

In spite of this simplicity it is, however, still questionable, whether the effort for this extra
analysis is worth the cost. Firstly, the problem occurs very rarely. Secondly, our parser is a full
backtrack parser. Hence, in those cases where the two-symbol lookahead is needed it will simply
employ some backtracking in order to perform a successful parse. This one-step backtrack has
negligeable cost.

We illustrate this effect by a parse of the simple term `i o i i o i i "'. We look at the parsing process
as defined in Def. 5.2.1 with an obvious generalization for ambiguous parses: when an ambiguous
production is encountered, all possibilities are recorded. Whenever there is a potential reduce
action, we list it as a candidate (with ", that is, "nothing" as the other alternative).

\Theta Z

0\Lambda  i

2664

0

1
Z5
Z2

3775

o

2664

0

1
Z5
Z3

3775

i

26666
4

0
1
Z5

4

Z6

37777
5 \Phi 

" 6 \Psi 

26666
4

0
1
Z5

4

Z6

0
1
Z5

4

37777
5 \Phi 

" 4 \Psi 

26666
4

0
1
Z5

4

Z6

0
1
Z5

37777
5

i

26666
664

0
1
Z5

4
5
Z6

0
1
2
Z5
Z2

37777
775

o : : :
At this point it is evident that the first alternative is not valid, because `o' is an impossible
symbol for Z6. Hence, we can eliminate all first choices before we proceed:

i o i 6 4 i o

26666
4

0
1
2
Z5
Z3

37777
5

i

26666
664

0
1
2
Z5

4
Z6

37777
775 \Phi 

" 6 \Psi 

26666
664

0
1
2
Z5

4
Z6

0
1
2
Z5

4

37777
775 \Phi 

" 4 \Psi 

26666
664

0
1
2
Z5

4
Z6

0
1
2
Z5

37777
775

i

26666
66664

0
1
2
Z5

4
5
Z6

0
1
2
2
Z5
Z2

37777
77775

"
This time the second variant is excluded, because the symbol `"' is impossible for Z2. Hence we
can eliminate all second choices before we proceed.

i o i 6 4 i o i i

26666
66664

0
1
2
Z5

4
5
Z6

37777
77775

6

26666
664

0
1
2
Z5

4
5

37777
775

5

26666
4

0
1
2
Z5

4

37777
5

4

2664

0

1
2
Z5

3775

3 24

0
1
2 35 2 ^

0
1 * 1 \Theta  0 \Lambda  "

The realization is slightly more complex than indicated by this example: it can happen that one
of the branches is split further, in which case we cannot simply "flatten" the choice from pairs to
triples but rather have to use nested choices in order to allow a correct backwards elimination.

We note in passing that the internal organization of lists in a functional language like Opal
effects the desired sharing of sublists automatically. So the implementation becomes extremely
simple in such languages.

9.3.2 Multi-Pass Parsing Versus Generalized LR Parsing
The above realization of the parsing process has a major disadvantage: the simultaneous performance of the multiple parses will frequently encounter situations, where several threads lead to

59

the same Zi. This happens e.g. for if-then-else constructs and the like, where all possible parses
perform the same local subparse. There are several means to avoid this waste of work.

ffl In order not to duplicate the work one could realize the various threads as a more complex

data structure, viz. a dag. This was e.g. worked out by M. Tomita under the concept of
Genralized LR parsing in [36] (see also [32], where further references are given). Bates and
Lavie [4] use a similar technique, termed forest-structured stacks.

ffl In practice the Genralized LR parsing with its complex data handling might not be the

most efficient way of proceeding. Using a multi-pass parsing scheme may actually be more
efficient. This technique will be sketched in the following.

The essential point in multi-pass parsing is that we actually perform the reductions - at least
those that are unambiguously possible. Whenever we encounter an ambiguity we proceed - as
demonstrated in Section 9.3.1 above - with the set of possible states Zi. From then on many
steps will create new sets of states, but very frequently we will encounter locally unambiguous
substrings, where the sets are reduced to simple nonterminals.

This way we can perform all unique subparses once and for all. As a result we obtain a maximally
shortended sentential form, which focusses only on the actual ambiguities. Since this sentential
form will in general be very short, we can easily accept the overhead of backtrack parsing for it.

This idea can actually be improved further: as has been shown in Section 9.3.1 above we will
find out after a while, that certain members of the set of possible states are inconsistent with
the next input symbol(s). This allows a backward elimination of all their predecessors. A more
efficient variant is, however, a second pass after the first one has been finished. In this backward
pass all inconsistencies are removed in one sweep.

As has been illustrated by the example of the previous section this second pass will solve all
problems in the case of LR(k) grammars also for k * 2. So it will only happen for some truly
ambiguous grammars that real backtracking is needed.

We note in passing that the same principles are also applicable to the design of parallel parsers
(see [25]).

Ambiguous grammars. These principles also apply to truly ambiguous grammars. The
important aspect from the point of view of efficiency is that the splitting into potential branches
is managed in such a way that common subparses still are done only once.

We illustrate this by an example that is derived from the (slightly modified) syntax of letdeclarations in languages such as Opal, Haskell or Ml.

let b = f a

h x y = g (f x) y
z = h a b
in : : :

Without additional conventions such as Haskell's "offside rule" there is no way of parsing this
program fragment uniquely. For example, it is by no means clear, whether a, h and x are still
arguments of f or the leading function symbol of the next declaration.

The essence of this situation is captured in the grammar of Table 9.13.
When we transform this grammar into its 3NF, then we obtain one Zi that exhibits a conflict:

60

Grammar L0
S ! L 1
L ! D L 2

D 3
D ! E = E 4
E ! T E 5

T 6
T ! ( E ) 7

i 8

Table 9.13: An ambiguous grammar

Z8 ! E 5

T Z8 5
( Z5 Z8 5
i 8 Z8 5

6 E

Since E

\Lambda  iZ

8 : : : , there is a shift/reduce conflict. Moreover, we can immediately see that

this conflict cannot be resolved by any k-lookahead. In other words, the grammar is not LR(k)

for any k.

Even though the backtracking is unavoidable here, it has at least been reduced to an absolute
minimum: all other Zi are uniquely determined! So the multi-pass paradigm will shorten the
input string to a very short sentential form, before it enters the true backtracking process.

This is the clue for the feasability of a simple method for type-dependent overload resolution:
for the ambiguous constructs one may generate all possible parses, apply the typing algorithm
to all resulting trees and throw away those that are not type-correct.

9.3.3 Error Handling
In the literature there are many treatments of the integration of error handling into parsing (see
e.g. [33]). In connection with the paradigm of functional parsing the subject has been extensively
treated in [22]. The special form of our 3NF allows a very systematic and straightforward error
handling.

ffl As has already been pointed out in Section 9.3.1 the error handling can be performed

along the same lines as the treatment of non-LR(1) grammars.

As a matter of fact, we might even consider generating a second 3NF for the inverse
direction, that is, for parsing from right to left. Then the second pass would encapsulate
the error positions with high accuracy.

ffl But we also have other possibilities. For example, [1] points out that during error recovery

we need "stop sets", that is, collections of terminals, the occurrence of which indicates
that we have to pop up from a deeper error level. A typical instance is the occurrence of
an unexpected `else' during the parsing of a then-clause. This might indicate that we are
e.g. lacking a closing paranthesis. This error is recognized, if the symbol `else' is in the
stop set (where it has been put upon the parsing of the symbol `then').

In our parsing process as described by Def. 5.2.1 in Section 5.2 this stop set is directly
obtainable from the intermediate (composed) functions ['].

61

9.3.4 Integrating Context-Sensitive Aspects
In the functional-programming community it is well-known that the flexibility provided by the
functional parsing as defined in Section 5 also allows - at least to some extent - the integration
of context-dependent grammars. We cannot go into details here but merely want to point out
the pertinent principle. The parsers as defined in Section 5 convert a production such as

A ! B C i
into a function definition such as

def A == B ; C ; actioni

If we want to integrate context-dependent information (such as typing information, potential
offside positions etc.), we may extend the above function by an appropriate parameter. This
leads to definitions of the kind

def A(t) == B(h1(t)) ; C(h2(t)) ; actioni(h3(t))

The auxiliary functions hi propagate the context-dependent information to the various parts of
the parser.

It is a simple exercise to extend the notations for grammars such that "nonterminals" of the
form A(t), B(h(t)) etc. are allowed. For these extended forms our constructions go through
analogously. Even though this is far from providing the full power of attribute grammars, it is
a simple tool that solves a number of practical problems.

62

Chapter 10
Sketch of a Non-standard
Application:
Type-Dependent Overload
Resolution and Mixfix Operators

As a nonstandard and nontrivial application of our approach we consider the use of parsing for
type analysis combined with user-defined mixfix operations. We want to allow the programmer
to write down declarations of the form

def + : Int \Theta  Int ! Int [[ left-associative ]] [[ + OE \Lambda  ]]
def + : Matrix \Theta  Matrix ! Matrix [[ left-associative ]] [[ + OE \Lambda  ]]
def with at : Array[ff] \Theta  ff \Theta  Nat ! Array[ff]
def ffi : (fi ! fl) \Theta  (ff ! fi) ! (ff ! fl)

That is, we need mixfix operators, higher-order functions, overloading and polymorphism. Moreover, there have to be modularisation concepts available such as classes, modules or structures.1

The fundamental principle underlying this concept is given by an observation that has already
been made by the ADJ group in 1977 [35]: every grammar induces the signature of an algebraic
structure (which makes the notion of abstract syntax more precise). This observation also works
the other way round: every signature of a structure induces a grammar.

The correspondence is simple: the types of the signature correspond to the nonterminals of the
grammar. And the operations (in the above mixfix examples: the fragments of the operations)
correspond to the terminal symbols.

From the above wish list we can deduce a number of requirements:

ffl The analyzer cannot be constrained to LR(1) grammars, since the programmers of such

modules will not be willing to obey such constraints and often not even be capable of
recognizing them. Hence, we need the full power of backtrack parsing. This is provided by
our parser as described in Section 5.

ffl The modularization entails that a module A can import a module B. At this moment

the grammar for B is extended by the grammar for A. Hence, we need incremental

1With the exception of the arbitrary mixfix notations the parser of our language Opal [24, 26] can cope with
these features.

63

parser generation. Section 9.2.2 has shown that this requirement can be easily met in our
framework.

ffl In some situations it is not known a priori, which role a certain identifier plays. This

happens, for example, in pattern-based definitions that are characterisitic for functional
programming. Here it may not be immediately clear, whether an identifier is a parameter
or a fragment of a mixfix operator. So we obtain something like "fuzzy" terminal symbols.
The techniques presented in Sections 9.3.1 and 9.3.2 can be adapted easily to this slight
complication.

ffl When we translate the above ADJ-correspondences to the situation of polymorphic functions, we run into a concept of "generic nonterminals". This is best dealt with by using
context-dependent parsing (as sketched in Section 9.3.4).

In practice such a system will be based on a "coarse" grammar that takes care of the basic
structure of a program: if-then-else, parantheses and the like. As a result we obtain a coarse
syntax tree that contains many sequences of yet unparsed fragments. For example, a program
fragment such as

if a + 1 ? b then sin a + b \Lambda  c else cos a \Lambda  (b + c) fi

may lead to a tree of the kind

cond

a + 1 ? b sin a + b * c cos a * E

b + c
Then we have to parse the subexpressions in this tree using the grammar that is deduced from
the function declarations in the program modules.

Fortunately, the general need for backtracking does not hurt too much, since it is only applied
to very short terms. This is the advantage of first applying the coarse structure parse.

However, it can also be seen that this approach requires an intricate interaction between the
parser and the Hindley-Milner type algorithm.

Even though the various aspects of our approach - as discussed in the previous sections - can be
used to enable this application, the concrete realiziation still requires some research effort.

64

Chapter 11
Conclusion
We have presented a technique for the simple and flexible generation of (LL and) LR parsers.
The idea is to transform the grammar into a particularly suitable form and then use the means
of recursive-descent parsing.

The recursive-descent parsing from Section 5.1 is an extremely simple mechanism, which is
attractive, short and elegant. But we have to keep a few problems in mind:

ffl To start with the biggest drawback, LL parsing does not work for grammars with leftrecursive productions.

However, our construction eliminates left recursion!

ffl The parsing process may be slow, because it may perform some backtacking before finding

the right production for a given nonterminal.

However, our 3NF construction eliminates all possible shift/shift conflicts. And it also
performs the lookahead for reductions. Therefore, true backtracking may only occur,
when the grammar is not LR(k) - and then we actually want the backtrack power!

ffl We have not provided a decent treatment of errors.

But this can be easily added (as described in [22]). Moreover, our brief discussion in
Section 9.3.3 indicates that the error handling is even fostered by our approach.

Simple as it may be, the technique has a number of important advantages:

ffl We have a full backtrack parser that works for arbitrary context-free grammars, not just

for LL(k) or LR(k) grammars.

ffl Our parser yields the first successful parse, but it can easily be varied such that it yields

the set of all parses instead.

ffl One can easily change the viewpoint of a grammar from a parsing function that is directly

executed to a data structure that is interpreted. Then one can switch back and forth
between transformations of the grammar and parsing with the grammar. This way, we can
apply "lazy" grammar transformations triggered by the parsing process itself (as sketched
in Section 6).

ffl The higher-order functions `;', `j', etc., are defined once and for all and thereafter can

be fetched from a library. Therefore, all one has to do in order to obtain a parser is
transliterate the given grammar.

65

ffl Since the parsing operators are part of the library, we can exploit the full power of the

programming language, e.g. for writing the semantic actions. This is much more flexible
than working with external tools such as parser generators. For instance, we can easily
add attribute computations to the parser (see Section 10).

ffl Due to its simplicity, our parser can be easily verified (or even be considered "obviously

correct".) The verification is in fact outlined in the Appendix.

ffl Finally, the special form of our 3NF allows us to simplify the implementation of our parser

even further (see Appendix).

Therefore we feel that the combination of our grammar transformations with this highly elegant,
simple and flexible implementation scheme provides an extremely powerful parsing concept.

Acknowledgement. The ideas for this approach evolved over many years, also stimulated by teaching principles of compiler construction to students. During that time several people have provided critical
assessments of the proceeding, notably Wolfram Schulte, Andreas Fett and, above all, Carola Gerke. I am
especially grateful to Doug Smith, Cordell Green and other members of Kestrel Institute; the stimulating research environment at this place and the intensive discussions about the formalization of software
development lead to the final shaping of this paper. A number of valuable comments were provided by
the members of IFIP WG 2.1. Niamh Warde helped to formulate parts of the paper.

66

Appendix A
Appendix: More About (Functional)
Parsing

Since this is not a paper on functional parsing, we have sketched its basic principles only very
briefly in Section 5. For the sake of completeness we want to fill in the missing details in this
Appendix.

Essentially a parser should take an input string and yield a corresponding tree (or indicate
failure). However, this is only true for the overall parsing of the full input; in all intermediate
parsing stages we only turn some initial fragment of the input into a tree. The unread remainder
of the input therefore has to be returned as a result as well. Finally, programming is slightly
facilitated if we add -- for reasons of symmetry -- the type Tree also as an argument. Hence,
a parser takes as input a partial tree and a string and yields as output a properly extended tree
plus the unread remainder of the string:

type Parser == Tree \Theta  String ! Tree \Theta  String

Since this is a function type, all operations working on the type Parser are higher-order functions. Next we look at the definitions of the operators ";", "j", etc. We can explain them on the
basis of a more elementary operation `A fby (S; F)', which essentially connects the first parser A
with two possible continuation parsers: S for the success and F for the failure of A.

fun ; : Parser \Theta  Parser ! Parser
def A; B == A fby (B; fail)

-- sequential composition

fun j : Parser \Theta  Parser ! Parser
def AjB == A fby (id; B)

-- alternative parsers (not commutative)

Using these connectors we can also introduce the Kleene star and its "at-least-once" variant:

fun \Lambda  : Parser ! Parser
def A \Lambda  == A fby (A\Lambda ; id)

-- Kleene star

fun + : Parser ! Parser
def A + == A ; A\Lambda 

-- "at least once"

So all we need to understand are three operations, viz. `fby', `fail' and the lifting operator `!'.
On this basis we can now program the basic operations.

67

fun fby : Parser \Theta  Parser \Theta  Parser ! Parser
def A fby (SuccCont; FailCont)(OldTree; Input) ==

let (NewTree; Remainder) == A(OldTree; Input)
in
if okay?(NewTree) then SuccCont(NewTree; Remainder)
if fail?(NewTree) then FailCont(OldTree; Input)
fi

-- continuations
-- backtrack!
The parser `fail' merely returns a failure indication (note the overloading between the parser
called fail and the tree called fail). Note: we need to return some string for reasons of type
conformity; for this purpose, the empty string is as good as any other.

fun fail : Parser
def fail(OldTree; Input) == (fail; \Sigma  )

Terminal symbols are "lifted" to parsers which simply look at the next input token x to check
whether it is the expected one; if not, a failure indication is returned. (The operation `::' on
trees will be discussed in a moment.)

fun ! : Token ! Parser
def (t!)(OldTree; \Sigma  ) == (fail; \Sigma  )
def (t!)(OldTree; x::Remainder) ==

if t = x then (OldTree::x; Remainder)
if t 6= x then (fail; \Sigma  )
fi

-- lift tokens
-- unexpected end of input

-- input symbol shifted to tree
-- unexpected input symbol

We do not want to go into the details of the representation or the construction of trees. If we
adopt the view from Section 2.3, we can actually realize the operation `t::x' by simply appending
the token x to the tree (i.e. string) t. The operations actioni also do nothing but append their
number to the tree, that is, actioni(Tree; Input) = (Tree:: i ; Input).1

For the special form of the 3NF we could actually simplify the operator `A j B': we write a case
distinction for the leading terminal symbols and thus avoid the one-step backtrackings with the
subseqeunt fail tests.

We conclude this brief sketch by noting that the concrete programming that we have chosen is
not obligatory; the details can easily be varied to suit different tastes and styles. (For example,
Hutton and Meijer [14] design their operators such that they fulfil the monad laws that recently
have become so popular in functional programming).

Correctness. Even though the above functional parser is so straightforward that its correctness appears to be evident, we present a verifying derivation for the sake of completeness.

We start from an initial specification that directly represents the correspondence between functional parsers and derivations: A parser P transforms a string s into a tree t. To ease the
presentation we consider trees in their postorder form, i.e. as strings for the number-augmented
grammar G0 (as described in Section 2.3). Moreover, for this initial specification we actually employ a type Parser as we would like to have it: Parser = String ! Tree. With the help of the
filter function ' that eliminates the numbers i we can then formulate the initial specification:

axm P(s) = t =) t 2 LP(G0) ^ '(t) = s

From this definition we can e.g. deduce the following properties:

1Of course, in paractical implementations one will realize the actions i by actually building the trees right
away.

68

(A; B)(s) == t
) t 2 LAB(G0) ^ '(t) = s
) 9 t1 2 LA(G0); t2 2 LB(G0) :

t = t1 t2 ^ '(t) = '(t1 t2) = '(t1) '(t2) = s
) 9 t1 2 LA(G0); t2 2 LB(G0); s1; s2 :

t = t1 t2 ^ = s1 = '(t1) ^ s2 = '(t2) ^ s = s1 s2
) 9 s1; s2 :

s = s1 s2 ^ t = A(s1) B(s2)

Summarizing: When there is a parse AB

\Lambda ) s then this parse fulfils the property

(A; B)(s) = t ) 9 s1; s2 : s = s1 s2 ^

(A; B)(s) = A(s1) B(s2)

To ease readability we consider from now on only the well-defined case and therefore omit the
premise (A; B)(s) = t ) : : : in all further specifications.

This specification evidently suffers from the existential quantifier, which makes it non-constructive.
Existential quantifiers can be converted into Skolem functions, which leads to the new specification

(A; B)(s) = A(s1) B(s2) ^ s = s1 s2 where s2 = hA(s)

Now we embed every parser P into a new parser bP that also returns the value of the corresponding
function hP . Moreover, bP does not yield a simple tree, but it rather extends a given tree:

def bP(t; s) == (t P(s1); hP (s)) where s = s1 hP (s)

Note that this specification is constructive if hP is.
This clearly is an embedding of P, because we have P(s) = ss1(bP( \Sigma  ; s)) requiring ss2(bP( \Sigma  ; s)) = \Sigma  .
That is, the first component is the result of the full parse (provided that the second result hP (s)
is empty).

Now take P = A; B. Then we have the derivation

69

def bP(t; s) == [(A; B)(t; s)

== (t P(s1); hP (s)) where s = s1 hP (s)
== (t (A; B)(s1); hP (s)) where s = s1 hP (s)
== (t A(s11) B(s12); hP (s)) where s = s1 hP (s)

s1 = s11 s12
s12 = hA(s1)
== (t1 B(s12); hP (s)) where s = s1 hP (s)

(t1; s0) = bA(t; s)
s0 = hA(s)
s = s11 s0
s1 = s11 s12
s12 = hA(s1)
== (t1 B(s12); hP (s)) where s = s11 s12 hP (s)

(t1; s0) = bA(t; s)
s0 = hA(s)
s = s11 s0
== (t1 B(s12); hP (s)) where (t1; s0) = bA(t; s)

s0 = s12 hP (s)
== (t2; hP (s)) where (t1; s0) = bA(t; s)

s0 = s12 hP (s)
(t2; s00) = bB(t1; s0)
s00 = hB(s0)
s0 = s12 s00
== (t2; hP (s)) where (t1; s0) = bA(t; s)

(t2; s00) = bB(t1; s0)
s00 = hP (s)
== (t2; s00) where (t1; s0) = bA(t; s)

(t2; s00) = bB(t1; s0)

This way we have formally derived the definition that was given above - via the auxiliary function
fby - for the operator `;'. The other operators can be derived analogously.

Improved implementation. In Section 5.3 we have employed a backward search in order
to find out, which Zi or i has to be taken as the next bottom element of the function ['].
This is, of course, unnecessarily inefficient and should be implemented in a more elaborate way.
The clue to the improvement is - as in many cases - the introduction of a data structure for
"remembering" the necessary information.

Consider the reductions (\Lambda 1) and (\Lambda 2) in Section 5.3. Their effect is actually twofold: they reduce
a substring (the handle ff) to a symbol. And they also eliminate all intermediate functions inside
ff. Even if we do not apply the reduction explicitly (but rather only add the action symbol i
to the output), we still have to perform the elimination of the intermediate functions.

Therefore the idea suggests itself to keep these functions (actually only the bottom ones) in a
separate list. The resulting process is illustrated by the following example; the corresponding

70

rules will be given in a moment.\Theta 

Z0\Lambda  \Lambda  i = i "
 \Lambda  \Theta Z0=Z3\Lambda  i = i "
 \Lambda  i \Theta Z0=Z3= 4 \Lambda  = i "
 \Lambda  i 4 \Theta Z0=Z3= 5 \Lambda  = i "
 \Lambda  i 4 5 \Theta Z0=Z3= 3 \Lambda  = i "
 \Lambda  i 4 5 3 \Theta Z0=Z4\Lambda  = i "
 \Lambda  i 4 5 3 = \Theta Z0=Z4=Z2\Lambda  i "
 \Lambda  i 4 5 3 = i \Theta Z0=Z4=Z2= 4 \Lambda  "
 \Lambda  i 4 5 3 = i 4 \Theta Z0=Z4=Z2= 5 \Lambda  "
 \Lambda  i 4 5 3 = i 4 5 \Theta Z0=Z4=Z2= 1 \Lambda  "
 \Lambda  i 4 5 3 = i 4 5 1 \Theta Z0\Lambda  "
 \Lambda  i 4 5 3 = i 4 5 1 \Theta Z0= 0 \Lambda  "
 \Lambda  i 4 5 3 = i 4 5 1 0

The specification of these steps is quite simple; it consists of three main rules:

1. We can perform the step (with terminal symbol t and reststring u)

: : : \Theta : : : =Zi\Lambda  t u  : : : t \Theta : : : =Zi=Zj\Lambda  u
or : : : \Theta : : : =Zi\Lambda  t u  : : : t \Theta : : : =Zi= i \Lambda  u

if there is a production (Zi ! tZj : : : ) or (Zi ! t i : : : ) in the 3NF grammar G0.
2. We can perform the step (with reststring u)

: : : \Theta Zi1= : : : =Zin= : : : =Zin+k

\Gamma 1 = j \Lambda  u  : : : j \Theta Zi1 = : : : =Zin=Zj\Lambda  u

or : : : \Theta Zi1 = : : :=Zin = : : : =Zin+k

\Gamma 1 = j \Lambda  u  : : : j \Theta Zi1= : : : =Zin = i \Lambda  u

if there is a production (A ! ff j ) in the original grammar. The number k of elements to
be popped off the list is the length of the handle ff. The corresponding nonterminal A
then determines the newly added Zj or i : it comes from the production (Zin ! AZj : : : )
or (Zin ! A i : : : ), respectively, in the 3NF grammar G0.

3. We can perform the step (with reststring u)

: : : \Theta Zi1 = : : : =Zin= : : : =Zin+k

\Gamma 1\Lambda  u  : : : j \Theta Zi1 = : : : =Zin=Zj\Lambda  u

or : : : \Theta Zi1= : : : =Zin = : : : =Zin+k

\Gamma 1\Lambda  u  : : : j \Theta Zi1= : : : =Zin = i \Lambda  u

if there is an applicable production (Zin+k

\Gamma 1 ! j : : : ) in the 3NF grammar G

0. The rest

is analogous to case (2).

Evidently these rules just mimic the effect of the reduction of the handle ff to the symbol A
without actually performing the reduction itself. So they are obviously correct.

Discussion. The above transformation process has major influences on our programming techniques. The main aspects of the resulting changes are briefly discussed in the following.

ffl List of functions. If we want to adhere to the functional programming style, we can no

longer use function composition (because "popping" functions off compositions is impossible). We therefore need a list of functions \Theta Zi1 = : : : =Zin\Lambda  , from which the appropriate
number of functions can be popped by the actions i . But otherwise the programming
remains the same.

71

ffl Interpretative parsing. Since we work with a list \Theta Zi1 = : : : =Zin\Lambda  anyhow, we may also

consider changing the Zi from functions to simple values (such as numbers). Then they
play exactly the role of "states" in traditional LR-style parsers.

The programming is not much different from the above functional version, though: instead
of direct applications of the functions Zi we now have an operation apply(Zi), that is, we
work in the style of an interpreter.

ffl Efficiency. An interesting issue is the efficiency of the different variants.

- In our emulated-LR concept we carry around a (composed) function that needs to be

applied to the rest string. If one considers the overhead of higher-order functions as
being too costly, one can easily implement this composition as a stack of functions
(or as a stack of numbers to be "interpreted"). A shift operation adds one or more
elements Zj to this stack, whereas a reduce operation i takes one element off this
stack.

- In our true-LR version we also carry around a stack, albeit a different one.2 A shift

operation adds one element Zj to this stack, whereas a reduce operation i takes one
or more elements off this stack (and adds another one).

- The traditional LR-style technique actually amalgamates this stack management with

the explicit generation of the tree (fragments).

From this conceptual comparison it should be clear that the various implementation techniques should actually not cause very different costs.

2If we consider the stacks of the original version over time, we obtain a stack of stacks. And the bottom
elements of these stacks form the stack in the second approach.

72

Bibliography

[1] Alfred A. Aho, Ravi Sethi, and Jeffrey D. Ullmann. Compilers: Principles, Techniques,

and Tools. Addison-Wesley, 1985.

[2] Roland C. Backhouse. Syntax of Programming Languages. Prentice-Hall, 1979.
[3] R. Bahlke and G. Snelting. The PSG system: From formal language definitions to interactive programming environments. ACM Transactions on Programming Languages, 8(4):547-
576, 1986.

[4] J. Bates and A. Lavie. Recognizing substrings of lr(k) languages in linear time. ACM

Transactions on Programming Languages, 16(3):1051-1077, 1994.

[5] J. C. Beatty. On the relationship between the LL(1) and LR(1) grammars. Journal of the

ACM, 29(4):1007-1022, 1982.

[6] M. E. Bermudez and K. M. Schimpf. Practical arbitrary lokkahead lr parsing. Journal of

Computer and System Sciences, 41:230-250, 1990.

[7] P. Degano, S. Mannucci, and B. Mojana. Efficient incremental LR parsing for syntaxdirected editors. ACM Transactions on Programming Languages, 10(3):345-373, 1988.

[8] F. L. DeRemer and Th. Pennello. Efficient computation of LALR(1) look-ahead sets. ACM

Transactions on Programming Languages, 4(4):615-649, 1982.

[9] F.L. DeRemer. Simple LR(k) grammars. Comm. ACM, 14:453-460, 1971.
[10] K. Didrich, A. Fett, C. Gerke, W. Grieskamp, and P. Pepper. Design and implementation of

an algebraic programming language. In J. Gutknecht, editor, Programming Languages and
System Architectures., Lecture Notes in Computer Science 782, pages 228-244. Springer
Verlag, 1994.

[11] K. Didrich, W. Grieskamp, J. Exner, Ch. Maeder, M. S"udholt, C. Gerke, and Pepper P.

Towards a redesign of Opal. Technical Report 96 - 3, Fachbereich Informatik, Technische
Universit"at Berlin, February 1997.

[12] S. Hill. Combinators for parsing expressions. J. Functional Programming, 6(3):445-464,

May 1996.

[13] G. Hutton. Higher-order functions for parsing. J. Functional Programming, 2(3):323-343,

July 1992.

[14] G. Hutton and E. Meijer. Monadic parser combinators. J. Functional Programming, 1(1),

May 1993.

73

[15] K. Jensen and N. Wirth. Pacal User Manual and Report. Springer, 1978.
[16] T. Kasami. An efficient recognition and syntax analysis algorithm for context-free languages. Technical Report AFCRL-65-758, Air Force Cambridge Research Laboratory, 1965.

[17] B. W. Kernighan and D. M. Ritchie. The C Programming Language. Prentice-Hall, 1978.
[18] D. E. Knuth. On the translation of languages from left to right. Information and Control,

8:607-639, 1965.

[19] R. N. Moll, M. A. Arbib, and A.J. Kfoury. An Introduction to Formal Language Theory.

Springer Verlag, 1988.

[20] A. Nijholt. Deterministic Top-Down and Bottom-Up Parsing: Historical Notes and Bibliographies. Mathematical Centre, Amsterdam, 1983.

[21] R. Nozohoor-Farshi. GLR parsing for "-grammars. In M. Tomita, editor, Generalized LR

Parsing, pages 61-75. Kluwer Academic Publishers, 1991.

[22] A. Partridge and D. Wright. Predictive parser combinators need four values to report errors.

J. Functional Programming, 6(2):355-364, March 1996.

[23] P. Pepper. Grundlagen des "Ubersetzerbaus. Course notes, Fachbereich Informatik, Technische Universit"at Berlin, 1990.

[24] P. Pepper. The programming language Opal. Technical Report 91 - 10, Fachbereich

Informatik, Technische Universit"at Berlin, June 1991.

[25] P. Pepper. Deductive derivation of parallel programs. In R. Paige, J. Reif, and R. Wachter,

editors, Parallel Algorithm Derivation and Program Transformation, pages 1-54. Kluwer
Academic Publishers, 1993.

[26] P. Pepper. Funktionale Programmierung in Opal, Ml, Haskell und Gofer. Springer

Verlag, 1999.

[27] R. Plasmeijer and M. van Eckelen. Functional Programming and Parallel Graph Rewriting.

Addison-Wesley, 1990.

[28] Jan Rekers. Parser generation for Interactive Environments. PhD thesis, University of

Amsterdam, 1992.

[29] A. Salomaa. Formal Languages. Academic Press, 1973.
[30] L. Schmitz. Syntaxbasierte Programmierwerkzeuge. Teubner, 1995.
[31] Th. Sch"obel-Theuer. Ein Ansatz f"ur eine allgemeine Theorie kontextfreier Spracherkennung.

PhD thesis, Fakult"at Informatik der Universit"at Stuttgart, 1996.

[32] K. Sikkel. Parsing Schemata. Springer Verlag, 1997.
[33] S. Sippu and E. Soisalon-Soininen. A syntax-error-handling technique and its experimental

analysis. ACM Transactions on Programming Languages, 5(4):656-679, 1983.

[34] S. Sippu and E. Soisalon-Soininen. Parsing Theory, Vol.II: LR(k) and LL(k) Parsing.

EATCS Monographs on Theoretical Computer Science. Springer Verlag, 1990.

74

[35] J. W. Thatcher, E. G. Wagner, and J. B. Wright. Initial algebra semantics and continuous

algebras. Journal of the ACM, 24(1):68-95, 1977.

[36] M. Tomita. Efficient Parsing for Natural Languages. Kluwer Academic Publishers, 1985.
[37] T.A. Wagner and S.L. Graham. Incremental analysis of real programming languages. In

ACM SIGPLAN Conference on Programming Language Design and Implementation, pages
31-43, 1997.

[38] T.A. Wagner and S.L. Graham. Efficient and flexible incremental parsing. ACM Transactions on Programming Languages, 20:980-1013, 1998.

[39] W. W. Waite and G. Goos. Compiler Construction. Springer Verlag, 1984.
[40] D.H. Younger. Recognition of context-free languages in time n3. Information and Control,

10:189-208, 1967.

75