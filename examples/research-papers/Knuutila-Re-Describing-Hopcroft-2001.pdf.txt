

Theoretical Computer Science 250 (2001) 333-363

www.elsevier.com/locate/tcs

Re-describing an algorithm by Hopcroft

Timo Knuutila
Department of Computer Science, University of Turku, Lemmink"aisenkatu 14 A,

SF-20520 Turku, Finland

Received September 1997; revised March 1998

Communicated by M. Crochemore

Abstract

J. Hopcroft introduced already in 1970 an O(n log n)-time algorithm for minimizing a finite
deterministic automaton of n states. Although the existence of the algorithm is widely known,
its theoretical justification, correctness and running time analysis are not. We give here a tutorial
reconstruction of Hopcroft's algorithm focusing on a firm theoretical basis, clear correctness
proofs and a well-founded computational analysis. Our analysis reveals that if the size of the
input alphabet m is not fixed, then Hopcroft's original algorithm does not run in time O(mn log n)
as is commonly believed in the literature. The O(mn log n) holds, however, for the variation
presented later by D. Gries and for a new variant given in this article. We also propose a new
efficient routine for refining the equivalence classes constructed in the algorithm and suggest a
computationally sound heuristics as an enhancement. cfl 2001 Elsevier Science B.V. All rights
reserved.

Keywords: Finite automata; Algorithms; Minimization

1. Introduction

Minimization of a deterministic finite automaton (DFA) is a well-studied problem
of formal languages. A survey due to Watson [24] gives a broad introduction to most
known minimization algorithms. The asymptotically most efficient algorithm for this
problem was introduced in 1970 by Hopcroft [11, 12]. Hopcroft's algorithm is able to
perform its task in time O(|A| log |A|), where A is the state set of the DFA. The algorithm was presented under the implicit assumption that the size of the input alphabet
X of the DFA is a constant. Actually, the Algol 60 code found in [11] is specialized to the case |X | = 2. Blum [2] gives a new and detailed implementation of the
algorithm.

E-mail address: knuutila@cs.utu.fi (T. Knuutila).
0304-3975/01/$ - see front matter cfl 2001 Elsevier Science B.V. All rights reserved.
PII: S0304-3975(99)00150-4

334 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363

It is to be noted that there are some special cases of DFA for which more efficient
algorithms are known: acyclic DFA can be minimized in time O(|X ||A|) [19] and
single-letter alphabet DFA in time O(|A|) [17]. There exist also more general partition
refinement algorithms (of similar time complexity) from which the DFA minimization
can be drawn as a special case [5, 16].

Although Hopcroft's algorithm is a remarkable improvement over the classical Moore
algorithm and its later refinements, only the classical ones are usually given in the literature. The more advanced algorithm is typically either totally omitted or just mentioned
briefly with the standard references. For example, Watson's survey [24] skips the correctness proof and running time analysis because they are considered too complicated.
The only textbooks to the author's knowledge attempting to clarify the ideas of the
Hopcroft algorithm are the ones by Brauer [3] and Mikolajczak [15]. Both of these
are more or less just repeating Hopcroft's original article or its re-description due to
Gries [9].

Gries criticized the correctness proof and run-time analysis of Hopcroft [12], and
aimed to present Hopcroft's algorithm in an understandable way with the help of the
then (and even now) popular structured top-down approach. Though Gries unarguably
managed to describe the algorithm and the different analyses more formal and clear,
there is still room for some development - at least these clarifications have not lead to
a general adoption of the algorithm in textbooks. Even in the ones mentioned above the
exposition has remained unreadable. The best text in the author's opinion on Hopcroft's
minimization algorithm is in [1], but there it is given under the title of partitioning a
set with respect to a single function on this set (a task for which even a linear time
algorithm is nowadays known), and the more general case needed to minimize a DFA
is left as an exercise. It will turn out that this exercise is not simple.

We give in this paper yet another presentation that actually leads us to an algorithm very similar to Hopcroft's. Our goal is to perform a `tutorial reconstruction' with
clear and exact correctness proofs and computational analyses. Unlike earlier texts on
the subject using `guess and verify'-techniques in the time analysis, we are able to
present the complexity analysis in a constructive way. Our analysis reveals that the
later texts [9, 3, 15] concerning Hopcroft's original algorithm have made the quick and
unjustified conclusion that this algorithm has time complexity O(|X ||A| log |A|) for an
arbitrary alphabet X . This bound can, however, be achieved by applying a simple
enhancement.

The rest of the article is constructed as follows. Section 2 contains the basic definitions and notation needed in the sequel. We also review some of the most fundamental
properties of DFA with the emphasis on the ones concerning state equivalence and
minimization. After these preliminaries we develop, implement and analyze an efficient
minimization algorithm in Sections 3, 4 and 5, respectively. All this work leads to a
common algorithmic framework over existing variants of the Hopcroft algorithm, and
a common analysis tool, colored derivation tree, that will be of central importance
both in the correctness proofs and time analyses. Section 6 concludes the article with
discussion on practical aspects and possible extensions.

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 335
2. Preliminaries

We define formally the concepts of strings and DFA. The results presented here are
just stated without proofs, which can be found in most textbooks on the subject (e.g.
[20, 13]).

2.1. Sets, relations and mappings

We present here some elementary concepts that can be just scanned for terminology
and notation.

The cardinality of a set A is denoted by |A|. Let A and B be sets and ae ` A * B a
(binary) relation from A to B. The fact that (a; b) 2 ae (a 2 A; b 2 B) is also expressed by
writing aaeb. For any a 2 A, we denote by aae the set of elements of B that are in relation
ae with a, i.e. aae = {b 2 B | a ae b}. The converse of ae is the relation ae-1 = {(b; a) | a ae b}.
Obviously bae-1 = {a | a ae b}.

Next we consider relations on a set A, i.e. subsets of A * A. These include the diagonal relation !A = {(a; a) | a 2 A}, and the universal relation 'A = A * A. The powers
aen (n?0) of a relation ae are defined as follows: ae0 = !A, and aen+1 = aen ffi ae for n?0.
The relation ae is called reflexive if !A ` ae; symmetric if ae-1 ` ae; and transitive if
ae2 ` ae.

A relation on A is called an equivalence relation on A, if it is reflexive, symmetric
and transitive. The set of all equivalence relations on a set A is denoted by Eq(A). It
is obvious that both !A 2 Eq(A) and 'A 2 Eq(A). Let ae 2 Eq(A). The ae-class aae of an
element a (2 A) is also denoted by a=ae. The quotient set or the partition of A with
respect to ae, is A=ae = {aae | a 2 A}. If ss 2 Eq(A) and ss ` ae, then the partition A=ss is a
refinement of A=ae (each ae-class is a union of some ss-classes); this is also expressed by
saying that ss is finer than ae or that ae is coarser than ss. We often define an equivalence
relation ae on A via the set A=ae, i.e. in the form A=ae = {C1; : : : ; Cm}, where the sets Ci
are the classes of ae.

The cardinality of A=ae is called the index of ae; especially, if |A=ae| is finite, then ae
is said to have a finite index. For any subset H of A, H=ae denotes {aae | a 2 H }. The
equivalence ae saturates the subset H , if H is the union of some ae-classes. Hence, ae
saturates H iff aae 2 H=ae implies aae ` H .

A mapping or a function OE : A ! B is a relation OE ` A * B such that |aOE| = 1 for
all a 2 A. If aOEb, then b is the image of a and a is a preimage of b. That b is an
image of a is expressed by writing b = OE(a) and that a is a preimage of b is written as
a 2 OE-1(b). The notation a = OE-1(b) is justified, when OE is bijective (see below). The
restriction of a mapping OE : A ! B to a set C ` A is the mapping OE|C : C ! B where
OE|C = OE " (C * B).

The composition of two mappings OE : A ! B and   : B ! C is the mapping
OE  : A ! C, where OE  is the product of OE and   as relations. The kernel OEOE-1
of a mapping OE : A ! B, also denoted by ker OE, is an equivalence relation on A
and a OEOE-1 b iff aOE = bOE (a; b 2 A). A mapping OE : A ! B is called injective if

336 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363
ker OE = !A; surjective (or onto) if AOE = B; and bijective if it is both injective and
surjective.

2.2. Recognizable string languages and DFA

An alphabet is a finite nonempty set of letters. In what follows X always denotes
an alphabet. A finite sequence of letters from an alphabet X is called a string over
X . Consider a string w of the form w = x1x2 : : : xn, where each xi 2 X . If n = 0, then
w is the empty string, denoted by ffl. The length of w, written as |w|, is n. Especially,|

ffl| = 0. The set of all strings over X is denoted by X *. A language over X , or an
X -language is any subset of X *. We will later on assume that X is clear from the
context and talk only about languages.

Definition 1. A deterministic finite automaton (DFA), A = (A; X; ffi; a0; A0) consists of

(1) a finite, nonempty set A of states,
(2) the input alphabet X ,
(3) a transition function ffi : A * X ! A,
(4) an initial state a0 2 A, and

(5) a set A0 ` A of final states.

The function ffi is extended to a function ^ffi : A * X * ! A as usual (we will omit
the cap from ^ffi in the sequel): ^ffi(a; ffl) = a, and ^ffi(a; xw) = ^ffi(ffi(a; x); w) (a 2 A; w 2 X *;
x 2 X ). The language recognized by a DFA A is now defined as

L(A) = {w 2 X * | ^ffi(a0; w) 2 A0}:
A language L is recognizable, if there exists a DFA A such that L = L(A). Two DFA
A and B are said to be equivalent iff L(A) = L(B).

2.3. Minimal DFA

DFA A is said to be minimal, if no DFA with fewer states recognizes L(A). The
minimal DFA for any recognizable language is unique up to isomorphism. It is also
well-known that given an arbitrary DFA A, one can effectively construct the minimal
DFA equivalent to A. We review here briefly the classical construction.

DFA A = (A; X; ffi; a0; A0) is said to be connected, if A = {ffi(a0; w) | w 2 X *}. We assume hereafter that our DFA are connected (the construction of a connected equivalent
DFA from a given one is trivial).

Two states a and b of A are equivalent, which we express by writing a aeA b, if

(8w 2 X *) (ffi(a; w) 2 A0 , ffi(b; w) 2 A0):
DFA A is reduced, if a aeA b implies a = b, i.e. aeA = !A.

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 337
A relation ae 2 Eq(A) is a congruence of A, if
(1) a ae b implies ffi(a; x) ae ffi(b; x) for all a; b 2 A and all x 2 X , and
(2) ae saturates A0.

We denote by Con(A) the set of all congruences of A. It is well-known that
the relation aeA is the greatest (coarsest) congruence of A. For any ae 2 Con(A), the
quotient DFA A=ae is defined as A=ae = (A=ae; X; ffi=ae; a0=ae; A0=ae), where ffi=ae(a=ae; x) =
ffi(a; x)=ae.

We state next a few facts showing how the minimal DFA can be constructed from
a given one.

Proposition 2. Let A = (A; X; ffi; a0; A0) be a DFA and ae 2 Con(A). Then L(A) =
L(A=ae); and in particular; L(A) = L(A=aeA).

Proposition 3. Let A = (A; X; ffi; a0; A0) be a DFA. Then the quotient DFA A=aeA is
minimal.

2.4. DFA as unary algebras

The exposition of the following material is simplified somewhat by regarding DFA
as unary algebras as proposed by B"uchi and Wright (cf. [4], for example). This means
that X is viewed as a set of unary operation symbols and the transition function ffi of
A = (A; X; ffi; a0; A0) is replaced by the X -indexed family (xA: x 2 X ) of unary operations
which are defined so that for any x 2 X and a 2 A, xA(a) = ffi(a; x). We shall omit A
from the superscript and write simply x(a) for ffi(a; x). Clearly, a string w = x1x2 : : : xn
is accepted by A if and only if xn(: : : (x2(x1(a0)) : : :) 2 A0.

3. Developing an efficient minimization algorithm

The process of minimizing a DFA A is essentially the same as the computation of
the relation aeA.

3.1. The classical algorithm

The classical minimization algorithm is based on the following `layer-wise' definition
for the equivalence relation aeA.

Proposition 4. Let A = (A; X; ffi; a0; A0) and a series aei (i?0) of equivalence relations
on A be defined as follows:

ae0 = {(a; b) | a; b 2 A0} [ {(a; b) | a; b 2 A - A0};
aei+1 = {(a; b) 2 aei | (8x 2 X ) (x(a); x(b)) 2 aei}:

338 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363

Then the following hold.
(1) ae0 ' ae1 ' * * * .
(2) If aei = aei+1 then aei = aei+j for all j ? 0 and furthermore; aei = aeA.
(3) There exists 06k6|A| such that aek = aek+1.

When the construction of Proposition 4 is implemented, each refinement step leading
from aei to aei+1 consists of a series of refinements on individual classes of aei. The
refinement of a single class B can be implemented with suitably chosen data structures
for equivalence relations to run in time O(|X ||B|), and each refinement from aei to aei+1
takes thus time O(|X ||A|).

The classical minimization algorithm is ineffective, since we can construct (the worst
kind of) a DFA A = ({a1; : : : ; am}; X; ffi; a1; {am}), where x(ai) = ai+1 for 16i6m - 1
and x(am) = am (x 2 X ), and y(ai) = ai for all ai 2 A and y 2 X; y 6= x. Now A=ae0 ={{

am}; A-{am}}, A=aeA = !A, and each refinement step from aei to aei+1 removes always
one state from the only nonsingleton class. The work performed by the algorithm (even
if we optimize it to avoid considering singleton classes) will then be proportional to|

X | P|A|-2i=1 (|A| - i), which leads to O(|X ||A|2) execution time.

3.2. Atomic refinements

The classical approach just described represents the end of one line of evolution
starting from Proposition 4. Due to the facts noted, it cannot be made more efficient
by just using smarter and more efficient data structures. In order to develop an asymptotically faster minimization algorithm, we must try to find other ways to perform the
construction.

Note that the construction of Proposition 4 reaches the situation aei = aei+1 when aei
becomes a congruence, i.e. (8a; b 2 A; x 2 X ) a aei b ) x(a) aei x(b). Now consider the
situation where aei 6= aei+1. Obviously,

aei 6= aei+1 , (9a; b 2 A; x 2 X ) (a; b) 2 aei and (x(a); x(b)) 62 aei

, (9B 2 A=aei; x 2 X ) a; b 2 B and (x(a); x(b)) 62 aei
, (9B; C 2 A=aei; x 2 X ) a; b 2 B and x(a) 2 C and x(b) 62 C
, (9B; C 2 A=aei; x 2 X ) x(B) " C 6= ; and x(B) 6` C:

The step from aei to aei+1 can thus be understood as a series of `atomic' refinements
performed for such x 2 X , B; C 2 A=aei that

x(B) " C 6= ; and x(B) 6` C (1)
holds. Each such atomic refinement partitions then B into B " x-1(C) and B -
(B " x-1(C)). We denote these refinements of B with BC;x and BC;x, respectively. We
use the notation B0 and B00 for these refinements of B when their relation to the pair
(C; x) has no importance.

Let us next rewrite Proposition 4 in the terms of these atomic refinements.

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 339
Proposition 5. Let A = (A; X; ffi; a0; A0) be a DFA and a series `i (i?0) of equivalence
relations on A be defined as follows:

A=`0 = {A0; A - A0};

A=`i+1 = 8!:

(A=`i - {B}) [ {BC;x; BC;x} if (1) holds for some B; C 2 A=`i;

x 2 X;
A=`i otherwise:

Then there exists a k6|A| such that `k+l = `k for all l?0 and `k = aeA.

Proof. The upper bound comes from the observation, that each atomic refinement
increases the index of `i by one. Naturally this index cannot be increased more than|

A| - 1 times.

It is clear that `k is both an equivalence relation saturating A0 (it is a refinement of
`0) and a congruence of A (since Eq. (1) does not hold for `k). What remains, is to
show that `k is also the greatest congruence aeA of A.

We show first that `i ' aeA for all i?0. When contraposed, the claim is that if
(a; b) 62 `i then (a; b) 62 aeA (for all i?0). This is clearly true for `0, since final and
non-final states are not in aeA. Suppose then that the claim holds for all 06l6i and
let (a; b) 2 `i. If (a; b) 62 `i+1, it must be the case that for some x 2 X , (x(a); x(b)) 62 `i.
But this implies (by IA) that x(a) and x(b) are not in aeA. Thus, a and b become
inequivalent in `i+1 only when they are shown to be inequivalent in aeA, formally
(a; b) 62 `i+1 ) (a; b) 62 aeA.

It now holds that `0 ' `1 ' * * * ' `k (by definition), and consequently `0 ' `1 ' * * * '
`k ' aeA. Since aeA is the greatest congruence of A, and `k is a congruence, it must be
the case that `k = aeA.

Note that the construction given in Proposition 5 does not fix the order in which
the triples B, C, x are exploited to refine `i. Thus, all the different orderings yield the
same result at the end: the unique greatest congruence.

3.3. A change of view
Algorithm 1 (Computing aeA using atomic refinements)
EQUIVALENCE(A)
1 A=`  {A0; A - A0}
2 while (9B; C 2 A=`; x 2 X ) s.t. Eq. (1) holds do
3 A=`  (A=` - {B}) [ {BC; x; BC;x}
4 return `

Proposition 5 leads directly to Algorithm 1. As such, it is yet rather abstract, and
in particular we have to decide how to efficiently find some triple B; C; x for which
Eq. (1) holds. Given some class B 2 A=` it seems natural to try to consider, instead

340 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363
of (A=`) * X , only those class-letter pairs (C; x) for which some B exists such that
Eq. (1) holds. Let us call these pairs as refiners of B in `, shortly ref (B; `):

ref (B; `) = {(C; x) 2 (A=`) * X | x(B) " C 6= ; and x(B) 6` C}:
The crucial change of view made in [11] was that, instead of iterating over the classes
B that get refined, we iterate over the refiners (C; x) causing the classes B to get
refined.

As each B is related to ref (B; `), so is each pair (C; x) related to its objects of refinement in `, obj(C; x; `) = {B 2 A=` | (C; x) 2 ref (B; `)}. The definition of obj(C; x; `)
leads us directly to Algorithm 2, which is easily shown to implement the construction
of Proposition 5. It just avoids considering all the irrelevant classes B which would
not get refined with respect to pair (C; x).

Algorithm 2 (Refiner-driven implementation)
EQUIVALENCE(A)
1 A=`  {A0; A - A0}
2 while some (C; x) 2 (A=`) * X with obj(C; x; `) 6= ; exists
3 for B 2 obj(C; x; `) do
4 replace B with BC;x and BC;x in A=`
5 return `

We next consider the selection of the pairs (C; x) for which obj(C; x; `) 6= ;, or
equivalently, the rejection of the pairs for which the opposite holds. Suppose we start
with all the pairs created from A=` = {A0; A - A0} and X , pick some (C; x) and refine
the classes in obj(C; x; `). The following lemma (from [9]) tells us that (in the case C
remains unmodified) at least (C; x) itself can be counted out in the next iterations.

Lemma 6. Let A = (A; X; ffi; a0; A0); ` 2 Eq(A); B; C 2 A=` and x 2 X . Suppose we refine
B 2 A=` into BC; x and BC;x with respect to (C; x). Let D be a subset of BC;x or BC;x.
Then D 62 obj(C; x; `).

Proof. Let us first consider the case D 2 {BC;x; BC; x}. Then either x(a) 2 C for all a 2 D
or x(a) 62 C for all a 2 D. The same property holds naturally for all subsets of D.

The bookkeeping needed for telling whether a particular pair (C; x) has been used
can be implemented as follows:*

We maintain a set L of candidate refiners, shortly candidates, in (A=`) * X . Initially
L = {A0; A - A0} * X .*
Pairs (C; x) are now selected from L, not (blindly) from all of (A=`) * X .*
Every time we select a pair (C; x) from L, we remove it from L. This is justified
by Lemma 6.

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 341

* After refining ` to `0, we must also update L to contain only classes of `0. We do

the following for each x 2 X and each class B refined to B0 and B00:ffi

If (B; x) 2 L, we remove (B; x) and add both (B0; x) and (B00; x) to L. This is
(indirectly) justified by Proposition 5, since only the current `-classes are used
in the construction.ffi
If (B; x) 62 L, we simply add (B0; x) and (B00; x) to L.
Note that new items are inserted into L only when some class gets refined. Thus,
L will eventually become empty, because each iteration removes one element from L,
and the total number of elements added into L is bounded by 2|X ||A| (the maximum
number of different equivalence classes created in any refinement sequence is 2|A|-1).
The ideas above are collected into Algorithm 3.

Algorithm 3 (Set-driven implementation)
EQUIVALENCE(A)
1 A=`  {A0; A - A0}
2 L  (A=`) * X
3 while L 6= ; do
4 remove a pair (C; x) from L
5 for each B 2 obj(C; x; `) do
6 replace B with BC;x and BC;x in A=`
7 for each y 2 X do
8 if (B; y) 2 L then
9 replace (B; y) with (B0; y) and (B00; y) in L
10 else
11 insert (B0; y) and (B00; y) to L
12 return `

3.4. Derivation trees

Consider some A = (A; X; ffi; a0; A0) and the execution of Algorithm 3. At each iteration of the main loop, the refinement process leading from the initial `0 to the current
` can be described by a binary tree DT`, the derivation tree of `.*

Each node of DT` is labeled with a subset of A. We denote by DT`(B) the subtree
rooted at a node labeled with B.*
The label of the root is A, and its (immediate) descendants are labeled with A0 and
A - A0.*
The leaves of the tree are labeled with the (current) classes of `.*
Each intermediate node labeled B has two descendants labeled with B0 and B00.
Note that the exact shape of DT` depends on the order in which the candidates are
selected from L. However, all orderings lead eventually to the same result, namely
aeA.

342 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363
3.5. Colored derivation trees

We introduce here a marking technique that helps us to establish properties of derivation trees in a compact manner. It is to be noted that these marks, colors, are not used
directly in our algorithms but induced by its execution.

The set L is manipulated via the following operations during the execution of
Algorithm 3: deletion (line 4), insertion (line 11), and replacement (line 9). These
operations can be indicated in DT` by allowing each x 2 X to color the nodes of DT`.
The color assigned by x to some node B tells us what is the operational history behind
the pair (B; x).

For each pair (B; x), where B is in DT` and x 2 X , we define the x-color of B,
Col(x)(B) (the curried form is used here on purpose), to be one of the following:*

black (B; x) 2 L, because it was inserted (lines 2, 9, and 11).*
green (B; x) 62 L, because it was removed (line 4).*
amber (B; x) 62 L, because it was replaced (line 9).
Note that if Col(x)(B) = amber for some x 2 X , then B is necessarily an inner node of
DT`, and if Col(x)(B) = black, then B must be a leaf node. Green nodes may appear
both as leaves and as inner nodes. The root of the tree may be colored green for all
x 2 X , although (A; x) has not to be used in the refinement process (it will not refine
any class).

3.6. Reducing the number of candidates

Consider once again the execution of Algorithm 3, where some class B (leaf of DT`)
is refined with respect to some (C; x). Then DT` will change in such a way that B is
given two descendants, which are colored black for each letter y 2 X . Col(x)(B) either
stays green or changes from black to amber. Note that the latter case implies that of
the triple (B; y); (B0; y); (B00; y) only the last two are (possibly) used as future refiners.
The fact that this is exactly the way in which Proposition 5 works implies immediately
the following corollary.

Corollary 7. Let A = (A; X; ffi; a0; A0); ` 2 Eq(A) and B 2 A=`. Suppose we refine B into
B0 and B00. Then; for any x 2 X; refining all the classes of ` with respect to (B0; x) and
(B00; x) yields the same result as refining ` with respect to (B; x); (B0; x) and (B00; x).

Now consider DT`, y 2 X and the nodes B for which Col(y)(B) = green. If, after
refining ` with respect to (B; y), B itself gets refined, we add both (B0; y) and (B00; y)
into L. However, as seen in Fig. 1, from the viewpoint of any D ` A, (B; y) refines
D to parts {D1 [ D2; D3}. Now clearly refining {D1 [ D2; D3} with either (B0; y) or
(B00; y) gives the same result, the set {D1; D2; D3}. Combining Corollary 7 and this
remark we get the following Lemma (a modification of Lemma 6 in [9]).

Lemma 8. Let A = (A; X; ffi; a0; A0); ` 2 Eq(A) and B 2 A=`. Suppose we refine B into
B0 and B00. Then; for any y 2 X; refining all the classes of ` with respect to any two

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 343

Fig. 1. Illustration of Lemma 8.
of the pairs (B; y); (B0; y) and (B00; y) gives the same result as refining them with
respect to all three of them.

Proof. Consider an arbitrary class D 2 A=`, a 2 D and y 2 X . As seen in Fig. 1, the
transition y(a) satisfies exactly one of the following: y(a) 2 B0 (1), y(a) 2 B00 (2), or
y(a) 62 B (3). Now each possible refinement sequence with respect to any two of the
sets B, B0 and B00 and letter y partitions D into D1; D2; D3, which is the same as the
result yielded by performing all the three refinements.

Corollary 9. Let A = (A; X; ffi; a0; A0); ` 2 Eq(A); x 2 X; B 2 A=`; and {B0; B00} a refinement of B. If obj(B; x; `) = ; and obj(B0; x; `) = ; then obj(B00; x; `) = ;.

Corollary 9 can be exploited to enhance Algorithm 3 as follows: when we add new
refiners (B0; y) and (B00; y) into L, we first check, whether Col(y)(B) is green. If this
is the case, then only either one of the pairs has to be added. This modification leads
to a new kind of nodes B in DT`: those for which a pair (B; x) has never been added
to L. We augment our coloring to include also this case by using a new color red and
defining Col(x)(B) = red for these nodes.

Finally, the following corollary will be of use when initializing our list L.

Corollary 10. Let A = (A; X; ffi; a0; A0); A=` = {A0; A - A0}; and x 2 X . Then refining `
with respect to either (A0; x) or (A - A0; x) yields the same result as refining ` with
respect to both of them.

When we have the freedom of making a choice between two candidates, which one
of them should be included in L? The natural basis for this selection is the amount of
computation caused by the alternatives. However, we postpone the efficiency issues for
the moment, and simply assume we have an auxiliary function ADDBETTER(B0; B00; x; L)
performing the informed update of L.

3.7. Reducing further the number of candidates

In order to apply Corollary 9 to enhance our minimization algorithm we have to
check somehow whether Col(y)(B) = green when considering pairs (B0; y) and (B00; y).
One possible way to perform this checking would be to just implement the correspond344 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363
ing colored trees and inspect them. Hopcroft, however, simply tests in [12] whether
(B; y) 2 L or not.

At the first glimpse this simple test seems not to be enough, since it may be the case
that Col(y)(B) = red, and the `better sibling' of B was added to L with y. However,
if we just knew that the refinement process performed by the parent class of B and y
had been done, then we could again use Lemma 8 to justify the addition of only either
of B0 and B00. In general, if we had the right to - in addition to taking only either one
of the refinements of the green nodes - take only either one of the refinements of the
red ones, then the simple check (B; y) 2 L would suffice.

The required justification will be given in the following section (Theorem 14). Before
that, let us rewrite Algorithm 3 using the optimizations discussed. Algorithm 4 uses the
test (B; y) 2 L? to control the addition process, and it uses subprogram ADDBETTER to
make the choice when we have one. We have split the loop over the set obj(C; x; `) into
two parts (lines 6 and 8) in order to make it resemble more the final implementation
(see next section) where this separation yields some computational benefits.

Algorithm 4 (Optimized set-driven algorithm)
EQUIVALENCE(A)
1 A=`  {A0; A - A0}
2 L  ;
3 for x 2 X do ADDBETTER(A0; A - A0; x; L)
4 while L 6= ; do
5 remove a pair (C; x) from L
6 for each B 2 obj(C; x; `) do
7 replace B with BC;x and BC;x in A=`
8 for each B just refined do
9 for each y 2 X do
10 if (B; y) 2 L then
11 replace (B; y) with (B0; y) and (B00; y) in L
12 else
13 ADDBETTER(B0; B00; y; L)
14 return `

Example 11. Let us trace the execution of Algorithm 4 with the DFA

A = ({a1; : : : ; a8}; {x; y}; ffi; a1; {a8});
where x(ai) = ai+1 for 16i67 and x(a8) = a8, and y(ai) = ai for all ai 2 A. In the
beginning we have A=` = {B1; B2} where B1 = {a7} and B2 = {a1; : : : ; a6}.

Let us use the sizes of classes as our evaluation measure in routine ADDBETTER. Since
the root node A is treated as green for all letters, we get Col(x)(B1) = Col(y)(B1)
= black and Col(x)(B1) = Col(y)(B2) = red. Suppose the algorithm chooses the pair
(B1; y) as a candidate. Then obj(B1; y; `) = {B1} and no refinement is done. However,

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 345
Col(y)(B1) = green after using the candidate. The only pair remaining in L is (B1; x)
and obj(B1; x; `) = {B1; B2}. Now B2 is refined to B3 = {a6} and B4 = {a1; : : : ; a5}. Updates of L add pairs (B3; x) and (B3; y) into L leaving Col(x)(B4) = Col(y)(B4) = red.
The process continues similarly until only singleton classes are left.

3.8. Properties of derivation trees

We are next to show that Algorithm 4 still computes aeA. Before that we make some
remarks on the colors of the nodes our latest algorithm assigns to the nodes of DT`.
It is straightforward to show that the following properties hold.

(1) When a green or red node is refined ((B; y) 62 L), its descendants are colored black

and red (line 13).
(2) Black nodes may change to green (line 5) or amber (line 11). In the latter case

both descendants of the node are colored black.
(3) Green, red, and amber nodes do not change their color.
(4) Immediate ancestors of red nodes are either red or green.
(5) Each red node has a green ancestor (recall that the root of DT` is colored green

for all x 2 X ).

Definition 12. Let x 2 X , B 2 DT`, and Col(x)(B) 2 {amber; green}. We define the
green fringe of B in DT`, gf(B; x) as follows.

gf(B; x) = ae {B} if Col(x)(B) = green;gf(B0; x) [ gf(B00; x) if Col(x)(B) = amber:
Lemma 13. Let x 2 X; B 2 DT`; and Col(x)(B) = amber. Then

(1) gf(B; x) is well-defined; and
(2) gf(B; x) is a partition of B.

Proof. The first claim is a direct consequence of the fact that the initially black (direct)
descendants of B can later become only green or amber. Thus, each path emanating
from B must contain a green node, and all the intermediate nodes in the path to first
such node are amber. The second claim follows directly from the definition of gf(B; x).

We are now ready to establish the correctness of Algorithm 4.
Theorem 14. Let A = (A; X; ffi; a0; A0) and ` be the relation returned by Algorithm 4.
Then obj(B; x; `) = ; for all B 2 DT` and x 2 X .

Proof. The proof branches on Col(x)(B). Note that Col(x)(B) 6= black, since L is
empty when the algorithm terminates.*

green The claim clearly holds, since (B; x) has been used to refine `.*
amber Let gf(B; x) = {B1; B2; : : : ; Bm} (m ? 2). We know that ` has been refined
with all the pairs (Bi; x) (1 6 i 6 m) and thus obj(Bi; x; `) = ;. Consequently (by a

346 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363

simple extension of Corollary 9) obj(B1 [ * * * [ Bm; x; `) = ;, and, since gf(B; x) is
a partition of B, we have that obj(B; x; `) = ;.*
red Recalling the fact that B must have a green ancestor, let G be the nearest of them, and let B = R1; : : : ; Rm; G (m ? 1) be the path from B to G. Since
Col(x)(G) = green and Col(x)(Rm) = red, it must be the case that the sibling of Rm,
say R0, has Col(x)(R0) 6= red. The cases above tell us that obj(R0; x; `) = ;. Since
also obj(G; x; `) = ;, it follows directly from Corollary 9 that obj(Rm; x; `) = ;. The
same reasoning can now be continued downwards the path by using Rm in the place
of G to show that obj(Rm-1; x; `) = ; and, similarly, all the way down to R1 = B.

Corollary 15. The relation ` returned by Algorithm 4 is the greatest congruence of
the given DFA A.

3.9. Comparison to previous work

Hopcroft gives in [11, 12] proof on the property that the relation ` computed by
the minimization algorithm is the greatest congruence of the input DFA. The proof
is correct under the assumption that all relevant candidates are considered (directly or
indirectly) in the algorithm. This assumption is neither stated explicitly nor established
as a property of the algorithm.

Gries tackles in [9] the task of showing that the effect of refining the relation with
respect to all candidates is actually achieved. This is done by establishing the following
loop invariant (quotation rewritten to the formalism used here) for the main loop of
the algorithm.

If (C; x) 62 L then, for all classes B, either (1) does not hold for B, C and x
or we are assured by other means that it does not hold when the algorithm
terminates.

The `other means' are related to the execution of line 13 of Algorithm 4. If (B; x) is
not in L but Col(x)(B) = green, it is clearly justified to add either of the refinements
of B by Lemma 8. If, however, Col(x)(B) = red, we have to show, where our `other
means' come from: in [9] they are not given a formal description.

Aho and Ullman give a different proof in their textbook on algorithms [1]. When
rewritten for the formalism used in this article and nonunary alphabets, their loop
invariant can be stated as follows.

Pair (B; x) where B ` A is said to be safe for ` if obj(B; x; `) = ;. If (B; x) 62 L
then there is a set {B1; : : : ; Bk} such that (B1; x); : : : ; (Bk; x) 2 L and (B [ B1 [* * * [

Bk; x) is safe for `.

The proof (in the restricted case of unary alphabets) found in [1] of the invariant
above is lengthy and technical. Our proof of the same property for the general case
(Theorem 14) is a dozen lines long yet intuitive (of course, the prerequisites take care
of most of the content).

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 347
4. Implementation

Before we proceed to time analysis we have to address the implementation of the
various data structures and operations needed by Algorithm 4. Here we also introduce different variants of the algorithm caused by different implementations of routine
ADDBETTER.

4.1. Basic data structures

In what follows, we assume that each letter x 2 X , state a 2 A and class B 2 A=` can
be interpreted as a natural number. Then, for example, the transitions x : A ! A are
trivially implemented as a |X | * |A| matrix trans, where trans[x][a] contains the
value of x(a).

Most of the set-like structures we manipulate (e.g. x-1(a)) are partitions of A. Furthermore, the only nontrivial operation on these sets is the refining of them. These
remarks enable us to use a simple and efficient implementation where these sets are
represented as segments of a fixed-size array. Segments are accessed via header elements containing the index of the first element and the size of the segment.

We use the following names for our partition structures:*

inv_head is an |X | * |A| matrix of headers of the segments x-1(a).*
inv_elts is an |X | * |A| matrix of segment elements.*
cls_head is an array of size |A| of headers of equivalence classes.*
cls_elts is an array of size |A| of segment elements.
For example, inv_elts[x][inv_head[x][a].first] contains the first element of
x-1(a).

Each state a 2 A is linked to its class information via entries in array states, where
each element states[a] contains the following fields:*

cls_of: the number corresponding to class a=`; and*
idx_of: the index of a in its class segment (in array cls_elts).
The data structure implementing the set L has to support additions, deletions and membership tests. Since we are allowed (when choosing the current candidate) to remove an
arbitrary element of L, we can use a very simple implementation where L is presented
just as a list of integer pairs, and the membership tests are done using an |A| * |X | bit
matrix L_member.

4.2. Refinement step

Consider the implementation of the refinement loop (lines 6 and 7) of Algorithm 4.
We will save some space and time by re-using the names of the refined classes for
either of their refinements. We must, however, be careful not to refine the class C of the
current refiner (C; x) before all the classes intersecting x-1(C) have been considered.
Thus, the refinement is implemented in two steps:

348 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363

(1) Collect all the classes B that might be refined (those for which x(B) " C 6= ;) into

list suspects.
(2) Refine the classes in suspects (omit B for which x(B) " C ` C).

In the implementation, we augment the class header records to contain (for each
B 2 A=`) fields counter for the number |x-1(C) " B|, and move for the set (list)
x-1(C) " B. The algorithm will be composed in such a way that the counters are
always 0 before the first (collection) step is performed.

The list of suspect classes intersecting x-1(C) and states that would be moved in
the execution of the actual refinement are constructed by iterating over all the states
a in x-1(C), and adding a into the list of states to be moved in future from a=`.
At the same time, we can add class a=` in the suspect list (if not already there)
and increase its counter field. Note that the maximum number of elements in all
move-lists constructed above is at most |A|. Moreover, we can re-use the space needed
for these lists at each iteration. After the whole set x-1(C) has been considered, a
simple check (counter = size) tells us whether x-1(C) " B = B or not.

Algorithm 5 contains the implementation of procedure COLLECT performing this collection step. Routine IINSERT used by COLLECT just places its first argument to the
beginning of the second argument (list).

Algorithm 5 (Collecting classes and states)
COLLECT(x, C, suspects)
1 suspects  nil; CH  cls head[C]
2 for i  CH.first to CH.first + CH.size do
3 inv x a  inv head[x][cls elts[i]]
4 for j  inv x a.first to inv x a.first + inv x a.size do
5 b  inv elts[j]
6 B  states[b].cls of
7 if (cls head[B].counter = 0) then
8 INSERT(B, suspects)
9 cls head[B].move  nil
10 cls head[B].counter  cls head[B].counter + 1
11 INSERT(b, cls head[B].move)

The implementation of the second step, procedure REFINE, where the actual refinement
takes place is given in Algorithm 6. We have amalgamated the updates of L into this
step, since here we actually know what classes are really refined, and, what are the
(names of the) corresponding refinements. Updating L immediately after the creation
of a new class releases us from maintaining yet another data structure relating sibling
classes with each other. Note also that the re-using of old class names means that
when both (B0; x) and (B00; x) are to be inserted into L (descendants of amber nodes),
we know that (B00; x) is already in L (since B00 uses the same class name as B). The
implementation of routine SPLIT refining an individual class is discussed in the next
section.

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 349
Algorithm 6 (Performing the refinements)
REFINE(suspects)
1 for each B in suspects do
2 if cls head[B].counter ! cls head[B].size then
3 B'  SPLIT(B, cls head[B].move)
4 cls head[B].counter  0
5 cls head[B'].counter  0
6 for each x in X do
7 if L member[B][x] then ADD(B', x, L)
8 else ADDBETTER(B, B', x, L)

4.3. Splitting an individual class

Routine REFINE calls SPLIT with a class B and a proper subset (list of states) B0 of B.
SPLIT is then supposed to update the relevant data structures in such a way that

(1) B is split into B0 and B - B0,
(2) B - B0 is given the same name that was assigned to B, and
(3) B0 is assigned a new class name.
Algorithm 7 shows the implementation of SPLIT. It exploits the widely used technique
of a `moving wheel', where one location of an array is used as a `hole' for moving
data in the array. In addition to the fact that the partitioning clearly takes time linear
to the number of elements in the input list, the moving wheel approach is known to be
very efficient in practice. New class names are drawn from a global variable MaxClass.

Algorithm 7 (Splitting a class)
SPLIT(B, move) : Integer
1 Bnew  MaxClass
2 MaxClass  MaxClass + 1
3 hole  cls head[B].first
4 for each a in move do
5 apos  states[a].idx of
6 b  cls elts[hole]
7 states[a].idx of  hole
8 states[b].idx of  apos
9 cls elts[hole]  a
10 cls elts[apos]  b
11 states[a].cls of  Bnew
12 hole  hole + 1
13 cls head[B].first  hole
14 cls head[Bnew].first  cls head[B].first
15 cls head[B].size  cls head[B].size - move.size
16 cls head[Bnew].size  move.size
17 return Bnew

350 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363
4.4. Minimization algorithm

We have now implemented the actions needed in the main loop of the minimization
algorithm. What remains, are the initialization actions required to construct the inverse
mappings x-1, the initial equivalence relation and list L, and a simple loop calling
COLLECT and REFINE until L becomes empty. Algorithm 8 does exactly this.

It is assumed in the implementation that the input DFA A contains its transitions in
A.trans and the set of final states in list A.final (suitable as an input for SPLIT)
and that the first (universal) equivalence class is given name 1. After the algorithm
terminates, the equivalence of states can be obtained either via cls_head or states,
whichever is appropriate.

Algorithm 8 (Final minimization algorithm)
EQUIVALENCE(A)
1 -- construct the inverse of A.trans
2 `  A * A; MaxClass = 1
3 SPLIT(1, A.final)
5 for each x in X do ADDBETTER(1, 2, x, L)
6 while not EMPTY(L) do
7 C, x  REMOVE(L)
8 COLLECT(x, C, suspects)
9 REFINE(suspects)

4.5. Variations

Here we finally address the selection criterion used in the updates of L, that is, the
implementation of routine ADDBETTER, which is supposed to add the computationally
more feasible of the pairs (B0; y) and (B00; y) into L. Excluding the time required to
execute the updates of L themselves, the work done in routines COLLECT and REFINE
depends (mainly) on |x-1(C)|. This number should thus be the most natural measure
to use in the selection. The measures used in the literature are, however* |

B0|6|B00| ([9, 1]), and* |
x-1(A) " B0|6|x-1(A) " B00| ([12], also as an enhancement in [9]).
Let us write shortly x-1 " B for x-1(A) " B. Note that, for arbitrary B and x, both|

x-1(B)|?|x-1 " B| and |B|?|x-1 " B|, whereas the relation of |x-1(B)| and |B| is not
fixed. In the updates of L, it may well be the case that |x-1(B0)|!|x-1(B00)| although|

B0|?|B00| or even when |x-1 " B0|?|x-1 " B00| (only a few transitions enter B0), but
also vice versa (many transitions enter a small subset of B00). Thus, these three criteria
are not generally comparable.

4.6. Measure |B|

The implementation of the required test is simple (no other modifications are required
in Algorithm 8):

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 351
ADDBETTER(B1, B2, x, L)
1 if cls head[B1].size6cls head[B2].size then ADD(B1, x, L)
2 else ADD(B2, x, L)

Note that with this measure a call COLLECT(x, C, suspects) does not necessarily
take time proportional to |x-1(C)|, because the first for-loop iterating over the elements of class C (line 3) may sometimes take more time than this. The reason for
the extra work is, that it might happen that most a 2 C have x-1 " {a} = ;, and,
henceforth, |C| supersedes |x-1(C)|. A single execution of COLLECT takes thus time
O(max(|C|; |x-1(C)|)). To our knowledge, this point was covered in somewhat disguised way in the analysis of Gries, but it seems to be forgotten in other texts.

4.6.1. Measures |x-1 " B| and |x-1(B)|

Measures |x-1 " B| and |x-1(B)| have many common features and are thus covered
in the same section. In particular, their efficient uses require the same additional data
structures, namely the sets x-1 " B. This is because x-1(B) can be easily computed
from x-1 " B without notable computational overhead. A direct implementation would
construct sets x-1(B) as segments of segment headers of x-1(a) for a 2 B. Sets x-1 " B
are implemented just as our equivalence classes.

We store the segment headers of x-1 " B and numbers |x-1(B)| in an |X | * |A|
matrix named in_class containing the usual segment information (first, size) and|

x-1(B)| in field inv_size. The segment elements are stored (for each letter) in a|
X | * |A| matrix in_c_elts. Furthermore, we need an |X | * |A| matrix in_c_idx_of
for the indices of states a within their segments. These entries contain some special
number (say, -1) for a 2 A with x-1(a) = ;.

The routine ADDBETTER can now be written as below for measure |x-1 " B|. Note
how our more informed measures guide the minimization algorithm better than the
sizes |B|. In particular, if |x-1 " B| = 0 (and consequently |x-1(B)| = 0), we do not
have to insert pair (B; x) into L. These pairs can be treated as green nodes in our
correctness analysis.

ADDBETTER(B1, B2, x, L)
1 if (in class[x][B1].size = 0) or
2 (in class[x][B2].size = 0) then
3 return
4 if (in class[x][B1].size 6 in class[x][B2].size) then
5 ADD(B1, x, L)
6 else ADD(B2, x, L)

For measure |x-1(B)| we just change line 4 to
4 if (in class[x][B1].inv size 6 in class[x][B2].inv size) then

With a set x-1 " B available, the elements of the set x-1(B) can be effectively found
by traversing the lists x-1(a) for each a 2 x-1 " B. Note that x-1(a) " x-1(b) = ; for

352 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363
all a 6= b (a; b 2 A), since x is a mapping. Thus, we do not have to be afraid of visiting
any state of x-1(B) more than once.

When some class B is refined to B0 and B00, we must also refine x-1 " B into x-1 " B0
and x-1 " B00 for each x 2 X . At the same time, we can compute the numbers |x-1(B0)|
and |x-1(B00)|. Supposing B0 = BC; x we have that

|x-1(B0)| = P

a2B0 |x

-1(a)|;

|x-1(B00)| = |x-1(B)| - |x-1(B0)|:
The partitioning itself can be done as in Algorithm 7, the only change being that we
skip those states of the move -list which are not in x-1 " B. Each update takes then|

X | iterations over each element of x-1 " B0.

Below is a list of other changes needed in the algorithm. The latter change gives us
the extra enhancement that COLLECT now makes always |x-1(C)| iterations, since the
useless elements of C are automatically skipped over.*

The sets x-1 " B and numbers |x-1(B)| for B 2 {A0; A - A0} must be initialized
before the main loop of the algorithm begins. This initialization can be implemented
to run in time linear in |X ||A|.*
The first for-loop of COLLECT is rewritten as

InxC  in class[x][C]
for i  InxC.first to InxC.first + InxC.size do

inv x a  inv head[x][in c elts[x][i]]

5. Time analysis

It is easy to show that the initialization steps done before the main loop are bounded
by O(|A||X |) (in all variants). We also know from the previous discussion that the time
spent in the updates of L is bounded by O(|X ||A|). The complexity of the remaining
steps of the main loop is analyzed in the following parts:*

In routine COLLECT we have to consider separately its outer and inner loop when |B|
is used as the selection measure. For other measures, the inner loop is dominating.*
In routine REFINE we separate the time spent in constructing the refinements
(equal to the inner loop of COLLECT) and the time needed to update the sets
x-1 " B. Of course, the latter work is not needed when |B| is used as the
measure.
In what follows, we first point out some general properties of cost functions defined on derivation trees. These properties enable us to establish (for each x 2 X ) an
O(|A| log |A|) bound for*

the work done in COLLECT and (consequently) in the refinement part of REFINE for
measure |x-1(B)|, and*
the outer for-loop of COLLECT for measure |B|.

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 353
After that we give an O(|X ||A| log |A|) bound for the number of steps taken in the
inner for-loop of COLLECT (and consequently for REFINE) for measure |B|. This analysis
suits also for the measure |x-1 " B|.

Next we tackle the problem of updating the sets x-1 " B. It turns out that the traditional implementation may take in some cases O(|X |2|A|) steps. If |X | is of the same
magnitude as |A| then methods exploiting these sets run in time O(|A|3), whereas the
method based on |B| has complexity O(|A|2 log |A|). A simple enhancement suffices,
however, to take us back to the O(|X ||A| log |A|) bound.

Summing up all of the above, we have that all the different variants (provided we
apply our enhancement) run in time O(|X ||A| log |A|), whereas the original implementation of the Hopcroft algorithm (using either |x-1 " B| or |x-1(B)|) may take O(|A|3)
when X is of the same magnitude as A.

Finally, we study some properties of the algorithm based on the measure |x-1(B)|
that suggest an ordering on the members of L.

5.1. Cost functions on derivation trees

Given a derivation tree DT` and a letter x, we define the cost of each subtree DT`(B),
cost(B; x) in the natural way as cost(B; x) = work(B; x) + cost(B0; x) + cost(B00; x); where
work(B; x)?0 is the work assigned to the pair (B; x). The cost of the whole DT` is thenP

x2X cost(A; x). The work-functions we use have some useful properties described inthe following definition.

Definition 16. Function work(B; x) is well-behaving if*

work(B; x)?0 only when Col(x)(B) = green, and*
work(B; x)?work(B1; x) + * * * + work(Bn; x) for any partition B1; : : : ; Bn of B with
Col(x)(B) = green.

We denote the work done with the green nodes as wg(B; x) in the sequel. Note that
we are allowed to write wg(B; x) even if Col(x)(B) 6= green: in that case it implies the
work that would be done if the pair (B; x) were selected from L.

Consider now work(B; x) as the number of steps needed to collect and refine classes
with measure |x-1(B)|. We clearly execute some c|x-1(B)| (c?0) instructions for each
selected candidate pair (B; x), and pairs with other colors do not participate in this work.
Furthermore, for any partition B1; : : : ; Bn of B we have that Pni=1 c|x-1(Bi)| = c|x-1(B)|.
Similarly, the outer loop of COLLECT is executed |B| times when |B| is used as the
selection measure, and Pni=1 c|Bi| = c|B| (c?0).

5.1.1. Proper colorings

Since the colors assigned to derivation trees depend on the measure used in the
selection, not all possible colorings are legal.

Definition 17. Let m(B; x) be the selection measure used in the updates of L. Mapping
Col(x) is a proper coloring of DT`, if the following conditions hold.

354 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363

(1) Col(x)(A) = green (root).
(2) Leaf nodes are not colored amber.
(3) If Col(x)(B) 2 {red; green} and m(B0; x)!m(B00; x) then Col(x)(B0) 2 {green;

amber} and Col(x)(B00) = red. If m(B0; x) = m(B00; x) an arbitrary choice can be
made.
(4) If Col(x)(B) = amber then both Col(x)(B0) 6= red and Col(x)(B00) 6= red.

It should be clear that all colorings induced by Algorithm 8 are proper.
5.1.2. Colorings with maximal cost

The definition of a well-behaving work-function leads directly to the intuition that
cost(A; x) has its maximum when x colors as many of the nodes green as possible, i.e.
none of the nodes are amber. We show next that this is indeed the case for proper
colorings.

Definition 18. The distance of colorings Col(x) and Col(y) is the number of nodes
in which they disagree, that is |{B 2 DT` | Col(x)(B) 6= Col(y)(B)}|.

Suppose we have a red-green coloring Col(x) on DT` such that Col(x)(B) 2 {green;
red} for all nodes B of DT`. If we define a new coloring Col(x0) such that the color
of one inner node B in DT` is changed from green to amber, then B0 and B00 must
be colored green (or amber with subsequent changes to descendant nodes) in order to
make also Col(x0) proper. The minimal changes from Col(x) to Col(x0) are (supposing B00 is red): Col(x0)(B) = amber and Col(x0)(B00) = green. The resulting Col(x0) is
proper since the numbers m(B; x) do not change. We denote this transformation with
Col(x0) = g2a(B; Col(x)) in the sequel.

Note that the set {g2a(B; Col(x)) | B 2 DT`; B =2 A=`} contains all proper colorings
within minimal distance (2) from Col(x). As a consequence, for any coloring Col(x0)
on DT` with k amber nodes, we have a red-green coloring Col(x) and nodes Bi (16i
6k) of DT` such that

Col(x0) = g2a(B1; g2a(B2; : : : ; g2a(Bk; Col(x)) * * *));
i.e. Col(x0) can be obtained from Col(x) with k iterations of g2a on appropriate nodes
of DT`. The next lemma tells us that the red-green colorings maximize cost(A; x).

Lemma 19. Let DT` be a partition tree; Col(x) a proper red-green coloring; g2a the
transformation described above; and Col(x0) a new coloring given by k?0 applications of g2a on Col(x). Then cost(A; x)?cost(A; x0).

Proof. The claim clearly holds for k = 0 (Col(x0) = Col(x)). Let the assumption hold
for k = l (l?0) and consider the case k = l + 1. Here we have two subcases:

(1) The region of the amber nodes is contiguous in the sense that there is only one

amber node with a nonamber parent.

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 355
(2) The coloring consists of a multitude of amber regions.
Consider case (1) and let B be the topmost node of the amber region. Then the colorings
Col(x) and Col(x0) must agree for all nodes below gf(B; x0) by the definition of g2a.

In the following derivation we use a shorthand cb(B; x) (cost below) for the number
cost(B; x) - work(B; x). Since work(D; x0) = 0 for all nodes D in the amber region
(work is well-behaving), we have that

cost(B; x0) = P

D2gf(B;x0) cost(D; x

0) 1

= P

D2gf(B;x0) [work(D; x

0) + cb(D; x0)] 2

= P

D2gf(B;x0) wg(D; x

0) + P

D2gf(B;x0)cb(D; x

0) 3

= P

D2gf(B;x0) wg(D; x) + PD2gf(B;x0)cb(D; x) 4

6 wg(B; x) + P

D2gf(B;x0) cb(D; x) 5

6 work(B; x) + P

D2gf(B;x0) cost(D; x) 6

6 cost(B; x) 7
The reasoning steps above are justified by the following facts:
1-2 Definition of cb.
2-3 Col(x0)(D) = green.
3-4 Col(x) and Col(x0) agree under D, and wg(D; x) = wg(D; x0) does not depend on

coloring.
4-5 work (and consequently wg) is well-behaving.
5-6 Col(x)(B) = green and work(D; x)?0.
6-7 work(E; x)?0 in the nodes E between B and gf(B; x0).

Consider then case (2). Each of the separate regions contains at most l amber nodes.
Let the topmost nodes of these regions be B1; : : : ; Bl (16l6|B|). Then it must hold by
the construction of Col(x0) that Col(x)(Bi) = green for all 16i6l, and subsequently
(by IA) that cost(Bi; x)?cost(Bi; x0).

Corollary 20. Let DT` be a partition tree; x 2 X; Col(x) a proper coloring of DT`;
and work(B; x) well-behaving. Then cost(A; x) has its maximum when Col(x)(B) 2 {red;
green} for all nodes B in DT`.

5.1.3. Recurrence relations for execution times

We next rewrite our formula for cost(B; x) in a form specialized to red-green colorings and to the case where m = wg, that is, the selection measure relates itself directly
to the actual work. This is the case for measure |x-1(B)| and the whole main loop;
measures |x-1 " B| and |B| are related only to the outer for-loop of the COLLECT routine.

356 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363

We denote by cg(n) the cost of a subtree rooted at a green node B with wg(B; x) = n,
and similarly with cr(n) the cost of a subtree rooted at a red node. If B is refined to
B0 and B00 where B0 has the smaller measure (wg(B0; x) = q6wg(B; x)=2), then the
following hold (cg(0) = cr(0) = 0):

cg(n) = n + cg(q) + cr(n - q);
cr(n) = cg(q) + cr(n - q) = cg(n) - n:

Combining the above we have that

cg(n) = n + cg(q) + cg(n - q) - (n - q) = q + cg(q) + cg(n - q):
Note that the equation for cg(n) above is very similar to f(n) = n + f(q) + f(n - q)
describing the complexity of quicksort [10, 21]. However, placing q = 1 at each step we
get cg(n) = n, whereas in the quicksort case we have f(n) = n2. If we place q = n=2,
we get in both cases n log n.

We next show that O(n log n) is also the upper bound for cg(n) for any 16q6n=2.
If wg(A; x) is initially bounded by k|A| (k?0), as it is with all of our measures, then
consequently cost(A; x) is bounded by O(|A| log |A|).

Lemma 21. Let cg(n) = q + cg(q) + cg(n-q); where 16q6bn=2c. Then cg is bounded
by O(n log n).

Proof. The proof follows immediately from the worst-case execution time analysis of
quicksort (see [7], for example). Guessing cg(n)6k n log n (k?0) and substituting the
recursive right-hand side of the equation with this guess, we get

cg(n) 6 max16q6n=2 [q + cg(q) + cg(n - q)]

= max16q6n=2 [q + kq log q + k(n - q) log(n - q)]:

To find the maximum of the function f(q) = q + k(q log q + (n - q) log(n - q)), we
differentiate it with respect to q and get

f0(q) = k0 + k(log q - log(n - q)) (k0?0);
f00(q) = (k= ln 2)(1=q + 1=(n - q)):

Since f00(q)?0 for all 16q6n=2, the maximum of f is either f(1) or f(n=2). Our
previous remarks tell us that f(1) = n and f(n=2) = n log n.

Corollary 22. Algorithm 8 executes O(|X ||A| log |A|) steps in COLLECT and REFINE
(excluding the updates of sets x-1 " B) when m(B; x) = |x-1(B)|.

Corollary 23. Algorithm 8 executes O(|X ||A| log |A|) steps in the outer loop of
COLLECT when m(B; x) = |B| or m(B; x) = |x-1 " B| (!|B|).

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 357
Note that the average case cannot be handled by simply assuming that all q in
the range 1::bn=2c are equally likely, and taking the average of the execution times
(as is done in the analysis of quicksort), since we should also consider all different
proper colorings and their distribution. This distribution is not necessarily uniform,
since Col(x) may depend on some Col(y). It is also unclear, whether all possible
proper colorings (for a fixed X and A) are induced by some DFA.

5.2. Measures |B| and |x-1 " B| and the remaining parts of the algorithm

For measures |B| and |x-1 " B|, the O(|x-1(B)|) work consumed in REFINE and in
the innermost loop of COLLECT is not directly proportional to m(B; x). Hence, the analysis of the previous section does not apply to them. We next show that the total
number of these executions is also of order O(|X ||A| log |A|). This is done by setting
an upper bound for the accesses of one particular transition x(a) = b when doing the
refinements. Lemma 24 below considers only the variant m(B; x) = |B|, since the case
m(B; x) = |x-1 " B| can be drawn from it by simply replacing |B| with |x-1 " B| in
appropriate places.

Lemma 24. Let a 2 A and x 2 X . Then the maximum number of times a pair (B; x)
with a 2 B is used as a parameter to COLLECT is bounded by log |A|.

Proof. Let us reinterpret the claim in the terms of derivation trees and their colorings.
Let B in DT`, a 2 B, and Col(x)(B) 2 {green; red}. We claim that if state a occurs in
some node D of a proper subtree of DT`(B), and Col(x)(D) = green, then |D|6|B|=2.
In particular, the claim holds for the nearest such D. The original claim follows from
the above, since A can be halved at most log |A| times.

We establish the claim by induction on the height h of DT`(B). The case h = 1
(B is a leaf) is clear, since 06 log |B|. Assume that the claim holds for all trees
of height k or less, h = k + 1, and (without loss of generality) that |B0|6|B00|. Then
Col(x)(B0) 2 {green; amber} and Col(x)(B00) = red. We have the following cases:

a 2 B0: Here we know that |B0|6|B|=2. If Col(x)(B0) = green, we are done. In the
amber case, if a ever occurs in a green node D under B0, it must also hold that|

D|6|B0|.

a 2 B00: We know that |B00|!|B|, and (by IA) that all the (possible) green nodes D
under B00 that contain a have the property |D|6|B00|=2.

Corollary 25. The total number of iterations done at the innermost for-loop of
COLLECT is bounded by O(|X ||A| log |A|) for both m(B; x) = |B| and m(B; x) = |x-1 " B|.

Proof. We make |x-1(a)| iterations whenever transition x(a) is considered. Summing
all these up we have thatP

a2A;x2X |x

-1(a)| log |A| = P

x2X |A| log |A| = |A||X | log |A|:

358 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363
5.3. Manipulation of sets x-1 " B

Here we tackle the problem of updating the numbers |x-1(B)| and sets x-1 " B.

5.3.1. Built and remaining classes

Consider the work performed when some class B is actually refined in routine REFINE
with respect to some pair (C; x): BC;x is built out of the states that are moved from
B whereas BC;x will simply consist of those states that remain in B. Similarly, in the
updates of y-1 " B, states of BC; x are moved to build up y-1 " BC;x for each y 2 X , but
no work is done for the states in the remaining part BC;x. Thus, for each refinement of
a class, we create two kinds of new classes: built and remaining classes, of which only
the first ones consume execution time in the updates of the sets x-1 " B. It follows
from the definition that each node of the partition tree has a built and a remaining
descendant.

We next consider how much the building work takes in total. If we want to preserve the time complexity within the O(|X ||A| log |A|) bound, we should establish the
following claim.

Claim 26. Let Built = {B1; : : : ; Bm} (06m6|A|) be the set of all built classes created
in the minimization algorithm. Then

|Built| =

mP

i=1 |Bi|6k|A| log |A| (k?0):

Our claim actually holds for the case causing maximal work for the other parts of
the algorithm, namely balanced derivation trees. This is because exactly half of the
classes at each level of the tree are built, and the sum of the sizes of these classes is|

A|=2 at each level. Since a balanced tree has log |A| levels, |Built| = (|A|=2) log |A|.

5.3.2. A counter-example to the O(|X ||A| log |A|) bound

We here construct a DFA such that |Built| is quadratic in |A|. The trick is to fool
the selection routine to select for 16i6|X | a pair that causes a class of size |A|=2 - i
to be built. With X large enough (e.g. |X | ss |A|=2), the total update time will be
quadratic in |A| thus falsifying Claim 26.

Let A = (A; X; ffi; a0; A0), where A = {a1; : : : ; a2n}, X = {x1; : : : ; xn}, A0 = {a1; : : : ; an},
and for all xi 2 X

xi(aj) = an+j for 16j6n;
xi(an+i) = an+i;
xi(aj) = ai for n!j 6= n + i62n:

Let us trace the execution of Algorithm 8 with m(B; x) = |x-1 " B|. Initially A=` ={

B1; B2} = {{a1; : : : ; an}; {an+1; : : : ; a2n}}. It follows from the construction that |x-1 "
B1| = 1 and |x-1 " B2| = |B2| = n for all x 2 X . Also, for each xi, all but one state from
B2 map to a single state ai in B1.

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 359
At the initialization step, only pairs (B1; x) are inserted into L. After refining `
with some (B1; xi), n - 1 states are used to build class B3 from B2 leaving only
state an+i in the original class. Assuming (without loss of generality) i = 1 we have
A=` = {B1; B2; B3} with B2 = {an+1} and B3 = {an+2 : : : ; a2n}. When L is updated, only
pairs (B2; x) are added since |x-1 " B2| = 1 and |x-1 " B3| = n - 1 for all x 2 X .

List L contains now two kinds of candidates: (B1; xi) (i 6= 1) and (B2; xi). If we pick
a pair (B1; xi) at the next iteration, we must again build a class of size n - 2 from B3.
Since we do not want to exploit any particular order of choosing pairs from L, let us
see what happens if we pick a pair (B2; xi) instead.

Refining ` with (B2; xi) will move state a1 from B1 because x(a1) = an+1 for all
x 2 X , and the result is A=` = {B1; B2; B3; B4} with B1 = {a2; : : : ; an} and B4 = {a1}.
When L is updated, we note that (B1; xi) are already in L (except for x1) and they are
thus replaced with both of their descendants. For x1, we do not add anything, since
x-11 (B1) = ;.

Consider now the contents of L. Candidates (B2; x) do not refine `, since both
x(B1) " B2 = ; and x(B3) " B2 = ;. Similarly, both x(B1) " B4 = ; and x(B3) " B4 = ;
for all x and candidates (B4; x) do not cause any refinement, either. Thus, we are
bound to eventually use a member (B1; x) (x 6= x1) of L, which forces us to build a
class of size n - 2.

From the above, we have that the each use of candidate (B1; xi) (which we eventually
will always end up to) builds a class of size (n- i) (supposing we try the letters in the
order of their indices). This arithmetic series sums up to n(n - 1)=2 = |A|2=8 - |A|=4.

The DFA used above is enough to fool also the metric |x-1(B)|. Here we have at
the beginning |x-1(B1)| = n - 1 and |x-1(B2)| = n + 1 for all x 2 X . Picking (B1; x1)
causes us again to build a class of size n-1, and the resulting partition is {B1; B2; B3} ={{

a1; : : : ; an}; {an+1}; {an+2 : : : ; a2n}}. In the update of L we have always that |x-1(B2)|
= 2 and |x-1(B3)| = n - 1. Now pairs (B2; x) will only move a1 out of B1, and the
resulting class (B4) will not refine ` any further. Thus, sooner or later, we must hit a
candidate (B1; x) now building a class of size n - 2.

5.3.3. A simple yet powerful enhancement

The counterexample given in the previous section exploited the fact that our selection
measures could be lurked to build sets x-1 " B of unacceptable sizes. When some class
B is first refined, we have no choice in selecting whether we build BC;x or BC; x: only
members of the former are collected into the move-list from the elements of x-1(C).
However, after BC;x and BC;x are once created, we are no longer forced to build sets
y-1 " BC; x, too. In fact, we have a total freedom of choice, and naturally, we choose
to build the smaller one.

Algorithm 9 (Splitting the sets x-1 " B)
SPLITINCOME(Old, Large, Small)
1 for each x 2 X do
2 hole  in class[x][Old].first

360 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363
3 in sum  0; s  0
4 for each a in Small do
5 apos  in c idx of[x][a]
6 if apos ? 0 then
7 in sum  in sum + inv head[x][a].size
8 b  in c elts[x][hole]
9 c idx of[x][a]  hole
10 c idx of[x][b]  apos
11 in c elts[x][hole]  a
12 in c elts[x][apos]  b
13 hole  hole + 1; s  s + 1
14 in class[x][Large].size  in class[x][Old].size - s
15 in class[x][Small].size  s
16 in class[x][Small].first  in class[x][Old].first
17 in class[x][Large].first  hole
18 in class[x][Large].in size 

in class[x][Old].in size - in sum
19 in class[x][Small].in size  in sum

Algorithm 9 shows the implementation of this enhanced update method. It is mostly
the same as Algorithm 7, but the list move is replaced with the segment header of the
smaller class. The routine is invoked by augmenting REFINE to contain lines

if cls head[Bnew].size < = cls head[B].size then

SPLITINCOME(B, B, Bnew)
else SPLITINCOME(B, Bnew, B)

after line 3 (building Bnew). Notice that the order in which the information is updated
in the end of REFINEINCOME is important, since either Small or Large is the same as
Old.

Consider then |Built| from the viewpoint of sets x-1 " B. At each inner node DT`(B)
with |B| = n, we have built a set of size q 6 n=2, and the rest of the building work
is performed in the child nodes. Thus, we have again our familiar recurrence relation
f(n) = kq + f(q) + f(n - q) with the known upper bound k n log n.

Corollary 27. Let Built = {B1; : : : ; Bm} be the set of classes iterated over when building sets x-1 " B with Algorithm 8 and 9. Then |Built| = Pmi = 1 |Bi| 6 |A| log |A|.

5.4. A note on uniform partitioning

Number |x-1(C)| is the most accurate selection measure, since it is directly related
to the number of steps taken in the main loop of the minimization algorithm. This
accuracy enables us to study some computational properties in more detail.

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 361
Recall that in the solving of the upper bound for the recurrence relation for cg(n) we
had complexity O(n) for the case q = 1 and O(n log n) for q = n=2. This implies that
cost(B; x) (where Col(x)(B) = green) has the smaller value the less work is assigned
to the green descendant of B. We next show that this is indeed the case, i.e. cost(B; x)
grows monotonically with the fraction assigned to the green descendant.

Let us assume that each node B with |x-1(B)| = n of DT` is divided uniformly to partitions B0 and B00 such that |x-1(B0)| = ffn and |x-1(B00)| = (1-ff)n, where 0!ff 6 1=2 is
a constant. The function cg can now be rewritten as cg(n) = kffn+ cg(ffn)+ cg((1-ff)n):
We next show that the closed form for cg is

cg(n) = k0n - kffn log nff log ff + (1 - ff) log(1 - ff) ;
where k0 ? 0 is some constant (note that the latter term is actually the more expensive
one):

cg(ffn) + cg((1 - ff)n) = k0ffn - kff(ffn) log(ffn)ff log ff + (1 - ff) log(1 - ff)

= k0(1 - ff)n - kff(1 - ff)n log(1 - ff)nff log ff + (1 - ff) log(1 - ff)
= k0n - ` cff(ffn)(log ff + log n)ff log ff + (1 - ff) log(1 - ff)

+ kff(1 - ff)n(log(1 - ff) + log n)ff log ff + (1 - ff) log(1 - ff) '
= k0n - kffn - cffn log n)ff log ff + (1 - ff) log(1 - ff)
= cg(n) - kffn:
If we define cg as a function of ff (and keep n constant), we have that

cg0(ff) = - k n log nff log ff + (1 - ff) log(1 - ff) + kffn log n(log ff - log(1 - ff))(ff log ff + (1 - ff) log(1 - ff))2

= * * *
= - kn log n(ff log ff + (1 - ff) log(1 - ff))2 log(1 - ff):

Thus, cg0(ff) ? 0 for all 0!ff 6 1=2.

The monotonicity of cg(ff) can be exploited as a heuristics to select items from L:
we try to minimize ff (and thus cg(ff)) by always taking the candidate (B; x) with
minimal |x-1(B)|. This can be achieved by implementing L as a heap. Assuming that
X is not of greater magnitude than A the management of the heap does not increase
the asymptotic time complexity of the algorithm. This is because L may contain at
most |X ||A| items, and insertions to the heap are bounded by O(log |X | + log |A|).

362 T. Knuutila / Theoretical Computer Science 250 (2001) 333-363
6. Conclusion and future work

We have presented a tutorial reconstruction of Hopcroft's minimization algorithm. As
a result of this, the different variations of the algorithm could be placed in a common
framework. With the invention of the concept `derivation tree', we were able to give a
firm and understandable correctness proof for the algorithm. Derivation trees enabled
us also to make the computational analysis of the algorithm in a constructive way.
In the implementation of the algorithm, we presented a new approach to refine the
partition structures, and proposed a simple enhancement that is required to keep the
more informed variants within the O(|X ||A| log |A|) bound.

All variants are of the same time complexity, but it is important to know, if some
of them is generally more efficient in practice than others. Although the method based
on measure |B| makes less informed choices in the updates of L, it does not need
to update sets x-1 " B. Initial benchmarks on these variants suggest that the savings
gained from the wise updates of L are not enough to cover the penalty caused by the
set updates even with binary alphabets. We are going to test, what is the effect of implementing L as a heap and to run our algorithms against other known implementations
[6, 18, 22, 23, 14, 2].

In many real-life applications, the transition table of the DFA need not be defined
totally, since most of the transitions end up to a single `garbage state'. One point of
further work is thus to study, whether the minimization algorithm can be modified to
gracefully adapt to partially defined transition functions. Here the goal is to find an
algorithm with running time O(|ffi| log |A|), where |ffi| 6 |X ||A| is the number of defined
transitions. Although the more general approach presented in [16] is already capable
of achieving this bound, we expect our approach to be more efficient in practice.

Some of our techniques (in particular colored derivation trees and Algorithm 7) could
be usable in the analysis and implementations of other kinds of partition refinement
problems as graph refinement [5] and relational coarsest partition problem [16]. Finally,
we are working to extend the minimization algorithm to handle tree automata [8].

Acknowledgements

The author would like to thank Olli Nevalainen, Timo Raita and Magnus Steinby
for their support during the preparation of the article. The detailed comments and
suggestions of the referee greatly helped to improve the paper.

References

[1] A.V. Aho, J. Hopcroft, J.D. Ullman, The Design and Analysis of Computer Algorithms, Prentice-Hall,

Englewood Cliffs, NJ, 1974.
[2] N. Blum, An O(n log n) implementation of the standard method for minimizing n-state finite automata,

Inform. Process. Lett. 57(2) (1996) 65-69.
[3] W. Brauer, Automatentheorie, B. G. Teubner, Stuttgart, 1984.

T. Knuutila / Theoretical Computer Science 250 (2001) 333-363 363
[4] J.R. B"uchi, in: D. Siefkes (Ed.), Finite Automata, Their Algebras and Grammars, Springer, New York,

1989.
[5] A. Cardon, M. Crochemore, Partitioning a graph in O(|A| log |V |); Theoret. Comput. Sci. 19 (1982)

85-98.
[6] J.-M. Champarnaud, G. Hansel, Automate, a computing package for automata and finite semigroups,

J. Symbolic Comput. 12 (1991) 197-220.
[7] T. Cormen, C. Leiserson, R. Rivest, Introduction to Algorithms, MIT Press, Cambridge, MA, 1990.
[8] F. G'ecseg, M. Steinby, Tree Automata, Akad'emiai Kiad'o, Budapest, 1984.
[9] D. Gries, Describing an algorithm by Hopcroft, Acta Inform. 2 (1973) 97-109.
[10] C.A.R. Hoare, Quicksort, Comput. J. 5(1) (1962) 10-15.
[11] J. Hopcroft, An n log n algorithm for minimizing states in a finite automaton, Technical Report CS-190,

Stanford University, 1970.
[12] J. Hopcroft, An n log n algorithm for minimizing states in a finite automaton, in: Z. Kohavi, A. Paz

(Eds.), Proc. Internat. Symp. on the Theory of Machines and Computations, Haifa, Israel, Academic
Press, New York, 1971, pp. 189-196.
[13] J. Hopcroft, J.D. Ullman, Introduction to Automata Theory, Languages and Computation, AddisonWesley, Reading, MA, 1979.
[14] O. Matz, A. Miller, A. Potthoff, W. Thomas, E. Valkema, Report on the program AMoRE, Technical

Report 9507, Inst. f. Informatik u. Prakt. Math., CAU Kiel, 1995.
[15] B. Mikolajczak (Ed.), Algebraic and Structural Automata Theory, Annals of Discrete Mathematics,

vol. 44, North-Holland, Amsterdam, 1991.
[16] R. Paige, R.E. Tarjan, Three partition refinement algorithms, SIAM J. Comput. 16(6) (1987) 973-989.
[17] R. Paige, R.E. Tarjan, R. Bonic, A linear time solution for the single function coarsest partition problem,

Theoret. Comput. Sci. 40(1) (1984) 67-84.
[18] D. Raymond, D. Wood, Grail: A C++ library for automata and expressions, J. Symbolic Comput.

17(4) (1994) 341-350.
[19] D. Revuz, Minimization of acylic deterministic automata in linear time, Theoret. Comput. Sci. 92 (1990)

181-189.
[20] A. Salomaa, Theory of Automata, Pergamon Press, Oxford, 1969.
[21] R. Sedgewick, Implementing quicksort programs, Commun. ACM 21(10) (1978) 847-857.
[22] K. Sutner, Implementing finite state machines, in: N. Dean, G. Shannon (Eds.), Computational Support

for Discrete Mathematics, DIMACS Series in Discrete Mathematics and Theoretical Computer Science,
vol. 15, American Mathematical Society, Providence, RI, 1994, pp. 347-364.
[23] B. Watson, The design and implementation of the FIRE engine, Technical Report Computer Science

Note 94=22, Eindhoven University of Technology, Faculty of Mathematics and Computing Science,
1994.
[24] B. Watson, A taxonomy of finite automata minimization algorithms, Technical Report Computer Science

Note 93=44, Eindhoven University of Technology, Faculty of Mathematics and Computing Science,
1994.