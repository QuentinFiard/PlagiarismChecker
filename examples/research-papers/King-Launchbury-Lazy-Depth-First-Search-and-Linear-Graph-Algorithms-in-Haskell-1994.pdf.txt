

Lazy Depth-First Search and Linear Graph Algorithms in HaskellDavid J. King John LaunchburyDepartment of Computing ScienceUniversity of Glasgow-gnik, jl""@dcs.glasgow.ac.ukAbstractDepth-first search is the key to a wide variety of graph al-gorithms. In this paper we explore the implementation ofdepth first search in a lazy functional language. For the firsttime in such languages we obtain a linear-time implemen-tation. But we go further. Unlike traditional imperativepresentations, algorithms are constructed from individualcomponents, which may be reused to create new algorithms.Furthermore, the style of program is quite amenable to for-mal proof, which we exemplify through a calculational-style
proof of a strongly-connected components algorithm.1 IntroductionGraph algorithms have long been a challenge to program-mers of lazy functional languages. It has not been at allclear how to express such algorithms without using side ef-fects to achieve efficiency. For example, many texts provideimplementations of search algorithms which are quadraticin the size of the graph (see Paulson (1991), Holyer (1991),or Harrison (1993)), compared with the standard linear im-plementations given for imperative languages (see Manber(1989), or citeasnounbig-white-book). In this paper we im-plement a variety of algorithms based on depth-first search(DFS), obtaining linear time efficiency for them all.The importance of depth-first search for graph algorithmswas established twenty years ago by Tarjan (1972) andHopcroft and Tarjan (1973) in their seminal work (our title
is an extension of Tarjan's original). They demonstratedhow depth-first search could be used as a skeleton on whichto build efficient graph algorithms. The particular code-fragments relevant to a particular algorithm are embeddedinto the DFS procedure in order to compute relevant in-formation while the search proceeds. While this is quiteelegant it has a number of drawbacks. Firstly, the DFScode becomes intertwined with the code for the particularalgorithm, resulting in opaque programs. Secondly, reason-ing about such algorithms is dynamic--it is a process underdiscussion rather than a value--and such reasoning is com-plex.Occasionally, reference is made to the depth-first forest to

provide a static intermediate value for reasoning. We buildon this latter idea. If having an explicit depth-first forestis good for reasoning then, so long as the overheads are notunacceptable, it is good for programming. In particular, wepresent a wide variety of DFS algorithms as combinations ofstandard components, passing explicit intermediate valuesfrom one to the other. In doing so we gain a far greaterdegree of modularity than is usually found in implementa-tions of these algorithms, while still retaining the standardcomplexity measure.There is one place where we do need to use destructive up-date in order to gain the same complexity (within a con-stant factor) of imperative graph algorithms (assuming anunderlying RAM machine). We make use of recent advancesin lazy functional languages which provide updatable state,as implemented within the Glasgow Haskell compiler. Thecompiler provides extensions to language Haskell (Hudaket al. 1992) providing updatable arrays, and allows thesestate-based actions to be encapsulated so that their externalbehaviour is purely functional. Consequently we obtain linear algorithms and yet retain the ability to perform purelyfunctional reasoning on all but one reusable component.In summary the main contributions of this paper are:ffl We provide implementations of DFS algorithms in lin-ear time in Haskell. As this has not been done before ina lazy functional language we are careful to provide realcode throughout, and avoid resorting to pseudo-code.ffl We construct the algorithms using reusable compo-nents, providing a greater level of modularity than istypical in other presentations.ffl We provide some examples of correctness proofs. Atyp-ically, these are not proofs which reason about the dy-namic process of DFS, but rather about a static value.This paper is organised as follows. Section 2 introduces adata type for graphs and some standard functions whichwill be used in subsequent algorithms. Section 3 intro-duces depth-first search. Section 4 describes the Haskell
implementation of several algorithms that use depth-firstsearch which includes: topological sorting, strongly con-nected components, as well as others. Section 5 describesthe linear implementation of depth-first search in Haskell.1

Section 6 describes some more complex algorithms that usedepth-first search, including edge classification and bicon-nected components. Section 7 discusses the complexity ofthe algorithms. Finally, Section 8 discusses related work.2 Representing graphsThere are many ways to represent (directed) graphs. Forour purposes, we use an array of adjacency lists. The arrayis indexed by vertices, and each component of the array isa list of those vertices reachable along a single edge. Thisadjacency structure is linear in the size of the graph, thatis, the sum of the number of vertices and the number ofedges. By using an indexed structure we are able to beexplicit about the sharing that occurs in the graph. Anotheralternative would have been to use a recursive tree structureand rely on cycles within the heap. However, the sharing ofnodes in the graph would then be implicit making a number
of tasks harder.So we will just use a standard Haskell immutable array. Thisgives constant time access (but not update--these arraysmay be shared arbitrarily).We can use the same mechanism to represent undirectedgraphs as well, simply by ensuring that we have edges inboth directions. An undirected graph is a symmetric di-rected graph. We could also represent multi-edged graphsby a simple extension, but will not consider them here.Graphs, therefore, may be thought of as a table indexed byvertices.type Table a = Array Vertex atype Graph = Table [Vertex]The type Vertex may be any type belonging to the Haskellindex class Ix, which includes Int, Char, tuples of in-dices, and more. Haskell arrays come with indexing (!) andthe functions indices (returning a list of the indices) andbounds (returning a pair of the least and greatest indices).
We provide vertices as an alternative for indices, whichreturns a list of all the vertices in a graph.vertices :: Graph -? [Vertex]vertices = indicesSometimes it is convenient to extract a list of edges fromthe graph, this is done with the function edges. An edge isa pair of vertices.type Edge = (Vertex,Vertex)edges :: Graph -? [Edge]edges g = [ (v,w) -- v !- vertices g, w !- g!v]To manipulate tables (including graphs) we provide ageneric function mapT which applies its function argumentto every table index/entry pair, and builds a new table.

mapT :: (Vertex -? a -? b) -? Table a -? Table bmapT f t = array (bounds t)[(v, f v (g!v)) -- v!-indices t]The Haskell function array takes low and high bounds anda list of index/value pairs1, and builds the correspondingarray in linear time. Because we are using an array-basedimplementation we often need to provide a pair of verticesas array bounds. So for convenience we define,type Bounds = (Vertex,Vertex)Using mapT we could define,outdegree :: Graph -? Table Intoutdegree g = mapT numEdges gwhere numEdges v ws = length wswhich builds a table detailing the number of edges leavingeach vertex.To build up a graph from a list of edges we define buildG.buildG :: Bounds -? [Edge] -? Graph
buildG bnds es = accumArray (flip (:)) [] bnds esLike array the Haskell function accumArray builds an ar-ray from a list if index/value pairs, with the difference thataccumArray accepts possibly many values for each indexedlocation, which are combined using the function provided asaccumArray's first argument. Here we simply build lists ofall the values associated with each index. Again, construct-ing the array takes linear time with respect to the length ofthe adjacency list. So in linear time, we can convert a graphdefined in terms of edges to the vertex table based graph.For example,graph = buildG ('a','j')[('a','b'), ('a','f'), ('b','c'),('b','e'), ('c','a'), ('c','d'),('e','d'), ('g','h'), ('g','j'),('h','f'), ('h','i'), ('h','j')]will produce the array representation for the graph shownin Figure 1.

d ec ba gf h jiFigure 1: A directed graphThen, to find the immediate successors to 'h', say, we com-1Actually Haskell uses the Assoc type, which is equivalent, butintroduces an unnecessary new notation2

pute: graph ! 'h'which returns ['f', 'i', 'j'].Combining the functions edges and buildG gives us a wayto reverse all the edges in a graph giving the transpose ofthe graph:transposeG :: Graph -? GraphtransposeG g = buildG (bounds g) (reverseE g)reverseE :: Graph -? [Edge]reverseE g = [ (w,v) -- (v,w) !- edges g]We extract the edges from the original graph, reverse theirdirection, and rebuild a graph with the new edges. Then,for example,(transposeG graph) ! 'h'will return ['g']. Now by using transposeG we can imme-diately define an indegree table for vertices:
indegree :: Graph -? Table Intindegree g = outdegree (transposeG g)3 Depth-first searchDepth-first search is often viewed as a process which mayloosely be described as follows. Initially, all the vertices ofthe graph are deemed "unvisited", so we choose one and ex-plore an edge leading to a new vertex. Now we start at thisvertex and explore an edge leading to another new vertex.We continue in this fashion until we reach a vertex whichhas no edges leading to unvisited vertices. At this point webacktrack, and continue from the latest vertex which doeslead to new unvisited vertices.Eventually we will reach a point where every vertex reach-able from the initial vertex has been visited. If there are anyunvisited vertices left, we choose one and begin the searchagain, until finally every vertex has been visited once, and
every edge has been examined.d c b ea f h ji gBC C FFigure 2: A depth-first forest of the graphIn this paper we will concentrate on depth first search as aspecification for a value, namely the spanning forest definedby a depth-first traversal of a graph. Such a forest for the

graph in Figure 1 is depicted in Figure 2. The (solid) treeedges are those graph edges which lead to unvisited vertices.The remaining graph edges are also shown, but in dashedlines. These edges are classified according to their relation-ship with the tree, namely, forward edges (which connect an-cestors in the tree to descendants), back edges (the reverse),and cross edges (which connect nodes across the forest, butalways from right to left). This standard classification isuseful for thinking about a number of algorithms and later,in Section 6, we give an algorithm for classifying edges inthis way.3.1 Specification of depth-first searchAs the approach to DFS algorithms which we explore inthis paper is to manipulate the depth-first forest explicitly,the first step, therefore, is to construct the depth-first forestfrom a graph. To do this we need an appropriate definitionof trees and forests.
A forest is a list of trees, and a tree is a node containingsome value, together with a forest of sub-trees. Both treesand forests are polymorphic in the type of data they maycontain.data Tree a = Node a (Forest a)type Forest a = [Tree a]A depth-first search of a graph takes a graph and an initialordering of vertices. All graph vertices in the initial orderingwill be in the returned forest.dfs :: Graph -? [Vertex] -? Forest VertexThis function is the pivot of this paper. For now we re-strict ourselves to considering its properties, and will leaveits Haskell implementation until Section 5.Sometimes the initial ordering of vertices is not important.When this is the case we use the related functiondff :: Graph -? Forest Vertexdff g = dfs g (vertices g)
What are the properties of depth-first forests? They can becompletely characterised by the following two conditions.(i) The depth-first forest of a graph is a spanning sub-graph, that is, it has the same vertex set, but the edgeset is a subset of the graph edge set.(ii) The graph contains no left-right cross-edges with re-spect to the forest.Later on in the paper, we find it convenient to talk in termsof paths rather than single edges: a path being made upof zero or more edges joined end to end. We will writev \Gamma !g w to mean that there is a path from v to w in thegraph g. Where there will be no confusion we will drop thegraph subscript.3

The ban on left-right cross edges translates into paths. Atthe top level, it implies that there is no path from any vertexin one tree to any vertex in a tree that occurs later in theforest. Thus2,Property 1If (ts++us=dff g), then 8v 2 ts : 8w 2 us : v =\Gamma ! w 2Deeper within each tree of the forest, there can be pathswhich traverse a tree from left to right, but the absence ofany graph edges which cross the tree structure from left toright implies that the path has to follow the tree structure.That is:Property 2If the tree (Node x (ts++us)) is a subtree occurring any-where within dff g, then8v 2 ts : 8w 2 us : v \Gamma ! w ) v \Gamma ! x 2So the only way to get from v to w is via (an ancestor of)
x , the point at which the forests that contain v and w arecombined (otherwise there would be a left-right cross edge).Thus there is also a path from v to x .The last property we pick out focusses on dfs, and providesa relationship between the initial order, and the structureof the forest3.Property 3Let a and b be any two vertices. Write \Gamma ! for edges inthe graph g, and ^ for the ordering induced by the list ofvertices vs. Then9t 2 dfs g vs : a 2 t ^ b 2 t, 9c : c \Gamma ! a ^ c \Gamma ! b ^(8d : d \Gamma ! a . d \Gamma ! b ) c ^ d) 2This Property says that:) given two vertices which occur within a single depth-first tree (taken from the forest), then there is a pre-decessor of both (with respect to \Gamma !) which occurs

earlier in vs than any other predecessor of either. (Ifthis were not the case, then a and b would end up indifferent trees).( if the earliest predecessor of either a or b is a prede-cessor of them both, then they will end up in the sametree (rooted by this predecessor).These three properties are certainly true of DFS spanningforests, but we make no claim about their completeness.There are other useful properties not derivable from these.2We use the notation ts++us to indicate any division of the list oftrees in the forest, such that the order of the trees is preserved. Notethat either ts or us could be empty. Also, we use 2 to indicate listmembership and not purely for set membership.3We further overload the 2 notation, to mean that both a and boccur within the tree t.

4 Depth-first search algorithmsAlgorithm 1. Depth-first search numberingHaving specified DFS (at least partly) we turn to considerhow it may be used. The first algorithm is straightforward.We wish to assign to each vertex a number which indicateswhere that vertex came in the search. A number of otheralgorithms make use of this depth-first search number, in-cluding the biconnected components algorithm that appearslater, for example.We can express depth-first ordering of a graph g most simplyby flattening the depth-first forest in preorder. Preorder ontrees and forests places ancestors before descendants andleft subtrees before right subtrees4:preorder :: Tree a -? [a]preorder (Node a ts) = [a] ++ preorderF tspreorderF :: Forest a -? [a]
preorderF ts = concat (map preorder ts)Now obtaining a list of vertices in depth-first order is easy:preOrd :: Graph -? [Vertex]preOrd g = preorderF (dff g)However, it is often convenient to translate such an orderedlist into actual numbers. For this we could use the functiontabulate:tabulate :: Bounds -? [Vertex] -? Table Inttabulate bnds vs = array bnds (zip vs [1..])which zips the vertices together with the positive integers1 ; 2 ; 3 ; : : :, and (in linear time) builds an array of thesenumbers, indexed by the vertices.We can package these up into a function as follows:preArr :: Bounds -? Forest Vertex -? Table IntpreArr bnds ts = tabulate bnds (preorderF ts)(it turns out to be convenient for later algorithms if suchfunctions take the depth-first forest as an argument, rather
than construct the forest themselves.)Algorithm 2. Topological sortingThe converse to preorder is postorder, and unsurprisinglythis turns out to be useful in its own right. Postorder placesdescendants before ancestors and left subtrees before rightsubtrees:postorder :: Tree a -? [a]postorder (Node a ts) = postorderF ts ++ [a]4The use of repeated appends (++) introduces an extra logarithmicfactor here, but this is easily removed using standard transformations.4

postorderF :: Forest a -? [a]postorderF ts = concat (map postorder ts)So, like with preorder, we define,postOrd :: Graph -? [Vertex]postOrd g = postorderF (dff g)The lack of left-right cross edges in DFS forests leads toa pleasant property when a DFS forest is flattened in pos-torder. To express this we need a definition.DefinitionA linear ordering ^ on vertices is a post-ordering with re-spect to a graph g exactly when,v ^ w ^ v \Gamma ! w ) 9u : v ! u ^ w ^ u 2(where v ! u means v \Gamma ! u and u \Gamma ! v ). In words, thisdefinition states that, if from some vertex v there is a path
to a vertex later in the ordering, then there is also a vertexu which occurs no earlier than w and which, like w is alsoreachable by a path from v . In addition, however, there isalso a path from u to v .This property is so-named because post order flattening ofdepth first forests have this property.Theorem 4If vs=postOrd g, then the order in which the vertices ap-pear in vs is a post-ordering with respect to g.ProofIf v comes before w in a post order flattening of a forest,then either w is an ancestor of v , or w is to the right of vin the forest. In the first case, take w as u. For the second,note that as v \Gamma ! w, by Property 1, v and w cannot be indifferent trees of the forest. Then by Property 2, the lowestcommon ancestor of v and w will do. 2
We can apply all this to topological sorting. A topologicalsort is an arrangement of the vertices of a directed acyclicgraph into a linear sequence v1 ; : : : ; vn such that there areno edges from vj to vi where i ! j . This problem arisesquite frequently, where a set of tasks need to be scheduled,such that every task can only be performed after the tasksit depends on are performed.We define,topSort :: Graph -? [Vertex]topSort g = reverse (postOrd g)Why is this is correct? If w comes before v in the re-sult of topSort g, then v comes before w in the result ofpostOrd g. Thus, by Theorem 4, there exists a vertex u noearlier than w which is in a cycle with v. But, by assump-tion, the graph is acyclic, so no such edge v \Gamma ! exists.

Algorithm 3. Connected componentsTwo vertices in an undirected graph are connected if there isa path from the one to the other. In a directed graph, twovertices are connected if they would be connected in thegraph made by viewing each edge as undirected. Finally,with an undirected graph, each tree in the depth-first span-ning forest will contain exactly those vertices which consti-tute a single component.We can translate this directly into a program. The functioncomponents takes a graph and produces a forest, where eachtree represents a connected component.components :: Graph -? Forest Vertexcomponents g = dff (undirected g)where a graph is made undirected by:undirected :: Graph -? Graphundirected g = buildG (bounds g)(edges g ++ reverseE g)
The undirected graph we actually search may have dupli-cate edges, but this has no effect on the structure of thecomponents.Algorithm 4. Strongly connected compo-nentsTwo vertices in a directed graph are said to be strongly con-nected if each is reachable from the other. A strongly con-nected component is a maximal subgraph, where all the ver-tices are strongly connected with each other. This problemis well known to compiler writers as the dependency anal-ysis problem--separating procedures/functions into mutu-ally recursive groups. We implement the double depth-first search algorithm of Kosaraju (unpublished), and Sharir(1981).scc :: Graph -? Forest Vertexscc g = dfs (transposeG g) (reverse (postOrd g))The vertices of a graph are ordered using postOrd. The
reverse of this ordering is used as the initial vertex orderfor a depth-first traversal on the transpose of the graph.The result is a forest, where each tree constitutes a singlestrongly connected component.The algorithm is simply stated, but its correctness is not atall obvious. However, it may be proved as follows.Theorem 5Let a and b be any two vertices of g. Then(9t 2 scc g : a 2 t ^ b 2 t) , a ! bProofThe proof proceeds by calculation. We write gT for thetranspose of g. Edges \Gamma ! in g will be edges \Gamma  in gT.Further, let ^ be the post-ordering defined by postOrd g.5

Then its reversal induces the ordering *. Now,9t 2 scc g : a 2 t ^ b 2 t, f Definition of scc g9t 2 dfs gT (reverse (postOrd g)) : a; b 2 t, f By Property 3 g9c : c \Gamma  a ^ c \Gamma  b ^(8d : d \Gamma  a . d \Gamma  b ) c * d), 9c : a \Gamma ! c ^ b \Gamma ! c ^(8d : a \Gamma ! d . b \Gamma ! d ) d ^ c)From here on we construct a loop of implications., 9c : a \Gamma ! c ^ b \Gamma ! c ^(8d : a \Gamma ! d . b \Gamma ! d ) d ^ c)) f Take d = a and d = b g9c : a \Gamma ! c ^ a ^ c ^ b \Gamma ! c ^ b ^ c ^(8d : a \Gamma ! d . b \Gamma ! d ) d ^ c)) f ^ is a post-ordering g9c : (9e : a ! e ^ c ^ e) ^(9f : b ! f ^ c ^ f ) ^

(8d : a \Gamma ! d . b \Gamma ! d ) d ^ c)) f e = c and f = c using (8d : : :) g9c : a ! c ^ b ! c) f Transitivity ga ! bwhich gives us one direction. But to complete the loop:) f There is a latest vertex reachable from a or b ga ! b ^ 9c : (a \Gamma ! c . b \Gamma ! c) ^(8d : a \Gamma ! d . b \Gamma ! d ) d ^ c)) f Transitivity of \Gamma ! g9c : a \Gamma ! c ^ b \Gamma ! c ^(8d : a \Gamma ! d . b \Gamma ! d ) d ^ c)as required, and so the theorem is proved. 2To the best of our knowledge, this is the first calculationalproof of this algorithm. Traditional proofs (see Corman etal. (1990), for example) typically take many pages of wordyargument. In contrast, because we are reusing an earlier
algorithm, we are able to reuse its properties also, and soobtain a compact proof. Similarly, we believe that it isbecause we are using the DFS forest as the basis of ourprogram that our proofs are simplified as they are proofsabout values rather than about processes.A minor variation on this algorithm is to reverse the rolesof the original and transposed graphs:scc' :: Graph -? Forest Vertexscc' g = dfs g (reverse (postOrd (transposeG g)))The advantage now is that not only does the result expressthe strongly connected components, but it is also a validdepth-first forest for the original graph (rather than for thereversed graph). This alternative works as the strongly con-nected components in a graph are the same as the stronglyconnected components in the transpose of the graph.

5 Implementing depth-first searchIn order to translate a graph into a depth-first spanningtree we make use of a technique common in lazy functionalprogramming: generate then prune. Given a graph and alist of vertices (a root set), we first generate a (potentiallyinfinite) forest consisting of all the vertices and edges inthe graph, and then prune this forest in order to removerepeats. The choice of pruning pattern determines whetherthe forest ends up being depth-first (traverse in a left-most,top-most fashion) or breadth-first (top-most, left-most), orperhaps some combination of the two.5.1 GeneratingWe define a function generate which, given a graph g and avertex v builds a tree rooted at v containing all the verticesin g reachable from v.generate :: Graph -? Vertex -? Tree Vertex
generate g v = Node v (map (generate g) (g!v))Unless g happens to be a tree anyway, the generated treewill contain repeated subtrees. Further, if g is cyclic, thegenerated tree will be infinite (though rational).Of course, as the tree is generated on demand, only a finiteportion will be generated. The parts that prune discardswill never be constructed.5.2 PruningThe goal of pruning the (infinite) forest is to discard sub-trees whose roots have occurred previously. Thus we needto maintain a set of vertices (traditionally called "marks")of those vertices to be discarded. The set-operations werequire are initialisation (the empty set), membership test,and addition of a singleton. While we are prepared to spendlinear time in generating the empty set (as it is only doneonce), it is essential that the other operations may be per-formed in constant time.
The easiest way to achieve this is to make use of statetransformers, and mimic the imperative technique of main-taining an array of booleans, indexed by the set elements.This is what we do. We provide an explanation of state-transformers in the Appendix, but as they have alreadybeen described in a number of papers (Moggi 1989, Wadler1990, Peyton Jones and Wadler 1993, Launchbury 1993),and already been implemented in more than one Haskellvariant, we avoid cluttering the main text.The implementation of vertex sets is easy:type Set s = MutArr s Vertex BoolmkEmpty :: Bounds -? ST s (Set s)mkEmpty bnds = newArr bnds False6

contains :: Set s -? Vertex -? ST s Boolcontains m v = readArr m vinclude :: Set s -? Vertex -? ST s ()include m v = writeArr marks v TrueUsing these, we define prune as follows.prune :: Bounds -? Forest Vertex -? Forest Vertexprune bnds ts= runST (mkEmpty bnds `thenST` "m -?chop m ts)The prune function begins by introducing a fresh statethread, then generates an empty set within that thread andcalls chop. The final result of prune is the value generatedby chop, the final state being discarded.chop :: Set s -? Forest Vertex-? ST s (Forest Vertex)chop m [] = returnST []chop m (Node v ts : us)= contains m v `thenST` "visited -?

if visited thenchop m uselseinclude m v `thenST` ". -?chop m ts `thenST` "as -?chop m us `thenST` "bs -?returnST ((Node v as) : bs)When chopping a list of trees, the root of the first is exam-ined. If it has occurred before, the whole tree is discarded.If not, the vertex is added to the set represented by m, andtwo further calls to chop are made in sequence.The first, namely, chop m ts, prunes the forest of descen-dants of v, adding all these to the set of marked vertices.Once this is complete, the pruned subforest is named as,and the remainder of the original forest is chopped. Theresult of this is, in turn, named bs, and the resulting forestis constructed from the two.All this is done lazily, on demand. The state combinatorsforce the computation to follow a predetermined linear sequence, but exactly where in that sequence the computa-tion is, is determined by external demand. Thus if only thetop-most left-most vertex were demanded then that is allthat would be produced. On the other hand, if only thefinal tree of the forest is demanded, then because the setof marks is single-threaded, all the previous trees will beproduced. However, this is demanded by the very natureof DFS anyway, so it is not as restrictive as it may at firstseem.At this point one may wonder whether any benefit has beengained by using a functional language. After all, the codelooks fairly imperative. To some extent such a commentwould be justified, but it is important to note that this is theonly place in the development that destructive operations

have to be used to gain efficiency. We have the flexibilityto gain the best of both worlds: where destructive updateis vital we use it, where it is not vital we use the powerfulmodularity options provided by lazy functional languages.5.3 DFSThe components of generation an pruning are combined toprovide the definition of DFS.dfs g vs = prune (bounds g) (map (generate g) vs)The argument vs is a list of vertices, so the generate func-tion is mapped across this (having been given the graphg). The resulting forest is pruned in a left-most top-mostfashion by prune.If paying an extra logarithmic factor is acceptable, then it ispossible to dispense completely with the imperative featuresused in prune, and to use an implementation of sets basedupon balanced trees, for example.
6 More algorithmsAlgorithm 5. Classifying edgesWe have already seen the value of classifying the graph edgeswith respect to a given depth-first search. Here we codifythe idea by building subgraphs of the original containing allthe same vertices, but only a particular kind of edge.Tree edges are easiest, these are just the edges that ap-pear explicitly in the spanning forest. The other edges maybe distinguished by comparing preorder and/or postordernumbers of the vertices of an edge. We can summarise thesituation in the following diagram:preorder: ......v........................w......TFBC

postorder: ......v........................w......BTFCOnly back edges go from lower postorder numbers to higher,whereas only cross edges go from higher to lower in bothorderings. Forward edges, which are the composition oftree edges, cannot be distinguished from tree edges by thismeans--both tree edges and forward edges go from lowerpreorder numbers to higher (and conversely in postorder)--but as we can already determine which are tree edges thereis no problem. The implementation of these principles is7

tree :: Bounds -? Forest Vertex -? Graphtree bnds forest = buildG bnds (concat (map flat ts))whereflat (Node v ts) = [ (v,w) -- Node w us !- ts] ++ concat (map flat ts)back :: Graph -? Table Int -? Graphback g post = mapT select gwhere select v ws = [ w -- w !- ws, post!v!post!w ]cross :: Graph -? Table Int -? Table Int -? Graphcross g pre post = mapT select gwhere select v ws = [ w -- w !- ws, post!v?post!w, pre!v?pre!w]forward :: Graph -? Graph -? Table Int -? Graphforward g tree pre = mapT select gwhere select v ws = [ w -- w !- ws, pre!v!pre!w] "" tree!vFigure 3: Classification of graph edgesnow immediate and presented in Figure 3.
To classify an edge we generate the depth-first spanning for-est, and use this to produce preorder and postorder num-bers. We then have all the information required to constructthe appropriate subgraph.Algorithm 6. Finding reachable verticesFinding all the vertices that are reachable from a singlevertex v demonstrates that the dfs doesn't have to take allthe vertices as its second argument. Commencing a searchat v will construct a tree containing all of v's reachablevertices. We then flatten this with preorder to produce thedesired list.reachable :: Graph -? Vertex -? [Vertex]reachable g v = preorderF (dfs g [v])One application of this algorithm is to test for the existenceof a path between two vertices:path :: Graph -? Vertex -? Vertex -? Bool
path g v w = w `elem` (reachable g v)The elem test is lazy: it returns True as soon as a matchis found. Thus the result of reachable is demanded lazily,and so only produced lazily. As soon as the required ver-tex is found the generation of the DFS forest ceases. Thusdfs implements a true search and not merely a completetraversal.Algorithm 7. Biconnected componentsWe end by programming a more complex algorithm--finding biconnected components. An undirected graph is bi-connected if the removal of any vertex leaves the remaining

subgraph connected. This has a bearing in the problem ofreliability in communication networks. For example, if youwant to avoid driving through a particular town, is there analternative route?c b\Lambda  a\Lambda d e\Lambda  h if gFigure 4: An undirected graphIf a graph is not biconnected the vertices whose removal dis-connects the graph are known as articulation points. Locat-ing articulation points allows a graph to be partitioned into
biconnected components (actually a partition of the edges).In Figure 4 vertices that are articulation points are markedwith an asterisk. The naive, brute force method requiresO(V (V + E )) time (where the problem graph has V ver-tices and E edges). A more efficient algorithm is describedby Tarjan (1972), where biconnected components are foundduring the course of a depth-first search in O(V + E ) time.Here we apply the same theory as Tarjan, but express it viaexplicit intermediate values.Tarjan's method is based on the following theorem:Theorem 6Given a depth-first spanning forest of a graph, v is an artic-ulation point in the graph if and only if: (i) v is a root withmore than one child; or (ii) v is not a root, and for all de-scendants w of v there are no edges to any proper ancestors8

of v.We apply this theorem by associating a low point numberwith every vertex. The low point number of v is the smallestDFS numbered vertex that can be reached by following zeroor more tree edges, and then along a single graph edge.We calculate low point numbers by traversing the DFS treesbottom-up, and associating each vertex with its low pointnumber. The function label (see Figure 6) annotates a treewith both depth-first numbers and low-point numbers. Atany vertex, the low point number is the minimum of:1. the DFS number of the vertex;2. the DFS numbers of the vertices reached by a singleedge; and3. the low point numbers of the vertex's descendants inthe tree.For example, the result of running label on the DFS spanning tree produced from the graph in Figure 4, gives theannotated tree depicted in Figure 5.d(4;2) c(3;2) g(7;5)b(2;1) f(6;5)a(1;1) e(5;1) h(8;1) i(9;1)Figure 5: The depth-first forest for the undirected graphDashed lines are the important back edges used for cal-culating low points. Tree nodes are triples, for instance,e(5;1), represents the triple (e; 5 ; 1 ), where 5 is the depth-first number and 1 the low point number of vertex e.Now that we have low points for vertices we can calculatearticulation points. By part (ii) of Theorem 6 if the depth-first number of v is less than or equal to the low point of w
then v is an articulation point.The function collect coalesces each DFS tree into a bi-connected tree, that is, a tree where the node elements arebiconnected components. At each node the DFS number iscompared with the low-point number of all the children. Ifthe child's low-point number is strictly less than the node'sDFS number, then the component involving that vertex isnot completed. On the other hand, if the node's DFS num-ber is less than or equal to the child's low-point number,then that component is completed once the node is included.The function bicomps handles the special case of the root.Finally, bcc ties all the other functions together.Coalescing the tree from Figure 5 will produce the followingforest containing two trees.

[a; b][b; d; c] [a; e; h; i][e; f; g]Figure 7: The biconnected treesWhile this algorithm is complex, again it is made up ofindividual components whose correctness may (potentiallyat least) be established independently of the other compo-nents. This is quite unlike typical imperative presentationswhere the bones of the recursive DFS procedure are filledout with the other components of the algorithm, resultingin a single monolithic procedure.7 Analysis of depth-first search7.1 Complexity
Models for complexity analysis of imperative languages havebeen established for many years, and verified with respect toreality across many implementations. Using these models itis possible to show that traditional implementations of thevarious DFS algorithms are linear in the size of the graph(that is, run in O(V + E ) time).Corresponding models for lazy functional languages havenot been developed to the same level, and where they havebeen developed there has not yet been the same extensiveverification. Using these models, (see for example Sands(1993)) we believe our implementation of the DFS algo-rithms to be linear, but because these models have not beenfully tested, we also ran empirical tests ourselves.

500 1000 1500

2000 2500 3000

3500 4000 4500

5000 0

5001000

15002000

25003000

35004000

45005000

5
10
15
20
25
30

Vertices

Edges

SecondsFigure 8: Measurements taken on the strongly connectedcomponents algorithmWe took measurements on the strongly connected compo-nents algorithm, which uses two depth-first searches. Theresults of our experiment are in Figure 8. Timings weretaken on randomly generated graphs (with differing num-bers of vertices and edges) and are accurate to approximately 1%.9

bcc :: Graph -? Forest [Vertex]bcc g = (concat . map bicomps . map (label g dnum)) forestwhere forest = dff gdnum = preArr (bounds g) forestlabel :: Graph -? Table Int -? Tree Vertex -? Tree (Vertex,Int,Int)label g dnum (Node v ts) = Node (v,dnum!v,lv) uswhere us = map (label g dnum) tslv = minimum ([dnum!v]++[ dnum!w -- w !- g!v]++[ lu -- Node (u,dw,lu) xs !- us])bicomps :: Tree (Vertex,Int,Int) -? Forest [Vertex]bicomps (Node (v,dv,lv) ts)= [ Node (v:vs) us -- (l, Node vs us) !- map collect ts]collect :: Tree (Vertex,Int,Int) -? (Int, Tree [Vertex])collect (Node (v,dv,lv) ts) = (lv, Node (v:vs) cs)where collected = map collect tsvs = concat [ ws -- (lw, Node ws us) !- collected, lw!dv]cs = concat [ if lw!dv then us else [Node (v:ws) us]-- (lw, Node ws us) !- collected]

Figure 6: Biconnected components algorithmThe results are quite clear. The plotted points clearly all lieon a plane, indicating the linearity of the algorithm.We were also curious as to the constant factor that we arepaying over an imperative language. We coded up Tar-jan's biconnected components algorithm in C, and com-pared with our Haskell implementation. For the graphs wetested Haskell was between 10 and 20 times slower than C.This was better than we expected as the Haskell implemen-tation is multi-pass whereas the C implementation was themonolithic single-pass algorithm.8 Related workKashiwagi and Wise (1991) also express their graph algo-rithms in Haskell. They express a graph problem in terms of
a set of recursive equations, and the algorithm is the fixedpoint of these equations. The graphs are represented bylists, so the algorithms have poor complexity, but are suit-able for parallel evaluation. Unfortunately, many of theiralgorithm implementations are long and unreadable, givinglittle insight into the structure of the problem. For example,their strongly connected components algorithm is a page ofintricate Haskell.Barth et al. (1991) describe M-structures in the parallelfunctional language Id which are well suited for state basedcomputation. For instance, an M-structure array can beused for holding marks to express whether a vertex has beenvisited before or not during a traversal. The strength ofM-structures is that they are designed to support parallel evaluation: their drawback is that referential transparencyis lost. With regard to depth-first search, Reif (1985) givesstrong evidence that it is inherently sequential; its computa-tional complexity cannot be improved upon by parallel com-putation. So while M-structures provide a valuable methodfor general graph searching in parallel, they provide littlehelp for the particular case of depth-first search.The Graph Exploration Language (GEL) of Erwig (1992)provides explicit extensions to a lazy functional language.These are exploration operators, which give a concise way ofexpressing many graph algorithms. However, not all graphproblems can be expressed in terms of a given set of prede-fined high-level operations, and it seems less than ideal toadd new language concepts for every new class of problemthat is tackled.Burton and Yang (1990) experimented with multi-linkedstructures. They use arrays which are implemented usingbalanced trees to represent heaps. They give many exam-ples of using multi-linked structures using heaps, one ex-ample is an arbitrary depth-first search function. A draw-back with their approach is that heaps have to be passedto and returned from each function. Another is that, byusing balanced trees a logarithmic factor is incured, so theirdepth-first search function is not linear in the size of thegraph.10

9 AcknowledgementsWe would like to thank Kieran Clenaghan and Simon Pey-ton Jones for the helpful comments they gave on a pre-liminary draft of this paper. We are also very grateful toRichard Bird for suggesting that DFS can be expressed us-ing the generate/prune paradigm, and to him, Geraint Jonesand Theo Norvell for inspiring the correctness proofs. Thework was partly supported by the UK Science and Engi-neering Research Council.ReferencesBarth, P. S., Nikhil, R. S. and Arvind (1991), M-structures:Extending a parallel, non-strict, functional languagewith state, in J. Hughes, ed., `Conference on Func-tional Programming Languages and Computer Archi-tecture', LNCS 523, Springer-Verlag, Cambridge, Mas-sachusetts, pp. 538-568.
Burton, F. W. and Yang, H.-K. (1990), `Manipulating mul-tilinked data structures in a pure functional language',Software--Practice and Experience 20, 1167-1185.Corman, T. H., Leiserson, C. E. and Rivest, R. L. (1990), In-troduction to Algorithms, The MIT Press, Cambridge,Massachusetts.Erwig, M. (1992), Graph algorithms = iteration + datastructures? The structure of graph algorithms anda style of programming, in E. Mayr, ed., `Graph-Theoretic Concepts in Computer Science', LNCS 657,Springer-Verlag, pp. 277-292.Harrison, R. (1993), Abstract data types in Standard ML,John Wiley and Sons.Holyer, I. (1991), Functional programming with Miranda,Pitman, London.Hopcroft, J. E. and Tarjan, R. E. (1973), `Algorithm 447:Efficient algorithms for graph manipulation', Commu-nications of the ACM 16(6), 372-378.
Hudak, P., Peyton Jones, S. L., Wadler, P., Arvind, Boutel,B., Fairbairn, J., Fasel, J., Guzm'an, M. M., Hammond,K., Hughes, J., Johnsson, T., Kieburtz, R., Nikhil,R. S., Partain, W. and Peterson, J. (1992), `Report onthe functional programming language Haskell, Version1.2', ACM SIGPLAN Notices 27(5).Kashiwagi, Y. and Wise, D. S. (1991), Graph algorithms ina lazy functional programming language, in `Proceed-ings of the 4'th International Symposium on Lucid andIntensional Programming', pp. 35-46.Launchbury, J. (1993), Lazy imperative programming, in`Workshop on State in Programming Languages', ACMSIGPLAN, Copenhagen, Denmark, pp. 46-56.

Launchbury, J. and Peyton Jones, S. L. (1993), Lazy func-tional state threads, University of Glasgow.Manber, U. (1989), Introduction to Algorithms--A CreativeApproach, Addison-Wesley, Reading, Massachusetts.Moggi, E. (1989), Computational lambda-calculus and mon-ads, in `Symposium on Logic in Computer Science',IEEE, Asilomar, California.Paulson, L. C. (1991), ML for the working programmer,Cambridge University Press, Cambridge.Peyton Jones, S. L. and Wadler, P. (1993), Imperative func-tional programming, in `20'th Symposium on Princi-ples of Programming Languages', ACM, Charleston,North Carolina.Reif, J. H. (1985), `Depth-first search is inherently sequen-tial', Information Processing Letters 20, 229-234.Sands, D. (1993), A na"ive time analysis and its theory ofcost equivalence, TOPPS report D-173, DIKU, University of Copenhagen, Denmark.Sharir, M. (1981), `A strong-connectivity algorithm andits applications in data flow analysis', Computers andmathematics with applications 7(1).Tarjan, R. E. (1972), `Depth-first search and linear graphalgorithms', SIAM Journal of Computing 1(2), 146-160.Wadler, P. (1990), Comprehending monads, in `Conferenceon Lisp and Functional Programming', ACM, Nice,France, pp. 61-78.

11

AppendixImperative features were initially introduced into the Glas-gow Haskell compiler to perform input and output, see Pey-ton Jones and Wadler (1993). The approach is based onmonads (Moggi 1989, Wadler 1990), and can easily be ex-tended to achieve in-situ array updates. Launchbury (1993)showed how the original model could be extended to allowthe imperative actions to be delayed until their results arerequired. This is the model we use.We will use the monad of state-transformers with type con-structor ST which is defined:type ST s a = a -? (a,s)So elements of type ST s Int, say, are functions which,when applied to the state, return a pair of an integertogether with a new state. As usual we have the unitreturnST and the sequencing combinator thenST:returnST :: a -? ST s a
returnST a s = (a,s)thenST :: ST s a -? (a -? ST s b) -? ST s b(m `thenST` k) s = k a t where (a,t) = m sThe ST monad provides three basic array operations:newArr ::Ix i=? (i,i) -? a -?ST s (MutArr s i a)readArr ::Ix i=? MutArr s i a -? i -? ST s awriteArr::Ix i=? MutArr s i a -? i -? a -?ST s ()The first, newArr, takes a pair of index bounds (the type amust lie in the index class Ix) together with an initial value,and returns a reference to an initialised array. The timethis operation takes is linear with respect to the number ofelements in the array. The other two provide for reading andwriting to an element of the array, and both take constanttime.Finally, the ST monad comes equipped with a functionrunST.runST :: (8s . ST s a) -? a
This takes a state-transformer function, applies it to an ini-tial state, extracts the final value and discards the finalstate. The type of runST is not Hindley-Milner because ofthe nested quantifier, so it must be built-in to Haskell. Theuniversal quantifier ensures that in a state thread variablesfrom other state threads are not referenced. For details ofthis see Launchbury and Peyton Jones (1993).So, for example,runST (newArr (1,8) 0 `thenST` ("nums -?writeArr nums 5 42 `thenST` (". -?readArr nums 5 `thenST` ("v -?returnST v))))will return 42. This can be read as follows: run a new statethread extracting the final value when finished; create a new

array indexed from 1 to 8 with components all 0; then bindthis array to nums; write to array nums at index 5 the value42; then read the component in nums at index 5 and bindthis value to v; finally return value v. Note that the finalexpression returnST v is unnecessary as readArr returnsa value. The parentheses immediately after `thenST` arealso unnecessary, as Haskell's grammar binds lambda ex-pressions tighter than infix functions.

12