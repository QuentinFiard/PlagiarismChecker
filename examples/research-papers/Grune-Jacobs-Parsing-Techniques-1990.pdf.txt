

Preface
Parsing (syntactic analysis) is one of the best understood branches of computer science.Parsers are already being used extensively in a number of disciplines: in computer science (for compiler construction, database interfaces, self-describing data-bases, artifi-cial intelligence), in linguistics (for text analysis, corpora analysis, machine translation,
textual analysis of biblical texts), in document preparation and conversion, in typeset-ting chemical formulae and in chromosome recognition, to name a few; they can be
used (and perhaps are) in a far larger number of disciplines. It is therefore surprisingthat there is no book which collects the knowledge about parsing and explains it to the
non-specialist. Part of the reason may be that parsing has a name for being "difficult".In discussing the Amsterdam Compiler Kit and in teaching compiler construction, it
has, however, been our experience that seemingly difficult parsing techniques can beexplained in simple terms, given the right approach. The present book is the result of
these considerations.This book does not address a strictly uniform audience. On the contrary, while
writing this book, we have consistently tried to imagine giving a course on the subjectto a diffuse mixture of students and faculty members of assorted faculties, sophisticated
laymen, the avid readers of the science supplement of the large newspapers, etc. Such acourse was never given; a diverse audience like that would be too uncoordinated to
convene at regular intervals, which is why we wrote this book, to be read, studied,perused or consulted wherever or whenever desired.

Addressing such a varied audience has its own difficulties (and rewards).Although no explicit math was used, it could not be avoided that an amount of
mathematical thinking should pervade this book. Technical terms pertaining to parsinghave of course been explained in the book, but sometimes a term on the fringe of the
subject has been used without definition. Any reader who has ever attended a lecture ona non-familiar subject knows the phenomenon. He skips the term, assumes it refers to
something reasonable and hopes it will not recur too often. And then there will be pas-sages where the reader will think we are elaborating the obvious (this paragraph may
be one such place). The reader may find solace in the fact that he does not have to doo-dle his time away or stare out of the window until the lecturer progresses.

On the positive side, and that is the main purpose of this enterprise, we hope thatby means of a book with this approach we can reach those who were dimly aware of
the existence and perhaps of the usefulness of parsing but who thought it would forever

12 Preface
be hidden behind phrases like:

Let \Gamma  be a mapping VN o""

\Delta 

2(VNE`VT)

* and \Theta  a homomorphism ...

No knowledge of any particular programming language is required. The book con-tains two or three programs in Pascal, which serve as actualizations only and play a
minor role in the explanation. What is required, though, is an understanding of algo-rithmic thinking, especially of recursion. Books like Learning to program by Howard
Johnston (Prentice-Hall, 1985) or Programming from first principles by Richard Bornat(Prentice-Hall 1987) provide an adequate background (but supply more detail than
required). Pascal was chosen because it is about the only programming language moreor less widely available outside computer science environments.

The book features an extensive annotated bibliography. The user of the bibliogra-phy is expected to be more than casually interested in parsing and to possess already a
reasonable knowledge of it, either through this book or otherwise. The bibliography asa list serves to open up the more accessible part of the literature on the subject to the
reader; the annotations are in terse technical prose and we hope they will be useful asstepping stones to reading the actual articles.

On the subject of applications of parsers, this book is vague. Although we suggesta number of applications in Chapter 1, we lack the expertise to supply details. It is
obvious that musical compositions possess a structure which can largely be describedby a grammar and thus is amenable to parsing, but we shall have to leave it to the
musicologists to implement the idea. It was less obvious to us that behaviour at cor-porate meetings proceeds according to a grammar, but we are told that this is so and
that it is a subject of socio-psychological research.
AcknowledgementsWe thank the people who helped us in writing this book. Marion de Krieger has
retrieved innumerable books and copies of journal articles for us and without her effortthe annotated bibliography would be much further from completeness. Ed Keizer has
patiently restored peace between us and the pic|tbl|eqn|psfig|troff pipeline, on the manyoccasions when we abused, overloaded or just plainly misunderstood the latter. Leo
van Moergestel has made the hardware do things for us that it would not do for theuninitiated. We also thank Erik Baalbergen, Frans Kaashoek, Erik Groeneveld, Gerco
Ballintijn, Jaco Imthorn, and Egon Amada for their critical remarks and contributions.The rose at the end of Chapter 2 is by Arwen Grune. Ilana and Lily Grune typed parts
of the text on various occasions.We thank the Faculteit Wiskunde en Informatica of the Vrije Universiteit for the
use of the equipment.In a wider sense, we extend our thanks to the hundreds of authors who have been
so kind as to invent scores of clever and elegant algorithms and techniques for us toexhibit. We hope we have named them all in our bibliography.

Dick GruneCeriel J.H. Jacobs
Amstelveen/Amsterdam, July 1990; Sept 1998

1
Introduction

Parsing is the process of structuring a linear representation in accordance with a givengrammar. This definition has been kept abstract on purpose, to allow as wide an
interpretation as possible. The "linear representation" may be a sentence, a computerprogram, a knitting pattern, a sequence of geological strata, a piece of music, actions in
ritual behaviour, in short any linear sequence in which the preceding elements in someway restrict# the next element. For some of the examples the grammar is well-known,
for some it is an object of research and for some our notion of a grammar is only justbeginning to take shape.

For each grammar, there are generally an infinite number of linear representations("sentences") that can be structured with it. That is, a finite-size grammar can supply
structure to an infinite number of sentences. This is the main strength of the grammarparadigm and indeed the main source of the importance of grammars: they summarize
succinctly the structure of an infinite number of objects of a certain class.There are several reasons to perform this structuring process called parsing. One
reason derives from the fact that the obtained structure helps us to process the objectfurther. When we know that a certain segment of a sentence in German is the subject,
that information helps in translating the sentence. Once the structure of a document hasbeen brought to the surface, it can be converted more easily.

A second is related to the fact that the grammar in a sense represents our under-standing of the observed sentences: the better a grammar we can give for the movements of bees, the deeper our understanding of them is.A third lies in the completion of missing information that parsers, and especially
error-repairing parsers, can provide. Given a reasonable grammar of the language, anerror-repairing parser can suggest possible word classes for missing or unknown words
on clay tablets.

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

If there is no restriction, the sequence still has a grammar, but this grammar is trivial and unin-formative.

14 Introduction [Ch. 1
1.1 PARSING AS A CRAFT
Parsing is no longer an arcane art; it has not been so since the early 70's when Aho,Ullman, Knuth and many others put various parsing techniques solidly on their theoretical feet. It need not be a mathematical discipline either; the inner workings of a parsercan be visualized, understood and modified to fit the application, with not much more
than cutting and pasting strings.There is a considerable difference between a mathematician's view of the world
and a computer-scientist's. To a mathematician all structures are static: they havealways been and will always be; the only time dependence is that we just haven't
discovered them all yet. The computer scientist is concerned with (and fascinated by)the continuous creation, combination, separation and destruction of structures: time is
of the essence. In the hands of a mathematician, the Peano axioms create the integerswithout reference to time, but if a computer scientist uses them to implement integer
addition, he finds they describe a very slow process, which is why he will be lookingfor a more efficient approach. In this respect the computer scientist has more in common with the physicist and the chemist; like these, he cannot do without a solid basis inseveral branches of applied mathematics, but, like these, he is willing (and often virtually obliged) to take on faith certain theorems handed to him by the mathematician.Without the rigor of mathematics all science would collapse, but not all inhabitants of a
building need to know all the spars and girders that keep it upright. Factoring off cer-tain detailed knowledge to specialists reduces the intellectual complexity of a task,
which is one of the things computer science is about.This is the vein in which this book is written: parsing for anybody who has parsing to do: the compiler writer, the linguist, the data-base interface writer, the geologistor musicologist who want to test grammatical descriptions of their respective objects of
interest, and so on. We require a good ability to visualize, some programming experi-ence and the willingness and patience to follow non-trivial examples; there is nothing
better for understanding a kangaroo than seeing it jump. We treat, of course, the popu-lar parsing techniques, but we will not shun some weird techniques that look as if they
are of theoretical interest only: they often offer new insights and a reader might find anapplication for them.

1.2 THE APPROACH USED
This book addresses the reader at at least three different levels. The interested non-computer scientist can read the book as "the story of grammars and parsing"; he or she

can skip the detailed explanations of the algorithms: each algorithm is first explained ingeneral terms. The computer scientist will find much technical detail on a wide array of
algorithms. To the expert we offer a systematic bibliography of over 400 entries, whichis intended to cover all articles on parsing that have appeared in the readily available
journals. Each entry is annotated, providing enough material for the reader to decide ifthe referred article is worth reading.

No ready-to-run algorithms have been given, except for the general context-freeparser of Chapter 12. The formulation of a parsing algorithm with sufficient precision
to enable a programmer to implement and run it without problems requires a consider-able supporting mechanism that would be out of place in this book and in our experience does little to increase one's understanding of the process involved. The popularmethods are given in algorithmic form in most books on compiler construction. The

Sec. 1.2] The approach used 15
less widely used methods are almost always described in detail in the original publica-tion, for which see Chapter 13.

1.3 OUTLINE OF THE CONTENTS
Since parsing is concerned with sentences and grammars and since grammars are them-selves fairly complicated objects, ample attention is paid to them in Chapter 2. Chapter

3 discusses the principles behind parsing and gives a classification of parsing methods.In summary, parsing methods can be classified as top-down or bottom-up and as directional or non-directional; the directional methods can be further distinguished intodeterministic and non-deterministic. This scheme dictates the contents of the next few
chapters. In Chapter 4 we treat non-directional methods, including Unger and CYK.Chapter 5 forms an intermezzo with the treatment of finite-state automata, which are
needed in the subsequent chapters. Chapters 6 through 9 are concerned with directionalmethods. Chapter 6 covers non-deterministic directional top-down parsers (recursive
descent, Definite Clause Grammars), Chapter 7 non-deterministic directional bottom-up parsers (Earley). Deterministic methods are treated in Chapters 8 (top-down: LL in
various forms) and 9 (bottom-up: LR, etc.). A combined deterministic/non-deterministic method (Tomita) is also described in Chapter 9. That completes the parsing methods per se.Error handling for a selected number of methods is treated in Chapter 10. The
comparative survey of parsing methods in Chapter 11 summarizes the properties of thepopular and some less popular methods. Chapter 12 contains the full code in Pascal for
a parser that will work for any context-free grammar, to lower the threshold for experi-menting.

1.4 THE ANNOTATED BIBLIOGRAPHY
The annotated bibliography is presented in Chapter 13 and is an easily accessible sup-plement of the main body of the book. Rather than listing all publications in alphabetic

order, it is divided into fourteen named sections, each concerned with a particularaspect of parsing; inside the sections, the publications are listed chronologically. An
author index replaces the usual alphabetic list. The section name plus year of publica-tion, placed in brackets, are used in the text to refer to an author's work. For instance,
the annotated reference to Earley's publication of the Earley parser [CF 1970] can befound in the section CF at the position of the papers of 1970. Since the name of the
first author is printed in bold letters, the actual reference is then easily located.

2
Grammars as a generating device

2.1 LANGUAGES AS INFINITE SETS
In computer science as in everyday parlance, a "grammar" serves to "describe" a"language". If taken on face value, this correspondence, however, is misleading, since

the computer scientist and the naive speaker mean slightly different things by the threeterms. To establish our terminology and to demarcate the universe of discourse, we
shall examine the above terms, starting with the last one.
2.1.1 LanguageTo the larger part of mankind, language is first and foremost a means of communication, to be used almost unconsciously, certainly so in the heat of a debate. Communica-tion is brought about by sending messages, through air vibrations or through written
symbols. Upon a closer look the language messages ("utterances") fall apart into sen-tences, which are composed of words, which in turn consist of symbol sequences when
written. Languages can differ on all these three levels of composition. The script can beslightly different, as between English and Irish, or very different, as between English
and Chinese. Words tend to differ greatly and even in closely related languages peoplecall un cheval or ein Pferd, that which is known to others as a horse. Differences in
sentence structure are often underestimated; even the closely related Dutch often has analmost Shakespearean word order: "Ik geloof je niet", "I believe you not", and unrelated
languages readily come up with constructions like the Hungarian "Pe'nzem van","Money-my is", where the English say "I have money".

The computer scientist takes a very abstracted view of all this. Yes, a languagehas sentences, and these sentences possess structure; whether they communicate something or not is not his concern, but information may possibly be derived from theirstructure and then it is quite all right to call that information the meaning of the sentence. And yes, sentences consist of words, which he calls tokens, each possibly carry-ing a piece of information, which is its contribution to the meaning of the whole sentence. But no, words cannot be broken down any further. The computer scientist is notworried by this. With his love of telescoping solutions and multi-level techniques, he
blithely claims that if words turn out to have structure after all, they are sentences in adifferent language, of which the letters are the tokens.

Sec. 2.1] Languages as infinite sets 17

The practitioner of formal linguistics, henceforth called the formal-linguist (to dis-tinguish him from the "formal linguist", the specification of whom is left to the imagination of the reader) again takes an abstracted view of this. A language is a "set" ofsentences, and each sentence is a "sequence" of "symbols"; that is all there is: no
meaning, no structure, either a sentence belongs to the language or it does not. The onlyproperty of a symbol is that it has an identity; in any language there are a certain
number of different symbols, the alphabet, and that number must be finite. Just for con-venience we write these symbols as a,b,c . . . , but

&, (, q, . . . would do equallywell, as long as there are enough symbols. The word sequence means that the symbols

in each sentence are in a fixed order and we should not shuffle them. The word setmeans an unordered collection with all the duplicates removed; a set can be written
down by writing the objects in it, surrounded by curly brackets. All this means that tothe formal-linguist the following is a language: {a, b, ab, ba}, and so is {a, aa, aaa,
aaaa, . . . } although the latter has notational problems that will be solved later. Inaccordance with the correspondence that the computer scientist sees between
sentence/word and word/letter, the formal-linguist also calls a sentence a word and hesays that "the word ab is in the language {a, b, ab, ba}".

Now let's consider the implications of these compact but powerful ideas.To the computer scientist, a language is a probably infinitely large set of sentences, each composed of tokens in such a way that it has structure; the tokens and thestructure cooperate to describe the semantics of the sentence, its "meaning" if you will.
Both the structure and the semantics are new, that is, were not present in the formalmodel, and it is his responsibility to provide and manipulate them both. To a computer
scientist 3+4*5 is a sentence in the language of "arithmetics on single digits" ("singledigits" to avoid having an infinite number of symbols), its structure can be shown, for
instance, by inserting parentheses: (3+(4*5)) and its semantics is probably 23.To the linguist, whose view of languages, it has to be conceded, is much more
normal than that of either of the above, a language is an infinite set of possibly interre-lated sentences. Each sentence consists, in a structured fashion, of words which have a
meaning in the real world. Structure and words together give the sentence a meaning,which it communicates. Words, again, possess structure and are composed of letters;
the letters cooperate with some of the structure to give a meaning to the word. Theheavy emphasis on semantics, the relation with the real world and the integration of the
two levels sentence/word and word/letters are the domain of the linguist. "The circlespins furiously" is a sentence, "The circle sleeps red" is nonsense.

The formal-linguist holds his views of language because he wants to study thefundamental properties of languages in their naked beauty; the computer scientist holds
his because he wants a clear, well-understood and unambiguous means of describingobjects in the computer and of communication with the computer, a most exacting
communication partner, quite unlike a human; and the linguist holds his view oflanguage because it gives him a formal tight grip on a seemingly chaotic and perhaps
infinitely complex object: natural language.
2.1.2 GrammarsEveryone who has studied a foreign language knows that a grammar is a book of rules
and examples which describes and teaches the language. Good grammars make a care-ful distinction between the sentence/word level, which they often call syntax or syntaxis, and the word/letter level, which they call grammar. Syntax contains rules like

18 Grammars as a generating device [Ch. 2
"pour que is followed by the subjunctive, but parce que is not"; grammar contains ruleslike "the plural of an English noun is formed by appending an -s, except when the word
ends in -s, -sh, -o, -ch or -x, in which case -es is appended, or when the word has anirregular plural."

We skip the computer scientist's view of a grammar for the moment and proceedimmediately to the formal-linguist's one. His view is at the same time very abstract and
quite similar to the above: a grammar is any exact, finite-size, complete description ofthe language, i.e., of the set of sentences. This is in fact the school grammar, with the
fuzziness removed. Although it will be clear that this definition has full generality, itturns out that it is too general, and therefore relatively powerless. It includes descriptions like "the set of sentences that could have been written by Chaucer"; platonicallyspeaking this defines a set, but we have no way of creating this set or testing whether a
given sentence belongs to this language. This particular example, with its "could havebeen" does not worry the formal-linguist, but there are examples closer to his home that
do. "The longest block of consecutive sevens in the decimal expansion of p" describesa language that has at most one word in it (and then that word will consist of sevens
only), and as a definition it is exact, finite-size and complete. One bad thing with it,however, is that one cannot find this word; suppose one finds a block of one hundred
sevens after billions and billions of digits, there is always a chance that further on thereis an even longer block. And another bad thing is that one cannot even know if such a
longest block exists at all. It is quite possible that, as one proceeds further and furtherup the decimal expansion of p, one would find longer and longer stretches of sevens,
probably separated by ever-increasing gaps. A comprehensive theory of the decimalexpansion of p might answer these questions, but no such theory exists.

For these and other reasons, the formal-linguists have abandoned their static, pla-tonic view of a grammar for a more constructive one, that of the generative grammar: a
generative grammar is an exact, fixed-size recipe for constructing the sentences in thelanguage. This means that, following the recipe, it must be possible to construct each
sentence of the language (in a finite number of actions) and no others. This does notmean that, given a sentence, the recipe tells us how to construct that particular sentence,
only that it is possible to do so. Such recipes can have several forms, of which some aremore convenient than others.

The computer scientist essentially subscribes to the same view, often with theadditional requirement that the recipe should imply how a sentence can be constructed.

2.1.3 ProblemsThe above definition of a language as a possibly infinite set of sequences of symbols,
and of a grammar as a finite recipe to generate these sentences, immediately gives riseto two embarrassing questions:
1. How can finite recipes generate enough infinite sets of sentences?2. If a sentence is just a sequence and has no structure and if the meaning of a sentence derives, among other things, from its structure, how can we assess the mean-ing of a sentence?
These questions have long and complicated answers, but they do have answers.We shall first pay some attention to the first question and then devote the main body of
this book to the second.

Sec. 2.1] Languages as infinite sets 19
2.1.3.1 Infinite sets from finite descriptionsIn fact there is nothing wrong with getting a single infinite set from a single finite
description: "the set of all positive integers" is a very finite-size description of a defin-itely infinite-size set. Still, there is something disquieting about the idea, so we shall
rephrase our question: "Can all languages be described by finite descriptions?". As thelead-up already suggests, the answer is "No", but the proof is far from trivial. It is,
however, very interesting and famous, and it would be a shame not to present at leastan outline of it here.

2.1.3.2 Descriptions can be enumeratedThe proof is based on two observations and a trick. The first observation is that
descriptions can be listed and given a number. This is done as follows. First, take alldescriptions of size one, that is, those of only one letter long, and sort them alphabetically. This is the beginning of our list. Depending on what, exactly, we accept as adescription, there may be zero descriptions of size one, or 27 (all letters + space), or
128 (all ASCII characters) or some such; this is immaterial to the discussion which fol-lows.

Second, we take all descriptions of size two, sort them alphabetically to give thesecond chunk on the list, and so on for lengths 3, 4 and further. This assigns a position
on the list to each and every description. Our description "the set of all positiveintegers", for instance, is of size 32, not counting the quotation marks. To find its position on the list, we have to calculate how many descriptions there are with less than 32characters, say L. We then have to generate all descriptions of size 32, sort them and
determine the position of our description in it, say P, and add the two numbers L and P.This will, of course, give a huge number# but it does ensure that the description is on
the list, in a well-defined position; see Figure 2.1.

{ descriptions of size 1
{ descriptions of size 2
{ descriptions of size 3

. . . . .

{descriptions of size 31

L

. . . . . . . . . . . . . . . . . . . . . . .{

descriptions of size 32"the set of all positive integers"P

Figure 2.1 List of all descriptions of length 32 or less
Two things should be pointed out here. The first is that just listing all descriptionsalphabetically, without reference to their lengths, would not do: there are already infinitely many descriptions starting with an "a" and no description starting with a higher
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

Some (computer-assisted) calculations tell us that, under the ASCII-128 assumption, thenumber is 248 17168 89636 37891 49073 14874 06454 89259 38844 52556 26245 57755 89193

30291, or roughly 2.5*1067.

20 Grammars as a generating device [Ch. 2
letter could get a number on the list. The second is that there is no need to actually doall this. It is just a thought experiment that allows us to examine and draw conclusion
about the behaviour of a system in a situation which we cannot possibly examine physi-cally.

Also, there will be many nonsensical descriptions on the list; it will turn out thatthis is immaterial to the argument. The important thing is that all meaningful descriptions are on the list, and the above argument ensures that.
2.1.3.3 Languages are infinite bit-stringsWe know that words (sentences) in a language are composed of a finite set of symbols;
this set is called quite reasonably the alphabet. We will assume that the symbols in thealphabet are ordered. Then the words in the language can be ordered too. We shall indicate the alphabet by \Sigma .Now the simplest language that uses alphabet \Sigma  is that which consists of all words
that can be made by combining letters from the alphabet. For the alphabet \Sigma ={a, b} weget the language { , a, b, aa, ab, ba, bb, aaa, . . . }. We shall call this language \Sigma *, for
reasons to be explained later; for the moment it is just a name.The set notation \Sigma * above started with " { , a,", a remarkable construction; the
first word in the language is the empty word, the word consisting of zero a's and zerob's. There is no reason to exclude it, but, if written down, it may easily get lost, so we
shall write it as e (epsilon), regardless of the alphabet. So, \Sigma *= {e, a, b, aa, ab, ba, bb,aaa, . . . }. In some natural languages, forms of the present tense of the verb "to be"
are the empty word, giving rise to sentences of the form "I student"; Russian andHebrew are examples of this.

Since the symbols in the alphabet \Sigma  are ordered, we can list the words in thelanguage \Sigma *, using the same technique as in the previous section: First all words of size
zero, sorted; then all words of size one, sorted; and so on. This is actually the orderalready used in our set notation for \Sigma *.

The language \Sigma * has the interesting property that all languages using alphabet \Sigma are subsets of it. That means that, given another possibly less trivial language over \Sigma ,
called L, we can go through the list of words in \Sigma * and put ticks on all words that are inL. This will cover all words in L, since \Sigma * contains any possible word over \Sigma .

Suppose our language L is "the set of all words that contain more a's than b's".L={a, aa, aab, aba, baa, . . . }. The beginning of our list, with ticks, will look as follows:

e
4 ab

4 aaab

babb
4 aaa
4 aab
4 abaabb

4 baa

Sec. 2.1] Languages as infinite sets 21

babbba
bbb
4 aaaa... ...

Given the alphabet with its ordering, the list of blanks and ticks alone is entirely suffi-cient to identify and describe the language. For convenience we write the blank as a 0
and the tick as a 1 as if they were bits in a computer, and we can now writeL=0101000111010001 . . . (and \Sigma *=1111111111111111 . . . ). It should be noted that
this is true for any language, be it a formal language like L, a programming languagelike Pascal or a natural language like English. In English, the 1's in the bit-string will
be very scarce, since hardly any arbitrary sequence of words is a good English sentence(and hardly any arbitrary sequence of letters is a good English word, depending on
whether we address the sentence/word level or the word/letter level).
2.1.3.4 DiagonalizationThe previous section attaches the infinite bit-string 0101000111010001... to the
description "the set of all the words that contain more a's than b's". In the same veinwe can attach such bit-strings to all descriptions; some descriptions may not yield a
language, in which case we can attach an arbitrary infinite bit-string to it. Since alldescriptions can be put on a single numbered list, we get, for instance, the following
picture:

Description LanguageDescription #1 000000100...
Description #2 110010001...Description #3 011011010...
Description #4 110011010...Description #5 100000011...
Description #6 111011011...... ...

At the left we have all descriptions, at the right all languages they describe. We nowclaim that many languages exist that are not on the list of languages above: the above
list is far from complete, although the list of descriptions is complete. We shall provethis by using the diagonalization process ("Diagonalverfahren") of Cantor.

Consider the language C=100110 . . . , which has the property that its n-th bit isunequal to the n-th bit of the language described by Description #n. The first bit of C is
a 1, because the first bit for Description #1 is a 0; the second bit of C is a 0, because thesecond bit for Description #2 is a 1, and so on. C is made by walking the NW to SE
diagonal of the language field and copying the opposites of the bits we meet.The language C cannot be on the list! It cannot be on line 1, since its first bit
differs (is made to differ, one should say) from that on line 1, and in general it cannotbe on line n, since its n-th bit will differ from that on line n, by definition.

So, in spite of the fact that we have exhaustively listed all possible finite descrip-tions, we have at least one language that has no description on the list. Moreover, any
broken diagonal yields such a language, where a diagonal is "broken" by replacing a

22 Grammars as a generating device [Ch. 2
section of it as follows,

...
...
...
...
...
...
.... . . . . . . . . . . . . . . . . . . . ......

...
...
...
...
...
...................... ...
...
...
...
...
...
...

. . . . . . . . . . . . . . . . . . . . . o""

...
...
...
...
...
...
.... . . . . . . . . . . . . . . . . . . . ......

...
...
...
...
...
...................... ...
...
...
...
...
...
...

. . . . . . . . . . . . . . . . . . . . .

and so does any multiply-broken diagonal. In fact, for each language on the list, thereare infinitely many languages not on it; this statement is, however, more graphical than
it is exact, and we shall not prove it.The diagonalization technique is described more formally in most books on
theoretical computer science; see e.g., Rayward-Smith [Books 1983, pp. 5-6] or Hop-croft and Ullman [Books 1979, pp 6-9].

2.1.3.5 ConclusionsThe above demonstration shows us several things. First, it shows the power of treating
languages as formal objects. Although the above outline clearly needs considerableamplification and substantiation to qualify as a proof (for one thing it still has to be
clarified why the above explanation, which defines the language C, is not itself on thelist of descriptions), it allows us to obtain insight in properties not otherwise assessable.

Secondly, it shows that we can only describe a tiny subset (not even a fraction) ofall possible languages: there is an infinity of languages out there, forever beyond our
reach.Thirdly, we have proved that, although there are infinitely many descriptions and
infinitely many languages, these infinities are not equal to each other and that the latteris larger than the former. These infinities are called A`

0 and A`1 by Cantor, and theabove is just an adaptation of his proof that A`
0<A`1.

2.1.4 Describing a language through a finite recipeA good way to build a set of objects is to start with a small object and to give rules how
to add to it and construct new objects from it. "Two is an even number and the sum oftwo even numbers is again an even number" effectively generates the set of all even
numbers. Formalists will add "...and no other numbers are even", but we'll skip that.Suppose we want to generate the set of all enumerations of names, of the type
"Tom, Dick and Harry", in which all names but the last two are separated by commas.We will not accept "Tom, Dick, Harry" nor "Tom and Dick and Harry", but we shall
not object to duplicates: "Grubb, Grubb and Burrowes"# is all right. Although these arenot complete sentences in normal English, we shall still call them sentences since that
is what they are in our midget language of name enumerations. A simple-minded recipewould be:

0. Tom is a name, Dick is a name, Harry is a name;1. a name is a sentence;
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

The Hobbit, by J.R.R. Tolkien, Allen and Unwin, 1961, p. 311.

Sec. 2.1] Languages as infinite sets 23

2. a sentence followed by a comma and a name is again a sentence;3. before finishing, if the sentence ends in ", name", replace it by

"and name".
Although this will work for a cooperative reader, there are several things wrongwith it. Clause 3 is especially wrought with trouble. For instance, the sentence does not
really end in ", name", it ends in ", Dick" or such, and "name" is just a symbol thatstands for a real name; such symbols cannot occur in a real sentence and must in the
end be replaced by a real name as given in clause 0. Likewise, the word "sentence" inthe recipe is a symbol that stands for all the actual sentences. So there are two kinds of
symbols involved here: real symbols, which occur in finished sentences, like "Tom","Dick", a comma and the word "and"; and there are intermediate symbols, like "sentence" and "name" that cannot occur in a finished sentence. The first kind correspondsto the words or tokens explained above and the technical term for them is terminal symbols (or terminals for short) while the latter are called non-terminals (a singularly unin-spired term). To distinguish them, we write terminals in small letters and start nonterminals with a capital.To stress the generative character of the recipe, we shall replace "X is a Y" by "Y
may be replaced by X": if "tom" is an instance of a Name, then everywhere we have aName we may narrow it down to "tom". This gives us:

0. Name may be replaced by "tom"Name may be replaced by "dick"

Name may be replaced by "harry"1. Sentence may be replaced by Name
2. Sentence may be replaced by Sentence, Name3. ", Name" at the end of a Sentence must be replaced by "and Name"

before Name is replaced by any of its replacements4. a sentence is finished only when it no longer contains non-terminals
5. we start our replacement procedure with Sentence
Clause 0 through 3 describe replacements, but 4 and 5 are different. Clause 4 is notspecific to this grammar. It is valid generally and is one of the rules of the game.
Clause 5 tells us where to start generating. This name is quite naturally called the startsymbol, and it is required for every grammar.

Still clause 3 looks worrisome; most rules have "may be replaced", but this onehas "must be replaced", and it refers to the "end of a Sentence". The rest of the rules
work through replacement, but the problem remains how we can use replacement totest for the end of a Sentence. This can be solved by adding an end-marker after it. And
if we make the end-marker a non-terminal which cannot be used anywhere except inthe required replacement from ", Name" to "and Name", we automatically enforce the
restriction that no sentence is finished unless the replacement test has taken place. Forbrevity we write

-->> instead of "may be replaced by"; since terminal and non-terminalsymbols are now identified as technical objects we shall write them in a typewriter-like

typeface. The part before the -->> is called the left-hand side, the part after it the right-hand side. This results in the recipe in Figure 2.2.

This is a simple and relatively precise form for a recipe, and the rules are equallystraightforward: start with the start symbol, and keep replacing until there are no non24 Grammars as a generating device [Ch. 2

0. NNaammee -->> ttoomm

NNaammee -->> ddiicckk
NNaammee -->> hhaarrrryy1.
SSeenntteennccee -->> NNaammee
SSeenntteennccee -->> LLiisstt EEnndd2.

LLiisstt -->> NNaammee
LLiisstt -->> LLiisstt ,, NNaammee3.
,, NNaammee EEnndd -->> aanndd NNaammee4. the start symbol is

SSeenntteennccee

Figure 2.2 A finite recipe for generating strings in the t, d & h language
terminals left.

2.2 FORMAL GRAMMARS
The above recipe form, based on replacement according to rules, is strong enough toserve as a basis for formal grammars. Similar forms, often called "rewriting systems",

have a long history among mathematicians, but the specific form of Figure 2.2 was firststudied extensively by Chomsky [Misc 1959]. His analysis has been the foundation for
almost all research and progress in formal languages, parsers and a considerable part ofcompiler construction and linguistics.

Since formal languages are a branch of mathematics, work in this field is done ina special notation which can be a hurdle to the uninitiated. To allow a small peep into
the formal linguist's kitchen, we shall give the formal definition of a grammar and thenexplain why it describes a grammar like the one in Figure 2.2. The formalism used is
indispensable for correctness proofs, etc., but not for understanding the principles; it isshown here only to give an impression and, perhaps, to bridge a gap.

Definition 2.1: A generative grammar is a 4-tuple (VN,VT,R,S) such that (1) VNand V

T are finite sets of symbols, (2) VNC,VT = AE, (3) R is a set of pairs (P,Q ) suchthat (3a) P I^(V

NE`VT)+ and (3b) Q I^(VNE`VT)*, and (4) SI^VN.A 4-tuple is just an object consisting of 4 identifiable parts; they are the nonterminals, the terminals, the rules and the start symbol, in that order; the above defini-tion does not tell this, so this is for the teacher to explain. The set of non-terminals is
named VN and the set of terminals VT. For our grammar we have:

VN = {NNaammee, SSeenntteennccee, LLiisstt, EEnndd}V

T = {ttoomm, ddiicckk, hhaarrrryy, ,,, aanndd}

(note the ,, in the set of terminal symbols).The intersection of V

N and VT (2) must be empty, that is, the non-terminals andthe terminals may not have a symbol in common, which is understandable.

R is the set of all rules (3), and P and Q are the left-hand sides and right-handsides, respectively. Each P must consist of sequences of one or more non-terminals
and terminals and each Q must consist of sequences of zero or more non-terminals andterminals. For our grammar we have:

R = {(NNaammee, ttoomm), (NNaammee, ddiicckk), (NNaammee, hhaarrrryy),

Sec. 2.2] Formal grammars 25

(SSeenntteennccee, NNaammee), (SSeenntteennccee, LLiisstt EEnndd),(

LLiisstt, NNaammee), (LLiisstt, LLiisstt ,, NNaammee), (,, NNaammee EEnndd, aanndd NNaammee)}

Note again the two different commas.The start symbol S must be an element of V

N, that is, it must be a non-terminal:

S = SSeenntteennccee
This concludes our field trip into formal linguistics; the reader can be assured thatthere is lots and lots more. A good simple introduction is written by Re've'sz [Books
1985].
2.2.1 Generating sentences from a formal grammarThe grammar in Figure 2.2 is what is known as a phrase structure grammar for our
t,d&h language (often abbreviated to PS grammar). There is a more compact notation,in which several right-hand sides for one and the same left-hand side are grouped
together and then separated by vertical bars, ||. This bar belongs to the formalism, justas the arrow

-->> and can be read "or else". The right-hand sides separated by verticalbars are also called alternatives. In this more concise form our grammar becomes:

0. NNaammee -->> ttoomm || ddiicckk || hhaarrrryy1.

SSeenntteenncceeSS -->> NNaammee || LLiisstt EEnndd2.

LLiisstt -->> NNaammee || NNaammee ,, LLiisstt3.
,, NNaammee EEnndd -->> aanndd NNaammee

where the non-terminal with the subscript S is the start symbol. (The subscript S identi-fies the symbol, not the rule.)

Now let's generate our initial example from this grammar, using replacementaccording to the above rules only. We obtain the following successive forms for

SSeenn--
tteennccee:

Intermediate form Rule used Explanation
SSeenntteennccee the start symbol
LLiisstt EEnndd SSeenntteennccee -->> LLiisstt EEnndd rule 1
NNaammee ,, LLiisstt EEnndd LLiisstt -->> NNaammee ,, LLiisstt rule 2
NNaammee ,, NNaammee ,, LLiisstt EEnndd LLiisstt -->> NNaammee ,, LLiisstt rule 2
NNaammee ,, NNaammee ,, NNaammee EEnndd LLiisstt -->> NNaammee rule 2
NNaammee ,, NNaammee aanndd NNaammee ,, NNaammee EEnndd -->> aanndd NNaammee rule 3
ttoomm ,, ddiicckk aanndd hhaarrrryy rule 0, three times

The intermediate forms are called sentential forms; if a sentential form contains nonon-terminals it is called a sentence and belongs to the generated language. The transitions from one line to the next are called production steps and the rules are often calledproduction rules, for obvious reasons.

The production process can be made more visual by drawing connective linesbetween corresponding symbols, as shown in Figure 2.3. Such a picture is called a production graph or syntactic graph, because it depicts the syntactic structure (with regardto the given grammar) of the final sentence. We see that the production graph normally

26 Grammars as a generating device [Ch. 2

SSeenntteennccee
LLiisstt EEnndd
NNaammee ,, LLiisstt

NNaammee ,, LLiisstt

,, NNaammee EEnndd

aanndd NNaammee
ttoomm ,, ddiicckk aanndd hhaarrrryy

Figure 2.3 Production graph for a sentence
fans out downwards, but occasionally we may see starlike constructions, which resultfrom rewriting a group of symbols.

It is patently impossible to have the grammar generate ttoomm,, ddiicckk,, hhaarrrryy,since any attempt to produce more than one name will drag in an

EEnndd and the only wayto get rid of it again (and get rid of it we must, since it is a non-terminal) is to have it

absorbed by rule 3, which will produce the aanndd. We see, to our amazement, that wehave succeeded in implementing the notion "must replace" in a system that only uses
"may replace"; looking more closely, we see that we have split "must replace" into"may replace" and "must not be a non-terminal".

Apart from our standard example, the grammar will of course also produce manyother sentences; examples are:

hhaarrrryy aanndd ttoomm
hhaarrrryy
ttoomm,, ttoomm,, ttoomm,, aanndd ttoomm

and an infinity of others. A determined and foolhardy attempt to generate the incorrectform without the

aanndd will lead us to sentential forms like:

ttoomm,, ddiicckk,, hhaarrrryy EEnndd
which are not sentences and to which no production rule applies. Such forms are calledblind alleys. Note that production rules may not be applied in the reverse direction.

Sec. 2.2] Formal grammars 27
2.2.2 The expressive power of formal grammarsThe main property of a formal grammar is that it has production rules, which may be
used for rewriting part of the sentential form (= sentence under construction) and astarting symbol which is the mother of all sentential forms. In the production rules we
find non-terminals and terminals; finished sentences contain terminals only. That isabout it: the rest is up to the creativity of the grammar writer and the sentence producer.This is a framework of impressive frugality and the question immediately rises: Is
it sufficient? Well, if it isn't, we don't have anything more expressive. Strange as itmay sound, all other methods known to mankind for generating sets have been proved
to be equivalent to or less powerful than a phrase structure grammar. One obviousmethod for generating a set is, of course, to write a program generating it, but it has
been proved that any set that can be generated by a program can be generated by aphrase structure grammar. There are even more arcane methods, but all of them have
been proved not to be more expressive. On the other hand there is no proof that no suchstronger method can exist. But in view of the fact that many quite different methods all
turn out to halt at the same barrier, it is highly unlikely# that a stronger method willever be found. See, e.g. Re've'sz [Books 1985, pp 100-102].

As a further example of the expressive power we shall give a grammar for themovements of a Manhattan turtle. A Manhattan turtle moves in a plane and can only
move north, east, south or west in distances of one block. The grammar of Figure 2.4produces all paths that return to their own starting point.

1. MMoovveeSS -->> nnoorrtthh MMoovvee ssoouutthh || eeaasstt MMoovvee wweesstt || ee2.

nnoorrtthh eeaasstt -->> eeaasstt nnoorrtthh
nnoorrtthh ssoouutthh -->> ssoouutthh nnoorrtthh

nnoorrtthh wweesstt -->> wweesstt nnoorrtthh
eeaasstt nnoorrtthh -->> nnoorrtthh eeaasstt
eeaasstt ssoouutthh -->> ssoouutthh eeaasstt

eeaasstt wweesstt -->> wweesstt eeaasstt
ssoouutthh nnoorrtthh -->> nnoorrtthh ssoouutthh

ssoouutthh eeaasstt -->> eeaasstt ssoouutthh
ssoouutthh wweesstt -->> wweesstt ssoouutthh
wweesstt nnoorrtthh -->> nnoorrtthh wweesstt

wweesstt eeaasstt -->> eeaasstt wweesstt
wweesstt ssoouutthh -->> ssoouutthh wweesstt

Figure 2.4 Grammar for the movements of a Manhattan turtle
As to rule 2, it should be noted that some authors require at least one of the symbols inthe left-hand side to be a non-terminal. This restriction can always be enforced by
adding new non-terminals.The simple round trip

nnoorrtthh eeaasstt ssoouutthh wweesstt is produced as shown in Fig-ure 2.5 (names abbreviated to their first letter). Note the empty alternative in rule 1

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

Paul Vita'ny has pointed out that if scientists call something "highly unlikely" they are stillgenerally not willing to bet a year's salary on it, double or quit.

28 Grammars as a generating device [Ch. 2
(the e), which results in the dying out of the third MM in the above production graph.

MM
nn MM ss

ee MM ww ss
nn ee ss ww

Figure 2.5 How the grammar of Figure 2.4 produces a round trip

2.3 THE CHOMSKY HIERARCHY OF GRAMMARS AND LANGUAGES
The grammars from Figures 2.2 and 2.4 are easy to understand and indeed some simplephrase structure grammars generate very complicated sets. The grammar for any given

set is, however, usually far from simple. (We say "The grammar for a given set"although there can be, of course, infinitely many grammars for a set. By the grammar
for a set, we mean any grammar that does the job and is not obviously overly compli-cated.) Theory says that if a set can be generated at all (for instance, by a program) it
can be generated by a phrase structure grammar, but theory does not say that it will beeasy to do so, or that the grammar will be understandable. In this context it is illustrative to try to write a grammar for those Manhattan turtle paths in which the turtle isnever allowed to the west of its starting point. (Hint: use a special (non-terminal)
marker for each block the turtle is located to the east of its starting point).Apart from the intellectual problems phrase structure grammars pose, they also
exhibit fundamental and practical problems. We shall see that no general parsing algo-rithm for them can exist, and all known special parsing algorithms are either very inefficient or very complex; see Section 3.5.2.The desire to restrict the unmanageability of phrase structure grammars, while
keeping as much of their generative powers as possible, has led to the Chomsky hierar-chy of grammars. This hierarchy distinguishes four types of grammars, numbered from
0 to 3; it is useful to include a fifth type, called Type 4 here. Type 0 grammars are the(unrestricted) phrase structure grammars of which we have already seen examples. The
other types originate from applying more and more restrictions to the allowed form ofthe rules in the grammar. Each of these restrictions has far-reaching consequences; the
resulting grammars are gradually easier to understand and to manipulate, but are alsogradually less powerful. Fortunately, these less powerful types are still very useful,
actually more useful even than Type 0. We shall now consider each of the threeremaining types in turn, followed by a trivial but useful fourth type.

2.3.1 Type 1 grammarsThe characteristic property of a Type 0 grammar is that it may contain rules that
transform an arbitrary (non-zero) number of symbols into an arbitrary (possibly zero)number of symbols. Example:

,, NN EE -->> aanndd NN

Sec. 2.3] The Chomsky hierarchy of grammars and languages 29
in which three symbols are replaced by two. By restricting this freedom, we obtainType 1 grammars. Strangely enough there are two, intuitively completely different
definitions of Type 1 grammars, which can be proved to be equivalent.A grammar is Type 1 monotonic if it contains no rules in which the left-hand side
consists of more symbols than the right-hand side. This forbids, for instance, the rule ,,
NN EE -->> aanndd NN.A grammar is Type 1 context-sensitive if all of its rules are context-sensitive. A

rule is context-sensitive if actually only one (non-terminal) symbol in its left-hand sidegets replaced by other symbols, while we find the others back undamaged and in the
same order in the right-hand side. Example:

NNaammee CCoommmmaa NNaammee EEnndd -->> NNaammee aanndd NNaammee EEnndd
which tells that the rule

CCoommmmaa -->> aanndd
may be applied if the left context is NNaammee and the right context is NNaammee EEnndd. The con-texts themselves are not affected. The replacement must be at least one symbol long;
this means that context-sensitive grammars are always monotonic; see Section 2.6.Here is a monotonic grammar for our t,d&h example. In writing monotonic grammars one has to be careful never to produce more symbols than will be produced even-tually. We avoid the need to delete the end-marker by incorporating it into the rightmost name.

NNaammee -->> ttoomm || ddiicckk || hhaarrrryy
SSeenntteenncceeSS -->> NNaammee || LLiisstt

LLiisstt -->> EEnnddNNaammee || NNaammee ,, LLiisstt
,, EEnnddNNaammee -->> aanndd NNaammee

where EEnnddNNaammee is a single symbol.And here is a context-sensitive grammar for it.

NNaammee -->> ttoomm || ddiicckk || hhaarrrryy
SSeenntteenncceeSS -->> NNaammee || LLiisstt

LLiisstt -->> EEnnddNNaammee

|| NNaammee CCoommmmaa LLiisstt
CCoommmmaa EEnnddNNaammee -->> aanndd EEnnddNNaammee context is ...... EEnnddNNaammee

aanndd EEnnddNNaammee -->> aanndd NNaammee context is aanndd ......

CCoommmmaa -->> ,,

Note that we need an extra non-terminal CCoommmmaa to be able to produce the terminal aannddin the correct context.

Monotonic and context-sensitive grammars are equally powerful: for eachlanguage that can be generated by a monotonic grammar a context-sensitive grammar
exists that generates the same language, and vice versa. They are less powerful thanthe Type 0 grammars, that is, there are languages that can be generated by a Type 0
grammar but not by any Type 1. Strangely enough no simple examples of such

30 Grammars as a generating device [Ch. 2
languages are known. Although the difference between Type 0 and Type 1 is funda-mental and is not just a whim of Mr. Chomsky, grammars for which the difference
matters are too complicated to write down; only their existence can be proved (see e.g.,Hopcroft and Ullman [Books 1979, pp. 183-184] or Re've'sz [Books 1985, p. 98]).

Of course any Type 1 grammar is also a Type 0 grammar, since the class of Type1 grammars is obtained from the class of Type 0 grammars by applying restrictions.
But it would be confusing to call a Type 1 grammar a Type 0 grammar; it would be likecalling a cat a mammal: correct but not informative enough. A grammar is named after
the smallest class (that is, the highest type number) in which it will still fit.We saw that our t,d&h language, which was first generated by a Type 0 grammar,
could also be generated by a Type 1 grammar. We shall see that there is also a Type 2and a Type 3 grammar for it, but no Type 4 grammar. We therefore say that the t,d&h
language is Type 3 language, after the most restricted (and simple and amenable) gram-mar for it. Some corollaries of this are: A Type n language can be generated by a Type
n grammar or anything stronger, but not by a weaker Type n +1 grammar; and: If alanguage is generated by a Type n grammar, that does not necessarily mean that there is
no (weaker) Type n +1 grammar for it. The use of a Type 0 grammar for our t,d&hlanguage was a serious case of overkill, just for demonstration purposes.

The standard example of a Type 1 language is the set of words that consist ofequal numbers of

aa's, bb's and cc's, in that order:

a a . . . . a
n of them

b b . . . . b
n of them

c c . . . . c
n of them

2.3.1.1 Constructing a Type 1 grammarFor the sake of completeness and to show how one writes a Type 1 grammar if one is
clever enough, we shall now derive a grammar for this toy language. Starting with thesimplest case, we have the rule

0. SS -->> aabbcc
Having got one instance of SS, we may want to prepend more aa's to the beginning; if wewant to remember how many there were, we shall have to append something to the end
as well at the same time, and that cannot be a bb or a cc. We shall use a yet unknownsymbol

QQ. The following rule pre- and postpends:

1. SS -->> aabbcc || aaSSQQ
If we apply this rule, for instance, three times, we get the sentential form

aaaaaabbccQQQQ
Now, to get aaaaaabbbbbbcccccc from this, each QQ must be worth one bb and one cc, as was to beexpected, but we cannot just write

QQ -->> bbcc

Sec. 2.3] The Chomsky hierarchy of grammars and languages 31
because that would allow bb's after the first cc. The above rule would, however, be allright if it were allowed to do replacement only between a

bb and a cc; there, the newlyinserted
bbcc will do no harm:

2. bbQQcc -->> bbbbcccc
Still, we cannot apply this rule since normally the QQ's are to the right of the cc; this canbe remedied by allowing a

QQ to hop left over a cc:

3. ccQQ -->> QQcc
We can now finish our derivation:

aaaaaabbccQQQQ (3 times rule 1)
aaaaaabbQQccQQ (rule 3)
aaaaaabbbbccccQQ (rule 2)
aaaaaabbbbccQQcc (rule 3)
aaaaaabbbbQQcccc (rule 3)
aaaaaabbbbbbcccccc (rule 2)

It should be noted that the above derivation only shows that the grammar will producethe right strings, and the reader will still have to convince himself that it will not generate other and incorrect strings.

SSSS -->> aabbcc || aaSSQQ
bbQQcc -->> bbbbcccc

ccQQ -->> QQcc

Figure 2.6 Monotonic grammar for a nb nc n
The grammar is summarized in Figure 2.6; since a derivation tree of a 3b 3c 3 isalready rather unwieldy, a derivation tree for a 2b 2c 2 is given in Figure 2.7. The grammar is monotonic and therefore of Type 1; it can be proved that there is no Type 2grammar for the language.

SS
aa SS QQ

aa bb cc QQ

bb QQ cc
aa aa bb bb cc cc

Figure 2.7 Derivation of a 2b 2c 2

32 Grammars as a generating device [Ch. 2

Type 1 grammars are also called context-sensitive grammars (CS grammars); thelatter name is often used even if the grammar is actually monotonic. There are no standard initials for monotonic, but MT may do.
2.3.2 Type 2 grammarsType 2 grammars are called context-free grammars (CF grammars) and their relation to
context-sensitive grammars is as direct as the name suggests. A context-free grammaris like a context-sensitive grammar, except that both the left and the right contexts are
required to be absent (empty). As a result, the grammar may contain only rules thathave a single non-terminal on their left-hand side. Sample grammar:

0. NNaammee -->> ttoomm || ddiicckk || hhaarrrryy1.

SSeenntteenncceeSS -->> NNaammee || LLiisstt aanndd NNaammee2.

LLiisstt -->> NNaammee ,, LLiisstt || NNaammee

Since there is always only one symbol on the left-hand side, each node in a pro-duction graph has the property that whatever it produces is independent of what its
neighbours produce: the productive life of a non-terminal is independent of its context.Starlike forms as we saw in Figures 2.3, 2.5 or 2.7 cannot occur in a context-free production graph, which consequently has a pure tree-form and is called a production tree.An example is shown in Figure 2.8.

SSeenntteennccee
LLiisstt aanndd NNaammee
NNaammee ,, LLiisstt

NNaammee
ttoomm ,, ddiicckk aanndd hhaarrrryy
Figure 2.8 Production tree for a context-free grammar
Also, since there is only one symbol on the left-hand side, all right-hand sides for agiven non-terminal can always be collected in one grammar rule (we have already done
that in the above grammar) and then each grammar rule reads like a definition of theleft-hand side:
\Gamma  A SSeenntteennccee is either a NNaammee or a LLiisstt followed by aanndd followed by a NNaammee.
\Gamma  A LLiisstt is either a NNaammee followed by a ,, followed by a LLiisstt, or it is a NNaammee.

In the actual world, many things are defined in terms of other things. Context-free

Sec. 2.3] The Chomsky hierarchy of grammars and languages 33
grammars are a very concise way to formulate such interrelationships. An almost trivialexample is the composition of a book, as given in Figure 2.9.

BBooookkSS -->> PPrreeffaaccee CChhaapptteerrSSeeqquueennccee CCoonncclluussiioonn
PPrreeffaaccee -->> ""PPRREEFFAACCEE"" PPaarraaggrraapphhSSeeqquueennccee
CChhaapptteerrSSeeqquueennccee -->> CChhaapptteerr || CChhaapptteerr CChhaapptteerrSSeeqquueennccee

CChhaapptteerr -->> ""CCHHAAPPTTEERR"" NNuummbbeerr PPaarraaggrraapphhSSeeqquueennccee
PPaarraaggrraapphhSSeeqquueennccee -->> PPaarraaggrraapphh || PPaarraaggrraapphh PPaarraaggrraapphhSSeeqquueennccee

PPaarraaggrraapphh -->> SSeenntteenncceeSSeeqquueennccee
SSeenntteenncceeSSeeqquueennccee -->> ......

......
CCoonncclluussiioonn -->> ""CCOONNCCLLUUSSIIOONN"" PPaarraaggrraapphhSSeeqquueennccee

Figure 2.9 A simple (and incomplete!) grammar of a book
Of course, this is a context-free description of a book, so one can expect it to also gen-erate a lot of good-looking nonsense like

PPRREEFFAACCEE
qqwweerrttyyuuiioopp
CCHHAAPPTTEERR VV
aassddffgghhjjkkll
zzxxccvvbbnnmm,,..
CCHHAAPPTTEERR IIII
qqaazzwwssxxeeddccrrffvvttggbb
yyhhnnuujjmmiikkoollpp
CCOONNCCLLUUSSIIOONN
AAllll ccaattss ssaayy bblleerrtt wwhheenn wwaallkkiinngg tthhrroouugghh wwaallllss..

but at least the result has the right structure. The document preparation and text mark-up language SGML# uses this approach to control the basic structure of documents.

A shorter but less trivial example is the language of all elevator motions thatreturn to the same point (a Manhattan turtle restricted to 5th Avenue would make the
same movements)

ZZeerrooMMoottiioonnSS -->> uupp ZZeerrooMMoottiioonn ddoowwnn ZZeerrooMMoottiioonn

|| ddoowwnn ZZeerrooMMoottiioonn uupp ZZeerrooMMoottiioonn
|| ee

(in which we assume that the elevator shaft is infinitely long; they are, in Manhattan).If we ignore enough detail we can also recognize an underlying context-free structure in the sentences of a natural language, for instance, English:
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

David Barron, "Why use SGML?", Electronic Publishing, vol. 2, no. 1, p. 3-24, April 1989.Short introduction to

SGML (Standard Generalized Markup Language) and comparison to othersystems. Provides further references.

34 Grammars as a generating device [Ch. 2

SSeenntteenncceeSS -->> SSuubbjjeecctt VVeerrbb OObbjjeecctt

SSuubbjjeecctt -->> NNoouunnPPhhrraassee

OObbjjeecctt -->> NNoouunnPPhhrraassee
NNoouunnPPhhrraassee -->> tthhee QQuuaalliiffiieeddNNoouunn
QQuuaalliiffiieeddNNoouunn -->> NNoouunn || AAddjjeeccttiivvee QQuuaalliiffiieeddNNoouunn

NNoouunn -->> ccaassttllee || ccaatteerrppiillllaarr || ccaattss
AAddjjeeccttiivvee -->> wweellll--rreeaadd || wwhhiittee || wwiissttffuull || ......

VVeerrbb -->> aaddmmiirreess || bbaarrkk || ccrriittiicciizzee || ......

which produces sentences like:

tthhee wweellll--rreeaadd ccaattss ccrriittiicciizzee tthhee wwiissttffuull ccaatteerrppiillllaarr
Since, however, no context is incorporated, it will equally well produce the incorrect

tthhee ccaattss aaddmmiirreess tthhee wwhhiittee wweellll--rreeaadd ccaassttllee
For keeping context we could use a phrase structure grammar (for a simplerlanguage):

SSeenntteenncceeSS -->> NNoouunn NNuummbbeerr VVeerrbb

NNuummbbeerr -->> SSiinngguullaarr || PPlluurraall
NNoouunn SSiinngguullaarr -->> ccaassttllee SSiinngguullaarr || ccaatteerrppiillllaarr SSiinngguullaarr || ......
SSiinngguullaarr VVeerrbb -->> SSiinngguullaarr aaddmmiirreess || ......

SSiinngguullaarr -->> ee
NNoouunn PPlluurraall -->> ccaattss PPlluurraall || ......
PPlluurraall VVeerrbb -->> PPlluurraall bbaarrkk || PPlluurraall ccrriittiicciizzee || ......

PPlluurraall -->> ee

where the markers SSiinngguullaarr and PPlluurraall control the production of actual Englishwords. Still this grammar allows the cats to bark.... For a better way to handle context,
see the section on van Wijngaarden grammars (2.4.1).The bulk of examples of CF grammars originate from programming languages.
Sentences in these languages (that is, programs) have to be processed automatically(that is, by a compiler) and it was soon recognized (around 1958) that this is a lot easier
if the language has a well-defined formal grammar. The syntaxes of almost all pro-gramming languages in use today are defined through a formal grammar.#

Some authors (for instance, Chomsky) and some parsing algorithms, require a CFgrammar to be monotonic. The only way a CF rule can be non-monotonic is by having
an empty right-hand side; such a rule is called an e-rule and a grammar that contains nosuch rules is called e-free. The requirement of being e-free is not a real restriction, just
a nuisance. Any CF grammar can be made e-free be systematic substitution of the e-rules (this process will be explained in detail in 4.2.3.1), but this in general does not
improve the appearance of the grammar. The issue will be discussed further in Section
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

COBOL and FORTRAN also have grammars but theirs are informal and descriptive, and werenever intended to be generative.

Sec. 2.3] The Chomsky hierarchy of grammars and languages 35
2.6.
2.3.2.1 Backus-Naur FormThere are several different styles of notation for CF grammars for programming
languages, each with endless variants; they are all functionally equivalent. We shallshow two main styles here. The first is Backus-Naur Form (BNF) which was first used
for defining ALGOL 60. Here is a sample:

<<nnaammee>>::::== ttoomm || ddiicckk || hhaarrrryy
<<sseenntteennccee>>SS::::== <<nnaammee>> || <<lliisstt>> aanndd <<nnaammee>>
<<lliisstt>>::::== <<nnaammee>>,, <<lliisstt>> || <<nnaammee>>

This form's main properties are the use of angle brackets to enclose non-terminals andof

::::== for "may produce". In some variants, the rules are terminated by a semicolon.

2.3.2.2 van Wijngaarden formThe second style is that of the CF van Wijngaarden grammars. Again a sample:

nnaammee:: ttoomm ssyymmbbooll;; ddiicckk ssyymmbbooll;; hhaarrrryy ssyymmbbooll..
sseenntteenncceeSS:: nnaammee;; lliisstt,, aanndd ssyymmbbooll,, nnaammee..
lliisstt:: nnaammee,, ccoommmmaa ssyymmbbooll,, lliisstt;; nnaammee..

The names of terminal symbols end in ...ssyymmbbooll; their representations are hardware-dependent and are not defined in the grammar. Rules are properly terminated (with a
period). Punctuation is used more or less in the traditional way; for instance, thecomma binds tighter than the semicolon. The punctuation can be read as follows:

:: "is defined as a(n)"
;; ", or as a (n)"
,, "followed by a(n)"
.. ", and as nothing else."

The second rule in the above grammar would be read as: "a sentence is defined as aname, or as a list followed by an and-symbol followed by a name, and as nothing else."
Although this notation achieves its full power only when applied in the two-level vanWijngaarden grammars, it also has its merits on its own: it is formal and still quite
readable.
2.3.2.3 Extended CF grammarsCF grammars are often made both more compact and more readable by introducing
special short-hands for frequently used constructions. If we return to the Book grammarof Figure 2.9, we see that rules like:

SSoommeetthhiinnggSSeeqquueennccee -->> SSoommeetthhiinngg || SSoommeetthhiinngg SSoommeetthhiinnggSSeeqquueennccee
occur repeatedly. In an extended context-free grammar (ECF grammar), we can write
SSoommeetthhiinngg++ meaning "one or more SSoommeetthhiinnggs" and we do not need to give a rulefor

SSoommeetthhiinngg++; the rule

36 Grammars as a generating device [Ch. 2

SSoommeetthhiinngg++ -->> SSoommeetthhiinngg || SSoommeetthhiinngg SSoommeetthhiinngg++
is implicit. Likewise we can use SSoommeetthhiinngg** for "zero or more SSoommeetthhiinnggs" and
SSoommeetthhiinngg?? for "zero or one SSoommeetthhiinngg" (that is, "optionally a SSoommeetthhiinngg"). Inthese examples, the operators +, * and ? work on the preceding symbol; their range can

be extended by using parentheses: ((SSoommeetthhiinngg ;;))?? means "optionally a
SSoommeetthhiinngg-followed-by-a-;;". These facilities are very useful and allow the Bookgrammar to be written more efficiently (Figure 2.10). Some styles even allow constructions like SSoommeetthhiinngg++44 meaning "one or more SSoommeetthhiinnggs with a maximum of 4" or
SSoommeetthhiinngg++,, meaning "one or more SSoommeetthhiinnggs separated by commas"; this seemsto be a case of overdoing a good thing.

BBooookkSS -->> PPrreeffaaccee CChhaapptteerr++ CCoonncclluussiioonn
PPrreeffaaccee -->> ""PPRREEFFAACCEE"" PPaarraaggrraapphh++
CChhaapptteerr -->> ""CCHHAAPPTTEERR"" NNuummbbeerr PPaarraaggrraapphh++
PPaarraaggrraapphh -->> SSeenntteennccee++

SSeenntteennccee -->> ......

......
CCoonncclluussiioonn -->> ""CCOONNCCLLUUSSIIOONN"" PPaarraaggrraapphh++

Figure 2.10 An extended CF grammar of a book
The extensions of an ECF grammar do not increase its expressive powers: allimplicit rules can be made explicit and then a normal CF grammar results. Their
strength lies in their user-friendliness. The star in the notation X * with the meaning "asequence of zero or more X's" is called the Kleene star. If X is a set, X * should be read
as "a sequence of zero or more elements of X"; it is the same star that we saw in \Sigma * inSection 2.1.3.3. Forms involving the repetition operators **, ++ or ?? and possibly the
separators (( and )) are called regular expressions. ECF's, which have regular expres-sions for their right-hand sides, are for that reason sometimes called regular right part
grammars (RRP grammars) which is more descriptive than "extended context free",but which is perceived to be a tongue twister by some.

There are two different schools of thought about the structural meaning of a regu-lar right-hand side. One school maintains that a rule like:

BBooookk -->> PPrreeffaaccee CChhaapptteerr++ CCoonncclluussiioonn
is an abbreviation of

BBooookk -->> PPrreeffaaccee aa CCoonncclluussiioonnaa

-->> CChhaapptteerr || CChhaapptteerr aa

as shown above. This is the "(right)recursive" interpretation; it has the advantage that itis easy to explain and that the transformation to "normal" CF is simple. Disadvantages
are that the transformation entails anonymous rules (identified by a here) and that thelopsided parse tree for, for instance, a book of four chapters does not correspond to our
idea of the structure of the book; see Figure 2.11.The seconds school claims that

Sec. 2.3] The Chomsky hierarchy of grammars and languages 37

BBooookk
PPrreeffaaccee aa CCoonn--cclluussiioonn

CChhaapptteerr aa

CChhaapptteerr aa

CChhaapptteerr CChhaapptteerr

Figure 2.11 Parse tree for the (right)recursive interpretation

BBooookk -->> PPrreeffaaccee CChhaapptteerr++ CCoonncclluussiioonn
is an abbreviation of

BBooookk -->> PPrreeffaaccee CChhaapptteerr CCoonncclluussiioonn

|| PPrreeffaaccee CChhaapptteerr CChhaapptteerr CCoonncclluussiioonn
|| PPrreeffaaccee CChhaapptteerr CChhaapptteerr CChhaapptteerr CCoonncclluussiioonn
|| ......
......

This is the "iterative" interpretation; it has the advantage that it yields a beautiful parsetree (Figure 2.12), but the disadvantages that it involves an infinite number of production rules and that the nodes in the parse tree have a varying fan-out.

BBooookk
PPrreeffaaccee CChhaapptteerr CChhaapptteerr CChhaapptteerr CChhaapptteerr CCoonn--cclluussiioonn

Figure 2.12 Parse tree for the iterative interpretation
Since the implementation of the iterative interpretation is far from trivial, mostpractical parser generators use the recursive interpretation in some form or another,
whereas most research has been done on the iterative interpretation.
2.3.3 Type 3 grammarsThe basic property of CF grammars is that they describe things that nest: an object may
contain other objects in various places, which in turn may contain ... etc. When duringthe production process we have produced one of the objects, the right-hand side still
"remembers" what has to come after it: in the English grammar, after having descendedinto the depth of the non-terminal

SSuubbjjeecctt to produce something like tthhee wwiissttffuull
ccaatt, the right-hand side SSuubbjjeecctt VVeerrbb OObbjjeecctt still remembers that a VVeerrbb must

38 Grammars as a generating device [Ch. 2
follow. While we are working on the SSuubbjjeecctt, the VVeerrbb and OObbjjeecctt remain queuedat the right in the sentential form, for instance,

tthhee wwiissttffuull QQuuaalliiffiieeddNNoouunn VVeerrbb OObbjjeecctt
In the right-hand side

uupp ZZeerrooMMoottiioonn ddoowwnn ZZeerrooMMoottiioonn
after having performed the uupp and an arbitrarily complicated ZZeerrooMMoottiioonn, the right-hand side still remembers that a

ddoowwnn must follow.The restriction to Type 3 disallows this recollection of things that came before: a

right-hand side may only contain one non-terminal and it must come at the end. Thismeans that there are only two kinds of rules:#

A non-terminal produces zero or more terminalsA non-terminal produces zero or more terminals followed by one non-terminal
The original Chomsky definition of Type 3 restricts the kinds of rules to

A non-terminal produces one terminalA non-terminal produces one terminal followed by one non-terminal

Our definition is equivalent and more convenient, although the conversion to ChomskyType 3 is not completely trivial.

Type 3 grammars are also called regular grammars (RE grammars) or finite-stategrammars (FS grammars). Since regular grammars are used very often to describe the
structure of text on the character level, it is customary for the terminal symbols of aregular grammar to be single characters. We shall therefore write

tt for TToomm, dd for
DDiicckk, hh for HHaarrrryy and && for aanndd. Figure 2.13 shows a Type 3 grammar for our t,d&hlanguage in this style.

SSeenntteenncceeSS -->> tt || dd || hh || LLiisstt

LLiisstt -->> tt LLiissttTTaaiill || dd LLiissttTTaaiill || hh LLiissttTTaaiill
LLiissttTTaaiill -->> ,, LLiisstt || &&tt || &&dd || &&hh

Figure 2.13 A Type 3 grammar for the t, d & h language
The production tree for a sentence from a Type 3 grammar degenerates into achain of non-terminals that drop a sequence of terminals on their left. Figure 2.14
shows an example.The deadly repetition exhibited by the above grammar is typical of regular grammars; a number of notational devices have been invented to abate this nuisance. The
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

There is a natural in-between class, Type 2.5 so to say, in which only a single non-terminal isallowed in a right-hand side, but where it need not be at the end. This gives us the so-called

linear grammars.

Sec. 2.3] The Chomsky hierarchy of grammars and languages 39

SSeenntteennccee

LLiisstt
tt LLiissttTTaaiill

,, LLiisstt

dd LLiissttTTaaiill

&& hh

Figure 2.14 Production chain for a regular (Type 3) grammar
most common one is the use of square brackets to indicate "one out of a set of charac-ters":

[[ttddhh]] is an abbreviation for tt||dd||hh:

SSSS -->> [[ttddhh]] || LL

LL -->> [[ttddhh]] TT
TT -->> ,, LL || && [[ttddhh]]

which may look more cryptic at first but is actually much more convenient and in factallows simplification of the grammar to

SSSS -->> [[ttddhh]] || LL

LL -->> [[ttddhh]] ,, LL || [[ttddhh]] && [[ttddhh]]

A second way is to allow macros, names for pieces of the grammar that are substi-tuted properly into the grammar before it is used:

NNaammee -->> tt || dd || hh

SSSS -->> $NNaammee || LL

LL -->> $NNaammee ,, LL || $NNaammee && $NNaammee

The popular parser generator for regular grammars lex (designed and written by Leskand Schmidt [FS 1975]) features both facilities.

Note that if we adhere to the Chomsky definition of Type 3, our grammar will notget smaller than:

SSSS -->> tt || dd || hh || ttMM || ddMM || hhMM

MM -->> ,,NN || &&PP
NN -->> ttMM || ddMM || hhMM
PP -->> tt || dd || hh

This form is evidently easier to process but less user-friendly than the lex version. We

40 Grammars as a generating device [Ch. 2
observe here that while the formal-linguist is interested in and helped by minimallysufficient means, the computer scientist values a form in which the concepts underlying
the grammar ($NNaammee, etc.) are easily expressed, at the expense of additional processing.There are two interesting observations about regular grammars which we want to
make here. First, when we use a RE grammar for generating a sentence, the sententialforms will only contain one non-terminal and this will always be at the end; that's
where it all happens (using the grammar of Figure 2.13):

SSeenntteenncceeSS
LLiisstt
tt LLiissttTTaaiill
tt ,, LLiisstt
tt ,, dd LLiissttTTaaiill
tt ,, dd && hh

The second observation is that all regular grammars can be reduced considerablyin size by using the regular expression operators **, ++ and ?? introduced in Section 2.3.2
for "zero or more", "one or more" and "optionally one", respectively. Using theseoperators and

(( and )) for grouping, we can simplify our grammar to:

SSSS -->> (((( [[ttddhh]],, ))** [[ttddhh]]&& ))?? [[ttddhh]]
Here the parentheses serve to demarcate the operands of the ** and ?? operators. Regularexpressions exist for all Type 3 grammars. Note that the ** and the ++ work on what precedes them; to distinguish them from the normal multiplication and addition operators,they are often printed higher than the level text in print, but in computer input they are
in line with the rest.
2.3.4 Type 4 grammarsThe last restriction we shall apply to what is allowed in a production rule is a pretty
final one: no non-terminal is allowed in the right-hand side. This removes all the gen-erative power from the mechanism, except for the choosing of alternatives. The start
symbol has a (finite) list of alternatives from which we are allowed to choose; this isreflected in the name finite-choice grammar (FC grammar).

There is no FC grammar for our t,d&h language; if, however, we are willing torestrict ourselves to lists of names of a finite length (say, no more than a hundred), then
there is one, since one could enumerate all combinations. For the obvious limit of threenames, we get:

SSSS -->> [[ttddhh]] || [[ttddhh]] && [[ttddhh]] || [[ttddhh]] ,, [[ttddhh]] && [[ttddhh]]
for a total of 3+3* 3+3* 3* 3=39 production rules.FC grammars are not part of the official Chomsky hierarchy, that is, they are not
identified by Chomsky. They are nevertheless very useful and are often required as atail-piece in some process or reasoning. The set of reserved words (keywords) in a programming language can be described by a FC grammar. Although not many grammarsare FC in their entirety, some of the rules in many grammars are finite-choice. E.g., the
first rule of our first grammar (Figure 2.2) was FC. Another example of a FC rule was

Sec. 2.3] The Chomsky hierarchy of grammars and languages 41
the macro introduced in Section 2.3.3; we do not need the macro mechanism if wechange

zero or more terminals
in the definition of a regular grammar to

zero or more terminals or FC non-terminals
In the end, the FC non-terminals will only introduce a finite number of terminals.

2.4 VW GRAMMARS
2.4.1 The human inadequacy of CS and PS grammarsIn the preceding paragraphs we have witnessed the introduction of a hierarchy of grammar types:- phrase structure,
- context-sensitive,- context-free,
- regular and- finite-choice.
Although each of the boundaries between the types is clear-cut, some boundaries aremore important than others. Two boundaries specifically stand out: that between
context-sensitive and context-free and that between regular (finite-state) and finite-choice; the significance of the latter is trivial, being the difference between productive
and non-productive, but the former is profound.The border between CS and CF is that between global correlation and local
independence. Once a non-terminal has been produced in a sentential form in a CFgrammar, its further development is independent of the rest of the sentential form; a
non-terminal in a sentential form of a CS grammar has to look at its neighbours on theleft and on the right, to see what production rules are allowed for it. The local production independence in CF grammars means that certain long-range correlations cannotbe expressed by them. Such correlations are, however, often very interesting, since they
embody fundamental properties of the input text, like the consistent use of variables ina program or the recurrence of a theme in a musical composition. When we describe
such input through a CF grammar we cannot enforce the proper correlations; one(often-used) way out is to settle for the CF grammar, accept the parsing it produces and
then check the proper correlations with a separate program. This is, however, quiteunsatisfactory since it defeats the purpose of having a grammar, that is, having a concise and formal description of all the properties of the input.The obvious solution would seem to be the use of a CS grammar to express the
correlations (= the context-sensitivity) but here we run into another, non-fundamentalbut very practical problem: CS grammars can express the proper correlations but not in
a way a human can understand. It is in this respect instructive to compare the CF gram-mars in Section 2.3.2 to the one CS grammar we have seen that really expresses a
context-dependency, the grammar for a nb nc n in Figure 2.6. The grammar for the con-tents of a book (Figure 2.9) immediately suggests the form of the book, but the

42 Grammars as a generating device [Ch. 2
grammar of Figure 2.6 hardly suggests anything, even if we can still remember how itwas constructed and how it works. This is not caused by the use of short names like

QQ:a version with more informative names (Figure 2.15) is still puzzling. Also, one would

expect that, having constructed a grammar for a nb nc n, making one for a nb nc nd nwould be straightforward. Such is not the case; a grammar for a nb nc nd n is substantially more complicated (and even more opaque) than one for a nb nc n and requiresrethinking of the problem.

SSSS -->> aa bb cc || aa SS bbcc__ppaacckk
bb bbcc__ppaacckk cc -->> bb bb cc cc

cc bbcc__ppaacckk -->> bbcc__ppaacckk cc

Figure 2.15 Monotonic grammar for a nb nc n with more informative names
The cause of all this misery is that CS and PS grammars derive their power toenforce global relationships from "just slightly more than local dependency". Theoretically, just looking at the neighbours can be proved to be enough to express any globalrelation, but the enforcement of a long-range relation through this mechanism causes
information to flow through the sentential form over long distances. In the productionprocess of, for instance, a 4b 4c 4, we see several

bbcc__ppaacckks wind their way through thesentential form, and in any serious CS grammar, many messengers run up and down the

sentential form to convey information about developments in far-away places. How-ever interesting this imagery may seem, it requires almost all rules to know something
about almost all other rules; this makes the grammar absurdly complex.Several grammar forms have been put forward to remedy this situation and make
long-range relationships more easily expressible; among them are indexed grammars(Aho [PSCS 1968]), recording grammars (Barth [PSCS 1979]), affix grammars
(Koster [VW 1971]) and VW grammars (van Wijngaarden [VW 1969]). The last arethe most elegant and effective, and are explained below. Affix grammars are discussed
briefly in 2.4.5.
2.4.2 VW grammarsIt is not quite true that CF grammars cannot express long-range relations; they can only
express a finite number of them. If we have a language the strings of which consist of a
bbeeggiinn, a mmiiddddllee and an eenndd and suppose there are three types of bbeeggiinns and eenndds,then the CF grammar of Figure 2.16 will enforce that the type of the

eenndd will properlymatch that of the
bbeeggiinn.

tteexxttSS -->> bbeeggiinn11 mmiiddddllee eenndd11

|| bbeeggiinn22 mmiiddddllee eenndd22
|| bbeeggiinn33 mmiiddddllee eenndd33

Figure 2.16 A long-range relation-enforcing CF grammar
We can think of (( and )) for bbeeggiinn11 and eenndd11, [[ and ]] for bbeeggiinn22 and eenndd22 and {{and

}} for bbeeggiinn33 and eenndd33; the CF grammar will then ensure that closing parentheseswill match the corresponding open parentheses.

By making the CF grammar larger and larger, we can express more and more

Sec. 2.4] VW grammars 43
long-range relations; if we make it infinitely large, we can express any number oflong-range relations and have achieved full context-sensitivity. Now we come to the
fundamental idea behind VW grammars. The rules of the infinite-size CF grammarform an infinite set of strings, i.e., a language, which can in turn be described by a
grammar. This explains the name "two-level grammar".To introduce the concepts and techniques we shall give here an informal construction of a VW grammar for the above language L = a nb nc n for n>=1. We shall use theVW notation as explained in 2.3.2.2: the names of terminal symbols end in

ssyymmbbooll andtheir representations are given separately; alternatives are separated by semicolons (

;;),members inside alternatives are separated by commas (which allows us to have spaces

in the names of non-terminals) and a colon (::) is used instead of an arrow.We could describe the language L through a context-free grammar if grammars of
infinite size were allowed:

tteexxttSS:: aa ssyymmbbooll,, bb ssyymmbbooll,, cc ssyymmbbooll;;

aa ssyymmbbooll,, aa ssyymmbbooll,,

bb ssyymmbbooll,, bb ssyymmbbooll,,
cc ssyymmbbooll,, cc ssyymmbbooll;;
aa ssyymmbbooll,, aa ssyymmbbooll,, aa ssyymmbbooll,,

bb ssyymmbbooll,, bb ssyymmbbooll,, bb ssyymmbbooll,,
cc ssyymmbbooll,, cc ssyymmbbooll,, cc ssyymmbbooll;;
...... ......

We shall now try to master this infinity by constructing a grammar which allowsus to produce the above grammar for as far as needed. We first introduce an infinite
number of names of non-terminals:

tteexxttSS:: aaii,, bbii,, ccii;;

aaiiii,, bbiiii,, cciiii;;
aaiiiiii,, bbiiiiii,, cciiiiii;;
...... ......

together with three infinite groups of rules for these non-terminals:

aaii:: aa ssyymmbbooll..
aaiiii:: aa ssyymmbbooll,, aaii..
aaiiiiii:: aa ssyymmbbooll,, aaiiii..
...... ......

bbii:: bb ssyymmbbooll..
bbiiii:: bb ssyymmbbooll,, bbii..
bbiiiiii:: bb ssyymmbbooll,, bbiiii..
...... ......

44 Grammars as a generating device [Ch. 2

ccii:: cc ssyymmbbooll..
cciiii:: cc ssyymmbbooll,, ccii..
cciiiiii:: cc ssyymmbbooll,, cciiii..
...... ......

Here the ii's count the number of aa's, bb's and cc's. Next we introduce a specialkind of name called a metanotion. Rather than being capable of producing (part of) a
sentence in the language, it is capable of producing (part of) a name in a grammar rule.In our example we want to catch the repetitions of

ii's in a metanotion NN, for which wegive a context-free production rule (a metarule):

NN :::: ii ;; ii NN ..
Note that we use a slightly different notation for metarules: left-hand side and right-hand side are separated by a double colon (

::::) rather than by a single colon andmembers are separated by a blank ( ) rather than by a comma. The metanotion

NN pro-duces
ii, iiii, iiiiii, etc., which are exactly the parts of the non-terminal names we need.We can use the production rules of

NN to collapse the four infinite groups of rulesinto four finite rule templates called hyper-rules.

tteexxttSS:: aa NN,, bb NN,, cc NN..
aa ii:: aa ssyymmbbooll..
aa ii NN:: aa ssyymmbbooll,, aa NN..

bb ii:: bb ssyymmbbooll..
bb ii NN:: bb ssyymmbbooll,, bb NN..

cc ii:: cc ssyymmbbooll..
cc ii NN:: cc ssyymmbbooll,, cc NN..

Each original rule can be obtained from one of the hyper-rules by substituting aproduction of

NN from the metarules for each occurrence of NN in that hyper-rule, pro-vided that the same production of

NN is used consistently throughout. To distinguishthem from normal names, these half-finished combinations of small letters and metanotions (like aa NN or bb ii NN) are called hypernotions. Substituting, for instance, NN=iiiiii inthe hyperrule

bb ii NN:: bb ssyymmbbooll,, bb NN..
yields the CF rule for the CF non-terminal bbiiiiiiii

bbiiiiiiii:: bb ssyymmbbooll,, bbiiiiii..
We can also use this technique to condense the finite parts of a grammar by hav-ing a metarule

AA for the symbols aa, bb and cc. Again the rules of the game require thatthe metanotion

AA be replaced consistently. The final result is shown in Figure 2.17.This grammar gives a clear indication of the language it describes: once the

Sec. 2.4] VW grammars 45

NN :::: ii ;; ii NN ..
AA :::: aa ;; bb ;; cc ..

tteexxttSS:: aa NN,, bb NN,, cc NN..
AA ii:: AA ssyymmbbooll..
AA ii NN:: AA ssyymmbbooll,, AA NN..

Figure 2.17 A VW grammar for the language a nb nc n
"value" of the metanotion NN is chosen, production is straightforward. It is now trivial toextend the grammar to a nb nc nd n. It is also clear how long-range relations are established without having confusing messengers in the sentential form: they are esta-blished before they become long-range, through consistent substitution of metanotions
in simple right-hand sides. The "consistent substitution rule" for metanotions is essen-tial to the two-level mechanism; without it, VW grammars would be equivalent to CF
grammars (Meersman and Rozenberg [VW 1978]).A very good and detailed explanation of VW grammars has been written by Craig
Cleaveland and Uzgalis [VW 1977], who also show many applications. Sintzoff [VW1967] has proved that VW grammars are as powerful as PS grammars, which also
shows that adding a third level to the building cannot increase its powers. VanWijngaarden [VW 1974] has shown that the metagrammar need only be regular
(although simpler grammars may be possible if it is allowed to be CF).
2.4.3 Infinite symbol setsIn a sense, VW grammars are even more powerful than PS grammars: since the name
of a symbol can be generated by the grammar, they can easily handle infinite symbolsets. Of course this just shifts the problem: there must be a (finite) mapping from symbol names to symbols somewhere. The VW grammar of Figure 2.18 generates sen-tences consisting of arbitrary numbers of equal-length stretches of equal symbols, for
instance, s 1s 1s 1s 2s 2s 2 or s 1s 1s 2s 2s 3s 3s 4s 4s 5s 5, where sn is the representation of
iinn ssyymmbbooll. The minimum stretch length has been set to 2, to prevent the grammarfrom producing \Sigma *.

NN :::: nn NN;; ee ..
CC :::: ii;; ii CC..

tteexxttSS:: NN ii ttaaiill..
NN CC ttaaiill:: ee;; NN CC,, NN CC ii ttaaiill..
NN nn CC :: CC ssyymmbbooll,, NN CC..
CC :: ee..

Figure 2.18 A grammar handling an infinite alphabet
2.4.4 BNF notation for VW grammarsThere is a different notation for VW grammars, sometimes used in formal language
theory (for instance, Greibach [VW 1974]), which derives from the BNF notation (seeSection 2.3.2.1). A BNF form of our grammar from Figure 2.17 is given in Figure 2.19;
hypernotions are demarcated by angle brackets and terminal symbols are represented

46 Grammars as a generating device [Ch. 2
by themselves.

NN -->> ii || ii NN
AA -->> aa || bb || cc

<<tteexxtt>>SS -->> <<aaNN>> <<bbNN>> <<ccNN>>

<<AAii>> -->> AA
<<AAiiNN>> -->> AA <<AANN>>

Figure 2.19 The VW grammar of Figure 2.17 in BNF notation
2.4.5 Affix grammarsLike VW grammars, affix grammars establish long-range relations by duplicating
information in an early stage; this information is, however, not part of the non-terminalname, but is passed as an independent parameter, an affix, which can, for instance, be
an integer value. Normally these affixes are passed on to the members of a rule, untilthey are passed to a special kind of non-terminal, a primitive predicate. Rather than
producing text, a primitive predicate contains a legality test. For a sentential form to belegal, all the legality tests in it have to succeed. The affix mechanism is equivalent to
the VW metanotion mechanism, is slightly easier to handle while parsing and slightlymore difficult to use when writing a grammar.

An affix grammar for a nb nc n is given in Figure 2.20. The first two lines are affixdefinitions for

NN, MM, AA and BB. Affixes in grammar rules are traditionally preceded by a
++. The names of the primitive predicates start with wwhheerree. To produce aabbcc, start with
tteexxtt ++ 11; this produces

lliisstt ++ 11 ++ aa,, lliisstt ++ 11 ++ bb,, lliisstt ++ 11 ++ cc
The second member of this, for instance, produces

lleetttteerr ++ bb,, wwhheerree iiss ddeeccrreeaasseedd ++ 00 ++ 11,, lliisstt ++ 00 ++ bb
the first member of which produces

wwhheerree iiss ++ bb ++ bb,, bb ssyymmbbooll..
All the primitive predicates in the above are fulfilled, which makes the final sentencelegal. An attempt to let

lleetttteerr ++ bb produce aa ssyymmbbooll introduces the primitivepredicate
wwhheerree iiss ++ aa ++ bb which fails, invalidating the sentential form.Affix grammars have largely been replaced by attribute grammars, which achieve

roughly the same effect through similar but conceptually different means (see Section2.9.1).

Sec. 2.4] VW grammars 47

NN,, MM:::: iinntteeggeerr..
AA,, BB:::: aa;; bb;; cc..

tteexxttSS ++ NN:: lliisstt ++ NN ++ aa,, lliisstt ++ NN ++ bb,, lliisstt ++ NN ++ cc..
lliisstt ++ NN ++ AA:: wwhheerree iiss zzeerroo ++ NN;;

lleetttteerr ++ AA,, wwhheerree iiss ddeeccrreeaasseedd ++ MM ++ NN,,

lliisstt ++ MM ++ AA..

lleetttteerr ++ AA:: wwhheerree iiss ++ AA ++ aa,, aa ssyymmbbooll;;

wwhheerree iiss ++ AA ++ bb,, bb ssyymmbbooll;;
wwhheerree iiss ++ AA ++ cc,, cc ssyymmbbooll..

wwhheerree iiss zzeerroo ++ NN:: {{NN == 00}}..
wwhheerree iiss ddeeccrreeaasseedd

++ MM ++ NN:: {{MM == NN -- 11}}..

wwhheerree iiss ++ AA ++ BB:: {{AA == BB}}..

Figure 2.20 Affix grammar for a nb nc n

2.5 ACTUALLY GENERATING SENTENCES FROM A GRAMMAR
2.5.1 The general caseUntil now we have only produced single sentences from our grammars, in an ad hoc
fashion, but the purpose of a grammar is to generate all its sentences. Fortunately thereis a systematic way to do so. We shall use the a nb nc n grammar as an example. We
start from the start symbol and systematically make all possible substitutions to gen-erate all sentential forms; we just wait and see which ones evolve into sentences and
when. Try this by hand for, say, 10 sentential forms. If we are not careful, we are apt togenerate forms like aSQ, aaSQQ, aaaSQQQ,... only and we will never see a finished
sentence. The reason is that we focus too much on a single sentential form; we have togive equal time to all of them. This can be done through the following algorithm, which
keeps a queue (that is, a list to which we add at the end and remove from the begin-ning), of sentential forms.

Start with the start symbol as the only sentential form in the queue. Now continuedoing the following:
\Gamma  Consider the first sentential form in the queue.
\Gamma  Scan it from left to right, looking for strings of symbols that match the left-hand

side of a production rule.
\Gamma  For each such string found, make enough copies of the sentential form, replace in

each one the string that matched a left-hand side of a rule by a different alternativeof that rule, and add them all to the end of the queue.

\Gamma  If the original sentential form did not contain any non-terminals, write it down as

a sentence in the language.

48 Grammars as a generating device [Ch. 2
\Gamma  Throw away the sentential form; it has been fully processed.

If no rule matched, and the sentential form was not a finished sentence, it was ablind alley; they are removed automatically by the above process and leave no trace.

The first couple of steps of this process for our a nb nc n grammar from Figure 2.6are depicted in Figure 2.21. The queue runs to the right, with the first item on the left.

Step Queue Result1

SS2
aabbcc aaSSQQ aabbcc3
aaSSQQ4
aaaabbccQQ aaaaSSQQQQ5
aaaaSSQQQQ aaaabbQQcc6
aaaabbQQcc aaaaaabbccQQQQ aaaaaaSSQQQQQQ7
aaaaaabbccQQQQ aaaaaaSSQQQQQQ aaaabbbbcccc8
aaaaaaSSQQQQQQ aaaabbbbcccc aaaaaabbQQccQQ9
aaaabbbbcccc aaaaaabbQQccQQ aaaaaaaabbccQQQQQQ aaaaaaaaSSQQQQQQQQ aaaabbbbcccc10
aaaaaabbQQccQQ aaaaaaaabbccQQQQQQ aaaaaaaaSSQQQQQQQQ11
aaaaaaaabbccQQQQQQ aaaaaaaaSSQQQQQQQQ aaaaaabbbbccccQQ aaaaaabbQQQQcc
...... ......

Figure 2.21 The first couple of steps in producing for a nb nc n
We see that we do not get a sentence for each time we turn the crank; in fact, in thiscase real sentences will get scarcer and scarcer. The reason is of course that as the process progresses, more and more side lines develop, which all require equal attention.Still, we can be certain that every sentence that can be produced, will in the end be produced: we leave no stone unturned. This way of doing things is called breadth-first pro-duction; computers are better at it than people.

It is tempting to think that it is unnecessary to replace all left-hand sides that wefound in the top-most sentential form. Why not just replace the first one and wait for
the resulting sentential form to come up again and then do the next one? This is wrong,however, since doing the first one may ruin the context for doing the second one. A
simple example is the grammar

SSSS -->> AACC

AA -->> bb
AACC -->> aacc

First doing AA-->>bb will lead to a blind alley and the grammar will produce nothing.Doing both possible substitutions will lead to the same blind alley, but then there will
also be a second sentential form, aacc. This is also an example of a grammar for whichthe queue will get empty after a (short) while.

If the grammar is context-free there is no context to ruin and it is quite safe to justreplace the first match.
There are two remarks to be made here. First, it is not at all certain that we willindeed obtain a sentence for all our effort: it is quite possible that every new sentential
form again contains non-terminals. We should like to know this in advance by examin-ing the grammar, but it can be proven that it is in general impossible to do so. The

Sec. 2.5] Actually generating sentences from a grammar 49
formal-linguist says "It is undecidable whether a PS grammar produces the empty set",which means that there cannot be an algorithm that will for every PS grammar
correctly tell if the grammar produces at least one sentence. This does not mean that wecannot prove for some given grammar that it generates nothing, if that is the case, only
that the proof method used will not work for all grammars: we could have a programthat correctly says Yes in finite time if the answer is Yes but that takes infinite time if
the answer is No; in fact, our generating procedure above is such an algorithm thatgives the correct Yes/No answer in infinite time (although we can have an algorithm
that gives a Yes/Don't know answer in finite time). Although it is true that because ofsome deep theorem in formal linguistics we cannot always get exactly the answer we
want, this does not prevent us from obtaining all kinds of useful information that getsclose. We shall see that this is a recurring phenomenon. The computer scientist is
aware of but not daunted by the impossibilities from formal linguistics.The second remark is that when we do get sentences from the above production
process, they may be produced in an unpredictable order. For non-monotonic grammarsthe sentential forms may grow for a while and then suddenly shrink again, perhaps to
the empty string. Formal linguistics says that there cannot be an algorithm that for allPS grammars will produce their sentences in increasing (actually "non-decreasing")
length.The production of all sentences from a van Wijngaarden grammar poses a special
problem in that there are effectively infinitely many left-hand sides to match with. Fora technique to solve this problem, see Grune [VW 1984].

2.5.2 The CF caseWhen we generate sentences from a CF grammar, many things are a lot simpler. It can
still happen that our grammar will never produce a sentence, but now we can test forthat beforehand, as follows. First scan the grammar to find all non-terminals that have a
right-hand side that contains terminals only or is empty. These non-terminals areguaranteed to produce something. Now scan again to find non-terminals that have a
right-hand side that consists of only terminals and non-terminals that are guaranteed toproduce something. This will give us new non-terminals that are guaranteed to produce
something. Repeat this until we find no more new such non-terminals. If we have notmet the start symbol this way, it will not produce anything.

Furthermore we have seen that if the grammar is CF, we can afford to just rewritethe left-most non-terminal every time (provided we rewrite it into all its alternatives).
Of course we can also consistently rewrite the right-most non-terminal; bothapproaches are similar but different. Using the grammar

0. NN -->> tt || dd || hh1.

SSSS -->> NN || LL && NN2.

LL -->> NN ,, LL || NN

let us follow the adventures of the sentential form that will eventually result in d,h&h.Although it will go several times up and down the production queue, we only depict
here what changes are made to it. We show the sentential forms for left-most andright-most substitution, with the rules and alternatives involved; for instance, (1b)
means rule 1 alternative b.

50 Grammars as a generating device [Ch. 2

SS SS1b 1b
LL&&NN LL&&NN2a 0c
NN,,LL&&NN LL&&hh0b 2a
dd,,LL&&NN NN,,LL&&hh2b 2b
dd,,NN&&NN NN,,NN&&hh0c 0c
dd,,hh&&NN NN,,hh&&hh0c 0b
dd,,hh&&hh dd,,hh&&hh
The sequences of production rules used are not as similar as we would expect; ofcourse, in grand total the same rules and alternatives are applied but the sequences are
neither equal nor each other's mirror image, nor is there any other obvious relationship.Still both define the same production tree:

SS
LL NN
NN LL

NN
dd ,, hh && hh

but if we number the non-terminals in it in the order they were rewritten, we would getdifferent numberings:

SS
LL NN
NN LL

NN
dd ,, hh && hh

1
2 6
3 4

5

Left-most derivation order

SS
LL NN
NN LL

NN
dd ,, hh && hh

1
3 2
6 4

5

Right-most derivation order
The sequence of production rules used in left-most rewriting is called the left-mostderivation of a sentence. We do not have to indicate where each rule must be applied

Sec. 2.5] Actually generating sentences from a grammar 51
and need not even give its rule number; both are implicit in the left-most substitution.A right-most derivation is defined in the obvious way.

The production sequence SS o"" LL&&NN o"" NN,,LL&&NN o"" dd,,LL&&NN o"" dd,,NN&&NN o"" dd,,hh&&NN o""
dd,,hh&&hh can be abbreviated to SS o""*l dd,,hh&&hh. Likewise, the sequence SS o"" LL&&NN o"" LL&&hh o""
NN,,LL&&hh o"" NN,,NN&&hh o"" NN,,hh&&hh o"" dd,,hh&&hh can be abbreviated to SS o""*r dd,,hh&&hh. The fact that
SS produces dd,,hh&&hh in any way is written as SS o""* dd,,hh&&hh.The task of parsing is to reconstruct the parse tree (or graph) for a given input

string, but some of the most efficient parsing techniques can be understood more easilyif viewed as attempts to reconstruct a left- or right-most derivation of the input string;
the parse tree then follows automatically. This is why the notion "[left|right]-mostderivation" will occur frequently in this book (note the FC grammar used here).

2.6 TO SHRINK OR NOT TO SHRINK
In the previous paragraphs, we have sometimes been explicit as to the question if aright-hand side of a rule may be shorter than its left-hand side and sometimes we have

been vague. Type 0 rules may definitely be of the shrinking variety, monotonic rulesdefinitely may not, and Type 2 and 3 rules can shrink only by producing empty (e), that
much is sure.The original Chomsky hierarchy [Misc 1959] was very firm on the subject: only
Type 0 rules are allowed to make a sentential form shrink. Type 1 to 3 rules are allmonotonic. Moreover, Type 1 rules have to be of the context-sensitive variety, which
means that only one of the non-terminals in the left-hand side is actually allowed to bereplaced (and then not by e). This makes for a proper hierarchy in which each next
class is a proper subset of its parent and in which all derivation graphs except for thoseof Type 0 grammars are actually derivation trees.

As an example consider the grammar for the language a nb nc n given in Figure2.6:

1. SSSS -->> aabbcc || aaSSQQ2.

bbQQcc -->> bbbbcccc3.

ccQQ -->> QQcc

which is monotonic but not context-sensitive in the strict sense. It can be made CS byexpanding the offending rule 3 and introducing a non-terminal for

cc:

1. SSSS -->> aabbCC || aaSSQQ2.

bbQQCC -->> bbbbCCCC3a.

CCQQ -->> CCXX3b.
CCXX -->> QQXX3c.
QQXX -->> QQCC4.

CC -->> cc

Now the production graph of Figure 2.7 turns into a production tree:

52 Grammars as a generating device [Ch. 2

SS
aa SS QQ

aa bb CC QQ

CC XX
QQ XX
bb QQ CC
bb bb CC CC
aa aa bb bb cc cc

. . . . . . . . . . . .

There is an additional reason for shunning e-rules: they make both proofs andparsers more complicated. So the question arises why we should bother with e-rules at
all; the answer is that they are very convenient for the grammar writer and user.If we have a language that is described by a CF grammar with e-rules and we
want to describe it by a grammar without e-rules, then that grammar will almost alwaysbe more complicated. Suppose we have a system that can be fed bits of information,
like: "Amsterdam is the capital of the Netherlands", "Truffles are expensive", and canthen be asked a question. On a very superficial level we can define its input as:

iinnppuuttSS:: zzeerroo--oorr--mmoorree--bbiittss--ooff--iinnffoo qquueessttiioonn
or, in an extended notation

iinnppuuttSS:: bbiitt--ooff--iinnffoo** qquueessttiioonn
Since zzeerroo--oorr--mmoorree--bbiittss--ooff--iinnffoo will, among other strings, produce the emptystring, at least one of the rules used in its grammar will be an e-rule; the ** in the
extended notation already implies an e-rule somewhere. Still, from the user's point ofview, the above definition of input neatly fits the problem and is exactly what we want.

Any attempt to write an e-free grammar for this input will end up defining anotion that comprises some of the later

bbiittss--ooff--iinnffoo together with the qquueessttiioonn(since the
qquueessttiioonn is the only non-empty part, it must occur in all rules involved!);but such a notion does not fit our problem at all and is an artifact:

iinnppuuttSS:: qquueessttiioonn--pprreecceeddeedd--bbyy--iinnffoo
qquueessttiioonn--pprreecceeddeedd--bbyy--iinnffoo:: qquueessttiioonn

|| bbiitt--ooff--iinnffoo qquueessttiioonn--pprreecceeddeedd--bbyy--iinnffoo

As a grammar becomes more and more complicated, the requirement that it be e-freebecomes more and more a nuisance: the grammar is working against us, not for us.

This presents no problem from a theoretical point of view: any CF language can

Sec. 2.6] To shrink or not to shrink 53
be described by an e-free CF grammar and e-rules are never needed. Better still, anygrammar with e-rules can be mechanically transformed into an e-free grammar for the
same language; we saw an example of such a transformation above and details of thealgorithm are given in Section 4.2.3.1. But the price we pay is that of any grammar
transformation: it is no longer our grammar and it reflects the original structure lesswell.

The bottom line is that the practitioner finds the e-rule to be a useful tool, and itwould be interesting to see if there exists a hierarchy of non-monotonic grammars
alongside the usual Chomsky hierarchy. To a large extend there is: Type 2 and Type 3grammars need not be monotonic (since they can always be made so if the need arises);
it turns out that context-sensitive grammars with shrinking rules are equivalent tounrestricted Type 0 grammars; and monotonic grammars with e-rules are also
equivalent to Type 0 grammars. We can now draw the two hierarchies in one picture;see Figure 2.22.

\Gamma \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma Chomsky (monotonic)

hierarchy non-monotonichierarchy\Gamma \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

global Type 0

unrestrictedphrase structure grammars

monotonicgrammars
with e-rules

unrestricted phrasestructure grammars
\Theta \Theta 
\Theta \Theta 

\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta 

\Gamma \Lambda \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma production
effects Type 1 context-sensitive

grammars

monotonicgrammars
without e-rules

context-sensitivegrammars with nonmonotonic rules\Theta \Theta \Theta 
\Theta \Theta 
\Theta 

\Gamma \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

local Type 2 context-free e-free grammars context-free gram-mars\Theta \Theta 

\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Gamma \Lambda \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma production
effects Type 3 regular (e-free) grammars regular grammarsregular expressions\Theta \Theta 

\Theta 

\Gamma \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma no production Type 4 finite-choice
\Gamma \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

Figure 2.22 Summary of grammar hierarchies
Drawn lines separate grammar types with different power, broken lines separate con-ceptually different grammar types with the same power. We see that if we insist on
non-monotonicity, the distinction between Type 0 and Type 1 disappears.A special case arises if the language of a Type 1 to Type 3 grammar itself contains
the empty string. This cannot be incorporated into the grammar in the monotonichierarchy since the start symbol has already length 1 and no monotonic rule can make it
shrink; the empty string has to be attached as a special property to the grammar. Nosuch problem occurs in the non-monotonic hierarchy.

Many parsing methods will in principle work for e-free grammars only: if some-thing does not produce anything, you can't very well see if it's there. Often, however,
the parsing method can be doctored so that it will be able to handle e-rules.

54 Grammars as a generating device [Ch. 2
2.7 A CHARACTERIZATION OF THE LIMITATIONS OF CF AND FSGRAMMARS
When one has been working for a while with CF grammars, one gradually gets the feel-ing that almost anything could be expressed in a CF grammar. That there are, however,
serious limitations to what can be said by a CF grammar is shown by the famous uvwxytheorem, which is explained below.

2.7.1 The uvwxy theoremWhen we have obtained a sentence from a CF grammar, we may look at each (terminal) symbol in it, and ask: How did it get here? Then, looking at the production tree, wesee that it was produced as, say, the n-th member of the right-hand side of rule number
m. The left-hand side of this rule, the parent of our symbol, was again produced as thep-th member of rule q, and so on, until we reach the start symbol. We can, in a sense,
trace the lineage of the symbol in this way. If all rule/member pairs in the lineage of asymbol are different, we call the symbol original, and if all the symbols in a sentence
are original, we call the sentence "original".Now there is only a finite number of ways for a given symbol to be original. This
is easy to see as follows. All rule/member pairs in the lineage of an original symbolmust be different, so the length of its lineage can never be more than the total number
of different rule/member pairs in the grammar. There are only so many of these, whichyields only a finite number of combinations of rule/member pairs of this length or
shorter. In theory the number of original lineages of a symbol can be very large, but inpractice it is very small: if there are more than, say, ten ways to produce a given symbol from a grammar by original lineage, your grammar will be very convoluted!This puts severe restrictions on original sentences. If a symbol occurs twice in an
original sentence, both its lineages must be different: if they were the same, they woulddescribe the same symbol in the same place. This means that there is a maximum
length to original sentences: the sum of the numbers of original lineages of all symbols.For the average grammar of a programming language this length is in the order of some
thousands of symbols, i.e., roughly the size of the grammar. So, since there is a longestoriginal sentence, there can only be a finite number of original sentences, and we arrive
at the surprising conclusion that any CF grammar produces a finite-size kernel of origi-nal sentences and (probably) an infinite number of unoriginal sentences!

S..
...
...
...
...
...
...
...
...
...
. u

....

....

....

....

....

....

....

....

....

....

...

y

A..
...
...
...
...
...
...

v

....

....

....

....

....

....

....

x

A..
...
...
.

....

....

....

.q w

Figure 2.23 An unoriginal sentence: uvwxy
What do "unoriginal" sentences look like? This is where we come to the uvwxy

Sec. 2.7] A characterization of the limitations of CF and FS grammars 55
theorem. An unoriginal sentence has the property that it contains at least one symbol inthe lineage of which a repetition occurs. Suppose that symbol is a q and the repeated
rule is A. We can then draw a picture similar to Figure 2.23, where w is the part pro-duced by the most recent application of A, vwx the part produced by the other application of A and uvwxy is the entire unoriginal sentence. Now we can immediately findanother unoriginal sentence, by removing the smaller triangle headed by A and replacing it by a copy of the larger triangle headed by A; see Figure 2.24.

S..
...
...
...
...
...
...
...
...
...
. u

....

....

....

....

....

....

....

....

....

....

...

y

A..
...
...
...
...
...
...

v

....

....

....

....

....

....

....

x

A..
...
...
...
...
...
...

v

....

....

....

....

....

....

....

x

A..
...
...
.

....

....

....

.q w

Figure 2.24 Another unoriginal sentence, uv 2wx 2y
This new tree produces the sentence uvvwxxy and it is easy to see that we can, in thisway, construct a complete family of sentences uv nwx ny for all n>=0; the w is nested in a
number of v and x brackets, in an indifferent context of u and y.The bottom line is that when we examine longer and longer sentences in a
context-free language, the original sentences become exhausted and we meet only fam-ilies of closely related sentences telescoping off into infinity. This is summarized in
the uvwxy theorem: any sentence generated by a CF grammar, that is longer than thelongest original sentence from that grammar, can be cut into five pieces u, v, w, x and y
in such a way that uv nwx ny is a sentence from that grammar for all n>=0. The uvwxytheorem has several variants; it is also called the pumping lemma for context-free
languages.Two remarks must be made here. The first is that if a language keeps on being original in longer and longer sentences without reducing to families of nested sentences,there cannot be a CF grammar for it. We have already encountered the contextsensitive language a nb nc n and it is easy to see (but not quite so easy to prove!) that itdoes not decay into such nested sentences, as sentences get longer and longer. Consequently, there is no CF grammar for it.The second is that the longest original sentence is a property of the grammar, not
of the language. By making a more complicated grammar for a language we canincrease the set of original sentences and push away the border beyond which we are
forced to resort to nesting. If we make the grammar infinitely complicated, we can pushthe border to infinity and obtain a phrase structure language from it. How we can make
a CF grammar infinitely complicated, is described in the Section on two-level gram-mars, 2.4.

56 Grammars as a generating device [Ch. 2
2.7.2 The uvw theorem

Start_symbol. . .

P. . . . . .

Q

u A
u . . . R
u . . . . . . S
u v A A appears again
u v . . . T
u v . . . . . . U
u v w

Figure 2.25 Repeated occurrence of A may result in repeated occurrence of v
A simpler form of the uvwxy theorem applies to regular (Type 3) languages. We haveseen that the sentential forms occurring in the production process for a FS grammar all
contain only one non-terminal, which occurs at the end. During the production of avery long sentence, one or more non-terminals must occur two or more times, since
there are only a finite number of non-terminals. Figure 2.25 shows what we see, whenwe list the sentential forms one by one; the substring v has been produced from one
occurrence of A to the next, u is a sequence that allows us to reach A, and w is asequence that allows us to terminate the production process. It will be clear that, starting from the second A, we could have followed the same path as from the first A, andthus have produced uvvw. This leads us to the uvw theorem, or the pumping lemma for
regular languages: any sufficiently long string from a regular language can be cut intothree pieces u, v and w, so that uv nw is a string in the language for all n>=0.

2.8 HYGIENE IN GRAMMARS
Although the only requirement for a CF grammar is that there is exactly one non-terminal in the left-hand sides of all its rules, such a general grammar can suffer from a

(small) number of ailments.
2.8.1 Undefined non-terminalsThe right-hand sides of some rules may contain non-terminals for which no production
rule is given. Remarkably, this does not seriously affect the sentence generation pro-cess described in 2.5.2: if a sentential form containing an undefined non-terminal turns
up for processing in a left-most production process, there will be no match, and the sen-tential form is a blind alley and will be discarded. The rule with the right-hand side
containing the undefined non-terminal will never have issue and can indeed beremoved from the grammar. (If we do this, we may of course remove the last definition of another non-terminal, which will then in turn become undefined, etc.)From a theoretical point of view there is nothing wrong with an undefined nonterminal, but if a user-specified grammar contains one, there is almost certainly anerror, and any grammar-processing program should mark such an occurrence as an
error.

Sec. 2.8] Hygiene in grammars 57
2.8.2 Unused non-terminalsIf a non-terminal never occurs in the right-hand side of any rule, its defining rules will
never be used. Again this is no problem, but almost certainly implies an error some-where.

This error is actually harder to detect than it looks. Just searching all right-handsides is not good enough: imagine a rule X o""aX where X does not occur elsewhere in
the grammar. Then X occurs in a right-hand side, yet it will never be used. An algo-rithm to determine the set of unused non-terminals is given in Section 4.2.3.4.

2.8.3 Non-productive non-terminalsSuppose X has as its only rule X o""aX and suppose X can be reached from the start symbol. Now X will still not contribute anything to the sentences of the language of thegrammar, since once X is introduced, there is no way to get rid of X: any non-terminals
that does not in itself produce a sublanguage is non-productive and its rules can beremoved. Note that such removal will make the non-terminal undefined. An algorithm
to determine if a non-terminal generates anything at all is given in 4.2.3.3.To clean up a grammar, it is necessary to first remove the non-productive nonterminals, then the undefined ones and then the unused ones. These three groupstogether are called useless non-terminals.

2.8.4 LoopsThe above definition makes "non-useless" all rules that can be involved in the production of a sentence, but there still is a class of rules that are not really useful: rules of theform A o""A. Such rules are called loops: loops can also be indirect: A o""B, B o""C, Co""A.
A loop can legitimately occur in the production of a sentence, but if it does there is alsoa production of that sentence without the loop. Loops don't contribute to the language
and any sentence the production of which involves a loop is infinitely ambiguous,meaning that there are infinitely many production trees for it. Algorithms for loop
detection are given in Section 4.1.2.Different parsers react differently to grammars with loops. Some (most of the general parsers) faithfully attempt to construct an infinite number of parse trees, some (forinstance, the CYK parser) collapse the loop as described above and some (most deterministic parsers) reject the grammar. The problem is aggravated by the fact that loopscan be concealed by e-rules: a loop may only become visible when certain nonterminals produce e.

2.9 THE SEMANTIC CONNECTION
Sometimes parsing serves only to check the correctness of a string; that the string con-forms to a given grammar may be all we want to know, for instance because it confirms

our hypothesis that certain observed patterns are indeed correctly described by thegrammar we have designed for it. Often, however, we want to go further: we know that
the string conveys a meaning, its semantics, and this semantics is directly related to thestructure of the production tree of the string. (If it is not, we have the wrong grammar!)

Attaching semantics to a grammar is done in a very simple and effective way: toeach rule in the grammar, a semantic clause is attached that relates the semantics of the
members of the right-hand side of the rule to the semantics of the entire rule (in whichcase the semantic information flows from the leaves of the tree upwards to the start

58 Grammars as a generating device [Ch. 2
symbol) or the other way around (in which case the semantic information flows down-wards from the start symbol to the leaves) or both ways (in which case the semantic
information may have to flow up and down for a while until a stable situation isreached). Semantic information flowing down is called inherited: each rule inherits it
from its parent in the tree; semantic information flowing up is called derived: each rulederives it from its children.

There are many ways to express semantic clauses; since our subject is parsing andsyntax rather than semantics, we will briefly describe only two often-used and wellstudied techniques: attribute grammars and transduction grammars. We shall explainboth using the same simple example, the language of sums of one-digit numbers; the
semantics of a sentence in this language is the value of the sum. The language is gen-erated by the grammar of Figure 2.26.

SSuummSS -->> DDiiggiitt

SSuumm -->> SSuumm ++ DDiiggiitt
DDiiggiitt -->> 00 || 11 || ...... || 99

Figure 2.26 A grammar for sums of one-digit numbers
One of its sentences is, for instance, 33++55++11; its semantics is 9.
2.9.1 Attribute grammarsThe semantic clauses in an attribute grammar assume that each node in the production
tree has room for one or more attributes, which are just values (numbers, strings oranything else) sitting in nodes in production trees. For simplicity we restrict ourselves
to attribute grammars with only one attribute per node. The semantic clause of a rule insuch a grammar contains some formulas which calculate the attributes of some of the
non-terminals in that rule (=nodes in the production tree) from other non-terminals inthat rule.

If the semantic clause of a rule R calculates the attribute of the left-hand side of R,that attribute is derived; if it calculates an attribute of one of the non-terminals in the
right-hand side of R, say T, then that attribute is inherited by T. Derived attributes arealso called "synthesized attributes". The attribute grammar for our example is:

1. SSuummSS -->> DDiiggiitt {{A 0:=A 1}}2.

SSuumm -->> SSuumm ++ DDiiggiitt {{A 0:=A 1+A 3}}3a.
DDiiggiitt -->> 00 {{A 0:=0}}

...... ......3j.
DDiiggiitt -->> 99 {{A 0:=9}}

The semantic clauses are given between curly brackets. A 0 is the (derived) attribute ofthe left-hand side, A

1 . . . An are the attributes of the members of the right-hand side.Traditionally, terminal symbols in a right-hand side are also counted in determining the

index of A, although they do not (normally) carry attributes; the Digit in rule 2 is inposition 3 and its attribute is A

3. Most systems for handling attribute grammars haveless repetitive ways to express rule 3a through 3j.

The initial parse tree for 33++55++11 is given in Figure 2.27. First only the attributesfor the leaves are known, but as soon as all attributes in a right-hand side of a

Sec. 2.9] The semantic connection 59

++
++ 11
33 55

A 0=1
A 0=3 A 0=5

Figure 2.27 Initial stage of the attributed parse tree
production rule are known, we can use its semantic clause to calculate the attribute ofits left-hand side. This way the attribute values (semantics) percolate up the tree, finally
reach the start symbol and provide as with the semantics of the whole sentence, asshown in Figure 2.28. Attribute grammars are a very powerful method of handling the
semantics of a language.

++
++ 11
33 55

A 0=9
A 0=8 A 0=1
A 0=3 A 0=5

Figure 2.28 Fully attributed parse tree
2.9.2 Transduction grammarsTransduction grammars define the semantics of a string (the "input string") as another
string, the "output string" or "translation", rather than as the final attribute of the startsymbol. This method is less powerful but much simpler than using attributes and often
sufficient. The semantic clause in a rule just contains the string that should be outputfor the corresponding node. We assume that the string for a node is output just after the
strings for all its children. Other variants are possible and in fact usual. We can nowwrite a transduction grammar which translates a sum of digits into instructions to calculate the value of the sum.
1. SSuummSS -->> DDiiggiitt {{""mmaakkee iitt tthhee rreessuulltt""}}2.

SSuumm -->> SSuumm ++ DDiiggiitt {{""aadddd iitt ttoo tthhee pprreevviioouuss rreessuulltt""}}3a.
DDiiggiitt -->> 00 {{""ttaakkee aa 00""}}

...... ......3j.
DDiiggiitt -->> 99 {{""ttaakkee aa 99""}}

This transduction grammar translates 33++55++11 into:

take a 3make it the result
take a 5add it to the previous result
take a 1add it to the previous result

60 Grammars as a generating device [Ch. 2
which is indeed what 33++55++11 "means".

2.10 A METAPHORICAL COMPARISON OF GRAMMAR TYPES
Text books claim that "Type n grammars are more powerful than Type n +1 grammars,for n=0,1,2", and one often reads statements like "A regular (Type 3) grammar is not

powerful enough to match parentheses". It is interesting to see what kind of power ismeant. Naively, one might think that it is the power to generate larger and larger sets,
but this is clearly incorrect: the largest possible set of strings, \Sigma *, is easily generated bythe straightforward Type 3 grammar:

SSSS -->> [[\Sigma \Sigma ]] SS || ee
where [\Sigma ] is an abbreviation for the symbols in the language. It is just when we want torestrict this set, that we need more powerful grammars. More powerful grammars can
define more complicated boundaries between correct and incorrect sentences. Someboundaries are so fine that they cannot be described by any grammar (that is, by any
generative process).This idea has been depicted metaphorically in Figure 2.29, in which a rose is
approximated by increasingly finer outlines. In this metaphor, the rose corresponds tothe language (imagine the sentences of the language as molecules in the rose); the
grammar serves to delineate its silhouette. A regular grammar only allows us straighthorizontal and vertical line segments to describe the flower; ruler and T-square suffice,
but the result is a coarse and mechanical-looking picture. A CF grammar wouldapproximate the outline by straight lines at any angle and by circle segments; the drawing could still be made using the classical tools of compasses and ruler. The result isstilted but recognizable. A CS grammar would present us with a smooth curve tightly
enveloping the flower, but the curve is too smooth: it cannot follow all the sharp turnsand it deviates slightly at complicated points; still, a very realistic picture results. An
unrestricted phrase structure grammar can represent the outline perfectly. The roseitself cannot be caught in a finite description; its essence remains forever out of our
reach.A more prosaic and practical example can be found in the successive sets of Pascal# programs that can be generated by the various grammar types.
\Gamma  The set of all lexically correct Pascal programs can be generated by a regular

grammar. A Pascal program is lexically correct if there are no newlines insidestrings, comment is terminated before end-of-file, all numerical constants have the

right form, etc.
\Gamma  The set of all syntactically correct Pascal programs can be generated by a

context-free grammar. These programs conform to the (CF) grammar in themanual.

\Gamma  The set of all semantically correct Pascal programs can be generated by a CS

grammar (although a VW grammar would be more practical). These are the
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

We use the programming language Pascal here because we expect that most of our readers willbe more or less familiar with it. Any programming language for which the manual gives a CF

grammar will do.

Sec. 2.10] A metaphorical comparison of grammar types 61

Figure 2.29 The silhouette of a rose, approximated by Type 3 to Type 0 grammars

programs that pass through a Pascal compiler without drawing error messages.
\Gamma  The set of all Pascal programs that would terminate in finite time when run with a

given input can be generated by an unrestricted phrase structure grammar. Such agrammar would, however, be very complicated, even in van Wijngaarden form,

since it would incorporate detailed descriptions of the Pascal library routines andthe Pascal run-time system.
\Gamma  The set of all Pascal programs that solve a given problem (for instance, play

chess) cannot be generated by a grammar (although the description of the set isfinite).

Note that each of the above sets is a subset of the previous set.

3
Introduction to parsing

To parse a string according to a grammar means to reconstruct the production tree (ortrees) that indicate how the given string can be produced from the given grammar.
There are two important points here; one is that we do require the entire production treeand the other is that there may be more than one such tree.

The requirement to recover the production tree is not natural. After all, a grammaris a condensed description of a set of strings, i.e., a language, and our input string either
belongs or does not belong to that language; no internal structure or production path isinvolved. If we adhere to this formal view, the only meaningful question we can ask is
if a given string can be recognized according to a grammar; any question as to how,would be a sign of senseless, even morbid curiosity. In practice, however, grammars
have semantics attached to them; specific semantics is attached to specific rules, and inorder to find out which rules were involved in the production of a string and how, we
need the production tree. Recognition is (often) not enough, we need parsing to get thefull benefit of the syntactic approach.

3.1 VARIOUS KINDS OF AMBIGUITY
A sentence from a grammar can easily have more than one production tree, i.e., therecan easily be more than one way to produce the sentence. From a formal point of view

this is again a non-issue (a set does not count how many times it contains an element),but as soon as we are interested in the semantics, the difference becomes significant.
Not surprisingly, a sentence with more than one production tree is called ambiguous,but we must immediately distinguish between essential ambiguity and spurious ambiguity. The difference comes from the fact that we are not interested in the productiontrees per se, but rather in the semantics they describe. An ambiguous sentence is spuriously ambiguous if all its production trees describe the same semantics; if some ofthem differ in their semantics, the ambiguity is essential. The notion "ambiguity" can
also be defined for grammars: a grammar is essentially ambiguous if it can produce anessentially ambiguous sentence, spuriously ambiguous if it can produce a spuriously
ambiguous sentence (but not an essentially ambiguous one) and unambiguous if it can-not do either. Strangely enough, languages also can be ambiguous: there are (contextfree) languages for which there is no unambiguous grammar; such languages belong ina research lab, in a cage. For testing the possible ambiguity of a grammar, see Section

Sec. 3.1] Various kinds of ambiguity 63
9.10.

1. SSuummSS -->> DDiiggiitt {{ A 0:=A 1 }}2.

SSuumm -->> SSuumm ++ SSuumm {{ A 0:=A 1+A 3 }}3a.
DDiiggiitt -->> 00 {{ A 0:=0 }}
..............3j.

DDiiggiitt -->> 99 {{ A 0:=9 }}

Figure 3.1 A simple ambiguous grammar
A simple ambiguous grammar is given in Figure 3.1. Note that rule 2 differs fromthat in Figure 2.26. Now

33++55++11 has two production trees (Figure 3.2) but the semanticsis the same in both cases: 9. The ambiguity is spurious. If we change the

++ into a --,however, the ambiguity becomes essential, Figure 3.3. The unambiguous grammar in

Figure 2.26 remains unambiguous and retains the correct semantics if ++ is changed into
--.

22
11 22

11 11
33cc 33ee 33aa

33 ++ 55 ++ 11

9
3 6

5 1

22
22 11
11 11
33cc 33ee 33aa

33 ++ 55 ++ 11

9
8 1
3 5

Figure 3.2 Spurious ambiguity: no change in semantics

22
11 22

11 11
33cc 33ee 33aa

33 -- 55 -- 11

-1
3 4

5 1

22
22 11
11 11
33cc 33ee 33aa

33 -- 55 -- 11

-3
-2 1
3 5

Figure 3.3 Essential ambiguity: the semantics differ

64 Introduction to parsing [Ch. 3
3.2 LINEARIZATION OF THE PARSE TREE
Often it is inconvenient and unnecessary to construct the actual production tree: manyparsers produce a list of rule numbers instead, which means that they linearize the

parse tree. There are three main ways to linearize a tree, prefix, postfix and infix. Inprefix notation, each node is listed by listing its number followed by prefix listings of
the subnodes in left-to-right order; this gives us the left-most derivation (for the righttree in Figure 3.2):

left-most: 2 2 1 3c 1 3e 1 3a
In postfix notation, each node is listed by listing in postfix notation all the subnodes inleft-to-right order, followed by the number of the rule in the node itself; this gives us
the right-most derivation (for the same tree):

right-most: 3c 1 3e 1 2 3a 1 2
In infix notation, each node is listed by first giving an infix listing between parenthesesof the first n subnodes, followed by the rule number in the node, followed by an infix
listing between parentheses of the remainder of the subnodes; n can be chosen freelyand can even differ from rule to rule, but n =1 is normal. Infix notation is not common
for derivations, but is occasionally useful. The case with n =1 is called the left-cornerderivation; in our example we get:

left-corner: (((3c)1) 2 ((3e)1)) 2 ((3a)1)
The infix notation requires parentheses to enable us to reconstruct the production treefrom it. The left-most and right-most derivations can do without, provided we have the
grammar ready to find the number of subnodes for each node. Note that it is easy to tellif a derivation is left-most or right-most: a left-most derivation starts with a rule for the
start symbol, a right-most derivation starts with a rule that produces terminal symbolsonly (if both conditions hold, there is only one rule, which is both left-most and rightmost derivation).The existence of several different derivations should not be confused with ambiguity. The different derivations are just notational variants for one and the same pro-duction tree. No semantic significance can be attached to their differences.

3.3 TWO WAYS TO PARSE A SENTENCE
The basic connection between a sentence and the grammar it derives from is the parsetree, which describes how the grammar was used to produce the sentence. For the

reconstruction of this connection we need a parsing technique. When we consult theextensive literature on parsing techniques, we seem to find dozens of them, yet there
are only two techniques to do parsing; all the rest is technical detail and embellishment.The first method tries to imitate the original production process by rederiving the
sentence from the start symbol. This method is called top-down, because the productiontree is reconstructed from the top downwards.#
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

Trees grow from their roots downwards in computer science; this is comparable to electrons

Sec. 3.3] Two ways to parse a sentence 65

The second methods tries to roll back the production process and to reduce thesentence back to the start symbol. Quite naturally this technique is called bottom-up.

3.3.1 Top-down parsingSuppose we have the monotonic grammar for the language a nb nc n from Figure 2.6,
which we repeat here:

SSSS -->> aaSSQQ

SS -->> aabbcc
bbQQcc -->> bbbbcccc

ccQQ -->> QQcc

and suppose the (input) sentence is aaaabbbbcccc. First we try the top-down parsing method.We know that the production tree must start with the start symbol:

SS
Now what could the second step be? We have two rules for SS: SS-->>aaSSQQ and SS-->>aabbcc.The second rule would require the sentence to start with

aabb, which it does not; thisleaves us
SS-->>aaSSQQ:

SS
aa SS QQ

This gives us a good explanation of the first aa in our sentence. Again two rules apply:
SS-->>aaSSQQ and SS-->>aabbcc. Some reflection will reveal that the first rule would be a badchoice here: all production rules of

SS start with an aa, and if we would advance to thestage
aaaaSSQQQQ, the next step would inevitably lead to aaaaaa........, which contradicts theinput string. The second rule, however, is not without problems either:

SS
aa SS QQ
aa aa bb cc QQ

since now the sentence starts with aaaabbcc......, which also contradicts the input sentence.Here, however, there is a way out:

ccQQ-->>QQcc:

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

having a negative charge in physics.

66 Introduction to parsing [Ch. 3

SS
aa SS QQ

aa bb cc QQ
aa aa bb QQ cc

Now only one rule applies: bbQQcc-->>bbbbcccc, and we obtain our input sentence (togetherwith the production tree):

SS
aa SS QQ

aa bb cc QQ

bb QQ cc
aa aa bb bb cc cc

Top-down parsing tends to identify the production rules (and thus to characterizethe parse tree) in prefix order.

3.3.2 Bottom-up parsingUsing the bottom-up technique, we proceed as follows. One production step must have
been the last and its result must still be visible in the string. We recognize the right-hand side of

bbQQcc-->>bbbbcccc in aaaabbbbcccc. This gives us the final step in the production (andthe first in the reduction):

aa aa bb QQ cc
aa aa bb bb cc cc

Now we recognize the QQcc as derived by ccQQ-->>QQcc:

aa aa bb cc QQ

bb QQ cc
aa aa bb bb cc cc

Again we find only one recognizable substring: aabbcc:

Sec. 3.3] Two ways to parse a sentence 67

aa SS QQ

aa bb cc QQ

bb QQ cc
aa aa bb bb cc cc

and also our last reduction step leaves us no choice:

SS
aa SS QQ

aa bb cc QQ

bb QQ cc
aa aa bb bb cc cc

Bottom-up parsing tends to identify the production rules in postfix order. It isinteresting to note that bottom-up parsing turns the parsing process into a production
process. The above reduction can be viewed as a production with the reversed gram-mar:

aaSSQQ -->> SS
aabbcc -->> SS
bbbbcccc -->> bbQQcc

QQcc -->> ccQQ

augmented with a rule that turns the start symbol into a new terminal symbol:

SS -->> !!
and a rule which introduces a new start symbol, the original sentence:

IISS -->> aaaabbbbcccc
If, starting from II, we can produce !! we have recognized the input string, and if wehave kept records of what we did, we also have obtained the parse tree.

3.3.3 ApplicabilityThe above examples show that both the top-down and the bottom-up method will work
under certain circumstances, but also that sometimes quite subtle considerations areinvolved, of which it is not at all clear how we can teach them to a computer. Almost
the entire body of parser literature is concerned with formalizing these subtle

68 Introduction to parsing [Ch. 3
considerations, and with considerable success.Note: It is also possible to reconstruct some parts of the production tree top-down
and other parts bottom-up. Such methods identify the production rules in some infixorder and are called left-corner.

3.4 NON-DETERMINISTIC AUTOMATA
Both examples above feature two components: a machine that can make substitutionsand record a parse tree, and a control mechanism that decides which moves the

machine should make. The machine is relatively simple since its substitutions are res-tricted to those allowed by the grammar, but the control mechanism can be made arbitrarily complex and may incorporate extensive knowledge of the grammar.This structure can be discerned in all parsing methods; there always is a substituting and record-keeping machine and a guiding control mechanism (Figure 3.4).

controlmechanism substituting andrecord-keeping

mechanism

Figure 3.4 Global structure of a parser
The substituting machine is called a non-deterministic automaton or NDA; it is called"non-deterministic" since it often has several possible moves and the particular choice
is not predetermined, and an "automaton" since it fits the Webster# definition "anapparatus that automatically performs certain actions by responding to preset controls
or encoded instructions". It manages three items: the input string (actually a copy ofit), the partial parse tree and some internal administration. Every move of the NDA
transfers some information from the input string through the administration to the par-tial parse tree; each of the three items may be modified in the process:

partialparse

trees control input

internaladministration
The great strength of a NDA, and the main source of its usefulness, is that it caneasily be constructed so that it can only make "correct" moves, that is, moves that keep
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

Webster's New Twentieth Century Dictionary, The World Publ. Comp., Cleveland, 1970.

Sec. 3.4] Non-deterministic automata 69
the system of partially processed input, internal administration and partial parse treeconsistent. This has the consequence that we may move the NDA any way we choose:
it may move in circles, it may even get stuck, but if it ever gives us an answer, i.e., afinished parse tree, that answer will be correct. It is also essential that the NDA can
make all correct moves, so that it can produce all parsings if the control mechanism isclever enough to guide the NDA there. This property of the NDA is also easily
arranged.The inherent correctness of the NDA allows great freedom to the control mechanism, the "control" for short. It may be naive or sophisticated, it may be cumbersome orit may be efficient, it may even be wrong, but it can never cause the NDA to produce
an incorrect parsing; and that is a comforting thought. (If it is wrong it may, however,cause the NDA to miss a correct parsing, to loop infinitely or to get stuck in a place
where it should not).
3.4.1 Constructing the NDAThe NDA derives directly from the grammar. For a top-down parser its moves consist
essentially of the production rules of the grammar and the internal administration is ini-tially the start symbol. The control moves the machine until the internal administration
is equal to the input string; then a parsing has been found. For a bottom-up parser themoves consist essentially of the reverse of the production rules of the grammar (see
3.3.2) and the internal administration is initially the input string. The control moves themachine until the internal administration is equal to the start symbol; then a parsing has
been found. A left-corner parser works like a top-down parser in which a carefullychosen set of production rules has been reversed and which has special moves to undo
this reversion when needed.
3.4.2 Constructing the control mechanismConstructing the control of a parser is quite a different affair. Some controls are
independent of the grammar, some consult the grammar regularly, some use largetables precalculated from the grammar and some even use tables calculated from the
input string. We shall see examples of each of these: the "hand control" that wasdemonstrated at the beginning of this section comes in the category "consults the grammar regularly", backtracking parsers often use a grammar-independent control, LL andLR parsers use precalculated grammar-derived tables, the CYK parser uses a table
derived from the input string and Earley's and Tomita's parsers use several tablesderived from the grammar and the input string.

Constructing the control mechanism, including the tables, from the grammar isalmost always done by a program. Such a program is called a parser generator; it is fed
the grammar and perhaps a description of the terminal symbols and produces a programwhich is a parser. The parser often consists of a driver and one or more tables, in which
case it is called table-driven. The tables can be of considerable size and of extremecomplexity.

The tables that derive from the input string must of course be calculated by a rou-tine that is part of the parser. It should be noted that this reflects the traditional setting
in which a large number of different input strings is parsed according to a relativelystatic and unchanging grammar. The inverse situation is not at all unthinkable: many
grammars are tried to explain a given input string (for instance, an observed sequenceof events).

70 Introduction to parsing [Ch. 3
3.5 RECOGNITION AND PARSING FOR TYPE 0 TO TYPE 4 GRAMMARS
Parsing a sentence according to a grammar if we know in advance that the string indeedderives from the grammar, is in principle always possible. If we cannot think of anything better, we can just run the general production process of 2.5.1 on the grammarand sit back and wait until the sentence turns up (and we know it will); this by itself is
not exactly enough, we must extend the production process a little, so that each senten-tial form carries its own partial production tree, which must be updated at the appropriate moments, but it is clear that this can be done with some programming effort. Wemay have to wait a little while (say a couple of million years) for the sentence to show
up, but in the end we will surely obtain the parse tree. All this is of course totallyimpractical, but it still shows us that at least theoretically any string can be parsed if we
know it is parsable, regardless of the grammar type.
3.5.1 Time requirementsWhen parsing strings consisting of more than a few symbols, it is important to have
some idea of the time requirements of the parser, i.e., the dependency of the timerequired to finish the parsing on the number of symbols in the input string. Expected
lengths of input range from some tens (sentences in natural languages) to some tens ofthousands (large computer programs); the length of some input strings may even be virtually infinite (the sequence of buttons pushed on a coffee vending machine over itslife-time). The dependency of the time requirements on the input length is also called
time complexity.Several characteristic time dependencies can be recognized. A time dependency
is exponential if each following input symbol multiplies the required time by a constantfactor, say 2: each additional input symbol doubles the parsing time. Exponential time
dependency is written O (C n) where C is the constant multiplication factor. Exponentialdependency occurs in the number of grains doubled on each field of the famous chess
board; this way lies bankrupcy.A time dependency is linear if each following input symbol takes a constant
amount of time to process; doubling the input length doubles the processing time. Thisis the kind of behaviour we like to see in a parser; the time needed for parsing is proportional to the time spent on reading the input. So-called real-time parsers behaveeven better: they can produce the parse tree within a constant time after the last input
symbol was read; given a fast enough computer they can keep up indefinitely with aninput stream of constant speed. (Note that the latter is not necessarily true of lineartime parsers: they can in principle read the entire input of n symbols and then take atime proportional to n to produce the parse tree.)

Linear time dependency is written O (n). A time dependency is called quadratic ifthe processing time is proportional to the square of the input length (written O (n 2)) and
cubic if it is proportional to the to the third power (written O (n 3)). In general, a depen-dency that is proportional to any power of n is called polynomial (written O (n p)).

3.5.2 Type 0 and Type 1 grammarsIt is a remarkable result in formal linguistics that the recognition problem for a arbitrary Type 0 grammar cannot be solved. This means that there cannot be an algorithmthat accepts an arbitrary Type 0 grammar and an arbitrary string and tells us in finite
time if the grammar can produce the string or not. This statement can be proven, but theproof is very intimidating and, what is worse, does not provide any insight into the

Sec. 3.5] Recognition and parsing for Type 0 to Type 4 grammars 71
cause of the phenomenon. It is a proof by contradiction: we can prove that, if such analgorithm existed, we could construct a second algorithm of which we can prove that it
only terminates if it never terminates. Since the latter is a logical impossibility andsince all other premisses that went into the intermediate proof are logically sound we
are forced to conclude that our initial premiss, the existence of a recognizer for Type 0grammars, is a logical impossibility. Convincing, but not food for the soul. For the full
proof see Hopcroft and Ullman [Books 1979, pp. 182-183] or Re've'sz [Books 1985, p.98].

It is quite possible to construct a recognizer that works for a certain number ofType 0 grammars, using a certain technique. This technique, however, will not work
for all Type 0 grammars. In fact, however many techniques we collect, there willalways be grammars for which they do not work. In a sense we just cannot make our
recognizer complicated enough.For Type 1 grammars, the situation is completely different. The seemingly inconsequential property that Type 1 production rules cannot make a sentential form shrinkallows us to construct a control mechanism for a bottom-up NDA that will at least
work in principle, regardless of the grammar. The internal administration of this controlconsists of a set of sentential forms that could have played a role in the production of
the input sentence; it starts off containing only the input sentence. Each move of theNDA is a reduction according to the grammar. Now the control applies all possible
moves of the NDA to all sentential forms in the internal administration in an arbitraryorder, and adds each result to the internal administration if it is not already there. It
continues doing so until each move on each sentential form results in a sentential formthat has already been found. Since no move of the NDA can make a sentential form
longer (because all right-hand sides are at least as long as their left-hand sides) andsince there are only a finite number of sentential forms as long as or shorter than the
input string, this must eventually happen. Now we search the sentential forms in theinternal administration for one that consists solely of the start symbol; if it is there, we
have recognized the input string, if it is not, the input string does not belong to thelanguage of the grammar. And if we still remember, in some additional administration,
how we got this start symbol sentential form, we have obtained the parsing. All thisrequires a lot of book-keeping, which we are not going to discuss, since nobody does it
this way anyway.To summarize the above, we cannot always construct a parser for a Type 0 grammar, but for a Type 1 grammar we always can. The construction of a practical and rea-sonably efficient parser for such grammars is a very difficult subject on which slow but
steady progress has been made during the last 20 years (see the bibliography on"Unrestricted PS and CS Grammars"). It is not a hot research topic, mainly because
Type 0 and Type 1 grammars are well-known to be human-unfriendly and will neversee wide application. Yet it is not completely devoid of usefulness, since a good parser
for Type 0 grammars would probably make a good starting point for a theorem prover.#The human-unfriendliness consideration does not apply to two-level grammars.
Having a practical parser for two-level grammars would be marvellous, since it wouldallow parsing techniques (with all their built-in automation) to be applied in many more
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

A theorem prover is a program that, given a set of axioms and a theorem, proves or disprovesthe theorem without or with minimal human intervention.

72 Introduction to parsing [Ch. 3
areas than today, especially there where context conditions are important. The prob-lems in constructing such a parser are at least as great as those seen above, but Fisher
[VW 1985] has obtained some encouraging results.All known parsing algorithms for Type 0, Type 1 and unrestricted VW grammars
have exponential time dependency.
3.5.3 Type 2 grammarsFortunately, much better parsing algorithms are known for CF (Type 2) grammars than
for Type 0 and Type 1. Almost all practical parsing is done using CF and FS grammars,and almost all problems in context-free parsing have been solved. The cause of this
large difference can be found in the locality of the CF production process: the evolutionof one non-terminal in the sentential form is totally independent of the evolution of any
other non-terminal, and, conversely, during parsing we can combine partial parse treesregardless of their histories. Neither is true in a context-sensitive grammar.

Both the top-down and the bottom-up parsing processes are readily applicable toCF grammars. In the examples below we shall use the simple grammar

SSeenntteenncceeSS -->> SSuubbjjeecctt VVeerrbb OObbjjeecctt
SSuubbjjeecctt -->> tthhee NNoouunn || aa NNoouunn || PPrrooppeerrNNaammee
OObbjjeecctt -->> tthhee NNoouunn || aa NNoouunn || PPrrooppeerrNNaammee
VVeerrbb -->> bbiitt || cchhaasseedd
NNoouunn -->> ccaatt || ddoogg
PPrrooppeerrNNaammee -->> ......

3.5.3.1 Top-down parsingIn top-down parsing we start with the start symbol and try to produce the input. The
keywords here are predict and match. At any time there is a left-most non-terminal Ain the sentential form and the parser tries systematically to predict a fitting alternative
for A, as far as compatible with the symbols found in the input at the position where theresult of A could start. Consider the example of Figure 3.5, where

OObbjjeecctt is the left-most non-terminal.

Input: tthhee ccaatt bbiitt aa ddooggSentential form:

tthhee ccaatt bbiitt OObbjjeecctt(the internal administration)

Figure 3.5 Top-down parsing as the imitation of the production process
In this situation, the parser will first predict tthhee NNoouunn for OObbjjeecctt, but will immedi-ately reject this alternative since it requires

tthhee where the input has aa. Next, it will try
aa NNoouunn, which is temporarily accepted. The aa is matched and the new left-most non-terminal is

NNoouunn. This parse will succeed when NNoouunn eventually produces ddoogg. Theparser will then attempt a third prediction for

OObbjjeecctt, PPrrooppeerrNNaammee; this alternative isnot immediately rejected as the parser cannot see that

PPrrooppeerrNNaammee cannot start with aa.It will fail at a later stage.

There are two serious problems with this approach. Although it can, in principle,handle arbitrary CF grammars, it will loop on some grammars if implemented naively.
This can be avoided by using some special techniques, which result in general topSec. 3.5] Recognition and parsing for Type 0 to Type 4 grammars 73
down parsers; these are treated in detail in Chapter 6. The second problem is that thealgorithm requires exponential time since any of the predictions may turn out wrong
and may have to be corrected by trial and error. The above example shows that someefficiency can be gained by preprocessing the grammar: it is advantageous to know in
advance what tokens can start PPrrooppeerrNNaammee, to avoid predicting an alternative that isdoomed in advance. This is true for most non-terminals in the grammar and this kind of
information can be easily calculated from the grammar and stored in a table for useduring parsing. For a reasonable set of grammars, linear time dependency can be
achieved, as explained in Chapter 8.
3.5.3.2 Bottom-up parsingIn bottom-up parsing we start with the input and try to reduce it to the start symbol.
Here the keywords are shift and reduce. When we are in the middle of the process, wehave in our hands a sentential form reduced from the input. Somewhere in this sentential form there must be a segment (a substring) that was the result of the last productionstep that produced this sentential form; this segment is the right-hand side of a nonterminal to which it must now be reduced. This segment is called the handle of the sen-tential form, a quite adequate expression. See Figure 3.6. The trick is to find the handle.
It must be the right-hand side of a rule, so we start looking for such a right-hand side byshifting symbols from the sentential form into the internal administration. When we
find a right-hand side we reduce it to its left-hand side and repeat the process, until onlythe start symbol is left. We will not always find the correct handle this way; if we err,
we will get stuck further on, will have to undo some steps, shift in more symbols andtry again. In the above example we could have reduced the

aa NNoouunn to OObbjjeecctt, therebyboldly heading for a dead end.

SSuubbjjeecctt cchhaasseedd aa ddoogg
aa NNoouunn cchhaasseedd aa ddooggproduction reduction

handle

Figure 3.6 Bottom-up parsing as the inversion of the production process
There are essentially the same two problems with this approach as with the top-down technique. It may loop, and will do so on grammars with e-rules: it will continue
to find empty productions all over the place. This can be remedied by touching up thegrammar. And it can take exponential time, since the correct identification of the handle may have to be done by trial and error. Again, doing preprocessing on the grammaroften helps: it is easy to see from the grammar that

SSuubbjjeecctt can be followed by
cchhaasseedd, but OObbjjeecctt cannot; so it is unprofitable to reduce a handle to OObbjjeecctt if thenext symbol is

cchhaasseedd.

3.5.4 Type 3 grammarsA right-hand side in a regular grammar contains at most one non-terminal, so there is
no difference between left-most and right-most production. Top-down methods aremuch more efficient for regular grammars than bottom-up methods.# When we take the
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

Some regular grammars have, however, rules of the form A o""a and A o""Ba (and no others); inthat case bottom-up methods work better.

74 Introduction to parsing [Ch. 3
production tree of Figure 2.14 and if we turn it 45o counterclockwise, we get the pro-duction line of Figure 3.7. The sequence of non-terminals roll on to the right, producing
terminals symbols as they go. In parsing, we are given the terminals symbols and aresupposed to construct the sequence of non-terminals. The first one is given, the start
symbol (hence the preference for top-down). If only one rule for the start symbol startswith the first symbol of the input we are lucky and know which way to go. Very often,
however, there are many rules starting with the same symbol and then we are in need ofmore wisdom. As with Type 2 grammars, we can of course find the correct continuation by trial and error, but far more efficient methods exist that can handle any regulargrammar. Since they form the basis of some advanced parsing techniques, they are
treated separately, in Chapter 5.

SSeenntteennccee LLiisstt LLiissttTTaaiill LLiisstt LLiissttTTaaiill

tt ,, dd && hh
Figure 3.7 The production tree of Figure 2.14 as a production line
3.5.5 Type 4 grammarsFinite-choice (FC) grammars do not involve production trees, and membership of a
given input string to the language of the FC grammar can be determined by simplelook-up. This look-up is generally not considered to be "parsing", but is still mentioned
here for two reasons. First it can benefit from parsing techniques and second it is oftenrequired in a parsing environment. Natural languages have some categories of words
that have only a very limited number of members; examples are the pronouns, theprepositions and the conjunctions. It is often important to decide quickly if a given
word belongs to one of these finite-choice categories or will have to be analysedfurther. The same applies to reserved words in a programming language.

One approach is to consider the FC grammar as a regular grammar and apply thetechniques of Chapter 5. This is often amazingly efficient.
A second often-used approach is that using a hash table. See any book on algo-rithms, for instance, Smith [CSBooks 1989].

3.6 AN OVERVIEW OF PARSING METHODS
The reader of literature about parsing is confronted with a large number of techniqueswith often unclear interrelationships. Yet (almost) all techniques can be placed in a single framework, according to some simple criteria; see Figure 3.10. We have alreadyseen that a parsing technique is either top-down or bottom-up. The next division is that
between non-directional and directional.
3.6.1 DirectionalityA non-directional method constructs the parse tree while accessing the input in any
order it sees fit; this of course requires the entire input to be in memory before parsingcan start. There is a top-down and a bottom-up version.

Sec. 3.6] An overview of parsing methods 75
3.6.1.1 Non-directional methodsThe non-directional top-down method is simple and straightforward and has probably
been invented independently by many people. It was first described by Unger [CF1968] but in his article he gives the impression that the method already existed. The
method has not received much attention in the literature but is more important than onemight think, since it is used anonymously in a number of other parsers. We shall call it
Unger's method; it is treated in Section 4.1.The non-directional bottom-up method has also been discovered independently by
a number of people, among whom Cocke, Younger [CF 1967] and Kasami [CF 1969];an earlier description is by Sakai [CF 1962]. It is named CYK (or sometimes CKY)
after the three best-known inventors. It has received considerable attention since itsnaive implementation is much more efficient than that of Unger's method. The efficiency of both methods can be improved, however, arriving at roughly the same perfor-mance (see Sheil [CF 1976]). The CYK method is treated in Section 4.2.

3.6.1.2 Directional methodsThe directional methods process the input symbol by symbol, from left to right. (It is
also possible to parse from right to left, using a mirror image of the grammar; this isoccasionally useful.) This has the advantage that parsing can start, and indeed progress,
considerably before the last symbol of the input is seen. The directional methods are allbased explicitly or implicitly on the parsing automaton described in Section 3.5.3,
where the top-down method performs predictions and matches and the bottom-upmethod performs shifts and reduces.

3.6.2 Search techniquesThe next subdivision concerns the search technique used to guide the (nondeterministic!) parsing automaton through all its possibilities to find one or all parsings.There are in general two methods for solving problems in which there are several
alternatives in well-determined points: depth-first search, and breadth-first search. Indepth-first search we concentrate on one half-solved problem; if the problem bifurcates
at a given point P, we store one alternative for later processing and keep concentratingon the other alternative. If this alternative turns out to be a failure (or even a success,
but we want all solutions), we roll back our actions until point P and continue with thestored alternative. This is called backtracking. In breadth-first search we keep a set of
half-solved problems. From this set we calculate a new set of (better) half-solved prob-lems by examining each old half-solved problem; for each alternative, we create a copy
in the new set. Eventually, the set will come to contain all solutions.Depth-first search has the advantage that it requires an amount of memory that is
proportional to the size of the problem, unlike breadth-first search, which may requireexponential memory. Breadth-first search has the advantage that it will find the simplest solution first. Both methods require in principle exponential time; if we wantmore efficiency (and exponential requirements are virtually unacceptable), we need
some means to restrict the search. See any book on algorithms, for instance,Sedgewick [CSBooks 1988], for more information on search techniques.

These search techniques are not at all restricted to parsing and can be used in awide array of contexts. A traditional one is that of finding an exit from a maze. Figure
3.8(a) shows a simple maze with one entrance and two exits. Figure 3.8(b) depicts thepath a depth-first search will take; this is the only option for the human maze-walker:

76 Introduction to parsing [Ch. 3
he cannot duplicate himself and the maze. Dead ends make the depth-first search back-track to the most recent untried alternative. If the searcher will also backtrack at each
exit, he will find all exits. Figure 3.8(c) shows which rooms are examined in eachstage of the breadth-first search. Dead ends (in stage 3) cause the search branches in
question to be discarded. Breadth-first search will find the shortest way to an exit (theshortest solution) first; if it continues until all there are no branches left, it will find all
exits (all solutions).

(a) (b)

...
...... .. . . . . . . . ....

.................
.... .. . . . . . . . .. ..

....

.... .. . . .

..........
.....
........
...
...
.......
...
.....
........
....
.......

...
...
...
...
...

(c)
0

1
2

2

3

3

3

4

4
5
5

6

Figure 3.8 A simple maze with depth-first and breadth-first visits
3.6.3 General directional methodsCombining depth-first or breadth-first with top-down or bottom-up gives four classes
of parsing techniques. The top-down techniques are treated in Chapter 6. The depth-first top-down technique allows a very simple implementation called recursive descent;
this technique, which is explained in Section 6.6, is very suitable for writing parsers byhand. The bottom-up techniques are treated in Chapter 7. The combination of breadthfirst and bottom-up leads to the class of Earley parsers, which have among them somevery effective and popular parsers for general CF grammars. See Section 7.2.

3.6.4 Linear methodsMost of the general search methods indicated in the previous paragraph have exponential time dependency in the worst case: each symbol more in the input multiplies theparsing time by a constant factor. Such methods are unusable except for very small
input length, where 20 symbols is about the maximum. Even the best of the abovemethods require cubic time in the worst case: for 10 tokens they do 1000 actions, for
100 tokens 1000 000 actions and for 1000 tokens 1 000 000 000 actions, which, at 10microseconds per action will already take almost 3 hours. It is clear that for real speed
we should like to have a linear-time general parsing method. Unfortunately no suchmethod has been discovered to date. On the other hand, there is no proof and not even
an indication that such a method could not exist. (Compare this to the situation aroundunrestricted phrase structure parsing, where it has been proved that no algorithm for it
can exist; see Section 3.5.2.) Worse even, nobody has ever come up with a specific CFgrammar for which no ad hoc linear-time parser could be designed. The only thing is
that we have at present no way to construct such a parser in the general case. This is atheoretically and practically unsatisfactory state of affairs that awaits further clarification.#
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

There is a theoretically interesting but impractical method by Valiant [CF 1975] which doesgeneral CF parsing in

O(n 2.81). Since this is only very slightly better than O(n 3.00) and since

Sec. 3.6] An overview of parsing methods 77

In the meantime (and perhaps forever), we shall have to drop one of the twoadjectives from our goal, a linear-time general parser. We can have a general parser,
which will need cubic time at best, or we can have a linear-time parser, which will notbe able to handle all CF grammars, but not both. Fortunately there are parsing methods
(in particular LR parsing) that can handle very large classes of grammars but still, agrammar that is designed without regard for a parsing method and just describes the
intended language in the most natural way has a small chance of allowing linear pars-ing automatically. In practice, grammars are often first designed for naturalness and
then adjusted by hand to conform to the requirements of an existing parsing method.Such an adjustment is usually relatively simple, depending on the parsing method
chosen. In short, making a linear-time parser for an arbitrary given grammar is 10%hard work; the other 90% can be done by computer.

We can achieve linear parsing time by restricting the number of possible moves ofour non-deterministic parsing automaton to one in each situation. Since the moves of
such an automaton involve no choice, it is called a deterministic automaton.The moves of a deterministic automaton are determined unambiguously by the
input stream (we can speak of a stream now, since the automaton operates from left toright); as a result it can give only one parsing for a sentence. This is all right if the
grammar is unambiguous, but if it is not, the act of making the automaton deterministichas pinned us down to one specific parsing; we shall say more about this in Section
9.6.5.All that remains is to explain how a deterministic control mechanism for a parsing
automaton can be derived from a grammar. Since there is no single good solution to theproblem, it is not surprising that quite a number of sub-optimal solutions have been
found. From a very global point of view they all use the same technique: they analysethe grammar in depth to bring to the surface information that can be used to identify
dead ends. These are then closed. If the method, applied to a grammar, closes enoughdead ends so that no choices remain, the method succeeds for that grammar and gives
us a linear-time parser. Otherwise it fails and we either have to look for a differentmethod or adapt our grammar to the method.

A (limited) analogy with the maze problem can perhaps make this clearer. If weare allowed to do preprocessing on the maze (unlikely but instructive) the following
method will often make our search through it deterministic. We assume that the mazeconsists of a grid of square rooms; see Figure 3.9(a). Now, if there is a room with three
walls, add the fourth wall. Continue with this process until no rooms with three wallsare left. If all rooms now have either two or four walls, there are no choices left and our
method has succeeded; see Figure 3.9(b, c). We see how this method brings informa-tion about dead ends to the surface, to help restricting the choice.

It should be pointed out that the above analogy is a limited one. It is concernedwith only one object, the maze, which is preprocessed. In parsing we are concerned
with two objects, the grammar, which is static and can be preprocessed, and the input,which varies.

Returning to the parsing automaton, we can state the fact that it is deterministic
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

the actions required are very complicated and time-consuming, Valiant's algorithm is betteronly for inputs of millions of symbols. Also, as it is a non-directional method, it would require

all these symbols to be in memory.

78 Introduction to parsing [Ch. 3

(a) (b)

...
...... .. . . . . . . . ....

.................
.... .. . . . . . . . .. ..

...
.............
...........

...
...
.....
...

....
...........
............

...
...
...
...
.

(c)

...
...
....

.... .. .. ..

......

...
...
...

Figure 3.9 A single-exit maze made deterministic by preprocessing
more precisely: a parsing automaton is deterministic with look-ahead k if its controlcan, given the internal administration and the next k symbols of the input, decide unambiguously what to do next (to either match or predict and what to predict in the top-down case, and to either shift or reduce and how to reduce in the bottom-up case). Like
grammar types, linear parsing methods are indicated by initials, like LL, LALR etc. If amethod X uses a look-ahead of k symbols it is called X (k ).

3.6.5 Linear top-down and bottom-up methodsThere is only one linear top-down method, called LL; the first L stands for Left-toright, the second for "identifying the Left-most production", as directional top-downparsers do. LL parsing is treated in Chapter 8. LL parsing, especially LL(1) is very
popular. LL(1) parsers are often generated by a parser generator but a simple variantcan, with some effort, be written by hand, using recursive-descent techniques; see Section 8.2.6. Occasionally, the LL(1) method is used starting from the last token of theinput backwards; it is then called RR(1).

There are quite a variety of linear bottom-up methods, the most powerful beingcalled LR, where again the L stand for Left-to-right and the R stand for "identifying the
Right-most production". Linear bottom-up methods are treated in Chapter 9. Theirparsers are invariably generated by a parser generator: the control mechanism of such a
parser is so complicated that it is not humanly possible to construct it by hand. Some ofthe linear bottom-up methods are very popular and are perhaps used even more widely
than the LL(1) method. LR(1) parsing is more powerful than LL(1) parsing, but alsomore difficult to understand and less convenient. The other methods cannot be compared easily to the LL(1) method. See Chapter 12 for a comparison of practical parsingmethods. The LR(1) method can also be applied backwards and is then called RL(1).

The great difference in variety between top-down and bottom-up methods is easilyunderstood when we look more closely at the choices the corresponding parsers face. A
top-down parser has by nature little choice: if a terminal symbol is predicted, it has nochoice and can only ascertain that a match is present; only if a non-terminal is predicted
it has a choice in the production of that non-terminal. A bottom-up parser can alwaysshift the next input symbol, even if a reduction is also possible (and it often has to do
so). If, in addition, a reduction is possible, it may have a choice between a number ofright-hand sides. In general it has more choice than a top-down parser and more powerful methods are needed to make it deterministic.

Sec. 3.6] An overview of parsing methods 79
3.6.6 Almost deterministic methodsWhen our attempt to construct a deterministic control for a parser fails and leaves us
with an almost deterministic one, we need not despair yet. We can fall back onbreadth-first search to solve the remnants of non-determinism at run-time. The better
our original method was, the less non-determinism will be left, the less often breadth-first search will be needed and the more efficient our parser will be. This avenue of
thought has been explored for bottom-up parsers by Tomita [CF 1986], who achieveswith it what is probably the best general CF parser available today.

Of course, by reintroducing breadth-first search we are taking chances. The gram-mar and the input could conspire so that the non-determinism gets hit by each input
symbol and our parser will again have exponential time dependency. In practice, how-ever, they never do so and such parsers are very useful.

Tomita's parser is treated in Section 9.8. No corresponding research on top-downparsers has been reported in the literature. This is perhaps due to the fact that no
amount of breadth-first searching can handle left-recursion in a grammar (left-recursion is explained in Section 6.3.2).

3.6.7 Left-corner parsingIn Section 3.6 we wrote that "almost" all parsing methods could be assigned a place in
Figure 3.10. The principal class of methods that has been left out concerns "left-cornerparsing". It is a third division alongside top-down and bottom-up, and since it is a
hybrid between the two it should be assigned a separate column between these.In left-corner parsing, the right-hand side of each production rule is divided into
two parts: the left part is called the left corner and is identified by bottom-up methods.The division of the right-hand side is done so that once its left corner has been identified, parsing of the right part can proceed by a top-down method.Although left-corner parsing has advantages of its own, it tends to combine the
disadvantages or at least the problems of top-down and bottom-up parsing, and ishardly used in practice. For this reason it has not been included in Figure 3.10. From a
certain point of view, top-down and bottom-up can each be considered special cases ofleft-corner, which gives it some theoretical significance. See Section 13.7 for literature
references.
3.6.8 ConclusionFigure 3.10 summarizes parsing techniques as they are treated in this book. Nijholt
[Misc 1981] paints a more abstract view of the parsing landscape, based on left-cornerparsing. See Deussen [Misc 1979] for an even more abstracted overview. An early systematic survey was given by Griffiths and Petrick [CF 1965].

80 Introduction to parsing [Ch. 3

Top-down Bottom-up
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma Non-directional

methods Unger parser CYK parser\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

Directional methods

The predict/match automatonDepth-first search (backtrack)
Breadth-first search(Greibach)
Recursive descentDefinite Clause grammars

The shift/reduce automatonDepth-first search (backtrack)
Breadth-first searchBreadth-first search, restricted

(Earley)
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

Linear directionalmethods:
breadth-first, withbreadth restricted to 1

There is only one top-downmethod:

LL(k)

There is a whole gamut ofmethods:

precedencebounded-context
LR(k)LALR(1)
SLR(1)\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
Efficient generaldirectional methods:

maximally restrictedbreadth-first search

(no research reported) Tomita

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

Figure 3.10 An overview of parsing techniques
4
General non-directional methods

In this chapter we will present two general parsing methods, both non-directional:Unger's method and the CYK method. These methods are called non-directional
because they access the input in an seemingly arbitrary order. They require the entireinput to be in memory before parsing can start.

Unger's method is top-down; if the input belongs to the language at hand, it mustbe derivable from the start symbol of the grammar. Therefore, it must be derivable
from a right-hand side of the start symbol, say A 1A 2 . . . Am . This, in turn, means thatA

1 must derive a first part of the input, A 2 a second part, etc. If the input sentence isz
1z 2 . . . zn, this demand can be depicted as follows:

A 1 . . . Ai . . . Am

z 1 . . . zk . . . zn
Unger's method tries to find a partition of the input that fits this demand. This is arecursive problem: if a non-terminal A

i is to derive a certain part of the input, theremust be a partition of this part that fits a right-hand side of A

i. Ultimately, such aright-hand side must consist of terminal symbols only, and these can easily be matched

with the current part of the input.The CYK method approaches the problem the other way around: it tries to find
occurrences of right-hand sides in the input; whenever it finds one, it makes a note thatthe corresponding left-hand side derives this part of the input. Replacing the
occurrence of the right-hand side with the corresponding left-hand side results in somesentential forms that derive the input. These sentential forms are again the subject of a
search for right-hand sides, etc. Ultimately, we may find a sentential form that bothderives the input sentence and is a right-hand side of the start symbol.

In the next two sections, these methods are investigated in detail.

82 General non-directional methods [Ch. 4
4.1 UNGER'S PARSING METHOD
The input to Unger's parsing method [CF 1968] consists of a CF grammar and an inputsentence. We will first discuss Unger's parsing method for grammars without e-rules

and without loops (see Section 2.8.4). Then, the problems introduced by e-rules will bediscussed, and the parsing method will be modified to allow for all CF grammars.

4.1.1 Unger's method without e-rules or loopsTo see how Unger's method solves the parsing problem, let us consider a small example. Suppose we have a grammar rule

S o"" ABC | DE | F
and we want to find out whether S derives the input sentence pqrs. The initial parsingproblem can then be schematically represented as:

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma S
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma pqrs

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 

\Theta \Theta  \Theta \Theta \Theta 

For each right-hand side we must first generate all possible partitions of the input sen-tence. Generating partitions is not difficult: if we have m cups, numbered from 1 to m,
and n marbles, numbered from 1 to n, we have to find all possible partitions such thateach cup contains at least one marble, the numbers of the marbles in any cup are consecutive, and any cup does not contain lower-numbered marbles than any marble in alower-numbered cup. We proceed as follows: first, we put marble 1 in cup 1, and then
generate all partitions of the other n -1 marbles over the other m -1 cups. This gives usall partitions that have marble 1 in the first cup. Next, we put marbles 1 and 2 in the
first cup, and then generate all partitions of the other n -2 marbles over the other m -1cups, etc. If n is less than m, no partition is possible.

Partitioning the input corresponds to partitioning the marbles (the input symbols)over the cups (the right-hand side symbols). If a right-hand side has more symbols than
the sentence, no partition can be found (there being no e-rules). For the first right-handside the following partitions must be tried:

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma S
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma A B C
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma p q rs

p qr spq r s
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta 

The first partition results in the following sub-problems: does A derive p, does B deriveq, and does C derive rs? These sub-problems must all be answered in the affirmative,
or the partition is not the right one.For the second right-hand side, we obtain the following partitions:

Sec. 4.1] Unger's parsing method 83

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma S
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma D E
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma p qrs

pq rspqr s
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta 

The last right-hand side results in the following partition:

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma S
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma F
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma pqrs

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

All these sub-problems deal with shorter sentences, except the last one. They willall lead to similar split-ups, and in the end many will fail because a terminal symbol in
a right-hand side does not match the corresponding part of the partition. The only par-tition that causes some concern is the last one. It is as complicated as the one we started
with. This is the reason that we have disallowed loops in the grammar. If the grammarhas loops, we may get the original problem back again and again. For instance, if there
is a rule F o""S in the example above, this will certainly happen.The above demonstrates that we have a search problem here, and we can attack it
with either the depth-first or the breadth-first search technique (see Section 3.6.2).Unger uses depth-first search.

In the following discussion, the grammar of Figure 4.1 will serve as an example.

EExxpprrSS -->> EExxpprr ++ TTeerrmm || TTeerrmm

TTeerrmm -->> TTeerrmm ** FFaaccttoorr || FFaaccttoorr
FFaaccttoorr -->> (( EExxpprr )) || ii

Figure 4.1 A grammar describing simple arithmetic expressions
This grammar represents the language of simple arithmetic expressions, with operators
++ and **, and operand ii. We will use the sentence ((ii++ii))**ii as input example. So, theinitial problem can be represented as:

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

((ii++ii))**ii\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 

\Theta \Theta  \Theta \Theta \Theta 

Fitting the first right-hand side of EExxpprr with the input results in the following parti-tions:

84 General non-directional methods [Ch. 4

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr ++ TTeerrmm
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

(( ii ++ii))**ii
(( ii++ ii))**ii
(( ii++ii ))**ii
(( ii++ii)) **ii
(( ii++ii))** ii
((ii ++ ii))**ii
((ii ++ii ))**ii
((ii ++ii)) **ii
((ii ++ii))** ii
((ii++ ii ))**ii
((ii++ ii)) **ii
((ii++ ii))** ii
((ii++ii )) **ii
((ii++ii ))** ii
((ii++ii)) ** ii\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

Even a small example like this already results in 15 partitions, and we will not examinethem all here, although the unoptimized version of the algorithm requires this. We will
only examine the partitions that have at least some chance of succeeding: we can elim-inate all partitions that do not match the terminal symbol of the right-hand side. So, the
only partition worth investigating further is:

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr ++ TTeerrmm
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

((ii ++ ii))**ii\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 

\Theta \Theta 

\Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 

\Theta \Theta 

The first sub-problem here is to find out whether and, if so, how EExxpprr derives ((ii. Wecannot partition

((ii into three non-empty parts because it only consists of 2 symbols.Therefore, the only rule that we can apply is the rule

EExxpprr -->> TTeerrmm. Similarly, theonly rule that we can apply next is the rule
TTeerrmm -->> FFaaccttoorr. So, we now have

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma TTeerrmm
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma FFaaccttoorr
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

((ii\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 

However, this is impossible, because the first right-hand side of FFaaccttoorr has too manysymbols, and the second one consists of one terminal symbol only. Therefore, the partition we started with does not fit, and it must be rejected. The other partitions werealready rejected, so we can conclude that the rule

EExxpprr -->> EExxpprr ++ TTeerrmm does notderive the input.

The second right-hand side of EExxpprr consists of only one symbol, so we only haveone partition here, consisting of one part. Partitioning this part for the first right-hand
side of TTeerrmm again results in 15 possibilities, of which again only one has a chance of

Sec. 4.1] Unger's parsing method 85
succeeding:

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma TTeerrmm
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma TTeerrmm ** FFaaccttoorr
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

((ii++ii)) ** ii\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 

Continuing our search, we will find the following derivation:

EExxpprr -->>
TTeerrmm -->>
TTeerrmm ** FFaaccttoorr -->>
FFaaccttoorr ** FFaaccttoorr -->>
(( EExxpprr )) ** FFaaccttoorr -->>
(( EExxpprr ++ TTeerrmm )) ** FFaaccttoorr -->>
(( TTeerrmm ++ TTeerrmm )) ** FFaaccttoorr -->>
(( FFaaccttoorr ++ TTeerrmm )) ** FFaaccttoorr -->>
(( ii ++ TTeerrmm )) ** FFaaccttoorr -->>
(( ii ++ FFaaccttoorr )) ** FFaaccttoorr -->>
(( ii ++ ii )) ** FFaaccttoorr -->>
(( ii ++ ii )) ** ii

and this is the only derivation to be found.This example demonstrates several aspects of the method: even small examples
require a considerable amount of work, but even some simple checks can result in hugesavings. For instance, matching the terminal symbols in a right-hand side with the partition at hand often leads to the rejection of the partition without investigating it anyfurther. Unger [CF 1968] presents several more of these checks. For instance, one can
compute the minimum length of strings of terminal symbols derivable from each non-terminal. Once it is known that a certain non-terminal only derives terminal strings of
length at least n, all partitions that fit this non-terminal with a substring of length lessthan n can be immediately rejected.

4.1.2 Unger's method with e-rulesSo far, we only have dealt with grammars without e-rules, and not without reason.
Complications arise when the grammar contains e-rules, as is demonstrated by the fol-lowing example: consider the grammar rule So""ABC and input sentence pqr. If we
want to examine whether this rule derives the input sentence, and we allow for e-rules,many more partitions will have to be investigated, because each of the non-terminals A,
B, and C may derive the empty string. In this case, generating all partitions proceedsjust as above, except that we first generate the partitions that have no marble at all in
the first cup, then the partitions that have marble 1 in the first cup, etc.:

86 General non-directional methods [Ch. 4

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma S
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma A B C
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma pqr

p qrpq r
pqrp qr
p q rp qr
pq rpq r
pqr\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

Now suppose that we are investigating whether B derives pqr, and suppose there is arule B o""SD. Then, we will have to investigate the following partitions:

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma B
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma S D
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma pqr

p qrpq r
pqr\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

It is the last of these partitions that will cause trouble: in the process of finding outwhether S derives pqr, we end up asking the same question again, in a different context.
If we are not careful and do not detect this, our parser will loop forever, or run out ofmemory.

When searching along this path, we are looking for a derivation that is using aloop in the grammar. This may even happen if the grammar does not contain loops. If
this loop actually exists in the grammar, there are infinitely many derivations to befound along this path, provided that there is one, so we will never be able to present
them all. The only interesting derivations are the ones without the loop. Therefore, wewill cut off the search process in these cases. On the other hand, if the grammar does
not contain such a loop, a cut-off will not do any harm either, because the search isdoomed to fail anyway. So, we can avoid the problem altogether by cutting off the
search process in these cases. Fortunately, this is not a difficult task. All we have to dois to maintain a list of questions that we are currently investigating. Before starting to
investigate a new question (for instance "does S derive pqr?") we first check that thequestion does not already appear in the list. If it does, we do not investigate this question. Instead, we proceed as if the question were answered negatively.Consider for instance the following grammar:

SS -->> LLSSDD || ee
LL -->> ee
DD -->> dd

This grammar generates sequences of dd's in an awkward way. The complete search for

Sec. 4.1] Unger's parsing method 87
the questions SS o""o""** dd? and SS o""o""** dddd? is depicted in Figure 4.2.

SS o""o""** dd??

LL SS DD
-- -- dd
-- dd --
dd -- --

ee o""o""** dd?? no

LL o""o""** ee?? ee o""o""** ee?? yes

SS o""o""** ee??

LL SS DD

-- -- --

LL o""o""** ee?? ee o""o""** ee?? yes

SS o""o""** ee?? cut-off: noee o""o""
** ee?? yes

DD o""o""** dd?? dd o""o""** dd?? yes
LL o""o""** ee?? ee o""o""** ee?? yes

SS o""o""** dd?? cut-off: no
LL o""o""** dd?? ee o""o""** dd?? no

SS o""o""** dddd??

LL SS DD
-- -- dddd
-- dd dd
-- dddd --
dd -- dd
dd dd --
dddd -- --

LL o""o""** ee?? ee o""o""** ee?? yes

SS o""o""** ee??

LL SS DD

-- -- --

LL o""o""** ee?? ee o""o""** ee?? yes

SS o""o""** ee?? cut-off: noee o""o""
** ee?? yes

DD o""o""** dddd?? dd o""o""** dddd?? no

LL o""o""** ee?? ee o""o""** ee?? yes

SS o""o""** dd?? see above, yes
DD o""o""** dd?? dd o""o""** dd?? yes
LL o""o""** ee?? ee o""o""** ee?? yes
SS o""o""** dddd?? cut-off: no
LL o""o""** dd?? ee o""o""** dd?? no
LL o""o""** dd?? ee o""o""** dd?? no
LL o""o""** dddd?? ee o""o""** dddd?? noee o""o""
** dddd?? no

Figure 4.2 Unger's parser at work for the sentences dd and dddd
Figure 4.2 must be read from left to right, and from top to bottom. The questions aredrawn in an ellipse, with the split-ups over the right-hand sides in boxes. A question is

88 General non-directional methods [Ch. 4
answered affirmatively if at least one of the boxes results in a "yes". In contrast, a par-tition only results in an affirmative answer if all questions arising from it result in a
"yes".Checking for cut-offs is easy: if a new question is asked, we follow the arrows in
the reversed direction (to the left). This way, we traverse the list of currently investi-gated questions. If we meet the question again, we have to cut off the search.

To find the parsings, every question that is answered affirmatively has to passback a list of rules that start the derivation asked for in the question. This list can be
placed into the ellipse, together with the question. We have not done so in Figure 4.2,because it is complicated enough as it is. However, if we strip Figure 4.2 of its dead
ends, and leave out the boxes, we get Figure 4.3.

SS o""o""** dddd?? yes

SS o""o"" LLSSDD

LL o""o""** ee?? yes

LL o""o"" e

SS o""o""** dd?? yes

SS o""o"" LLSSDD

LL o""o""** ee?? yes

LL o""o"" e

SS o""o""** ee?? yes

SS o""o"" e

DD o""o""** dd?? yes

DD o""o"" dd
DD o""o""** dd?? yes

DD o""o"" dd

Figure 4.3 The result of Unger's parser for the sentence dddd
In this case, every ellipse only has one possible grammar rule. Therefore, there is onlyone parsing, and we obtain it by reading Figure 4.3 from left to right, top to bottom:

SS -->> LLSSDD -->> SSDD -->> LLSSDDDD -->> SSDDDD -->> DDDD -->> ddDD -->> dddd.
In general, the total number of parsings is equal to the product of the number of gram-mar rules in each ellipse.

This example shows that we can save much time by remembering answers toquestions. For instance, the question whether

LL derives e is asked many times. Sheil[CF 1976] has shown that the efficiency improves dramatically when this is done: it

goes from exponential to polynomial. Another possible optimization is achieved bycomputing in advance which non-terminals can derive e. In fact, this is a special case of
computing the minimum length of a terminal string that each non-terminal derives. If anon-terminal derives e, this minimum length is 0.

4.2 THE CYK PARSING METHOD
The parsing method described in this section is attributed to J. Cocke, D.H. Younger,and T. Kasami, who, independently, discovered variations of the method; it is now

known as the Cocke-Younger-Kasami method, or the CYK method. The most accessi-ble original description is that of Younger [CF 1967]. A much earlier description is by
Sakai [CF 1962].As with Unger's parsing method, the input to the CYK algorithm consists of a CF

Sec. 4.2] The CYK parsing method 89
grammar and an input sentence. The first phase of the algorithm constructs a table tel-ling us which non-terminal(s) derive which substrings of the sentence. This is the
recognition phase. It ultimately also tells us whether the input sentence can be derivedfrom the grammar. The second phase uses this table and the grammar to construct all
possible derivations of the sentence.We will first concentrate on the recognition phase, which really is the distinctive
feature of the algorithm.
4.2.1 CYK recognition with general CF grammarsTo see how the CYK algorithm solves the recognition and parsing problem, let us consider the grammar of Figure 4.4. This grammar describes the syntax of numbers inscientific notation. An example sentence produced by this grammar is

3322..55ee++11. Wewill now use this grammar and sentence as an example.

NNuummbbeerrSS -->> IInntteeggeerr || RReeaall
IInntteeggeerr -->> DDiiggiitt || IInntteeggeerr DDiiggiitt

RReeaall -->> IInntteeggeerr FFrraaccttiioonn SSccaallee
FFrraaccttiioonn -->> .. IInntteeggeerr

SSccaallee -->> ee SSiiggnn IInntteeggeerr || EEmmppttyy
DDiiggiitt -->> 00 || 11 || 22 || 33 || 44 || 55 || 66 || 77 || 88 || 99
EEmmppttyy -->> ee

SSiiggnn -->> ++ || --

Figure 4.4 A grammar describing numbers in scientific notation
The CYK algorithm first concentrates on substrings of the input sentence, shortestsubstrings first, and then works its way up. The following derivations of substrings of
length 1 can be read directly from the grammar:

DDiiggiitt DDiiggiitt DDiiggiitt SSiiggnn DDiiggiitt

33 22 .. 55 ee ++ 11

This means that DDiiggiitt derives 33, DDiiggiitt derives 22, etc. Note however, that this pic-ture is not yet complete. For one thing, there are several other non-terminals deriving
33. This complication arises because the grammar contains so-called unit rules, rules ofthe form A o""B, where A and B are non-terminals. Such rules are also called single

rules or chain rules. We can have chains of them in a derivation. So, the next stepconsists of applying the unit rules, repetitively, for instance to find out which other
non-terminals derive 33. This gives us the following result:

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

SSiiggnn

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

33 22 .. 55 ee ++ 11

90 General non-directional methods [Ch. 4
Now, we already see some combinations that we recognize from the grammar: Forinstance, an

IInntteeggeerr followed by a DDiiggiitt is again an IInntteeggeerr, and a .. (dot) fol-lowed by an
IInntteeggeerr is a FFrraaccttiioonn. We get (again also using unit rules):

NNuummbbeerr, IInntteeggeerr FFrraaccttiioonn SSccaallee
NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

SSiiggnn

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

33 22 .. 55 ee ++ 11

At this point, we see that the RReeaall-rule is applicable in several ways, and then the
NNuummbbeerr-rule, so we get:

NNuummbbeerr, RReeaall

NNuummbbeerr, RReeaall
NNuummbbeerr, IInntteeggeerr FFrraaccttiioonn SSccaallee
NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

SSiiggnn

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

33 22 .. 55 ee ++ 11

We find that NNuummbbeerr does indeed derive 3322..55ee++11.In the example above, we have seen that unit rules complicate things a bit.
Another complication, one that we have avoided until now, is formed by e-rules. Forinstance, if we want to recognize the input

4433..11 according to the example grammar,we have to realize that
SSccaallee derives e here, so we get the following picture:

NNuummbbeerr, RReeaall

NNuummbbeerr, RReeaall
NNuummbbeerr, IInntteeggeerr FFrraaccttiioonn SSccaallee
NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

NNuummbbeerr
IInntteeggeerr

DDiiggiitt

44 33 .. 11

In general this is even more complicated. We must take into account the fact thatseveral non-terminals can derive e between any two adjacent terminal symbols in the
input sentence, and also in front of the input sentence or at the back. However, as weshall see, the problems caused by these kinds of rules can be solved, albeit at a certain
cost. In the meantime, we will not let these problems discourage us. In the example,

Sec. 4.2] The CYK parsing method 91
we have seen that the CYK algorithm works by determining which non-terminalsderive which substrings, shortest substrings first. Although we skipped them in the
example, the shortest substrings of any input sentence are, of course, the e-substrings.We shall have to be able to recognize them in arbitrary position, so let us first see if we
can compute R e , the set of non-terminals that derive e.Initially, this set R

e consists of the set of non-terminals A for which A o""e is agrammar rule. For the example grammar, R

e is initially the set { EEmmppttyy }. Next, wecheck each grammar rule: If a right-hand side consists only of symbols that are a

member of R e , we add the left-hand side to R e (it derives e, because all symbols in theright-hand side do). In the example,

SSccaallee would be added. This process is repeateduntil no new non-terminals can be added to the set. For the example, this results in

R e = { EEmmppttyy, SSccaallee }.
Now, we direct our attention to the non-empty substrings of the input sentence. Sup-pose we have an input sentence z = z

1z 2 . . . zn and we want to compute the set ofnon-terminals that derive the substring of z starting at position i, of length l. We will

use the notation si,l for this substring, so,

si,l = zizi +1 . . . zi +l -1.
Figure 4.5 presents this notation graphically, using a sentence of 4 symbols.

s 1,4

s 2,3
s 1,3

s 3,2
s 2,2
s 1,2
s 1,1 s 2,1 s 3,1 s 4,1

z 1 z 2 z 3 z 4

Figure 4.5 A graphical presentation of substrings
We will use the notation Rsi,l for the set of non-terminals deriving the substring si,l.
This notation can be extended to deal with substrings of length 0: si, 0 = e, andR

si, 0 = R e .Because shorter substrings are dealt with first, we can assume that we are at a

stage in the algorithm where all information on substrings with length smaller than acertain l is available. Using this information, we check each right-hand side in the
grammar, to see if it derives si,l, as follows: suppose we have a right-hand sideA

1 . . . Am . Then we divide si,l into m (possibly empty) segments, such that A 1 derivesthe first segment, A

2 the second, etc. We start with A 1. If A 1 . . . Am is to derive si,l,A
1 has to derive a first part of it, say of length k. That is, A 1 must derive si,k (be amember of R

si,k ), and A 2 . . . Am must derive the rest:

92 General non-directional methods [Ch. 4

A 1 A 2 . . . Am

zi . . . zi +k -1 zi +k zi +k +1 . . . zi +l -1
This is attempted for every k for which A 1 is a member of Rsi,k , including 0. Naturally,
if A 1 is a terminal, then A 1 must be equal to zi, and k is 1. Checking if A 2 . . . Amderives z

i +k . . . zi +l -1 is done in the same way. Unlike Unger's method, we do nothave to try all partitions, because we already know which non-terminals derive which

substrings.Nevertheless, there are several problems with all this: in the first place, m could
be 1 and A 1 a non-terminal, so we are dealing with a unit rule. In this case, A 1 mustderive the whole substring s

i,l, and thus be a member of Rsi,l , which is the set that weare computing now, so we do not know yet if this is the case. This problem can be

solved by observing that if A 1 is to derive si,l, somewhere along the derivation theremust be a first step not using a unit rule. So we have:

A 1 o"" B o"" . . . o"" C o""* si,l
where C is the first non-terminal using a non-unit rule in the derivation. Disregardinge-rules (the second problem) for a moment, this means that at a certain stage, C will be

added to the set Rsi,l . Now, if we repeat the process again and again, at some point, B
will be added, and during the next repetition, A 1 will be added. So, we have to repeatthe process until no new non-terminals are added to R

si,l . The second problem iscaused by the e-rules. If all but one of the A
t derive e, we have a problem that is basi-cally equivalent to the problem of unit rules. It can be solved in the same way.

In the end, when we have computed all the Rsi,l , the recognition problem is solved:
the start symbol S derives z (= s 1,n) if and only if S is a member of Rs 1,n .

This is a complicated process, but part of this complexity stems from the e-rulesand the unit rules. Their presence forces us to do the R

si,l computation repetitively.Another, less serious source of the complexity is that a right-hand side may consist of

arbitrary many non-terminals, so trying all possibilities can be a lot of work. So, impos-ing certain restrictions on the rules may simplify this process a great deal. However,
these restrictions should not limit the generative power of the grammar significantly.
4.2.2 CYK recognition with a grammar in Chomsky Normal FormTwo of the restrictions that we want to impose on the grammar are obvious by now: no
unit rules and no e-rules. We would also like to limit the maximum length of a right-hand side to 2; this would simplify checking that a right-hand side derives a certain
substring. It turns out that there is a form for CF grammars that exactly fits these res-trictions: the Chomsky Normal Form. It is as if this normal form was invented for this
algorithm. A grammar is in Chomsky Normal Form (CNF), when all rules either havethe form A o""a, or A o""BC, where a is a terminal and A, B, and C are non-terminals.
Fortunately, as we shall see later, almost all CF grammars can be mechanicallytransformed into a CNF grammar.

We will first discuss how the CYK-algorithm works for a grammar in CNF.

Sec. 4.2] The CYK parsing method 93
There are no e-rules in a CNF grammar, so R e is empty. The sets Rsi, 1 can be read
directly from the rules: they are determined by the rules of the form A o""a. A ruleA o""BC can never derive a single terminal, because there are no e-rules.

Next, we proceed iteratively as before, first processing all substrings of length 2,then all substrings of length 3, etc. When a right-hand side BC is to derive a substring
of length l, B has to derive the first part (which is non-empty), and C the rest (alsonon-empty).

B C
zi . . . zi +k -1 zi +k . . . zi +l -1
So, B must derive si,k , that is, B must be a member of Rsi,k , and, likewise, C must
derive si +k,l -k , that is, C must be a member of Rsi +k,l -k . Determining if such a k exists is
easy: just try all possibilities; they range from 1 to l -1. All sets Rsi,k and Rsi +k,l -k have
already been computed at this point.This process is much less complicated than the one we saw before, with a general

CF grammar, for two reasons: the most important one is that we do not have to repeatthe process again and again until no new non-terminals are added to R

si,l . Here, the sub-strings we are dealing with are really substrings. They cannot be equal to the string we

started out with. The second reason is that we only have to find one place where thesubstring must be split in two, because the right-hand side only consists of two nonterminals. In ambiguous grammars, there can be several different splittings, but at thispoint, that does not worry us. Ambiguity is a parsing issue, not a recognition issue.

The algorithm results in a complete collection of sets Rsi,l . The sentence z consists of only n symbols, so a substring starting at position i can never have more thann +1-i symbols. This means that there are no substrings s

i,l with i +l >n +1. Therefore,the R
si,l sets can be organized in a triangular table, as depicted in Figure 4.6.

Rs1,n
Rs1,n-1 Rs2,n-1

.. .. ..
.. .. .. ..
Rs1,l .. Rsi,l .. ..

.. .. .. .. .. ..
.. .. .. .. .. .. ..
Rs1,1 .. Rsi, 1 .. .. Rsi+l-1,1 .. Rsn, 1

V W

Figure 4.6 Form of the recognition table
This table is called the recognition table, or the well-formed substring table. Rsi,l is
computed following the arrows V and W simultaneously, looking for rules A o""BC with

94 General non-directional methods [Ch. 4
B a member of a set on the V arrow, and C a member of the corresponding set on the Warrow. For B, substrings are taken starting at position i, with increasing length k. So the
V arrow is vertical and rising, visiting Rsi, 1 , Rsi, 2 , . . . , Rsi,k , . . . , Rsi,l -1 ; for C, substrings are taken starting at position i +k, with length l -k, with end-position i +l -1, sothe W arrow is diagonally descending, visiting R

si +1,l -1 , Rsi +2,l -2 , . . . , Rsi +k,l -k , . . . ,R
si +l -1,1 .As described above, the recognition table is computed in the order depicted in

Figure 4.7(a). We could also compute the recognition table in the order depicted in Fig-ure 4.7(b). In this last order, R

si,l is computed as soon as all sets and input symbolsneeded for its computation are available. For instance, when computing R

s 3,3 , Rs 5,1 isrelevant, but R
s 6,1 is not, because the substring at position 3 with length 3 does not con-tain the substring at position 6 with length 1. This order makes the algorithm particularly suitable for on-line parsing, where the number of symbols in the input is notknown in advance, and additional information is computed each time a symbol is
entered.

(a) off-line order (b) on-line order
Figure 4.7 Different orders in which the recognition table can be computed
Now, let us examine the cost of this algorithm. Figure 4.6 shows that there are(n*(n +1))/2 substrings to be examined. For each substring, at most n -1 different kpositions have to be examined. All other operations are independent of n, so the algo-rithm operates in a time at most proportional to the cube of the length of the input sentence. As such, it is far more efficient than exhaustive search, which needs a time thatis exponential in the length of the input sentence.

4.2.3 Transforming a CF grammar into Chomsky Normal FormThe previous section has demonstrated that it is certainly worth while to try to
transform a general CF grammar into CNF. In this section, we will discuss thistransformation, using our number grammar as an example. The transformation is split
up into several stages:
\Gamma  first, e-rules are eliminated.

\Gamma  then, unit rules are eliminated.
\Gamma  then, non-productive non-terminals are removed.
\Gamma  then, non-reachable non-terminals are removed.
\Gamma  then, finally, the remaining grammar rules are modified, and rules are added, until

they all have the desired form, that is, either A o""a or A o""BC.All these transformations will not change the language defined by the grammar. This is

not proven here. Most books on formal language theory discuss these transformations

Sec. 4.2] The CYK parsing method 95
more formally and provide proofs, see for example Hopcroft and Ullman [Books 1979].
4.2.3.1 Eliminating e-rulesSuppose we have a grammar G, with an e-rule A o""e, and we want to eliminate this
rule. We cannot just remove the rule, as this would change the language defined by thenon-terminal A, and also probably the language defined by the grammar G. So, something has to be done about the occurrences of A in the right-hand sides of the grammarrules. Whenever A occurs in a grammar rule B o""aA b, we replace this rule with two
others: B o""aA c'b, where A c' is a new non-terminal, for which we shall add rules later(these rules will be the non-empty grammar rules of A), and B o""ab, which handles the
case where A derives e in a derivation using the B o""aA b rule. Notice that the a and bin the rules above could also contain A; in this case, each of the new rules must be
replaced in the same way, and this process must be repeated until all occurrences of Aare removed. When we are through, there will be no occurrence of A left in the grammar. Every e-rule must be handled in this way. Of course, during this process new erules may originate. This is only to be expected: the process makes all e-derivationsexplicit. The newly created e-rules must be dealt with in exactly the same way. Ultimately, this process will stop, because the number of non-terminals deriving e is lim-ited and, in the end, none of these non-terminals occur in any right-hand side.

The next step in eliminating the e-rules is the addition of grammar rules for thenew non-terminals. If A is a non-terminal for which an A c' was introduced, we add a
rule A c'o""a for all non-e-rules A o""a. Since all e-rules have been made explicit, we canbe sure that if a rule does not derive e directly, it cannot do so indirectly. A problem
that may arise here is that there may not be a non-e-rule for A. In this case, A onlyderives e, so we remove all rules using A c'.

All this leaves us with a grammar that still contains e-rules. However, none of thenon-terminals having an e-rule occurs in any right-hand side. These occurrences have
just been carefully removed. So, these non-terminals can never play a role in anyderivation from the start symbol S, with one important exception: S itself. In particular,
we now have a rule So""e if and only if e is a member of the language defined by thegrammar G. All other non-terminals with e-rules can be removed safely. Cleaning up
the grammar is left to later transformations.

SS -->> LL aa MM
LL -->> LL MM
LL -->> ee
MM -->> MM MM
MM -->> ee

Figure 4.8 An example grammar to test e-rule elimination schemes
The grammar of Figure 4.8 is a nasty grammar to test your e-rule eliminationscheme. Our scheme transforms this grammar into the grammar of Figure 4.9. This
grammar still has e-rules, but these will be eliminated by the removal of non-productive and/or non-reachable non-terminals. Cleaning up this mess will leave only
one rule: So""a. Removing the e-rules in our number grammar results in the grammarof Figure 4.10. Note that the two rules to produce e,

EEmmppttyy and SSccaallee, are still

96 General non-directional methods [Ch. 4

SS -->> LL'' aa MM'' || aa MM'' || LL'' aa || aa
LL -->> LL'' MM'' || LL'' || MM'' || ee
MM -->> MM'' MM'' || MM'' || ee
LL'' -->> LL'' MM'' || LL'' || MM''
MM'' -->> MM'' MM'' || MM''

Figure 4.9 Result after our e-rule elimination scheme
NNuummbbeerrSS -->> IInntteeggeerr || RReeaall
IInntteeggeerr -->> DDiiggiitt || IInntteeggeerr DDiiggiitt

RReeaall -->> IInntteeggeerr FFrraaccttiioonn SSccaallee'' || IInntteeggeerr FFrraaccttiioonn
FFrraaccttiioonn -->> .. IInntteeggeerr

SSccaallee'' -->> ee SSiiggnn IInntteeggeerr

SSccaallee -->> ee SSiiggnn IInntteeggeerr || ee
EEmmppttyy -->> ee
DDiiggiitt -->> 00 || 11 || 22 || 33 || 44 || 55 || 66 || 77 || 88 || 99

SSiiggnn -->> ++ || --

Figure 4.10 Our number grammar after elimination of e-rules
present but are not used any more.
4.2.3.2 Eliminating unit rulesThe next trouble-makers to be eliminated are the unit rules, that is, rules of the form
A o""B. It is important to realize that, if such a rule A o""B is used in a derivation, it mustbe followed at some point by the use of a rule B o""a. Therefore, if we have a rule
A o""B, and the rules for B are

B o"" a1 | a2 | . . . | an,
we can replace the rule A o""B with

A o"" a1 | a2 | . . . | an.
In doing this, we can of course introduce new unit rules. In particular, when repeatingthis process, we could at some point again get the rule A o""B. In this case, we have an

infinitely ambiguous grammar, because B derives B. Now this may seem to pose aproblem, but we can just leave such a unit rule out; the effect is that we short-cut
derivations like

A o"" B o"" . . . o"" B o"" . . .
Also rules of the form A o""A are left out. In fact, a pleasant side-effect of removing e-rules and unit rules is that the resulting grammar is not infinitely ambiguous any more.

Removing the unit rules in our e-free number grammar results in the grammar ofFigure 4.11.

4.2.3.3 Removing non-productive non-terminalsNon-productive non-terminals are non-terminals that have no terminal derivation.
Every sentential form that can be derived from it will contain non-terminals. These are

Sec. 4.2] The CYK parsing method 97

NNuummbbeerrSS -->> 00 || 11 || 22 || 33 || 44 || 55 || 66 || 77 || 88 || 99
NNuummbbeerrSS -->> IInntteeggeerr DDiiggiitt
NNuummbbeerrSS -->> IInntteeggeerr FFrraaccttiioonn SSccaallee'' || IInntteeggeerr FFrraaccttiioonn
IInntteeggeerr -->> 00 || 11 || 22 || 33 || 44 || 55 || 66 || 77 || 88 || 99
IInntteeggeerr -->> IInntteeggeerr DDiiggiitt

RReeaall -->> IInntteeggeerr FFrraaccttiioonn SSccaallee'' || IInntteeggeerr FFrraaccttiioonn
FFrraaccttiioonn -->> .. IInntteeggeerr

SSccaallee'' -->> ee SSiiggnn IInntteeggeerr

SSccaallee -->> ee SSiiggnn IInntteeggeerr || ee
EEmmppttyy -->> ee
DDiiggiitt -->> 00 || 11 || 22 || 33 || 44 || 55 || 66 || 77 || 88 || 99

SSiiggnn -->> ++ || --

Figure 4.11 Our number grammar after eliminating unit rules
not pleasant things to have in a grammar. Naturally, "proper" grammars do not havethem. Nevertheless, we must be able to determine which non-terminals do have a terminal derivation, if only to check that a grammar is "proper".To find out which non-terminals have a terminal derivation we use a scheme that
hinges on the fact that a non-terminal has a terminal derivation if and only if it has aright-hand side consisting of symbols that all have a terminal derivation. Of course, terminals have themselves a terminal derivation. The scheme works as follows: First, wemark the non-terminals that have a right-hand side containing only terminals: they
obviously have a terminal derivation. Next, we mark all non-terminals that have aright-hand side consisting only of terminals and marked non-terminals: they too have a
terminal derivation. We keep on doing this until there are no more non-terminals to bemarked.

Now, the non-productive non-terminals are the ones that have not been marked inthe process. We remove all rules that contain a non-marked non-terminal in either the
left-hand side or the right-hand side. This process does not remove all rules of amarked non-terminal, as there must be at least one rule for it with a right-hand side
consisting only of terminals and marked non-terminals, or it would not have beenmarked in the first place. (This may remove all rules, including those for the startsymbol, in which case the grammar describes the empty language).Our number grammar does not contain non-productive non-terminals, so it will
not be changed by this phase.
4.2.3.4 Removing non-reachable non-terminalsA non-terminal is called reachable or accessible if there exists at least one sentential
form, derivable from the start symbol, in which it occurs. So, a non-terminal A is reach-able if So""* aA b for some a and b. A non-terminal is non-reachable if it is not reachable. For non-reachable non-terminals the same holds as for non-productive non-terminals: they do not occur in "proper" grammars. However, they can be introduced
by some of the transformations that we have seen before, so we must be able to findthem to "clean up" a grammar again.

We found the non-productive non-terminals by finding the "useful" ones. Like-wise, we find the non-reachable non-terminals by finding the reachable ones. For this,

98 General non-directional methods [Ch. 4
we can use the following scheme: First, the start symbol is marked: it is reachable.Then, any time an as yet unmarked non-terminal is marked, all non-terminals occurring
in any of its right-hand sides are marked. In the end, the unmarked non-terminals arenot reachable and their rules can be removed. They do not occur in any right-hand side
of a reachable non-terminal, for otherwise it would have been marked in the process.It is interesting to note that removing non-reachable non-terminals does not introduce non-productive non-terminals. However, first removing non-reachable non-terminals and then removing non-productive non-terminals may produce a grammar
which contains again non-reachable non-terminals. Finding an example demonstratingthis is left to the reader.

In our number grammar, the non-terminals RReeaall, SSccaallee, and EEmmppttyy are non-reachable, which leaves us with the grammar of Figure 4.12.

NNuummbbeerrSS -->> 00 || 11 || 22 || 33 || 44 || 55 || 66 || 77 || 88 || 99
NNuummbbeerrSS -->> IInntteeggeerr DDiiggiitt
NNuummbbeerrSS -->> IInntteeggeerr FFrraaccttiioonn SSccaallee'' || IInntteeggeerr FFrraaccttiioonn
IInntteeggeerr -->> 00 || 11 || 22 || 33 || 44 || 55 || 66 || 77 || 88 || 99
IInntteeggeerr -->> IInntteeggeerr DDiiggiitt
FFrraaccttiioonn -->> .. IInntteeggeerr

SSccaallee'' -->> ee SSiiggnn IInntteeggeerr

DDiiggiitt -->> 00 || 11 || 22 || 33 || 44 || 55 || 66 || 77 || 88 || 99

SSiiggnn -->> ++ || --

Figure 4.12 Our number grammar after removal of non-reachable rules
4.2.3.5 Finally, to Chomsky Normal FormAfter all these grammar transformations, we have a grammar without e-rules or unit
rules, all non-terminal are reachable, and there are no non-productive non-terminals.So, we are left with two types of rules: rules of the form A o""a, which are already in the
proper form, and rules of the form A o""X 1X 2 . . . Xm , with m >=2. For every terminal boccurring in such a rule we create a new non-terminal T

b with as only rule Tbo""b, andwe replace each occurrence of b in a rule A o""X
1X 2 . . . Xm with Tb. Now, the onlyrules not yet in CNF are of the form A o""X
1X 2 . . . Xm , with m >=3, and all Xi a non-terminal. These rules can now just be split up:

A o"" X 1X 2 . . . Xm
is replaced by the following two rules:

A o"" A 1X 3 . . . XmA

1 o"" X 1X 2

where A 1 is a new non-terminal. Now, we have replaced the original rule with one thatis one shorter, and one that is in CNF. This splitting can be repeated until all parts are
in CNF. Figure 4.13 represents our number grammar in CNF.

Sec. 4.2] The CYK parsing method 99

NNuummbbeerrSS -->> 00 || 11 || 22 || 33 || 44 || 55 || 66 || 77 || 88 || 99
NNuummbbeerrSS -->> IInntteeggeerr DDiiggiitt
NNuummbbeerrSS -->> NN11 SSccaallee'' || IInntteeggeerr FFrraaccttiioonn

NN11 -->> IInntteeggeerr FFrraaccttiioonn
IInntteeggeerr -->> 00 || 11 || 22 || 33 || 44 || 55 || 66 || 77 || 88 || 99
IInntteeggeerr -->> IInntteeggeerr DDiiggiitt
FFrraaccttiioonn -->> TT11 IInntteeggeerr

TT11 -->> ..
SSccaallee'' -->> NN22 IInntteeggeerr

NN22 -->> TT22 SSiiggnn
TT22 -->> ee
DDiiggiitt -->> 00 || 11 || 22 || 33 || 44 || 55 || 66 || 77 || 88 || 99

SSiiggnn -->> ++ || --

Figure 4.13 Our number grammar in CNF
4.2.4 The example revisitedNow, let us see how the CYK algorithm works with our example grammar, which we
have just transformed into CNF. Again, our input sentence is 3322..55ee++11. The recogni-tion table is given in Figure 4.14. The bottom row is read directly from the grammar;
for instance, the only non-terminals having a production rule with right-hand side 33 are
NNuummbbeerr, IInntteeggeerr, and DDiiggiitt. Notice that for each symbol a in the sentence theremust be at least one non-terminal A with a production rule A o""a, or else the sentence

cannot be derived from the grammar.The other rows are computed as described before. Actually, there are two ways to
compute a certain Rsi,l . The first method is to check each right-hand side in the grammar; for instance, to check whether the right-hand side NN11 SSccaallee'' derives the sub-string

22..55ee (= s 2,4). The recognition table derived so far tells us that
\Gamma  NN11 is not a member of Rs 2,1 or Rs 2,2 ,

\Gamma  NN11 is a member of Rs 2,3 , but SSccaallee'' is not a member of Rs 5,1

so the answer is no. Using this method, we have to check each right-hand side in thisway, adding the left-hand side to R

s 2,4 if we find that the right-hand side derives s 2,4.The second method is to compute possible right-hand sides from the recognition

table computed so far; for instance, Rs 2,4 is the set of non-terminals that have a righthand side AB where either
\Gamma  A is a member of Rs 2,1 and B is a member of Rs 3,3 , or

\Gamma  A is a member of Rs 2,2 and B is a member of Rs 4,2 , or
\Gamma  A is a member of Rs 2,3 and B is a member of Rs 5,1 .

This gives as possible combinations for AB: NN11 TT22 and NNuummbbeerr TT22. Now we checkall rules in the grammar to see if they have a right-hand side that is a member of this

set. If so, the left-hand side is added to Rs 2,4 .
4.2.5 CYK parsing with Chomsky Normal FormWe now have an algorithm that determines whether a sentence belongs to a language or
not, and it is much faster than exhaustive search. Most of us, however, not only want toknow whether a sentence belongs to a language, but also, if so, how it can be derived

100 General non-directional methods [Ch. 4

NNuummbbeerr

AE NNuummbbeerr
AE AE AE
NNuummbbeerr,

NN11 AE AE AE

AE NNuummbbeerr,NN11 AE AE SSccaallee''
NNuummbbeerr,
IInntteeggeerr AE FFrraaccttiioonn AE NN22 AE

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

TT11

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

TT22 SSiiggnn

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

7
6
5
4
l 3

2
1

331 222 ..3 554 ee5 ++6 117

i
Figure 4.14 The recognition table for the input sentence 3322..55ee++11
from the grammar. If it can be derived in more than one way, we probably want toknow all possible derivations. As the recognition table contains the information on all
derivations of substrings of the input sentence that we could possible make, it also con-tains the information we want. Unfortunately, this table contains too much information, so much that it hides what we want to know. The table may contain informationabout non-terminals deriving substrings, where these derivations cannot be used in the
derivation of the input sentence from the start symbol S. For instance, in the exampleabove, R

s 2,3 contains NN11, but the fact that NN11 derives 22..55 cannot be used in the deriva-tion of
3322..55ee++11 from NNuummbbeerr.The key to the solution of this problem lies in the simple observation that the

derivation must start with the start-symbol S. The first step of the derivation of theinput sentence z, with length n, can be read from the grammar, together with the recognition table. If n =1, there must be a rule So""z; if n >=2, we have to examine all rulesSo""AB, where A derives the first k symbols of z, and B the rest, that is, A is a member
of Rs 1,k and B is a member of Rsk +1,n -k , for some k. There must be at least one such a
rule, or else S would not derive z.Now, for each of these combinations AB we have the same problem: how does A

derive s 1,k and B derive sk +1,n -k ? These problems are solved in exactly the same way.It does not matter which non-terminal is examined first. Consistently taking the leftSec. 4.2] The CYK parsing method 101
most one results in a left-most derivation, consistently taking the right-most one resultsin a right-most derivation.

Notice that we can use an Unger-style parser for this. However, it would not haveto generate all partitions any more, because we already know which partitions will
work.Let us try to find a left-most derivation for the example sentence and grammar,
using the recognition table of Figure 4.14. We begin with the start symbol, NNuummbbeerr.Our sentence contains seven symbols, which is certainly more than one, so we have to
use one of the rules with a right-hand side of the form AB. The IInntteeggeerr DDiiggiitt rule isnot applicable here, because the only instance of

DDiiggiitt that could lead to a derivationof the sentence is the one in R

s 7,1 , but IInntteeggeerr is not a member of Rs 1,6 . The
IInntteeggeerr FFrraaccttiioonn rule is not applicable either, because there is no FFrraaccttiioonnderiving the last part of the sentence. This leaves us with the production rule

NNuummbbeerr
-->> NN11 SSccaallee'', which is indeed applicable, because NN11 is a member of Rs 1,4 , and

SSccaallee'' is a member of Rs 5,3 , so NN11 derives 3322..55 and SSccaallee'' derives ee++11.

Next, we have to find out how NN11 derives 3322..55. There is only one applicablerule:

NN11 -->> IInntteeggeerr FFrraaccttiioonn, and it is indeed applicable, because IInntteeggeerr is amember of R

s 1,2 , and FFrraaccttiioonn is a member of Rs 3,2 , so IInntteeggeerr derives 3322, and
FFrraaccttiioonn derives ..55. In the end, we find the following derivation:

NNuummbbeerr -->>
NN11 SSccaallee'' -->>
IInntteeggeerr FFrraaccttiioonn SSccaallee'' -->>
IInntteeggeerr DDiiggiitt FFrraaccttiioonn SSccaallee'' -->>
33 DDiiggiitt FFrraaccttiioonn SSccaallee'' -->>
33 22 FFrraaccttiioonn SSccaallee'' -->>
33 22 TT11 IInntteeggeerr SSccaallee'' -->>
33 22 .. IInntteeggeerr SSccaallee'' -->>
33 22 .. 55 SSccaallee'' -->>
33 22 .. 55 NN22 IInntteeggeerr -->>
33 22 .. 55 TT22 SSiiggnn IInntteeggeerr -->>
33 22 .. 55 ee SSiiggnn IInntteeggeerr -->>
33 22 .. 55 ee ++ IInntteeggeerr -->>
33 22 .. 55 ee ++ 11

Unfortunately, this is not exactly what we want, because this is a derivation that usesthe rules of the grammar of Figure 4.13, not the rules of the grammar of Figure 4.4, the
one that we started with.
4.2.6 Undoing the effect of the CNF transformationWhen we examine the grammar of Figure 4.4 and the recognition table of Figure 4.14,
we see that the recognition table contains the information we need on most of the non-terminals of the original grammar. However, there are a few non-terminals missing in
the recognition table: SSccaallee, RReeaall, and EEmmppttyy. SSccaallee and EEmmppttyy were removedbecause they became non-reachable, after the elimination of e-rules.

EEmmppttyy wasremoved altogether, because it only derived the empty string, and
SSccaallee was replacedby
SSccaallee'', where SSccaallee'' derives exactly the same as SSccaallee, except for the empty

102 General non-directional methods [Ch. 4
string. We can use this to add some more information to the recognition table: at everyoccurrence of

SSccaallee'', we add SSccaallee.The non-terminal

RReeaall was removed because it became non-reachable after elim-inating the unit rules. Now, the CYK algorithm does not require that all non-terminals

in the grammar be reachable. We could just as well have left the non-terminal RReeaall inthe grammar, and have transformed its rules to CNF. The CYK algorithm would then
have added RReeaall to the recognition table, wherever that would be appropriate. Therules for

RReeaall that would be added to the grammar of Figure 4.13 are:

RReeaall -->> NN11 SSccaallee'' || IInntteeggeerr FFrraaccttiioonn
The resulting recognition table is presented in Figure 4.15. In this figure, we havealso added an extra row at the bottom of the triangle. This extra row represents the
non-terminals that derive the empty string. These non-terminals can be considered aspossibly occurring between any two adjacent symbols in the sentence, and also in front
of or at the end of the sentence. The set Rsi, 0 represents the non-terminals that can be
considered as possibly occurring just in front of symbol zi and the set Rsn +1,0 represents
the ones that can occur at the end of the sentence.

NNuummbbeerr,

RReeaall

AE NNuummbbeerr,RReeaall

AE AE AE
NNuummbbeerr,

RReeaall,

NN11 AE AE AE

AE

NNuummbbeerr,

RReeaall,

NN11 AE AE

SSccaallee'',

SSccaallee

NNuummbbeerr,
IInntteeggeerr AE FFrraaccttiioonn AE NN22 AE

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

TT11

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

TT22 SSiiggnn

NNuummbbeerr,
IInntteeggeerr,

DDiiggiitt

SSccaallee,

EEmmppttyy

SSccaallee,

EEmmppttyy

SSccaallee,

EEmmppttyy

SSccaallee,

EEmmppttyy

SSccaallee,

EEmmppttyy

SSccaallee,

EEmmppttyy

SSccaallee,

EEmmppttyy

SSccaallee,

EEmmppttyy

7
6
5
4
l 3

2
1
0

331 222 ..3 554 ee5 ++6 117 8

i
Figure 4.15 The recognition table with SSccaallee, RReeaall, and EEmmppttyy added
Now, we have a recognition table which contains all the information we need to parse a

Sec. 4.2] The CYK parsing method 103
sentence with the original grammar. Again, a derivation starts with the start-symbol S.If A

1A 2 . . . Am is a right-hand side of S, we want to know if this rule can be applied,that is, if A

1A 2 . . . Am derives s 1,n. This is checked, starting with A 1. There are twocases:

\Gamma  A 1 is a terminal symbol. In this case, it must be the first symbol of s 1,n, or this

rule is not applicable. Then, we must check if A 2 . . . Am derives s 2,n -1, in thesame way that we are now checking if A

1A 2 . . . Am derives s 1,n.
\Gamma  A 1 is a non-terminal. In this case, it must be a member of a Rs 1,k , for some k, or

this rule is not applicable. Then, we must check if A 2 . . . Am derives sk +1,n -k , inthe same way that we are now checking if A

1A 2 . . . Am derives s 1,n. If we wantall parsings, we must do this for every k for which A

1 is a member of Rs 1,k .Notice that non-terminals deriving the empty string pose no problem at all,

because they appear as a member of Rsi, 0 for all i.
We have now determined whether the rule is applicable, and if it is, which parts of therule derive which substrings. The next step now is to determine how the substrings can

be derived. These tasks are similar to the task we started with, and are solved in thesame way. This process will terminate at some time, provided the grammar does not
contain loops. This is simply an Unger parser that knows in advance which partitionswill lead to a successful parse.

Let us go back to the grammar of Figure 4.4 and the recognition table of Figure4.15, and see how this works for our example input sentence. We now know that
NNuummbbeerr does derive 3322..55ee++11, and want to know how. We first ask ourselves: can weuse the

NNuummbbeerr -->> IInntteeggeerr rule? IInntteeggeerr is a member of Rs 1,1 and Rs 1,2 , but there

is nothing behind the IInntteeggeerr in the rule to derive the rest of the sentence, so we can-not use this rule. Can we use the

NNuummbbeerr -->> RReeaall rule? Yes we can, because RReeaallis a member of R

s 1,7 , and the length of the sentence is 7. So, we start our derivationwith

NNuummbbeerr -->> RReeaall -->> ......
Now, we get similar questions for the RReeaall non-terminal: can we use the RReeaall -->>
IInntteeggeerr FFrraaccttiioonn SSccaallee rule? Well, IInntteeggeerr is a member of Rs 1,1 , but we cannot find a FFrraaccttiioonn in any of the Rs 2,k sets. However, IInntteeggeerr is also a member of
Rs 1,2 , and FFrraaccttiioonn is a member of Rs 3,2 . Now, SSccaallee is a member of Rs 5,0 ; this does
not help because it would leave nothing in the rule to derive the rest. Fortunately,
SSccaallee is also a member of Rs 5,3 , and that matches exactly to the end of the string. So,

this rule is indeed applicable, and we continue our derivation:

NNuummbbeerr -->> RReeaall -->> IInntteeggeerr FFrraaccttiioonn SSccaallee -->> ......
The sentence is now split up into three parts:

104 General non-directional methods [Ch. 4

\Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma NNuummbbeerr
\Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma RReeaall
\Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma IInntteeggeerr FFrraaccttiioonn SSccaallee
\Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

33 22 .. 55 ee ++ 11\Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 

It is left to the reader to verify that we will find only one derivation, and that this is it:

NNuummbbeerr -->>
RReeaall -->>
IInntteeggeerr FFrraaccttiioonn SSccaallee -->>
IInntteeggeerr DDiiggiitt FFrraaccttiioonn SSccaallee -->>
DDiiggiitt DDiiggiitt FFrraaccttiioonn SSccaallee -->>
33 DDiiggiitt FFrraaccttiioonn SSccaallee -->>
33 22 FFrraaccttiioonn SSccaallee -->>
33 22 .. IInntteeggeerr SSccaallee -->>
33 22 .. DDiiggiitt SSccaallee -->>
33 22 .. 55 SSccaallee -->>
33 22 .. 55 ee SSiiggnn IInntteeggeerr -->>
33 22 .. 55 ee ++ IInntteeggeerr -->>
33 22 .. 55 ee ++ DDiiggiitt -->>
33 22 .. 55 ee ++ 11

4.2.7 A short retrospective of CYKWe have come a long way. We started with building a recognition table using the original grammar. Then we found that using the original grammar with its unit rules ande-rules is somewhat complicated, although it can certainly be done. We proceeded by
transforming the grammar to CNF. CNF does not contain unit rules or e-rules; our gainin this respect was that the algorithm for constructing the recognition table became
much simpler. The limitation of the maximum length of a right-hand side to 2 was again in efficiency, and also a little in simplicity. However, Sheil [CF 1976] has demonstrated that the efficiency only depends on the maximum number of non-terminalsoccurring in a right-hand side of the grammar, not on the length of the right-hand sides.
This can easily be understood, once one realizes that the efficiency depends (amongothers) on the number of cuts in a substring that are "difficult" to find, when checking
whether a right-hand side derives this substring. This number of "difficult" cuts onlydepends on the number of non-terminals in the right-hand side. So, for efficiency, CNF
is a bit too restrictive.A disadvantage of this transformation to CNF is that the resulting recognition
table lacks some information that we need to construct a derivation using the originalgrammar. In the transformation process, some non-terminals were thrown away,
because they became non-productive. Fortunately, the missing information couldeasily be added. Ultimately, this process resulted in almost the same recognition table
that we would get with our first attempt using the original grammar. It only containssome extra information on non-terminals that were added during the transformation of
the grammar to CNF. More importantly, however, it was obtained in a simpler andmuch more efficient way.

Sec. 4.2] The CYK parsing method 105
4.2.8 Chart parsingThe CYK algorithm is also known under the name of chart parsing. More precisely,
both techniques have a number of variants and some variants of the CYK algorithm areidentical to some variants of chart parsing. The most striking difference between them
lies in the implementation; conceptually both algorithms do the same thing: they collectpossible parsings for larger and larger chunks of the input.

Although often presented in a different format, a chart is just a recognition table.Figure 4.16 shows the recognition table of Figure 4.14 in a chart format: each arc
represents a non-terminal deriving the part of the sentence spanned by the arc.

33 22 .. 55 ee ++ 11
\Gamma  \Gamma  \Gamma  \Gamma  \Gamma  \Gamma  \Gamma  \Gamma DDiiggiitt

IInntteeggeerr

NNuummbbeerr

DDiiggiitt
IInntteeggeerr

NNuummbbeerr

TT11 DDiiggiitt

IInntteeggeerr

NNuummbbeerr

TT22 SSiiggnn DDiiggiitt

IInntteeggeerr

NNuummbbeerr

IInntteeggeerr

NNuummbbeerr

FFrraaccttiioonn NN22

NN11
NNuummbbeerr

SSccaallee''

NN11
NNuummbbeerr

NNuummbbeerr
NNuummbbeerr

Figure 4.16 The recognition table of Figure 4.14 in chart format
Several variants of chart parsing are discussed and compared in Bolc [NatLang1987].

5
Regular grammars and finite-state automata

Regular grammars are the simplest form of grammars that still have generative power.They can describe concatenation (joining two texts together) and repetition and can
specify alternatives, but they cannot express nesting. Regular grammars are probablythe best-understood part of formal linguistics and almost all questions about them can
be answered.

5.1 APPLICATIONS OF REGULAR GRAMMARS
In spite of their simplicity there are many applications of regular grammars, of whichwe will briefly mention the most important ones.

5.1.1 CF parsingIn some parsers for CF grammars, a subparser can be discerned that handles a regular
grammar; such a subparser is based implicitly or explicitly on the following surprisingphenomenon. Consider the sentential forms in left-most or right-most derivations.
Such a sentential form consists of a closed (finished) part, which contains terminalsymbols only and an open (unfinished) part which contains non-terminals as well. In
left-most derivations, the open part starts at the left-most non-terminal and extends tothe right, in right-most derivations, the open part starts at the right-most non-terminal
and extends to the left; see Figure 5.1 which uses sentential forms from Section 2.5.2.

dd ,, NN && NN NN ,, NN && hh

Figure 5.1 Open parts in left-most and right-most productions
Now it can be proved (and it is not difficult to show) that these open parts can bedescribed by a regular grammar (which follows from the CF grammar). Furthermore,
these open parts of the sentential form play an important role in some CF parsingmethods which explains the significance of regular grammars for CF parsing.

Sec. 5.1] Applications of regular grammars 107
5.1.2 Systems with finite memorySince CF (or stronger) grammars allow nesting and since nesting can, in principle, be
arbitrarily deep, the generation of correct CF (or stronger) sentences may, in principle,require an arbitrary amount of memory to temporarily hold the unprocessed nesting
information. Mechanical systems do not possess an arbitrary amount of memory andconsequently cannot exhibit CF behaviour and are restricted to regular behaviour. This
is immediately clear for simple mechanical systems like vending machines, trafficlights and video-recorders: they all behave according to a regular grammar. It is also in
principle true for more complicated mechanical systems, like a country's train systemor a computer. Here, the argument gets, however, rather vacuous since nesting information can be represented very efficiently and a little memory can take care of a lot ofnesting. Consequently, although these systems in principle exhibit regular behaviour, it
is often easier to describe them with CF or stronger means, even though that wouldincorrectly ascribe infinite memory to them.

Conversely, the global behaviour of many systems that do have much memory canstill be described by a regular grammar, and many CF grammars are already for a large
part regular. This is because regular grammars already take adequate care of concatena-tion, repetition and choice; context-freeness is only required for nesting. If we apply a
rule that produces a regular (sub)language (and which consequently could be replacedby a regular rule) "quasi-regular", we can observe the following. If all alternatives of a
rule contain terminals only, that rule is quasi-regular (choice). If all alternatives of arule contain only terminals and non-terminals the rules of which are quasi-regular and
non-recursive, then that rule is quasi-regular (concatenation). And if a rule is recursivebut recursion occurs only at the end of an alternative and involves only quasi-regular
rules, then that rule is again quasi-regular (repetition). This often covers large parts of aCF grammar. See Krzemien' and L/ukasiewicz [FS 1976] for an algorithm to identify all
quasi-regular rules in a grammar.Natural languages are a case in point. Although CF or stronger grammars seem
necessary to delineate the set of correct sentences (and they may very well be, to catchmany subtleties), quite a good rough description can be obtained through regular
languages. Consider the stylized grammar for the main clause in an Subject-Verb-Object (SVO) language in Figure 5.2.

MMaaiinnCCllaauussee -->> SSuubbjjeecctt VVeerrbb OObbjjeecctt

SSuubbjjeecctt -->> [[ aa || tthhee ]] AAddjjeeccttiivvee** NNoouunn

OObbjjeecctt -->> [[ aa || tthhee ]] AAddjjeeccttiivvee** NNoouunn

VVeerrbb -->> vveerrbb11 || vveerrbb22 || ......
AAddjjeeccttiivvee -->> aaddjj11 || aaddjj22 || ......

NNoouunn -->> nnoouunn11 || nnoouunn22 || ......

Figure 5.2 A not obviously quasi-regular grammar
This grammar is quasi-regular: VVeerrbb, AAddjjeeccttiivvee and NNoouunn are regular by themselves,
SSuubbjjeecctt and OObbjjeecctt are concatenations of repetitions of regular forms (regular non-terminals and choices) and are therefore quasi-regular, and so is

MMaaiinnCCllaauussee. It takessome work to bring this grammar into standard regular form, but it can be done, as

shown in Figure 5.3, in which the lists for verbs, adjectives and nouns have been abbre-viated to

vveerrbb, aaddjjeeccttiivvee and nnoouunn, to save space. Even (finite) context108 Regular grammars and finite-state automata [Ch. 5

MMaaiinnCCllaauussee -->> aa SSuubbjjAAddjjNNoouunn__vveerrbb__OObbjjeecctt
MMaaiinnCCllaauussee -->> tthhee SSuubbjjAAddjjNNoouunn__vveerrbb__OObbjjeecctt

SSuubbjjAAddjjNNoouunn__vveerrbb__OObbjjeecctt -->> nnoouunn vveerrbb__OObbjjeecctt
SSuubbjjAAddjjNNoouunn__vveerrbb__OObbjjeecctt -->> aaddjjeeccttiivvee SSuubbjjAAddjjNNoouunn__vveerrbb__OObbjjeecctt

vveerrbb__OObbjjeecctt -->> vveerrbb OObbjjeecctt

OObbjjeecctt -->> aa OObbjjAAddjjNNoouunn
OObbjjeecctt -->> tthhee OObbjjAAddjjNNoouunn

OObbjjAAddjjNNoouunn -->> nnoouunn
OObbjjAAddjjNNoouunn -->> aaddjjeeccttiivvee OObbjjAAddjjNNoouunn

vveerrbb -->> vveerrbb11 || vveerrbb22 || ......
aaddjjeeccttiivvee -->> aaddjj11 || aaddjj22 || ......

nnoouunn -->> nnoouunn11 || nnoouunn22 || ......

Figure 5.3 A regular grammar in standard form for that of Figure 5.2
dependency can be incorporated: for languages that require the verb to agree in numberwith the subject, we duplicate the first rule:

MMaaiinnCCllaauussee -->> SSuubbjjeeccttSSiinngguullaarr VVeerrbbSSiinngguullaarr OObbjjeecctt

|| SSuubbjjeeccttPPlluurraall VVeerrbbPPlluurraall OObbjjeecctt

and duplicate the rest of the grammar accordingly. The result is still regular. Nestedsubordinate clauses may seem a problem, but in practical usage the depth of nesting is
severely limited. In English, a sentence containing a subclause containing a subclausecontaining a subclause will baffle the reader, and even in German and Dutch nestings
over say five deep are frowned upon. We replicate the grammar the desired number oftimes and remove the possibility of further recursion from the deepest level. Then the
deepest level is regular, which makes the other levels regular in turn. The resultinggrammar will be huge but regular and will be able to profit from all simple and efficient techniques known for regular grammars. The required duplications and modifica-tions are mechanical and can be done by a program. Dewar, Bratley and Thorne
[NatLang 1969] describe an early example of this approach, Blank [NatLang 1989] arecent one.

5.1.3 Pattern searchingMany linear patterns, especially text patterns, have a structure that is easily expressed
by a (quasi-)regular grammar. Notations that indicate amounts of money in variouscurrencies, for instance, have the structure given by the grammar of Figure 5.4, where

\Gamma \Gamma  has been used to indicate a space symbol. Examples are $$\Gamma \Gamma  1199..9955 and YY\Gamma \Gamma  11660000. Suchnotations, however, do not occur in isolation but are usually embedded in long stretches
of text that itself does not conform to the grammar of Figure 5.4. To isolate the nota-tions, a recognizer (rather than a parser) is derived from the grammar that will accept
arbitrary text and will indicate where sequences of symbols are found that conform to

Sec. 5.1] Applications of regular grammars 109

AAmmoouunnttSS -->> CCuurrrreennccyySSyymmbbooll SSppaaccee** DDiiggiitt++ CCeennttss??
CCuurrrreennccyySSyymmbbooll -->> ff || $$ || YY || ## || ......

SSppaaccee -->> \Gamma \Gamma 
DDiiggiitt -->> [[00112233445566778899]]
CCeennttss -->> .. DDiiggiitt DDiiggiitt || ..----

Figure 5.4 A quasi-regular grammar for currency notations
the grammar. Parsing (or an other form of analysis) is deferred to a later stage. A tech-nique for constructing such a recognizer is given in Section 5.3.4.

5.2 PRODUCING FROM A REGULAR GRAMMAR
When producing from a regular grammar, the producer needs to remember only onething: which non-terminal is next. We shall illustrate this and further concepts using the

simple regular grammar of Figure 5.5.

SSSS -->> aa AA

SS -->> aa BB
AA -->> bb BB
AA -->> bb CC
BB -->> cc AA
BB -->> cc CC
CC -->> aa

Figure 5.5 Sample regular grammar
This grammar produces sentences consisting of an aa followed by an alternatingsequence of

bb's and cc's followed by a terminating aa. For the moment we shall restrictourselves to regular grammars in standard notation; further on we shall extend our

methods to more convenient forms.The one non-terminal the producer remembers is called its state and the producer
is said to be in that state. When a producer is in a given state, for instance, AA, it choosesone of the rules belonging to that state, for instance,

AA-->>bbCC, produces the bb and movesto state
CC. Such a move is called a state transition. It is customary to represent thestates and the possible transitions of a producer in a transition diagram, Figure 5.6,

where the above state transition is represented by the arc marked bb from AA to CC.

SS

AA

BB

CCCC a`a`
aa

aa

bb
cc

aabbcc

Figure 5.6 Transition diagram for the regular grammar of Figure 5.5

110 Regular grammars and finite-state automata [Ch. 5
SS is the initial state and the accepting state is marked a`; another convention (not usedhere) is to draw an accepting state as a double circle. The symbols on the arcs are those

produced by the corresponding move. The producer stops when it is in an acceptingstate. Like the non-deterministic automaton we saw in Section 3.4, the producer is an
automaton, a finite non-deterministic automaton, or finite-state automaton, to be exact."Finite" because it can only be in a finite number of states (5 in this case; 3 bits of
internal memory would suffice) and "non-deterministic" because, for instance, in state
SS it has more than one way to produce an aa.

5.3 PARSING WITH A REGULAR GRAMMAR
The above automaton for producing a sentence can in principle also be used for pars-ing. If we have a sentence, for instance,

aabbccbbaa, and want to check and parse it, we canview the above transition diagram as a maze and the (tokens in the) sentence as a guide.

If we manage to follow a path through the maze, matching symbols from our sentenceto those on the walls of the corridors as we go and end up in a` exactly at the end of the
sentence, we have checked the sentence and the names of the rooms we have visitedform the backbone of the parse tree. See Figure 5.7, where the path is shown as a dotted
line.

SS

AA

BB

CCCC a`a`
aa

aa

bb
cc

aabbcc. . . . . . . . . . . . .. . . . .
. . . . .

. . . . .. . . . . . . . .

. . . . .

.. . . . . . . . . . . . .

.....

......

......

......
......
..........
......
......

......

...

SS

aa

AA

bb

BB

cc

AA

bb

CC

aa

a`a`

Figure 5.7 Actual and linearized passage through the maze
Now this is easier said than done. How did we know, for instance, to turn left in room SSrather than right? Of course we could employ general maze-solving techniques (and
they would give us our answer in exponential time) but a much simpler and much moreefficient answer is available here: we split ourselves in two and head both ways. After
the first aa of aabbccbbaa we are in the set of rooms {AA, BB}. Now we have a bb to follow;from

BB there are no exits marked bb but from AA there are two, which lead to BB and CC. Sowe are now in rooms {

BB, CC}. Our path is now more difficult to depict but still easy tolinearize, as shown in Figure 5.8. We can find the parsing by starting at the end and

following the pointers backwards: a`a` <<-- CC <<-- AA <<-- BB <<-- AA <<-- SS. If the grammaris ambiguous the backward pointers may bring us to a fork in the road: an ambiguity
has been found and both paths have to be followed separately to find both parsings.With regular grammars, however, one is often not interested in the parse, but only in
the recognition: the fact that the input is correct and it ends here suffices.

Sec. 5.3] Parsing with a regular grammar 111

SS

AA
BB

BB
CC

AA
CC

BB
CC a`a`

aa bb cc bb aa

Figure 5.8 Linearized set-based passage through the maze
5.3.1 Replacing sets by statesAlthough the process described above is linear in the length of the input (each next
token takes an amount of work that is not dependent on the length of the input), still alot of work has to be done for each token. What is worse, the grammar has to be consulted repeatedly and so we expect the speed of the process to depend adversely on thesize of the grammar. Fortunately there is a surprising and fundamental improvement
possible: from the NFA in Figure 5.6 we construct a new automaton with a new set ofstates, where each new state is equivalent to a set of old states. Where the original
(non-deterministic) automaton was in doubt after the first aa, a situation we representedas {

AA, BB}, the new automaton firmly knows that after the first aa it is in state AABB.The states of the new automaton can be constructed systematically as follows. We

start with the initial state of the old automaton, which is also the initial state of the newone. For each new state we create, we examine its contents in terms of the old states,
and for each token in the language we determine to which set of old states the given setleads. These sets of old states are then considered states of the new automaton. If we
create the same state a second time, we do not analyse it again. This process is calledthe subset construction and results initially in a (deterministic) state tree. The state tree
for the grammar of Figure 5.5 is depicted in Figure 5.9. To stress that it systematicallychecks all new states for all symbols, outgoing arcs leading nowhere are also shown.
Newly generated states that have already been generated before are marked with a 4.

SS

AABB BBCC

AACC

a`a`
AACC
a`a`

BBCC

4
4
4

aa

bb

cc

aa

bb

cc

aa

bb

ccaa
bb

cc

Figure 5.9 Deterministic state tree for the grammar of Figure 5.5
The state tree of Figure 5.9 is turned into a transition diagram by leading thearrows to states marked

4 to their first-time representatives and removing the deadends. The new automaton is shown in Figure 5.10. When we now use the sentence

aabbccbbaa as a guide for traversing this transition diagram, we find that we are never indoubt and that we safely arrive at the accepting state. All outgoing arcs from a state

112 Regular grammars and finite-state automata [Ch. 5

SS AABB

BBCC

AACC

a`a`a`a`aa
bb

cc

aa
aa
ccbb

Figure 5.10 Deterministic automaton for the grammar of Figure 5.5
bear different symbols, so when following a list of symbols, we are always pointed toat most one direction. If in a given state there is no outgoing arc for a given symbol,
then that symbol may not occur in that position. If it is, the input is in error.There are two things to be noted here. The first is that we see that most of the possible states of the new automaton do not actually materialize: the old automaton had 5states, so there were 25=32 possible states for the new automaton while in fact it has
only 5; states like SSBB or AABBCC do not occur. This is usual; although there are non-deterministic finite-state automata with n states that turn into a DFA with 2n states,
these are rare and have to be constructed on purpose. The average garden variety NFAwith n states typically results in a DFA with less than or around 10*n states.

The second is that consulting the grammar is no longer required; the state of theautomaton together with the input token fully determine the next state. To allow efficient look-up the next state can be stored in a table indexed by the old state and theinput token. The table for our DFA is given in Figure 5.11.

input symbol
aa bb cc\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

SS AABB\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
AABB BBCC AACCold state \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

AACC a` BBCC\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
BBCC a` AACC\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 

\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

Figure 5.11 Transition table for the automaton of Figure 5.10
Using such a table, an input string can be checked at the expense of only a few machineinstructions per token. For the average DFA, most of the entries in the table are empty
(cannot be reached by correct input and refer to error states). Since the table can be ofconsiderable size (300 states times 100 tokens is normal), several techniques exist to
exploit the empty space by compressing the table. Dencker, Du"rre and Heuft [Misc1984] give a survey of some techniques.

The parse tree obtained looks as follows:

SS

aa

AABB

bb

BBCC

cc

AACC

bb

BBCC

aa

a`a`

Sec. 5.3] Parsing with a regular grammar 113
which is not the original parse tree. If the automaton is used only to recognize the inputstring this is no drawback; if the parse tree is required, it can be reconstructed in the
following fairly obvious bottom-up way. Starting from the last state a` and the last token
aa, we conclude that the last right-hand side (the "handle" in bottom-up parsing) was aa.Since the state was

BBCC, a combination of BB and CC, we look through the rules for BB and CC.We find that
aa derived from CC-->>aa, which narrows down BBCC to CC. The right-most bb andthe
CC combine into the handle bbCC which in the set {AA, CC} must derive from AA. Workingour way backwards we find the parsing:

SS

aa

AABB

AA. . . . . . . .

bb

BBCC

BB. . . . . . . .

cc

AACC

AA. . . . . . . .

bb

BBCC

CC. . . . . . . .

aa

a`a`

This method again requires the grammar to be consulted repeatedly; moreover, the wayback will not always be so straight as in the above example and we will have problems
with ambiguous grammars. Efficient full parsing of regular grammars has receivedrelatively little attention; substantial information can be found in Ostrand, Paull and
Weyuker [FS 1981].
5.3.2 Non-standard notationA regular grammar in standard form can only have rules of the form A o"" a and A o"" aB.
We shall now first extend our notation with two other types of rules, A o"" B and A o"" e,and show how to construct NFA's and DFA's for them. We shall then turn to regular
expressions and rules that have regular expressions as right-hand sides (for instance,P o""a *bQ) and show how to convert them into rules in the extended notation.

The grammar in Figure 5.12 contains examples of both new types of rules; Figure5.13 presents the usual trio of NFA, state tree and DFA for this grammar. First consider
the NFA. When we are in state SS we see the expected transition to state BB on the token
aa, resulting in the standard rule SS-->>aaBB. The non-standard rule SS-->>AA indicates that wecan get from state

SS to state AA without reading (or producing) a symbol; we then saythat we read the zero-length string e and that we make an e-transition (or e-move). The

rule AA-->>aaAA creates a transition from AA to AA marked aa and BB-->>bbBB does something simi-lar. The standard rule

BB-->>bb creates a transition marked bb to the accepting state, and thenon-standard rule
AA-->>ee creates an e-transition to the accepting state. e-transitionsshould not be confused with e-rules: unit rules create e-transitions to non-accepting

states and e-rules create e-transitions to accepting states.

SSSS -->> AA

SS -->> aa BB
AA -->> aa AA
AA -->> ee
BB -->> bb BB
BB -->> bb

Figure 5.12 Sample regular grammar with e-rules

114 Regular grammars and finite-state automata [Ch. 5

SS

AA

BB

a`a`a`a`
ee

aa

ee
bb

aa

bb
(a)

SSAAa`a`

AABBa`a`

AAa`a`

BBa`a`

AAa`a`

BBa`a`

4
4
aa

bb

aa
bb

aa
bb
aa

bb
(b)

SSAAa`a` AABBa`a`

AAa`a`

BBa`a`
aa

aa

bb

aa

bb(c)
Figure 5.13 NFA (a), state tree (b) and DFA (c) for the grammar of Figure 5.12

Now that we have constructed an NFA with e-moves, the question arises how wecan process the e-moves to obtain a DFA. To answer this question we use the same reasoning as before; in Figure 5.6, after having seen an aa we did not know if we were instate

AA or state BB and we represented that as {AA, BB}. Here, when we enter state SS, evenbefore having processed a single symbol, we already do not know if we are in states

SS,
AA or a`, since the latter two are reachable from SS through e-moves. So the initial state ofthe DFA is already compound:

SSAAa`a`. We now have to consider where this state leads tofor the symbols
aa and bb. If we are in SS then aa will bring us to BB and if we are in AA, aawill bring us to
AA. So the new state includes AA and BB, and since a`a` is reachable from AAthrough e-moves, it also includes a`a` and its name is

AABBa`a`. Continuing in this vein we canconstruct the complete state tree (Figure 5.13(b)) and collapse it into a DFA (c). Note

that all states of the DFA contain the NFA state a`a`, so the input may end in all of them.The set of NFA states reachable from a given state through e-moves is called the
e-closure of that state. The e-closure of, for instance, SS is {SS, AA, a`a`}.
5.3.3 DFA's from regular expressionsAs mentioned in Section 2.3.3, regular languages are often specified by regular expressions rather than by regular grammars. Examples of regular expressions are [[00--
99]]++((..[[00--99]]++))?? which should be read as "one or more symbols from the set 0 through9, possibly followed by a dot which must then be followed by one or more symbols

from 0 through 9" (and which represents numbers with possibly a dot in them) and
((aabb))**((pp||qq))++, which should be read as "zero or more strings aabb followed by one ormore

pp's or qq's" (and which is not directly meaningful). The usual forms occurring inregular expressions are recalled in the table in Figure 5.14; some systems provide more

possibilities, some provide fewer. In computer input, no difference is generally made

Sec. 5.3] Parsing with a regular grammar 115

Form Meaning Name
R 1R 2 R 1 followed by R 2 concatenationR

1 | R 2 R 1 or R 2 alternativeR * zero or more R's optional sequence (Kleene star)

R + one or more R's (proper) sequenceR ? zero or one R optional
(R ) R nesting[abc . . . ] any symbol from the set abc . . .
a the symbol a itself

Figure 5.14 Some usual elements of regular expressions
between the metasymbol ** and the symbol **, etc. Special notations will be necessary ifthe language to be described contains any of the symbols

|| ** ++ ?? (( )) [[ or ]].

Rule pattern replaced by:
P o""a (standard)P o""aQ (standard)
P o""Q (extended standard)P o""e (extended standard)

P o""a . . . P o""aTTo"" . . .
P o""(R 1 | R 2 | . . . ) . . . P o""R 1 . . .P o""R

2 . . .. . .

P o""(R ) . . . P o""R . . .P o""R * . . . P o""T

To""RTTo"" . . .
P o""R + . . . P o""RTTo""RT

To"" . . .P o""R ? . . . P o""R . . .
P o"" . . .
P o""[abc . . . ] . . . P o""(a | b | c | . . . ) . . .
Figure 5.15 Transformations on regular grammars
A regular expression can be converted into a regular grammar by using thetransformations given in Figure 5.15; this regular grammar can then be used to produce
a DFA as described above. There is also a method to create an NFA directly from theregular expression, which requires, however, some preprocessing on the regular expression; see Thompson [FS 1968].We shall illustrate the method using the expression

((aabb))**((pp||qq))++. Our method

116 Regular grammars and finite-state automata [Ch. 5
will also work for regular grammars that contain regular expressions (like A o""ab *cB)and we shall in fact immediately turn our regular expression into such a grammar:

SSSS -->> ((aabb))**((pp||qq))++
The T in the transformations stands for an intermediate non-terminal, to be chosenfresh for each application of a transformation; we use

AA, BB, CC . . . in the example sincethat is less confusing than T

1, T 2, T 3, . . . . The transformations are to be applied untilall rules are in (extended) standard form.

The first transformation that applies is P o"" R * . . . , which replaces
SSSS-->>((aabb))**((pp||qq))++ by

SSSS -->> AA 4

AA -->> ((aabb)) AA
AA -->> ((pp||qq))++

The first rule is already in the desired form and has been marked 4. The transforma-tions P o""(R ) . . . and P o""a . . . work on

AA-->>((aabb))AA and result in

AA -->> aa BB 4
BB -->> bb AA 4

Now the transformation P o"" R + . . . must be applied to AA-->>((pp||qq))++, yielding

AA -->> ((pp||qq)) CC
CC -->> ((pp||qq)) CC
CC -->> ee 4

The e originated from the fact that ((pp||qq))++ in AA-->>((pp||qq))++ is not followed by anything(of which e is a faithful representation). Now

AA-->>((pp||qq))CC and CC-->>((pp||qq))CC are easilydecomposed into

AA -->> pp CC 4
AA -->> qq CC 4
CC -->> pp CC 4
CC -->> qq CC 4

The complete extended-standard version can be found in Figure 5.16; an NFA andDFA can now be derived using the methods of Section 5.3.1 (not shown).

5.3.4 Fast text search using finite-state automataSuppose we are looking for the occurrence of a short piece of text, for instance, a word
or a name (the "search string") in a large piece of text, for instance, a dictionary or anencyclopedia. One naive way of finding a search string of length n in a text would be to
try to match it to the characters 1 to n; if that fails, shift the pattern one position and tryto match against characters 2 to n +1, etc., until we find the search string or reach the
end of the text. (Dictionaries and encyclopedias may be organized better, but a file con-taining a million business letters almost certainly would not.)

Sec. 5.3] Parsing with a regular grammar 117

SSSS -->> AA

AA -->> aa BB
BB -->> bb AA
AA -->> pp CC
AA -->> qq CC
CC -->> pp CC
CC -->> qq CC
CC -->> ee

Figure 5.16 Extended-standard regular grammar for ((aabb))**((pp||qq))++
Finite automata offer a much more efficient way to do text search. We derive aDFA from the string, let it run down the text and when it reaches an accepting state, it
has found the string. Assume for example that the search string is aabbaabbcc and that thetext will contain only

aa's, bb's and cc's. The NFA that searches for this string is shownin Figure 5.17(a); it was derived as follows. At each character in the text there are two

possibilities: either the search string starts there, which is represented by the chain ofstates going to the right, or it does not start there, in which case we have to skip the
present character and return to the initial state. The automaton is non-deterministic,since when we see an

aa in state A, we have two options: to believe that it is the start ofan occurrence of
aabbaabbcc or not to believe it.

AA BB CC DD EE a`a`aa bb aa bb cc

aabbcc

(a)

AA

AABB

AA
AA

aa
bb

cc

4
4

AABB
AACC

AA

aa
bb

cc

4
4

AABBDD

AA
AA

aa
bb

cc

4
4

AABB
AACCEE

AA

aa
bb

cc

4
4

AABBDD

AA
AAa`a`

aa
bb

cc

4
4 AABB

AA
AA

aa
bb

cc

4
4
4

(b)

AA AABB AACC AABBDD AACCEE AAa`a`aa bb aa bb cc

bbcc

cc bbcc cc bb bbcc

aa

aa aa aa

(c)
Figure 5.17 NFA (a), state tree (b) and DFA (c) to search for aabbaabbcc

118 Regular grammars and finite-state automata [Ch. 5

Using the traditional techniques, this NFA can be used to produce a state tree (b)and then a DFA (c). Figure 5.18 shows the states the DFA goes through when fed the
text aaaabbaabbaabbccaa.

AA AABB AABB AACC AABBDD AACCEE AABBDD AACCEE AAa`a` AABB

aa aa bb aa bb aa bb cc aa

Figure 5.18 State transitions of the DFA of Figure 5.17(c) on aaaabbaabbaabbccaa
This application of finite-state automata is known as the Aho and Corasick biblio-graphic search algorithm [FS 1975]. Like any DFA, it requires only a few machine
instructions per character. As an additional bonus it will search for several strings forthe price of one. The DFA corresponding to the NFA of Figure 5.19 will search simultaneously for KKaawwaabbaattaa, MMiisshhiimmaa and TTaanniizzaakkii; note that three different acceptingstates result, a`a`

KK, a`a`MM and a`a`TT.

AAKK BBKK CCKK DDKK EEKK FFKK GGKK HHKK a`a`KKkk aa ww aa bb aa tt aa

\Sigma  AAMM BBMM CCMM DDMM EEMM FFMM GGMM a`a`MMmm ii ss hh ii mm aa

AATT BBTT CCTT DDTT EETT FFTT GGTT HHTT a`a`TTtt aa nn ii zz aa kk ii

ee
ee

ee

Figure 5.19 Example of an NDA for searching multiple strings
The Aho and Corasick algorithm is not the last word in string search; it faces stiffcompetition from the Rabin-Karp algorithm# and the Boyer-Moore algorithm## neither
of which will be treated here, since they are based on different principles.

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

R.M. Karp, M.O. Rabin, "Efficient randomized pattern matching algorithms", Technical Re-port TR-31-81, Harvard Univ., Cambridge, Mass., 1981. We want to find a string

S of length lin a text
T. First we choose a hash function H that assigns a large integer to any string of length land calculate

H (S) and H (T [1..l ]). If they are equal, we compare S and T [1..l ]. If either failswe calculate

H (T [2..l +1]) and repeat the process. The trick is to choose H so that
H (T [p +1..p +l ]) can be calculated cheaply from H (T [p..p +l -1]). See also Sedgewick[CSBooks 1988], page 289.

## Robert S. Boyer, J. Strother Moore, "A fast string searching algorithm", Commun. ACM, vol.
20, no. 10, p. 762-772, Oct 1977. We want to find a string S of length l in a text T and start bypositioning

S [1] at T [1]. Now suppose that T [l ] does not occur in S; then we can shift S to
T [l +1] without missing a match, and thus increase the speed of the search process. This princi-ple can be extended to blocks of more characters. See also Sedgewick [CSBooks 1988], page

286.

6
General directional top-down methods

In this chapter, we will discuss top-down parsing methods that try to rederive the inputsentence by prediction. As explained in Section 3.3.1, we start with the start symbol
and try to produce the input from it. At any point in time, we have a sentential formthat represents our prediction of the rest of the input sentence:

rest of input
prediction

This sentential form consists of both terminals and non-terminals. If a terminal symbolis in front, we match it with the current input symbol. If a non-terminal is in front, we
pick one of its right-hand sides and replace the non-terminal with this right-hand side.This way, we all the time replace the left-most non-terminal, and in the end, if we
succeed, we have imitated a left-most production.

6.1 IMITATING LEFT-MOST PRODUCTIONS
Let us see how such a rederiving process could proceed with an example. Consider theexample grammar of Figure 6.1. This grammar produces all sentences with equal

numbers of aa's and bb's.

SS -->> aaBB || bbAA
AA -->> aa || aaSS || bbAAAA
BB -->> bb || bbSS || aaBBBB

Figure 6.1 A grammar producing all sentences with equal numbers of aa's and bb's
Let us try to parse the sentence aaaabbbb, by trying to rederive it from the start-symbol, SS.
SS is our first prediction. The first symbol of our prediction is a non-terminal, so wehave to replace it by one of its right-hand sides. In this grammar, there are two choices

for SS: either we use the rule SS-->>aaBB, or we use the rule SS-->>bbAA. The sentence starts withan

aa and not with a bb, so we cannot use the second rule here. Applying the first rule

120 General directional top-down methods [Ch. 6
leaves us with the prediction aaBB. Now, the first symbol of the prediction is a terminalsymbol. Here, we have no choice:

aa aabbbb
aa BB

We have to match this symbol with the current symbol of the sentence, which is also an
aa. So, we have a match, and accept the aa. This leaves us with the prediction BB for therest of the sentence:

aabbbb. The first symbol of the prediction is again a non-terminal, soit has to be replaced by one of its right-hand sides. Now, we have three choices. However, the first and the second are not applicable here, because they start with a bb, andwe need another

aa. Therefore, we take the third choice, so now we have prediction
aaBBBB:

aa aa bbbb

aa BBBBaa

Again, we have a match with the current input symbol, so we accept it and continuewith the prediction

BBBB for bbbb. Again, we have to replace the left-most BB by one of itschoices. The next terminal in the sentence is a

bb, so the third choice is not applicablehere. This still leaves us with two choices,
bb and bbSS. So, we can either try them both,or be a bit more intelligent about it. If we would take

bbSS, then we would get at leastanother
aa (because of the SS), so this cannot be the right choice. So, we take the bbchoice, and get the prediction

bbBB for bbbb. Again, we have a match, and this leaves uswith prediction
BB for bb. For the same reason, we take the bb choice again. After match-ing, this leaves us with an empty prediction. Luckily, we are also at the end of the input

sentence, so we accept it. If we had made notes of the production rules used, we wouldhave found the following derivation:

SS -->> aaBB -->> aaaaBBBB -->> aaaabbBB -->> aaaabbbb.
Figure 6.2 presents the steps of the parse in a tree-form. The dashed line separates thealready processed part from the prediction. All the time, the left-most symbol of the
prediction is processed.This example demonstrates several aspects that the parsers discussed in this
chapter have in common:
\Gamma  we always process the left-most symbol of the prediction;

\Gamma  if this symbol is a terminal, we have no choice: we have to match it with the

current input symbol or reject the parse;
\Gamma  if this symbol is a non-terminal, we have to make a prediction: it has to be

replaced by one of its right-hand sides. Thus, we always process the left-mostnon-terminal first, so we get a left-most derivation.

Sec. 6.1] The pushdown automaton 121

SS SS

aa BB

SS
aa BB

SS
aa BB

aa BB BB

SS
aa BB

aa BB BB

SS
aa BB

aa BB BB

bb
SS
aa BB

aa BB BB

bb

SS
aa BB

aa BB BB

bb bb

SS
aa BB

aa BB BB

bb bb

Figure 6.2 Production trees for the sentence aaaabbbb
6.2 THE PUSHDOWN AUTOMATON
The steps we have taken in the example above resemble very much the steps of a so-called pushdown automaton. A pushdown automaton (PDA) is an imaginary

mathematical device that reads input and has control over a stack. The stack can con-tain symbols that belong to a so-called stack alphabet. A stack is a list that can only be
accessed at one end: the last symbol entered on the list ("pushed") is the first symbol tobe taken from it ("popped"). This is also sometimes called a "first-in, last-out" list, or
a FILO list: the first symbol that goes in is the last symbol to come out. In the exampleabove, the prediction works like a stack, and this is what the pushdown automaton uses
the stack for too. We therefore often call this stack the prediction stack. The stack alsoexplains the name "pushdown" automaton: the automaton "pushes" symbols on the
stack for later processing.The pushdown automaton operates by popping a stack symbol and reading an
input symbol. These two symbols then in general give us a choice of several lists ofstack symbols to be pushed on the stack. So, there is a mapping of (input symbol, stack
symbol) pairs to lists of stack symbols. The automaton accepts the input sentencewhen the stack is empty at the end of the input. If there are choices (so an (input symbol, stack symbol) pair maps to more than one list), the automaton accepts a sentencewhen there are choices that lead to an empty stack at the end of the sentence.

This automaton is modeled after context-free grammars with rules in the so-called

122 General directional top-down methods [Ch. 6
Greibach Normal Form (GNF). In this normal form, all grammar rules have either theform A o""a or A o""aB

1B 2 . . . Bn, with a a terminal and A, B 1, ... , Bn non-terminals.The stack symbols are, of course, the non-terminals. A rule of the form

A o""aB 1B 2 . . . Bn leads to a mapping of the (a, A) pair to the list B 1B 2 . . . Bn. Thismeans that if the input symbol is an a, and the prediction stack starts with an A, we
could accept the a, and replace the A part of the prediction stack with B 1B 2 . . . Bn. Arule of the form A o""a leads to a mapping of the (a, A) pair to an empty list. The automaton starts with the start symbol of the grammar on the stack. Any context-freegrammar that does not produce the empty string can be put into Greibach Normal
Form. Most books on formal language theory discuss how to do this (see for instanceHopcroft and Ullman [Books 1979]).

The example grammar of Figure 6.1 already is in Greibach Normal Form, so wecan easily build a pushdown automaton for it. The automaton is characterized by the
mapping shown in Figure 6.3.

((aa,, SS)) -->> BB
((bb,, SS)) -->> AA
((aa,, AA)) -->>
((aa,, AA)) -->> SS
((bb,, AA)) -->> AAAA
((bb,, BB)) -->>
((bb,, BB)) -->> SS
((aa,, BB)) -->> BBBB

Figure 6.3 Mapping of the PDA for the grammar of Figure 6.1
An important remark to be made here is that many pushdown automata are non-deterministic. For instance, the pushdown automaton of Figure 6.3 can choose between
an empty list and an SS for the pair (aa, AA). In fact, there are context-free languages forwhich we cannot build a deterministic pushdown automaton, although we can build a
non-deterministic one. We should also mention that the pushdown automata as dis-cussed here are a simplification of the ones we find in automata theory. In automata
theory, pushdown automata have so-called states, and the mapping is from (state, inputsymbol, stack symbol) triplets to (state, list of stack symbols) pairs. Seen in this way,
they are like finite-state automata (discussed in Chapter 5), extended with a stack.Also, pushdown automata come in two different kinds: some accept a sentence by
empty stack, others accept by ending up in a state that is marked as an accepting state.Perhaps surprisingly, having states does not make the pushdown automaton concept
more powerful. Pushdown automata with states still only accept languages that can bedescribed with a context-free grammar. In our discussion, the pushdown automaton
only has one state, so we have taken the liberty of leaving it out.Pushdown automata as described above have several shortcomings that must be
resolved if we want to convert them into parsing automata. Firstly, pushdown auto-mata require us to put our grammar into Greibach Normal Form. While grammar
transformations are no problem for the formal linguist, we would like to avoid them asmuch as possible, and use the original grammar if we can. Now we could relax the
Greibach Normal Form requirement a little by also allowing terminals as stack sym-bols, and adding

Sec. 6.2] The pushdown automaton 123

(a, a) o""
to the mapping for all terminals a. We could then use any grammar all of whose right-hand sides start with a terminal. We could also split the steps of the pushdown automaton into separate "match" and "predict" steps, as we did in the example of Section 6.1.The "match" steps then correspond to usage of the

(a, a) o""
mappings, and the "predict" step then corresponds to a

(, A) o"" . . .
mapping, that is, a non-terminal on the top of the stack is replaced by one of its right-hand sides, without consuming a symbol from the input. For the grammar of Figure
6.1, this would result in the mapping shown in Figure 6.4, which is in fact just a rewriteof the grammar of Figure 6.1.

((,, SS)) -->> aaBB
((,, SS)) -->> bbAA
((,, AA)) -->> aa
((,, AA)) -->> aaSS
((,, AA)) -->> bbAAAA
((,, BB)) -->> bb
((,, BB)) -->> bbSS
((,, BB)) -->> aaBBBB
((aa,, aa)) -->>
((bb,, bb)) -->>

Figure 6.4 Match and predict mappings of the PDA for the grammar of Figure 6.1
We will see later that, even using this approach, we may have to modify the grammaranyway, but in the meantime, this looks very promising so we adopt this strategy. This
strategy also solves another problem: e-rules do not need special treatment any more.To get Greibach Normal Form, we would have to eliminate them. This is not necessary
any more, because they now just correspond to a

(, A) o""
mapping.The second shortcoming is that the pushdown automaton does not keep a record of
the rules (mappings) it uses. Therefore, we introduce an analysis stack into the auto-maton. For every prediction step, we push the non-terminal being replaced onto the
analysis stack, suffixed with the number of the right-hand side taken (numbering theright-hand sides of a non-terminal from 1 to n). For every match, we push the matched
terminal onto the analysis stack. Thus, the analysis stack corresponds exactly to theparts to the left of the dashed line in Figure 6.2, and the dashed line represents the
separation between the analysis stack and the prediction stack. This results in an

124 General directional top-down methods [Ch. 6
automaton that at any point in time has a configuration as depicted in Figure 6.5. In theliterature, such a configuration, together with its current state, stacks, etc. is sometimes
called an instantaneous description. In Figure 6.5, matching can be seen as pushing thevertical line to the right.

matched input rest of input

analysis prediction

Figure 6.5 An instantaneous description
The third and most important shortcoming, however, is the non-determinism.Formally, it may be satisfactory that the automaton accepts a sentence if and only if
there is a sequence of choices that leads to an empty stack at the end of the sentence,but for our purpose it is not, because it does not tell us how to obtain this sequence. We
have to guide the automaton to the correct choices. Looking back to the example ofSection 6.1, we had to make a choice at several points in the derivation, and we did so
based on some ad hoc considerations that were specific for the grammar at hand: some-times we looked at the next symbol in the sentence, and there were also some points
where we had to look further ahead, to make sure that there were no more aa's coming.In the example, the choices were easy, because all the right-hand sides start with a terminal symbol. In general, however, finding the correct choice is much more difficult.The right-hand sides could for instance equally well have started with a non-terminal
symbol that again has right-hand sides starting with a non-terminal, etc.In Chapter 8 we will see that many grammars still allow us to decide which righthand side to choose, given the next symbol in the sentence. In this chapter, however,we will focus on top-down parsing methods that work for a larger class of grammars.
Rather than trying to pick a choice based on ad hoc considerations, we would like toguide the automaton through all the possibilities. In Chapter 3 we saw that there are in
general two methods for solving problems in which there are several alternatives inwell-determined points: depth-first search and breadth-first search. We shall now see
how we can make the machinery operate for both search methods. Since the effectscan be exponential in size, even a small example can get quite big. We will use the
grammar of Figure 6.6, with test input aaaabbcc. This grammar generates a rather complexlanguage: sentences consist either of a number of

aa's followed by a number of bb's fol-lowed by an equal number of
cc's, or of a number of aa's followed by an equal numberof
bb's followed by a number of cc's. Example sentences are for instance: aabbcc, aaaabbbbcc.

SS -->> AABB || DDCC
AA -->> aa || aaAA
BB -->> bbcc || bbBBcc
DD -->> aabb || aaDDbb
CC -->> cc || ccCC

Figure 6.6 A more complicated example grammar

Sec. 6.2] Breadth-first top-down parsing 125
6.3 BREADTH-FIRST TOP-DOWN PARSING
The breadth-first solution to the top-down parsing problem is to maintain a list of allpossible predictions. Each of these predictions is then processed as described in Section 6.2 above, that is, if there is a non-terminal in front, the prediction stack is replacedby several new prediction stacks, as many as there are choices for this non-terminal. In
each of these new prediction stacks, the non-terminal is replaced by the correspondingchoice. This prediction step is repeated for all prediction stacks it applies to (including
the new ones), until all prediction stacks have a terminal in front. Then, for each of theprediction stacks we match the terminal in front with the current input symbol, and
strike out all prediction stacks that do not match. If there are no prediction stacks left,the sentence does not belong to the language. So, instead of one prediction
stack/analysis stack pair, our automaton now maintains a list of predictionstack/analysis stack pairs, one for each possible choice, as depicted in Figure 6.7.

matched input rest of input

analysis1 prediction1
analysis2 prediction2

... ...

Figure 6.7 An instantaneous description of our extended automaton
The method is suitable for on-line parsing, because it processes the input from leftto right. Any parsing method that processes its input from left to right and results in a
left-most derivation is called an LL parsing method. The first L stands for Left to right,and the second L for Left-most derivation.

Now, we almost know how to write a parser along these lines, but there is onedetail that we have not properly dealt with yet: termination. Does the input sentence
belong to the language defined by the grammar when, ultimately, we have an emptyprediction stack? Only when the input is exhausted! To avoid this extra check, and to
avoid problems about what to do when we arrive at the end of sentence but haven't fin-ished parsing yet, we introduce a special so-called end-marker

##, that is appended atthe end of the sentence. Also, a new grammar rule
SS''-->>SS## is added to the grammar,where
SS'' is a new non-terminal that serves as a new start symbol. The end-markerbehaves like an ordinary terminal symbol; when we have an empty prediction, we

know that the last step taken was a match with the end-marker, and that this match suc-ceeded. This also means that the input is exhausted, so it must be accepted.

6.3.1 An exampleFigure 6.8 presents a complete breadth-first parsing of the sentence

aaaabbcc##. At firstthere is only one prediction stack: it contains the start-symbol; no symbols have been

accepted yet (a). The step leading to (b) is a simple predict step; there is no otherright-hand side for

SS''. Another predict step leads us to (c), but this time there are twopossible right-hand sides, so we obtain two prediction stacks; note that the difference of

the prediction stacks is also reflected in the analysis stacks, where the different suffixesof

SS represent the different right-hand sides predicted. Another predict step withseveral right-hand sides leads to (d). Now, all prediction stacks have a terminal on top;

126 General directional top-down methods [Ch. 6

\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
(a) aaaabbcc## (b) aaaabbcc##\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS'' SS''11 SS##\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
(c) aaaabbcc## (d) aaaabbcc##\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS''11SS11 DDCC## SS''11SS11DD11 aabbCC##
SS''11SS22 AABB## SS''11SS11DD22 aaDDbbCC##

SS''11SS22AA11 aaBB##
SS''11SS22AA22 aaAABB##\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
(e) aa aabbcc## (f) aa aabbcc##\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS''11SS11DD11aa bbCC## SS''11SS11DD11aa bbCC##
SS''11SS11DD22aa DDbbCC## SS''11SS11DD22aaDD11 aabbbbCC##
SS''11SS22AA11aa BB## SS''11SS11DD22aaDD22 aaDDbbbbCC##
SS''11SS22AA22aa AABB## SS''11SS22AA11aaBB11 bbcc##

SS''11SS22AA11aaBB22 bbBBcc##
SS''11SS22AA22aaAA11 aaBB##
SS''11SS22AA22aaAA22 aaAABB##\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 
\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 
\Lambda \Lambda 
\Lambda 

\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
(g) aaaa bbcc## (h) aaaa bbcc##\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS''11SS11DD22aaDD11aa bbbbCC## SS''11SS11DD22aaDD11aa bbbbCC##
SS''11SS11DD22aaDD22aa DDbbbbCC## SS''11SS11DD22aaDD22aaDD11 aabbbbbbCC##
SS''11SS22AA22aaAA11aa BB## SS''11SS11DD22aaDD22aaDD22 aaDDbbbbbbCC##
SS''11SS22AA22aaAA22aa AABB## SS''11SS22AA22aaAA11aaBB11 bbcc##

SS''11SS22AA22aaAA11aaBB22 bbBBcc##
SS''11SS22AA22aaAA22aaAA11 aaBB##
SS''11SS22AA22aaAA22aaAA22 aaAABB##\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 
\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 
\Lambda \Lambda 
\Lambda 

\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
(i) aaaabb cc## (j) aaaabb cc##\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS''11SS11DD22aaDD11aabb bbCC## SS''11SS11DD22aaDD11aabb bbCC##
SS''11SS22AA22aaAA11aaBB11bb cc## SS''11SS22AA22aaAA11aaBB11bb cc##
SS''11SS22AA22aaAA11aaBB22bb BBcc## SS''11SS22AA22aaAA11aaBB22bbBB11 bbcccc##

SS''11SS22AA22aaAA11aaBB22bbBB22 bbBBcccc##\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
(k) aaaabbcc ## (l) aaaabbcc##\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS''11SS22AA22aaAA11aaBB11bbcc ## SS''11SS22AA22aaAA11aaBB11bbcc##\Gamma \Delta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma  \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

Figure 6.8 The breadth-first parsing of the sentence aaaabbcc##
all happen to match, resulting in (e). Next, we again have some predictions with anon-terminal in front, so another predict step leads us to (f). The next step is a match
step, and fortunately, some matches fail; these are just dropped as they can never leadto a successful parse. From (g) to (h) is again a predict step. Another match where,
again, some matches fail, leads us to (i). A further prediction results in (j) and then twomatches result in (k) and (l), leading to a successful parse (the predict stack is empty).
The analysis is

Sec. 6.3] Breadth-first top-down parsing 127

SS''11SS22AA22aaAA11aaBB11bbcc##.
For now, we do not need the terminals in the analysis; discarding them gives

SS''11SS22AA22AA11BB11.
This means that we get a left-most derivation by first applying rule SS''11, then rule SS22,then rule

AA22, etc., all the time replacing the left-most non-terminal. Check:

SS'' -->> SS## -->> AABB## -->> aaAABB## -->> aaaaBB## -->> aaaabbcc##.
The breadth-first method described here was first presented by Greibach [CF1964]. However, in that presentation, grammars are first transformed into Greibach
Normal Form, and the steps taken are like the ones our initial pushdown automatonmakes. The predict and match steps are combined.

6.3.2 A counterexample: left-recursionThe method discussed above clearly works for this grammar, and the question arises
whether it works for all context-free grammars. One would think it does, because allpossibilities are systematically tried, for all non-terminals, in any occurring prediction.
Unfortunately, this reasoning has a serious flaw that is demonstrated by the followingexample: let us see if the sentence

aabb belongs to the language defined by the simplegrammar

SS -->> SSbb || aa
Our automaton starts off in the following state:

\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

aabb##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS''\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda \Lambda 
\Lambda \Lambda 

As we have a non-terminal at the beginning of the prediction, we use a predict step,resulting in:

\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

aabb##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS''11 SS##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda \Lambda 

\Lambda \Lambda 

Now, another predict step results in:

\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

aabb##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS''11SS11 SSbb##
SS''11SS22 aa##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda \Lambda 

\Lambda \Lambda 

\Lambda 

As one prediction again starts with a non-terminal, we predict again:

128 General directional top-down methods [Ch. 6

\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

aabb##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS''11SS11SS11 SSbbbb##
SS''11SS11SS22 aabb##

SS''11SS22 aa##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

By now, it is clear what is happening: we seem to have ended up in an infiniteprocess leading us nowhere. The reason for this is that we keep trying the

SS-->>SSbb rulewithout ever coming to a state where a match can be attempted. This problem can

occur whenever there is a non-terminal that derives an infinite sequence of sententialforms, all starting with a non-terminal, so no matches can take place. As all these sentential forms in this infinite sequence start with a non-terminal, and the number ofnon-terminals is finite, there is at least one non-terminal A occurring more than once at
the start of those sentential forms. So, we have: A o"" . . . o"" A a. A non-terminal thatderives a sentential form starting with itself is called left-recursive. Left recursion
comes in two kinds: we speak of immediate left-recursion when there is a grammar ruleA o""A a, like in the rule

SS-->>SSbb; we speak of indirect left-recursion when the recursiongoes through other rules, for instance A o""B a, B o""A b. Both forms of left-recursion

can be concealed by e-producing non-terminals. For instance in the grammar

SS -->> AABBcc
BB -->> CCdd
BB -->> AABBff
CC -->> SSee
AA -->> ee

the non-terminals SS, BB, and CC are all left-recursive. Grammars with left-recursive non-terminals are called left-recursive as well.

If a grammar has no e-rules and no loops, we could still use our parsing scheme ifwe use one extra step: if a prediction stack has more symbols than the unmatched part
of the input sentence, it can never derive the sentence (no e-rules), so it can be dropped.However, this little trick has one big disadvantage: it requires us to know the length of
the input sentence in advance, so the method no longer is suitable for on-line parsing.Fortunately, left-recursion can be eliminated: given a left-recursive grammar, we can
transform it into a grammar without left-recursive non-terminals that defines the samelanguage. As left-recursion poses a major problem for any top-down parsing method,
we will now discuss this grammar transformation.

6.4 ELIMINATING LEFT-RECURSION
We will first discuss the elimination of immediate left-recursion. We will assume thate-rules and unit rules already have been eliminated (see Section 4.2.3.1 and 4.2.3.2).

Now, let A be a left-recursive rule, and

A o"" A a1 | . . . | A an | b1 | . . . | bm
be all the rules for A. None of the ai are equal to e, or we would have a rule A o""A, aunit rule. None of the b

j are equal to e either, or we would have an e-rule. The senten-tial forms generated by A using only the A o""A a

k rules all have the form

Sec. 6.4] Eliminating left-recursion 129

A ak 1 ak 2 . . . akj
and as soon as one of the A o""bi rules is used, the sentential form has no longer an A infront; it has the following form:

biak 1 ak 2 . . . akj
for some i, and some k 1, . . . , kj, where j could be 0. These same sentential forms aregenerated by the following set of rules:

A_head o"" b1 | . . . | bmA_tail o"" a

1 | . . . | anA_tails o"" A_tail A_tails | e

A o"" A_head A_tails
or, without re-introducing e-rules,

A_head o"" b1 | . . . | bmA_tail o"" a

1 | . . . | anA_tails o"" A_tail A_tails | A_tail

A o"" A_head A_tails | A_head
where A_head, A_tail, and A_tails are newly introduced non-terminals. None of the aiis e, so A_tail does not derive e, so A_tails is not left-recursive. A could still be leftrecursive, but it is not immediately left-recursive, because none of the bj start with anA. They could, however, derive a sentential form starting with an A.

In general, eliminating the indirect left-recursion is more complicated. The idea isthat first the non-terminals are numbered, say A

1, A 2, . . . , An. Now, for a left-recursive non-terminal A there is a derivation

A o"" B a o"" . . . o"" Cg o"" A d
with all the time a non-terminal at the left of the sentential form, and repeatedly replac-ing this non-terminal using one of its right-hand sides. All these non-terminals have a
number associated with them, say i 1, i 2, . . . , im , and in the derivation we get the fol-lowing sequence of numbers: i

1, i 2, . . . , im , i 1. Now, if we did not have any rulesA
io""Aja with j <=i, this would be impossible, because i 1 < i 2 < . . . < im < i 1 isimpossible.

The idea now is to eliminate all rules of this form. We start with A 1. For A 1, theonly rules to eliminate are the immediately left-recursive ones, and we already have
seen how to do just that. Next, it is A 2's turn. Each production rule of the formA

2o""A 1a is replaced by the production rules

A 2 o"" a1a | . . . | am a
where

A 1 o"" a1 | . . . | am

130 General directional top-down methods [Ch. 6
are the A 1-rules. This cannot introduce new rules of the form A 2o""A 1g because wehave just eliminated A

1's left-recursive rules, and the ai's are not equal to e. Next, weeliminate the immediate left-recursive rules of A

2. This finishes the work we have todo for A
2. Likewise, we deal with A 3 through An, in this order, always first replacingrules A
io""A 1g, then rules Aio""A 2d, etc. We have to obey this ordering, however,because for instance replacing a A

io""A 2d rule could introduce a Aio""A 3g rule, but not aA
io""A 1a rule.

6.5 DEPTH-FIRST (BACKTRACKING) PARSERS
The breadth-first method presented in the previous section has the disadvantage that ituses a lot of memory. The depth-first method also has a disadvantage: in its general

form it is not suitable for on-line parsing. However, there are many applications whereparsing does not have to be done on-line, and then the depth-first method is advantageous since it does not need much memory.In the depth-first method, when we are faced with a number of possibilities, we
choose one and leave the other possibilities for later. First, we fully examine the conse-quences of the choice we just made. If this choice turns out to be a failure (or even a
success, but we want all solutions), we roll back our actions until the present point andcontinue with the other possibilities.

Let us see how this search technique applies to top-down parsing. Our depth-firstparser follows the same steps as our breadth-first parser, until it encounters a choice: a
non-terminal that has more than one right-hand side lies on top of the prediction stack.Now, instead of creating a new analysis stack/prediction stack pair, it chooses the first
right-hand side. This is reflected on the analysis stack by the appearance of the non-terminal involved, with suffix 1, exactly as it was in our breadth-first parser. This time
however, the analysis stack is not only used for remembering the parse, but also forbacktracking.

The parser continues in this way, until a match fails, or the prediction stack isempty. If the prediction stack is empty, we have found a parse, which is represented by
the analysis stack (we know that the input is also exhausted, because of the end-marker
##). If a match fails, the parser will backtrack. This backtracking consists of the fol-lowing steps: first, any terminal symbols at the end of the analysis stack are popped

from this stack, and pushed back on top of the prediction stack. Also, these symbolsare removed from the matched input and added to the beginning of the rest of the input
(this is the reversal of the "match" steps), that is, backtracking over a terminal is doneby moving the vertical line backwards, as is demonstrated in Figure 6.9.

a 1a 2 . . . ai ai +1 . . . an##

aai b

a 1a 2 . . . ai -1 aiai +1 . . . an##

a aib

Figure 6.9 Backtracking over a terminal
Then, there are two possibilities: if the analysis stack is empty, there are no other possi-bilities to try, and the parsing stops; otherwise, there is a non-terminal on top of the
analysis stack, and the top of the prediction stack corresponds to a right-hand side of

Sec. 6.5] Depth-first (backtracking) parsers 131
this non-terminal. The choice of this right-hand side just resulted in a failed match. Inthe latter case, we pop the non-terminal from the analysis stack and replace the righthand side part in the prediction stack with this non-terminal (this is the reversal of aprediction step). This is demonstrated in Figure 6.10.

a 1a 2 . . . ai ai +1 . . . an##

aA gb

a 1a 2 . . . ai ai +1 . . . an##

a Ab

Figure 6.10 Backtracking over a A o""g choice
Next, there are again two possibilities: if this was the last right-hand side of this non-terminal, we have already tried its right-hand sides and have to backtrack further; if
not, we start parsing again, first using a predict step that replaces the non-terminal withits next right-hand side.

Now, let us try to parse the sentence aaaabbcc, this time using the backtrackingparser. Figure 6.11 presents the parsing process step by step; the backtracking steps are
marked with a B. The example demonstrates another disadvantage of the backtrackingmethod: it can make wrong choices and find out about this only much later. Of course,
it could also start with the right choices and be finished rapidly.As presented here, the parsing stops when a parsing is found. If we want to find
all parsings, we should not stop when the prediction stack is empty. We can continueby backtracking just as if we had not found a successful parse, and write down the
analysis stack (that represents the parse) every time that the prediction stack is empty.Ultimately, we will end with an empty analysis part, indicating that we have exhausted
all analysis possibilities, and the parsing stops.

6.6 RECURSIVE DESCENT
In the previous sections, we have seen several automata at work, using a grammar todecide the parsing steps while processing the input sentence. Now this is just another

way of stating that these automata use a grammar as a program. Looking at a grammaras a program for a parsing machine is not as far-fetched as it may seem at first. After
all, a grammar is a prescription for deriving sentences of the language that the grammardescribes, and what we are doing in top-down parsing is rederiving a sentence from the
grammar. This only differs from the classic view of a grammar as a generating devicein that we are now trying to rederive a particular sentence, not just any sentence. Seen
in this way, grammars are programs, written in a programming language with adeclarative style (that is, it specifies what to do, but not the steps that need to be done
to achieve the result).If we want to write a top-down parser for a certain context-free grammar in one of
the more common programming languages, like Pascal, C, or Modula-2, there areseveral options. The first option is to write a program that emulates one of the automata described in the previous sections. This program can then be fed a grammar andan input sentence. This is a perfectly sound approach and is easy to program. The difficulty comes when the parser must perform some other actions as parts of the input arerecognized. For instance, a compiler must build a symbol table when it processes a

132 General directional top-down methods [Ch. 6

aaaabbcc##
SS''

aaaabbcc##
SS''11 SS##

aaaabbcc##
SS''11SS11 DDCC##

aaaabbcc##
SS''11SS11DD11 aabbCC##

aa aabbcc##
SS''11SS11DD11aa bbCC##

B aaaabbcc##

SS''11SS11DD11 aabbCC##

B

aaaabbcc##
SS''11SS11 DD11CC##

aaaabbcc##
SS''11SS11DD22 aaDDbbCC##

aa aabbcc##
SS''11SS11DD22aa DDbbCC##

aa aabbcc##
SS''11SS11DD22aaDD11 aabbbbCC##

aaaa bbcc##
SS''11SS11DD22aaDD11aa bbbbCC##

aaaabb cc##
SS''11SS11DD22aaDD11aabb bbCC##

B

aaaa bbcc##
SS''11SS11DD22aaDD11aa bbbbCC##

B aa aabbcc##

SS''11SS11DD22aaDD11 aabbbbCC##

B aa aabbcc##

SS''11SS11DD22aa DD11bbCC##

aa aabbcc##
SS''11SS11DD22aaDD22 aaDDbbbbCC##

aaaa bbcc##
SS''11SS11DD22aaDD22aa DDbbbbCC##

aaaa bbcc##
SS''11SS11DD22aaDD22aaDD11 aabbbbbbCC##

B

aaaa bbcc##
SS''11SS11DD22aaDD22aa DD11bbbbCC##

aaaa bbcc##
SS''11SS11DD22aaDD22aaDD22 aaDDbbbbbbCC##

B aaaa bbcc##

SS''11SS11DD22aaDD22aa DD22bbbbCC##

B

aa aabbcc##
SS''11SS11DD22aaDD22 aaDD22bbbbCC##

B aa aabbcc##

SS''11SS11DD22aa DD22bbCC##

B aaaabbcc##

SS''11SS11DD22 aaDD22bbCC##

B

aaaabbcc##
SS''11SS11 DD22CC##

B aaaabbcc##

SS''11 SS11##

aaaabbcc##
SS''11SS22 AABB##

aaaabbcc##
SS''11SS22AA11 aaBB##

aa aabbcc##
SS''11SS22AA11aa BB##

aa aabbcc##
SS''11SS22AA11aaBB11 bbcc##

B

aa aabbcc##
SS''11SS22AA11aa BB11##

aa aabbcc##
SS''11SS22AA11aaBB22 bbBBcc##

B aa aabbcc##

SS''11SS22AA11aa BB22##

B

aaaabbcc##
SS''11SS22AA11 aaBB22##

B aaaabbcc##

SS''11SS22 AA11BB22##

aaaabbcc##
SS''11SS22AA22 aaAABB##

aa aabbcc##
SS''11SS22AA22aa AABB##

aa aabbcc##
SS''11SS22AA22aaAA11 aaBB##

aaaa bbcc##
SS''11SS22AA22aaAA11aa BB##

aaaa bbcc##
SS''11SS22AA22aaAA11aaBB11 bbcc##

aaaabb cc##
SS''11SS22AA22aaAA11aaBB11bb cc##

aaaabbcc ##
SS''11SS22AA22aaAA11aaBB11bbcc ##

aaaabbcc##
SS''11SS22AA22aaAA11aaBB11bbcc##

Figure 6.11 Parsing the sentence aaaabbcc

Sec. 6.6] Recursive descent 133
declaration sequence. This, and efficiency considerations lead to a second option: towrite a special purpose parser for the grammar at hand. Many of these special purpose
parsers have been written, and most of them use an implementation technique calledrecursive descent. We will assume that the reader has some programming experience,
and knows about procedures and recursion. If not, this section can be skipped. It doesnot describe a different parsing method, but merely an implementation technique that is
often used in hand-written parsers and also in some machine-generated parsers.
6.6.1 A naive approachAs a first approach, we regard a grammar rule as a procedure for recognizing its lefthand side. The rule
SS -->> aaBB || bbAA

is regarded as a procedure to recognize an SS. This procedure then states something likethe following:

SS succeeds if

aa succeeds and then BB succeedsor else

bb succeeds and then AA succeeds
This does not differ much from the grammar rule, but it does not look like a piece ofPascal or C either. Like a cookbook recipe that usually does not tell us that we must
peel the potatoes, let alone how to do that, the procedure is incomplete.There are several bits of information that we must maintain when carrying out
such a procedure. First, there is the notion of a "current position" in the rule. Thiscurrent position indicates what must be tried next. When we implement rules as procedures, this current position is maintained automatically, by the program counter,which tells us where we are within a procedure. Next, there is the input sentence itself.
When implementing a backtracking parser, we usually keep the input sentence in a glo-bal array, with one element for each symbol in the sentence. The array must be global,
because it contains information that must be accessible equally easily from all pro-cedures. Then, there is the notion of a current position in the input sentence. When the
current position in the rule indicates a terminal symbol, and this symbol corresponds tothe symbol at the current position in the input sentence, both current positions will be
advanced one position. The current position in the input sentence is also global infor-mation. We will therefore maintain this position in a global variable, of a type that is
suitable for indexing the array containing the input sentence. Also, when starting a rulewe must remember the current position in the input sentence, because we need it for the
"or else" clauses. These must all be started at the same position in the input sentence.For instance, starting with the rule for

SS of grammar 6.1, suppose that the aa matches thesymbol at the current position of the input sentence. The current position is advanced

and then BB is tried. For BB, we have a rule similar to that of SS. Now suppose that BB fails.We then have to try the next choice for

SS, and backup the position in the input sentenceto what it was when we started the rule for

SS. This is backtracking, just as we haveseen it earlier.

All this tells us how to deal with one rule. However, usually we are dealing with

134 General directional top-down methods [Ch. 6
a grammar that has more than one non-terminal, so there will be more than one rule.When we arrive at a non-terminal in a rule, we have to execute the rule for that nonterminal, and, if it succeeds, return to the current invocation and continue there. Weachieve this automatically by using the procedure-call mechanism of the implementation language.Another detail that we have not covered yet is that we have to remember the
grammar rules that we use. If we do not remember them, we will not know afterwardshow the sentence was derived. Therefore we note them in a separate list, striking them
out when they fail. Each procedure must keep its own copy of the index in this list,again because we need it for the "or else " clauses: if a choice fails, all choices that
have been made after the choice now failing must be discarded. In the end, when therule for

SS'' succeeds, the grammar rules left in this list represent a left-most derivationof the sentence.

Now, let us see how a parser, as described above, works for an example. Let usconsider again grammar of Figure 6.6, and input sentence

aabbbbcccc. As before, we add arule
SS''-->>SS## to the grammar and a ## to the end of the sentence, so our parser starts inthe following state:

\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma Active rules Sentence Parse
\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

1: SS'' -->>

\Gamma \Gamma  SS## \Gamma \Gamma  aabbcc## 1: SS'' -->> SS##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda 

\Lambda \Lambda  \Lambda \Lambda \Lambda  \Lambda \Lambda \Lambda  \Lambda \Lambda \Lambda  \Lambda \Lambda \Lambda 

Our administration is divided into three parts; the "Active rules" part indicates theactive rules, with a dot (

\Delta \Delta  ) indicating the current position within that rule. The bottom

rule in this part is the rule that we are currently working on. The "Sentence" part indi-cates the sentence, including a position marker indicating the current position in the

sentence. The "Parse" part will be used to remember the rules that we use (not only thecurrently active ones). The entries in this part are numbered, and each entry in the
"Active rules" part also contains its index in the "Parse" part. As we shall see later, thisis needed to backup after having taken a wrong choice.

There is only one possibility here: the current position in the procedure indicatesthat we must invoke the procedure for

SS, so let us do so:

\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma Active rules Sentence Parse
\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

1: SS'' -->> SS

\Gamma \Gamma  ## \Gamma \Gamma  aabbcc## 1: SS'' -->> SS##

2: SS -->>

\Gamma \Gamma  DDCC || AABB \Gamma \Gamma  aabbcc## 2: SS -->> DDCC\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda 

\Lambda \Lambda 

\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

Notice that we have advanced the position in the SS'' rule. It now indicates where wehave to continue when we are finished with

SS (the return address). Now we try the firstalternative for
SS. There is a choice here, so the current position in the input sentence issaved. We have not made this explicit in the pictures, because this position is already

present in the "Sentence"-part of the entry that invoked SS.

\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma Active rules Sentence Parse
\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

1: SS'' -->> SS

\Gamma \Gamma  ## \Gamma \Gamma  aabbcc## 1: SS'' -->> SS##

2: SS -->> DD

\Gamma \Gamma  CC || AABB \Gamma \Gamma  aabbcc## 2: SS -->> DDCC

3: DD -->>

\Gamma \Gamma  aabb || aaDDbb \Gamma \Gamma  aabbcc## 3: DD -->> aabb\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 

Now, the first choice for DD is tried. The aa succeeds, and next the bb also succeeds, so

Sec. 6.6] Recursive descent 135
we get:

\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma Active rules Sentence Parse
\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

1: SS'' -->> SS

\Gamma \Gamma  ## \Gamma \Gamma  aabbcc## 1: SS'' -->> SS##

2: SS -->> DD

\Gamma \Gamma  CC || AABB \Gamma \Gamma  aabbcc## 2: SS -->> DDCC

3: DD -->> aabb

\Gamma \Gamma  || aaDDbb aabb\Gamma \Gamma  cc## 3: DD -->> aabb\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 

Now, we are at the end of a choice for DD. This means that it succeeds, and we removethis entry from the list of active rules, after updating the current positions in the entry
above. Next, it is CC's turn:

\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma Active rules Sentence Parse
\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

1: SS'' -->> SS

\Gamma \Gamma  ## \Gamma \Gamma  aabbcc## 1: SS'' -->> SS##

2: SS -->> DDCC

\Gamma \Gamma  || AABB aabb\Gamma \Gamma  cc## 2: SS -->> DDCC

4: CC -->>

\Gamma \Gamma  cc || ccCC aabb\Gamma \Gamma  cc## 3: DD -->> aabb

4: CC -->> cc\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

Now, the cc succeeds, so the CC succeeds, and then the SS also succeeds.

\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma Active rules Sentence Parse
\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

1: SS'' -->> SS

\Gamma \Gamma  ## aabbcc\Gamma \Gamma  ## 1: SS'' -->> SS##

2: SS -->> DDCC3:

DD -->> aabb4:
CC -->> cc\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

Now, the ## also succeeds, and thus SS'' succeeds, resulting in:

\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma Active rules Sentence Parse
\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

1: SS'' -->> SS##

\Gamma \Gamma  aabbcc##\Gamma \Gamma  1: SS'' -->> SS##

2: SS -->> DDCC3:

DD -->> aabb4:
CC -->> cc\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

The "Parse" part now represents a left-most derivation of the sentence:

SS'' -->> SS## -->> DDCC## -->> aabbCC## -->> aabbcc##.
This method is called recursive descent. Descent, because it operates top-down,and recursive, because each non-terminal is implemented as a procedure that can
directly or indirectly (through other procedures) invoke itself. It should be stressed that"recursive descent" is merely an implementation issue, albeit an important one. It
should also be stressed that the parser described above is a backtracking parser,independent of the implementation method used. Backtracking is a property of the
parser, not of the implementation.The backtracking method developed above is aesthetically pleasing, because we in
fact use the grammar itself as a program (or we transform the grammar rules into pro-cedures, which can be done mechanically). There is only one problem: the recursive
descent method, as described above, does not always work! We already know that itdoes not work for left-recursive grammars, but the problem is worse than that. For

136 General directional top-down methods [Ch. 6
instance, aaaabbcc and aabbcccc are sentences that are not recognized, but should be. Parsingof the

aaaabbcc sentence gets stuck after the first aa, and parsing of the aabbcccc sentence getsstuck after the first

cc. Yet, aaaabbcc can be derived as follows:

SS -->> AABB -->> aaAABB -->> aaaaBB -->> aaaabbcc,
and aabbcccc can be derived with

SS -->> DDCC -->> aabbCC -->> aabbccCC -->> aabbcccc.
So, let us examine why our method fails. A little investigation shows that wenever try the

AA-->>aaAA choice when parsing aaaabbcc, because the AA-->>aa choice succeeds.Such a problem arises whenever more than one right-hand side can succeed, and this is

the case whenever a right-hand side can derive a prefix of a string derivable fromanother right-hand side of the same non-terminal. The method developed so far is too
optimistic, in that it assumes that if a choice succeeds, it must be the right choice. Itdoes not allow us to backtrack over such a choice, when it was the wrong one. This is a
particularly serious problem if the grammar has e-rules, because e-rules alwayssucceed. Another consequence of being unable to backup over a succeeding choice is
that it does not allow us to get all parses when there is more than one (this is possiblefor ambiguous grammars). Improvement is certainly needed here. Our criterion for
determining whether a choice is the right one clearly is wrong. Looking back at thebacktracking parser of the beginning of this section, we see that that parser does not
have this problem, because it does not consider choices independently of their context.One can only decide that a choice is the right one if taking it results in a successful
parse; even if the choice ultimately succeeds, we have to try the other choices as well ifwe want all parses. In the next section, we will develop a recursive-descent parser that
solves all the problems mentioned above. Meanwhile, the method above only worksfor grammars that are prefix-free. A non-terminal A is prefix-free if A o""* x and A o""* xy,
where x and y are strings of terminal symbols, implies that y = e. A grammar is calledprefix-free if all its non-terminals are prefix-free.

6.6.2 Exhaustive backtracking recursive descentIn the previous section we saw that we have to be careful not to accept a choice too
early; it can only be accepted when it leads to a successful parse. Now this demand isdifficult to express in a recursive-descent parser; how do we obtain a procedure that
tells us whether a choice leads to a successful parse? In principle, there are infinitelymany of these procedures, depending on the sentential form (the prediction) that must
derive the rest of the input. We cannot just write them all. However, at any point dur-ing the parsing process we are dealing with only one such sentential form: the current
prediction, so we could try to build a parsing procedure for this sentential form dynami-cally, during parsing. Many programming languages offer a useful facility for this purpose: procedure parameters. One procedure can accept a procedure as parameter, andcall it, or pass it on to another procedure, or whatever other things one does with procedures. Some languages (for instance Pascal) require these procedures to be named,that is, the actual parameter must be declared as a procedure; other languages, like
Algol 68, allow a procedure body for an actual parameter.Let us see how we can write a parsing procedure for a symbol X, given that it is

Sec. 6.6] Recursive descent 137
passed a procedure, which we will call tail, that parses the rest of the sentence (the partthat follows the X). This is the approach taken for all non-terminals, and, for the time
being, for terminals as well.The parsing procedure for a terminal symbol a is easy: it matches the current input
symbol with a; if it succeeds, it advances the input position, and calls the tail parame-ter; then, when tail returns, it restores the input position and returns.

Obviously, the parsing procedure for a non-terminal A is more complicated. Itdepends on the type of grammar rule we have for A. The simplest case is A o""e. This is
implemented as a call to tail. The next simple case is A o""X, where X is either a termi-nal or a non-terminal symbol. To deal with this case, we must remember that we
assume that we have a parsing procedure for X, so the implementation of this case con-sists of a call to X, with the tail parameter. The next case is A o""XY, with X and Y symbols. The procedure for X expects a procedure for "what comes after the X" as parame-ter. Here, this parameter procedure is built using the Y and the tail procedures: we
create a new procedure out of these two. This, by itself, is a simple procedure: it callsY, with tail as parameter. If we call this procedure Y_tail, we can implement A by calling X with Y_tail as parameter.# And finally, if the right-hand side contains more thantwo symbols, this technique has to be repeated: for a rule A o""X

1X 2 . . . Xn we create aprocedure for X
2 . . . Xn and tail using a procedure for X 3 . . . Xn and tail, and so on.Finally, if we have a choice, that is, we have A o""a | b, the parsing procedure for A has

two parts: one part for a, followed by a call to tail, and another part for b, followed bya call to tail. We have already seen how to implement these parts. If we only want one
parsing, all parsing procedures may be implemented as functions that return either falseor true, reflecting whether they result in a successful parse; the part for b is then only
started if the part for a, followed by tail, fails. If we want all parses, we have to tryboth choices.

Applying this technique to all grammar rules almost results in a parser. Only, wedon't have a starting point yet; this is easily obtained: we just call the procedure for the
start-symbol, with the procedure for recognizing the end-marker as parameter. Thisend-marker procedure is probably a bit different from the others, because this is the
procedure where we finally find out whether a parsing attempt succeeds.Figure 6.12 presents a fully backtracking recursive-descent parser for the grammar of Figure 6.6, written in Pascal. The program has a mechanism to remember therules used, so these can be printed for each successful parse. Figure 6.13 presents a
sample session with this program.
{{$$CC++:: ddiissttiinngguuiisshh bbeettwweeeenn uuppppeerr aanndd lloowweerr ccaassee }}
pprrooggrraamm ppaarrssee((iinnppuutt,, oouuttppuutt));;
{{ TThhiiss iiss aann eexxhhaauussttiivvee bbaacckkttrraacckkiinngg rreeccuurrssiivvee--ddeesscceenntt ppaarrsseerr tthhaatt wwiillll

ccoorrrreeccttllyy ppaarrssee aaccccoorrddiinngg ttoo tthhee ggrraammmmaarr

SS -->> DD CC || AA BB
AA -->> aa || aa AA
BB -->> bb cc || bb BB cc
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

For some programming languages this is difficult. The problem is that tail must be accessiblefrom

Y_tail. Therefore, Y_tail should be a local procedure within the procedure for A. But,some languages do not allow for local procedures (for instance C), and others do not allow local

procedures to be passed as parameters (like Modula-2). Some extensive trickery is required forthese languages, but this is beyond the scope of this book.

138 General directional top-down methods [Ch. 6

DD -->> aa bb || aa DD bb
CC -->> cc || cc CC
IItt iimmpplleemmeennttss pprrooppeerr bbaacckkttrraacckkiinngg bbyy oonnllyy cchheecckkiinngg oonnee ssyymmbbooll aatt aa
ttiimmee aanndd ppaassssiinngg tthhee rreesstt ooff tthhee aalltteerrnnaattiivvee aass aa ppaarraammeetteerr ffoorr
eevvaalluuaattiioonn oonn aa lloowweerr lleevveell.. AA mmoorree nnaaiivvee bbaacckkttrraacckkiinngg ppaarrsseerr wwiillll nnoott
aacccceepptt ee..gg.. aaaabbcc..
}}

ccoonnsstt iinnffiinniittyy == 110000;; {{ llaarrggee eennoouugghh }}
ttyyppee ssttrr == ppaacckkeedd aarrrraayy[[11....1100]] ooff cchhaarr;;
vvaarr ttpp:: iinntteeggeerr;; {{ iinnddeexx iinn tteexxtt }}

lleennggtthh:: iinntteeggeerr;; {{ nnuummbbeerr ooff ssyymmbboollss iinn tteexxtt }}
rrpp:: iinntteeggeerr;; {{ iinnddeexx iinn rruulleess }}
tteexxtt:: aarrrraayy [[11....iinnffiinniittyy]] ooff cchhaarr;; {{ iinnppuutt tteexxtt }}
rruulleess:: aarrrraayy [[11....iinnffiinniittyy]] ooff ssttrr;; {{ ssttoorree rruulleess uusseedd }}

{{ aaddmmiinniissttrraattiioonn ooff rruulleess uusseedd }}
pprroocceedduurree ppuusshhrruullee ((ss:: ssttrr));; bbeeggiinn rrpp ::== rrpp ++ 11;; rruulleess[[rrpp]] ::== ss eenndd;;
pprroocceedduurree ppoopprruullee;; bbeeggiinn rrpp ::== rrpp -- 11 eenndd;;

pprroocceedduurree eennddmmaarrkk;; {{ rreeccooggnniizzee eenndd aanndd rreeppoorrtt ssuucccceessss }}

vvaarr ii:: iinntteeggeerr;;
bbeeggiinn iiff tteexxtt[[ttpp]] == ''##'' tthheenn bbeeggiinn

wwrriitteellnn((''DDeerriivvaattiioonn::''));;
ffoorr ii ::== 11 ttoo rrpp ddoo wwrriitteellnn(('' '',, rruulleess[[ii]]));;
eenndd
eenndd;;

pprroocceedduurree aa((pprroocceedduurree ttaaiill));; {{ rreeccooggnniizzee aann ''aa'' aanndd ccaallll ttaaiill }}
bbeeggiinn iiff tteexxtt[[ttpp]] == ''aa'' tthheenn bbeeggiinn ttpp ::== ttpp ++ 11;; ttaaiill;; ttpp ::== ttpp -- 11 eenndd eenndd;;

pprroocceedduurree bb((pprroocceedduurree ttaaiill));; {{ rreeccooggnniizzee aa ''bb'' aanndd ccaallll ttaaiill }}
bbeeggiinn iiff tteexxtt[[ttpp]] == ''bb'' tthheenn bbeeggiinn ttpp ::== ttpp ++ 11;; ttaaiill;; ttpp ::== ttpp -- 11 eenndd eenndd;;

pprroocceedduurree cc((pprroocceedduurree ttaaiill));; {{ rreeccooggnniizzee aa ''cc'' aanndd ccaallll ttaaiill }}
bbeeggiinn iiff tteexxtt[[ttpp]] == ''cc'' tthheenn bbeeggiinn ttpp ::== ttpp ++ 11;; ttaaiill;; ttpp ::== ttpp -- 11 eenndd eenndd;;

pprroocceedduurree AA((pprroocceedduurree ttaaiill));; {{ rreeccooggnniizzee aann ''AA'' aanndd ccaallll ttaaiill }}

{{ pprroocceedduurreess ffoorr tthhee aalltteerrnnaattiivvee ttaaiillss }}
pprroocceedduurree tt;; bbeeggiinn ttaaiill eenndd;;
pprroocceedduurree AAtt;; bbeeggiinn AA((ttaaiill)) eenndd;;
bbeeggiinn

ppuusshhrruullee((''AA -->> aa ''));; aa((tt));; ppoopprruullee;;
ppuusshhrruullee((''AA -->> aaAA ''));; aa((AAtt));; ppoopprruullee
eenndd;;

pprroocceedduurree BB((pprroocceedduurree ttaaiill));; {{ rreeccooggnniizzee aa ''BB'' aanndd ccaallll ttaaiill }}

pprroocceedduurree cctt;; bbeeggiinn cc((ttaaiill)) eenndd;;
pprroocceedduurree BBcctt;;

pprroocceedduurree cctt;; bbeeggiinn cc((ttaaiill)) eenndd;;
bbeeggiinn BB((cctt)) eenndd;;
bbeeggiinn

ppuusshhrruullee((''BB -->> bbcc ''));; bb((cctt));; ppoopprruullee;;
ppuusshhrruullee((''BB -->> bbBBcc ''));; bb((BBcctt));; ppoopprruullee

Sec. 6.6] Recursive descent 139
eenndd;;
pprroocceedduurree DD((pprroocceedduurree ttaaiill));; {{ rreeccooggnniizzee aa ''DD'' aanndd ccaallll ttaaiill }}

pprroocceedduurree bbtt;; bbeeggiinn bb((ttaaiill)) eenndd;;
pprroocceedduurree DDbbtt;;

pprroocceedduurree bbtt;; bbeeggiinn bb((ttaaiill)) eenndd;;
bbeeggiinn DD((bbtt)) eenndd;;
bbeeggiinn

ppuusshhrruullee((''DD -->> aabb ''));; aa((bbtt));; ppoopprruullee;;
ppuusshhrruullee((''DD -->> aaDDbb ''));; aa((DDbbtt));; ppoopprruullee
eenndd;;

pprroocceedduurree CC((pprroocceedduurree ttaaiill));; {{ rreeccooggnniizzee aa ''CC'' aanndd ccaallll ttaaiill }}

pprroocceedduurree tt;; bbeeggiinn ttaaiill eenndd;;
pprroocceedduurree CCtt;; bbeeggiinn CC((ttaaiill)) eenndd;;
bbeeggiinn

ppuusshhrruullee((''CC -->> cc ''));; cc((tt));; ppoopprruullee;;
ppuusshhrruullee((''CC -->> ccCC ''));; cc((CCtt));; ppoopprruullee
eenndd;;

pprroocceedduurree SS((pprroocceedduurree ttaaiill));; {{ rreeccooggnniizzee aa ''SS'' aanndd ccaallll ttaaiill }}

pprroocceedduurree CCtt;; bbeeggiinn CC((ttaaiill)) eenndd;;
pprroocceedduurree BBtt;; bbeeggiinn BB((ttaaiill)) eenndd;;
bbeeggiinn

ppuusshhrruullee((''SS -->> DDCC ''));; DD((CCtt));; ppoopprruullee;;
ppuusshhrruullee((''SS -->> AABB ''));; AA((BBtt));; ppoopprruullee
eenndd;;

ffuunnccttiioonn rreeaaddlliinnee:: bboooolleeaann;;
bbeeggiinn

wwrriittee((''>> ''));; lleennggtthh ::== 11;;
iiff nnoott eeooff tthheenn
bbeeggiinn wwhhiillee nnoott eeoollnn ddoo bbeeggiinn

rreeaadd((tteexxtt[[lleennggtthh]]));; lleennggtthh ::== lleennggtthh ++ 11;;
eenndd;;
rreeaaddllnn;; rreeaaddlliinnee ::== ttrruuee
eenndd
eellssee rreeaaddlliinnee ::== ffaallssee;;
eenndd;;

pprroocceedduurree ppaarrsseerr;;
bbeeggiinn tteexxtt[[lleennggtthh]] ::== ''##'';; ttpp ::== 11;; rrpp ::== 00;; SS((eennddmmaarrkk)) eenndd;;

bbeeggiinn wwhhiillee rreeaaddlliinnee ddoo ppaarrsseerr eenndd..

Figure 6.12 A parser for the grammar of Figure 6.6

6.7 DEFINITE CLAUSE GRAMMARS
In the previous sections, we have seen how to create parsers that retain much of the ori-ginal structure of the grammar. The programming language Prolog allows us to take

this even one step further. Prolog has its foundations in logic. The programmerdeclares some facts about objects and their relationships, and asks questions about
these. The Prolog system uses a built-in search and backtrack mechanism to answer

140 General directional top-down methods [Ch. 6

>> aaaabbcc
DDeerriivvaattiioonn::

SS -->> AABB
AA -->> aaAA
AA -->> aa
BB -->> bbcc
>> aabbcccc
DDeerriivvaattiioonn::

SS -->> DDCC
DD -->> aabb
CC -->> ccCC
CC -->> cc
>> aabbcc
DDeerriivvaattiioonn::

SS -->> DDCC
DD -->> aabb
CC -->> cc
DDeerriivvaattiioonn::

SS -->> AABB
AA -->> aa
BB -->> bbcc

Figure 6.13 A session with the program of Figure 6.12
the questions with "yes" or "no". For instance, if we have told the Prolog system aboutthe fact that a table and a chair are pieces of furniture, as follows:

ffuurrnniittuurree((ttaabbllee))..
ffuurrnniittuurree((cchhaaiirr))..

and we then ask if a bread is a piece of furniture:

|| ??-- ffuurrnniittuurree((bbrreeaadd))..
the answer will be "no", but the answer to the question

|| ??-- ffuurrnniittuurree((ttaabbllee))..
will, of course, be "yes". We can also use variables, which can be either instantiated(have a value), or not. Variables start with a capital letter or an underscore (

__). Wecan use them for instance as follows:

|| ??-- ffuurrnniittuurree((XX))..
This is asking for an instantiation of the variable XX. The Prolog system will search fora possible instantiation and respond:

XX == ttaabbllee
We can then either stop by typing a RETURN, or continue searching by typing a

Sec. 6.7] Definite Clause grammars 141
semicolon (and then a RETURN). In the last case, the Prolog system will search foranother instantiation of

XX.Not every fact is as simple as the one in the example above. For instance, a Prolog clause that could tell us something about antique furniture is the following:

aannttiiqquuee__ffuurrnniittuurree((OObbjj,, AAggee)) ::-- ffuurrnniittuurree((OObbjj)),, AAggee>>110000..
Here we see a conjunction of two goals: an object OObbjj with age AAggee is an antique pieceof furniture if it is a piece of furniture AND its age is more than a 100 years.

An important data structure in Prolog is the list. The empty list is denoted by [[]],
[[aa]] is a list with head aa and tail [[]], [[aa,,bb,,cc]] is a list with head aa and tail [[bb,,cc]].Many Prolog systems allow us to specify grammars. For instance, the grammar of

Figure 6.6, looks like the one in Figure 6.14, when written in Prolog. The terminalsymbols appear as lists of one element.

%% OOuurr eexxaammppllee ggrraammmmaarr iinn DDeeffiinniittee CCllaauussee GGrraammmmaarr ffoorrmmaatt..
ssnn ---->> ddnn,, ccnn..
ssnn ---->> aann,, bbnn..
aann ---->> [[aa]]..
aann ---->> [[aa]],, aann..
bbnn ---->> [[bb]],, [[cc]]..
bbnn ---->> [[bb]],, bbnn,, [[cc]]..
ccnn ---->> [[cc]]..
ccnn ---->> [[cc]],, ccnn..
ddnn ---->> [[aa]],, [[bb]]..
ddnn ---->> [[aa]],, ddnn,, [[bb]]..

Figure 6.14 An example grammar in Prolog
The Prolog system translates these rules into Prolog clauses, also sometimes calleddefinite clauses, which we can investigate with the

lliissttiinngg question:

|| ??-- lliissttiinngg((ddnn))..
ddnn((__33,,__44)) ::--

cc((__33,,aa,,__1133)),,
cc((__1133,,bb,,__44))..
ddnn((__33,,__44)) ::--

cc((__33,,aa,,__1133)),,
ddnn((__1133,,__1144)),,
cc((__1144,,bb,,__44))..

yyeess
|| ??-- lliissttiinngg((ssnn))..
ssnn((__33,,__44)) ::--

142 General directional top-down methods [Ch. 6

ddnn((__33,,__1133)),,
ccnn((__1133,,__44))..
ssnn((__33,,__44)) ::--

aann((__33,,__1133)),,
bbnn((__1133,,__44))..

yyeess
We see that the clauses for the non-terminals have two parameter variables. The firstone represents the part of the sentence that has yet to be parsed, and the second one
represents the tail end of the first one, being the part that is not covered by the currentinvocation of this non-terminal.

The built-in cc-clause matches the head of its first parameter with the secondparameter, and the tail of this parameter with the third parameter. A sample Prolog
session with this grammar is presented below:

&& pprroolloogg
CC--PPrroolloogg vveerrssiioonn 11..55
|| ??-- [[ggrraamm11]]..
ggrraamm11 ccoonnssuulltteedd 996688 bbyytteess ..113333333333 sseecc..

yyeess
We have now started the Prolog system, and requested it to consult the file containingthe grammar. Here, the grammar resides in a file called

ggrraamm11.

|| ??-- ssnn((AA,,[[]]))..
AA == [[aa,,bb,,cc]] ;;
AA == [[aa,,bb,,cc,,cc]] ;;
AA == [[aa,,bb,,cc,,cc,,cc]] ..
yyeess
We have now asked the system to generate some sentences, by passing an uninstan-tiated variable to

ssnn, and requesting the system to find other instantiations twice. TheProlog system uses a depth-first searching mechanism, which is not suitable for sentence generation. It will only generate sentences starting with an aa, followed by a bb,and then followed by an ever increasing number of

cc's.

|| ??-- ssnn(([[aa,,bb,,cc]],,[[]]))..
yyeess
|| ??-- ssnn(([[aa,,aa,,bb,,cc]],,[[]]))..

yyeess

Sec. 6.7] Definite Clause grammars 143

|| ??-- ssnn(([[aa,,bb,,cc,,cc]],,[[]]))..
yyeess
|| ??-- ssnn(([[aa,,aa,,aa,,bb,,bb,,cc,,cc,,cc]],,[[]]))..

nnoo
|| ??-- hhaalltt..

[[ PPrroolloogg eexxeeccuuttiioonn hhaalltteedd ]]
&&

Here we have asked the system to recognize some sentences, including two on whichthe naive backtracking parser of Section 6.6.1 failed:

aaaabbcc and aabbcccc. This sessiondemonstrates that we can use Definite Clause Grammars for recognizing sentences, and

to a lesser extent also for generating sentences.Cohen and Hickey [CF 1987] discuss this and other applications of Prolog in
parsers in more detail. For more information on Prolog, see Programming in Prolog byWilliam F. Clocksin and Christopher S. Mellish (Springer-Verlag, Berlin, 1981).

7
General bottom-up parsing

As explained in Section 3.3.2, bottom-up parsing is conceptually very simple. At alltimes we are in the possession of a sentential form that derives from the input text
through a series of left-most reductions (which mirrored right-most productions).There is a cut somewhere in this sentential form which separates the already reduced
part (on the left) from the yet unexamined part (on the right). See Figure 7.1. The parton the left is called the "stack" and the part on the right "rest of input". The latter contains terminal symbols only, since it is an unprocessed part of the original sentence,while the stack contains a mixture of terminals and non-terminals, resulting from
recognized right-hand sides. We can complete the picture by keeping the partial parsetrees created by the reductions attached to their non-terminals. Now all the terminal
symbols of the original input are still there; the terminals in the stack are one part ofthem, another part is semi-hidden in the partial parse trees and the rest is untouched in
the rest of the input. No information is lost, but some structure has been added. Whenthe bottom-up parser has reached the situation where the rest of the input is empty and
the stack contains only the start symbol, we have achieved a parsing and the parse treewill be dangling from the start symbol. This view clearly exposes the idea that parsing
is nothing but structuring the input.

ttgg NNff ttee ttdd NNcc NNbb ttaa tt11 tt22 tt33 .. ..

CUT
STACK REST OF INPUT
terminalsand
non-terminals

terminalsonly

partial parsetrees

Figure 7.1 The structure of a bottom-up parse
The cut between stack and rest of input is often drawn as a gap, for clarity andsince in actual implementations the two are often represented quite differently in the

Ch. 7] General bottom-up parsing 145
parser.

ttgg NNff ttee ttdd NNcc NNbb ttaa tt11 tt22 tt33 .. ..

...
...
.

ttgg NNff ttee ttdd NNcc NNbb ttaa tt11

...
...
. tt22 tt33 .. ..

shifting tt11

Figure 7.2 A shift move in a bottom-up automaton
ttgg NNff ttee ttdd NNcc NNbb ttaa

...
...
. tt11 tt22 tt33 .. ..

ttgg NNff ttee ttdd RR

...
...
. tt11 tt22 tt33 .. ..

NNccNNbbttaa. . . . . . . . . . . . .......

.................
...
.

reducing NNccNNbbttaa to RR

Figure 7.3 A reduce move in a bottom-up automaton
Our non-deterministic bottom-up automaton can make only two moves: shift andreduce; see Figures 7.2 and 7.3. During a shift, a (terminal) symbol is shifted from the
rest of input to the stack; tt11 is shifted in Figure 7.2. During a reduce move, a numberof symbols from the right end of the stack, which form the right-hand side of a rule for
a non-terminal, are replaced by that non-terminal and are attached to that non-terminalas the partial parse tree.

NNccNNbbttaa is reduced to RR in Figure 7.3; note that the original
NNccNNbbttaa are still present inside the partial parse tree. There would, in principle, be noharm in performing the instructions backwards, an unshift and unreduce, although they

would seem to move us away from our goal, which is to obtain a parse tree. We shallsee that we need them to do backtracking.

At any point in time the machine can either shift (if there is an input symbol left)or not, or it can do one or more reductions, depending on how many right-hand sides
can be recognized. If it cannot do either, it will have to resort to the backtrack moves,

146 General bottom-up parsing [Ch. 7
to find other possibilities. And if it cannot even do that, it is finished, and has found all(zero or more) parsings.

7.1 PARSING BY SEARCHING
The only problem left is how to guide the automaton through all of the possibilities.This is easily recognized as a search problem, which can be handled by a depth-first or

a breadth-first method. We shall now see how the machinery operates for both searchmethods. Since the effects are exponential in size, even the smallest example gets quite
big and we shall use the unrealistic grammar of Figure 7.4. The test input is aaaaaaaabb.

1. SSSS -->> aa SS bb2.

SS -->> SS aa bb3.
SS -->> aa aa aa

Figure 7.4 A simple grammar for demonstration purposes

(a) e'e'aaaaaaaabb
(b) aa e'e'aaaaaabb
(c) aaaa e'e'aaaabb
(d) aaaaaa33 e'e'aabb
(e) aaaaaa33aa33 e'e'bb
(f) aaaaaa33aa33bbu`u`
(g) aaaaaa33aa33u`u` bb
(h) aaSS e'e'bb

aaaa33aa

(i) aaSSbb11u`u`

aaaa33aa

(j)

\Gamma SSu`u`

aaSSbb
aaaa33aa

(k) aaSSbbu`u`

aaaa33aa

(l) aaSSu`u` bb

aaaa33aa

(m) aaaaaa33aau`u` bb
(n) aaaaaa33u`u` aabb
(o) SS e'e'aabb

aaaaaa

(p) SSaa e'e'bb

aaaaaa

(q) SSaabb22u`u`

aaaaaa

(r)

\Gamma SSu`u`

SSaabb
aaaaaa

(s) SSaabbu`u`

aaaaaa

(t) SSaau`u` bb

aaaaaa

(u) SSu`u` aabb

aaaaaa

(v) aaaaaau`u` aabb
(w) aaaau`u` aaaabb
(x) aau`u` aaaaaabb
(y) u`u` aaaaaaaabb

Figure 7.5 Stages for the depth-first parsing of aaaaaaaabb
7.1.1 Depth-first (backtracking) parsingRefer to Figure 7.5, where the gap for a shift is shown as e'e' and that for an unshift as u`u` .
At first the gap is to the left of the entire input (a) and shifting is the only alternative;likewise with (b) and (c). In (d) we have a choice, either to shift, or to reduce using rule
3; we shift, but remember the possible reduction(s); the rule numbers of these are

Sec. 7.1] Parsing by searching 147
shown as subscripts to the symbols in the stack. Idem in (e). In (f) we have reached aposition in which shift fails, reduce fails (there are no right-hand sides

aaaaaaaabb, aaaaaabb,
aaaabb, aabb or bb) and there are no stored alternatives. So we start backtracking by unshift-ing (g). Here we find a stored alternative, "reduce by 3", which we apply (h), deleting

the index for the stored alternative in the process; now we can shift again (i). No moreshifts are possible, but a reduce by 1 gives us a parsing (j). After having enjoyed our
success we unreduce (k); note that (k) only differs from (i) in that the stored alternative1 has been consumed. Unshifting, unreducing and again unshifting brings us to (n)
where we find a stored alternative, "reduce by 3". After reducing (o) we can shiftagain, twice (p, q). A "reduce by 2" produces the second parsing (r). The rest of the
road is barren: unreduce, unshift, unshift, unreduce (v) and three unshifts bring theautomaton to a halt, with the input reconstructed (y).

(a1) initial
(b1) shifted from a1aa
(c1) shifted from b1aaaa
(d1) shifted from c1aaaaaa
(d2) reduced from d1SS

aaaaaa

(e1) shifted from d1aaaaaaaa
(e2) shifted from d2SSaa

aaaaaa

(e3) reduced from e1aaSS

aaaaaa

(f1) shifted from e1aaaaaaaabb
(f2) shifted from e2SSaabb

aaaaaa

(f3) shifted from e3aaSSbb

aaaaaa

(f4) reduced from f2

\Gamma SS

SSaabb
aaaaaa

(f5) reduced from f3

\Gamma SS

aaSSbb
aaaaaa

Figure 7.6 Stages for the breadth-first parsing of aaaaaaaabb
7.1.2 Breadth-first (on-line) parsingBreadth-first bottom-up parsing is simpler than depth-first, at the expense of a far
larger memory requirement. Since the input symbols will be brought in one by one(each causing a shift, possibly followed by some reduces), our representation of a partial parse will consist of the stack only, together with its attached partial parse trees.We shall never need to do an unshift or unreduce. Refer to Figure 7.6. We start our
solution set with only one empty stack (a1). Each parse step consist of two phases; inphase one the next input symbol is appended to the right of all stacks in the solution set;
in phase two all stacks are examined and if they allow one or more reductions, one ormore copies are made of it, to which the reductions are applied. This way we will never
miss a solution. The first and second aa are just appended (b1, c1), but the third allows areduction (d2). The fourth causes one more reduction (e2) and the fifth gives rise to
two reductions, each of which produces a parsing (f4 and f5).

148 General bottom-up parsing [Ch. 7
7.1.3 A combined representationThe configurations of the depth-first parser can be combined into a single graph; see
Figure 7.7(a) where numbers indicate the order in which the various shifts and reducesare performed. Shifts are represented by lines to the right and reduces by upward
arrows. Since a reduce often combines a number of symbols, the additional symbols arebrought in by arrows that start upwards from the symbols and then turn right to reach
the resulting non-terminal. These arrows constitute at the same time the partial parsetree for that non-terminal. Start symbols in the right-most column with partial parse
trees that span the whole input head complete parse trees.

aa aa aa aa bb1 2 3 4

SS5 bb6

SS7

\Gamma 

SS8 aa bb9 10

SS11

\Gamma 

(a)

aa aa aa aa bb1 2 4 7

SS6 bb8

SS10

\Gamma 

SS3 aa bb5 9

SS11

\Gamma 

(b)
Figure 7.7 The configurations of the parsers combined
If we complete the stacks in the solution sets in our breadth-first parser byappending the rest of the input to them, we can also combine them into a graph, and,
what is more, into the same graph; only the action order as indicated by the numbers isdifferent, as shown in Figure 7.7(b). This is not surprising, since both represent the
total set of possible shifts and reduces; depth-first and breadth-first are just two dif-ferent ways to visit all nodes of this graph. Figure 7.7(b) was drawn in the same form
as Figure 7.7(a); if we had drawn the parts of the picture in the order in which they areexecuted by the breadth-first search, many more lines would have crossed. The picture
would have been equivalent to (b) but much more complicated to look at.
7.1.4 A slightly more realistic exampleThe above algorithms are relatively easy to understand and implement# and although
they require exponential time in general, they behave reasonably well on a number ofgrammars. Sometimes, however, they will burst out in a frenzy of senseless activity,
even with an innocuous-looking grammar (especially with an innocuous-looking gram-mar!). The grammar of Figure 7.8 produces algebraic expressions in one variable,

aa,and two operators,
++ and --. QQ is used for the operators, since OO (oh) looks too much like
00 (zero). This grammar is unambiguous and for aa--aa++aa it has the correct productiontree

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

See, for instance, Hext and Roberts [CF 1970] for Do"mo"lki's method to find all possiblereductions simultaneously.

Sec. 7.1] Parsing by searching 149

++
-- aa
aa aa

which restricts the minus to the following aa rather than to aa++aa. Figure 7.9 shows thegraph searched while parsing

aa--aa++aa. It contains 108 shift lines and 265 reduce arrowsand would fit on the page only thanks to the exceedingly fine print the phototypesetter

is capable of. This is exponential explosion.

SSSS -->> EE

EE -->> EE QQ FF
EE -->> FF
FF -->> aa
QQ -->> ++
QQ -->> --

Figure 7.8 A grammar for expressions in one variable

7.2 TOP-DOWN RESTRICTED BREADTH-FIRST BOTTOM-UP PARSING
In spite of their occasionally vicious behaviour, breadth-first bottom-up parsers areattractive since they work on-line, can handle left-recursion without any problem and

can generally be doctored to handle e-rules. So the question remains how to curb theirneedless activity. Many methods have been invented to restrict the search breadth to at
most 1, at the expense of the generality of the grammars these methods can handle; seeChapter 9. A method that will restrict the fan-out to reasonable proportions while still
retaining full generality was developed by Earley [CF 1970].
7.2.1 The Earley parser without look-aheadWhen we take a closer look at Figure 7.9, we see after some thought that many reductions are totally pointless. It is not meaningful to reduce the third aa to EE or SS since thesecan only occur at the end if they represent the entire input; likewise the reduction of
aa--aa to SS is absurd, since SS can only occur at the end. Earley noticed that what waswrong with these spurious reductions was that they were incompatible with a top-down

parsing, that is: they could never derive from the start symbol. He then gave a methodto restrict our reductions only to those that derive from the start symbol. We shall see
that the resulting parser takes at most n 3 units of time for input of length n rather thanC n.

Earley's parser can also be described as a breadth-first top-down parser withbottom-up recognition, which is how it is explained by the author [CF 1970]. Since it
can, however, handle left-recursion directly but needs special measures to handle e-rules, we prefer to treat it as a bottom-up method.

We shall again use the grammar from Figure 7.8 and parse the input aa--aa++aa. Justas in the non-restricted algorithm, we have at all times a set of partial solutions which
is modified by each symbol we read. We shall write the sets between the input symbols

150 General bottom-up parsing [Ch. 7

aa -- aa ++ aaFFEE

SSQQ aaFF
EESSFF ++ aa
FFEESS
QQ aaFFEE

SSEE ++ aaFF
EESSQQ aa
FFEESS
EESS ++ aa

FFEESS
QQ aaFFEE

SSQQ aa ++ aaFF
EESSQQ aa
FFEESS
FF ++ aaFFEE

SSQQ aaFF
EESSEE ++ aa
FFEESS
QQ aaFFEE

SSEE
SS ++ aaFFEE

SSQQ aaFF
EESSFF -- aa ++ aa
FFEESS
QQ aaFFEE

SSFF ++ aaFF
EESSQQ aa
FFEESS
EE ++ aaFFEE

SSQQ aaFF
EESSEE

SS ++ aaFFEE

SSQQ aaFF
EESSQQ aa ++ aa
FFEESS
QQ aaFFEE

SSFF ++ aaFF
EESSQQ aa
FFEESS
EE ++ aaFFEE

SSQQ aaFF
EESSEE

SS ++ aaFFEE

SSQQ aaFF
EESSEE -- aa ++ aa
FFEESS
QQ aaFFEE

SSFF ++ aaFF
EESSQQ aa
FFEESS
EE ++ aaFFEE

SSQQ aaFF
EESSEE

SS ++ aaFFEE

SSQQ aaFF
EESSQQ aa ++ aa
FFEESS
QQ aaFFEE

SSFF ++ aaFF
EESSQQ aa
FFEESS
EE ++ aaFFEE

SSQQ aaFF
EESSEE

SS ++ aaFFEE

SSQQ aaFF
EESSEE
EE
SS -- aa ++ aaFFEE

SSQQ aaFF
EESSFF ++ aa
FFEESS
QQ aaFFEE

SSEE ++ aaFF
EESSQQ aa
FFEESS
EESS ++ aa

FFEESS
QQ aaFFEE

SSQQ aa ++ aaFF
EESSQQ aa
FFEESS
FF ++ aaFFEE

SSQQ aaFF
EESSEE ++ aa
FFEESS
QQ aaFFEE

SSEE
SS ++ aaFFEE

SSQQ aaFF
EESS

\Gamma 

Figure 7.9 The graph searched while parsing aa--aa++aa

Sec. 7.2] Top-down restricted breadth-first bottom-up parsing 151
as we go; we have to keep earlier sets, since they will still be used by the algorithm.Unlike the non-restricted algorithm, in which the sets contained stacks, the sets consist
of what is technically known as items, or Earley items to be more precise. An item is agrammar rule with a gap in its right-hand side; the part of the right-hand side to the left
of the gap (which may be empty) has already been recognized, the part to the right ofthe gap is predicted. The gap is traditionally shown as a fat dot:

\Delta  . Items are for

instance: EE-->>

\Delta \Delta  EEQQFF, EE-->>EE\Delta \Delta  QQFF, EE-->>EEQQ\Delta \Delta  FF, EE-->>EEQQFF\Delta \Delta  , FF-->>aa\Delta \Delta  , etc. It is unfortunate when a

vague every-day term gets endowed with a very specific technical meaning, but theexpression has taken hold, so it will have to do. An Earley item is an item with an indication of the position of the symbol at which the recognition of the recognized partstarted. Notations vary, but we shall write

@@n after the item (read: "at n"). If the set atthe end of position 7 contains the item
EE-->>EE

\Delta \Delta  QQFF@@33, we have recognized an EE in positions 3, 4, 5, 6, 7 and are looking forward to recognizing QQFF.The sets of items contain exactly those items a) of which the part before the dot

has been recognized so far and b) of which we are certain that we shall be able to usethe result when they will happen to be recognized in full (but we cannot, of course, be
certain that that will happen). If a set contains the item EE-->>EE

\Delta \Delta  QQFF@@33, we can be sure that

when we will have recognized the whole right-hand side EEQQFF, we can go back to the setat the beginning of symbol number 3 and find there an item that was looking forward to

recognizing an EE, i.e., that had an EE with a dot in front of it. Since that is true recur-sively, no recognition will be in vain.

7.2.1.1 The Scanner, Completer and PredictorThe construction of an item set from the previous item set proceeds in three phases.
The first two correspond to those of the non-restricted algorithm, where they werecalled "shift" and "reduce"; here they are called "Scanner" and "Completer". The third
is new and is related to the top-down component; it is called "Predictor".

items afterprevious

symbol
itemsetp-1

sp

itemscompleted
by sp

completedp

activeitems
after sppredicted

items
. . . . . . . . . . . . . . . . . .

act/predp

= itemsetp
Figure 7.10 The Earley items sets for one input symbol
The Scanner, Completer and Predictor deal with four sets of items for each tokenin the input. Refer to Figure 7.10, where the input symbol s

p at position p is sur-rounded by the four sets: itemset
p -1, which contains the items available just before sp;completed
p, the set of items that have become completed due to sp; activep, whichcontains the non-completed items that passed s

p; and predictedp, the set of newlypredicted items. The sets active
p and predictedp together form itemsetp; the internal

152 General bottom-up parsing [Ch. 7
division will be indicated in the drawings by a dotted line. Initially, itemsetp -1 is filled(as a result of processing s

p -1) and the other sets are empty; the construction of item-set
0 is special.The Scanner looks at s

p, goes through itemsetp -1 and makes copies of all itemsthat contain
\Delta  s (all other items are ignored); in those, the part before the dot was

already recognized and now s is recognized. Consequently, the Scanner changes

\Delta  s

into s

\Delta  . If the dot is now at the end, it stores the item in the set completedp; otherwise

it stores it in the set activep.Next the Completer inspects completed

p, which contains the items that have justbeen recognized completely and can now be reduced. This reduction goes as follows.

For each item of the form R o"" . . .

\Delta  @@m the Completer goes to itemsetm -1, and calls the

Scanner; the Scanner, which was used to work on the sp found in the input anditemset

p -1, is now directed to work on the R recognized by the Completer anditemset
m -1. It will make copies of all items in itemsetm -1 featuring a

\Delta  R, replace the \Delta  R

by R

\Delta  and store them in either completedp or activep, as appropriate. This can add

indirectly recognized items to the set completedp, which means more work for theCompleter. After a while, all completed items have been reduced, and the Predictor's

turn has come.The Predictor goes through the sets active

p (which was filled by the Scanner) andpredicted
p (which is empty initially), and considers all non-terminals which have a dotin front of them; these we expect to see in the input. For each expected (predicted)

non-terminal N and for each rule for that non-terminal No""P . . . , the Predictor adds anitem No""

\Delta  P . . . @@p +1 to the set predictedp. This may introduce new predicted nonterminals (for instance, P) in predictedp which cause more predicted items. After awhile, this too will stop.

The sets activep and predictedp together form the new itemsetp. If the completedset for the last symbol in the input contains an item So"" . . .

\Delta  @@1, i.e., an item spanning

the entire input and reducing to the start symbol, we have found at least one parsing.Now refer to Figure 7.11, which shows the items sets of the Earley parser working

on aa--aa++aa. The initial active item set active 0 is {SS-->>

\Delta \Delta  EE@@11}, indicating that this is the

only item that can derive directly from the start symbol. The Predictor first predicts
EE-->>

\Delta \Delta  EEQQFF@@11, from this EE-->>\Delta \Delta  EEQQFF@@11 and EE-->>\Delta \Delta  FF@@11 (but the first one is in the set already)

and from the last one FF-->>

\Delta \Delta  aa@@11. This gives itemset 0.

The Scanner working on itemset 0 and scanning for an aa, only catches FF-->>

\Delta \Delta  aa@@11,

which it turns into FF-->>aa

\Delta \Delta  @@11 and stores in completed 1. This not only means that we

have recognized and reduced an FF, but also that we have a buyer for it. The Completergoes to the set itemset

0 and copies all items that have

\Delta \Delta  FF. Result: one item, EE-->>\Delta \Delta  FF@@11,

which turns into EE-->>FF

\Delta \Delta  @@11 and is again stored in completed 1. More work for the Completer, which will now copy items containing

\Delta \Delta  EE; result: two items, SS-->>\Delta \Delta  EE@@11 which

becomes SS-->>EE

\Delta \Delta  @@11 and goes to the completed set, and EE-->>\Delta \Delta  EEQQFF@@11 which becomes

EE-->>EE

\Delta \Delta  QQFF@@11 and which becomes the first and only member of active 1. The completion

of SS yields no new information.The Predictor working on active

1 has an easy job:

\Delta \Delta  QQ causes two items for QQ, both

with @@22, since that is where recognition will have started, if it occurs at all. Nothingspectacular happens until the Scanner processes the second

aa; from itemset 2 it extracts
FF-->>

\Delta \Delta  aa@@33 which gives FF-->>aa\Delta \Delta  @@33 which is passed to the Completer (through completed 3). The latter sees the reduction of aa to FF starting at position 3, goes to itemset 2to see who ordered an

FF, and finds EE-->>EEQQ

\Delta \Delta  FF@@11; given the FF, this turns into EE-->>EEQQFF\Delta \Delta  @@11,

Sec. 7.2] Top-down restricted breadth-first bottom-up parsing 153

SS-->>

\Gamma \Gamma  EE @@11

EE-->>

\Gamma \Gamma  EEQQFF@@11

EE-->>

\Gamma \Gamma  FF @@11

FF-->>

\Gamma \Gamma  aa @@11

. . . . . . . . . . . . . . . . . .

act/pred0

= itemset0

aa1

FF-->>aa

\Gamma \Gamma  @@11

EE-->>FF

\Gamma \Gamma  @@11

SS-->>EE

\Gamma \Gamma  @@11

completed1

EE-->>EE

\Gamma \Gamma  QQFF@@11

QQ-->>

\Gamma \Gamma  ++ @@22

QQ-->>

\Gamma \Gamma  -- @@22

. . . . . . . . . . . . . . . . . .

act/pred1

= itemset1

--2

QQ-->>--

\Gamma \Gamma  @@22completed2

EE-->>EEQQ

\Gamma \Gamma  FF@@11

FF-->>

\Gamma \Gamma  aa @@33. . . . . . . . . . . . . . . . . .

act/pred2

= itemset2

aa3

FF-->>aa

\Gamma \Gamma  @@33

EE-->>EEQQFF

\Gamma \Gamma  @@11

SS-->>EE

\Gamma \Gamma  @@11

completed3

EE-->>EE

\Gamma \Gamma  QQFF@@11

QQ-->>

\Gamma \Gamma  ++ @@44

QQ-->>

\Gamma \Gamma  -- @@44

. . . . . . . . . . . . . . . . . .

act/pred3

= itemset3

++4

QQ-->>++

\Gamma \Gamma  @@44completed4

EE-->>EEQQ

\Gamma \Gamma  FF@@11

FF-->>

\Gamma \Gamma  aa @@55. . . . . . . . . . . . . . . . . .

act/pred4

= itemset4

aa5

FF-->>aa

\Gamma \Gamma  @@55

EE-->>EEQQFF

\Gamma \Gamma  @@11

SS-->>EE

\Gamma \Gamma  @@11

completed5

EE-->>EE

\Gamma \Gamma  QQFF@@11active5

Figure 7.11 Items sets of the Earley parser working on aa--aa++aa
which in its turn signals the reduction to EE of the substring from 1 to 3 (again throughcompleted

3). The Completer checks itemset 0 and finds two clients there for the EE:SS-->>
\Delta \Delta  EE@@11 and EE-->>\Delta \Delta  EEQQFF@@11; the first ends up as SS-->>EE\Delta \Delta  @@11 in completed 3, the second as

EE-->>EE

\Delta \Delta  QQFF@@11 in active 3.

After the last symbol has been processed by the Scanner, we still run the Com-pleter to do the final reductions, but running the Predictor is useless, since there is nothing to predict any more. Note that the parsing started by calling the Predictor on the ini-tial active set and that there is one Predictor/Scanner/Completer action for each symbol. Since the last completed set indeed contains an item SS-->>EE

\Delta \Delta  @@11, there is at least one

parsing.

7.2.1.2 Constructing a parse treeAll this does not directly give us a parse tree. As is more often the case in parser construction (see, for instance, Section 4.1) we have set out to build a parser and haveended up building a recognizer. The intermediate sets, however, contain enough information about fragments and their relations to construct a parse tree easily. As with theCYK parser, a simple top-down Unger-type parser can serve for this purpose, since the
Unger parser is very interested in the lengths of the various components of the parsetree and that is exactly what the sets in the Earley parser provide. In his 1970 article,
Earley gives a method of constructing the parse tree(s) while parsing, by keeping witheach item a pointer back to the item that caused it to be present. Tomita [CF 1986, p.
74-77] has, however, shown that this method will produce incorrect parse trees on cer-tain ambiguous grammars.

From the set completed 5 in Figure 7.11, which is the first we inspect after havingfinished the set construction, we see that there is a parse possible with

SS for a root andextending over symbols 1 to 5; we designate the parse root as
SS1-5 in Figure 7.12.

154 General bottom-up parsing [Ch. 7
Given the completed item SS-->>EE

\Delta \Delta  @@11 in completed 5 there must be a parse node EE1-5,

which is completed at 5. Since all items completed after 5 are contained in completed 5,we scan the latter to find a completed

EE starting at 1; we find EE-->>EEQQFF

\Delta \Delta  @@11. This gives

us parse tree (a), where the values at the question marks are still to be seen. Since itemsare recognized at their right ends, we start by finding a parse for the

FF?-5, to be foundin completed

5. We find FF-->>aa

\Delta \Delta  @@55, giving us parse tree (b). It suggests that we find a

parse for QQ?-4 completed after 4; in completed 4 we find QQ-->>++

\Delta \Delta  @@44. Consequently QQ?-4

is QQ4-4 and the EE1-? in (b) must be EE1-3. This makes us look in completed 3 for an
EE-->>......@@11, where we find EE-->>EEQQFF

\Delta \Delta  @@11. We now have parse tree (c), and, using the

same techniques, we easily complete it (d).

SS11--55

EE11--55
EE11--?? QQ??--?? FF??--55

(a)

SS11--55
EE11--55
EE11--?? QQ??--44 FF55--55

aa55(b)

SS11--55
EE11--55
EE11--33 QQ44--44 FF55--55
EE11--?? QQ??--?? FF??--33 ++44 aa55

(c)

SS11--55
EE11--55
EE11--33 QQ44--44 FF55--55
EE11--11 QQ22--22 FF33--33 ++44 aa55
FF11--11 --22 aa33

aa11 (d)
Figure 7.12 Construction of the parse trees

Sec. 7.2] Top-down restricted breadth-first bottom-up parsing 155
7.2.1.3 Space and time requirementsIt is interesting to have a look at the space and time needed for the construction of the
sets. First we calculate the maximum size of the sets just after symbol number p. Thereis only a fixed number of different items, I, limited by the size of the grammar; for our
grammar it is I =14. However, each item can occur with any of the additions @@1 to
@@p +1, of which there are p +1. So the number of items in the set itemsetp is limited toI *(p +1). The exact calculation of the maximum number of items in each of the sets is

complicated by the fact that different rules apply to the first, last and middle items.Disregarding these complications, we find that the maximum number of items in all
itemsets up to p is roughly I *p 2 / 2. The same applies to the completed sets. So, for aninput of length n, the memory requirement is O (n 2), as with the CYK algorithm. In
actual practice, the amount of memory used is often far less than this theoretical max-imum. In our case all sets together could conceivably contain about 14*52=350 items,
with which the actual number of 4+3+3+1+2+3+3+1+2+3+1=26 items compares veryfavourably.

Although a set at position p can contain a maximum of O (p) items, it may requirean amount of work proportional to p 2 to construct that set, since each item could, in
principle, be inserted by the Completer once from each preceding position. Under thesame simplifying assumptions as above, we find that the maximum number of actions
needed to construct all sets up to p is roughly I *p 3 / 6. So the total amount of workinvolved in parsing a sentence of length n with the Earley algorithm is O (n 3), as it is
with the CYK algorithm. Again, in practice it is much better: on many grammars,including the one from Figure 7.8, it will work in linear time (O (n)) and on any unambiguous grammar it will work in O (n 2). In our example, a maximum of about14*53 / 6~-300 actions might be required, compared to the actual number of 28 (both
items for EE in predicted 0 were inserted twice).It should be noted that once the calculation of the sets is finished, only the completed sets are consulted. The active and predicted sets can be thrown away to makeroom for the parse tree(s).

The practical efficiency of this and the CYK algorithms is not really surprising,since in normal usage most arbitrary fragments of the input will not derive from any
non-terminal. The sentence fragment "letter into the upper left-most" does notrepresent any part of speech, nor does any fragment of it of a size larger than one. The
O (n 2) and O (n 3) bounds only materialize for grammars in which almost all non-terminals produce almost all substrings in almost all combinatorially possible ways, as
for instance in the grammar SS-->>SSSS, SS-->>xx.
7.2.2 The relation between the Earley and CYK algorithmsThe similarity in the time and space requirement between the Earley and the CYK
algorithm suggest a deeper relation between the two and indeed there is one. The Ear-ley sets can be accommodated in a CYK-like grid; see Figure 7.13. To stress the similarity, the sets are distributed over diagonals of boxes slanting from north-west tosouth-east. Since the columns indicate the beginnings of possibly recognized fragments, all items with the same @@p come in the same column. This arrangement assignsa natural position to each item. Completed items are drawn in the top left corner of a
box, active items in the bottom right corner. Predicted items have not yet recognizedanything and live in the bottom layer.

When we compare this picture to that produced by the CYK parser (Figure 7.14)

156 General bottom-up parsing [Ch. 7

EE-->>EEQQFF
SS-->>EE

EE-->>EE

\Gamma \Gamma  QQFF

EE-->>EEQQ

\Gamma \Gamma  FF

EE-->>EEQQFF
SS-->>EE

EE-->>EE

\Gamma \Gamma  QQFF

EE-->>EEQQ

\Gamma \Gamma  FF

FF-->>aa
EE-->>FF
SS-->>EE

EE-->>EE

\Gamma \Gamma  QQFF

SS-->>

\Gamma \Gamma  EE

EE-->>

\Gamma \Gamma  EEQQFF

EE-->>

\Gamma \Gamma  EE

FF-->>

\Gamma \Gamma  aa

QQ-->>--
QQ-->>

\Gamma \Gamma  ++

QQ-->>

\Gamma \Gamma  --

FF-->>aa
FF-->>

\Gamma \Gamma  aa

QQ-->>++
QQ-->>

\Gamma \Gamma  ++

QQ-->>

\Gamma \Gamma  --

FF-->>aa
FF-->>

\Gamma \Gamma  aa

0

1
2
3
4
5

lengthrecognized

@@11

@@22

@@33

@@44

@@55

aa1

set0

--2

set1

aa3

set2

++4

set3

aa5

set4 set5

part ofcompletedp part of

activep

setp

predict-ions
Figure 7.13 The Earley sets represented in CYK fashion
we see correspondences and differences. Rather than having items, the boxes containnon-terminals only. All active and predicted items are absent. The left-hand sides of the
completed items also occur in the CYK picture, but the latter features more recognizednon-terminals; from the Earley picture we know that these will never play a role in any
parse tree. The costs and the effects of the top-down restriction are clearly shown.The correspondence between the Earley and the CYK algorithms has been
analysed by Graham and Harrison [CF 1976]. This has resulted in a combined algo-rithm described by Graham, Harrison and Ruzzo [CF 1980].

7.2.3 Ambiguous sentencesCalculating the sets for a parsing of an ambiguous sentence does not differ from that
for an unambiguous one. Some items will be inserted more than once into the same set,but that can happen even with unambiguous sentences. The parse trees will be faithfully produced by the Unger parser; when searching a completed set for items of the

Sec. 7.2] Top-down restricted breadth-first bottom-up parsing 157

EE
SS

EE
SS

FF
EE
SS

QQ

EE
SS

FF
EE
SS

QQ

FF
EE
SS1

2
3
4
5

lengthrecognized

@@11

@@22

@@33

@@44

@@55

aa1 --2 aa3 ++4 aa5
Figure 7.14 CYK sets for the parsing of Figure 7.11
form A o"" . . .

\Delta  @@p, it may find several. Each will produce a different parse tree (or set

of parse trees if further ambiguities are found). There may be exponentially many parsetrees (even though the work to produce the sets is limited to O (n 3)) or even infinitely

many of them. Infinite ambiguity is cut out automatically by the Unger parser, butexponential numbers of parse trees will just have to be suffered. If they are essential to
the application, Tomita [CF 1986, p. 17-20] has given an efficient packing method forthem.

The enumeration of all possible parse trees is often important, since manymethods augment the CF grammar with more long-range restrictions formulated outside the CF framework, to thus approximate a context-sensitive analysis. To this end,all parse trees are produced and checked; only those that meet the restrictions are
accepted.Figure 7.15 shows the sets for the parsing of an ambiguous sentence

xxxxxx accord-ing to the grammar
SS-->>SSSS, SS-->>xx; again an artificial example is the only one which canbe shown, for reasons of size. Figure 7.16 gives the parse trees. There is only one root

in completed 3: SS-->>SSSS

\Delta \Delta  @@11, leading to parse tree (a). Looking up a parsing for SS?-3 in

completed 3, we come up with three possibilities: SS-->>

\Delta \Delta  xx@@33, SS-->>SSSS\Delta \Delta  @@22 and SS-->>SSSS\Delta \Delta  @@11.

The first and second lead to parse trees (b) and (c) but the third is suppressed by theUnger parser (it would lead to infinite recursion). No further ambiguities occur and the

final parse trees are found in (d) and (e). All this is the same as in the CYK parser.
7.2.4 Handling e-rulesLike most parsers, the above parser cannot handle e-rules without special measures. erules show up first as an anomaly in the work of the Predictor. While predicting itemsof the form A o""

\Delta  . . . @@p +1 as a consequence of having a \Delta  A in an item in activep or

158 General bottom-up parsing [Ch. 7

SS-->>

\Gamma \Gamma  SSSS@@11

SS-->>

\Gamma \Gamma  xx @@11. . . . . . . . . . . . . . . . . .

act/pred0

= itemset0

xx1

SS-->>xx

\Gamma \Gamma  @@11completed1

SS-->>SS

\Gamma \Gamma  SS@@11

SS-->>

\Gamma \Gamma  SSSS@@22

SS-->>

\Gamma \Gamma  xx @@22

. . . . . . . . . . . . . . . . . .

act/pred1

= itemset1

xx2

SS-->>xx

\Gamma \Gamma  @@22

SS-->>SSSS

\Gamma \Gamma  @@11

completed2

SS-->>SS

\Gamma \Gamma  SS@@22

SS-->>SS

\Gamma \Gamma  SS@@11

SS-->>

\Gamma \Gamma  SSSS@@33

SS-->>

\Gamma \Gamma  xx @@33

. . . . . . . . . . . . . . . . . .

act/pred2

= itemset2

xx3

SS-->>xx

\Gamma \Gamma  @@33

SS-->>SSSS

\Gamma \Gamma  @@22

SS-->>SSSS

\Gamma \Gamma  @@11

completed3

SS-->>SS

\Gamma \Gamma  SS@@33

SS-->>SS

\Gamma \Gamma  SS@@22

SS-->>SS

\Gamma \Gamma  SS@@11

active3

Figure 7.15 Parsing of xxxxxx according to SS-->>SSSS, SS-->>xx
SS11--33

SS11--?? SS??--33

(a)

SS11--33
SS11--22 SS33--33

xx33
(b)

SS11--33
SS11--11 SS22--33

SS22--?? SS??--33

(c)

SS11--33
SS11--22 SS33--33
SS11--11 SS22--22 xx33

xx11 xx22

(d)

SS11--33
SS11--11 SS22--33

xx11 SS22--22 SS33--33

xx22 xx33

(e)

Figure 7.16 Parse tree construction for the parsing of Figure 7.15
predictedp, it may stumble upon an empty prediction A o""

\Delta  @@p +1; this means that the

non-terminal A has been completed just before symbol number p +1 and this completeditem should be added to the set completed

p, which up to now only contained items with@@p at most. So we find that there was more work for the Completer after all. But that is

not the end of the story. If we now run the Completer again, it will draw the conse-quences of the newly completed item(s) which have

@@p +1. So it will consult itemsetp,which is, however, incomplete since items are still being added to its constituents,

activep and predictedp. If it finds occurrences of

\Delta  A there, it will add copies with A \Delta 

instead; part of these may require new predictions to be done (if the dot lands in front

Sec. 7.2] Top-down restricted breadth-first bottom-up parsing 159
of another non-terminal), part may be completed items, which will have to go intocompleted

p and which mean more work for the Completer. The latter items can have astarting point lower than p, which brings in items from further back, which may or may

not now be completed through this action or through empty completed items at p.The easiest way to handle this mare's nest is to stay calm and keep running the
Predictor and Completer in turn until neither has anything more to add. Since thenumber of items is finite this will happen eventually, and in practice it happens rather
sooner than later.The Completer and Predictor loop has to be viewed as a single operation called
"X" by Graham, Harrison and Ruzzo [CF 1980]. Just like the Predictor it has to beapplied to the initial state, to honour empty productions before the first symbol; just
like the Completer it has to be applied to the final state, to honour empty productionsafter the last symbol.

Part of the effects are demonstrated by the grammar of Figure 7.17 which is basedon a grammar similar to that of Figure 7.8. Rather than addition and subtraction, this
one handles multiplication and division, with the possibility to omit the multiplicationsign:

aaaa means aa**aa.

SSSS -->> EE

EE -->> EE QQ FF
EE -->> FF
FF -->> aa
QQ -->> **
QQ -->> //
QQ -->> ee

Figure 7.17 A grammar with an e-rule
The parsing is given in Figure 7.18. The items pointed at by a

\Gamma  have been added

by a second pass of the Completer/Predictor. The QQ-->>

\Delta \Delta  @@22, inserted by the Predictor

into completed 1 as a consequence of EE-->>EE

\Delta \Delta  QQFF@@11 in active 1, is picked up by the second

pass of the Completer, and is used to clone EE-->>EE

\Delta \Delta  QQFF@@11 in active 1 into EE-->>EEQQ\Delta \Delta  FF@@11.

This in turn is found by the Predictor which predicts the item FF-->>

\Delta \Delta  aa@@22 from it. Note

that we now do have to consider the full active/predicted set after the last symbol; itsprocessing by the Completer/Predictor may insert an item of the form

SS-->>......@@11 in thelast completed set, indicating a parsing.

7.2.5 Prediction look-aheadIn the following we shall describe a series of increasingly complicated (and more efficient) parsers of the Earley type; somewhere along the line we will also meet a parserthat is (almost) identical to the one described by Earley in his paper.

When we go back to Figure 7.11 and examine the actions of the Predictor, we seethat it sometimes predicts items that it could know were useless if it could look ahead
at the next symbol. When the next symbol is a --, it is kind of foolish to proudly predict
QQ-->>

\Delta \Delta  ++@@22. The Predictor can of course easily be modified to check such simple cases,

but it is possible to have a Predictor that will never predict anything obviously errone-ous; all its predicted items will be either completed or active in the next set. (The predictions may, however, fail on the symbol after that; after all, it is a Predictor, not an

160 General bottom-up parsing [Ch. 7

SS-->>

\Gamma \Gamma  EE @@11

EE-->>

\Gamma \Gamma  EEQQFF@@11

EE-->>

\Gamma \Gamma  FF @@11

FF-->>

\Gamma \Gamma  aa @@11

. . . . . . . . . . . . . . . . . .

act/pred0

= itemset0

aa1

FF-->>aa

\Gamma \Gamma  @@11

EE-->>FF

\Gamma \Gamma  @@11

SS-->>EE

\Gamma \Gamma  @@11

QQ-->>

\Gamma \Gamma  @@22

completed1

EE-->>EE

\Gamma \Gamma  QQFF@@11

EE-->>EEQQ

\Gamma \Gamma  FF@@11

QQ-->>

\Gamma \Gamma  ** @@22

QQ-->>

\Gamma \Gamma  // @@22

FF-->>

\Gamma \Gamma  aa @@22

\Gamma 

. . . . . . . . . . . . . . . . . .

act/pred1

= itemset1

aa2

FF-->>aa

\Gamma \Gamma  @@22

EE-->>EEQQFF

\Gamma \Gamma  @@11

SS-->>EE

\Gamma \Gamma  @@11

QQ-->>

\Gamma \Gamma  @@33

completed2

EE-->>EE

\Gamma \Gamma  QQFF@@11

EE-->>EEQQ

\Gamma \Gamma  FF@@11

QQ-->>

\Gamma \Gamma  ** @@33

QQ-->>

\Gamma \Gamma  // @@33

FF-->>

\Gamma \Gamma  aa @@33

\Gamma 

. . . . . . . . . . . . . . . . . .

act/pred2

= itemset2

//3

QQ-->>//

\Gamma \Gamma  @@33completed3

EE-->>EEQQ

\Gamma \Gamma  FF@@11

FF-->>

\Gamma \Gamma  aa @@44. . . . . . . . . . . . . . . . . .

act/pred3

= itemset3

aa4

FF-->>aa

\Gamma \Gamma  @@44

EE-->>EEQQFF

\Gamma \Gamma  @@11

SS-->>EE

\Gamma \Gamma  @@11

QQ-->>

\Gamma \Gamma  @@55

completed4

EE-->>EE

\Gamma \Gamma  QQFF@@11

EE-->>EEQQ

\Gamma \Gamma  FF@@11

QQ-->>

\Gamma \Gamma  ** @@55

QQ-->>

\Gamma \Gamma  // @@55

FF-->>

\Gamma \Gamma  aa @@55

\Gamma 

. . . . . . . . . . . . . . . . . .

act/pred4

= itemset4
Figure 7.18 Recognition of empty productions in an Earley parser
Oracle.)To see how we can obtain such a perfect Predictor we need a different example
(after removing QQ-->>

\Delta \Delta  ++@@22 and QQ-->>\Delta \Delta  --@@44 from Figure 7.11 all predictions there come

true, so nothing can be gained any more).

SS''SS -->> SS

SS -->> AA || AABB || BB FIRST(SS) = {pp, qq}
AA -->> CC FIRST(AA) = {pp}
BB -->> DD FIRST(BB) = {qq}
CC -->> pp FIRST(CC) = {pp}
DD -->> qq FIRST(DD) = {qq}

Figure 7.19 A grammar for demonstrating prediction look-ahead and its FIRST sets

The artificial grammar of Figure 7.19 produces nothing but the three sentences pp,
qq and ppqq, and does so in a straightforward way. The root is SS'' rather than SS, which is aconvenient way to have a grammar with only one rule for the root. This is not necessary but it simplifies the following somewhat, and it is usual in practice.

SS''-->>

\Gamma \Gamma  SS @@11

SS-->>

\Gamma \Gamma  AA @@11

SS-->>

\Gamma \Gamma  AABB @@11

SS-->>

\Gamma \Gamma  BB @@11

AA-->>

\Gamma \Gamma  CC @@11

BB-->>

\Gamma \Gamma  DD @@11

CC-->>

\Gamma \Gamma  pp @@11

DD-->>

\Gamma \Gamma  qq @@11

. . . . . . . . . . . . . . . . . .

act/pred0

= itemset0

qq1

DD-->>qq

\Gamma \Gamma  @@11

BB-->>DD

\Gamma \Gamma  @@11

SS-->>BB

\Gamma \Gamma  @@11

SS''-->>SS

\Gamma \Gamma  @@11

completed1

. . . . . . . . . . . . . . . . . .act/pred1= itemset

1

(a)

SS''-->>

\Gamma \Gamma  SS @@11

SS-->>

\Gamma \Gamma  BB @@11

BB-->>

\Gamma \Gamma  DD @@11

DD-->>

\Gamma \Gamma  qq @@11

. . . . . . . . . . . . . . . . . .

act/pred0

= itemset0

qq1

DD-->>qq

\Gamma \Gamma  @@11

BB-->>DD

\Gamma \Gamma  @@11

SS-->>BB

\Gamma \Gamma  @@11

SS''-->>SS

\Gamma \Gamma  @@11

completed1

. . . . . . . . . . . . . . . . . .act/pred1= itemset

1

(b)

Figure 7.20 Parsing the sentence qq without look-ahead (a) and with look-ahead (b)

The parsing of the sentence qq is given in Figure 7.20(a) and (b). Starting from the

Sec. 7.2] Top-down restricted breadth-first bottom-up parsing 161
initial item, the Predictor predicts a list of 7 items (a). Looking at the next symbol, qq,the Predictor could easily avoid the prediction

CC-->>

\Delta \Delta  pp@@11, but several of the other predictions are also false, for instance, AA-->>

\Delta \Delta  CC@@11. The Predictor could avoid the first since it

sees that it cannot begin with qq; if it knew that CC cannot begin with a qq, it could alsoavoid

AA-->>

\Delta \Delta  CC@@11. (Note that itemset 1 is empty, indicating that there is no way for the

input to continue.)The required knowledge can be obtained by calculating the FIRST sets of all

non-terminals in the grammar (FIRST sets and a method of calculating them areexplained in Sections 8.2.1.1 and 8.2.2.1). The use of the FIRST sets is very effective:
the Predictor again starts from the initial item, but since it knows that qq is not inFIRST(

AA), it will not predict SS-->>

\Delta \Delta  AA@@11. Items like AA-->>\Delta \Delta  CC@@11 do not even have to be

avoided, since their generation will never be contemplated in the first place. Only the
BB-line will be predicted (b) and it will consist of three predictions, all of them to thepoint.

SS''SS -->> SS

SS -->> AA || AABB || BB FIRST(SS) = {ee, pp, qq}
AA -->> CC FIRST(AA) = {ee, pp}
BB -->> DD FIRST(BB) = {qq}
CC -->> pp || ee FIRST(CC) = {ee, pp}
DD -->> qq FIRST(DD) = {qq}

Figure 7.21 A grammar with an e-rule and its FIRST sets
Handling e-rules is easier now: we know for every non-terminal whether it canproduce e (in which case e is in the FIRST set of that non-terminal). If we add a rule
CC-->>ee to our grammar (Figure 7.21), the entire picture changes. Starting from the initialitem

SS''-->>

\Delta \Delta  SS@@11 (Figure 7.22), the Predictor will still not predict SS-->>\Delta \Delta  AA@@11 since

FIRST(AA) does not contain qq, but it will predict SS-->>

\Delta \Delta  AABB@@11 since FIRST(AABB) does contain a qq (BB combined with the transparency of AA). The line continues by predicting
AA-->>

\Delta \Delta  CC@@11, but CC-->>\Delta \Delta  @@11 is a completed item and goes into completed 0. When the Completer starts, it finds CC-->>

\Delta \Delta  @@11, applies it to AA-->>\Delta \Delta  CC@@11 and produces AA-->>CC\Delta \Delta  @@11, likewise

completed. The latter is then applied to SS-->>

\Delta \Delta  AABB@@11 to produce the active item

SS-->>AA

\Delta \Delta  BB@@11. This causes another run of the Predictor, to follow the new \Delta \Delta  BB, but all those

items have already been added.Bouckaert, Pirotte and Snelling, who have analysed variants of the Earley parsers

for two different look-ahead regimes [CF 1975], show that predictive look-aheadreduces the number of items by 20 to 50% or even more on "practical" grammars.

7.2.6 Reduction look-aheadOnce we have gone through the trouble of calculating the FIRST sets, we can use them
for a second type of look-ahead: reduction look-ahead. Prediction look-ahead reducesthe number of predicted items, reduction look-ahead reduces the number of completed
items. Referring back to Figure 7.11, which depicted the actions of an Earley parserwithout look-ahead, we see that it does two silly completions:

SS-->>EE

\Delta \Delta  @@11 in completed 1,

and SS-->>EE

\Delta \Delta  @@11 in completed 3. The redundancy of these completed items stems from the

fact that they are only meaningful at the end of the input. Now this may seem a veryspecial case, not worth testing for, but the phenomenon can be put in a more general

162 General bottom-up parsing [Ch. 7

CC-->>

\Gamma \Gamma  @@11

AA-->>CC

\Gamma \Gamma  @@11\Gamma 

\Gamma  completed0

SS''-->>

\Gamma \Gamma  SS @@11

SS-->>AA

\Gamma \Gamma  BB @@11

SS-->>

\Gamma \Gamma  AABB @@11

SS-->>

\Gamma \Gamma  BB @@11

AA-->>

\Gamma \Gamma  CC @@11

BB-->>

\Gamma \Gamma  DD @@11

DD-->>

\Gamma \Gamma  qq @@11

\Gamma  . . . . . . . . . . . . . . . . . .

act/pred0

= itemset0

qq1

DD-->>qq

\Gamma \Gamma  @@11

BB-->>DD

\Gamma \Gamma  @@11

SS-->>AABB

\Gamma \Gamma  @@11

SS-->>BB

\Gamma \Gamma  @@11

SS''-->>SS

\Gamma \Gamma  @@11

completed1

. . . . . . . . . . . . . . . . . .act/pred1= itemset

1

Figure 7.22 Parsing the sentence qq with the grammar of Figure 7.21
setting: if we introduce an explicit symbol for end-of-file (for instance, ##), we can saythat the above items are redundant because they are followed by a symbol (

-- and ++,respectively) which is not in the set of symbols the item should be followed by on completion.The trick is now to keep, together with any item, a set of symbols which may
come after that item, the reduction look-ahead set; if the item seems completed but thenext symbol is not in this set, the item is discarded. The rules for constructing the
look-ahead set for an item are straightforward, but unlike the prediction look-ahead itcannot be calculated in advance; it must be constructed as we go. (A limited and less
effective set could be calculated statically, using the FOLLOW sets explained in8.2.2.2.)

The initial item starts with a look-ahead set of [[##]] (the look-ahead set will beshown between square brackets at the end of the item). When the dot advances in an
item, its look-ahead set remains the same, since what happens inside an item does notaffect what may come after it. When a new item is created by the Predictor, a new
look-ahead set must be composed. Suppose the item is

P o""A

\Delta  BCD [abc ] @@n

and predicted items for B must be created. We now ask ourselves what symbols mayfollow the occurrence of B in this item. It is easy to see that they are:
\Gamma  any symbol C can start with,
\Gamma  if C can produce the empty string, any symbol D can start with,
\Gamma  if D can also produce the empty string, any of the symbols a, b and c.

Given the FIRST sets for all non-terminals, which can also tell us if a non-terminal canproduce empty, the resulting new reduction look-ahead set is easily calculated. It is also

written as FIRST(CD [abc ]), which is of course the set of first symbols of anythingproduced by CDa | CDb | CDc.

The Earley sets with reduction look-ahead for our example aa--aa++aa are given inFigure 7.23, where we have added a

## symbol in position 6. The calculation of the setsfollow the above rules. The look-ahead of the item

EE-->>

\Delta \Delta  EEQQFF[[##++--]]@@11 in predicted 0

results from its being inserted twice: once predicted from SS-->>

\Delta \Delta  EE[[##]]@@11, which contributes the ##, and once from EE-->>

\Delta \Delta  EEQQFF[[......]]@@11, which contributes the ++-- from FIRST(QQ).

Sec. 7.2] Top-down restricted breadth-first bottom-up parsing 163
The look-ahead ...... is used to indicate that the look-ahead is not yet known but does notinfluence the look-ahead the item contributes.

Note that the item SS-->>EE

\Delta \Delta  [[##]]@@11 is not placed in completed 1, since the actual symbol ahead (--2) is not in the item's look-ahead set; something similar occurs in com-pleted

3, but not in completed 5.

SS-->>

\Gamma \Gamma  EE [[##]] @@11

EE-->>

\Gamma \Gamma  EEQQFF[[##++--]]@@11

EE-->>

\Gamma \Gamma  FF [[##++--]]@@11

FF-->>

\Gamma \Gamma  aa [[##++--]]@@11

. . . . . . . . . . . . . . . . . . . . . . . . . .

act/pred0

= itemset0

aa1

FF-->>aa

\Gamma \Gamma  @@11

EE-->>FF

\Gamma \Gamma  @@11

completed1

EE-->>EE

\Gamma \Gamma  QQFF[[##++--]]@@11

QQ-->>

\Gamma \Gamma  ++ [[aa]] @@22

QQ-->>

\Gamma \Gamma  -- [[aa]] @@22

. . . . . . . . . . . . . . . . . . . . . . . . . .

act/pred1

= itemset1

--2

QQ-->>--

\Gamma \Gamma  @@22completed2

EE-->>EEQQ

\Gamma \Gamma  FF[[##++--]]@@11

FF-->>

\Gamma \Gamma  aa [[##++--]]@@33. . . . . . . . . . . . . . . . . . . . . . . . . .

act/pred2

= itemset2

aa3

FF-->>aa

\Gamma \Gamma  @@33

EE-->>EEQQFF

\Gamma \Gamma  @@11

completed3

EE-->>EE

\Gamma \Gamma  QQFF[[##++--]]@@11

QQ-->>

\Gamma \Gamma  ++ [[aa]] @@44

QQ-->>

\Gamma \Gamma  -- [[aa]] @@44

. . . . . . . . . . . . . . . . . . . . . . . . . .

act/pred3

= itemset3

++4

QQ-->>++

\Gamma \Gamma  @@44completed4

EE-->>EEQQ

\Gamma \Gamma  FF[[##++--]]@@11

FF-->>

\Gamma \Gamma  aa [[##++--]]@@55. . . . . . . . . . . . . . . . . . . . . . . . . .

act/pred4

= itemset4

aa5

FF-->>aa

\Gamma \Gamma  @@55

EE-->>EEQQFF

\Gamma \Gamma  @@11

SS-->>EE

\Gamma \Gamma  @@11

completed5

EE-->>EE

\Gamma \Gamma  QQFF[[##++--]]@@11active5

##6

Figure 7.23 Item sets with reduction look-ahead
As with prediction look-ahead, the gain in our example is meagre. The effective-ness in the general case is not easily determined. Earley recommends the reduction
look-ahead, but does not take into account the effort required to calculate and maintainthe look-ahead sets. Bouckaert, Pirotte and Snelling definitely condemn the reduction
look-ahead, on the grounds that it may easily double the number of items to be carriedaround, but they count, for instance,

EE-->>

\Delta \Delta  FF[[++--]]@@11 as two items. All in all, since the

gain from reduction look-ahead cannot be large and its implementation cost and over-head are probably considerable, it is likely that its use should not be recommended.

The well-tuned Earley/CYK parser by Graham, Harrison and Ruzzo [CF 1980] doesnot feature reduction look-ahead.

8
Deterministic top-down methods

In Chapter 6 we discussed two general top-down methods: one using breadth-firstsearch and one using depth-first search. These methods have in common the need to
search to find derivations, and thus are not efficient. In this chapter and the next wewill concentrate on parsers that do not have to search: there will always be only one
possibility to choose from. Parsers with this property are called deterministic. Deter-ministic parsers are much faster than non-deterministic ones, but there is a penalty: the
class of grammars that the parsing method is suitable for, while depending on themethod chosen, is more restricted than that of the grammars suitable for nondeterministic parsing methods.In this chapter, we will focus our attention on deterministic top-down methods.
As has been explained in Section 3.6.5, there is only one such method, this in contrastwith the deterministic bottom-up methods, which will be discussed in the next chapter.
From Chapters 3 and 6 we know that in a top-down parser we have a prediction for therest of the input, and that this prediction has either a terminal symbol in front, in which
case we "match", or a non-terminal, in which case we "predict".It is the predict step that, until now, has caused us so much trouble. The predict
step consists of replacing a non-terminal by one of its right-hand sides, and if we haveno means to decide which right-hand side to select, we have to try them all. One restriction we could impose on the grammar, one that immediately comes to mind, is lim-iting the number of right-hand sides of each non-terminal to one. Then we would need
no search, because no selection would be needed. However, such a restriction is far toosevere, as it would leave us with only finite languages. So, limiting the number of
right-hand sides per non-terminal to one is not a solution.There are two sources of information that could help us in selecting the right
right-hand side. First of all, there is the partial derivation as it is constructed so far.However, apart from the prediction this does not give us any information about the rest
of the input. The other source of information is the rest of the input. We will see thatlooking at the next symbol or the next few symbols will, for certain grammars, tell us
which choice to take.

Ch. 8] Replacing search by table look-up 165
8.1 REPLACING SEARCH BY TABLE LOOK-UP
Grammars that make it particularly easy to at least limit the search are ones in whicheach right-hand side starts with a terminal symbol. In this case, a predict step is always

immediately followed by a match step, matching the next input symbol with the symbolstarting the right-hand side selected in the prediction. This match step can only succeed
for right-hand sides that start with this input symbol. The other right-hand sides willimmediately lead to a match step that will fail. We can use this fact to limit the number
of predictions as follows: only the right-hand sides that start with a terminal symbolthat is equal to the next input symbol will be considered. For instance, consider the
grammar of Figure 6.1, repeated in Figure 8.1, and the input sentence aaaabbbb.

SS -->> aaBB || bbAA
AA -->> aa || aaSS || bbAAAA
BB -->> bb || bbSS || aaBBBB

Figure 8.1 A grammar producing sentences with an equal number of aa's and bb's
Using the breadth-first top-down method of Chapter 6, extended with the observationdescribed above, results in the steps of Figure 8.2: (a) presents the start of the automaton; we have added the ## end-marker; only one right-hand side of SS starts with an aa, sothis is the only applicable right-hand side; this leads to (b); next, a match step leads to
((cc)); the next input symbol is again an aa, so only one right-hand side of BB is applicable,resulting in (d); (e) is the result of a match step; this time, the next input symbol is a

bb,so two right-hand sides of
BB are applicable; this leads to (f); (g) is the result of a matchstep; again, the next input symbol is a

bb, so two right-hand sides of BB are applicable;only one right-hand side of
SS is applicable; this leads to (h), and this again calls for amatch step, leading to (i); now, there are no applicable right-hand sides for

SS and AA,because there are no right-hand sides starting with a
##; thus, these predictions are deadends; this leaves a match step for the only remaining prediction, leading to (j).

We could enhance the efficiency of this method even further by precomputing theapplicable right-hand sides for each non-terminal/terminal combination, and enter these
in a table. For the grammar of Figure 8.1, this would result in the table of Figure 8.3.Such a table is called a parse table.

Despite its title, most of this chapter concerns the construction of these parsetables. Once such a parse table is obtained, the actions of the parser are obvious. The
parser does not need the grammar any more. Instead, every time a predict step is calledfor, the parser uses the next input symbol and the non-terminal at hand as indices in the
parse table. The corresponding table entry contains the right-hand sides that have to beconsidered. For instance, in Figure 8.2(e), the parser would use input symbol

bb andnon-terminal
BB to determine that it has to consider the right-hand sides BB11 and BB22. Ifthe corresponding table entry is empty, we have found an error in the input and the

input sentence cannot be derived from the grammar. Using the parse table of Figure8.3 instead of the grammar of Figure 8.1 for parsing the sentence

aaaabbbb will again leadto Figure 8.2. The advantage of using a parse table is that we do not have to check all

right-hand sides of a non-terminal any more, to see if they start with the right terminalsymbol.

Still, we have a search process, albeit a more limited one than we had before. Thesearch is now confined to the elements of the parse table entries. In fact, we now only

166 Deterministic top-down methods [Ch. 8

(a) aaaabbbb##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS##\Lambda 
\Lambda \Lambda 

(b) aaaabbbb##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS11 aaBB##\Lambda 

\Lambda \Lambda 

(c) aa aabbbb##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS11aa BB##\Lambda 

\Lambda \Lambda 

(d) aa aabbbb##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS11aaBB33 aaBBBB##\Lambda 

\Lambda \Lambda 

(e) aaaa bbbb##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS11aaBB33aa BBBB##\Lambda 

\Lambda \Lambda 

(f) aaaa bbbb##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS11aaBB33aaBB11 bbBB##
SS11aaBB33aaBB22 bbSSBB##\Lambda 

\Lambda \Lambda 

\Lambda 

(g) aaaabb bb##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS11aaBB33aaBB11bb BB##
SS11aaBB33aaBB22bb SSBB##\Lambda 

\Lambda \Lambda 

\Lambda 

(h) aaaabb bb##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS11aaBB33aaBB11bbBB11 bb##
SS11aaBB33aaBB11bbBB22 bbSS##
SS11aaBB33aaBB22bbSS22 bbAABB##\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

(i) aaaabbbb ##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS11aaBB33aaBB11bbBB11bb ##
SS11aaBB33aaBB11bbBB22bb SS##
SS11aaBB33aaBB22bbSS22bb AABB##\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

(j) aaaabbbb##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

SS11aaBB33aaBB11bbBB11bb## \Lambda 

\Lambda \Lambda 

Figure 8.2 The limited breadth-first parsing of the sentence aaaabbbb##

a b #\Gamma \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
SS SS11:: aaBB SS22:: bbAA\Gamma \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

AA AA11:: aa AA33:: bbAAAA

AA22:: aaSS\Gamma \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

BB BB33:: aaBBBB BB11:: bb

BB22:: bbSS\Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 
\Lambda 

Figure 8.3 The parse table for the grammar of Figure 8.1
need a search because of the (AA,aa) and the (BB,bb) entry of the table. These entries havemore than one element, so we need the search to determine which one results in a
derivation of the input sentence.This last observation is an important one: it immediately leads to a restriction that

Sec. 8.1] Replacing search by table look-up 167
we could impose on the grammar, to make the parsing deterministic: we could requirethat each parse table entry contain at most one element. In terms of the grammar, this
means that all right-hand sides of a non-terminal start with a different terminal symbol.A grammar that fulfills this requirement is called a simple LL(1) grammar (SLL(1)), or
an s-grammar. Here, LL(1) means that the grammar allows a deterministic parser thatoperates from Left to right, produces a Left-most derivation, using a look-ahead of one
(1) symbol.Consider for instance the grammar of Figure 8.4.

SS -->> aaBB
BB -->> bb || aaBBbb

Figure 8.4 An example SLL(1) grammar
This grammar generates all sentences starting with a number of aa's, followed by anequal number of

bb's. The grammar is clearly SLL(1). It leads to the parse table of Fig-ure 8.5.

aa bb ##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
SS SS11:: aaBB\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

BB BB22:: aaBBbb BB11:: bb\Lambda 

\Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda 

Figure 8.5 The parse table for the grammar of Figure 8.4
The parsing of the sentence aaaabbbb is presented in Figure 8.6. Again we have added the
## end-marker.

aaaabbbb##
SS##

aaaabbbb##
SS11 aaBB##

aa aabbbb##
SS11aa BB##

aa aabbbb##
SS11aaBB22 aaBBbb##

aaaa bbbb##
SS11aaBB22aa BBbb##

aaaa bbbb##
SS11aaBB22aaBB11 bbbb##

aaaabb bb##
SS11aaBB22aaBB11bb bb##

aaaabbbb ##
SS11aaBB22aaBB11bbbb ##

aaaabbbb##
SS11aaBB22aaBB11bbbb##

Figure 8.6 The SLL(1) parsing of the sentence aaaabbbb##
As expected, there is always only one prediction, so no search is needed. Thus, the pro-cess is deterministic, and therefore very efficient. The efficiency could be enhanced
even further by combining the predict step with the match step that always follows thepredict step.

So, SLL(1) grammars lead to simple and very efficient parsers. However, the res-trictions that we have placed on the grammar are severe. Not many practical grammars
are SLL(1), although many can be transformed into SLL(1) form. In the next section,

168 Deterministic top-down methods [Ch. 8
we will consider a more general class of grammars that still allows for the same kind ofparser.

8.2 LL(1) GRAMMARS
For the deterministic top-down parser described in the previous section, the crucial res-triction placed on the grammar is that all right-hand sides of a non-terminal start with a

different terminal symbol. This ensures that each parse table entry contains at most oneelement. In this section, we will drop the requirement that right-hand sides start with a
terminal symbol. We will see that we can still construct a parse table in that case. Lateron, we will see that we can even construct a parse table for grammars with e-rules.

8.2.1 LL(1) grammars without e-rulesIf a grammar has no e-rules, there are no non-terminals that derive the empty string. In
other words, each non-terminal ultimately derives strings of terminal symbols of lengthat least one, and this also holds for each right-hand side. The terminal symbols that start
these strings are the ones that we are interested in. Once we know for each right-handside which terminal symbols can start a string derived from this right-hand side, we can
construct a parse table, just as we did in the previous section. So, we have to computethis set of terminal symbols for each right-hand side.

8.2.1.1 FIRST1 setsThese sets of terminal symbols are called the FIRST

1 sets: if we have a non-empty sen-tential form x, then FIRST
1(x) is the set of terminal symbols that can start a sententialform derived from x in zero or more production steps. The subscript

1 indicates that theset contains single terminal symbols only. Later, we will see FIRST
k sets, consisting ofstrings of terminal symbols of length at most k. For now, we will drop the subscript

1:we will use FIRST instead of FIRST
1. If x starts with a terminal symbol, thenFIRST(x) is a set that has this symbol as its only member. If x starts with a nonterminal A, then FIRST(x) is equal to FIRST(A), because A cannot produce e. So, if wecan compute the FIRST set for any non-terminal A, we can compute it for any sentential form x. However, FIRST(A) depends on the right-hand sides of the A-rules: it isthe union of the FIRST sets of these right-hand sides. These FIRST sets may again
depend on the FIRST set of some non-terminal. This could even be A itself, if the ruleis directly or indirectly left-recursive. This observation suggests the iterative process
described below to compute the FIRST sets of all non-terminals:
\Gamma  We first initialize the FIRST sets to the empty set.

\Gamma  Then we process each grammar rule in the following way: if the right-hand side

starts with a terminal symbol, we add this symbol to the FIRST set of the left-hand side, since it can be the first symbol of a sentential form derived from the

left-hand side. If the right-hand side starts with a non-terminal symbol, we add allsymbols of the present FIRST set of this non-terminal to the FIRST set of the
left-hand side. These are all symbols that can be the first terminal symbol of asentential form derived from the left-hand side.
\Gamma  The previous step is repeated until no more new symbols are added to any of the

FIRST sets.Eventually, no more new symbols can be added, because the maximum number of elements in a FIRST set is the number of symbols, and the number of FIRST sets is equal

Sec. 8.2] LL(1) grammars 169
to the number of non-terminals. Therefore, the total number of times that a new sym-bol can be added to any FIRST set is limited by the product of the number of symbols
and the number of non-terminals.
8.2.1.2 Producing the parse tableWith the help of these FIRST sets, we can now construct a parse table for the grammar.
We process each grammar rule A o""a in the following way: if a starts with a terminalsymbol a, we add a to the (A,a) entry of the parse table; if a starts with a non-terminal,
we add a to the (A,a) entry of the parse table for all symbols a in FIRST(a).Now let us compute the parse table for the example grammar of Figure 8.7. This
grammar describes a simple language that could be used as the input language for arudimentary consulting system: the user enters some facts, and then asks a question.
There is also a facility for sub-sessions. The contents of the facts and questions are ofno concern here. They are represented by the word

SSTTRRIINNGG, which is regarded as a ter-minal symbol.

SSeessssiioonn -->> FFaacctt SSeessssiioonn
SSeessssiioonn -->> QQuueessttiioonn
SSeessssiioonn -->> (( SSeessssiioonn )) SSeessssiioonn

FFaacctt -->> !! SSTTRRIINNGG
QQuueessttiioonn -->> ?? SSTTRRIINNGG

Figure 8.7 An example grammar
We first compute the FIRST sets. Initially, the FIRST sets are all empty. Then,we process all grammar rules in the order of Figure 8.7. The rule

SSeessssiioonn -->> FFaacctt
SSeessssiioonn results in adding the symbols from FIRST(FFaacctt) to FIRST(SSeessssiioonn), butFIRST(

FFaacctt) is still empty. The rule SSeessssiioonn -->> QQuueessttiioonn results in adding thesymbols from FIRST(

QQuueessttiioonn) to FIRST(SSeessssiioonn), but FIRST(QQuueessttiioonn) is stillempty too. The rule
SSeessssiioonn -->> (( SSeessssiioonn )) SSeessssiioonn results in adding (( toFIRST(
SSeessssiioonn). The rule FFaacctt -->> !! SSTTRRIINNGG results in adding !! to FIRST(FFaacctt),and the rule

QQuueessttiioonn -->> ?? SSTTRRIINNGG results in adding ?? to FIRST(QQuueessttiioonn). So,after processing all right-hand sides once, we have the following:

FIRST(SSeessssiioonn) FIRST(FFaacctt) FIRST(QQuueessttiioonn)\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

(( !! ??\Lambda 

\Lambda \Lambda  \Lambda \Lambda \Lambda 

Next, we process all grammar rules again. This time, the rule SSeessssiioonn -->> FFaacctt
SSeessssiioonn will result in adding !! (from FIRST(FFaacctt)) to FIRST(SSeessssiioonn), the rule
SSeessssiioonn -->> QQuueessttiioonn will result in adding ?? to FIRST(SSeessssiioonn), and no otherchanges will take place. So now we get:

FIRST(SSeessssiioonn) FIRST(FFaacctt) FIRST(QQuueessttiioonn)\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

(( !! ?? !! ??\Lambda 

\Lambda \Lambda  \Lambda \Lambda \Lambda 

There were some changes, so we have to repeat this process again. This time, there areno changes, so the table above presents the FIRST sets of the non-terminals. Now we
have all the information we need to create the parse table. We have to add FFaacctt

170 Deterministic top-down methods [Ch. 8
SSeessssiioonn to the (SSeessssiioonn,a) entry for all terminal symbols a in FIRST(FFaacctt SSeess--
ssiioonn). The only terminal symbol in FIRST(FFaacctt SSeessssiioonn) is !!, so we add FFaacctt
SSeessssiioonn to the (SSeessssiioonn,!!) entry. Likewise, we add QQuueessttiioonn to the (SSeessssiioonn,??)entry. Next we add

(( SSeessssiioonn )) SSeessssiioonn to the (SSeessssiioonn,(() entry, !! SSTTRRIINNGG tothe (
FFaacctt,!!) entry, and ?? SSTTRRIINNGG to the (QQuueessttiioonn,??) entry. This results in the parsetable of Figure 8.8.

!! ?? (( )) SSTTRRIINNGG ##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
SSeessssiioonn FFaacctt SSeessssiioonn QQuueessttiioonn (( SSeessssiioonn )) SSeessssiioonn\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
QQuueessttiioonn ?? SSTTRRIINNGG\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
FFaacctt !! SSTTRRIINNGG\Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 

Figure 8.8 The parse table for the grammar of Figure 8.7
All parse table entries have at most one element, so the parser will be deterministic. Agrammar without e-rules is called LL(1) if all entries of the parse table, as constructed
above, have at most one element, or, in other words, if for every non-terminal A theFIRST sets of A are pairwise disjoint (no symbol occurs in more than one). We have
lost the S (simplicity) of SLL(1), but the parser is still as simple as before. Producingthe parse table has become more difficult, but we have gained a lot: many practical
grammars are LL(1), or are easily transformed into an LL(1) grammar.
8.2.2 LL(1) grammars with e-rulesNot allowing for e-rules is, however, still a major drawback. Certain language constructs are difficult, if not impossible, to describe with an LL(1) grammar without e-rules. For instance, non-terminals that describe lists of terminals or non-terminals are
difficult to express without e-rules. Of course, we could write

A o"" aA | a
for a list of a's, but this is not LL(1). Compare also the grammar of Figure 8.7 with theone of Figure 8.9. They describe the same language, but the one of Figure 8.9 is much
clearer.

SSeessssiioonn -->> FFaaccttss QQuueessttiioonn || (( SSeessssiioonn )) SSeessssiioonn

FFaaccttss -->> FFaacctt FFaaccttss || ee

FFaacctt -->> !! SSTTRRIINNGG
QQuueessttiioonn -->> ?? SSTTRRIINNGG

Figure 8.9 The grammar of Figure 8.7 rewritten
8.2.2.1 Extending the FIRST setsThe main problem with allowing e-rules is that the FIRST sets, as we have discussed
them in the previous section, are not sufficient any more. For instance, the FFaaccttssnon-terminal in the grammar of Figure 8.9 has an e-rule. The FIRST set for this righthand side is empty, so it does not tell us on which look-ahead symbols we shouldchoose this right-hand side. Also, in the presence of e-rules, the computation of the
FIRST sets itself needs some revision. For instance, if we compute the FIRST set of

Sec. 8.2] LL(1) grammars 171
the first right-hand side of SSeessssiioonn using the method of the previous section, ?? willnot be a member, but it should, because

FFaaccttss can derive e (it is transparent), and then
?? starts a sentential form that can be derived from SSeessssiioonn.Let us first extend the FIRST definition to also deal with e-rules. This time, in

addition to terminal symbols, e will also be allowed as a member of a FIRST set. Wewill now also have to deal with empty sentential forms, so we will sometimes need the
FIRST(e) set. We will define it as the set containing only the empty string e. We willalso add e to the FIRST set of a sentential form if this sentential form derives e.

These may seem minor changes, but the presence of e-rules affects the computa-tion of the FIRST sets. FIRST(u

1u 2 . . . un) is now equal to FIRST(u 1), e excluded,but extended with FIRST(u
2 . . . un) if u 1 derives e. In particular, FIRST(ue) (=FIRST(u)) is equal to FIRST(u), e excluded, but extended with FIRST(e) (= {e}) if u

derives e.Apart from this, the computation of the revised FIRST sets proceeds in exactly the
same way as before. When we need to know whether a non-terminal A derives e, wehave two options: we could compute this information separately, using the method
described in Section 4.2.1, or we could check if e is a member of the FIRST(A) set as itis computed so far. This last option uses the fact that if a non-terminal derives e, e will
ultimately be a member of its FIRST set.Now let us compute the FIRST sets for the grammar of Figure 8.9. They are first
initialized to the empty set. Then, we process each grammar rule: the rule SSeessssiioonn -->>
FFaaccttss QQuueessttiioonn results in adding the terminal symbols from FIRST(FFaaccttss) toFIRST(

SSeessssiioonn). However, FIRST(FFaaccttss) is still empty. The rule SSeessssiioonn -->> ((
SSeessssiioonn )) SSeessssiioonn results in adding (( to FIRST(SSeessssiioonn). Then, the rule FFaaccttss
-->> FFaacctt FFaaccttss results in adding the symbols from FIRST(FFaacctt) to FIRST(FFaaccttss),and the rule

FFaaccttss -->> ee results in adding e to FIRST(FFaaccttss). Then, the rule FFaacctt
-->> !! SSTTRRIINNGG results in adding !! to FIRST(FFaacctt), and the rule QQuueessttiioonn -->> ??
SSTTRRIINNGG results in adding ?? to FIRST(QQuueessttiioonn). This completes the first pass overthe grammar rules, resulting in:

FIRST(SSeessssiioonn) FIRST(FFaaccttss) FIRST(FFaacctt) FIRST(QQuueessttiioonn)\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

(( e !! ??\Lambda 

\Lambda \Lambda  \Lambda \Lambda \Lambda  \Lambda \Lambda \Lambda 

The second pass is more interesting: this time, we know that FFaaccttss derives e, andtherefore, the rule

SSeessssiioonn -->> FFaaccttss QQuueessttiioonn results in adding the symbols fromFIRST(
QQuueessttiioonn) to FIRST(SSeessssiioonn). The rule FFaaccttss -->> FFaacctt FFaaccttss results inadding
!! to FIRST(FFaaccttss). So we get:

FIRST(SSeessssiioonn) FIRST(FFaaccttss) FIRST(FFaacctt) FIRST(QQuueessttiioonn)\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

(( ?? e !! !! ??\Lambda 

\Lambda \Lambda  \Lambda \Lambda \Lambda  \Lambda \Lambda \Lambda 

In the third pass, the only change is the addition of !! to FIRST(SSeessssiioonn), because it isnow a member of FIRST(

FFaaccttss). So we have:

FIRST(SSeessssiioonn) FIRST(FFaaccttss) FIRST(FFaacctt) FIRST(QQuueessttiioonn)\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

(( ?? !! e !! !! ??\Lambda 

\Lambda \Lambda  \Lambda \Lambda \Lambda  \Lambda \Lambda \Lambda 

The fourth pass does not result in any new additions.

172 Deterministic top-down methods [Ch. 8

The question remains how to decide when an e right-hand side or, for that matter,a right-hand side that derives e is to be predicted. Suppose that we have a grammar
rule

A o"" a1 | a2 | . . . | an
and also suppose that am is or derives e. Now suppose we find A at the front of a pred-iction, as in

. . . a . . . ##\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
. . . Ax##\Theta 

\Theta \Theta 

where we again have added the ## end-marker. A breadth-first parser would have toinvestigate the following predictions:

. . . a . . . ##\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
. . . a1x##. . .

.. . . .

. . . anx##\Theta 

\Theta \Theta 

\Theta \Theta 
\Theta 

None of these predictions derive e, because of the end-marker (##). We know how tocompute the FIRST sets of these predictions. If the next input symbol is not a member
of any of these FIRST sets, either the prediction we started with (Ax##) is wrong, orthere is an error in the input sentence. Otherwise, the next input symbol is a member of
one or more of these FIRST sets, and we can strike out the predictions that do not havethe symbol in their FIRST set. If none of these FIRST sets have a symbol in common
with any of the other FIRST sets, the next input symbol can only be a member of atmost one of these FIRST sets, so at most one prediction remains, and the parser is
deterministic at this point.A context-free grammar is called LL(1) if this is always the case. In other words,
a grammar is LL(1) if for any prediction Ax##, with A a non-terminal with right-handsides a

1, ..., and an, the sets FIRST(a1x##), ..., and FIRST(anx##) are pairwise disjoint(no symbol is a member of more than one set). This definition does not conflict with

the one that we gave in the previous section for grammars without e-rules, because inthis case FIRST(a

ix##) is equal to FIRST(ai), so in this case the sets FIRST(a1), ..., andFIRST(a
n) are pairwise disjoint.

8.2.2.2 The need for FOLLOW setsSo, what do we have now? We can construct a deterministic parser for any LL(1)
grammar. This parser operates by starting with the prediction S##, and its predictionsteps consist of replacing the non-terminal at hand with each of its right-hand sides,
computing the FIRST sets of the resulting predictions, and checking whether the nextinput symbol is a member of any of these sets. We then continue with the predictions
for which this is the case. If there is more than one, the parser announces that the gram-mar is not LL(1) and stops. Although this is a deterministic parser, it is not very efficient, because it has to compute several FIRST sets at each prediction step. We cannotcompute all these FIRST sets before starting the parser, because such a FIRST set

Sec. 8.2] LL(1) grammars 173
depends on the whole prediction (of which there are infinitely many), not just on thenon-terminal. So, we still do not know if, and if so, how we can construct a parse table
for an LL(1) grammar with e-rules, nor do we have a method to determine if a gram-mar is LL(1).

Now suppose we have a prediction Ax## and a rule A o""a, and a is or derives e.The input symbols that lead to the selection of A o""a are the symbols in the set
FIRST(ax##), and this set of symbols is formed by the symbols in FIRST(a), extendedwith the symbols in FIRST(x

##) (because of the transparency of a). The selection ofA o""a on an input symbol that is not a member of FIRST(a) is called an e-move. The

set FIRST(x##) is the problem: we cannot compute it at parser generation time. Whatwe can calculate, though, is the union of all FIRST(x

##) sets such that x## can follow Ain any prediction. This is just the set of all terminal symbols that can follow A in any

sentential form derivable from S## (not just the present prediction) and is called, quitereasonably, the FOLLOW set of A, FOLLOW(A).

Now it would seem that such a gross approximation would seriously weaken theparser or even make it incorrect. This is not so. Suppose that this set contains a symbol
a that is not a member of FIRST(x##), and a is the next input symbol. If a is not amember of FIRST(A), we will predict A o""a, and we will ultimately end up with a failing match, because ax## does not derive a string starting with an a. So, the input stringwill (correctly) be rejected, although the error will be detected a bit later than before,
because the parser will make some e-moves before finding out that something is wrong.If a is a member of FIRST(A) then we may have a problem if a is a member of one of
the FIRST sets of the other right-hand sides of A. We will worry about this a bit later.The good thing about the FOLLOW set is that we can compute it at parser generation time. Each non-terminal has a FOLLOW set, and they can be computed as follows:
\Gamma  as with the computation of the FIRST sets, we start with the FOLLOW sets all

empty.
\Gamma  Next we process all right-hand sides, including the S## one. Whenever a right-hand

side contains a non-terminal, as in A o"" . . . By, we add all symbols from FIRST(y)to FOLLOW(B); these symbols can follow a B. In addition, if y derives e, we add

all symbols from FOLLOW(A) to FOLLOW(B).
\Gamma  The previous step is repeated until no more new symbols can be added to any of

the FOLLOW sets.Now let us go back to our example and compute the FOLLOW sets. Starting with

SSeessssiioonn ##, ## is added to FOLLOW(SSeessssiioonn). Next, the symbols ofFIRST(

QQuueessttiioonn) are added to FOLLOW(FFaaccttss), because of the rule SSeessssiioonn -->>
FFaaccttss QQuueessttiioonn. This rule also results in adding all symbols ofFOLLOW(

SSeessssiioonn) to FOLLOW(QQuueessttiioonn). The rule SSeessssiioonn -->> (( SSeessssiioonn
)) SSeessssiioonn results in adding the )) symbol to FOLLOW(SSeessssiioonn) and the addition ofall symbols of FOLLOW(

SSeessssiioonn) to FOLLOW(SSeessssiioonn), which does not addmuch. The next rule is the rule

FFaaccttss -->> FFaacctt FFaaccttss. All symbols fromFIRST(
FFaaccttss) are added to FOLLOW(FFaacctt), and all symbols fromFOLLOW(

FFaaccttss) are added to FOLLOW(FFaaccttss). The other rules do not result inany additions. So, after the first pass we have:

FOLLOW(SSeessssiioonn) FOLLOW(FFaaccttss) FOLLOW(FFaacctt) FOLLOW(QQuueessttiioonn)\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

)) ## ?? !! ##\Lambda 

\Lambda \Lambda  \Lambda \Lambda \Lambda  \Lambda \Lambda \Lambda 

174 Deterministic top-down methods [Ch. 8
In the second pass, )) is added to FOLLOW(QQuueessttiioonn), because it is now a member ofFOLLOW(

SSeessssiioonn), and all members of FOLLOW(SSeessssiioonn) become a member ofFOLLOW(
QQuueessttiioonn) because of the rule SSeessssiioonn -->> FFaaccttss QQuueessttiioonn. Noother changes take place. The resulting FOLLOW sets are presented below:

FOLLOW(SSeessssiioonn) FOLLOW(FFaaccttss) FOLLOW(FFaacctt) FOLLOW(QQuueessttiioonn)\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 

)) ## ?? !! ## ))\Lambda 

\Lambda \Lambda  \Lambda \Lambda \Lambda  \Lambda \Lambda \Lambda 

8.2.2.3 Using the FOLLOW sets to produce a parse tableOnce we know the FOLLOW set for each non-terminal that derives e, we can once
again construct a parse table: first, we compute the FIRST set of each non-terminal.This also tells us which non-terminals derive e. Next, we compute the FOLLOW set of
each non-terminal. Then, starting with an empty parse table, we process each grammarrule A o""a as follows: we add a to the (A,a) entry of the parse table for all terminal
symbols a in FIRST(a), as we did before. This time however, we also add a to the(A,a) entry of the parse table for all terminal symbols a in FOLLOW(A) when a is or
derives e (when FIRST(a) contains e). A shorter way of saying this is that we add a tothe (A,a) entry of the parse table for all terminal symbols a in FIRST(a FOLLOW(A)).
This last set consists of the union of the FIRST sets of the sentential forms ab for allsymbols b in FOLLOW(A).

Now let us produce a parse table for our example. The SSeessssiioonn -->> FFaaccttss
QQuueessttiioonn rule does not derive e, because QQuueessttiioonn does not. Therefore, only the ter-minal symbols in FIRST(

FFaaccttss QQuueessttiioonn) lead to addition of this rule to the table.These symbols are
!! and ?? (because FFaaccttss also derives e). Similarly, all other rulesare added, resulting in the parse table presented in Figure 8.10.

(( )) ## !! ?? SSTTRRIINNGG\Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
SSeessssiioonn (( SSeessssiioonn )) SSeessssiioonn FFaaccttss QQuueessttiioonn FFaaccttss QQuueessttiioonn\Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
FFaaccttss FFaacctt FFaaccttss ee\Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
FFaacctt !! SSTTRRIINNGG\Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
QQuueessttiioonn ?? SSTTRRIINNGG\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

Figure 8.10 The parse table for the grammar of Figure 8.9
8.2.3 LL(1) versus strong-LL(1)If all entries of the resulting parse table have at most one element, the parser is again
deterministic. In this case, the grammar is called strong-LL(1) and the parser is calleda strong-LL(1) parser. In the literature, strong-LL(1) is referred to as "strong LL(1)"
(note that there is a space between the words "strong" and "LL"). However, we findthis term a bit misleading because it suggests that the class of strong-LL(1) grammars
is more powerful than the class of LL(1) grammars, but this is not the case. Everystrong-LL(1) grammar is LL(1).

It is perhaps more surprising that every LL(1) grammar is strong-LL(1). In otherwords, every grammar that is not strong-LL(1) is not LL(1), and this is demonstrated
with the following argument: if a grammar is not strong-LL(1), there is a parse tableentry, say (A,a), with at least two elements, say a and b. This means that a is a
member of both FIRST(a FOLLOW(A)) and FIRST(b FOLLOW(A)). Now, there arethree possibilities:

Sec. 8.2] LL(1) grammars 175
\Gamma  a is a member of both FIRST(a) and FIRST(b). In this case, the grammar cannot

be LL(1), because for any prediction Ax##, a is a member of both FIRST(ax#) andFIRST(bx#).

\Gamma  a is a member of either FIRST(a) or FIRST(b), but not both. Let us say that a is a

member of FIRST(a). In this case, a still is a member of FIRST(b FOLLOW(A))so there is a prediction Ax

##, such that a is a member of FIRST(bx##). However, ais also a member of FIRST(ax

##), so the grammar is not LL(1). In other words, inthis case there is a prediction in which an LL(1) parser cannot decide which

right-hand side to choose either.
\Gamma  a is neither a member of FIRST(a), nor a member of FIRST(b). In this case a

and b must derive e and a must be a member of FOLLOW(A). This means thatthere is a prediction Ax

## such that a is a member of FIRST(x##) and thus a is amember of both FIRST(ax

##) and FIRST(bx##), so the grammar is not LL(1). Thismeans that in an LL(1) grammar at most one right-hand side of any non-terminal

derives e.
8.2.4 Full LL(1) parsingWe already mentioned briefly that an important difference between LL(1) parsing and
strong-LL(1) parsing is that the strong-LL(1) parser sometimes makes e-moves beforedetecting an error. Consider for instance the following grammar:

SSSS -->> aa AA bb || bb AA aa

AA -->> cc SS || ee

The strong-LL(1) parse table of this grammar is:

aa bb cc ##\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
SS aa AA bb bb AA aa\Gamma  \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
AA ee ee cc SS\Lambda 

\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

Now, on input sentence aaaaccaabbbb, the strong-LL(1) parser makes the following moves:

aaaaccaabbbb##
SS##

aaaaccaabbbb##
SS11 aaAAbb##

aa aaccaabbbb##
SS11aa AAbb##

aa aaccaabbbb##
SS11aaAA22 bb##

The problem here is that the prediction is destroyed by the time the error is detected. Incontrast, an LL(1) parser would not do the last step, because neither FIRST(

bb##), norFIRST(
ccSSbb##) contain aa, so the LL(1) parser would detect the error before choosing aright-hand side for

AA. A full LL(1) parser has the immediate error detection property,which means that an error is detected as soon as the erroneous symbol is first examined, whereas a strong-LL(1) parser only has the correct-prefix property, which meansthat the parser detects an error as soon as an attempt is made to match (or shift) the
erroneous symbol. In Chapter 10, we will see that the immediate error detection

176 Deterministic top-down methods [Ch. 8
property will help improve error recovery.Given a prediction A . . .

##, a full LL(1) parser bases its parsing decisions onFIRST(A . . .
##) rather than on the approximation FIRST(A FOLLOW(A)); this avoidsany parsing decisions on erroneous input symbols (which can never occur in

FIRST(A . . . ##) but may occur in FIRST(A FOLLOW(A))). So, if we have predictionA . . .

## and input symbol a, we first have to determine if a is a member ofFIRST(A . . .

##), before consulting the parse table to choose a right-hand side for A.The penalty for this is in efficiency: every time that parse table has to be consulted, a

FIRST set has to be computed and a check made that the input symbol is a member.Fortunately, we can do better than this. A first step to improvement is the following: suppose that we maintain between all symbols in the prediction a set of terminalsymbols that are correct at this point, like this:

X Y Z ##

A`A'A^A~

Here, A` is the set of symbols that are legal at this point; this is just the FIRST set of theremaining part of the prediction: FIRST(

##); likewise, A' is FIRST(Z##), A^ isFIRST(YZ
##), and A~ is FIRST(XYZ##) (none of these sets contain e). These sets caneasily be computed, from right to left. For instance,

A^ consists of the symbols inFIRST(Y), with the symbols from
A' added if Y derives e (if e is a member ofFIRST(Y)). When a non-terminal is replaced by one of its right-hand sides, the set

behind this right-hand side is available, and we can use this to compute the sets withinthis right-hand side and in front of it.

Now let us see how this works for our example. As the reader can easily verify,

FIRST(SS) = { aa, bb}, andFIRST(

AA) = { cc, e}.

The parser starts with the prediction SS##. We have to find a starting point for the sets: itmakes sense to start with an empty one to the right of the

##, because no symbols arecorrect after the
##. So, the parser starts in the following state:

aaaaccaabbbb##

SS ##
aa,bb ##

The first input symbol is a member of the current FIRST set, so it is correct. The (SS, aa)entry of the parse table contains

aaAAbb so we get parser state

aaaaccaabbbb##
SS11 aa AA bb ##

? ? ? ##

Computing the sets marked with a question mark from right to left results in the

Sec. 8.2] LL(1) grammars 177
following parser state:

aaaaccaabbbb##
SS11 aa AA bb ##

aa bb,cc bb ##

Note that bb now is a member of the set in front of AA, but aa is not, although it is amember of FOLLOW(

AA). After the match step, the parser is in the following state:

aa aaccaabbbb##
SS11aa AA bb ##

bb,cc bb ##

The next input symbol is not a member of the current FIRST set, so an error isdetected, and no right-hand side of

AA is chosen. Instead, the prediction is left intact, soerror recovery can profit from it.

It is not clear that all this is more efficient than computing the FIRST set of aprediction to determine the correctness of an input symbol before choosing a right-hand
side. However, it does suggest that we can do this at parser generation time, by com-bining non-terminals with the FIRST sets that can follow it in a prediction. For our
example, we always start with non-terminal SS and the set {##}. We will indicate thiswith the pair [

SS,{##}]. Starting with this pair, we will try to make rules for the behaviourof each pair that turns up, for each valid look-ahead. We know from the FIRST sets of

the alternatives for SS that on look-ahead symbol aa, [SS,{##}] results in right-hand side
aaAAbb. Now the only symbol that can follow AA here is a bb. So in fact, we have:

on look-ahead symbol aa, [SS,{##}] results in right-hand side aa [AA,{bb}] bb.
Similarly we find:

on look-ahead symbol bb, [SS,{##}] results in right-hand side bb [AA,{aa}] aa.
We have now obtained pairs for AA followed by a bb, and AA followed by an aa. So we haveto make rules for them: We know that on look-ahead symbol

cc, [AA,{bb}] results inright-hand side
ccSS. Because AA can only be followed by a bb in this context, the sameholds for this
SS. This gives:

on look-ahead symbol cc, [AA,{bb}] results in right-hand side cc [SS,{bb}].
Likewise, we get the following rules:

on look-ahead symbol bb, [AA,{bb}] results in right-hand side e;on look-ahead symbol

cc, [AA,{aa}] results in right-hand side cc [SS,{aa}];on look-ahead symbol
aa, [AA,{aa}] results in right-hand side e.

Now we have to make rules for the pairs SS followed by an aa, and SS followed by a bb:

178 Deterministic top-down methods [Ch. 8

on look-ahead symbol aa, [SS,{aa}] results in right-hand side aa [AA,{bb}] bb;on look-ahead symbol

bb, [SS,{aa}] results in right-hand side bb [AA,{aa}] aa;on look-ahead symbol
aa, [SS,{bb}] results in right-hand side aa [AA,{bb}] bb;on look-ahead symbol
bb, [SS,{bb}] results in right-hand side bb [AA,{aa}] aa.

In fact, we find that we have rewritten the grammar, using the [non-terminal,followed-by set] pairs as non-terminals, into the following form:

[SS,{##}] -->> aa [AA,{bb}] bb | bb [AA,{aa}] aa[

SS,{aa}] -->> aa [AA,{bb}] bb | bb [AA,{aa}] aa[
SS,{bb}] -->> aa [AA,{bb}] bb | bb [AA,{aa}] aa[
AA,{aa}] -->> cc [SS,{aa}] | e[
AA,{bb}] -->> cc [SS,{bb}] | e

For this grammar, the following parse table can be produced:

aa bb cc ##\Gamma \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Gamma \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
[SS,{##}] aa [AA,{bb}] bb bb [AA,{aa}] aa\Gamma \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
[SS,{aa}] aa [AA,{bb}] bb bb [AA,{aa}] aa\Gamma \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
[SS,{bb}] aa [AA,{bb}] bb bb [AA,{aa}] aa\Gamma \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
[AA,{aa}] e cc [SS,{aa}]\Gamma \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma \Theta \Gamma 
[AA,{bb}] e cc [SS,{bb}]\Lambda 

\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 
\Lambda 

\Lambda \Lambda 
\Lambda \Lambda 

\Lambda \Lambda 
\Lambda \Lambda 
\Lambda 

The entries for the different [SS,...] rules are identical so we can merge them. Afterthat, the only change with respect to the original parse table is the duplication of the

AA-rule: now there is one copy for each context in which
AA has a different set behind it in aprediction.

Now, after accepting the first aa of aaaaccaabbbb, the prediction is [AA,{bb}]bb##; since theparse table entry ([

AA,{bb}], aa) is empty, parsing will stop here and now.The resulting parser is exactly the same as the strong-LL(1) one. Only the parse

table is different. Often, the LL(1) table is much larger than the strong-LL(1) one. Asthe benefit of having an LL(1) parser only lies in that it detects some errors a bit earlier,
this usually is not considered worth the extra cost, and thus most parsers that are adver-tised as LL(1) parsers are actually strong-LL(1) parsers.

8.2.5 Solving LL(1) conflictsIf a parse table entry has more than one element, we have what we call an LL(1) conflict. In this section, we will discuss how to deal with them. One way to deal with con-flicts is one that we have seen before: use a depth-first or a breadth-first parser with a
one symbol look-ahead. This, however, has several disadvantages: the resulting parseris not deterministic any more, it is less efficient (often to such an extent that it becomes
unacceptable), and it still does not work for left-recursive grammars. Therefore, wehave to try and eliminate these conflicts, so we can use an ordinary LL(1) parser.

8.2.5.1 Eliminating left-recursionThe first step to take is the elimination of left-recursion. Left-recursive grammars
always lead to LL(1) conflicts, because the right-hand side causing the left-recursionhas a FIRST set that contains all symbols from the FIRST set of the non-terminal.

Sec. 8.2] LL(1) grammars 179
Therefore, it also contains all terminal symbols of the FIRST sets of the other right-hand sides of the non-terminal. Eliminating left-recursion has already been discussed
in Section 6.4.
8.2.5.2 Left-factoringAnother technique for removing LL(1) conflicts is left-factoring. Left-factoring of
grammar rules is like factoring arithmetic expressions:

a * b + a * c = a * (b + c ).
The grammatical equivalent to this is a rule

A o"" xy | xz,
which clearly has an LL(1) conflict on the terminal symbols in FIRST(x). We replacethis grammar rule with the two rules

A o"" xNN o"" y | z
where N is a new non-terminal. There have been some attempts to automate this pro-cess; see Foster [Transform 1968] and Rosenkrantz and Hunt [Transform 1987].
8.2.5.3 Conflict resolversSometimes, these techniques do not help much. We could for instance deal with a
language for which no LL(1) grammar exists. In fact, many languages can be describedby a context-free grammar, but not by an LL(1) grammar. Another method of handling
conflicts is to resolve them by so-called disambiguating rules. An example of such adisambiguating rule is: "on a conflict, the textually first one of the conflicting righthand sides is chosen". With this disambiguating rule, the order of the right-hand sideswithin a grammar rule becomes crucial, and unexpected results may occur if the
grammar-processing program does not clearly indicate where conflicts occur and howthey are resolved.

A better method is to have the grammar writer specify explicitly how each con-flict must be resolved, using so-called conflict resolvers. One option is to resolve conflicts at parser generation time. Parser generators that allow for this kind of conflictresolver usually have a mechanism that enables the user to indicate (at parser generation time) which right-hand side must be chosen on a conflict. Another, much moreflexible method is to have conflicts resolved at parse time. When the parser meets a
conflict, it calls a user-specified conflict resolver. Such a user-specified conflictresolver has the complete left-context at its disposal, so it could base its choice on this
left-context. It is also possible to have the parser look further ahead in the input, andthen resolve the conflict based on the symbols found. See Milton, Kirchhoff and Rowland [LL 1979] and Grune and Jacobs [LL 1988], for similar approaches using attributegrammars.

180 Deterministic top-down methods [Ch. 8
8.2.6 LL(1) and recursive descentMost hand-written parsers are LL(1) parsers. They usually are written in the form of a
non-backtracking recursive-descent parser (see Section 6.6). In fact, this is a very sim-ple way to implement a strong-LL(1) parser. For a non-terminal A with grammar rule

A o"" a1 | . . . | an
the parsing routine has the following structure:

procedure A;if lookahead I^ FIRST(a

1 FOLLOW(A)) thencode for a
1 ...else if lookahead I^ FIRST(a

2 FOLLOW(A)) thencode for a
2 ....

.else if lookahead I^ FIRST(a

n FOLLOW(A)) thencode for a
n ...else ERROR;

end A;
The look-ahead symbol always resides in a variable called "lookahead". The procedureERROR announces an error and stops the parser.

The code for a right-hand side consists of the code for the symbols of the right-hand side. A non-terminal symbol results in a call to the parsing routine for this nonterminal, and a terminal symbol results in a call to a MATCH routine with this symbolas parameter. This MATCH routine has the following structure:

procedure MATCH(sym);if lookahead = sym then

lookahead := NEXTSYMelse ERROR;
end MATCH;
The NEXTSYM procedure reads the next symbol from the input.Several LL(1) parser generators produce a recursive descent parser instead of a
parse table that is to be interpreted by a grammar-independent parser. The advantagesof generating a recursive descent parser are numerous:
\Gamma  Semantic actions are easily embedded in the parsing routines.
\Gamma  A parameter mechanism or attribute mechanism comes virtually for free: the

parser generator can use the parameter mechanism of the implementationlanguage.

\Gamma  Non-backtracking recursive descent parsers are quite efficient, often more efficient than the table-driven ones.
\Gamma  Dynamic conflict resolvers are implemented easily.

The most important disadvantage of generating a recursive descent parser is thesize of the parser. A recursive descent parser is usually larger than a table-driven one

(including the table). However, this becomes less of a problem as computer memories

Sec. 8.2] LL(1) grammars 181
get bigger and bigger. See Waite and Carter [Misc 1985] for measurements of table-driven parsers versus recursive descent parsers.

8.3 LL(k) GRAMMARS
Up until now, we have limited the look-ahead to just one symbol, and one mightwonder if having a look-ahead of k symbols instead of one makes the method more

powerful. It does, so let us define LL(k) grammars. For this, we need a definition ofFIRST

k sets: if x is a sentential form, then FIRSTk(x) is the set of terminal strings wsuch that |w| (the length of w) is less than k and x o""* w, or |w| is equal to k, and x o""* wy,

for some sentential form y. For k = 1 this definition coincides with the definition of theFIRST sets as we have seen it before.

We now have the instruments needed to define LL(k): a grammar is LL(k) if forany prediction Ax

##, with A a non-terminal with right-hand sides a1, ..., and an, the setsFIRST

k(a1x##), are pairwise disjoint. Obviously, for any k, the set of LL(k) grammarsis a subset of the set of LL(k +1) grammars, and in fact, for any k there are LL(k +1)

grammars that are not LL(k). A trivial example of this is given in Figure 8.11.

SSss -->> aakbb || aakaa
Figure 8.11 An LL(k +1) grammar that is not LL(k)
Less obvious is that for any k there are languages that are LL(k +1), but not LL(k). Anexample of such a language is given in Figure 8.12.

SSSS -->> aaSSAA || ee
AA -->> aakbbSS || cc

Figure 8.12 A grammar defining an LL(k +1) language that is not LL(k)
See Kurki-Suonio [LL 1969] for more details.With LL(k) grammars we have the same problem as with the LL(1) grammars:
producing a parse table is difficult. In the LL(1) case, we solved this problem with theaid of the FOLLOW sets, obtaining strong-LL(1) parsers. We can try the same with
LL(k) grammars using FOLLOWk sets. For any non-terminal A, FOLLOWk(A) is nowdefined as the union of the sets FIRST

k(x#### . . . ####), for any prediction Ax#### . . . #### (inLL(k) parsing, we add k end-markers instead of just one).

Once we have the FIRSTk sets and the FOLLOWk sets, we can produce a parsetable for the grammar. Like the LL(1) parse table, this parse table will be indexed with

pairs consisting of a non-terminal and a terminal string of length equal to k. Everygrammar rule A o""a is processed as follows: a is added to the (A, w) entry of the table
for every w in FIRSTk(a FOLLOWk(A)) (as we have seen before, this last set denotesthe union of several FIRST

k sets: it is the union of all FIRSTk(av) sets with v an ele-ment of FOLLOW
k(A)). All this is just an extension to k look-ahead symbols of whatwe did earlier with one look-ahead symbol.

If this results in a parse table where all entries have at most one element, thegrammar is strong-LL(k). Unlike the LL(1) case however, for k > 1 there are grammars

that are LL(k), but not strong-LL(k). An example of such a grammar is given in Figure

182 Deterministic top-down methods [Ch. 8
8.13.

SSSS -->> aaAAaaaa || bbAAbbaa

AA -->> bb || ee

Figure 8.13 An LL(2) grammar that is not strong-LL(2)
This raises an interesting question, one that has kept the authors busy for quite awhile: how come? Why is it different for k = 1? If we try to repeat our proof from Section 8.2.3 for a look-ahead k > 1, we see that we fail at the very last step: let us examinea strong-LL(k) conflict: suppose that the right-hand sides a and b both end up in the (A,
w) entry of the parse table. This means that w is a member of both FIRSTk(aFOLLOW

k(A)) and FIRSTk(b FOLLOWk(A)). Now, there are three cases:\Gamma  w is a member both FIRST

k(a) and FIRSTk(b). In this case, the grammar cannotbe LL(k), because for any prediction Ax#### . . . ####, w is a member of both

FIRSTk(ax#### . . . ####) and FIRSTk(bx#### . . . ####).
\Gamma  w is a member of either FIRSTk(a) or FIRSTk(b), but not both. Let us say that w

is a member of FIRSTk(a). In this case, w still is a member of FIRSTk(bFOLLOW

k(A)) so there is a prediction Ax#### . . . ####, such that w is a member ofFIRST
k(bx#### . . . ####). However, w is also a member of FIRSTk(ax#### . . . ####), sothe grammar is not LL(k). In other words, in this case there is a prediction in

which an LL(k) parser cannot decide which right-hand side to choose either.
\Gamma  w is neither a member of FIRSTk(a) nor a member of FIRSTk(b). Here, we have

to deviate from the reasoning we used in the LL(1) case. As w is an element ofFIRST

k(a FOLLOWk(A)), w can now be split into two parts w 1.1 and w 1.2, suchthat w
1.1 is an element of FIRSTk(a) and w 1.2 is a non-empty start of an elementof FOLLOW

k(A). Likewise, w can be split into two parts w 2.1 and w 2.2 such thatw
2.1 is an element of FIRSTk(b) and w 2.2 is a non-empty start of an element ofFOLLOW

k(A). So, we have the following situation:

w
w 1.1 w 1.2

w 2.1 w 2.2
Now, if w 1.1=w 2.1, w 1.1 is a member of FIRSTk(a), as well as FIRSTk(b), andthere is a prediction Ax

#### . . . #### such that x#### . . . #### o""* w 1.2 . . . . So,FIRST

k(ax#### . . . ####) contains w and so does FIRSTk(bx#### . . . ####), and there-fore, the grammar is not LL(k). So the only case left is that w

1.1z'w 2.1. Neitherw

1.2 nor w 2.2 are e, and this is just impossible if | w | =1.Strong-LL(k) parsers with k > 1 are seldom used in practice, because the parse

tables are huge, and there are not many languages that are LL(k) for some k > 1, but notLL(1). Even the languages that are LL(k) for some k > 1, but not LL(1), are usually for
the most part LL(1), and can be parsed using an LL(1) parser with conflict resolvers atthe places where the grammar is not LL(1).

To obtain a full LL(k) parser, the method that we used to obtain a full LL(1)parser can be extended to deal with pairs [A, L], where L is a FIRST

k set of. . . #### . . . #### in some prediction A . . . #### . . . ####. This extension is straightforward

and will not be discussed further.

Sec. 8.3] Extended LL(1) grammars 183
8.4 EXTENDED LL(1) GRAMMARS
Several parser generators accept an extended context-free grammar instead of an ordi-nary one. See for instance Lewi et al.[LL 1978], Heckmann [LL 1986], Grune and

Jacobs[LL 1988]. Extended context-free grammars have been discussed in Chapter 2.To check that an extended context-free grammar is LL(1), we have to transform the
extended context-free grammar into an ordinary one, in a way that will avoid introduc-ing LL(1) conflicts. For instance, the transformation for

SSoommeetthhiinngg+ given in Chapter2:

SSoommeetthhiinngg+ -->> SSoommeetthhiinngg | SSoommeetthhiinngg SSoommeetthhiinngg+
will not do, because it will result in an LL(1) conflict on the symbols inFIRST(

SSoommeetthhiinngg). Instead, we will use the following transformations:

SSoommeetthhiinngg* -->> e | SSoommeetthhiinngg SSoommeetthhiinngg*
SSoommeetthhiinngg+ -->> SSoommeetthhiinngg SSoommeetthhiinngg*

SSoommeetthhiinngg? -->> e | SSoommeetthhiinngg

If the resulting grammar is LL(1), the original extended context-free grammar wasELL(1) (Extended LL(1)). This is the recursive interpretation of Chapter 2. Parser
generation usually proceeds as follows: first transform the grammar to an ordinarycontext-free grammar, and then produce a parse table for it.

Extended LL(1) grammars allow for a more efficient implementation in recursivedescent parsers. In this case,

SSoommeetthhiinngg? can be implemented as an if statement:

if lookahead I^ FIRST(SSoommeetthhiinngg) thencode for

SSoommeetthhiinngg ...else if lookahead I" FOLLOW(

SSoommeetthhiinngg?) thenERROR;

SSoommeetthhiinngg* can be implemented as a while loop:

while lookahead I^ FIRST(SSoommeetthhiinngg) docode for

SSoommeetthhiinngg ...if lookahead I" FOLLOW(

SSoommeetthhiinngg*) thenERROR;

and SSoommeetthhiinngg+ can be implemented as a repeat loop:

repeatif lookahead I" FIRST(

SSoommeetthhiinngg) thenERROR;

code for SSoommeetthhiinngg ...until lookahead I^ FOLLOW(

SSoommeetthhiinngg+);

Here, procedure calls are replaced by much more efficient repetitive constructs.

9
Deterministic bottom-up parsing

There is a great variety of deterministic bottom-up parsing methods. The first deter-ministic parsers (Wolpe [Precedence 1958], Wegstein [Precedence 1959]) were
bottom-up parsers and interest has only increased since. The annotated bibliography inthis book contains about 140 entries on deterministic bottom-up parsing against some
30 on deterministic top-down parsing. These figures may not reflect the relativeimportance of the methods, they are certainly indicative of the fascination and complexity of the subject of this chapter.There are two families of deterministic bottom-up parsers, those that are purely
bottom-up and those that have an additional top-down component. The first familycomprises the precedence and bounded-context techniques, which are treated in Sections 9.1 to 9.3; the second family, which is both more powerful and more complicated,contains the LR techniques and is treated in Sections 9.4 to 9.7. Tomita's parser in
Section 9.8 is not purely deterministic but leans so heavily on the LR techniques that itstreatment in this chapter is warranted. The chapter closes with a short section on noncanonical bottom-up parsing and one on the use of LR(k) as an ambiguity test.The proper setting for the subject at hand can best be obtained by summarizing a
number of relevant facts from previous chapters.
\Gamma  A right-most production expands the right-most non-terminal in a sentential form,

by replacing it by one of its right-hand sides. A sentence is produced by repeatedright-most production until no non-terminal remains. See Figure 9.1 (a), where the

sentential forms are right-aligned to show how the production process creeps tothe left, where it terminates. The grammar used is that of Figure 7.8.
\Gamma  Each step of a bottom-up parser, working on a sentential form, identifies the latest

right-most production in it and undoes it by reducing a segment of the input to thenon-terminal it derived from. The identified segment is called the handle. Since

the parser starts with the final sentential form of the production process (that is,the input) it finds its first reduction rather to the left end, which is convenient. A
bottom-up parser identifies right-most productions in reverse order. See Figure9.1(b) where the handles are aligned.
\Gamma  To obtain an efficient parser we have to have an efficient method to identify handles, without elaborate searching. The identified handle has to be correct (or theinput is in error); we do not want to consider alternative choices for handles.

Although this chapter is called "Deterministic Bottom-Up Parsing", it is almost

Ch. 9] Deterministic bottom-up parsing 185

SS
EE
EEQQFF
EEQQ aa
EE ++aa
EEQQFF ++aa
EEQQ aa++aa
EE --aa++aa
FF --aa++aa

aa--aa++aa

(a)

aa--aa++aa
FF --aa++aa
EE --aa++aa
EEQQ aa++aa
EEQQFF ++aa

EE ++aa
EEQQ aa
EEQQFF

EE
SS

(b)
Figure 9.1 Right-most production (a) and right-most reduction (b)
exclusively concerned with methods for finding handles. Once the handle is found,parsing is (almost always) trivial. The exceptions will be treated separately.

Unlike top-down parsing, which identifies productions right at their beginning,that is, before any of its constituents have been identified, bottom-up parsing identifies
a production only at its very end, when all its constituents have already been identified.A top-down parser allows semantic actions to be performed at the beginning of a production and these actions can help in determining the semantics of the constituents. In abottom-up parser, semantic actions are only performed during a reduction, which
occurs at the end of a production, and the semantics of the constituents have to bedetermined without the benefit of knowing in which production they occur. We see that
the increased power of bottom-up parsing comes at a price: since the decision whatproduction applies is postponed to the latest possible moment, that decision can be
based upon the fullest possible information, but it also means that the actions thatdepend on this decision come very late.

9.1 SIMPLE HANDLE-ISOLATING TECHNIQUES
There is a situation in (more or less) daily life in which the (more or less) averagecitizen is called upon to identify a handle. If one sees a formula like

44 ++ 55 ** 66 ++ 88
one immediately identifies the handle and evaluates it:

44 ++ 55 ** 66 ++ 88----------
44 ++ 3300 ++ 88
The next handle is

44 ++ 3300 ++ 88----------------

3344 ++ 88

186 Deterministic bottom-up parsing [Ch. 9
and then

3344 ++ 88--------------------

4422
If we look closely, we can discern in this process shifts and reduces. The persondoing the arithmetic shifts symbols until he reaches the situation

44 ++ 55 ** 66 ++ 88
in which the control mechanism in his head tells him that this is the right moment to doa reduce. If asked why, he might answer something like: "Ah, well, I was taught in
school that multiplication comes before addition". Before we can formalize this notionand turn it into a parsing method, we consider an even simpler case below (Section
9.1.1).

SSSS -->> ## EE ##

EE -->> EE ++ TT
EE -->> TT
TT -->> TT ** FF
TT -->> FF
FF -->> nn
FF -->> (( EE ))

Figure 9.2 A grammar for simple arithmetic expressions
Meanwhile we note that formulas like the one above are called "arithmetic expres-sions" and are produced by the grammar of Figure 9.2.

SS is the start symbol, EE standsfor "expression",
TT for "term", FF for "factor" and nn for any number. The last causes noproblems, since the exact value of the number is immaterial to the parsing process. We

have demarcated the beginning and the end of the expression with ## marks; the blankspace that normally surrounds a formula is not good enough for automatic processing.
This also simplifies the stop criterion: the parser accepts the input as correct and stopswhen the terminating

## is shifted, or upon the subsequent reduce.

9.1.1 Fully parenthesized expressions

SSSS -->> ## EE ##

EE -->> (( EE ++ TT ))
EE -->> TT
TT -->> (( TT ** FF ))
TT -->> FF
FF -->> nn
FF -->> (( EE ))

Figure 9.3 A grammar for fully parenthesized arithmetic expressions

Sec. 9.1] Simple handle-isolating techniques 187
An arithmetic expression is fully parenthesized if each operator together with itsoperands has parentheses around it. Such expressions are generated by the grammar of
Figure 9.3. Our example expression would have the form

## (( (( 44 ++ (( 55 ** 66 )) )) ++ 88 )) ##

Now finding the handle is trivial: go to the first closing parenthesis and then back to thenearest opening parenthesis. The segment between and including the parentheses is the
handle. Reduce it and repeat the process as often as required. Note that after the reduc-tion there is no need to start all over again, looking for the first closing parenthesis:
there cannot be any closing parenthesis on the left of the reduction spot, so we can startsearching right where we are. In the above example we find the next right parenthesis
immediately and do the next reduction:

## (( (( 44 ++ 3300 )) ++ 88 )) ##

9.2 PRECEDENCE PARSING
Of course, grammars are not normally kind enough to have begin- and end-markers toeach compound right-hand side, and the above parsing method has little practical value

(as far as we know it does not even have a name). Yet, suppose we had a method forinserting the proper parentheses into an expression that was lacking them. At a first
glance this seems trivial to do: when we see ++nn** we know we can replace this by ++((nn**and we can replace **

nn++ by **nn))++. There is a slight problem with ++nn++, but since the first
++ has to be performed first, we replace this by ++nn))++. The ##'s are easy; we can replace
##nn by ##((nn and nn## by nn))##. For our example we get:

## (( 44 ++ (( 55 ** 66 )) ++ 88 )) ##
This is, however, not completely correct - it should have been ##((((44++((55**66))))++88))## -and for

44++55**66 we get the obviously incorrect form ##((44++((55**66))##.The problem is that we do not know how many parentheses to insert in, for

instance, ++nn**; in 44++55**66**77 we should replace it by ++((((nn**: ##((44++((((55**66))**77))))##. Wesolve this problem by inserting parentheses generators rather than parentheses. A generator for open parentheses is traditionally written as <*, one for closing parentheses as>* ; we shall also use a "non-parenthesis", =. . These symbols look confusingly like <, >
and =, to which they are only remotely related. Now, our tentatively insertedparentheses become firmly inserted parentheses generators; see Figure 9.4 in which we
have left out the nn since its position can be inferred from the pattern. Still, the table inFigure 9.4 is incomplete: the pattern ** ** is missing and so are all patterns involving
parentheses. In principle there should be a pattern for each combination of two opera-tors (where we count the genuine parentheses as operators too), and only the generator
to be inserted is relevant for each combination. This generator is called the precedencerelation between the two operators. It is convenient to collect all combinations of
operators in a table, the precedence table. The precedence table for the grammar of

188 Deterministic bottom-up parsing [Ch. 9

++ ** o"" ++ <* ****

++ o"" ** >* ++
++ ++ o"" ++ >* ++
## ...... o"" ## <* ......
...... ## o"" ...... >* ##

Figure 9.4 Preliminary table of precedence relations
Figure 9.2 is given in Figure 9.5; the left-most column contains the left-hand symbolsand the top-most row the right-hand symbols.

## ++ ** (( ))\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
## =. <* <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
++ >* >* <* <* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

** >* >* >* <* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
(( <* <* <* =.\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
)) >* >* >* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 

\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

Figure 9.5 Operator-precedence table to the grammar of Figure 9.2
There are three remarks to be made about this precedence table. First, we haveadded a number of <* and >* tokens not covered above (for instance, **>* **). Second, there
is ##==..## and ((==..)) (but not ))==..(( !); we shall shortly see what they mean. And third, thereare three empty entries, meaning that when we find these combinations in the input, it
contains an error (and is not produced by the grammar for which we made our pre-cedence table).

Such a table is called a precedence table because for symbols that are normallyregarded as operators it gives their relative precedence. An entry like

++<*** indicates thatin the combination
++**, the ** has a higher precedence than the ++. We shall first showhow the precedence table is used in parsing and then how such a precedence table can

be constructed systematically for a given grammar, if the grammar allows it.The stack in an operator-precedence parser differs from the normal bottom-up
parser stack in that it contains "important" symbols, the operators, between which rela-tions are defined, and "unimportant" symbols, the numbers, which are only consulted to
determine the value of a handle and which do not influence the parsing. Moreover, weneed places on the stack to insert the parentheses generators (one can, in principle, do
without these, by reevaluating them whenever necessary). Since there is a parenthesesgenerator between each pair of operators and there is also (almost) always a value
between such a pair, we shall indicate both in the same position on the stack, with theparentheses generator in line and the value below it; see Figure 9.6.

To show that, contrary to what is sometimes thought, operator-precedence can domore than just calculate a value (and since we have seen too often now that
44++55**66++88==4422), we shall have the parser construct the parse tree rather than the value.The stack starts with a

##. Values and operators are shifted onto it, interspersed withparentheses generators, until a >* generator is met; the following operator is not shifted

and is left in the input (Figure 9.6(b)). It is now easy to identify the handle, which is

Sec. 9.2] Precedence parsing 189

Stack rest of input

## 44 ++ 55 ** 66 ++ 88 ##(a)

## <* ++ <* ** >* ++ 88 ##

44 55 66. . . . . . . . . . . . . . ....

...

...................
...
.

(b)

## <* ++ >* ++ 88 ##

44 **

55 66
. . . . . . . . . . . . . . ....

...
...................
...
.

(c)

## <* ++ >* ##

++

44 **

55 66

88. . . . . . . . . . . . . . ....

...

...................
...
.

(d)

## ==.. ##

++

++
44 **

55 66

88
(e)

Figure 9.6 Operator-precedence parsing of 44++55**66++88
demarcated by a dotted rectangle in the figure and which is reduced to a tree; see (c), inwhich also the next >* has already appeared between the

++ on the stack and the ++ in theinput. Note that the tree and the new generator have come in the position of the <* of the

handle. A further reduction brings us to (d) in which the ++ and the 88 have already beenshifted, and then to the final state of the operator-precedence parser, in which the stack
holds ##==..## and the parse tree dangles from the value position.We see that the stack only holds <* markers and values, plus a >* on the top each
time a handle is found. The meaning of the =. becomes clear when we parse an inputtext which includes parentheses, like

44**((55++66)); see Figure 9.7, in which we have theparser calculate the value rather than the parse tree. We see that the =. is used to allow

handles consisting of more than one operator and two operands; the handle in (c) hastwo operators, the

(( and the )) and one operand, the 1111. Note that as already indicatedin Section 5.1.1, the set of stack configurations can be described by a regular expression; for this type of parsers the expression is:

## || ##<*qq (([[<*==..]]qq))** >* ?? || ##==..##
where qq is any operator; the first alternative is the start situation and the third alterna-tive is the end situation.

190 Deterministic bottom-up parsing [Ch. 9

Stack rest of input

## 44 ** (( 55 ++ 66 )) ##(a)

## <* ** <* (( <* ++ >* )) ##

44 55 66. . . . . . . . . . . . . . ....

...

...................
...
.

(b)

## <* ** <* (( ==.. )) >* ##

44 1111. . . . . . . . . . . . . . . . . . . . . . . . ....

...

.............................
...
.

(c)

## <* ** >* ##

44 1111. . . . . . . . . . . . . . ....

...

...................
...
.

(d)

## ==.. ##

4444(e)

Figure 9.7 An operator-precedence parsing involving ==..
9.2.1 Constructing the operator-precedence tableThe above hinges on the difference between operators, which are terminal symbols and
between which precedence relations are defined, and operands, which are non-terminals. This distinction is captured in the following definition of an operator grammar:
\Gamma  A CF grammar is an operator grammar if (and only if) each right-hand side contains at least one terminal or non-terminal and no right-hand side contains twoconsecutive non-terminals.

So each pair of non-terminals is separated by at least one terminal; all the terminalsexcept those carrying values (

nn in our case) are called operators.For such grammars, setting up the precedence table is relatively easy. First we calculate for each non-terminal A the set FIRSTOP(A), which is the set of all operators thatcan occur as the first operator in any sentential form deriving from A, and LAST

OP(A),which is defined similarly. Note that this first operator in a sentential form can be preceded by at most one non-terminal in an operator grammar. The FIRSTOP's of allnon-terminals are constructed simultaneously as follows:

1. For each non-terminal A, find all right-hand sides of all rules for A; now for eachright-hand side R we insert the first terminal in R (if any) into FIRST

OP(A). Thisgives us the initial values of all FIRST
OP's.2. For each non-terminal A, find all right-hand sides of all rules for A; now for each

right-hand side R that starts with a non-terminal, say B, we add the elements ofFIRST

OP(B) to FIRSTOP(A). This is reasonable, since a sentential form of A maystart with B, so all terminals in FIRST

OP(B) should also be in FIRSTOP(A).3. Repeat step 2 above until no FIRST
OP changes any more.We have now found the FIRST
OP of all non-terminals. A similar algorithm, using thelast terminal in R in step 1 and a B which ends A in step 2 provides the LAST

OP's. Thesets for the grammar of Figure 9.2 are shown in Figure 9.8.

Now we can fill the precedence table using the following rules, in which q, q 1 andq
2 are operators and A is a non-terminal.
\Gamma  For each occurrence in a right-hand side of the form q 1q 2 or q 1Aq 2, set q 1=. q 2.

Sec. 9.2] Precedence parsing 191

FIRSTOP(SS) = {##} LASTOP(SS) = {##}FIRST

OP(EE) = {++, **, ((} LASTOP(EE) = {++, **, ))}FIRST
OP(TT) = {**, ((} LASTOP(TT) = {**, ))}FIRST
OP(FF) = {((} LASTOP(FF) = {))}

Figure 9.8 FIRSTOP and LASTOP sets for the grammar of Figure 9.2

This keeps operators from the same handle together.
\Gamma  For each occurrence q 1A, set q 1<*q 2 for each q 2 in FIRSTOP(A). This demarcates

the left end of a handle.
\Gamma  For each occurrence Aq 1, set q 2>* q 1 for each q 2 in LASTOP(A). This demarcates

the right end of a handle.If we obtain a table without conflicts this way, that is, if we never find two different relations between two operators, then we call the grammar operator-precedence.It will now be clear why

((==..)) and not ))==..((, and why ++>* ++ (because EE++ occurs in EE-->>EE++TTand
++ is in LASTOP(EE)).In this way, the table can be derived from the grammar by a program and be

passed on to the operator-precedence parser. A very efficient linear-time parser results.There is, however, one small problem we have glossed over: Although the method
properly identifies the handle, it often does not identify the non-terminal to which toreduce it. Also, it does not show any unit rule reductions; nowhere in the examples did
we see reductions of the form EE-->>FF or TT-->>FF. In short, operator-precedence parsinggenerates only skeleton parse trees.

Operator-precedence parsers are very easy to construct (often even by hand) andvery efficient to use; operator-precedence is the method of choice for all parsing problems that are simple enough to allow it. That only a skeleton parse tree is obtained, isoften not an obstacle, since operator grammars often have the property that the semantics is attached to the operators rather than to the right-hand sides; the operators areidentified correctly.

It is surprising how many grammars are (almost) operator-precedence. Almost allformula-like computer input is operator-precedence. Also, large parts of the grammars
of many computer languages are operator-precedence. An example is a constructionlike

CCOONNSSTT ttoottaall == hheeaadd ++ ttaaiill;; from a Pascal-like language, which is easilyrendered as:

Stack rest of input
## <* CCOONNSSTT <* == <* ++ >*

ttoottaall hheeaadd ttaaiill

;; ##

Ignoring the non-terminals has other bad consequences besides producing a skele-ton parse tree. Since non-terminals are ignored, a missing non-terminal is not noticed.
As a result, the parser will accept incorrect input without warning and will produce anincomplete parse tree for it. A parser using the table of Figure 9.5 will blithely accept
the empty string, since it immediately leads to the stack configuration ##==..##. It producesa parse tree consisting of one empty node.

The theoretical analysis of this phenomenon turns out to be inordinately difficult;see Levy [Precedence 1975], Williams [Precedence 1977, 1979, 1981] and many others

192 Deterministic bottom-up parsing [Ch. 9
in Section 13.8. In practice it is less of a problem than one would expect; it is easy tocheck for the presence of required non-terminals, either while the parse tree is being
constructed or afterwards.
9.2.2 Precedence functionsSeveral objections can be raised against operator-precedence. First, it cannot handle all
grammars that can be handled by other more sophisticated methods. Second, its errordetection capabilities are weak. Third, it constructs skeleton parse trees only. And
fourth, the two-dimensional precedence table, which for say a 100 tokens has 10000entries, may take too much room. The latter objection can be overcome for those precedence tables that can be represented by so-called precedence functions. The idea isthe following. Rather than having a table T such that for any two operators q

1 and q 2,T[q
1,q 2] yields the relation between q 1 and q 2, we have two integer functions f and gsuch that f

q 1 <gq 2 means that q 1<*q 2, fq 1 =gq 2 means q 1=. q 2 and fq 1 >gq 2 meansq

1>* q 2. fq is called the left priority of q, gq the right priority; they would probably bebetter indicated by l and r, but the use of f and g is traditional. Note that we write f

q 1rather than f (q

1); this allows us to write, for instance, f ( for the left priority of (( ratherthan the confusing f (((). It will be clear that two functions are required: with just one

function one cannot express, for instance, ++>* ++. Precedence functions take much lessroom than precedence tables. For our 100 tokens we need 200 function values rather
than 10000 tables entries. Not all tables allow a representation with precedence func-tions, but many do.

Finding the proper f and g for a given table seems simple enough and can indeedoften be done by hand. The fact, however, that there are two functions rather than one,
the size of the tables and the occurrence of the =. complicate things. A well-knownalgorithm to construct the functions was given by Bell [Precedence 1969] of which
several variants exist. The following technique is a straightforward and easily imple-mented variant of Bell's algorithm.
First we turn the precedence table into a list of numerical relations, as follows:
\Gamma  for each q 1<*q 2 we have fq 1 <gq 2 ,

\Gamma  for each q 1=. q 2 we have fq 1 =gq 2 ,
\Gamma  for each q 1>* q 2 we have fq 1 >gq 2 ,

Here we no longer view forms like fq as function values but rather as variables; rein-terpretation as function values will occur later. Making such a list is easier done by

computer than by hand; see Figure 9.9(a). Next we remove all equals-relations, as fol-lows:
\Gamma  for each relation fq 1 =gq 2 we create a new variable fq 1 gq 2 and replace all

occurrences of fq 1 and gq 2 by fq 1 gq 2 .
Note that fq 1 gq 2 is not the product of fq 1 and gq 2 but rather a new variable, i.e., the
name of a new priority value. Now a relation like fq 1 =gq 2 has turned into

fq 1 gq 2 =fq 1 gq 2 and can be deleted trivially. See (b).
Third we flip all > relations:
\Gamma  we replace each relation p 1>p 2 by p 2<p 1, where p 1 and p 2 are priority variables. See (c).The list has now assumed a very uniform appearance and we can start to assign numerical values to the variables. We shall do this by handing out the numbers 0,1, . . . as fol-lows:

Sec. 9.2] Precedence parsing 193

f # = g # f #g # < g + f #g # < g + f #g # = 0 f #g # = 0f

# < g + f #g # < g * f #g # < g * f ( g ) = 0 f ( g ) = 0f
# < g * f #g # < g ( f #g # < g ( g + = 1f
# < g ( f + > f #g # f #g # < f + g + < f + f + = 2f
+ > g # f + > g + g + < f + f + < g *f
+ > g + f + < g * f + < g * f + < g ( g * < f *f
+ < g * f + < g ( f + < g ( g + < f * f * < g (f
+ < g ( f + > f ( g ) f ( g ) < f + g * < f * g * < f )f
+ > g ) f * > f #g # f #g # < f * f * < g (f
* > g # f * > g + g + < f * g + < f ) (f)f
* > g + f * > g * g * < f * g * < f )f
* > g * f * < g ( f * < g ( f #g # = 0f
* < g ( f * > f ( g ) f ( g ) < f * (d) f ( g ) = 0f
* > g ) f ( g ) < g + f ( g ) < g + g + = 1f
( < g + f ( g ) < g * f ( g ) < g * f #g # = 0 f + = 2f
( < g * f ( g ) < g ( f ( g ) < g ( f ( g ) = 0 g * = 3f
( < g ( f ) > f #g # f #g # < f ) g + = 1f
( = g ) f ) > g + g + < f ) f * < g (f
) > g # f ) > g * g * < f ) f + < g *f
) > g + f ) > f ( g ) f ( g ) < f ) f + < g ( (g)f
) > g * g * < f *f
) > g ) (b) (c) f * < g (g

* < f )(a)

(e)f
#g # = 0 f # = 0 f # = 0f
( g ) = 0 g # = 0 f ( = 0g

+ = 1 f ( = 0 f + = 2f
+ = 2 g ) = 0 f * = 4g

* = 3 g + = 1 f ) = 5f
* = 4 f + = 2g

* = 3 g # = 0(h) f
* = 4 g ) = 0g

+ = 1(i) g
* = 3g
( = 5

(j)
Figure 9.9 Calculating precedence functions
\Gamma  Find all variables that occur only on the left of a relation; since they are clearly

smaller than all the others, they can all be given the value 0.In our example we find f

#g# and f ( g ) , which both get the value 0. Since the relationsthat have these two variables on their left will be satisfied provided we hand out no

more 0's, we can remove them (see (d)):
\Gamma  Remove all relations that have the identified variables on their left sides.

194 Deterministic bottom-up parsing [Ch. 9
This removal causes another set of variables to occur on the left of a relation only, towhich we now hand out the value 1. We repeat this process with increasing values until
the list of relations has become empty; see (e) through (h).
\Gamma  Decompose the compound variables and give each component the numerical value

of the compound variable. This decomposes, for instance, f ( g ) =0 into f ( =0 andg

) =0; see (i).This leaves without a value those variables that occurred on the right-hand side only in

the comparisons under (c):
\Gamma  To all still unassigned priority values, assign the lowest value that has not yet been

handed out.f
) and g ( both get the value 5 (see (j) where the values have also been reordered) andindeed these occur at the high side of a comparison only. It is easily verified that the

priority values found satisfy the initial comparisons as derived from the precedencetable.

It is possible that we reach a stage in which there are still relations left but thereare no variables that occur on the left only. It is easy to see that in that case there must
be a circularity of the form p 1<p 2<p 3 . . . <p 1 and that no integer functions represent-ing these relations can exist: the table does not allow precedence functions.

## )) ++ ** ((\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
## =. <* <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
(( =. <* <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
++ >* >* >* <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

** >* >* >* >* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
)) >* >* >* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 

\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

Figure 9.10 The precedence table of Figure 9.5 reordered
Note that finding precedence functions is equivalent to reordering the rows andcolumns of the precedence table so that the latter can be divided into three regions: a >*
region on the lower left, a <* region on the upper right and a =. border between them.See Figure 9.10.

There is always a way to represent a precedence table with more than two func-tions; see Bertsch [Precedence 1977] on how to construct such functions.

9.2.3 Simple-precedence parsingThe fact that operator-precedence parsing produces skeleton parse trees only is a serious obstacle to its application outside formula handling. The defect seems easy toremedy. When a handle is identified in an operator-precedence parser, it is reduced to a
node containing the value(s) and the operator(s), without reference to the grammar. Forserious parsing the matching right-hand side of the pertinent rule has to be found. Now
suppose we require all right-hand sides in the grammar to be different. Then, given ahandle, we can easily find the rule to be used in the reduction (or to find that there is no
matching right-hand side, in which case there was an error in the input).This is, however, not quite good enough. To properly do the right reductions and
to find reductions of the form A o""B (unit reductions), the non-terminals themselveshave to play a role in the identification of the right-hand side. They have to be on the

Sec. 9.2] Precedence parsing 195
stack like any other symbol and precedence relations have to be found for them. Thishas the additional advantage that the grammar need no longer be an operator grammar
and that the stack entries have a normal appearance again.A grammar is simple precedence if (and only if):
\Gamma  it has a conflict-free precedence table over all its symbols, terminals and nonterminals alike,
\Gamma  none of its right-hand sides is e,

\Gamma  all of its right-hand sides are different.

The construction of the simple-precedence table is again based upon two sets,FIRST

ALL(A) and LASTALL(A). FIRSTALL(A) is similar to the set FIRST(A) intro-duced in Section 8.2.2.1 and differs from it in that it also contains all non-terminals that

can start a sentential form derived from A (whereas FIRST(A) contains terminals only).LAST

ALL(A) contains all terminals and non-terminals that can end a sentential form ofA. Their construction is similar to that given in Section 8.2.2.1 for the FIRST set. Figure 9.11 shows the pertinent sets for our grammar.

FIRSTALL(SS) = {##} LASTALL(SS) = {##}FIRST

ALL(EE) = {EE, TT, FF, nn, ((} LASTALL(EE) = {TT, FF, nn, ))}FIRST
ALL(TT) = {TT, FF, nn, ((} LASTALL(TT) = {FF, nn, ))}FIRST
ALL(FF) = {nn, ((} LASTALL(FF) = {nn, ))}

Figure 9.11 FIRSTALL and LASTALL for the grammar of Figure 9.2

A simple-precedence table is now constructed as follows: For each two juxta-posed symbols X and Y in a right-hand side we have:
\Gamma  X =. Y; this keeps X and Y together in the handle;
\Gamma  if X is a non-terminal: for each symbol s in LASTALL(X) and each terminal t in

FIRST(Y) (or Y itself if Y is a terminal) we have s>* t; this allows X to be reducedcompletely when the first sign of Y appears in the input; note that we have

FIRST(Y) here rather than FIRSTALL(Y);
\Gamma  if Y is a non-terminal: for each symbol s in FIRSTALL(Y) we have X <*s; this protects X while Y is being recognized.

# E T F n + * ( )\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
# <*/=. <* <* <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
E =. =. =.\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
T >* >* =. >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

F >* >* >* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

n >* >* >* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
+ <*/=. <* <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

* =. <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

( <*/=. <* <* <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
) >* >* >* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 

\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

Figure 9.12 Simple-precedence table to Figure 9.2, with conflicts

196 Deterministic bottom-up parsing [Ch. 9

Simple precedence is not the answer to all our problems as is evident from Figure9.12 which displays the results of an attempt to construct the precedence table for the
operator-precedence grammar of Figure 9.2. Not even this simple grammar is simple-precedence, witness the conflicts for

##<*//==..EE, ((<*//==..EE and ++<*//==..TT.

SSSS -->> ## EE'' ##
EE'' -->> EE

EE -->> EE ++ TT''
EE -->> TT''
TT'' -->> TT

TT -->> TT ** FF
TT -->> FF
FF -->> nn
FF -->> (( EE ))

FIRSTALL(EE'') = {EE, TT'', TT, FF, nn, ((} LASTALL(EE'') = {TT'', TT, FF, nn, ))}FIRST

ALL(EE) = {EE, TT'', TT, FF, nn, ((} LASTALL(EE) = {TT, FF, nn, ))}FIRST
ALL(TT'') = {TT, FF, nn, ((} LASTALL(TT'') = {FF, nn, ))}FIRST
ALL(TT) = {TT, FF, nn, ((} LASTALL(TT) = {FF, nn, ))}FIRST
ALL(FF) = {nn, ((} LASTALL(FF) = {nn, ))}

## EE'' EE TT'' TT FF nn ++ ** (( ))\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
## =. <* <* <* <* <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
EE'' =.\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

EE >* =. =.\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
TT'' >* >* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

TT >* >* =. >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
FF >* >* >* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
nn >* >* >* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
++ =. <* <* <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

** =. <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
(( =. <* <* <* <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
)) >* >* >* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 

\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

Figure 9.13 A modified grammar with its simple-precedence table, without conflicts

There are two ways to remedy this. We can adapt the grammar by inserting extralevels around the troublesome non-terminals. This is done in Figure 9.13 and works in
this case; it brings us, however, farther away from our goal, to produce a correct parsetree, since we now produce a parse tree for a different grammar. Or we can adapt the
parsing method, as explained in the next section.
9.2.4 Weak-precedence parsingIt turns out that most of the simple-precedence conflicts are <*/=. conflicts. Now the
difference between <* and =. is in a sense less important than that between either of themand >* . Both <* and =. result in a shift and only >* asks for a reduce. Only when a reduce

Sec. 9.2] Precedence parsing 197
is found will the difference between <* and =. become significant for finding the head ofthe handle. Now suppose we drop the difference between <* and =. and combine them
into <=*; then we need a different means of identifying the handle and the proper right-hand side. This can be done by requiring not only that all right-hand sides be different,
but also that no right-hand side be equal to the tail of another right-hand side. A gram-mar that conforms to this and has a conflict-free <=*/>* precedence table is called weak
precedence. Figure 9.14 gives the (conflict-free) weak-precedence table for the gram-mar of Figure 9.2. It is of course possible to retain the difference between <* and =.
where it exists; this will improve the error detection capability of the parser.

# E T F n + * ( )\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
# <=* <* <* <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
E =. =. =.\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
T >* >* =. >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

F >* >* >* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

n >* >* >* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
+ <=* <* <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

* =. <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

( <=* <* <* <* <*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
) >* >* >* >*\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 

\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

Figure 9.14 Weak-precedence table to the grammar of Figure 9.2
The rule that no right-hand side should be equal to the tail of another right-handside is more restrictive than is necessary. More lenient rules exist in several variants,
which, however, all require more work in identifying the reduction rule. See, forinstance, Ichbiah and Morse [Precedence 1970] or Sekimoto [Precedence 1972].

Weak precedence is a useful method that applies to a relatively large group ofgrammars. Especially if parsing is used to roughly structure an input stream, as in the
first pass or scan of a complicated system, weak precedence can be of service.
9.2.5 Extended precedence and mixed-strategy precedenceThe above methods determine the precedence relations by looking at 1 symbol on the
stack and 1 token in the input. Once this has been said, the idea suggests itself toreplace the 1's by m and n respectively, and to determine the precedence relations from
the topmost m symbols on the stack and the first n tokens in the input. This is called(m,n)-extended precedence.

We can use the same technique to find the left end of the handle on the stack whenusing weak precedence: use k symbols on the left and l on the right to answer the question if this is the head of the handle. This is called (k,l)(m,n)-extended [weak] pre-cedence.

By increasing its parameters, extended precedence can be made reasonablypowerful. Yet the huge tables required (2 * 300 * 300 * 300 = 54 million entries for
(1,2)(2,1) extended precedence with 300 symbols) severely limit its applicability.Moreover, even with large values of k, l, m and n it is inferior still to LR(1), which we
treat in Section 9.5.

198 Deterministic bottom-up parsing [Ch. 9

If a grammar is (k,l)(m,n)-extended precedence, it is not always necessary to testthe full k, l, m and n symbols. Indeed it is almost never necessary and large parts of the
grammar can almost always be handled by (normal) weak-precedence methods; the full(k,l)(m,n)-extended precedence power is needed only in a small number of spots in the
grammar. This phenomenon has led to techniques in which the (normal) weak-precedence table has a (small) number of exception entries that refer to further, more
powerful tables. This technique is called mixed-strategy precedence. Mixed-strategyprecedence has been investigated by McKeeman [Books 1970].

9.2.6 Actually finding the correct right-hand sideAll the above methods identify only the bounds of the handle; the actual right-hand side
is still to be determined. It may seem that a search through all right-hand sides is neces-sary for each reduction, but this is not so. The right-hand sides can be arranged in a
tree structure with their right-most symbols forming the root of the tree, as in Figure9.15. When we have found a >* relation, we start walking down the stack looking for a <*
and at the same time we follow the corresponding path through the tree; when we findthe <* we should be at the beginning of a rule in the tree, or we have found an error in
the input; see Figure 9.15. The tree can be constructed by sorting the grammar rules ontheir symbols in backward order and combining equal tails. As an example, the path
followed for <* TT ==.. ** ==.. FF >* has been indicated by a dotted line.

SS -->> ## EE ##
FF -->> (( EE ))

FF -->> nn
TT -->> FF
TT -->> TT ** FF

EE -->> TT
EE -->> EE ++ TT

##EE##SS
))EE((FF
nnFF

FFTT**TTTT

TTEE++EEEE

..................
.....................

Figure 9.15 Tree structure for efficiently finding right-hand sides
For several methods to improve upon this, see the literature (Section 13.8).

9.3 BOUNDED-CONTEXT PARSING
There is a different way to solve the annoying problem of the identification of theright-hand side: let the identity of the rule be part of the precedence relation. A grammar is (m,n) bounded-context (BC(m,n)) if (and only if) for each combination of msymbols on the stack and n tokens in the input there is a unique parsing decision which
is either "shift" (<=*) or "reduce using rule X" (>* X ), as obtained by a variant of the rulesfor extended precedence. Figure 9.16 gives the BC(2,1) tables for the grammar of Figure 9.2. Note that the rows correspond to stack symbol pairs; the entry Accept meansthat the input has been parsed and Error means that a syntax error has been found.
Blank entries will never be accessed; all-blank rows have been left out. See, forinstance, Loeckx [Precedence 1970] for the construction of such tables.

Bounded-context (especially BC(2,1)) was once very popular but has been

Sec. 9.3] Bounded-context parsing 199

## ++ ** nn (( ))\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
##SS Accept
##EE >* SS-->>EE <=* Error
##TT >* EE-->>TT >* EE-->>TT <=* Error
##FF >* TT-->>FF >* TT-->>FF >* TT-->>FF Error
##nn >* FF-->>nn >* FF-->>nn >* FF-->>nn Error Error Error
##(( Error Error Error <=* <=* Error
EE++ Error Error Error <=* <=* Error
EE)) >* FF-->>((EE)) >* FF-->>((EE)) >* FF-->>((EE)) Error Error >* FF-->>((EE))
TT** Error Error Error <=* <=* Error
++TT >* EE-->>EE++TT >* EE-->>EE++TT <=* >* EE-->>EE++TT
++FF >* TT-->>FF >* TT-->>FF >* TT-->>FF >* TT-->>FF
++nn >* FF-->>nn >* FF-->>nn >* FF-->>nn Error Error >* FF-->>nn
++(( Error Error Error <=* <=* Error**

FF >* TT-->>TT**FF >* TT-->>TT**FF >* TT-->>TT**FF >* TT-->>TT**FF**
nn >* FF-->>nn >* FF-->>nn >* FF-->>nn Error Error >* FF-->>nn**
(( Error Error Error <=* <=* Error
((EE Error <=* <=*
((TT Error >* EE-->>TT <=* >* EE-->>TT
((FF Error >* TT-->>FF >* TT-->>FF >* TT-->>FF
((nn Error >* FF-->>nn >* FF-->>nn Error Error >* FF-->>nn
(((( Error Error Error <=* <=* Error\Theta 

\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

Figure 9.16 BC(2,1) tables for the grammar of Figure 9.2
superseded almost completely by LALR(1) (Section 9.6). Recently, interest inbounded-context grammars has been revived, since it has turned out that such grammars have some excellent error recovery properties; see Section 10.8. This is not com-pletely surprising if we consider that bounded-context grammars have the property that
a small number of symbols in the sentential form suffice to determine completely whatis going on.

9.3.1 Floyd productionsBounded-context parsing steps can be summarized conveniently by using Floyd productions. Floyd productions are rules for rewriting a string that contains a marker, \Delta ,on which the rules focus. A Floyd production has the form a\Delta b => g\Delta d and means
that if the marker in the string is preceded by a and is followed by b, the constructionmust be replaced by g\Delta d. The rules are tried in order starting from the top and the first
one to match is applied; processing then resumes on the resulting string, starting fromthe top of the list, and the process is repeated until no rule matches.

Although Floyd productions were not primarily designed as a parsing tool butrather as a general string manipulation language, the identification of the \Delta  in the string
with the gap in a bottom-up parser suggests itself and was already made in Floyd's ori-ginal article [Misc 1961]. Floyd productions for the grammar of Figure 9.2 are given in
Figure 9.17. The parser is started with the \Delta  at the left of the input.The apparent convenience and conciseness of Floyd productions makes it very
tempting to write parsers in them by hand, but Floyd productions are very sensitive tothe order in which the rules are listed and a small inaccuracy in the order can have a

200 Deterministic bottom-up parsing [Ch. 9

\Delta \Delta  nn ==>> nn \Delta \Delta \Delta \Delta 

(( ==>> (( \Delta \Delta 
nn \Delta \Delta  ==>> FF \Delta \Delta 
TT \Delta \Delta  ** ==>> TT** \Delta \Delta 
TT**FF \Delta \Delta  ==>> TT \Delta \Delta 
FF \Delta \Delta  ==>> TT \Delta \Delta 
EE++TT \Delta \Delta  ==>> EE \Delta \Delta 
TT \Delta \Delta  ==>> EE \Delta \Delta 
((EE)) \Delta \Delta  ==>> FF \Delta \Delta \Delta \Delta 

++ ==>> ++ \Delta \Delta \Delta \Delta 
)) ==>> )) \Delta \Delta \Delta \Delta 
## ==>> ## \Delta \Delta 
##EE## \Delta \Delta  ==>> SS \Delta \Delta 

Figure 9.17 Floyd productions for the grammar of Figure 9.2
devastating effect.

9.4 LR METHODS
The LR methods are based on the combination of two ideas that have already beentouched upon in previous sections. To reiterate, the problem is to find the handle in a

sentential form as efficiently as possible, for as large a class of grammars as possible.Such a handle is searched for from left to right. Now, from Section 5.3.4 we recall that
a very efficient way to find a string in a left-to-right search is by constructing a finite-state automaton. Just doing this is, however, not good enough. It is quite easy to construct an FS automaton that would recognize any of the right-hand sides in the grammarefficiently, but it would just find the left-most reducible substring in the sentential
form. This substring is, however, often not the handle.The idea can be made practical by applying the same trick that was used in
Earley's parser to drastically reduce the fan-out of the breadth-first search (see Section7.2): start the automaton with the start rule of the grammar and only consider, in any
position, right-hand sides that could be derived from the start symbol. This top-downrestriction device served in the Earley parser to reduce the cost to O (n 3), here we
require the grammar to be such that it reduces the cost to O (n). The resulting automa-ton is started in its initial state at the left end of the sentential form and allowed to run
to the right; it has the property that it stops at the right end of the handle and that itsaccepting state tells us how to reduce the handle. How this is done will be explained in
the next section.Since practical FS automata easily get so big that their states cannot be displayed
on a single page of a book, we shall use the grammar of Figure 9.18 for our examples.It is a simplified version of that of Figure 9.2, in which only one binary operator is left,
for which we have chosen the -- rather than the ++. Although this is not essential, itserves to remind us that the proper parse tree must be derived, since

((aa--bb))--cc is not thesame as
aa--((bb--cc)) (whereas ((aa++bb))++cc and aa++((bb++cc)) are). The ## indicates the end of theinput.

Sec. 9.4] LR methods 201

SSSS -->> EE ##

EE -->> EE -- TT
EE -->> TT
TT -->> nn
TT -->> (( EE ))

Figure 9.18 A very simple grammar for differences of numbers
9.4.1 LR(0)We shall now set out to construct a top-down-restricted handle-recognizing FS automaton for the grammar of Figure 9.18, and start by constructing a non-deterministic ver-sion. We recall that a non-deterministic automaton can be drawn as a set of states connected by arrows (transitions), each marked with one symbol or with e. Each state willcontain one item. Like in the Earley parser, an item consists of a grammar rule with a
dot

\Delta  embedded in its right-hand side. An item X o"" . . . Y \Delta  Z . . . in a state means that

the (non-deterministic!) automaton bets on X o"" . . . YZ . . . being the handle and that ithas already recognized . . . Y. Unlike the Earley parser there are no back-pointers. To

simplify the explanation of the transitions involved, we introduce a second kind ofstate, which we call a station. It has only e arrows incoming and outgoing, contains
something of the form

\Delta  X and is drawn in a rectangle rather than in an ellipse. When

the automaton is in such a station at some point in the sentential form, it thinks that atthis point a handle starts that reduces to X. Consequently each

\Delta  X station has etransitions to items for all rules for X, each with the dot at the left end, since no part ofthe rule has yet been recognized; see Figure 9.19. Equally reasonably, each state holding an item X o"" . . .

\Delta  Z . . . has an e-transition to the station \Delta  Z, since the bet on an X

may be over-optimistic and the automaton may have to settle for a Z. The third and lastsource of arrows in the non-deterministic automaton is straightforward. From each state

containing X o"" . . .

\Delta  P . . . there is a P-transition to the state containing

X o"" . . . P

\Delta  . . . , for P a terminal or a non-terminal. This corresponds to the move the

automaton makes when it really meets a P. Note that the sentential form may containnon-terminals, so transitions on non-terminals should also be defined.

With this knowledge we refer to Figure 9.19. The stations for SS, EE and TT aredrawn at the top of the picture, to show how they lead to all possible items for

SS, EE and
TT, respectively. From each station, e-arrows fan out to all states containing items withthe dot at the left, one for each rule for the non-terminal in that station; from each such

state, non-e-arrows lead down to further states. Now the picture is almost complete. Allthat needs to be done is to scan the items for a dot followed by a non-terminal (readily
discernable from the outgoing arrow marked with it) and to connect each such item tothe corresponding station through an e-arrow. This completes the picture.

There are two things to be noted on this picture. First, for each grammar rule witha right-hand side of length l there are l +1 items and they are easily found in the picture.
Moreover, for a grammar with r different non-terminals, there are r stations. So thenumber of states is roughly proportional to the size of the grammar, which assures us
that the automaton will have a modest number of states. For the average grammar of ahundred rules something like 300 states is usual. The second is that all states have outgoing arrows except the ones which contain an item with the dot at the right end.These are accepting states of the automaton and indicate that a handle has been found;
the item in the state tells us how to reduce the handle.

202 Deterministic bottom-up parsing [Ch. 9

\Gamma \Gamma  SS
SS-->>

\Gamma \Gamma  EE##

EE
SS-->>EE

\Gamma \Gamma  ##

##
SS-->>EE##

\Gamma \Gamma 

ee

\Gamma \Gamma  EE

EE-->>

\Gamma \Gamma  EE--TT

EE
EE-->>EE

\Gamma \Gamma  --TT

--
EE-->>EE--

\Gamma \Gamma  TT

TT
EE-->>EE--TT

\Gamma \Gamma 

ee

EE-->>

\Gamma \Gamma  TT

TT
EE-->>TT

\Gamma \Gamma 

ee

\Gamma \Gamma  TT

TT-->>

\Gamma \Gamma  nn

nn
TT-->>nn

\Gamma \Gamma 

ee

TT-->>

\Gamma \Gamma  ((EE))

((
TT-->>((

\Gamma \Gamma  EE))

EE
TT-->>((EE

\Gamma \Gamma  ))

))
TT-->>((EE))

\Gamma \Gamma 

eeee ee ee
ee

ee

Figure 9.19 A non-deterministic handle recognizer for the grammar of Figure 9.18

We shall now run this NDA on the sentential form EE--nn--nn, to see how it works. Asin the FS case we can do so if we are willing to go through the trouble of resolving the
non-determinism on the fly. The automaton starts at the station

\Delta \Delta  SS and can immediately

make e-moves to SS-->>

\Delta \Delta  EE##, \Delta \Delta  EE, EE-->>\Delta \Delta  EE--TT, EE-->>\Delta \Delta  TT, \Delta \Delta  TT, TT-->>\Delta \Delta  nn and TT-->>\Delta \Delta  ((EE)). Moving over

the EE reduces the set of states to SS-->>EE

\Delta \Delta  ## and EE-->>EE\Delta \Delta  --TT; moving over the next -- brings

us at EE-->>EE--

\Delta \Delta  TT from which e-moves lead to \Delta \Delta  TT, TT-->>\Delta \Delta  nn and TT-->>\Delta \Delta  ((EE)). Now the move

over nn leaves only one item: TT-->>nn

\Delta \Delta  , which tells us through the dot at the end of the

item, that we have found a handle, nn, and that we should reduce it to TT using TT-->>nn. SeeFigure 9.20. This reduction gives us a new sentential form,

EE--TT--nn, on which we canrepeat the process.

\Gamma \Gamma  SS
SS-->>

\Gamma \Gamma  EE##

\Gamma \Gamma  EE

EE-->>

\Gamma \Gamma  EE--TT

EE-->>

\Gamma \Gamma  TT

\Gamma \Gamma  TT

TT-->>

\Gamma \Gamma  nn

TT-->>

\Gamma \Gamma  ((EE))

EE SS-->>EE

\Gamma \Gamma  ##

EE-->>EE

\Gamma \Gamma  --TT --

EE-->>EE--

\Gamma \Gamma  TT

\Gamma \Gamma  TT

TT-->>

\Gamma \Gamma  nn

TT-->>

\Gamma \Gamma  ((EE))

nn TT-->>nn

\Gamma \Gamma  --nn

Figure 9.20 The sets of NDA states while analysing EE--nn--nn

Sec. 9.4] LR methods 203

Just as in the FS case, we will soon tire of doing it this way, and the first thing weneed to do is to make the NDA deterministic, if we are to use it in earnest. We use the
subset construction of Section 5.3.1 to construct a deterministic automaton that has setsof the items of Figure 9.19 as its states. The result is shown in Figure 9.21, where we
have left out the stations to avoid clutter and since they are evident from the otheritems. We see that the deterministic automaton looks a lot less understandable than
Figure 9.19; this is the price to be paid for having determinism. Yet we see that thesubset construction has correctly identified the subsets we had already constructed by
hand in the previous paragraph. This type of automaton is called an LR(0) automaton.

SS-->>

\Gamma \Gamma  EE##

EE-->>

\Gamma \Gamma  EE--TT

EE-->>

\Gamma \Gamma  TT

TT-->>

\Gamma \Gamma  nn

TT-->>

\Gamma \Gamma  ((EE))

1

TT EE-->>TT

\Gamma \Gamma 

2

TT

TT-->>((

\Gamma \Gamma  EE))

EE-->>

\Gamma \Gamma  EE--TT

EE-->>

\Gamma \Gamma  TT

TT-->>

\Gamma \Gamma  nn

TT-->>

\Gamma \Gamma  ((EE))

6

TT-->>nn

\Gamma \Gamma 

3nn nn

SS-->>EE

\Gamma \Gamma  ##

EE-->>EE

\Gamma \Gamma  --TT

4

-- EE-->>EE--

\Gamma \Gamma  TT

TT-->>

\Gamma \Gamma  nn

TT-->>

\Gamma \Gamma  ((EE))

7

-- TT-->>((EE

\Gamma \Gamma  ))

EE-->>EE

\Gamma \Gamma  --TT

9

SS-->>EE##

\Gamma \Gamma 

5

##

EE-->>EE--TT

\Gamma \Gamma 

8

TT

TT-->>((EE))

\Gamma \Gamma 

10

))

EE

nn ((

EE

((

((

Figure 9.21 The corresponding deterministic handle recognizer
It is customary to number the states of the deterministic automaton, as has alreadybeen done in Figure 9.21 (the order of the numbers is arbitrary, they serve identification purposes only). Now it has become much easier to represent the sentential formwith its state information, both implementationwise in a computer and in a drawing:

A` EE A~ -- AE nn A^ -- nn
The sequence A` A~ AE A^ can be read from Figure 9.21 using the path EE--nn. We start withstate

A` on the stack and shift in symbols from the sentential form, all the while assess-ing the new states. As soon as an accepting state shows up on the top of the stack (and

it cannot show up elsewhere on the stack) the shifting stops and a reduce is called for;the accepting state indicates how to reduce. Accepting state

A^ calls for a reduction
TT-->>nn, so our new sentential form will be EE--TT--nn.Repeating the handle-finding process on this new form we obtain:

A` EE A~ -- AE TT C, -- nn
which shows us two things. First, the automaton has identified a new reduce, EE-->>EE--TT,from state

C,, which is correct. The second thing is that by restarting the automaton at

204 Deterministic bottom-up parsing [Ch. 9
the beginning of the sentential form we have done superfluous work: up to state 7, thatis, up to the left end of the handle, nothing has changed. We can save work as follows:
after a reduction of a handle to X, we look at the new exposed state on the stack andfollow the path marked X in the automaton, starting from that state. In our example we
have reduced to TT, found a AE exposed on the stack and the automaton leads us fromthere to

C, along the path marked TT. This type of shift on a non-terminal that has justresulted from a reduction is called a GOTO-action. Note that the state exposed after a

reduction can never call for a reduction: if it did so, that reduction would already havebeen performed earlier.

It is convenient to represent the LR(0) automaton by means of table in which therows correspond to states and the columns to symbols. In the intersection we find what
to do with a given symbol in a given state. The LR(0) table for the automaton of Fig-ure 9.21 is given in Figure 9.22. An entry like s3 means "shift the input symbol onto
the stack and go to state A^", which is often abbreviated to "shift to 3". The entry emeans that an error has been found: the corresponding symbol cannot legally appear in
that position. A blank entry will never even be consulted: either the state calls for areduction or the corresponding symbol will never at all appear in that position, regardless of the form of the input. In state 4, for instance, we will never meet an EE: the EEwould have originated from a previous reduction, but no reduction would do that in that
position. Since non-terminals are only put on the stack in legal places no empty entryon a non-terminal will ever be consulted.

nn -- (( )) ## EE TT reduce by\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
1 s3 e s6 e e s4 s22

EE -->> TT3
TT -->> nn4 e s7 e e s5

5 SS -->> EE ##6 s3 e s6 e e s9 s2
7 s3 e s6 e e s88

EE -->> EE -- TT9 e s7 e s10 e

10 TT -->> (( EE ))\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

Figure 9.22 LR(0) table for the grammar of Figure 9.18
In practice the "reduce by" entries for the reducing states do not directly refer tothe rules to be used, but to routines that have built-in knowledge of these rules, that
know how many entries to unstack and that perform the semantic actions associatedwith the recognition of the rule in question. Parts of these routines will be generated by
a parser generator.The table in Figure 9.22 contains much empty space and is also quite repetitious.
As grammars get bigger, the parsing tables get larger and they contain progressivelymore empty space and redundancy. Both can be exploited by data compression techniques and it is not uncommon that a table can be reduced to 15% of its original size bythe appropriate compression technique. See, for instance, Al-Hussainin and Stone [LR
1986] and Dencker, Du"rre and Heuft [Misc 1984].The advantages of LR(0) over precedence and bounded-context are clear. Unlike

Sec. 9.4] LR methods 205
precedence, LR(0) immediately identifies the rule to be used for reduction, and unlikebounded-context, LR(0) bases its conclusions on the entire left context rather than on
the last m symbols of it. In fact, LR(0) can be seen as a clever implementation ofBC(e^,0), i.e., bounded-context with unrestricted left context and zero right context.

9.4.2 LR(0) grammarsBy now the reader may have the vague impression that something is wrong. On the
one hand we claim that there is no known method to make a linear-time parser for anarbitrary grammar; on the other we have demonstrated above a method that seems to
work for an arbitrary grammar. A non-deterministic automaton as in Figure 9.19 cancertainly be constructed for any grammar, and the subset construction will certainly
turn it into a deterministic one, which will definitely not require more than linear time.Voila`, a linear-time parser.

The problem lies in the accepting states of the deterministic automaton. Anaccepting state may still have an outgoing arrow, say on a symbol

++, and if the nextsymbol is indeed a
++, the state calls for both a reduction and for a shift: the automatonis not really deterministic after all. Or an accepting state may be an honest accepting

state but call for two different reductions. The first problem is called a shift/reduceconflict and the second a reduce/reduce conflict. Figure 9.23 shows examples (that
derive from a slightly different grammar than in Figure 9.18).

EE-->>TT

\Gamma \Gamma  ++EE

EE-->>TT

\Gamma \Gamma 

++

shift/reduce conflict(on

++)

EE-->>EE--TT

\Gamma \Gamma 

EE-->>TT

\Gamma \Gamma 

reduce/reduce conflict(always)
Figure 9.23 Two types of conflict
Note that there cannot be a shift/shift conflict. A shift/shift conflict would imply thattwo different arrows leaving the same state would carry the same symbol. This is, however, prevented by the subset algorithm (which would have made into one the twostates the arrows point to).

A state that contains a conflict is called an inadequate state. A grammar thatleads to a deterministic LR(0) automaton with no inadequate states is called LR(0).
The grammar of Figure 9.18 is LR(0).

9.5 LR(1)
Our initial enthusiasm about the clever and efficient LR(0) parsing technique will soonbe damped considerably when we find out that very few grammars are in fact LR(0). If

we augment the grammar of Figure 9.18 by a single non-terminal SS'' and replace
SS-->>EE## by SS''-->>SS## and SS-->>EE to better isolate the end-marker, the grammar ceases to beLR(0). The new grammar is given in Figure 9.24, the non-deterministic automaton in

Figure 9.25 and the deterministic one in Figure 9.26.Apart from the split of state 5 in the old automaton into states 5 and 11, we
observe to our dismay that state 4 (marked 8) is now inadequate, exhibiting a

206 Deterministic bottom-up parsing [Ch. 9

1. SS''SS -->> SS ##2.

SS -->> EE3.
EE -->> EE -- TT4.
EE -->> TT5.
TT -->> nn6.
TT -->> (( EE ))

Figure 9.24 A non-LR(0) grammar for differences of numbers

\Gamma \Gamma  SS''

SS''-->>

\Gamma \Gamma  SS##

SS
SS''-->>SS

\Gamma \Gamma  ##

##
SS''-->>SS##

\Gamma \Gamma 

ee

\Gamma \Gamma  SS

SS-->>

\Gamma \Gamma  EE

EE
SS-->>EE

\Gamma \Gamma 

ee

\Gamma \Gamma  EE

EE-->>

\Gamma \Gamma  EE--TT

EE
EE-->>EE

\Gamma \Gamma  --TT

--
EE-->>EE--

\Gamma \Gamma  TT

TT
EE-->>EE--TT

\Gamma \Gamma 

ee

EE-->>

\Gamma \Gamma  TT

TT
EE-->>TT

\Gamma \Gamma 

ee

\Gamma \Gamma  TT

TT-->>

\Gamma \Gamma  nn

nn
TT-->>nn

\Gamma \Gamma 

ee

TT-->>

\Gamma \Gamma  ((EE))

((
TT-->>((

\Gamma \Gamma  EE))

EE
TT-->>((EE

\Gamma \Gamma  ))

))
TT-->>((EE))

\Gamma \Gamma 

eeee ee ee ee
ee

ee

Figure 9.25 Non-deterministic automaton for the grammar in Figure 9.24
shift/reduce conflict on --, and the grammar is not LR(0). We are the more annoyedsince this is a rather stupid inadequacy:

SS-->>EE

\Delta \Delta  can never occur in front of a -- but only

in front of a ##, so there is no real problem at all. If we had developed the parser byhand, we could easily test in state 4 if the symbol ahead was a

-- or a ## and act accord-ingly (or else there was an error in the input). Since, however, practical parsers have

hundreds of states, such manual intervention is not acceptable and we have to findalgorithmic ways to look at the symbol ahead.

Taking our clue from the the explanation of the Earley parser,# we attach to eachdotted item a look-ahead symbol; we shall separate the look-ahead symbol from the
item by a space rather than enclose it between [[]]'s, to avoid visual clutter. The con-struction of a non-deterministic handle-finding automaton using this kind of items, and
the subsequent subset construction yield an LR(1) parser.
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

This is historically incorrect: LR(1) parsing was invented (Knuth [LR 1965]) before Earleyparsing (Earley [CF 1970]).

Sec. 9.5] LR(1) 207

SS''-->>

\Gamma \Gamma  SS##

SS-->>

\Gamma \Gamma  EE

EE-->>

\Gamma \Gamma  EE--TT

EE-->>

\Gamma \Gamma  TT

TT-->>

\Gamma \Gamma  nn

TT-->>

\Gamma \Gamma  ((EE))

1

TT EE-->>TT

\Gamma \Gamma 

2

TT

TT-->>((

\Gamma \Gamma  EE))

EE-->>

\Gamma \Gamma  EE--TT

EE-->>

\Gamma \Gamma  TT

TT-->>

\Gamma \Gamma  nn

TT-->>

\Gamma \Gamma  ((EE))

6

TT-->>nn

\Gamma \Gamma 

3nn nn

SS-->>EE

\Gamma \Gamma 

EE-->>EE

\Gamma \Gamma  --TT

48

-- EE-->>EE--

\Gamma \Gamma  TT

TT-->>

\Gamma \Gamma  nn

TT-->>

\Gamma \Gamma  ((EE))

7

-- TT-->>((EE

\Gamma \Gamma  ))

EE-->>EE

\Gamma \Gamma  --TT

9

SS''-->>SS

\Gamma \Gamma  ##

5

SS

SS''-->>SS##

\Gamma \Gamma 

11

##

EE-->>EE--TT

\Gamma \Gamma 

8

TT

TT-->>((EE))

\Gamma \Gamma 

10

))

EE

nn ((

EE

((

((

Figure 9.26 Inadequate LR(0) automaton for the grammar in Figure 9.24
We shall now examine Figure 9.27, the non-deterministic automaton. Like theitems, the stations have to carry a look-ahead symbol too. Actually, a look-ahead symbol in a station is more natural than that in an item. A station like

\Delta \Delta  EE## just means: hoping to see an EE followed by a ##. The parser starts at station

\Delta \Delta  SS'', which has an invisible

look-ahead. From it we have e-moves to all production rules for SS'', of which there isonly one; this yields the item

SS''-->>

\Delta \Delta  SS##, again with empty look-ahead. This item necessitates the station

\Delta \Delta  SS##; we do not automatically construct all possible stations as we did

for the LR(0) automaton, but only those to which there are actual moves from else-where in the automaton. The station

\Delta \Delta  SS## has ## for a look-ahead and produces one item,

SS-->>

\Delta \Delta  EE ##. It is easy to see how the look-ahead propagates. The station \Delta \Delta  EE##, arrived at

from the previous item, causes the item EE-->>

\Delta \Delta  EE--TT ##, which in its turn necessitates the

station

\Delta \Delta  EE--, since now the automaton can be in the state "hoping to find an EE followed

by a --". The rest of the automaton will hold no surprises.The look-ahead derives either from the symbol following the non-terminal:

the item EE-->>

\Delta \Delta  EE--TT leads to station \Delta \Delta  EE--

or from the previous look-ahead if the non-terminal is the last symbol in the item:

the item SS-->>

\Delta \Delta  EE ## leads to station \Delta \Delta  EE##

There is a complication which does not occur in our example. When a non-terminal isfollowed by another non-terminal:

P o""

\Delta  QR x

there will be e-moves from this item to all stations

\Delta  Q y, where for y we have to fill in

all terminals in FIRST(R). This is reasonable since all these and only these symbols

208 Deterministic bottom-up parsing [Ch. 9

\Gamma \Gamma  SS''

SS''-->>

\Gamma \Gamma  SS##

SS
SS''-->>SS

\Gamma \Gamma  ##

##
SS''-->>SS##

\Gamma \Gamma 

ee

\Gamma \Gamma  SS##

SS-->>

\Gamma \Gamma  EE ##

EE
SS-->>EE

\Gamma \Gamma  ##

ee

\Gamma \Gamma  EE##

EE-->>

\Gamma \Gamma  EE--TT ##

EE
EE-->>EE

\Gamma \Gamma  --TT ##

--
EE-->>EE--

\Gamma \Gamma  TT ##

TT
EE-->>EE--TT

\Gamma \Gamma  ##

ee

EE-->>

\Gamma \Gamma  TT ##

TT
EE-->>TT

\Gamma \Gamma  ##

ee

\Gamma \Gamma  TT##

TT-->>

\Gamma \Gamma  nn ##

nn
TT-->>nn

\Gamma \Gamma  ##

ee

TT-->>

\Gamma \Gamma  ((EE)) ##

((
TT-->>((

\Gamma \Gamma  EE)) ##

EE
TT-->>((EE

\Gamma \Gamma  )) ##

))
TT-->>((EE))

\Gamma \Gamma  ##

ee

\Gamma \Gamma  EE--
EE-->>

\Gamma \Gamma  EE--TT --

EE
EE-->>EE

\Gamma \Gamma  --TT --

--
EE-->>EE--

\Gamma \Gamma  TT --

TT
EE-->>EE--TT

\Gamma \Gamma  --

ee

EE-->>

\Gamma \Gamma  TT --

TT
EE-->>TT

\Gamma \Gamma  --

ee

\Gamma \Gamma  TT--

TT-->>

\Gamma \Gamma  nn --

nn
TT-->>nn

\Gamma \Gamma  --

ee

TT-->>

\Gamma \Gamma  ((EE)) --

((
TT-->>((

\Gamma \Gamma  EE)) --

EE
TT-->>((EE

\Gamma \Gamma  )) --

))
TT-->>((EE))

\Gamma \Gamma  --

ee

\Gamma \Gamma  EE))

EE-->>

\Gamma \Gamma  EE--TT ))

EE
EE-->>EE

\Gamma \Gamma  --TT ))

--
EE-->>EE--

\Gamma \Gamma  TT ))

TT
EE-->>EE--TT

\Gamma \Gamma  ))

ee

EE-->>

\Gamma \Gamma  TT ))

TT
EE-->>TT

\Gamma \Gamma  ))

ee

\Gamma \Gamma  TT))

TT-->>

\Gamma \Gamma  nn ))

nn
TT-->>nn

\Gamma \Gamma  ))

ee

TT-->>

\Gamma \Gamma  ((EE)) ))

((
TT-->>((

\Gamma \Gamma  EE)) ))

EE
TT-->>((EE

\Gamma \Gamma  )) ))

))
TT-->>((EE))

\Gamma \Gamma  ))

ee

ee ee

ee

ee

ee

ee

ee ee

ee

ee

ee

ee

ee

ee

Figure 9.27 Non-deterministic LR(1) automaton for the grammar in Figure 9.24
can follow Q in this particular item. It will be clear that this is a rich source of stations.The next step is to run the subset algorithm on this automaton to obtain the deterministic automaton; if the automaton has no inadequate states, the grammar was LR(1)and we have obtained an LR(1) parser. The result is given in Figure 9.28. As was to
be expected, it contains many more states than the LR(0) automaton although the 60%increase is very modest, due to the simplicity of the grammar. An increase of a factor
of 10 or more is more likely in practice. (Although Figure 9.28 was constructed byhand, LR automata are normally created by a parser generator exclusively.)

We are glad but not really surprised to see that the problem of state 4 in Figure

Sec. 9.5] LR(1) 209

SS''-->>\Gamma \Gamma  SS##

SS-->>\Gamma \Gamma  EE##
EE-->>\Gamma \Gamma  EE--TT ##
EE-->>\Gamma \Gamma  EE--TT --

EE-->>\Gamma \Gamma  TT --
TT-->>\Gamma \Gamma  nn --
TT-->>\Gamma \Gamma  ((EE)) --

EE-->>\Gamma \Gamma  TT ##
TT-->>\Gamma \Gamma  nn ##
TT-->>\Gamma \Gamma  ((EE)) ##

1

SS''-->>SS\Gamma \Gamma  ##
2

SS

SS''-->>SS##\Gamma \Gamma 
3

##

TT EE-->>TT\Gamma \Gamma  --

EE-->>TT\Gamma \Gamma  ##

4

TT-->>((\Gamma \Gamma  EE)) --
TT-->>((\Gamma \Gamma  EE)) ##
EE-->>\Gamma \Gamma  EE--TT ))

EE-->>\Gamma \Gamma  TT ))
TT-->>\Gamma \Gamma  nn ))
TT-->>\Gamma \Gamma  ((EE)) ))
EE-->>\Gamma \Gamma  EE--TT --

EE-->>\Gamma \Gamma  TT --
TT-->>\Gamma \Gamma  nn --
TT-->>\Gamma \Gamma  ((EE)) --

7
TT-->>nn\Gamma \Gamma  --
TT-->>nn\Gamma \Gamma  ##

5
nn

SS-->>EE\Gamma \Gamma  ##
EE-->>EE\Gamma \Gamma  --TT ##
EE-->>EE\Gamma \Gamma  --TT --

6

--

EE-->>EE--\Gamma \Gamma  TT ##
EE-->>EE--\Gamma \Gamma  TT --

TT-->>\Gamma \Gamma  nn ##
TT-->>\Gamma \Gamma  ((EE)) ##

TT-->>\Gamma \Gamma  nn --
TT-->>\Gamma \Gamma  ((EE)) -- 8

TT-->>((EE\Gamma \Gamma  )) --
TT-->>((EE\Gamma \Gamma  )) ##
EE-->>EE\Gamma \Gamma  --TT ))
EE-->>EE\Gamma \Gamma  --TT --

12

EE-->>EE--TT\Gamma \Gamma  ##
EE-->>EE--TT\Gamma \Gamma  --

9

TT

TT-->>((EE))\Gamma \Gamma  --
TT-->>((EE))\Gamma \Gamma  ##

13

))

EE nn ((

((

TT EE-->>TT\Gamma \Gamma  --

EE-->>TT\Gamma \Gamma  ))

10

TT

TT-->>((\Gamma \Gamma  EE)) --
TT-->>((\Gamma \Gamma  EE)) ))
EE-->>\Gamma \Gamma  EE--TT ))

EE-->>\Gamma \Gamma  TT ))
TT-->>\Gamma \Gamma  nn ))
TT-->>\Gamma \Gamma  ((EE)) ))
EE-->>\Gamma \Gamma  EE--TT --

EE-->>\Gamma \Gamma  TT --
TT-->>\Gamma \Gamma  nn --
TT-->>\Gamma \Gamma  ((EE)) --

14
TT-->>nn\Gamma \Gamma  --
TT-->>nn\Gamma \Gamma  ))

11
nn nn

EE-->>EE--\Gamma \Gamma  TT ))
EE-->>EE--\Gamma \Gamma  TT --

TT-->>\Gamma \Gamma  nn ))
TT-->>\Gamma \Gamma  ((EE)) ))

TT-->>\Gamma \Gamma  nn --
TT-->>\Gamma \Gamma  ((EE)) -- 15

--

TT-->>((EE\Gamma \Gamma  )) --
TT-->>((EE\Gamma \Gamma  )) ))
EE-->>EE\Gamma \Gamma  --TT ))
EE-->>EE\Gamma \Gamma  --TT --

17
--

EE-->>EE--TT\Gamma \Gamma  ))
EE-->>EE--TT\Gamma \Gamma  --

16

TT

TT-->>((EE))\Gamma \Gamma  --
TT-->>((EE))\Gamma \Gamma  ))

18

))

EE nn

((

EE

((

((

Figure 9.28 Deterministic LR(1) automaton for the grammar in Figure 9.24
9.26, which is now state A* in Figure 9.28, has been resolved: on ## reduce using SS-->>EE,on

-- shift to C, and on any other symbol give an error message.It is again useful to represent the LR(1) automaton in a table, the LR(1) parsing

table. Since some reduction rules now occur several times in the table, it is convenientto number the rules, so they can be referred to by number in the table. The table gives
for each (state, symbol) pair whether:
\Gamma  to shift the symbol onto the stack and go to state N (written sN),

\Gamma  to reduce using rule R, remove the entries corresponding to the right-hand side

from the stack and enter the table again with the pair (statenew, lhs ), where statenewis the state just uncovered and now on top of the stack and lhs is the left-hand side

of R (written rR), or
\Gamma  to announce an error (written e).

Figure 9.29 shows the LR(1) table; the blank entries can never be accessed.The sentential form

EE--nn--nn leads to the following stack:

A` EE A* -- C, nn A" -- nn
and since the look-ahead is --, the correct reduction TT-->>nn is indicated.Note that if the sentential form had been

EE--nnnn, the LR(1) parser would find anerror:

A` EE A* -- C, nn A" nn
since the pair (5, nn) yields e. It is instructive to see that the LR(0) parser of Figure 9.22would do the reduction:

210 Deterministic bottom-up parsing [Ch. 9

nn -- (( )) ## SS EE TT\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 
1 s5 e s7 e e s2 s6 s42 e e e e s3

3/acc4 e r4 e e r4

5 e r5 e e r56 e s8 e e r2
7 s11 e s14 e e s12 s108 s5 e s7 e e s9
9 e r3 e e r310 e r4 e r4 e
11 e r5 e r5 e12 e s15 e s13 e
13 e r6 e e r614 s11 e s14 e e s17 s10
15 s11 e s14 e e s1616 e r3 e r3 e
17 e s15 e s18 e18 e r6 e r6 e\Theta \Theta 

\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

Figure 9.29 LR(1) table for the grammar of Figure 9.24
A` EE A~ -- AE nn A^ nn
since state 3 is an accepting state. Even a second reduction would follow:

A` EE A~ -- AE TT C, nn
which through EE-->>EE--TT yields

A` EE A~ nn
Only now is the error found, since the pair (4, nn) in Figure 9.22 yields e. Not surpris-ingly, the LR(0) automaton is less alert than the LR(1) automaton.

All stages of the LR(1) parsing of the string nn--nn--nn are given in Figure 9.30. Notethat state

A* in h causes a shift (look-ahead --) while in l it causes a reduce 2 (look-ahead
##).

9.5.1 LR(1) with e-rulesIn Section 3.3.2 we have seen that one has to be careful with e-rules in bottom-up
parsers: they are hard to recognize bottom-up. Fortunately LR(1) parsers are strongenough to handle them without problems. In the non-deterministic automaton, an erule is nothing special; it is just an exceptionally short list of moves starting from a sta-tion (see station

\Delta \Delta  BBcc in Figure 9.32(a). In the deterministic automaton, the e-reduction

is possible in all states of which the e-rule is a member, but hopefully its look-aheadsets it apart from all other rules in those states. Otherwise a shift/reduce or

reduce/reduce conflict results, and indeed the presence of e-rules in a grammar raises

Sec. 9.5] LR(1) 211

a A` nn--nn--nn## shiftb

A` nn A" --nn--nn## reduce 5c
A` TT A~ --nn--nn## reduce 4d
A` EE A* --nn--nn## shifte
A` EE A* -- C, nn--nn## shiftf
A` EE A* -- C, nn A" --nn## reduce 5g
A` EE A* -- C, TT E` --nn## reduce 3h
A` EE A* --nn## shifti
A` EE A* -- C, nn## shiftj
A` EE A* -- C, nn A" ## reduce 5k
A` EE A* -- C, TT E` ## reduce 3l
A` EE A* ## reduce 2m
A` SS A' ## shiftn
A` SS A' ## A^ reduce 1o
A` SS'' A^ accept

Figure 9.30 LR(1) parsing of the string nn--nn--nn
the risks of such conflicts and reduces the likelihood of the grammar to be LR(1).

SS''SS -->> SS ##

SS -->> AA BB cc
AA -->> aa
BB -->> bb
BB -->> ee

Figure 9.31 A simple grammar with an e-rule
To avoid page-filling drawings, we demonstrate the effect using the trivial gram-mar of Figure 9.31. Figure 9.32(a) shows the non-deterministic automaton, Figure
9.32(b) the resulting deterministic one. Note that no special actions were necessary tohandle the rule

BB-->>ee.The only complication occurs again in determining the look-ahead sets in rules in

which a non-terminal is followed by another non-terminal; here we meet the samephenomenon as in an LL(1) parser (Section 8.2.2.1). Given an item, for instance,
P o""

\Delta  ABC [d ], we are required to produce the look-ahead set for the station \Delta  A [ . . . ].

If B had been a terminal, it would have been the look-ahead. Now we take the FIRSTset of B, and if B produces e (is nullable) we add the FIRST set of C since B can be

transparent and allow us to see the first token of C. If C is also nullable, we may evensee [d ], so in that case we also add d to the look-ahead set. The result of these operations is written as FIRST(BC [d ]) (which is, in fact, equal to FIRST(BCd)).
9.5.2 Some properties of LR(k) parsingInstead of a look-ahead of one token, k tokens can be used. It is not difficult to do so
but it is extremely tedious and the resulting tables assume gargantuan size (see, e.g.,Ukkonen [LR 1985]). Moreover it does not really help much. Although an LR(2)
parser is more powerful than an LR(1) parser, in that it can handle some grammars that

212 Deterministic bottom-up parsing [Ch. 9

\Gamma \Gamma  SS''
SS''-->>

\Gamma \Gamma  SS##

SS
SS''-->>SS

\Gamma \Gamma  ##

##
SS''-->>SS##

\Gamma \Gamma 

ee

\Gamma \Gamma  SS##

SS-->>

\Gamma \Gamma  AABBcc ##

AA
SS-->>AA

\Gamma \Gamma  BBcc ##

BB
SS-->>AABB

\Gamma \Gamma  cc ##

cc
SS-->>AABBcc

\Gamma \Gamma  ##

ee

\Gamma \Gamma  AAbb

AA-->>

\Gamma \Gamma  aa bb

aa
AA-->>aa

\Gamma \Gamma  bb

ee

\Gamma \Gamma  AAcc

AA-->>

\Gamma \Gamma  aa cc

aa
AA-->>aa

\Gamma \Gamma  cc

ee

\Gamma \Gamma  BBcc

BB-->>ee

\Gamma \Gamma  cc

ee

BB-->>

\Gamma \Gamma  bb cc

bb
BB-->>bb

\Gamma \Gamma  cc

eeee ee
ee

ee
(a)

SS''-->>

\Gamma \Gamma  SS##

SS-->>

\Gamma \Gamma  AABBcc ##

AA-->>

\Gamma \Gamma  aa cc

AA-->>

\Gamma \Gamma  aa bb

1

SS''-->>SS

\Gamma \Gamma  ##

2

SS''-->>SS##

\Gamma \Gamma 

3

SS
##

aa AA-->>aa

\Gamma \Gamma  cc

AA-->>aa

\Gamma \Gamma  bb

4

SS-->>AA

\Gamma \Gamma  BBcc ##

BB-->>ee

\Gamma \Gamma  cc

BB-->>

\Gamma \Gamma  bb cc

5

BB SS-->>AABB\Gamma \Gamma  cc ##

7

SS-->>AABBcc

\Gamma \Gamma  ##

8
cc

AA BB-->>bb

\Gamma \Gamma  cc

6

bb

(b)
Figure 9.32 Non-deterministic and deterministic LR(1) automata for Figure 9.31
the other cannot, the emphasis is on "some". If a common-or-garden variety grammaris not LR(1), chances are minimal that it is LR(2) or higher.

Some theoretically interesting properties of varying practical significance arebriefly mentioned here. It can be proved that any LR(k) grammar with k >1 can be
transformed into an LR(k -1) grammar (and so to LR(1), but not always to LR(0)),often at the expense of an enormous increase in size; see, e.g. Mickunas, Lancaster and
Schneider [LR 1976]. It can be proved that if a language allows parsing with a push-down automaton as described in Section 3.4, it has an LR(1) grammar; such languages
are called deterministic languages. It can be proved that if a grammar can be handledby any of the deterministic methods of Chapters 8 and 9 (except the non-canonical
methods of 9.9), it can be handled by an LR(k) parser (that is, all deterministic methodsare weaker than or equally strong as LR(k)).

An LR(1) parser has the immediate error detection property: the parser will stopat the first incorrect token in the input and not even perform another shift or reduce.
This is important because this early error detection property allows a maximum amount

Sec. 9.5] LR(1) 213
of context to be preserved for error recovery; see Section 10.2.6.In summary, LR(k) parsers are the strongest deterministic parsers possible and
they are the strongest linear-time parsers known (with the possible exception of somenon-canonical parsers; see Section 9.9). They react to errors immediately, are paragons
of virtue and beyond compare. They are also not widely used.

9.6 LALR(1) PARSING
The reader will have sensed that our journey has not yet come to an end; the goal of apractical, powerful, linear-time parser has still not be attained. Even at their inception

by Knuth in 1965 [LR 1965], it was realized that LR(1) parsers would be impractical inthat the space required for their deterministic automata would be prohibitive. A modest
grammar might already require hundreds of thousands or even millions of states,numbers that were totally incompatible with the computer memories of those days and
that would even tax present-day memories.In the face of this difficulty, development of this line of parsers came to a standstill, partially interrupted by Korenjak's invention of a method to partition the gram-mar, build LR(1) parsers for each of the parts and combine these into a single over-all
parser (Korenjak [LR 1969]). This helped, but not much, in view of the added com-plexity.

The problem was finally solved by using an unlikely and discouraging-lookingmethod. Consider the LR(1) automaton in Figure 9.28 and imagine boldly discarding
all look-ahead information from it. Then we see that each state in the LR(1) automatonreverts to a specific state in the LR(0) automaton; for instance, LR(1) states 7 and 14
collapse into LR(0) state 6 and LR(1) states 4 and 10 collapse into LR(0) state 3. Thereis not a single state in the LR(1) automaton that was not already present in a rudimentary form in the LR(0) automaton. Also, the transitions remain intact during the col-lapse: both LR(1) states 7 and 14 have a transition to state 10 on

TT, but so has LR(0)state 6 to 3. By striking out the look-ahead information from an LR(1) automaton, it

collapses into an LR(0) automaton for the same grammar, with a great gain as tomemory requirements but also at the expense of the look-ahead power. This will probably not surprise the reader too much, although a formal proof of this phenomenon isnot trivial.

The idea is now to collapse the automaton but to keep the look-ahead information(and collapse it too, but not discard it). The surprising fact is that this preserves almost
all the original look-ahead power and still saves an enormous amount of memory. Theresulting automaton is called an LALR(1) automaton, for "Look Ahead LR[(0)] with a
look-ahead of 1 token." LALR(k) also exists and is LR(0) with an add-on look-ahead ofk tokens.

The LALR(1) automaton for our grammar of Figure 9.24 is given in Figure 9.33,where the look-aheads are sets now and are shown between

[[ and ]]. We see that theoriginal conflict in state 4 is indeed still resolved, as it was in the LR(1) automaton, but

that its size is equal to that of the LR(0) automaton. That now is a very fortunate stateof affairs!

We have finally reached our goal. LALR(1) parsers are powerful, almost aspowerful as LR(1) parsers, they have fairly modest memory requirements, only slightly
inferior to (= larger than) those of LR(0) parsers,# and they are time-efficient. LALR(1)
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

Since the LALR(1) tables contain more information than the LR(0) tables (although they havethe same size), they lend themselves less well to data compression. So practical LALR(1)

214 Deterministic bottom-up parsing [Ch. 9

SS''-->>

\Gamma \Gamma  SS## [[]]

SS-->>

\Gamma \Gamma  EE [[##]]

EE-->>

\Gamma \Gamma  EE--TT [[##--]]

EE-->>

\Gamma \Gamma  TT [[##--]]

TT-->>

\Gamma \Gamma  nn [[##--]]

TT-->>

\Gamma \Gamma  ((EE)) [[##--]]

1

TT EE-->>TT

\Gamma \Gamma  [[##--))]]

2

TT

TT-->>((

\Gamma \Gamma  EE)) [[##--))]]

EE-->>

\Gamma \Gamma  EE--TT [[##--))]]

EE-->>

\Gamma \Gamma  TT [[##--))]]

TT-->>

\Gamma \Gamma  nn [[##--))]]

TT-->>

\Gamma \Gamma  ((EE)) [[##--))]]

6

TT-->>nn

\Gamma \Gamma  [[##--))]]

3nn nn

SS-->>EE

\Gamma \Gamma  [[##]]

EE-->>EE

\Gamma \Gamma  --TT [[##--]]

4

-- EE-->>EE--

\Gamma \Gamma  TT [[##--))]]

TT-->>

\Gamma \Gamma  nn [[##--))]]

TT-->>

\Gamma \Gamma  ((EE)) [[##--))]]

7

-- TT-->>((EE

\Gamma \Gamma  )) [[##--))]]

EE-->>EE

\Gamma \Gamma  --TT [[##--))]]

9

SS''-->>SS

\Gamma \Gamma  ## [[]]

5

SS

SS''-->>SS##

\Gamma \Gamma  [[]]

11

##

EE-->>EE--TT

\Gamma \Gamma  [[##--))]]

8

TT

TT-->>((EE))

\Gamma \Gamma  [[##--))]]

10

))

EE

nn ((

EE

((

((

Figure 9.33 The LALR(1) automaton for the grammar of Figure 9.24
parsing may very well be the most-used parsing method in the world today.
9.6.1 Constructing the LALR(1) parsing tablesWhen we have sufficiently drunk in the beauty of the vista that spreads before us on
these heights, and start thinking about returning home and actually building such aparser, it will come to us that there is a small but annoying problem left. We have
understood how the desired parser should look and also seen how to construct it, butduring that construction we used the unacceptably large LR(1) parser as an intermediate step.So the problem is to find a shortcut by which we can produce the LALR(1) parse
table without having to construct the one for LR(1). This particular problem has fas-cinated scores of computer scientists to this day (see the references in 13.6, for
instance, Park and Choe [LR 1987]), and several good (and some very clever) algo-rithms are known.

We shall treat here only one algorithm, one that is both intuitively relatively clearand reasonably efficient. It was (probably) first described in rather vague terms by
Anderson, Eve and Horning [LR 1973], it is used in the well-known parser generatoryacc (Johnson [LR 1978]) and is described in more detail by Aho, Sethi and Ullman
[Books 1986]. The algorithm does not seem to have a name; we shall call it the chan-nel algorithm here.

We again use the grammar of Figure 9.24, which we now know is LALR(1) (butnot LR(0)). Since we want to do look-ahead but do not yet know what to look for, we
use LR(0) items extended with a yet unknown look-ahead field, indicated by an empty
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 

parsers will be bigger than LR(0) parsers.

Sec. 9.6] LALR(1) parsing 215

\Gamma \Gamma  SS''\Gamma \Gamma 
SS''-->>

\Gamma \Gamma  SS## \Gamma \Gamma 

SS
SS''-->>SS

\Gamma \Gamma  ## \Gamma \Gamma 

##
SS''-->>SS##

\Gamma \Gamma  \Gamma \Gamma 

ee

\Gamma \Gamma  SS \Gamma \Gamma 

SS-->>

\Gamma \Gamma  EE \Gamma \Gamma 

EE
SS-->>EE

\Gamma \Gamma  \Gamma \Gamma 

ee

\Gamma \Gamma  EE \Gamma \Gamma 

EE-->>

\Gamma \Gamma  EE--TT \Gamma \Gamma 

EE
EE-->>EE

\Gamma \Gamma  --TT \Gamma \Gamma 

--
EE-->>EE--

\Gamma \Gamma  TT \Gamma \Gamma 

TT
EE-->>EE--TT

\Gamma \Gamma  \Gamma \Gamma 

ee

EE-->>

\Gamma \Gamma  TT \Gamma \Gamma 

TT
EE-->>TT

\Gamma \Gamma  \Gamma \Gamma 

ee

\Gamma \Gamma  TT \Gamma \Gamma 

TT-->>

\Gamma \Gamma  nn \Gamma \Gamma 

nn
TT-->>nn

\Gamma \Gamma  \Gamma \Gamma 

ee

TT-->>

\Gamma \Gamma  ((EE)) \Gamma \Gamma 

((
TT-->>((

\Gamma \Gamma  EE)) \Gamma \Gamma 

EE
TT-->>((EE

\Gamma \Gamma  )) \Gamma \Gamma 

))
TT-->>((EE))

\Gamma \Gamma  \Gamma \Gamma 

eeee ee ee ee
ee

ee

. . . . ....

...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
.

. . ....

..
..
...

..
...

..
..
...

..
..
...

..
...

.

...
...
...
...
....
...
...
....

...
..
...

..
...

..
...

...
...
...
..
...

...
...
..
...

..
..

...

...

...

.. .

...

.. ..

..
..
...
..
..
..
..
..
..
..
..

...
...
...
....
...
...
...
....

..
..
..
..
..
..
..
.

...

.. .

...

.. .

...

.. .

....

...
....
...
..
...

..
...

..
...

...
...
...
..
...

...
...
..
...

..
..

...

...

...

...

...

...

...

...

...

.. . . . . . . . . . . . . . . . . . ....

...
...

...

...

...

...

...

...

...

...

.... . . . . . . . . . . . . . . . . . . . . . . ......

...
.

. . . . . . .

. . . . . . .

. . . . . . .

. . . ...

...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
... . . . . . . . . . . . . . . . . . . . . . .....

..

Figure 9.34 Non-deterministic automaton with channels
square; a possible item would be AA-->>bbCC

\Delta \Delta  DDee \Gamma \Gamma  . Using such items, we construct the

non-deterministic LR(0) automaton in the usual fashion; see Figure 9.34. Now supposethat we were told by some oracle what the look-ahead set of the item

SS-->>

\Delta \Delta  EE \Gamma \Gamma  is

(second column, second row in Figure 9.34); call this look-ahead set L. Then we coulddraw a number of conclusions. The first is that the item

SS-->>EE

\Delta \Delta  \Gamma \Gamma  also has L. The next

is that the look-ahead set of the station

\Delta \Delta  EE\Gamma \Gamma  is also L, and from there L spreads to

EE-->>

\Delta \Delta  EE--TT, EE-->>EE\Delta \Delta  --TT, EE-->>EE--\Delta \Delta  TT, EE-->>EE--TT\Delta \Delta  , EE-->>\Delta \Delta  TT and EE-->>TT\Delta \Delta  . From EE-->>EE--\Delta \Delta  TT and EE-->>\Delta \Delta  TT

it flows to the station

\Delta \Delta  TT and from there it again spreads on.

The flow possibilities of look-ahead information from item to item once it isknown constitute "channels" which connect items. Each channel connects two items

and is one-directional. There are two kinds of channels. From each station channels rundown to each item that derives from it; the input to these channels comes from elsewhere. From each item that has the dot in front of a non-terminal A, a channel runsparallel to the e-arrow to the station

\Delta  A \Gamma  . If A is the last symbol in the right-hand side,

the channel propagates the look-ahead of the item it starts from. If A is not the last sym-bol, but is followed by, for instance, CDe (so the entire item would be something like

P o""B

\Delta  ACDe \Gamma  ), the input to the channel is FIRST(CDe); such input is said to be "generated spontaneously", as opposed to "propagated" input. The full set of channels hasbeen drawn as dotted lines (carrying propagated input) and as dashed lines (carrying

spontaneous input) in Figure 9.34. It can be represented in a computer as a list of inputand output ends of channels:

216 Deterministic bottom-up parsing [Ch. 9

Input end leads to output end

\Delta \Delta  SS''\Gamma \Gamma  ====>> SS''-->>\Delta \Delta  SS## \Gamma \Gamma 

SS''-->>

\Delta \Delta  SS## \Gamma \Gamma  ====>> SS''-->>SS\Delta \Delta  ## \Gamma \Gamma 

[[##]] ====>>

\Delta \Delta  SS\Gamma \Gamma 

SS''-->>SS

\Delta \Delta  ## \Gamma \Gamma  ====>> SS''-->>SS##\Delta \Delta  \Gamma \Gamma 

......

Next we run the subset algorithm on this (channelled) non-deterministic automa-ton in slow motion and watch carefully where the channels go. This procedure
severely taxes the human brain; a more practical way is to just construct the determinis-tic automaton without concern for channels and then use the above list (in its complete
form) to re-establish the channels. This is easily done by finding the input and outputend items and stations in the states of the deterministic automaton and construct the
corresponding channels. Note that a single channel in the non-deterministic automatoncan occur many times in the deterministic automaton, since items can (and will) be
duplicated by the subset algorithm. The result can best be likened to a bowl of mixedspaghetti and tagliatelli (the channels and the transitions) with occasional chunks of
ham (the item sets) and will not be printed in this book.Now we are close to home. For each channel we pump its input to the channel's
end. First this will only have effect for channels that have spontaneous input: a ## willflow in state 1 from item

SS''-->>

\Delta \Delta  SS##[[\Gamma \Gamma  ]] to station \Delta \Delta  SS[[\Gamma \Gamma  ]], which will then read \Delta \Delta  SS[[##]]; a

-- from EE-->>

\Delta \Delta  EE--TT[[\Gamma \Gamma  ]] flows to the \Delta \Delta  EE[[\Gamma \Gamma  ]], which changes to \Delta \Delta  EE[[--]]; etc. etc. We go on

pumping until all look-ahead sets are stable and nothing changes any more. We havenow obtained the LALR(1) automaton and can discard the channels (although we must,

of course, keep the transitions).It is interesting to look more closely at state 4 (see Figure 9.33) and to see how
SS-->>EE

\Delta \Delta  [[##]] gets its look-ahead which excludes the --, although the latter is present in the

look-ahead set of EE-->>EE

\Delta \Delta  --TT[[##--]] in state 4. To this end, a magnified view of the top

left corner of the full channelled LALR(1) automaton is presented in Figure 9.35; itcomprises the states 1 to 4. Again channels with propagated input are dotted, those

with spontaneous input are dashed and transitions are drawn. We can now see moreclearly that

SS-->>EE

\Delta \Delta  [[##]] derives its look-ahead from SS-->>\Delta \Delta  EE[[##]] in 1, while EE-->>EE\Delta \Delta  --TT[[##--

]] derives its look-ahead (indirectly) from

\Delta \Delta  EE[[--]] in 1. The latter has a look-ahead --

generated spontaneously in EE-->>

\Delta \Delta  EE--TT[[\Gamma \Gamma  ]] in 1. The channel from SS-->>\Delta \Delta  EE[[##]] to \Delta \Delta  EE[[##--]]

only works "downstream", which prevents the -- from flowing back. LALR(1) parsersoften give one the feeling that they succeed by a narrow margin!

9.6.2 LALR(1) with e-rulesThe same complications arise as in Section 9.5.1 in the determination of the FIRST set
of the rest of the right-hand side: when a non-terminal is nullable we have to alsoinclude the FIRST set of what comes after it, and so on. We meet a special complication if the entire rest of the right-hand side can be empty: then we may see the look-ahead

\Gamma  , which we do not know yet. In fact this creates a third kind of channel that has

to be watched in the subset algorithm. We shall not be so hypocritical as to suggest theconstruction of the LALR(1) automaton for the grammar of Figure 9.31 as an exercise

to the reader, but we hope the general principles are clear. Let a parser generator do therest.

Sec. 9.6] LALR(1) parsing 217

\Gamma \Gamma  SS'' [[]]
SS''-->>

\Gamma \Gamma  SS## [[]]

\Gamma \Gamma  SS [[##]]

SS-->>

\Gamma \Gamma  EE [[##]]

\Gamma \Gamma  EE [[##--]]

EE-->>

\Gamma \Gamma  EE--TT [[##--]]

EE-->>

\Gamma \Gamma  TT [[##--]]

\Gamma \Gamma  TT [[##--]]

TT-->>

\Gamma \Gamma  nn [[##--]]

TT-->>

\Gamma \Gamma  ((EE)) [[##--]]

1

EE-->>TT

\Gamma \Gamma  [[##--))]]

2
TT

TT-->>nn

\Gamma \Gamma  [[##--))]]

3
nn

SS-->>EE

\Gamma \Gamma  [[##]]

EE-->>EE

\Gamma \Gamma  --TT [[##--]]

4
EE

--

nn

SS

((
. . . ....

...
..

. . . ....

...
..

. . . . . ....

...

..

. . . . . ....

...
..

. . . . . ....

...
..

.......
......
......
......

......

.

.........
......
......
......

........

.....

. . . . . . . . . . . . . . .

. . . . . . . . . . . . . . .

. . . . . . . . . . . . . . .

. . . . . . .

. . . . . . . . . . . . . . .

. . . . . . . . . . . . . . .

. . . . . . . . . . . . . . .

. . . . . . .

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

....

...

.....
.....
....

.....
.....
....

Figure 9.35 Part of the deterministic automaton with channels (magnified cut)
9.6.3 Identifying LALR(1) conflictsWhen a grammar is not LR(1), the constructed LR(1) automaton will have conflicts,
and the user of the parser generator will have to be notified. Such notification oftentakes such forms as:

Reduce/reduce conflictin state 213 on look-ahead #

SS -->> EE versus AA -->> TT ++ EE
This may seem cryptic but the user soon learns to interpret such messages and to reachthe conclusion that indeed "the computer can't see this". This is because LR(1) parsers

218 Deterministic bottom-up parsing [Ch. 9
can handle all deterministic grammars and our idea of "what a computer can see" coin-cides reasonably well with what is deterministic.

The situation is worse for those (relatively rare) grammars that are LR(1) but notLALR(1). The user never really understands what is wrong with the grammar: the computer should be able to make the right parsing decisions, but it complains that it cannot.Of course there is nothing wrong with the grammar; the LALR(1) method is just marginally too weak to handle it.To alleviate the problem, some research has gone into methods to elicit from the
faulty automaton a possible input string that would bring it into the conflict state. SeeDeRemer and Pennello [LR 1982]. The parser generator can then display such input
with both its possible partial parse trees.
9.6.4 SLR(1)There is a simpler way to proceed with the non-deterministic automaton of Figure 9.34.
We can first pump around the look-ahead sets until they are all known and then applythe subset algorithm, rather than vice versa. This gives us the SLR(1) automaton (for
Simple LR(1)); see DeRemer [LR 1971]. The same automaton can be obtained withoutusing channels at all: construct the LR(0) automaton and then add to each item
A o"" . . . a look-ahead set that is equal to FOLLOW(A). Pumping around the look-ahead sets in the non-deterministic automaton effectively calculates the FOLLOW sets
of each non-terminal and spreads these over each item derived from it.The SLR(1) automaton is shown in Figure 9.36. FOLLOW(

SS)={##},FOLLOW(
EE)={##, --, ))} and FOLLOW(TT)={##, --, ))}; consequently, only states 1 and 4differ from those in the LALR(1) automaton of Figure 9.33. The increased look-ahead

sets do not spoil the adequateness of any states: the grammar is also SLR(1).

SS''-->>

\Gamma \Gamma  SS## [[]]

SS-->>

\Gamma \Gamma  EE [[##]]

EE-->>

\Gamma \Gamma  EE--TT [[##--))]]

EE-->>

\Gamma \Gamma  TT [[##--))]]

TT-->>

\Gamma \Gamma  nn [[##--))]]

TT-->>

\Gamma \Gamma  ((EE)) [[##--))]]

1

TT EE-->>TT

\Gamma \Gamma  [[##--))]]

2

TT

TT-->>((

\Gamma \Gamma  EE)) [[##--))]]

EE-->>

\Gamma \Gamma  EE--TT [[##--))]]

EE-->>

\Gamma \Gamma  TT [[##--))]]

TT-->>

\Gamma \Gamma  nn [[##--))]]

TT-->>

\Gamma \Gamma  ((EE)) [[##--))]]

6

TT-->>nn

\Gamma \Gamma  [[##--))]]

3nn nn

SS-->>EE

\Gamma \Gamma  [[##]]

EE-->>EE

\Gamma \Gamma  --TT [[##--))]]

4

-- EE-->>EE--

\Gamma \Gamma  TT [[##--))]]

TT-->>

\Gamma \Gamma  nn [[##--))]]

TT-->>

\Gamma \Gamma  ((EE)) [[##--))]]

7

-- TT-->>((EE

\Gamma \Gamma  )) [[##--))]]

EE-->>EE

\Gamma \Gamma  --TT [[##--))]]

9

SS''-->>SS

\Gamma \Gamma  ## [[]]

5

SS

SS''-->>SS##

\Gamma \Gamma  [[]]

11

##

EE-->>EE--TT

\Gamma \Gamma  [[##--))]]

8

TT

TT-->>((EE))

\Gamma \Gamma  [[##--))]]

10

))

EE

nn ((

EE

((

((

Figure 9.36 SLR(1) automaton for the grammar of Figure 9.24
SLR(1) parsers are intermediate in power between LR(0) and LALR(1). Since

Sec. 9.6] LALR(1) parsing 219
SLR(1) parsers have the same size as LALR(1) parsers and are considerably lesspowerful, LALR(1) parsers are generally preferred.

9.6.5 Conflict resolversWhen states in an automaton have conflicts and no stronger method is available, the
automaton can still be useful, provided we can find other ways to resolve the conflicts.Most LR parser generators have built-in automatic conflict resolvers that will make
sure that a deterministic automaton results, whatever properties the input grammar mayhave. Such a system will just enumerate the problems it has encountered and indicate
how it has solved them.Two useful and popular rules of thumb to solve LR conflicts are:
\Gamma  on a shift/reduce conflict, shift (only on those look-aheads for which the conflict

occurs);
\Gamma  on a reduce/reduce conflict, reduce using the longest rule.

Both rules implement the same idea: take the largest bite possible. If you find that thereis a production of A somewhere, make it as long as possible, including as much

material on both sides as possible. This is very often what the grammar writer wants.Systems with built-in conflict resolvers are a mixed blessing. On the one hand
they allow very weak or even ambiguous grammars to be used (see for instance, Aho,Johnson and Ullman [Misc 1975]). This can be a great help in formulating grammars
for difficult and complex analysis jobs; see, for instance, Kernighan and Cherry [Misc1975], who make profitable use of automatic conflict resolution for the specification of
typesetter input.On the other hand a system with conflict resolvers may impose a structure on the
input where there is none. Such a system does no longer correspond to any grammar-like sentence-generating mechanism, and it may be very difficult to specify exactly
what strings will be accepted and with what structure. How severe a drawback this isdepends on the application and of course on the capabilities of the parser generator
user. Note that it is not humanly possible to have dynamic (parse-time) conflictresolvers as in the LL case (Section 8.2.5.3). The conflict-resolver would be calledupon in a context that is still under construction, and its user would be required to fully
understand the underlying LR automaton. Some experiments have been done withinteractive conflict resolvers, which consult the user of the parser when a conflict actually arises: a large chunk of text around the conflict point is displayed and the user isasked to resolve the conflict. This is useful in, for instance, document conversion. See
Share [Misc 1988].

9.7 FURTHER DEVELOPMENTS OF LR METHODS
Although the LALR(1) method as explained in Section 9.6 is quite satisfactory for mostapplications, a number of extensions to and improvements of the LR methods have

been studied. The most important of these will be briefly explained in this section; fordetails see the literature, Section 13.6 and the original references. Most of the more
advanced methods have not yet found their way into existing parser generators.

220 Deterministic bottom-up parsing [Ch. 9
9.7.1 Elimination of unit rulesMany rules in practical grammars are of the form A o"" B; examples can be found in Figures 2.9, 4.4, 5.2, 7.8, 8.7, 9.37 and many others. Such rules are called unit rules, sin-gle rules or chain rules. They generally serve naming purposes only and have no
semantics attached to them. Consequently, their reduction is a matter of stack manipu-lation and state transition only, to no visible purpose for the user. Such "administrative
reductions" can take a considerable part of the parsing time (50% is not unusual). Sim-ple methods to short-cut such reductions are easily found (for instance, removal by systematic substitution) but may result in an exponential increase in table size. Bettermethods were found but turned out to be complicated and to impair the error detection
properties of the parser. The latter can again be corrected, at the expense of more com-plication. See Heilbrunner [LR 1985] for a thorough treatment and Chapman [LR
1987] for much practical information.Note that the term "elimination of unit rules" in this case is actually a misnomer:
the unit rules themselves are not removed from the grammar, but rather their effectfrom the parser tables. Compare this to the actual elimination of unit rules in Section
4.2.3.2.

MMeettrree -->> IIaammbbiicc || TTrroocchhaaiicc || DDaaccttyylliicc || AAnnaappeessttiicc

Figure 9.37 A (multiple) unit rule
9.7.2 Regular right part grammarsAs shown in Section 2.3.2.3, there are two interpretations of a regular right-hand side
of a rule: the recursive and the iterative interpretation. The recursive interpretation isno problem: for a form like A + anonymous non-terminals are introduced, the reduction
of which entails no semantic actions. The burden of constructing a list of the recog-nized A's lies entirely on the semantic routines attached to the A's.

The iterative interpretation causes more problems. When an A + has been recog-nized and is about to be reduced, the stack holds an indeterminate number of A's:

...... AA******AAAAAA||
The right end of the handle has been found, but the left end is doubtful. Scooping upall A's from the right may be incorrect since some may belong to another rule; after all,
the top of the stack may derive from a rule P o""QAAA +. A possible solution is to havefor each reducing state and look-ahead a FS automaton that scans the stack backwards
while examining states in the stack to determine the left end and the actual rule toreduce to. The part to be reduced (the handle) can then be shown to a semantic routine
which can, for instance, construct a list of A's, thereby relieving the A's from a taskthat is not structurally theirs. The resulting tables can be enormous and clever algorithms have been designed for their construction and reduction. See for instance,LaLonde [LR 1981]. Sassa and Nakata [LR 1987] provide a different and simpler technique.
9.7.3 Improved LALR(1) table constructionThe channel algorithm for the construction of LALR(1) parse tables explained in Section 9.6.1 is relatively fast as it is, but the underlying automata have a rich structure and

Sec. 9.7] Further developments of LR methods 221
many other algorithms are known for this problem. There exist simple and complicatedvariants and improvements, gaining factors of 5 or 10 over the simple channel algorithm. See for instance, DeRemer and Pennello [LR 1982] and the Park, Choe andChang [LR 1985, 1987] versus Ives [LR 1986, 1987] discussion. Bermudez and
Logothetis [LR 1989] present a remarkably elegant interpretation of LALR(1) parsing.
9.7.4 Incremental parsingIn incremental parsing, the structured input (a program text, a structured document,
etc.) is kept in linear form together with a parse tree. When the input is (incrementally)modified by the user, for instance, by typing or deleting a character, it is the task of the
incremental parser to update the corresponding parse tree, preferably at minimum cost.This requires serious measures inside the parser, to quickly determine the extent of the
damage done to the parse tree, localize its effect and take remedial steps. Formalrequirements for the grammar to make this easier have been found. See for instance,
Degano, Mannucci and Mojana [LR 1988] and many others in Section 13.6.
9.7.5 Incremental parser generationIn incremental parser generation, the parser generator keeps the grammar together with
its parsing table(s) and has to respond quickly to user-made changes in the grammar, byupdating and checking the tables. Research on this is in its infancy. See Heering, Klint
and Rekers [LR 1989] and Horspool [LR 1989].
9.7.6 LR-regularRather than trying to resolve inadequate states by looking ahead a fixed number of
tokens, we can have an FS automaton for each inadequate state that is sent off up theinput stream; the state in which this automaton stops is used as a look-ahead. This parsing technique is called LR-regular. See C^ ulik and Cohen [LR 1973].A variant of this method reads in the entire input into an array and runs a single
FS automaton (derived from the grammar) backwards over the array, recording thestate of the automaton with each token. Next, during (forward) LR parsing, these
recorded states rather than the tokens are used as look-aheads.
9.7.7 Recursive ascentIn Sections 8.2.6 and 8.4 we have seen that an LL parser can be implemented conveniently using recursive descent. Analogously, an LR parser can be implementedusing recursive ascent, but the required technique is not nearly as obvious as in the LL
case. The key idea is to have the recursion stack mimic the LR parsing stack. To thisend there is a procedure for each state; when a token is to be shifted to the stack, the
procedure corresponding to the resulting state is called instead. This indeed constructsthe correct recursion stack, but causes problems when a reduction has to take place: a
dynamically determined number of procedures has to return in order to unstack theright-hand side. A simple technique to achieve this is to have two global variables,
one, Nt, holding the non-terminal recognized and the second, l, holding the length ofthe right-hand side. All procedures will check l and if it is non-zero, they will decrease
l by one and return immediately. Once l is zero, the procedure that finds that situationwill call the appropriate state procedure based on Nt. For details see Roberts [LR 1988,
1989, 1990] and Kruseman Aretz [LR 1988]. The advantage of recursive ascent overtable-driven is its potential for high-speed parsing.

222 Deterministic bottom-up parsing [Ch. 9
9.8 TOMITA'S PARSER
Now that we have seen the precise criteria for the existence of an LR-like parser for agrammar, i.e., that there is a handle-recognizing finite-state automaton with no inadequate states for that grammar, we become interested in the grammars for which the cri-teria are not completely fulfilled and for which the automaton has some inadequate
states. Tomita [CF 1986] has given an efficient and very effective approach to suchgrammars.

Tomita's method can be summarized as doing breadth-first search as in Section7.1.2 over those parsing decisions that are not solved by the LR automaton (which can
be LR(1), LALR(1), SLR(1), LR(0), precedence or even simpler), while at the sametime keeping the partial parse trees in a form akin to the common representation of
Section 7.1.3. More precisely, whenever an inadequate state is encountered on the topof the stack, the following steps are taken:
1. For each possible reduce in the state, a copy of the stack is made and the reduce isapplied to it. This removes part of the right end of the stack and replaces it with a

non-terminal; using this non-terminal as a move in the automaton, we find a newstate to put on the top of the stack. If this state allows again reductions, this copy
step is repeated until all reduces have been treated, resulting in equally many stackcopies.
2. Stacks that have a right-most state that does not allow a shift on the next inputtoken are discarded (since they resulted from incorrect guesses). Copies of the

next input token are shifted onto the remaining stacks.There are a number of things to be noted here. First, if the automaton uses look-ahead,
this is of course taken into account in deciding which reduces are possible in step 1(ignoring this information would not be incorrect but would cause more stacks to be
copied and subsequently discarded). Second, the process in step 1 may not terminate.If a grammar has loops (rules of the form A o"" B, B o"" A) reduction will alternate
between A and B. There are two solutions: upon creating a stack, check if it is alreadythere (and then ignore it) or check the grammar in advance for loops (and then reject
it). Third, if all stacks are discarded in step 2 the input was in error, at that specificpoint.

SSSS -->> EE ##

EE -->> EE ++ EE
EE -->> dd

Figure 9.38 A moderately ambiguous grammar
The above forms the basic mechanism of the Tomita parser. Since simple stackduplication may cause a proliferation of stacks and is apt to duplicate much information
that is not in need of duplication, two optimizations are used in the practical form of theparser: combining equal states and combining equal stack prefixes. We shall demonstrate all three techniques using the grammar of Figure 9.38 as an example. The gram-mar is a variant of that of Figure 3.1 and is moderately ambiguous. Its LR(0) automaton is shown in Figure 9.39; it has one inadequate state, A". Since the grammar is ambi-guous, there is not much point in using a stronger LR method. For more (and larger!)
examples see Tomita [CF 1986].

Sec. 9.8] Tomita's parser 223

SS-->>

\Gamma \Gamma  EE##

EE-->>

\Gamma \Gamma  EE++EE

EE-->>

\Gamma \Gamma  dd

1

EE SS-->>EE

\Gamma \Gamma  ##

EE-->>EE

\Gamma \Gamma  ++EE

3

++ EE-->>EE++

\Gamma \Gamma  EE

EE-->>

\Gamma \Gamma  EE++EE

EE-->>

\Gamma \Gamma  dd

4

dd
EE-->>dd

\Gamma \Gamma 

2 dd

SS-->>EE##

\Gamma \Gamma 

6

##

EE
EE-->>EE++EE

\Gamma \Gamma 

EE-->>EE

\Gamma \Gamma  ++EE

58

++

Figure 9.39 LR(0) automaton to the grammar of Figure 9.38
9.8.1 Stack duplicationRefer to Figure 9.40, in which we assume the input

dd++dd++dd##. The automaton starts instate
A` (a). The steps shift (b), reduce, shift, shift (c) and reduce (d) are problem-freeand bring us to state

A". The last state, however, is inadequate, allowing a reduce and ashift. True to the breadth-first search method and in accordance with step 1 above, the

stack is now duplicated and the top of one of the copies is reduced (e1) while the otherone is left available for a subsequent shift (e2). Note that no further reduction is possible and that both stacks now have a different top state. Both states allow a shift andthen another (f1, f2) and then a reduce (g1, g2). Now both stacks carry an inadequate
state on top and need to be duplicated, after which operation one of the copies under-goes a reduction (h1.1, h1.2, h2.1, h2.2). It now turns out that the stack in h2.1 again
features an inadequate state A" after the reduction; it will again have to be duplicatedand have one copy reduced. This gives the stack in h2.1a. Now all possible reductions
have been done and it is time for a shift again. Only state A^ allows a shift on ##, so theother stacks are discarded and we are left with i1.1 and i2.1a. Both require a reduction,
yielding j1.1 and j2.1a, which are accepting states. The parser stops and has found twoparsings.

In order to save space and to avoid cluttering up the pictures, we have not shownthe partial parse trees that resulted from the various reductions that have taken place. If
we had done so, we would have found the two SS's in j.1.1 and j.2.1a holding the parsetrees of Figure 9.41.

9.8.2 Combining equal statesExamining Figure 9.40 f and g, we see that once both stacks have the same state on top,
further actions on both stacks will be identical, and the idea suggests itself to combinethe two stacks to avoid duplicate work. This approach is depicted in Figure 9.42(f) and
(g) (Figure 9.42(a) to (e) are identical to those of Figure 9.40 and are not shown). Thatthis is, however, not entirely without problems becomes evident as soon as we need to
do a reduce that spans the merge point. This happens in (g), which also features aninadequate state. Now a number of things happen. First, since the state is inadequate,
the whole set of combined stacks connected to it are duplicated. One copy (h3) is leftfor the shift, the other is subjected to the reduce. This reduce, however, spans the merge

224 Deterministic bottom-up parsing [Ch. 9
a A` dd++dd++dd## shiftb

A` dd A' ++dd++dd## reduce, shift, shiftc
A` EE A^ ++ A~ dd A' ++dd## reduced
A` EE A^ ++ A~ EE A" ++dd## duplicate to e1 ande2; reduce e1

e1 A` EE A^ ++dd## shift, shift, to f1e2

A` EE A^ ++ A~ EE A" ++dd## shift, shift, to f2f1
A` EE A^ ++ A~ dd A' ## reduce to g1f2
A` EE A^ ++ A~ EE A" ++ A~ dd A' ## reduce to g2g1
A` EE A^ ++ A~ EE A" ## duplicate to h1.1 andh1.2; reduce h1.1

g2 A` EE A^ ++ A~ EE A" ++ A~ EE A" ## duplicate to h2.1 andh2.2; reduce h2.1
h1.1 A` EE A^ ## shift to i1.1h1.2

A` EE A^ ++ A~ EE A" ## discardh2.1
A` EE A^ ++ A~ EE A" ## reduce again, to h2.1ah2.2
A` EE A^ ++ A~ EE A" ++ A~ EE A" ## discardh2.1a
A` EE A^ ## shift to i2.1ai1.1
A` EE A^ ## A* reduce to j1.1i2.1a
A` EE A^ ## A* reduce to j2.1aj1.1
A` SS acceptj2.1a
A` SS accept

Figure 9.40 Sequence of stack configurations while parsing dd++dd++dd##

SS
EE ##
EE ++ EE
EE ++ EE dd
dd dd

SS
EE ##
EE ++ EE
dd EE ++ EE

dd dd

Figure 9.41 Parse trees in the accepting states of Figure 9.40
point (state A~) and extends up both stacks, comprising a different left-most EE in bothbranches. To perform it properly, the stack combination is undone and the reduce is
applied to both stacks (h1, h2). The reduce in (h2) results again in state A", whichnecessitates another copy operation (h2.1, h2.2) and a reduce on one of the copies
(h2.1).Now the smoke has cleared and we have obtained five stacks (h1, h2.1, h2.2 and a
double h3) having four tops, two of which (h1 and h2.1) carry the state A^, while theother two (h2.2 and h3) carry a

A". These can be combined into two bundles (h' and h").Next the shift of
## obliterates all stacks with top state A" (i). State A*, which is now on

Sec. 9.8] Tomita's parser 225
f A` EE A^ ++ A~ dd A' ## reduce to gA` EE A^ ++ A~ EE A" ++
g A` EE A^ ++ A~ EE A" ## duplicate to g', g"A` EE A^ ++ A~ EE A" ++
g' A` EE A^ ++ A~ EE A" ## for reduce; undoA` EE A^ ++ A~ EE A" ++ combination: g'.1, g'.2
g" A` EE A^ ++ A~ EE A" ## for shift: h3A` EE A^ ++ A~ EE A" ++
g'.1 A` EE A^ ++ A~ EE A" ## reduce to h.1
g'.2 A` EE A^ ++ A~ EE A" ++ A~ EE A" ## reduce to h.2
h1 A` EE A^ ## shift
h2 A` EE A^ ++ A~ EE A" ## reduce again, h2.1, h2.2

h3 A` EE A^ ++ A~ EE A" ## shiftA` EE A^ ++ A~ EE A" ++
h2.1 A` EE A^ ## shift
h2.2 A` EE A^ ++ A~ EE A" ## shift

h' A` EE A^ ## shift to iA` EE

A` EE A^ ++h"
A` EE A^ ++ A~ EE A" ++ A~ EE A" ## discard
A` EE A^ ++ A~ EE

i A` EE A^ ## A* for reduce; undoA` EE combination, i', i"
i' A` EE A^ ## A* reduce to j1i"

A` EE A^ ## A* reduce to j2

j1 A` SS acceptj2

A` SS accept

Figure 9.42 Stack configurations with equal-state combination

226 Deterministic bottom-up parsing [Ch. 9
top, induces a reduce spanning a merge point, the combined stack is split and thereduce is applied to both stacks, resulting in the two parsings for

dd++dd++dd## (j1, j2).Although in this example the stack combinations are undone almost as fast as they

are performed, stack combination greatly contributes to the parsers efficiency in thegeneral case. It is essential in preventing exponential growth wherever possible. Note,
however, that, even though the state A^ in i is preceded by EE in all branches, we cannotcombine these

EE's since they differ in the partial parse trees attached to them.

9.8.3 Combining equal stack prefixesWhen step 1 above calls for the stack to be copied, there is actually no need to copy the
entire stack; just copying the top states suffices. When we duplicate the stack of Figure9.40(d), we have one forked stack for (e):

e' A` EE A^ ++ A~ EE A" ++dd##

A"

Now the reduce is applied to one top state A" and only so much of the stack is copied asis subject to the reduce:

e A` EE A^ ++dd## shift

EE A^ ++ A~ EE A" shift

In our example almost the entire stack gets copied, but if the stack is somewhat larger,considerable savings can result.

Note that the title of this section is in fact incorrect: in practice no equal stack pre-fixes are combined, they are never created in the first place. The pseudo-need for combination arises from our wish to explain first the simpler but impractical form of thealgorithm in Section 9.8. A better name for the technique would be "common stack
prefix preservation".Both optimizations can combine to produce shuntyard-like stack constellations
like the one in Figure 9.43; here Tomita's notation is used, in which \Gamma  represents astate and

\Delta  a symbol. The reader may verify that the constellation represents sevenstacks.

\Gamma 

\Gamma \Theta 
\Gamma 

\Gamma 
\Theta 

\Gamma  \Gamma \Theta \Gamma 

\Gamma 
\Theta 

\Gamma  \Gamma \Theta 

\Gamma  \Gamma \Theta 

\Gamma  \Gamma \Theta 
\Gamma 

\Gamma \Theta 
\Gamma  \Gamma \Theta 
\Gamma  \Gamma \Theta 

\Gamma  \Gamma \Theta 
\Gamma 

\Gamma \Theta 
\Gamma  \Gamma \Theta 

\Gamma  \Gamma \Theta 
\Gamma  \Gamma \Theta 
\Gamma 

\Gamma \Theta 
\Gamma  \Gamma \Theta 

\Gamma  \Gamma \Theta 
\Gamma 

\Gamma \Theta 
\Gamma 

\Gamma 
\Theta 

\Gamma  \Gamma \Theta 

Figure 9.43 Stack constellation with combined heads and tails, in Tomita's notation
9.8.4 DiscussionWe have explained Tomita's parser using an LR(0) table; in his book Tomita uses an
SLR(1) table. In fact the method will work with any bottom-up table or even with notable at all. The weaker the table, the more non-determinism will have to be resolved
by breadth-first search, and for the weakest of all tables, the absent table, the method

Sec. 9.8] Tomita's parser 227
degenerates into full breadth-first search. Since the latter is involved in principle in allvariants of the method, the time requirements are in theory exponential; in practice they
are very modest, generally linear or slightly more than linear and almost always lessthan those of Earley's parser or of the CYK parser, except for very ambiguous grammars.

9.9 NON-CANONICAL PARSERS
All parsers treated so far in this chapter are "canonical parsers", which means that theyidentify the productions in reverse right-most order. A "non-canonical parser" identifies the productions in arbitrary order, or rather in an unrestricted order. Removing therestriction on the identification order of the productions makes the parsing method
stronger, as can be expected. Realistic examples are too complicated to be shown here(see Tai [LR 1979] for some), but the following example will demonstrate the principle.

SSSS -->> PP QQ || RR SS

PP -->> aa
QQ -->> bb cc
RR -->> aa
SS -->> bb dd

Figure 9.44 A short grammar for non-canonical parsing
The grammar of Figure 9.44 produces two sentences, aabbcc and aabbdd. Suppose the inputis

aabbcc. The aa can be a PP or an RR; for both, the look-ahead is a bb, so an LR(1) parsercannot decide whether to reduce to

PP or to RR and the grammar is not LR(1). Suppose,however, that we leave the undecidable undecided and search on for another reducible

part (called a phrase in non-canonical parsing to distinguish it from the "handle").Then we find the tokens

bbcc, which can clearly be reduced to QQ. Now, this QQ providesthe decisive look-ahead for the reduction of the

aa. Since PP can be followed by a QQ and RRcannot, reduction to
PP is indicated based on look-ahead QQ; the grammar is NCLR(1)(Non-Canonical LR(1)). We see that in non-canonical parsers the look-ahead sets contain non-terminals as well as terminals.There are disadvantages too. After each reduce, one has to rescan possibly large
parts of the stack. This may jeopardize the linear time requirement, although with somedexterity the problem can often be avoided. A second problem is that rules are recognized in essentially arbitrary order which makes it difficult to attach semantics to them.A third point is that although non-canonical parsers are more powerful than canonical
ones, they are only marginally so: most grammars that are not LR(1) are not NCLR(1)either.

Overall the advantages do not seem to outweigh the disadvantages and non-canonical parsers are not used often. See, however, Salomon and Cormack [LR 1989].
Non-canonical precedence parsing has been described by Colmerauer [Precedence1970].

228 Deterministic bottom-up parsing [Ch. 9
9.10 LR(k) AS AN AMBIGUITY TEST
It is often important to be sure that a grammar is not ambiguous, but unfortunately itcan be proved that there cannot be an algorithm that can, for every CF grammar, decide

whether it is ambiguous or unambiguous. This is comparable to the situation describedin 3.5.2, where the fundamental impossibility of a recognizer for Type 0 grammars was
discussed. (See Hopcroft and Ullman [Books 1979, p. 200]). The most effective ambi-guity test for a CF grammar we have at present is the construction of the corresponding
LR(k) automaton, but it is of course not a perfect test: if the construction succeeds, thegrammar is guaranteed to be unambiguous; if it fails, in principle nothing is known. In
practice, however, the reported conflicts will often point to genuine ambiguities.Theoretically, the construction of an LR-regular parser (see 9.7.6) is an even stronger
test, but the choice of the look-ahead automaton is problematic.

10
Error handling

Until now, we have discussed parsing techniques while largely ignoring what happenswhen the input contains errors. In practice, however, the input often contains errors,
the most common being typing errors and misconceptions, but we could also be dealingwith a grammar that only roughly, not precisely, describes the input, for instance in pattern matching. So, the question arises how to deal with errors. A considerable amountof research has been done on this subject, far too much to discuss in one chapter. We
will therefore limit our discussion to some of the more well-known error handlingmethods, and not pretend to cover the field; see Section 13.11 for more in-depth information.

10.1 DETECTION VERSUS RECOVERY VERSUS CORRECTION
Usually, the least that is required of a parser is that it detects the occurrence of one ormore errors in the input, that is, we require error detection. The least informative version of this is that the parser announces: "input contains syntax error(s)". We say thatthe input contains a syntax error when the input is not a sentence of the language
described by the grammar. All parsers discussed in the previous chapters (exceptoperator-precedence) are capable of detecting this situation without extensive modification. However, there are few circumstances in which this behaviour is acceptable:when we have just typed a long sentence, or a complete computer program, and the
parser only tells us that there is a syntax error somewhere, we will not be pleased at all,not only about the syntax error, but also about the quality of the parser or lack thereof.

The question as to where the error occurs is much more difficult to answer; in factit is almost impossible. Although some parsers have the correct-prefix property, which
means that they detect an error at the first symbol in the input that results in a prefixthat cannot start a sentence of the language, we cannot be sure that this indeed is the
place in which the error occurs. It could very well be that there is an error somewherebefore this symbol but that this is not a syntax error at that point. There is a difference
in the perception of an error between the parser and the user. In the rest of this chapter,when we talk about errors, we mean syntax errors, as detected by the parser.

So, what happens when input containing errors is offered to a parser with a gooderror detection capability? The parser might say: "Look, there is a syntax error at position so-and-so in the input, so I give up". For some applications, especially highly

230 Error handling [Ch. 10
interactive ones, this may be satisfactory. For many, though, it is not: often, one wouldlike to know about all syntax errors in the input, not just about the first one. If the
parser is to detect further syntax errors in the input, it must be able to continue parsing(or at least recognizing) after the first error. It is probably not good enough to just
throw away the offending symbol and continue. Somehow, the internal state of theparser must be adapted so that the parser can process the rest of the input. This adaptation of the internal state is called error recovery.The purpose of error recovery can be summarized as follows:
\Gamma  an attempt must be made to detect all syntax errors in the input;
\Gamma  equally important, an attempt must be made to avoid spurious error messages.

These are messages about errors that are not real errors in the input, but resultfrom the continuation of the parser after an error with improper adaptation of its

internal state.Usually, a parser with an error recovery method can no longer deliver a parse tree
if the input contains errors. This is sometimes the cause of considerable trouble. In thepresence of errors, the adaptation of the internal state can cause semantic actions associated with grammar rules to be executed in an order that is impossible for syntacticallycorrect input, which sometimes leads to unexpected results. A simple solution to this
problem is to ignore semantic actions as soon as a syntax error is detected, but this isnot optimal and may not be acceptable. A better option is the use of a particular kind
of error recovery method, an error correction method.Error correction methods transform the input into syntactically correct input, usually by deleting, inserting, or changing symbols. It should be stressed that error correc-tion methods cannot always change the input into the input actually intended by the
user, nor do they pretend that they can. Therefore, some authors prefer to call thesemethods error repair methods rather than error correction methods. The main advantage of error correction over other types of error recovery is that the parser still canproduce a parse tree and that the semantic actions associated with the grammar rules
are executed in an order that could also occur for some syntactically correct input. Infact, the actions only see syntactically correct input, sometimes produced by the user
and sometimes by the error corrector.In summary, error detection, error recovery, and error correction require increasing levels of heuristics. Error detection itself requires no heuristics. A parser detects anerror, or it does not. Determining the place where the error occurs may require heuristics, however. Error recovery requires heuristics to adapt the internal parser state sothat it can continue, and error correction requires heuristics to repair the input.

10.2 PARSING TECHNIQUES AND ERROR DETECTION
Let us first examine how good the parsing techniques discussed in this book are atdetecting an error. We will see that some parsing techniques have the correct-prefix

property while other parsing techniques only detect that the input contains an error butgive no indication where the error occurs.

10.2.1 Error detection in non-directional parsing methodsIn Section 4.1 we saw that Unger's parsing method tries to find a partition of the input
sentence that matches one of the right-hand sides of the start symbol. The only thingthat we can be sure of in the case of one or more syntax errors is, that we will find no

Sec. 10.2] Parsing techniques and error detection 231
such partition. For example, suppose we have the grammar of Figure 4.1, repeated inFigure 10.1, and input **++.

EExxpprrSS -->> EExxpprr ++ TTeerrmm || TTeerrmm

TTeerrmm -->> TTeerrmm ** FFaaccttoorr || FFaaccttoorr
FFaaccttoorr -->> (( EExxpprr )) || ii

Figure 10.1 A grammar describing simple arithmetic expressions
Fitting the first right-hand side of EExxpprr with the input will not work, because the inputonly has two symbols. We will have to try the second right-hand side of

EExxpprr. Like-wise, we will have to try the second right-hand side of
TTeerrmm, and then we will find thatwe cannot find an applicable right-hand side of
FFaaccttoorr, because the first one requiresat least three symbols, and the second one only one. So, we know that there are one or

more errors, but we do not know how many errors there are, nor where they occur. In away, Unger's method is too well prepared for dealing with failures, because it expects
any partition to fail.For the CYK parser, the situation is similar. We will find that if the input contains
errors, the start symbol will not be a member of the top element of the recognitiontable.

So, the unmodified non-directional methods behave poorly on errors in the input.
10.2.2 Error detection in finite-state automataFinite-state automata are very good at detecting errors. Consider for instance the deterministic automaton of Figure 5.10, repeated in Figure 10.2.

SS AABB

BBCC

AACC

a`a`a`a`aa
bb

cc

aa
aa
ccbb

Figure 10.2 Deterministic automaton for the grammar of Figure 5.5
When this automaton is offered the input aabbccccaa, it will detect an error when it is instate

AACC, on the second cc in the input.Finite-state automata have the correct-prefix property. In fact, they have the

immediate error detection property, which we discussed in Chapter 8 and which meansthat an error is detected as soon as the erroneous symbol is first examined.

10.2.3 Error detection in general directional top-down parsersThe breadth-first general directional top-down parser also has the correct-prefix property. It stops as soon as there are no predictions left to work with. Predictions are onlydropped by failing match steps, and as long as there are predictions, the part of the
input parsed so far is a prefix of some sentence of the language.The depth-first general directional top-down parser does not have this property. It

232 Error handling [Ch. 10
will backtrack until all right-hand sides of the start symbol have failed. However, it caneasily be doctored so that it does have the correct-prefix property: the only thing that
we must remember is the furthest point in the input that the parser has reached, a kindof high-water mark. The first error is found right after this point.

10.2.4 Error detection in general directional bottom-up parsersThe picture is quite different for the general directional bottom-up parsers. They will
just find that they cannot reduce the input to the start symbol. This is only to beexpected because, in contrast to the top-down parsers, there is no test before an input
symbol is shifted.As soon as a top-down component is added, such as in Earley's parser, the parser
regains the correct-prefix property. For instance, if we use the Earley parser with thegrammar from Figure 7.8 and input

aa--++aa, we get the items sets of Figure 10.3 (com-pare this with Figure 7.11). Itemset

3 will be empty, and an error is detected.

SS-->>

\Gamma \Gamma  EE @@11

EE-->>

\Gamma \Gamma  EEQQFF@@11

EE-->>

\Gamma \Gamma  FF @@11

FF-->>

\Gamma \Gamma  aa @@11

. . . . . . . . . . . . . . . . . .

act/pred0

= itemset0

aa1

FF-->>aa

\Gamma \Gamma  @@11

EE-->>FF

\Gamma \Gamma  @@11

SS-->>EE

\Gamma \Gamma  @@11

completed1

EE-->>EE

\Gamma \Gamma  QQFF@@11

QQ-->>

\Gamma \Gamma  ++ @@22

QQ-->>

\Gamma \Gamma  -- @@22

. . . . . . . . . . . . . . . . . .

act/pred1

= itemset1

--2

QQ-->>--

\Gamma \Gamma  @@22completed2

EE-->>EEQQ

\Gamma \Gamma  FF@@11

FF-->>

\Gamma \Gamma  aa @@33. . . . . . . . . . . . . . . . . .

act/pred2

= itemset2

++3

completed3

. . . . . . . . . . . . . . . . . .

act/pred3

= itemset3

Figure 10.3 Items sets of the Earley parser working on aa--++aa
10.2.5 Error detection in deterministic top-down parsersIn Sections 8.2.3 and 8.2.4 we have seen that strong-LL(1) parsers have the correctprefix property but not the immediate error detection property, because in some cir-cumstances they may make some e-moves before detecting an error, and that full LL(1)
parsers have the immediate error detection property.
10.2.6 Error detection in deterministic bottom-up parsersLet us first examine the error detection capabilities of precedence parsers. We saw in
Section 9.2.1 that operator-precedence parsers fail to detect some errors. When they dodetect an error, it is because there is no precedence relation between the symbol on top
of the parse stack and the next input symbol. This is called a character-pair error.The other precedence parsers (simple, weak, extended, and bounded-context) have
three error situations:
\Gamma  there is no precedence relation between the symbol on top of the parse stack and

the next input symbol (a character-pair error).
\Gamma  the precedence relations indicate that a handle is found and that a reduction must

be applied, but there is no non-terminal with a right-hand side that matches thehandle. This is called a reduction error.

\Gamma  after a reduction has been made, there is no precedence relation between the symbol at the top of the stack (the symbol that was underneath the <*) and the left-handside to be pushed. This is called a stackability error.

Sec. 10.2] Parsing techniques and error detection 233

Reduction errors can be detected at an early stage by continuously checking thatthe symbols between the last <* and the top of the stack form the prefix of some righthand side. Graham and Rhodes [ErrHandl 1975] show that this can be done quite effi-ciently.

In Section 9.5.2 we saw that an LR(1) parser has the immediate error detectionproperty. LALR(1) and SLR(1) parsers do not have this property, but they do have the
correct prefix property. Error detection in Tomita's parser depends on the underlyingparsing technique.

10.3 RECOVERING FROM ERRORS
Error handling methods fall in different classes, depending on what level they approachthe error. The general parsers usually apply an error handling method that considers the

complete input. These methods use global context, and are therefore called globalerror handling methods. The Unger and CYK parsers need such a method, because
they have no idea where the error occurred. These methods are very effective, but thepenalty for this effectivity is paid for in efficiency: they are very time consuming,
requiring at least cubic time. As the general parsing methods already are time consum-ing anyway, this is usually deemed acceptable. We will discuss such a method in Section 10.4.On the other hand, efficient parsers are used because they are efficient. For them,
error handling methods are required that are less expensive. We will discuss the bestknown of these methods.

These methods have the following information at their disposal:
\Gamma  in the case of a bottom-up parser: the parse stack; in the case of a top-down

parser: the prediction;
\Gamma  the input string, and the point where the error was detected.

There are four classes of these methods: the ad hoc methods, which do not reallyform a class; the regional error handling methods, which use some (regional) context

around the point of error detection to determine how to proceed; the local error han-dling methods only use the parser state and the input symbol (local context) to determine what happens next; and the suffix methods, which use zero context. Examples ofthese methods will be discussed in Sections 10.5, 10.6, 10.7 and 10.8.

In our discussions, we will use the terms error detection point, indicating the pointwhere the parser detects the error, and error symbol, which indicates the input symbol
on which the error is detected.

10.4 GLOBAL ERROR HANDLING
The most well-known global error handling method is the least-error correctionmethod. The purpose of this method is to derive a syntactically correct input from the

supplied one using as few corrections as possible. Usually, a symbol deletion, a symbolinsertion, and a symbol change all count as one correction (one edit operation).

It is important to realize that the number of corrections needed can easily be lim-ited to a maximum: first, we compute the shortest sentence that can be generated from
the grammar. Let us say it has length m. If the input has length n, we can change thisinput into the shortest sentence with a number of edit operations that is the maximum
of m and n: change the first symbol of the input into the first symbol of the shortest

234 Error handling [Ch. 10
sentence, etc. If the input is shorter than the shortest sentence, this results in a max-imum of n changes, and we have to insert the last m -n symbols of the shortest sentence. If the input is longer than the shortest sentence, we have to delete the last n -msymbols of the input.

Another important point is that, when searching for a least-error correction, if wealready know that we can do it with, say, k corrections, we do not have to investigate
possibilities known to require more.With this in mind, let us see how such an error correction method works when
incorporated in an Unger parser. We will again use the grammar of Figure 10.1 as anexample, again with input sentence **++. This is a very short sentence indeed, to limit the
amount of work. The shortest sentence that can be generated from the grammar is ii, oflength one. The observation above limits the number of corrections needed to a maximum of two.Now, the first rule to be tried is

EExxpprr -->> EExxpprr ++ TTeerrmm. This leads to the fol-lowing partitions:

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr max:2
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr ++ TTeerrmm
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma ? 1 **++ ?

? ** 1 ++ ?? **++ 1 ?
** ? 1 ++ ?** ? ++ 0 ?
**++ ? 1 ? cut-off\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

Notice that we include the number of corrections needed for each part of a partition inthe right of the column, a question mark indicating that the number of corrections is yet
unknown. The total number of corrections needed for a certain partition is the sum ofthe number of corrections needed for each of the parts. The top of the table also contains the maximum number of corrections allowed for the rule. For the parts matchinga terminal, we can decide how many corrections are needed, which results in the
column below the ++. Also notice that we have to consider empty parts, although thegrammar does not have e-rules. The empty parts stand for insertions. The cut-off
comes from the Unger parser detecting that the same problem is already being exam-ined.

Now, the Unger parser continues by trying to derive e from EExxpprr. The currentpartition already requires one correction, so the maximum number of corrections
allowed is now one. The rule EExxpprr -->> EExxpprr ++ TTeerrmm has the following result:

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr max:1
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr ++ TTeerrmm
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma ? 1 ? cut-off

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 

\Theta \Theta 

so we will have to try the other rule for EExxpprr: EExxpprr -->> TTeerrmm. Likewise, TTeerrmm -->>
TTeerrmm ** FFaaccttoorr will result in a cut-off, so we will have to use TTeerrmm -->> FFaaccttoorr.The rule

FFaaccttoorr -->> (( EExxpprr )) will again result in a cut-off, so FFaaccttoorr -->> ii willbe used:

Sec. 10.4] Global error handling 235

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr max:1
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma TTeerrmm max:1
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma FFaaccttoorr max:1
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma ii max:1
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 1

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 

\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta 

So, we find, not surprisingly, that input part e can be corrected to ii, requiring onecorrection (inserting

ii) to make it derivable from EExxpprr (and TTeerrmm and FFaaccttoorr). Tocomplete our work on the first partition of **++ over the right-hand side

EExxpprr ++ TTeerrmm,we have to examine if, and how,
TTeerrmm derives **++. We already need two correctionsfor this partition, so no more corrections are allowed because of the maximum of two.

For the rule TTeerrmm -->> TTeerrmm ** FFaaccttoorr we get the following partitions (in which wecheated a bit: we used some information computed earlier):

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma TTeerrmm max:0
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma TTeerrmm ** FFaaccttoorr
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 1 1 **++ ? too many corrections

1 ** 0 ++ ? too many corrections1 **++ 1 1 too many corrections
** ? 1 ++ ? too many corrections** ? ++ 1 1 too many corrections
**++ ? 1 1 cut-off\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

So, we will have to try TTeerrmm -->> FFaaccttoorr. After that, FFaaccttoorr -->> (( EExxpprr )) resultsin the following partitions:

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma TTeerrmm max:0
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma FFaaccttoorr max:0
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma (( EExxpprr ))
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 1 1 **++ 2 too many corrections

1 ** ? ++ 1 too many corrections1 **++ ? 1 cut-off
** 1 1 ++ 1 too many corrections** 1 ++ ? 1 too many corrections
**++ 2 1 1 too many corrections\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

This does not work either. The rule FFaaccttoorr -->> ii results in the following:

\Gamma \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma TTeerrmm max:0
\Gamma \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma FFaaccttoorr max:0
\Gamma \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma ii max:0
\Gamma \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma **++ 2 too many corrections

\Gamma \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 

So we get either a cut-off or too many corrections (or both). This means that the parti-tion that we started with is the wrong one. The other partitions are tried in a similar

236 Error handling [Ch. 10
way, resulting in the following partition table, with completed error correction counts:

\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr max:2
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma EExxpprr ++ TTeerrmm
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma 1 1 **++ >0 too many corrections

1 ** 1 ++ 1 too many corrections1 **++ 1 1 too many corrections
** 1 1 ++ 1 too many corrections** 1 ++ 0 1
**++ ? 1 1 cut-off\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 

\Theta \Theta 
\Theta \Theta 
\Theta 

\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 
\Theta \Theta 

So, provided that we do not find better corrections later on, using the rule EExxpprr -->>
EExxpprr ++ TTeerrmm we find the corrected sentence ii++ii, by replacing the ** with an ii, andinserting an

ii at the end of the input. Now, the Unger parser proceeds by trying therule
EExxpprr -->> TTeerrmm. Continuing this process, we will find two more possibilitiesusing two corrections: the input can be corrected to

ii**ii by inserting an ii in front of theinput and replacing the ++ with another
ii, or the input can be corrected by replacing **with an
ii and deleting ++ (or deleting ** and replacing ++ with an ii).This results in three possible corrections for the input, all three requiring two edit

operations. Choosing between these corrections is up to the parser writer. If the parseris written to handle ambiguous input anyway, the parser might deliver three parse trees
for the three different corrections. If the parser must deliver only one parse tree, itcould just pick the first one found. Even in this case, however, the parser has to continue searching until it has exhausted all possibilities or it has found a correct parsing,because it is not until then that the parser knows if the input in fact did contain any
errors.As is probably clear by now, least-errors correction does not come cheap, and it is
therefore usually only applied in general parsers, because these do not come cheap any-way.

Lyon [ErrHandl 1974] has added least-errors correction to the CYK parser and theEarley parser, although his CYK parser only handles replacement errors. In his version
of the CYK parser, the non-terminals in the recognition table have an error count asso-ciated with it. In the bottom row, which is the one for the non-terminals deriving a single terminal symbol, all entries contain all non-terminals that derive a single terminalsymbol. If the non-terminal derives the corresponding terminal symbol it has error
count 0, otherwise it has error count 1 (a replacement). Now, when we find that anon-terminal A with rule A o""BC is applicable, it is entered in the recognition table with
an error count equal to the sum of that of B and C, but only if it is not already a memberof the same recognition table entry, but with a lower error count.

Aho and Peterson [ErrHandl 1972] also added least-errors correction to the Earleyparser by extending the grammar with error productions, so that it produces any string
of terminal symbols, with an error count. As in Lyon's method, the Earley items areextended with an error count indicating how many corrections were needed to create
the item. An item is only added to an item set if it does not contain one like it whichhas a lower error count.

Sec. 10.4] Ad hoc methods 237
10.5 AD HOC METHODS
The ad hoc error recovery methods are called ad hoc because they cannot be automati-cally generated from the grammar. These methods are as good as the parser writer

makes them, which in turn depends on how good the parser writer is in anticipatingpossible syntax errors. We will discuss three of these ad hoc methods: error productions, empty table slots and error tokens.
10.5.1 Error productionsError productions are grammar rules, added by the grammar writer so that anticipated
syntax errors become part of the language (and thus are no longer syntax errors). Theseerror productions usually have a semantic action associated with them that reports the
error; this action is triggered when the error production is used. An example where anerror production could be useful is the Pascal if-statement. The latter has the following
syntax:

iiff--ssttaatteemmeenntt -->> IIFF bboooolleeaann--eexxpprreessssiioonn

TTHHEENN ssttaatteemmeenntt eellssee--ppaarrtt
eellssee--ppaarrtt -->> EELLSSEE ssttaatteemmeenntt || ee

A common error is that an iiff--ssttaatteemmeenntt has an eellssee--ppaarrtt, but the statement infront of the

eellssee--ppaarrtt is terminated by a semicolon. In Pascal, a semicolon is a state-ment separator rather than a statement terminator and is not allowed in front of an

EELLSSEE. This situation could be detected by changing the grammar rule for eellssee--ppaarrttinto

eellssee--ppaarrtt -->> EELLSSEE ssttaatteemmeenntt || ee || ;; EELLSSEE ssttaatteemmeenntt
where the last right-hand side is the error production.The most important disadvantages of error productions are:
\Gamma  only anticipated errors can be handled;
\Gamma  the modified grammar might (no longer) be suitable for the parsing method used,

because conflicts could be introduced by the added rules.The advantage is that a very adequate error message can be given. Error productions

can be used profitably in conjunction with another error handling method, to handlesome frequent errors on which the other method does not perform well.

10.5.2 Empty table slotsIn most of the efficient parsing methods, the parser consults one or more parse tables
and bases its next parsing decision on the result. These parsing tables have errorentries (represented as the empty slots), and if one of these is consulted, an error is
detected. In this error handling method, the empty table slots are used to refer to errorhandling routines. Each empty slot has its own error handling routine, which is called
when the corresponding slot is consulted. The error handling routines themselves arewritten by the parser writer. By very careful design of these error handling routines,
very good results can be obtained; see for instance Conway and Wilcox [ErrHandl1973]. In order to achieve good results, however, the parser writer must invest considerable effort. Usually, this is not considered worth the gain, in particular because gooderror handling can be generated automatically.

238 Error handling [Ch. 10
10.5.3 Error tokensAnother popular error recovery method uses error tokens. An error token is a special
token that is inserted in front of the error detection point. The parser will pop statesfrom the parse stack until this token becomes valid, and then skip symbols from the
input until an acceptable symbol is found. The parser writer extends the grammar withrules using this error token. An example of this is the following grammar:

iinnppuutt -->> iinnppuutt iinnppuutt__lliinnee || ee
iinnppuutt__lliinnee -->> EERRRROORR__TTOOKKEENN NNEEWWLLIINNEE || SSTTRRIINNGG NNEEWWLLIINNEE

This kind of grammar is often seen in interactive applications, where the input is lineby line. Here,

EERRRROORR__TTOOKKEENN denotes the error token, and NNEEWWLLIINNEE denotes an end ofline marker. When an error occurs, states are popped until

EERRRROORR__TTOOKKEENN becomesacceptable, and then symbols are skipped until a
NNEEWWLLIINNEE is encountered.This method can be quite effective, provided that care is taken in designing the

rules using the error token.

10.6 REGIONAL ERROR HANDLING
In regional error handling, most often applied in bottom-up parsers, recovery fromerrors is done by collecting some context around the error detection point, usually as a

part of the parse stack around the error, and reducing that part (including the error) to aleft-hand side. Therefore, this class of error handling methods is also often called
phrase level error handling.
10.6.1 Backward/forward moveAn error handling method that is applicable to bottom-up parsers is the
backward/forward move error recovery method, presented by Graham and Rhodes[ErrHandl 1975]. It consists of two stages: the first stage condenses the context around
the error as much as possible. This is called the condensation phase. Then the secondstage, the correction phase, changes the parsing stack and/or the input so that parsing
can continue. The method is best applicable to simple precedence parsers, and we willuse such a parser as an example.

Our example comes from the grammar and precedence table of Figure 9.13. Sup-pose that we have input

##nn**++nn##. The simple precedence parser has the following parsestacks at the end of each step, up to the error detection point:

## <* nn >* shift nn, next symbol is **
## <* FF >* reduce nn
## <* TT =. ** reduce FF, shift **

No precedence relation is found to exist between the ** and the ++, resulting in an errormessage that ++ is not expected.

Let us now examine the condensation phase in some detail. As said before, thepurpose of this phase is to condense the context around the error as much as possible.
The left-context is condensed by a so-called backward move: assuming a >* relationbetween the top of the parse stack and the symbol on which the error is detected (that
is, assuming that the parse stack built so far has the end of a handle as its top element),

Sec. 10.6] Regional error handling 239
perform all possible reductions. In our example, no reductions are possible. Nowassume a =. or a <* between the top of the stack and the next symbol. This enables us to
continue parsing a bit. This step is the so-called forward move: first we shift the nextsymbol, resulting in the following parse stack:

## <* TT =. ** =. /<* ++ shift ++
Next, we disable the check that the top of the stack should represent a prefix of a right-hand side. Then, we continue parsing until either another error occurs or a reduction is
called for that spans the error detection point. This gives us some right-context to workwith, which can be condensed by a second backward move, if needed. For our example, this results in the following steps:

## <* TT =. ** =. /<* ++ <* nn >* ## shift nn, next symbol is ##
## <* TT =. ** =. /<* ++ <* FF >* ## reduce nn
## <* TT =. ** =. /<* ++ <* TT >* ## reduce FF
## <* TT =. ** =. /<* ++ =. TT'' >* ## reduce TT

So now we have the situation depicted in Figure 10.4.

. . . <* . . . . . .

nearest <* to the left ofthe error detection point error detectionpoint top ofstack

A` A'

A^

Figure 10.4 Situation after the backward/forward moves
This is where the correction phase starts. The correction phase considers three parts ofthe stack for replacement with some right-hand side. These parts are indicated with

A`,
A' and A^ in Figure 10.4. Part A` is considered because the precedence at the errordetection point could be >* , part

A' is considered because the precedence at the errordetection point could be <*, and part

A^ is considered because this precedence could be=. . Another option is to just delete one of these parts. This results in a fairly large

number of possible changes, which now must be limited by making sure that the parsercan continue after reducing the right-hand side to its corresponding left-hand side.

In the example, we have the following situation:

<* TT =. ** ?* ++ =. TT'' >*

A` A'

A^

The left-hand sides that could replace part A` are: EE, TT'', TT, and FF. These are the non-terminals that have a precedence relation with the next symbol: the ++. The only lefthand side that could replace part A' is FF. Part A^ could be replaced by EE, TT'', TT, and FF.

240 Error handling [Ch. 10
This still leaves a lot of choices, but some "corrections" are clearly better than others.Let us now see how we can discriminate between them.

Replacing part of the parse stack by a right-hand side can be seen as an edit opera-tion on the stack. The cost of this edit operation can be assessed as follows. With
every symbol, we can associate a certain insertion cost I and a certain deletion cost D.The cost for changing for instance

TT** to FF would then be D(TT)+D(**)+I(FF). These costsare determined by the parser writer. The cheapest parse stack correction is then chosen.

If there is more than one with the same lowest cost, we just pick one.Assigning identical costs to all edit operations, in our example, we end up with
two possibilities, both replacing part A`: TT (deleting the **), or TT**FF (inserting an FF).Assigning higher costs to editing a non-terminal, which is not unreasonable, would
only leave the first of these. Parsing then proceeds as follows:

<* TT =. ** ? ++ =. TT'' >* error situation<*

TT >* ++ =. TT'' >* error corrected by deleting **<*
TT'' >* ++ =. TT'' >* reducing TT<*
EE =. ++ =. TT'' >* reducing TT''<*
EE >* reducing EE++TT''<*
EE'' >* reducing EE<*
SS >* reducing EE''

The principles of this method have also been applied in LR parsers. There, how-ever, the backward move is omitted, because in an LR parser the state on top of the
stack, together with the next input symbol, determine the reduction that can be applied.If the input symbol is erroneous, we have no way of knowing which reductions can be
applied. For further details, see Pennello and DeRemer [ErrHandl 1978] and alsoMickunas and Modry [ErrHandl 1978].

10.7 LOCAL ERROR HANDLING
All local error recovery techniques are so-called acceptable-set error recovery tech-niques. These techniques work as follows: when a parser detects an error, a certain set

called the acceptable-set is calculated from the parser state. Next, symbols from theinput are skipped until a symbol is found that is a member of the acceptable-set. Then,
the parser state is adapted so that the symbol that is not skipped becomes acceptable.There is a family of such techniques; the members of this family differ in the way they
determine the acceptable-set, and in the way in which the parser state is adapted. Wewill now discuss several members of this family.

10.7.1 Panic modePanic mode is probably the simplest error recovery method that is still somewhat effective. In this method, the acceptable-set is determined by the parser writer, and is fixedfor the whole parsing process. The symbols in this set usually indicate the end of a
syntactic construct, for instance a statement in a programming language. For the pro-gramming language Pascal, this set could contain the symbols

;; and eenndd. When anerror is detected, symbols are skipped until a symbol is found that is a member of this

set. Then, the parser must be brought into a state that makes this symbol acceptable. Inan LL parser, this might require deleting the first few symbols of the prediction, in an

Sec. 10.7] Local error handling 241
LR parser this might involve popping states from the parse stack until a state isuncovered in which the symbol is acceptable.

The recovery capability of panic mode is often quite good, but many errors can goundetected, because sometimes large parts of the input are skipped. The method has
the advantage that it is very easy to implement.
10.7.2 FOLLOW set error recoveryAnother early acceptable-set recovery method is the FOLLOW set error recovery
method. The idea is applicable in an LL parser, and works as follows: when we areparsing a part of the input, and the top of the prediction stack results most recently from
a prediction for the non-terminal A, and we detect an error, we skip symbols until wefind a symbol that is a member of FOLLOW(A). Next, we remove the unprocessed
part of the current right-hand side of A from the prediction, and continue parsing. Aswe cannot be sure that the current input symbol can follow A in the present context and
is thus acceptable, this is not such a good idea. A better idea is to use that part ofFOLLOW(A) that can follow A in this particular context, making sure that the symbol
that is not skipped will be accepted, but this is not trivial to do.The existence of this method is probably the reason that the family of acceptableset error recovery methods is often called FOLLOW set error recovery. However, formost members of this family this is a confusing name.

A variant of this method that has become very popular in recursive descent parsersis based on the observation that at any point during the parsing process, there are a
number of active non-terminals (for which we are now trying to match a right-handside), and in general this number is larger than one. Therefore, we should use the union
of the FOLLOW sets of these non-terminals, rather than the FOLLOW set of just themost recent of them. A better variant uses the union of those parts of the FOLLOW sets
that can follow the non-terminals in this particular context. An expansion of this idea isthe following: suppose the parser is in the following state when it detects an error:

. . . a . . .
. . . X 1 . . . Xn##

We can then have the acceptable-set contain the symbols in FIRST(X 1), FIRST(X 2),. . . , and

##, and recover by skipping symbols until we meet a symbol of thisacceptable-set, and then removing symbols from the prediction until the input symbol

becomes acceptable.Many variations of this technique exist; see for instance Pemberton [ErrHandl
1980] and Stirling [ErrHandl 1985].
10.7.3 Acceptable-sets derived from continuationsA very interesting and effective member of the acceptable-set recovery method family
is the one discussed by Ro"hrich [ErrHandl 1980]. The idea is as follows. Suppose thata parser with the correct prefix property detects an error in the input after having processed a prefix u. Because of the correct prefix property, we know that this prefix u isthe start of some sentence in the language. Therefore, there must be a continuation,
which is a terminal string w, such that uw is a sentence of the language. Now suppose

242 Error handling [Ch. 10
we can compute such a continuation. We can then correct the error as follows:
\Gamma  Determine a continuation w of u.

\Gamma  For all prefixes wc' of w, compute the set of terminal symbols that would be

accepted by the parser after it has parsed wc', and take the union of these sets. If ais a member of this set, uwc'a is a prefix of some sentence in the language. This set

is our acceptable-set. Note that it includes all symbols of w, including the end-marker.
\Gamma  Skip symbols from the input until we find a symbol that is a member of this set.

Note that as a result of this, everything up to the end-marker may be skipped.
\Gamma  Insert the shortest prefix of w that makes this symbol acceptable in front of this

symbol. If everything up to the end-marker was skipped, insert w itself.
\Gamma  Produce an error message telling the user which symbols were skipped and which

symbols were inserted.
\Gamma  Restart the parser in the state where the error was detected and continue parsing,

starting with the inserted symbols. Now, the error is corrected, and the parsercontinues as if nothing has happened.

10.7.3.1 Continuation grammarsThere are two problems here, how to determine the continuation and how to calculate
the acceptable-set without going through all possible parsings. Let us regard a gram-mar as a generating device. Suppose we are generating a sentence from a grammar, and
have obtained a certain sentential form. Now, we want to produce a sentence from it assoon as possible, using the fewest possible production steps. We can do this if we
know for each non-terminal which right-hand side is the quickest "exit", that is, whichright-hand side leads to a terminal production in as few production steps as possible.

It turns out that we can compute these right-hand sides in advance. To this end, wecompute for each symbol the minimum number of production steps needed to obtain a
terminal derivation from it. We call this number the step count. Terminal symbolshave step count 0, non-terminal symbols have an as yet unknown step count, which we
set to infinity. Next, we examine each right-hand side in turn. If we already have a stepcount for each of the members of a right-hand side, the right-hand side itself needs the
sum of these step counts, and the left-hand side needs one more if it uses this right-hand side. If this is less than we had for this non-terminal, we update its step count.

We repeat this process until none of the step counts changes. If we had a propergrammar to begin with, all of the step counts will now be finite. Now, all we have to
do is for each left-hand side to mark the right-hand side with the lowest step count. Thegrammar rules thus obtained are called a continuation grammar, although these rules
together probably do not form a proper grammar.Let us see how this works with an example. Consider the grammar of Figure 8.9,
repeated in Figure 10.5 for reference.

SSeessssiioonn -->> FFaaccttss QQuueessttiioonn || (( SSeessssiioonn )) SSeessssiioonn

FFaaccttss -->> FFaacctt FFaaccttss || ee

FFaacctt -->> !! SSTTRRIINNGG
QQuueessttiioonn -->> ?? SSTTRRIINNGG

Figure 10.5 An example grammar

Sec. 10.7] Local error handling 243
The first pass over the right-hand sides shows us that FFaaccttss, FFaacctt, and QQuueessttiioonneach have step count 1. In the next pass, we find that

SSeessssiioonn has step count 3. Theresulting continuation grammar is presented in Figure 10.6.

SSeessssiioonn -->> FFaaccttss QQuueessttiioonn

FFaaccttss -->> ee

FFaacctt -->> !! SSTTRRIINNGG
QQuueessttiioonn -->> ?? SSTTRRIINNGG

Figure 10.6 The continuation grammar of the grammar of Figure 10.5
10.7.3.2 Continuation in an LL parserIn an LL parser, it now is easy to compute a continuation when an error occurs. We
take the prediction, and derive a terminal string from it using only rules from the con-tinuation grammar, processing the prediction from left to right. Each terminal that we
meet ends up in the acceptable-set; in addition, every time a non-terminal is replacedby its right-hand side from the continuation grammar, we add to the acceptable-set the
terminal symbols from the FIRST set of the current sentential form starting with thisnon-terminal.

Let us demonstrate this with an example. Suppose that we have the input (( ??
SSTTRRIINNGG ?? SSTTRRIINNGG for the LL(1) parser of Figure 8.10. When the parser detects anerror, it is in the following state:

(( ?? SSTTRRIINNGG ?? SSTTRRIINNGG ##

. . . )) SSeessssiioonn ##

Now, a continuation will be computed, starting with the sentential form )) SSeessssiioonn ##,using the continuation grammar. During this computation, when the prediction starts
with a non-terminal, the FIRST set of the prediction will be computed and the non-terminal will be replaced by its right-hand side in the continuation grammar. The
FIRST set is shown in square brackets below the line:

)) SSeessssiioonn ## -->>
)) [((!!??] FFaaccttss QQuueessttiioonn ## -->>
)) [((!!??] [!!??] e QQuueessttiioonn ## -->>
)) [((!!??] [!!??] [??] ?? SSTTRRIINNGG ##

Consequently, the continuation is )) ?? SSTTRRIINNGG ## and the acceptable-set contains ((, )),
!!, ??, SSTTRRIINNGG and ##. We see that we should keep the ?? and insert the first symbol ofthe continuation,

)). So, the parser is restarted in the following state:

244 Error handling [Ch. 10

(( ?? SSTTRRIINNGG )) ?? SSTTRRIINNGG ##

. . . )) SSeessssiioonn ##

and proceeds as usual.
10.7.3.3 Continuation in an LR parserUnlike an LL parser, an LR parser does not feature a sentential form which represents
the rest of the input. It is therefore more difficult to compute a continuation. Ro"hrich[ErrHandl 1980] demonstrates that an LR parser can be generated that has a terminal
symbol associated with each state of the handle recognizer so that we can obtain a con-tinuation by pretending that the parser has this symbol as input when it is in the
corresponding state. The sequence of states that the parser goes through when thesesymbols are given as input then determines the continuation. The acceptable-set consists of the terminal symbols on which a shift or reduce can take place (i.e. which areacceptable) in any of these states.

10.7.4 Insertion-only error correctionFischer, Milton and Quiring [ErrHandl 1980] propose an error correction method for
LL(1) parsers using only insertions. This method has become known as the FMQ errorcorrection method. In this method, the acceptable-set is the set of all terminal symbols.
Fischer, Milton and Quiring argue that the advantage of using only insertions (and thusno deletions or replacements) is that a syntactically correct input is built around the
input supplied by the user, so none of the symbols supplied by the user are deleted orchanged. Of course, the question arises if every input can be corrected in this way, and
in general the answer is no; for some languages it can however, and other languages areeasily modified so that it can.

Let us investigate which properties a language must have for every error to becorrectable by insertions only. Suppose we have an input xa . . . such that the start
symbol does derive a sentence starting with x, but not a sentence starting with xa; so xis a correct prefix, but xa is not. Now, if this error is to be corrected by an insertion y,
xya must again be a correct prefix. This leads to the notion of insert-correctable gram-mars: a grammar is said to be insert-correctable if for every prefix x of a sentence and
every symbol a in the language there is a continuation of x that includes a (so an inser-tion can always be found). Fischer, Milton and Quiring demonstrate that it is decidable
whether an LL(1) grammar is insert-correctable.So, the FMQ error correction method is applicable in an LL(1) parser built from
an insert-correctable grammar. In addition, the LL(1) parser must have the immediateerror detection property. As we have seen in Section 8.2.4, the usual (strong-)LL(1)
parser does not have this property, but the full LL(1) parser does. Fischer, Tai and Mil-ton [ErrHandl 1979] show that for the class of LL(1) grammars in which every nonterminal that derives e does so explicitly through an e-rule, the immediate error detec-tion property can be retained while using strong-LL(1) tables.

Now, how does the error corrector work? Suppose that an error is detected oninput symbol a, and the current prediction is X

1 . . . Xn##. The state of the parser isthen:

Sec. 10.7] Local error handling 245

. . . a . . .
. . . X 1 . . . Xn##

As a is an error, we know that it is not a member of FIRST(X 1 . . . Xn##). We alsoknow that the grammar is insert-correctable, so X

1 . . . Xn## must derive a terminalstring containing a. The error corrector now determines the cheapest insertion after

which a is acceptable. Again, every symbol has associated with it a certain insertioncost, determined by the parser writer; the cost of an insertion is the sum of the costs of
the symbols in the insertion.To compute the cheapest insertion, the error corrector uses some tables that are
precomputed for the grammar at hand (by the parser generator). First, there is a tablethat we will call

cchheeaappeesstt__ddeerriivvaattiioonn, giving the cheapest terminal derivation foreach symbol (for a terminal, this is of course the terminal itself). Second, there is a

table that we will call cchheeaappeesstt__iinnsseerrttiioonn giving for each symbol/terminal combi-nation (X,a) the cheapest insertion y such that X o""* ya . . . , if it exists, or an indication
that it does not exist. Note that in any prediction X 1 . . . Xn## there must be at least onesymbol X such that the (X,a) entry of the

cchheeaappeesstt__iinnsseerrttiioonn table contains aninsertion (or else the grammar was not insert-correctable).

Going back to our parser, we can now compute the cheapest insertion z such that abecomes acceptable. Consulting

cchheeaappeesstt__iinnsseerrttiioonn(X 1, a), we can distinguishtwo cases:

\Gamma  cchheeaappeesstt__iinnsseerrttiioonn(X 1, a) contains an insertion y 1; in this case, we have

found an insertion.
\Gamma  cchheeaappeesstt__iinnsseerrttiioonn(X 1, a) does not contains an insertion. In this case, we

use cchheeaappeesstt__ddeerriivvaattiioonn(X 1) as the first part of the insertion, and continuewith X

2 in exactly the same way as we did with X 1. In the end, this will result inan insertion y

1 . . . yi, where y 1, . . . ,yi -1 come from thecchheeaappeesstt__ddeerriivvaattiioonn table, and y

i comes from the cchheeaappeesstt__iinnsseerrttiioonntable.

In some LL(1) parsers, notably recursive descent ones, the prediction is not explicitlyavailable, only the first part X

1 is. In this case, we can use this first part to compute aninsertion y
1, either as cchheeaappeesstt__iinnsseerrttiioonn(X 1, a) or ascchheeaappeesstt__ddeerriivvaattiioonn(X

1) (which may or may not make a acceptable), and weinsert it:

. . . y 1a . . .
. . . X 1 . . . Xn##

If the insertion y 1 does not make a acceptable yet, after parsing y 1, the parser is in thefollowing state:

246 Error handling [Ch. 10

. . . y 1 a . . .

. . . X 2 . . . Xn##

and the process is repeated (X 2 is now explicitly available).The most serious disadvantage of the FMQ error corrector is that it behaves rather
poorly on those errors that are better corrected by a deletion. Advantages are that italways works, can be generated automatically, and is simple.

Anderson and Backhouse [ErrHandl 1982] present a significant improvement ofthe implementation described above, which is based on the observation that it is sufficient to only compute the first symbol of the insertion: if we detect an error symbol aafter having read prefix u, and w = w

1w 2 . . . wn is a cheapest insertion, then w 2 . . . wnis a cheapest insertion for the error a after having read uw

1. So, thecchheeaappeesstt__ddeerriivvaattiioonn and cchheeaappeesstt__iinnsseerrttiioonn tables are not needed. Instead,

tables are needed that are indexed similarly, but only contain the first symbol of theinsertion. The latter tables are much smaller, and easier to compute.

10.7.5 Locally least-cost error recoveryLike the FMQ error correction method, locally least-cost error recovery (see Backhouse [Books 1979] and Anderson, Backhouse, Bugge and Stirling [ErrHandl 1983]) isa technique for recovering from syntax errors by editing the input string at the error
detection point. The FMQ method corrects the error by inserting terminal symbols, thelocally least-cost method corrects the error by either deleting the error symbol, or
inserting a sequence of terminal or non-terminal symbols after which the error symbolbecomes correct, or changing the error symbol. Unlike the least-errors analysis discussed in Section 10.4, which considers the complete input string in determining thecorrections to be made, the locally least-cost method only considers the error symbol
itself and the symbol after that. The correction is determined by its cost: every symbolhas a certain insertion cost, every terminal symbol has a certain deletion cost, and every
replacement also has a certain cost. All these costs are determined by the parser writer.When considering if the error symbol is to be deleted, the cost of an insertion that
would make the next input symbol acceptable is taken into account. The cheapestcorrection is chosen.

This principle does not rely on a particular parsing method, although the imple-mentation does. The method has successfully been implemented in LL, LR, and Earley
parsers; see Backhouse [Books 1979], Anderson and Backhouse [ErrHandl 1981],Anderson, Backhouse, Bugge and Stirling [ErrHandl 1983], and Choe and Chang
[ErrHandl 1986] for details.

10.8 SUFFIX PARSING
Although the error correction and error recovery methods discussed above have theirgood and bad points, they all have the following problems in common:

\Gamma  On an error, they change the input and/or the parser state, using heuristics to

choose one of the many possibilities. We can, however, never be sure that wepicked the right change.

\Gamma  Selecting the wrong change can cause an avalanche of spurious error messages.

Sec. 10.8] Suffix parsing 247

Only the least-errors analysis of Section 10.4 does not have this problem.A quite different approach to error recovery is that of Richter [ErrHandl 1985].
He proposes a method that does not have the problems mentioned above, but has someproblems of its own. The author argues that we should not try to repair an error,
because we cannot be sure that we get it right. Neither should we try to change parserstate and/or input. The only thing that we can assume is that the rest of the input (the
input that comes after the error symbol, excluding the error symbol itself) is a suffix(tail) of a sentence of the language. This is an assumption made in several error
recovery methods, but the difference is that most error recovery methods assume morethan that, in that they use (some of) the parser state information built so far.

The set of strings that are some suffix of some sentence of a language forms itselfa language. This language of suffixes is called the suffix-language of the original
language and, in fact, the suffix-language of a context-free language is again acontext-free language, for which a grammar can be constructed given the grammar of
the original language. Such a grammar is called a suffix-grammar, and one can be con-structed in the following way: for every non-terminal A in the original grammar, we
introduce a new non-terminal Ac' which derives a suffix of a sentence generated by theoriginal non-terminal. If the original grammar contains a rule

A o""o"" X 1X 2 .. .. .. Xn
the suffix-grammar will also contain this rule and, in addition, it will contain the fol-lowing rules deriving a suffix of what A can derive:

Ac' o""o"" Xc'1X 2 .. .. .. XnAc' o""o"" Xc'

2 .. .. .. Xn...... ...... ......

Ac' o""o"" Xc'n
If Xi is a terminal symbol, Xc'i is the empty string.All the new non-terminals (marked with a c') derive the empty string, which is also
a suffix,albeit a degenerate one. If S is the start symbol of the original grammar, thesuffix-grammar has start symbol S suffix with the following rules:

S suffix o""o"" S || Sc'
The error recovery method now works as follows: parsing starts with a parser forthe original language, preferably one with the correct prefix property. When an error is
detected, it is reported, the error symbol is skipped and a parser derived from thesuffix-grammar, a so-called suffix-parser, is started on the rest of the input (which must
be a suffix or else there is another error). When another error is detected, it is againreported, the error symbol is skipped, and the suffix-parser is reset to its starting state,
ready to accept another suffix.This method has several advantages:
\Gamma  Each error reported is guaranteed to be a different syntax error, and no error is

reported more than once. This maintains a high level of user confidence in theerror messages.

\Gamma  After each error, the parser is restarted in the proper state. There are no spurious

248 Error handling [Ch. 10

error messages.
\Gamma  No input is skipped, apart from the error symbols.

This sounds just about perfect, so there must be a catch, and indeed there is one; it con-cerns the suffix-grammar. For the method to be practical, we need an efficient suffixparser. However, the suffix-grammar may not be suitable for any of the deterministicparsing methods, such as LL or LR. In fact, the way we constructed the suffix-grammar
almost certainly results in an ambiguous grammar. This, however, does not mean thatthe language is ambiguous. Richter conjectures that any kind of bounded-context property of the original grammar is sufficient for the existence of a deterministic suffix-grammar. This conjecture is confirmed by Cormack [ErrHandl 1989] for a subset of
the bounded-context grammars.Another, less important, disadvantage is that sometimes not all syntax errors are
reported. This is not really a disadvantage in a highly interactive environment, where itis probably more important that all reported errors are real errors. Also, in the presence
of errors, the parser is unable to deliver a meaningful parse tree, which may or may notbe a disadvantage.

11
Comparative survey

Practical parsing is concerned almost exclusively with context-free (Type 2) and regu-lar (Type 3) grammars. Unrestricted (Type 0) and context-sensitive (Type 1) grammars
are hardly used since, first, they are user-unfriendly in that it is next to impossible toconstruct a clear and readable Type 0 or Type 1 grammar and, second, all known
parsers for them have exponential time requirements. Van Wijngaarden grammars takea slightly different position: Van Wijngaarden grammars can be made very clear and
informative, but we do not at present have any parsing technique for even a reasonablesubset of them, regardless of time requirements; for some experimental results see Section 13.3.Regular grammars are used mainly to describe patterns that have to be found in
surrounding text. For this application a recognizer suffices. There is only one suchrecognizer: the finite-state automaton described in Section 5.3. Actual parsing with a
regular grammar, when required, is generally done using techniques for CF grammars.For parsing with register-vector grammars, which are a special form of regular grammars, see Section 13.10.In view of the above we shall restrict ourselves to CF grammars in the rest of this
chapter.

11.1 CONSIDERATIONS
The initial demands on a CF parsing technique are obvious: it should be general (i.e.,able to handle all CF grammars), it should be fast (i.e., have linear time requirements)

and preferably it should be easy to program. There are two serious obstacles to thisnaive approach to choosing a parser. The first is that the automatic generation of a
linear-time parser is possible only for a subset of the CF grammars. The second is that,although this subset is often described as "very large" (especially for LR(1) and
LALR(1)), experience shows that a grammar that is designed to best describe thelanguage without concern for parsing is virtually never in this set. What is true,
though, is that for almost any arbitrary grammar a slightly different grammar can befound that generates the same language and that does allow linear-time parsing; finding
such a grammar, however, almost always requires human intervention and cannot beautomated. Using a modified grammar has the disadvantage that the resulting parse
trees will differ to a certain extent from the ones implied by the original grammar.

250 Comparative survey [Ch. 11
Furthermore, it is important to notice that no linear-time method can handle ambiguousgrammars.

An immediate consequence of the above observations is that the stability of thegrammar is an important datum. If the grammar is subject to continual revision, it is
impossible or at least highly inconvenient to adapt each version by hand to the require-ments of a linear-time method, and we have no choice but to use a general method.
Likewise, if the grammar is ambiguous, we are forced to use a general method.

11.2 GENERAL PARSERS
There are three general methods that should be considered: Unger's, Earley's andTomita's.

11.2.1 UngerAn Unger parser (Section 4.1) is easy to program, especially the form given in Section
12.2, but its exponential time requirements limit its applicability to occasional use. Therelatively small effort of adding a well-formed substring table (Section 12.3) can
improve its efficiency dramatically, and in this form it can be very useful, especially ifthe average input string is limited to some tens of tokens. The thus modified Unger
parser requires in principle a time proportional to n N +1, where n is the number oftokens in the input and N is the maximum number of non-terminals in any right-hand
side in the grammar, but in practice it is often much faster. An additional advantage ofthe Unger parser is that it can usually be readily understood by all participants in a project, which is something that can be said of almost no other parser.
11.2.2 EarleyA simple, robust and efficient version of the Earley parser has been presented by Graham, Harrison and Ruzzo [CF 1980]. It requires a time proportional to n 3 for ambigu-ous grammars (plus the time needed to enumerate the parse trees), at most n 2 for
unambiguous grammars and at most n for grammars for which a linear-time methodwould work; in this sense the Earley parser is self-adapting. Since it does not require
preprocessing on the grammar, it is possible to have one grammar-independent Earleyparser and to supply it with the grammar and the input whenever a parsing is needed. If
this is convenient, the Earley parser is preferable to Tomita's method.
11.2.3 TomitaAt the expense of considerably more programming and some loss of convenience in
use, the Tomita parser (Section 9.8) will provide a parsing in slightly more than lineartime for all but the most ambiguous grammars. Since it requires preprocessing on the
grammar, it is convenient to generate a separate parser for each grammar (using aparser generator); if the grammar is, however, very unstable, the preprocessing can be
done each time the parser is called. The Tomita parser is presently the parser of choicefor serious parsing in situations where a linear-time method cannot be applied and the
grammar is reasonably stable.As explained in Section 9.8, the Tomita parser uses a table to restrict the breadthfirst search and the question arises what type of table would be optimal. Experimentaldata on this are lacking. An LR(0) table is relatively easy to construct (9.4.1) and
should give reasonable results but an SLR(1) table is still not difficult to construct

Sec. 11.2] General parsers 251
(9.6.4) and might be preferable. In view of the additional construction effort, anLALR(1) table may not have any advantage over the SLR(1) table in this case. An
LR(1) table probably requires too much space.
11.2.4 NotesIt should be noted that if any of the general parsers performs in linear time, it may still
be a factor of ten or so slower than a real linear-time method, due to the much heavieradministration they need.

None of the general parsers identifies with certainty a part of the parse tree beforethe whole parse tree is completed. Consequently, if semantic actions are connected to
the grammar rules, none of these actions can be performed until the whole parse is fin-ished. The actions certainly cannot influence the parsing process. They can, however,
reject certain parse trees afterwards; this is useful to implement context conditions in acontext-free parser.

11.3 LINEAR-TIME PARSERS
Among the grammars that allow linear-time parsing, the operator-precedence gram-mars (see Section 9.2.1) occupy a special place, in that they can be ambiguous. They

escape the general rule that ambiguous grammars cannot be parsed in linear time byvirtue of the fact that they do not provide a full parse tree but rather a parse skeleton. If
every sentence in the generated language has only one parse skeleton, the grammar canbe operator-precedence. Operator-precedence is by far the simplest practical method;
if the parsing problem can be brought into a form that allows an operator-precedencegrammar (and that is possible for almost all formula-like inputs), a parser can be constructed by hand in a very short time.
11.3.1 RequirementsNow we come to the full linear-time methods. As mentioned above, grammars are not
normally in a form that allows linear-time parsing and have to be modified by hand tobe so. This implies that for the use of a linear-time parser at least the following conditions must be fulfilled:
\Gamma  the grammar must be relatively stable, so that the modification process will not

have to be repeated too often;
\Gamma  the user must be willing to accept a slightly different parse tree than would

correspond to the original grammar.It should again be pointed out that the transformation of the grammar cannot, in general, be performed by a program (if it could, we would have a stronger parsingmethod).

11.3.2 Strong-LL(1) versus LALR(1)For two linear-time methods, strong-LL(1)# (Section 8.2.2) and LALR(1) (Section 9.6),
parser generators are readily available, both as commercial products and in the publicdomain. Using one of them will in almost all cases be more practical and efficient than
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

What is advertised as an "LL(1) parser generator" is almost always actually a strong-LL(1)parser generator.

252 Comparative survey [Ch. 11
writing your own; for one thing, writing a parser generator may be (is!) interesting, butdoing a reasonable job on the error recovery is a protracted affair, not to be taken on
lightly. So the choice is between (strong-)LL(1) and LALR(1); full LL(1) or LR(1)might occasionally be preferable, but parser generators for these are not (yet) easily
available and their advantages will probably not provide enough ground to write one.The main differences between (strong-)LL(1) and LALR(1) can be summarized as follows:
\Gamma  LL(1) generally requires larger modifications to be made to the grammar than

LALR(1).
\Gamma  LL(1) allows semantic actions to be performed even before the start of an alternative; LALR(1) performs semantic actions only at the end of an alternative.
\Gamma  LL(1) parsers are often easier to understand and modify.

\Gamma  If an LL(1) parser is implemented as a recursive-descent parser, the semantic

actions can use named variables and attributes, much as in a programminglanguage. No such use is possible in a table-driven parser.

\Gamma  Both methods are roughly equivalent as to speed and memory requirements; a

good implementation of either will outperform a mediocre implementation of theother.

People evaluate the difference in power between LL(1) and LALR(1) differently;for some the requirements made by LL(1) are totally unacceptable, others consider
them a minor inconvenience, largely offset by the advantages of the method.If one is in a position to design the grammar along with the parser, there is little
doubt that LL(1) is to be preferred: not only will parsing and performing semanticactions be easier, text that conforms to an LL(1) grammar is also clearer to the human
reader. A good example is the design of Modula-2 by Wirth (see Programming inModula-2 (Third, corrected edition) by Niklaus Wirth, Springer-Verlag, Berlin, 1985).

11.3.3 Table sizeThe table size of a linear-time parser (in the order of 10K to 100K bytes) may be a serious problem to some applications. The strongest linear-time method with negligibletable size is weak precedence with precedence functions.

12
A simple general context-free parser

Although LL(1) and LALR(1) parsers are easy to come by, they are of limited use out-side the restricted field of programming language processing, and general parsers are
not widely available. The general parser shown here in full detail will yield all parsingsof a sentence according to a CF grammar, with no restriction imposed on the grammar.
It can be typed in and made operational in a couple of hours, to enable the reader toexperiment directly with a general CF parser. The parser, which is rather primitive,
takes exponential time in the worst case; an extension to reduce the time requirement topolynomial time is discussed in Section 12.3. The interested reader who has access to a
Prolog interpreter may wish to look into Definite Clause Grammars (Section 6.7).These are perhaps easier to use than the parser in this chapter, but cannot handle leftrecursion.

12.1 PRINCIPLES OF THE PARSER
The parser, presented as a Pascal program in Figure 12.1, is the simplest we can thinkof that puts no restrictions on the grammar. Since it searches a forest of possible parse

trees to find the applicable ones, it is not completely trivial, though. The parser is anUnger parser in that a top-down analysis is made, dividing the input into segments that
are to be matched to symbols in the pertinent right-hand side. A depth-first search(using recursive-descent) is used to enumerate all possibilities. To keep the size of the
parser reasonable, a number of oversimplifications have been made (for one thing,names of non-terminals can be one character long only). Once the parser is running,
these can all be rectified.
pprrooggrraamm UUnnggeerr((iinnppuutt,, oouuttppuutt));; {{ UUnnggeerr ppaarrsseerr iinn PPaassccaall }}
ccoonnsstt

NNooSSyymmbbooll == '' '';;
MMaaxxRRuulleess == 1100;;
MMaaxxRRhhssLLeennggtthh == 1100;;
MMaaxxIInnppuuttLLeennggtthh == 1100;;
AArrrraayySSiizzee == 11000000;; {{ ffoorr aallll ssttaacckkss aanndd lliissttss }}

ttyyppee

254 A simple general context-free parser [Ch. 12

SSyymmbboollTTyyppee == cchhaarr;;
RRuulleeNNmmbbTTyyppee == iinntteeggeerr;;

RRhhssTTyyppee == ppaacckkeedd aarrrraayy [[11....MMaaxxRRhhssLLeennggtthh]] ooff SSyymmbboollTTyyppee;;
IInnppuuttSSttrriinnggTTyyppee == ppaacckkeedd aarrrraayy [[11....MMaaxxIInnppuuttLLeennggtthh]] ooff SSyymmbboollTTyyppee;;

RRuulleeTTyyppee == rreeccoorrdd LLhhssFFiieelldd:: SSyymmbboollTTyyppee;; RRhhssFFiieelldd:: RRhhssTTyyppee;; eenndd;;
SSttaacckkEElleemmTTyyppee ==
rreeccoorrdd

NNmmbbFFiieelldd:: RRuulleeNNmmbbTTyyppee;; RRhhssUUsseeddFFiieelldd:: iinntteeggeerr;; {{ tthhee rruullee }}
PPoossFFiieelldd,, LLeennFFiieelldd,, IInnppUUsseeddFFiieelldd:: iinntteeggeerr;; {{ tthhee ssuubbssttrriinngg }}
eenndd;;

vvaarr

IInnppuuttSSttrriinngg:: IInnppuuttSSttrriinnggTTyyppee;;
IInnppuuttLLeennggtthh:: iinntteeggeerr;;

GGrraammmmaarr:: aarrrraayy [[11....MMaaxxRRuulleess]] ooff RRuulleeTTyyppee;;
NNRRuulleess:: iinntteeggeerr;;
SSttaarrtt:: SSyymmbboollTTyyppee;;

SSttaacckk:: aarrrraayy [[11....AArrrraayySSiizzee]] ooff SSttaacckkEElleemmTTyyppee;;
NNSSttaacckkEElleemmss:: iinntteeggeerr;;

RRuulleeSSttaacckk:: aarrrraayy [[11....AArrrraayySSiizzee]] ooff RRuulleeNNmmbbTTyyppee;;
NNRRuulleessSSttaacckkeedd:: iinntteeggeerr;;
NNDDeerriivvaattiioonnss:: iinntteeggeerr;;

{{ RRUULLEE AADDMMIINNIISSTTRRAATTIIOONN }}
pprroocceedduurree SSttoorreeRRuullee((llhhss:: SSyymmbboollTTyyppee;; rrhhss:: RRhhssTTyyppee));;

bbeeggiinn

NNRRuulleess::== NNRRuulleess++11;;
wwiitthh GGrraammmmaarr[[NNRRuulleess]] ddoo
bbeeggiinn LLhhssFFiieelldd::== llhhss;; RRhhssFFiieelldd::== rrhhss;; eenndd;;
eenndd {{ SSttoorreeRRuullee }};;

pprroocceedduurree WWrriitteeRRhhss((rrhhss:: RRhhssTTyyppee));;

vvaarr nn:: iinntteeggeerr;;
bbeeggiinn

ffoorr nn::== 11 ttoo MMaaxxRRhhssLLeennggtthh ddoo

iiff rrhhss[[nn]] <<>> NNooSSyymmbbooll tthheenn wwrriittee((rrhhss[[nn]]));;
eenndd {{ WWrriitteeRRhhss }};;

pprroocceedduurree WWrriitteeRRuullee((nnmmbb:: RRuulleeNNmmbbTTyyppee));;

bbeeggiinn

wwiitthh GGrraammmmaarr[[nnmmbb]] ddoo
bbeeggiinn

wwrriittee((LLhhssFFiieelldd,, '' -->> ""''));;
WWrriitteeRRhhss((RRhhssFFiieelldd));;
wwrriittee((''""''));;
eenndd;;
eenndd {{ WWrriitteeRRuullee }};;

pprroocceedduurree PPuusshhRRuullee((nn:: RRuulleeNNmmbbTTyyppee));;

bbeeggiinn

Sec. 12.1] Principles of the parser 255

NNRRuulleessSSttaacckkeedd::== NNRRuulleessSSttaacckkeedd++11;;
RRuulleeSSttaacckk[[NNRRuulleessSSttaacckkeedd]]::== nn;;
eenndd;;

pprroocceedduurree PPooppRRuullee;;

bbeeggiinn NNRRuulleessSSttaacckkeedd::== NNRRuulleessSSttaacckkeedd--11;; eenndd;;

pprroocceedduurree PPaarrssiinnggFFoouunndd;;

vvaarr rr:: iinntteeggeerr;;
bbeeggiinn

NNDDeerriivvaattiioonnss::== NNDDeerriivvaattiioonnss++11;;
ffoorr rr::== 11 ttoo NNRRuulleessSSttaacckkeedd ddoo
bbeeggiinn WWrriitteeRRuullee((RRuulleeSSttaacckk[[rr]]));; wwrriitteellnn;; eenndd;;
wwrriitteellnn;;
eenndd {{ PPaarrssiinnggFFoouunndd }};;

{{ HHAANNDDLLIINNGG OOFF KKNNOOWWNN PPAARRSSIINNGGSS }}
pprroocceedduurree SSttaarrttNNeewwKKnnoowwnnGGooaall((nnmmbb:: RRuulleeNNmmbbTTyyppee;; ppooss,, lleenn:: iinntteeggeerr));;

bbeeggiinn eenndd;;

pprroocceedduurree RReeccoorrddKKnnoowwnnPPaarrssiinngg;;

bbeeggiinn eenndd;;

{{ PPAARRSSIINNGG SSTTAACCKK HHAANNDDLLIINNGG }}
pprroocceedduurree PPuusshhSSttaacckk((nnmmbb:: RRuulleeNNmmbbTTyyppee;; ppooss,, lleenn:: iinntteeggeerr));;

bbeeggiinn

NNSSttaacckkEElleemmss::== NNSSttaacckkEElleemmss++11;;
wwiitthh SSttaacckk[[NNSSttaacckkEElleemmss]] ddoo
bbeeggiinn

NNmmbbFFiieelldd::== nnmmbb;; RRhhssUUsseeddFFiieelldd::== 00;;
PPoossFFiieelldd::== ppooss;; LLeennFFiieelldd::== lleenn;; IInnppUUsseeddFFiieelldd::== 00;;
eenndd;;
eenndd {{ PPuusshhSSttaacckk }};;

pprroocceedduurree PPooppSSttaacckk;;

bbeeggiinn NNSSttaacckkEElleemmss::== NNSSttaacckkEElleemmss--11;; eenndd;;

ffuunnccttiioonn IIssTTooBBeeAAvvooiiddeedd((nnmmbb:: RRuulleeNNmmbbTTyyppee;; ppooss,, lleenn:: iinntteeggeerr)):: BBoooolleeaann;;

vvaarr ii:: iinntteeggeerr;;
bbeeggiinn

IIssTTooBBeeAAvvooiiddeedd::== ffaallssee;;
ffoorr ii::== 11 ttoo NNSSttaacckkEElleemmss ddoo

wwiitthh SSttaacckk[[ii]] ddoo

iiff ((NNmmbbFFiieelldd==nnmmbb))
aanndd ((PPoossFFiieelldd==ppooss))
aanndd ((LLeennFFiieelldd==lleenn)) tthheenn

IIssTTooBBeeAAvvooiiddeedd::== ttrruuee;;
eenndd {{ IIssTTooBBeeAAvvooiiddeedd }};;

pprroocceedduurree AAddvvaanncceeTTOOSS((lleenn:: iinntteeggeerr));;

bbeeggiinn

wwiitthh SSttaacckk[[NNSSttaacckkEElleemmss]] ddoo
bbeeggiinn

RRhhssUUsseeddFFiieelldd::== RRhhssUUsseeddFFiieelldd++11;;
IInnppUUsseeddFFiieelldd::== IInnppUUsseeddFFiieelldd++lleenn;;
eenndd;;

256 A simple general context-free parser [Ch. 12

eenndd {{ AAddvvaanncceeTTOOSS }};;
pprroocceedduurree RReettrraaccttTTOOSS((lleenn:: iinntteeggeerr));;

bbeeggiinn

wwiitthh SSttaacckk[[NNSSttaacckkEElleemmss]] ddoo
bbeeggiinn

RRhhssUUsseeddFFiieelldd::== RRhhssUUsseeddFFiieelldd--11;;
IInnppUUsseeddFFiieelldd::==IInnppUUsseeddFFiieelldd--lleenn;;
eenndd;;
eenndd {{ RReettrraaccttTTOOSS }};;

{{ TTHHEE AAUUTTOOMMAATTOONN }}
pprroocceedduurree TTrryyAAllllRRuulleessFFoorr((llhhss:: SSyymmbboollTTyyppee;; ppooss,, lleenn:: iinntteeggeerr));;

vvaarr nnmmbb:: RRuulleeNNmmbbTTyyppee;;

pprroocceedduurree DDooTTooppOOffSSttaacckk;;

vvaarr ttoossSSyymmbb:: SSyymmbboollTTyyppee;; {{ aaccttiivvee ssyymmbbooll oonn ttoopp ooff SSttaacckk }}

pprroocceedduurree DDooNNeexxttOOnnSSttaacckk;;

vvaarr ssvv:: SSttaacckkEElleemmTTyyppee;;
bbeeggiinn {{ tthhee nnoonn--tteerrmmiinnaall oonn ttoopp ooff SSttaacckk wwaass rreeccooggnniizzeedd }}

RReeccoorrddKKnnoowwnnPPaarrssiinngg;;
{{ ssaavvee ttoopp ooff SSttaacckk }}
ssvv::== SSttaacckk[[NNSSttaacckkEElleemmss]];; NNSSttaacckkEElleemmss::== NNSSttaacckkEElleemmss--11;;
iiff ((NNSSttaacckkEElleemmss == 00)) tthheenn PPaarrssiinnggFFoouunndd eellssee
bbeeggiinn

AAddvvaanncceeTTOOSS((ssvv..LLeennFFiieelldd));;
DDooTTooppOOffSSttaacckk;;
RReettrraaccttTTOOSS((ssvv..LLeennFFiieelldd));;
eenndd;;
{{ rreessttoorree ttoopp ooff SSttaacckk }}
NNSSttaacckkEElleemmss::== NNSSttaacckkEElleemmss++11;; SSttaacckk[[NNSSttaacckkEElleemmss]]::== ssvv;;
eenndd {{ DDooNNeexxttOOnnSSttaacckk }};;

pprroocceedduurree TTrryyAAllllLLeennggtthhssFFoorr

((llhhss:: SSyymmbboollTTyyppee;; ppooss,, mmaaxxlleenn:: iinntteeggeerr));;
vvaarr lleenn:: iinntteeggeerr;;
bbeeggiinn

ffoorr lleenn::== 00 ttoo mmaaxxlleenn ddoo

TTrryyAAllllRRuulleessFFoorr((llhhss,, ppooss,, lleenn));;
eenndd {{ TTrryyAAllllLLeennggtthhssFFoorr }};;

bbeeggiinn {{ DDooTTooppOOffSSttaacckk }}

wwiitthh SSttaacckk[[NNSSttaacckkEElleemmss]] ddoo
bbeeggiinn

ttoossSSyymmbb::== GGrraammmmaarr[[NNmmbbFFiieelldd]]..RRhhssFFiieelldd[[RRhhssUUsseeddFFiieelldd++11]];;

iiff ttoossSSyymmbb == NNooSSyymmbbooll tthheenn
bbeeggiinn

iiff ((IInnppUUsseeddFFiieelldd == LLeennFFiieelldd)) tthheenn DDooNNeexxttOOnnSSttaacckk;;
eenndd
eellssee iiff ((IInnppUUsseeddFFiieelldd << LLeennFFiieelldd)) aanndd

((ttoossSSyymmbb == IInnppuuttSSttrriinngg[[PPoossFFiieelldd++IInnppUUsseeddFFiieelldd]]))
tthheenn
bbeeggiinn {{ 11 ssyymmbbooll wwaass rreeccooggnniizzeedd }}

AAddvvaanncceeTTOOSS((11));;

Sec. 12.1] Principles of the parser 257

DDooTTooppOOffSSttaacckk;;
RReettrraaccttTTOOSS((11));;
eenndd
eellssee TTrryyAAllllLLeennggtthhssFFoorr((ttoossSSyymmbb,, PPoossFFiieelldd++IInnppUUsseeddFFiieelldd,,

LLeennFFiieelldd--IInnppUUsseeddFFiieelldd));;
eenndd;;
eenndd {{ DDooTTooppOOffSSttaacckk }};;

ffuunnccttiioonn KKnnoowwnnGGooaallSSuucccceeeeddss

((nnmmbb:: RRuulleeNNmmbbTTyyppee;; ppooss,, lleenn:: iinntteeggeerr)):: BBoooolleeaann;;
bbeeggiinn KKnnoowwnnGGooaallSSuucccceeeeddss::== ffaallssee;; eenndd;;

pprroocceedduurree TTrryyRRuullee((nnmmbb:: RRuulleeNNmmbbTTyyppee;; ppooss,, lleenn:: iinntteeggeerr));;

bbeeggiinn

iiff nnoott IIssTTooBBeeAAvvooiiddeedd((nnmmbb,, ppooss,, lleenn)) tthheenn

iiff nnoott KKnnoowwnnGGooaallSSuucccceeeeddss((nnmmbb,, ppooss,, lleenn)) tthheenn
bbeeggiinn

PPuusshhSSttaacckk((nnmmbb,, ppooss,, lleenn));;
SSttaarrttNNeewwKKnnoowwnnGGooaall((nnmmbb,, ppooss,, lleenn));;
wwrriittee((''TTrryyiinngg rruullee ''));; WWrriitteeRRuullee((nnmmbb));;
wwrriitteellnn(('' aatt ppooss '',, ppooss::00,, '' ffoorr lleennggtthh '',, lleenn::00));;
PPuusshhRRuullee((nnmmbb));;
DDooTTooppOOffSSttaacckk;;
PPooppRRuullee;;
PPooppSSttaacckk;;
eenndd;;
eenndd {{ TTrryyRRuullee }};;

bbeeggiinn {{ TTrryyAAllllRRuulleessFFoorr }}

ffoorr nnmmbb::== 11 ttoo NNRRuulleess ddoo

iiff GGrraammmmaarr[[nnmmbb]]..LLhhssFFiieelldd == llhhss tthheenn

TTrryyRRuullee((nnmmbb,, ppooss,, lleenn));;
eenndd {{ TTrryyAAllllRRuulleessFFoorr }};;

pprroocceedduurree PPaarrssee((iinnpp:: IInnppuuttSSttrriinnggTTyyppee));;

vvaarr nn:: iinntteeggeerr;;
bbeeggiinn

NNSSttaacckkEElleemmss::== 00;; NNRRuulleessSSttaacckkeedd::== 00;; NNDDeerriivvaattiioonnss::== 00;;
IInnppuuttLLeennggtthh::== 00;;
ffoorr nn::== 11 ttoo MMaaxxIInnppuuttLLeennggtthh ddoo
bbeeggiinn

IInnppuuttSSttrriinngg[[nn]]::== iinnpp[[nn]];;
iiff iinnpp[[nn]] <<>> NNooSSyymmbbooll tthheenn IInnppuuttLLeennggtthh::== nn;;
eenndd;;
wwrriitteellnn((''PPaarrssiinngg '',, IInnppuuttSSttrriinngg));;
TTrryyAAllllRRuulleessFFoorr((SSttaarrtt,, 11,, IInnppuuttLLeennggtthh));;
wwrriitteellnn((NNDDeerriivvaattiioonnss::00,, '' ddeerriivvaattiioonn((ss)) ffoouunndd ffoorr ssttrriinngg '',,

IInnppuuttSSttrriinngg));;
wwrriitteellnn;;
eenndd {{ PPaarrssee }};;

pprroocceedduurree IInniittGGrraammmmaarr;; {{ GGrraammmmaarr 44 }}

bbeeggiinn

SSttaarrtt::== ''SS'';;
SSttoorreeRRuullee((''SS'',, ''LLSSRR ''));;
SSttoorreeRRuullee((''SS'',, '' ''));;

258 A simple general context-free parser [Ch. 12

SSttoorreeRRuullee((''LL'',, ''(( ''));;
SSttoorreeRRuullee((''LL'',, '' ''));;
SSttoorreeRRuullee((''RR'',, '')) ''));;
eenndd;;

pprroocceedduurree DDooPPaarrsseess;;

bbeeggiinn

PPaarrssee((''(()))) ''));;
PPaarrssee((''(((((()))))))))) ''));;
eenndd;;

bbeeggiinn

NNRRuulleess::== 00;;
IInniittGGrraammmmaarr;;
DDooPPaarrsseess;;
eenndd..

Figure 12.1 A full context-free parser

12.2 THE PROGRAM
As is usual with well-structured Pascal programs, the program of Figure 12.1 can mosteasily be read from the end backwards. The body of the program initializes the number

of rules in the grammar NNRRuulleess to zero, fills the array GGrraammmmaarr by calling IInniittGGrraamm--
mmaarr and then calls DDooPPaarrsseess. The elements of GGrraammmmaarr are of the type RRuulleeTTyyppeeand consist of a

LLhhssFFiieelldd of the type SSyymmbboollTTyyppee and a RRhhssFFiieelldd which is apacked array of
SSyymmbboollTTyyppee. Packed arrays of SSyymmbboollTTyyppee use NNooSSyymmbbooll as a fillerand are required to contain at least one filler.

IInniittGGrraammmmaarr sets the SSttaarrtt symboland fills the array
GGrraammmmaarr through successive calls of SSttoorreeRRuullee, which alsoincreases
NNRRuulleess.In Figure 12.1, the grammar has been built into the program for simplicity; in

practice IInniittGGrraammmmaarr would read the grammar and call SSttoorreeRRuullee as needed. Thesame technique is used for

DDooPPaarrsseess: the input strings are part of the program text forsimplicity, whereas they would normally be read in or obtained in some other fashion.

DDooPPaarrsseess calls PPaarrssee for each input string (which again uses NNooSSyymmbbooll as a filler).
PPaarrssee initializes some variables of the parser, copies the input string to the global vari-able

IInnppuuttSSttrriinngg# and then comes to its main task: calling TTrryyAAllllRRuulleessFFoorr, to tryall rules for the

SSttaarrtt symbol to match IInnppuuttSSttrriinngg from 1 to IInnppuuttLLeennggtthh.Immediately above the declaration of

PPaarrssee we find the body of TTrryyAAllllRRuulleess--
FFoorr, which is seen to scan the the grammar for the proper left-hand side and to call
TTrryyRRuullee when it has found one.To understand the workings of

TTrryyRRuullee, we have to consider the parsing stack,implemented as the array
SSttaacckk. Its elements correspond to the nodes just on the leftof the dotted line in Figure 6.2 and together they form a list of goals to be achieved for

the completion of the parse tree presently under consideration. Each element (of the
\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

If the parser is incorporated in a larger Pascal program, many of the globally defined namescan be made local to the procedure

PPaarrssee. Although this technique reduces the danger of con-fusion between names when there are many levels, we have not done so here since it is artificial

to do so for the top level.

Sec. 12.2] The program 259
type SSttaacckkEElleemmTTyyppee) describes a rule, given by its number (NNmmbbFFiieelldd) and howmuch of its right-hand side has already been matched with the input (

RRhhssUUsseeddFFiieelldd);furthermore it records where the matched part in
IInnppuuttSSttrriinngg starts (PPoossFFiieelldd),how much must be matched (
LLeennFFiieelldd) and how much has already been matched(
IInnppUUsseeddFFiieelldd). An element is stacked by a call PPuusshhSSttaacckk((nnmmbb,, ppooss,, lleenn)),which records the number, position and length of the goal and sets both

UUsseeddFFiieelldds tozero. An element is removed by calling
PPooppSSttaacckk.
SSttaacckk contains the active nodes in the parse tree, which is only a fraction of thenodes of the parse tree as already recognized (Figure 6.2). The left-most derivation of

the parse tree as far as recognized can be found on the stack RRuulleeSSttaacckk, as an array ofrule numbers. When the parse stack

SSttaacckk becomes empty, a parsing has been found,recorded in
RRuulleeSSttaacckk.To prepare the way for a subsequent addition to the parser of a method to

remember known parsings, three hooks have been placed, SSttaarrttNNeewwKKnnoowwnnGGooaall,
RReeccoorrddKKnnoowwnnPPaarrssiinngg and KKnnoowwnnGGooaallSSuucccceeeeddss, each of which corresponds to adummy procedure or function. We shall ignore them until Section 12.3.

We now return to TTrryyRRuullee. Ignoring for the moment the tests nnoott IIssTToo--
BBeeAAvvooiiddeedd and nnoott KKnnoowwnnGGooaallSSuucccceeeeddss, we see that a match of rule number nnmmbbwith the input from

ppooss over lleenn symbols is established as a goal by calling
PPuusshhRRuullee. The goal is then pursued by calling the local procedure DDooTTooppOOffSSttaacckk.When this call returns,

TTrryyRRuullee is careful to restore the original situation, to allowfurther parsings to be found.

DDooTTooppOOffSSttaacckk is the most complicated of our system of (mutually recursive)procedures. It starts by examining the top element on the stack and establishes what the
first symbol in the right-hand side is that has not yet been matched (ttoossSSyymmbb). If this is
NNooSSyymmbbooll, the right-hand side is exhausted and cannot match anything any more. Thatis all right, however, if the input has been completely matched too, in which case we

call DDooNNeexxttOOnnSSttaacckk; otherwise the goal has failed and DDooTTooppOOffSSttaacckk returns. If theright-hand side is not exhausted, it is possible that we find a direct match of the terminal in the right-hand side (if there is one) to the terminal in the input. In that case werecord the match in the top element on the stack through a call of

AAddvvaanncceeTTOOSS((11)) andcall
DDooTTooppOOffSSttaacckk recursively to continue our search.If there is no direct match, we assume that

ttoossSSyymmbb is a non-terminal and use acall to the local procedure
TTrryyAAllllLLeennggtthhssFFoorr to try matches for all appropriatelengths of segments of the input starting at the first unmatched position. Since we do

not visibly distinguish between a terminal and a non-terminal in our program (one ofthe oversimplifications), we cannot prevent

TTrryyAAllllLLeennggtthhssFFoorr from being called fora terminal symbol. Since there is no rule for that terminal, the calls of

TTrryyAAllllRRuulleess--
FFoorr inside TTrryyAAllllLLeennggtthhssFFoorr will find no match.The local procedure

TTrryyAAllllLLeennggtthhssFFoorr selects increasingly longer segments ofthe input, and calls
TTrryyAAllllRRuulleessFFoorr for each of them; the latter procedure is alreadyknown.

DDooNNeexxttOOnnSSttaacckk is called when the goal on top of the stack has been attained.The top element of

SSttaacckk is removed and set aside, to be restored later to allow furtherparsings to be found. If this removes the last element from the stack, a parsing has been

found and the corresponding procedure is called. If not, there is more work to do on thepresent partial parsing. The successful match is recorded in the element presently on
top of the stack (which ordered the just attained match) through a call of AAddvvaanncceeTTOOSS,

260 A simple general context-free parser [Ch. 12
and DDooTTooppOOffSSttaacckk is called to continue the search. All procedures take care to restorethe original situation.

The other procedures except IIssTTooBBeeAAvvooiiddeedd perform simple administrativetasks only.
Note that besides the parse stack and the rule stack, there is also a search stack.Whereas the former are explicit, the latter is implicit and is contained in the Pascal
recursion stack and the variables ssvv in incarnations of DDooNNeexxttOOnnSSttaacckk.
12.2.1 Handling left recursionIt has been explained in Section 6.3.1 that a top-down parser will loop on a leftrecursive grammar and that this problem can be avoided by making sure that no goal isaccepted when that same goal is already being pursued. This is achieved by the test
nnoott IIssTTooBBeeAAvvooiiddeedd in TTrryyRRuullee. When a new goal is about to be put on the parsestack, the function

IIssTTooBBeeAAvvooiiddeedd is called, which runs down the parse stack to seeif the same goal is already active. If it is,

IIssTTooBBeeAAvvooiiddeedd returns ttrruuee and the goal isnot tried for the second time.

The program in Figure 12.1 was optimized for brevity and, hopefully, for clarity.It contains many obvious inefficiencies, the removal of which will, however, make the
program larger and less perspicuous. The reader will notice that the semicolon wasused as a terminator rather than as a separator; the authors find that this leads to a
clearer style.

12.3 PARSING IN POLYNOMIAL TIME
An effective and relatively simple way to avoid exponential time requirement in acontext-free parser is to equip it with a well-formed substring table, often abbreviated

to WFST. A WFST is a table that shows all partial parse trees for each substring (seg-ment) of the input string; it is very similar to the table generated by the CYK algorithm.
It is can be shown that the amount of work needed to construct the table cannot exceedO (n k +1) where n is the length of the input string and k is the maximum number of
non-terminals in any right-hand side. This takes the exponential sting out of the depth-first search.

The WFST can be constructed in advance (which is what the CYK algorithmdoes), or while parsing proceeds ("on the fly"). We shall do the latter here. Also, rather
than using a WFST as defined above, we shall use a known-parsing table, which showsthe partial parse trees for each combination of a grammar rule and a substring. These
two design decisions have to do with the order in which the relevant informationbecomes available in the parser of Figure 12.1.

KKnnoowwnnPPaarrssiinnggTTyyppee == rreeccoorrdd SSttaarrttFFiieelldd,, EEnnddFFiieelldd:: iinntteeggeerr;; eenndd;;
KKnnoowwnnGGooaallTTyyppee ==
rreeccoorrdd

NNmmbbFFiieelldd:: RRuulleeNNmmbbTTyyppee;; PPoossFFiieelldd,, LLeennFFiieelldd:: iinntteeggeerr;;{{ tthhee ggooaall}}
SSttaarrttPPaarrssiinnggFFiieelldd:: iinntteeggeerr;; {{ tteemmppoorraarryy vvaarriiaabbllee }}
KKnnoowwnnPPaarrssiinnggFFiieelldd:: aarrrraayy [[11....AArrrraayySSiizzee]] ooff KKnnoowwnnPPaarrssiinnggTTyyppee;;
NNKKnnoowwnnPPaarrssiinnggssFFiieelldd:: iinntteeggeerr;;
eenndd;;

Sec. 12.3] Parsing in polynomial time 261

KKnnoowwnnGGooaallLLiisstt:: aarrrraayy [[11....AArrrraayySSiizzee]] ooff KKnnoowwnnGGooaallTTyyppee;;
NNKKnnoowwnnGGooaallss:: iinntteeggeerr;;
KKnnoowwnnRRuulleeLLiisstt:: aarrrraayy [[11....AArrrraayySSiizzee]] ooff RRuulleeNNmmbbTTyyppee;;
NNKKnnoowwnnRRuulleess:: iinntteeggeerr;;

ffuunnccttiioonn KKnnoowwnnGGooaallNNuummbbeerr((nnmmbb:: RRuulleeNNmmbbTTyyppee;; ppooss,, lleenn:: iinntteeggeerr))::iinntteeggeerr;;

vvaarr nn:: iinntteeggeerr;;
bbeeggiinn

KKnnoowwnnGGooaallNNuummbbeerr::== 00;;
ffoorr nn::== 11 ttoo NNKKnnoowwnnGGooaallss ddoo

wwiitthh KKnnoowwnnGGooaallLLiisstt[[nn]] ddoo

iiff ((nnmmbb==NNmmbbFFiieelldd))
aanndd ((ppooss==PPoossFFiieelldd))
aanndd ((lleenn==LLeennFFiieelldd)) tthheenn

KKnnoowwnnGGooaallNNuummbbeerr::== nn;;
eenndd {{ KKnnoowwnnGGooaallNNuummbbeerr }};;

pprroocceedduurree SSttaarrttNNeewwKKnnoowwnnGGooaall((nnmmbb:: RRuulleeNNmmbbTTyyppee;; ppooss,, lleenn:: iinntteeggeerr));;

bbeeggiinn

NNKKnnoowwnnGGooaallss::== NNKKnnoowwnnGGooaallss++11;;
wwiitthh KKnnoowwnnGGooaallLLiisstt[[NNKKnnoowwnnGGooaallss]] ddoo
bbeeggiinn

NNmmbbFFiieelldd::== nnmmbb;; PPoossFFiieelldd::== ppooss;; LLeennFFiieelldd::== lleenn;;
SSttaarrttPPaarrssiinnggFFiieelldd::== NNRRuulleessSSttaacckkeedd++11;;
NNKKnnoowwnnPPaarrssiinnggssFFiieelldd::== 00;;
eenndd;;
eenndd {{ SSttaarrttNNeewwKKnnoowwnnGGooaall }};;

pprroocceedduurree RReeccoorrddKKnnoowwnnPPaarrssiinngg;;

vvaarr nn,, ii:: iinntteeggeerr;;
bbeeggiinn

wwiitthh SSttaacckk[[NNSSttaacckkEElleemmss]] ddoo
bbeeggiinn

nn::== KKnnoowwnnGGooaallNNuummbbeerr((NNmmbbFFiieelldd,, PPoossFFiieelldd,, LLeennFFiieelldd));;
wwiitthh KKnnoowwnnGGooaallLLiisstt[[nn]] ddoo
bbeeggiinn

NNKKnnoowwnnPPaarrssiinnggssFFiieelldd::== NNKKnnoowwnnPPaarrssiinnggssFFiieelldd++11;;
wwiitthh KKnnoowwnnPPaarrssiinnggFFiieelldd[[NNKKnnoowwnnPPaarrssiinnggssFFiieelldd]] ddoo
bbeeggiinn

SSttaarrttFFiieelldd::== NNKKnnoowwnnRRuulleess++11;;
ffoorr ii::== SSttaarrttPPaarrssiinnggFFiieelldd ttoo NNRRuulleessSSttaacckkeedd ddoo
bbeeggiinn

NNKKnnoowwnnRRuulleess::== NNKKnnoowwnnRRuulleess++11;;
KKnnoowwnnRRuulleeLLiisstt[[NNKKnnoowwnnRRuulleess]]::== RRuulleeSSttaacckk[[ii]];;
eenndd;;
EEnnddFFiieelldd::== NNKKnnoowwnnRRuulleess;;
eenndd;;
eenndd;;
eenndd;;
eenndd {{ RReeccoorrddKKnnoowwnnPPaarrssiinngg }};;

ffuunnccttiioonn KKnnoowwnnGGooaallSSuucccceeeeddss

((nnmmbb:: RRuulleeNNmmbbTTyyppee;; ppooss,, lleenn:: iinntteeggeerr)):: BBoooolleeaann;;
vvaarr nn,, oollddNNRRuulleessSSttaacckkeedd,, ii,, jj:: iinntteeggeerr;;
bbeeggiinn

nn::== KKnnoowwnnGGooaallNNuummbbeerr((nnmmbb,, ppooss,, lleenn));;

262 A simple general context-free parser [Ch. 12

iiff nn == 00 tthheenn KKnnoowwnnGGooaallSSuucccceeeeddss::== ffaallssee eellssee
bbeeggiinn

oollddNNRRuulleessSSttaacckkeedd::== NNRRuulleessSSttaacckkeedd;;
wwiitthh KKnnoowwnnGGooaallLLiisstt[[nn]] ddoo
bbeeggiinn

ffoorr ii::== 11 ttoo NNKKnnoowwnnPPaarrssiinnggssFFiieelldd ddoo

wwiitthh KKnnoowwnnPPaarrssiinnggFFiieelldd[[ii]] ddoo
bbeeggiinn

ffoorr jj::== SSttaarrttFFiieelldd ttoo EEnnddFFiieelldd ddoo
bbeeggiinn

NNRRuulleessSSttaacckkeedd::== NNRRuulleessSSttaacckkeedd++11;;
RRuulleeSSttaacckk[[NNRRuulleessSSttaacckkeedd]]::==

KKnnoowwnnRRuulleeLLiisstt[[jj]];;
eenndd;;
AAddvvaanncceeTTOOSS((lleenn));;
DDooTTooppOOffSSttaacckk;;
RReettrraaccttTTOOSS((lleenn));;
NNRRuulleessSSttaacckkeedd::== oollddNNRRuulleessSSttaacckkeedd;;
eenndd;;
eenndd;;
KKnnoowwnnGGooaallSSuucccceeeeddss::== ttrruuee;;
eenndd;;
eenndd {{ KKnnoowwnnGGooaallSSuucccceeeeddss }};;

NNKKnnoowwnnGGooaallss::== 00;; NNKKnnoowwnnRRuulleess::== 00;; {{ iinn pprroocceedduurree PPaarrssee }}

Figure 12.2 Additions for the known-parsing table
Our parser can be equipped with the known-parsing table by incorporating thedeclarations of Figure 12.2 in it.

SSttaarrttNNeewwKKnnoowwnnGGooaall, RReeccoorrddKKnnoowwnnPPaarrssiinngg and
KKnnoowwnnGGooaallSSuucccceeeeddss replace the dummy declarations in Figure 12.1, the otherdeclarations and the initialization statement are to be inserted in the appropriate places.

The thus modified parser will no longer require exponential time (if sufficient memoryis supplied; see the constant declarations).

Returning to the mechanism of the parser, we see that when a new goal is esta-blished in

TTrryyRRuullee for which IIssTTooBBeeAAvvooiiddeedd yields ffaallssee, a call is made to
KKnnoowwnnGGooaallSSuucccceeeeddss. This function accesses the known-parsing table to find out ifthe goal has been pursued before. When called for the very first time, it will yield

ffaallssee since there are no known parsings yet and nnoott KKnnoowwnnGGooaallSSuucccceeeeddss willsucceed as in the unmodified parser. We enter the block that really tries the rule, preceded by a call to SSttaarrttNNeewwKKnnoowwnnGGooaall. This prepares the table for the recording ofthe zero or more parsings that will be found for this new goal.

Goals are recorded in a three-level data structure. The first level is the array
KKnnoowwnnGGooaallLLiisstt, whose elements are of type KKnnoowwnnGGooaallTTyyppee. A known goal hasthree fields describing the rule number, position and length of the goal and a

KKnnoowwnn--
PPaarrssiinnggFFiieelldd, which is an array of elements of type KKnnoowwnnPPaarrssiinnggTTyyppee andwhich forms the second level; it has also a field

SSttaarrttPPaarrssiinnggFFiieelldd, which is onlymeaningful while the present table entry is being constructed. Each element in

KKnnoowwnn--
PPaarrssiinnggFFiieelldd describes a partial parse tree for the described goal. The partial parsetree is represented as a list of rule numbers, just as the full parse tree. These lists are

stored one after another in the array KKnnoowwnnRRuulleeLLiisstt, which is the third level; the

Sec. 12.3] Parsing in polynomial time 263
beginning and end of each parsing are recorded in the SSttaarrttFFiieelldd and EEnnddFFiieelldd ofthe known parsing.

SSttaarrttNNeewwKKnnoowwnnGGooaall records the goal in a new element of KKnnoowwnnGGooaallLLiisstt. Itsets
SSttaarrttPPaarrssiinnggFFiieelldd to NNRRuulleessSSttaacckkeedd++11, since that is the position in RRuulleeSS--
ttaacckk where the first rule number for the present goal will go. When the main mechan-ism of the parser has found a parsing for the goal (in

DDooNNeexxttOOnnSSttaacckk) it calls
RReeccoorrddKKnnoowwnnPPaarrssiinngg, which adds the parsing found in RRuulleeSSttaacckk between
SSttaarrttPPaarrssiinnggFFiieelldd and NNRRuulleessSSttaacckkeedd to the present known goal under con-struction. To this end, it finds the pertinent element in

KKnnoowwnnGGooaallLLiisstt, adds an ele-ment to the corresponding
KKnnoowwnnPPaarrssiinnggFFiieelldd and copies the parsing to the array
KKnnoowwnnRRuulleeLLiisstt while recording the begin and end in the element in KKnnoowwnnPPaarrssiinngg--
FFiieelldd. There is no need to signal the end of the construction of a known goal. Notethat as long as the goal is under construction, it is also on the parse stack; this means

that IIssTTooBBeeAAvvooiiddeedd will yield ttrruuee which in turn guarantees that SSttaarrttNNeewwKKnnoowwnn--
GGooaall will not be called again while the known goal is being constructed.The next time the goal is tried by

TTrryyRRuullee, KKnnoowwnnGGooaallSSuucccceeeeddss will indeedsucceed: for each of the elements in the pertinent

KKnnoowwnnPPaarrssiinnggFFiieelldd, thecorresponding segment of
KKnnoowwnnRRuulleeLLiisstt, which contains one partial parse tree, iscopied to the
RRuulleeSSttaacckk as if the parsing had been performed normally. The advancein length is noted and

DDooTTooppOOffSSttaacckk is called, again just as in the normal parsing pro-cess. Upon its return, the original situation is restored, including the value of

NNRRuu--
lleessSSttaacckkeedd.It will be obvious that copying a ready-made solution is much faster than reconstructing that solution. That it makes the difference between exponential and polyno-mial behaviour is less obvious, but true nevertheless. The unmodified parser tries
41624 rules for the built-in example, the parser with the known-parsing table only 203.The new parser can be improved considerably in many points, with a corresponding
increase in efficiency; the O (n k +1) dependency remains, though.

13
Annotated bibliography

The purpose of this annotated bibliography is to supply the reader with more materialand with more detail than was possible in the preceding chapters, rather than to just list
the works referenced in the text. The annotations cover a considerable number of sub-jects that have not been treated in the rest of the book.

The articles and books presented here have been selected for two criteria:relevancy and accessibility. The notion of relevancy has been interpreted very widely;
parsers are used in an increasing number of applications and relevancy to others is hardto judge. In practice, entries have only been rejected if they were either too theoretical
or did not supply insight into or understanding of parsing. Accessibility has been takento mean ready availability to a researcher who has access to a moderately wellequipped university or company research library. We expect such a library to hold mostof the prominent computer science journals, but not all or even a major part of the
proceedings of conferences on programming languages and compiler construction, letalone technical reports from various research institutes all over the world. We have
often been forced to compromise this criterion, to include pertinent material not other-wise available; for instance, nothing seems to have been published on left-corner parsing in journals. Fortunately, relevant material that was first published in a technicalreport or as a PhD thesis was often published later in a prominent journal; in these
cases a reference to the original publication can be found by consulting the journalpaper referenced here. We have kept the references to technical reports to the absolute
minimum. No non-English (that is, no non-English-language) material has beenincluded. It is our intention that the present collection be complete under the above criteria, but we have no real hope that such perfection has been attained. We shall begrateful to be pointed to additional references.

The bibliography contains about 400 entries, almost all of them from the Westernworld. Some papers from the Soviet Union and Eastern Europe have been included, if
available in translation. Much work on parsing has been done and is still being done inJapan; it has been sorely underrepresented in this collection, for reasons of accessibility. Only readily available full material in translation has been included, although muchmore is available in the form of abstracts in English.

This annotated bibliography differs in several respects from the habitual literaturelist.
\Gamma  The entries have been grouped into fourteen categories:

Ch. 13] Annotated bibliography 265

13.1 miscellaneous literature (Misc);13.2 unrestricted PS and CS grammars (PSCS);
13.3 van Wijngaarden grammars and affix grammars (VW);13.4 general context-free parsers (CF);
13.5 LL parsing (LL);13.6 LR parsing (LR);
13.7 left-corner parsing (LC);13.8 precedence and bounded-context parsing (Precedence);
13.9 finite-state automata (FS);13.10 natural language handling (NatLang);
13.11 error handling (ErrHandl);13.12 transformations on grammars (Transform);
13.13 general books on parsing (Books);13.14 some books on computer science (CSBooks).
The nature of publications in parsing is so that the large majority of them caneasily be assigned a single category. Some that span two categories have been
placed in one, with a reference in the other. Publications of a more general naturehave found a place in "Miscellaneous Literature".
\Gamma  The entries are annotated. This annotation is not a copy of the abstract provided

with the paper (which generally said something about the results obtained) but israther the result of an attempt to summarize the technical content in terms of what

has been explained elsewhere in this book.
\Gamma  The entries are ordered chronologically rather than alphabetically.

This arrangement has the advantage that it is much more meaningful than a singlealphabetic list, ordered on author names. Each section can be read as the history of

research on that particular aspect of parsing, related material is found closelytogether and recent material is easily separated from older publications. A disadvantage is that it is now difficult to locate entries by author; to remedy this, anauthor index has been supplied. Only a tiny fraction of the entries is referred to in
the previous chapters; these occurrences are also included in the author index.Terms from computer science have been used more freely in the annotations than in the
rest of the book (an example is "transitive closure"). See, for instance, Sedgewick[CSBooks 1988] or Smith [CSBooks 1989] for an explanation.

Note that there is a journal called Computer Languages (Elmsford, NY) and onecalled Computer Language (San Francisco, CA); both abbreviate to Comput. Lang.; the
place name is essential to distinguish between them (although the first originates fromExeter, Devon, England).

13.1 MISCELLANEOUS LITERATURE

\Gamma  Noam Chomsky, "Three models for the description of language", IEEE Trans.

Inform. Theory, vol. 2, no. 3, p. 113-124, 1956. In an attempt to delineate the set ofcorrect English sentences, the author considers three mechanisms. Finite-state automata are rejected on

the grounds that they cannot cope with arbitrary nesting. Phrase structure grammars are considered prob-ably applicable but declared impractical due to their problems in expressing context conditions. Most of
these problems can be solved if we augment PS grammars with transformation rules, which specify therearrangement of parts of the derivation tree.

\Gamma  Noam Chomsky, "On certain formal properties of grammars", Inform. Control,

266 Annotated bibliography [Ch. 13

vol. 2, p. 137-167, 1959. This article discusses what later became known as the Chomskyhierarchy. Chomsky defines type 1 grammars in the "context-sensitive" way. His motivation for this is
that it permits the construction of a tree as a structural description. Type 2 grammars exclude e-rules, soin Chomsky's system, type 2 grammars are a subset of type 1 grammars.
Next, the so called counter languages are discussed. A counter language is a language recognized by afinite automaton, extended with a finite number of counters, each of which can assume infinitely many
values. L 1={a nb n | n>0} is a counter language, L 2={xy | x,yI^{a,b}*, y is the mirror image of x} is not,so there are type 2 languages that are not counter languages. The reverse is not investigated.
The Chomsky Normal Form is introduced, but not under that name, and a bit different: Chomsky calls atype 2 grammar

regular if production rules have the form Ao""a or Ao""BC, with Bz'C, and if Ao""aAband
Ao""gAh then a=g and b=h. A grammar is self-embedding if there is a derivation Ao""* aAb with az'eand bz'e. The bulk of the paper is dedicated to the theorem that the extra power of type 2 grammars over

type 3 grammars lies in this self-embedding property.

\Gamma  J.W. Backus, F.L. Bauer, J. Green, C. Katz, J. McCarthy, P. Naur (Ed.), A.J.

Perlis, H. Rutishauser, K. Samelson, B. Vauquois, J.H. Wegstein, A.van Wijngaarden, M. Woodger, "Report on the algorithmic language ALGOL

60", Commun. ACM, vol. 3, no. 5, p. 299-314, May 1960. First application of a BNFgrammar (for the definition of a programming language). Revised report by the same authors:

Commun.
ACM, vol. 6, no. 1, p. 1-17, Jan 1963.

\Gamma  R.A. Brooker, I.R. MacCallum, D. Morris, J.S. Rohl, "The compiler-compiler",

Annual Review in Automatic Programming, vol. 3, p. 229-275, 1960 ????. One ofthe first extensive descriptions of a compiler-compiler. Parsing is done by a backtracking non-exhaustive

top-down parser using a transduction-like grammar. This grammar is kept in an integrated form andmodifications can be made to it while parsing.

\Gamma  Robert W. Floyd, "A descriptive language for symbol manipulation", J. ACM,

vol. 8, p. 579-584, Oct 1961. Original paper describing Floyd productions. See Section 9.3.1.
\Gamma  Robert S. Ledley, James B. Wilson, "Automatic-programming-language translation through syntactical analysis", Commun. ACM, vol. 5, no. 3, p. 145-155,March 1962.

An English-to-Korean (!) translation system is described in detail, in which partsof the Korean translation are stored in attributes in the parse tree, to be reordered and interspersed with

Korean syntactic markers on output. The parser is Irons' [CF 1961].

\Gamma  Melvin E. Conway, "Design of a separable transition-diagram compiler", Commun. ACM, vol. 6, no. 7, p. 396-408, July 1963. The first to introduce coroutines and toapply them to structure a compiler. The parser is Irons' [CF 1961], made deterministic by a No-Loop

Condition and a No-Backup Condition. It follows transition diagrams rather than grammar rules.

\Gamma  Robert W. Floyd, "The syntax of programming languages - a survey", IEEE

Trans. Electronic Comput., vol. EC-13, p. 346-353, 1964. Early analysis of theadvantages of and problems with the use of grammars for the specification of programming languages.

Contains a bibliography of almost 100 entries.

\Gamma  Jerome Feldman, David Gries, "Translator writing systems", Commun. ACM,

vol. 11, no. 2, p. 77-113, Feb. 1968. Grand summary of the work done on parsers (withsemantic actions) before 1968.

\Gamma  D.J. Cohen, C.C. Gotlieb, "A list structure form of grammars for syntactic

analysis", Computing Surveys, vol. 2, no. 1, p. 65-82, 1970. CF rules are represented aslinked lists of alternatives, which themselves are linked lists of members. The trick is that both lists end

in different null pointers. This representation is very amenable to various backtracking and non-backtracking top-down and bottom-up parsing methods (by interpreting the grammar). Several practical
parsers are given in flowchart form. An algorithm is given to "invert" a grammar, i.e. the linked lists, to

Sec. 13.1] Miscellaneous literature 267
create a data structure that will efficiently guide a bottom-up parser.

\Gamma  A. Birman, J.D. Ullman, "Parsing algorithms with backtrack", Inform. Control,

vol. 23, no. 1, p. 1-34, 1973. Models classes of recursive descent parsers, capable ofrecognizing all deterministic context-free languages and also some non-context-free languages.

\Gamma  B.W. Kernighan, L.L. Cherry, "A system for typesetting mathematics", Commun.

ACM, vol. 18, no. 3, p. 151-157, March 1975. A good example of the use of anambiguous grammar to specify the preferred analysis of special cases.

\Gamma  A.V. Aho, S.C. Johnson, J.D. Ullman, "Deterministic parsing of ambiguous grammars", Commun. ACM, vol. 18, no. 8, p. 441-452, 1975. Demonstrates how LL and LRparsers can be constructed for certain classes of ambiguous grammars, using simple disambiguating

rules, such as operator-precedence.

\Gamma  Jacques Cohen, "Experience with a conversational parser generation system",

Softw. Pract. Exper., vol. 5, p. 169-180, 1975. Realistic description of the construction ofa professional interactive parser generator.

\Gamma  Jay Earley, "Ambiguity and precedence in syntax description", Acta Inform., vol.

4, p. 183-192, 1975. Informal description of how to use precedence information fordisambiguation.

\Gamma  Michael Marcotty, Henry F. Ledgard, Gregor V. Bochmann, "A sampler of formal definitions", Computing Surveys, vol. 8, no. 2, p. 191-276, June 1976.Describes and compares four semantic definition methods: VW grammars, production systems and the

axiomatic approach, Vienna Definition Language, and attribute grammars. No clear winner emerges.

\Gamma  R.M. Wharton, "Resolution of ambiguity in parsing", Acta Inform., vol. 6, no. 4,

p. 387-395, 1976. It is proposed that ambiguity be resolved in a bottom-up parser by 1)reducing upon a shift/reduce conflict, 2) reducing the shorter right-hand side upon a reduce/reduce conflict and 3) reducing the textual first right-hand side upon a reduce/reduce conflict with equal lengths. Ina top-down parser, criteria similar to 2) and 3) are applied to each LL(1) conflict.

\Gamma  R.B. Hunter, A.D. McGettrick, R. Patel, "LL versus LR parsing with illustrations

from Algol 68", ACM SIGPLAN Notices, vol. 12, no. 6, p. 49-53, June 1977.Syntax-improved LL(1) (Foster [Transform 1968]) and LR(1) are equally unsuccessful in handling a CF

version of the grammar of Algol 68. After hand adaptation LL(1) has the advantage.

\Gamma  Niklaus Wirth, "What can we do about the unnecessary diversity of notation for

syntactic definitions?", Commun. ACM, vol. 20, no. 11, p. 822-823, Nov 1977.Introduces Wirth's notation for extended CF grammars, using

{{......}} for repetition, [[......]] for optional-ity,
((......)) for grouping and ""......"" for quoting.

\Gamma  Jacques Cohen, Martin S. Roth, "Analyses of deterministic parsing algorithms",

Commun. ACM, vol. 21, no. 6, p. 448-458, June 1978. Gives methods to calculate theaverage parsing times and their standard deviations from the input grammar, for several parsers. The

resulting formulae are finite series, and are sometimes given in closed form.

\Gamma  Kuo-Chung Tai, "On the implementation of parsing tables", ACM SIGPLAN

Notices, vol. 14, no. 1, p. 100-101, Jan 1979. How to implement parsing tables usinghashing.

\Gamma  Peter Deussen, "One abstract accepting algorithm for all kinds of parsers". In

Automata, languages and programming, Hermann A. Maurer (eds.), LectureNotes in Computer Science #71, Springer-Verlag, Berlin, p. 203-217, 1979.

Parsing

268 Annotated bibliography [Ch. 13
is viewed as an abstract search problem, for which a high-level algorithm is given. The selection predi-cate involved is narrowed down to give known linear parsing methods.

\Gamma  Robert Endre Tarjan, Andrew Chi-Chih Yao, "Storing a sparse table", Commun.

ACM, vol. 22, no. 11, p. 606-611, Nov. 1979. Two methods of storing sparse tables arepresented and analysed: trie structure and double displacement.

\Gamma  Hanan Samet, "A coroutine approach to parsing", ACM Trans. Prog. Lang. Syst.,

vol. 2, no. 3, p. 290-306, July 1980. Some inputs consist of interleaved chunks of textconforming to different grammars. An example is programming text interrupted at unpredictable points

by macro-processor directives. This situation can be handled by having separate parsers for each gram-mar, cooperating in coroutine fashion.

\Gamma  Anton Nijholt, "Parsing strategies: A concise survey". In Mathematical Foundations of Computer Science, J. Gruska & M. Chytil (eds.), Lecture Notes in Com-puter Science #118, Springer-Verlag, Berlin, p. 103-120, 1981.

The context-freeparser and language field is surveyed in terse prose. Highly informative to the connoisseur.

\Gamma  Esko Ukkonen, "Lower bounds on the size of deterministic parsers", J. Comput.

Syst. Sci., vol. 26, p. 153-170, 1983. Worst-case lower bounds for the parser sizes are givenfor the various classes of LL(

k) and LR(k) parsers for k=0,1 and k>=2. All LL(k) lower bounds are poly-nomial, except the one for full LL(

k >1), which is exponential; all LR(k) bounds are exponential.

\Gamma  Fernando C.N. Pereira, David H.D. Warren, "Parsing as deduction". In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics,Cambridge, Mass., p. 137-144, 1983.

The Prolog deduction mechanism is top-down depth-first. It can be exploited to do parsing, using Definite Clause grammars. Parsing can be done more efficiently with Earley's technique. The corresponding Earley deduction mechanism is derived andanalysed.

\Gamma  Anton Nijholt, Deterministic top-down and bottom-up parsing: historical notes

and bibliographies, Mathematisch Centrum, Amsterdam, p. 118, 1983. Over a 1000references about LL(

k), LR(k) and precedence parsing, with short histories and surveys of the threemethods.

\Gamma  Peter Dencker, Karl Du"rre, Johannes Heuft, "Optimization of parser tables for

portable compilers", ACM Trans. Prog. Lang. Syst., vol. 6, no. 4, p. 546-572, Oct1984.

Given an n*m parser table, an n*m bit table is used to indicate which entries are errorentries; this table is significantly smaller than the original table and the remaining table is now sparse

(typically 90-98% don't-care entries). The remaining table is compressed row-wise (column-wise) bysetting up an interference graph in which each node corresponds to a row (column) and in which there is
an edge between any two nodes the rows (columns) of which occupy an element in the same position. A(pseudo-)optimal partitioning is found by a minimal graph-colouring heuristic.

\Gamma  W.M. Waite, L.R. Carter, "The cost of a generated parser", Softw. Pract. Exper.,

vol. 15, no. 3, p. 221-237, 1985. Supports with measurements the common belief thatcompilers employing generated parsers suffer significant performance degradation with respect to recursive descent compilers. Reasons: interpretation of parse tables versus direct execution, attribute storageallocation and the mechanism to determine which action(s) to perform. Then, a parser interface is proposed that simplifies integration of the parser; implementation of this interface in assembly languageresults in generated parsers that cost the same as recursive descent ones. The paper does not consider
generated recursive descent parsers.

\Gamma  Gerard D. Finn, "Extended use of null productions in LR(1) parser applications",

Commun. ACM, vol. 28, no. 9, p. 961-972, Sept 1985. Extensive account of how to usean LR parser for conversion purposes. Makes a strong case for the use of parsers for conversion.

Sec. 13.1] Miscellaneous literature 269
Contains a good introduction to parsing.

\Gamma  Robert Gerardy, "Experimental comparison of some parsing methods", ACM

SIGPLAN Notices, vol. 22, no. 8, p. 79-88, Aug 1987. Experimental time measurementsfor recursive descent, operator-precedence and SLR(1) parsing show a ratio of 1 : 4 : 10, in that order.

All parsers were written in Pascal and parsed a mini-Pascal language.

\Gamma  Michael Share, "Resolving ambiguities in the parsing of translation grammars",

ACM SIGPLAN Notices, vol. 23, no. 8, p. 103-109, Aug 1988. The UNIX LALRparser generator

yacc is extended to accept LALR conflicts and to produce a parser that requests aninteractive user decision when a conflict occurs while parsing. The system is used in document conversion.

\Gamma  Josef Grosch, "Generators for high-speed front-ends". In Compiler Compilers

and High-Speed Compilation, D. Hammer (eds.), Lecture Notes in Computer Sci-ence #371, Springer-Verlag, Berlin, p. 81-92, 1989.

A coherent system of lexicalscanner generator, LALR(1) parser generator and LL(1) parser generator, using a uniform input syntax,

is presented. The scanner beats UNIX lex by a factor of 5, the LALR parser beats yacc by a factor of 2.

\Gamma  Vance E. Waddle, "Production trees: a compact representation of parsed programs", ACM Trans. Prog. Lang. Syst., vol. 12, no. 1, p. 61-83, Jan 1990.Redundant items are removed from a traditional parse tree through a number of techniques: unit productions are contracted, terminals symbols are removed, structure information in links is replaced by a rulenumber, etc. Each node in the resulting parse tree corresponds to one right-hand side and contains the
rule number and a list of pointer to the nodes for the non-terminals in the right-hand side. A space savingof a factor 20 is achieved on the average. A grammar form that corresponds more closely to this
representation is defined.

\Gamma  Frank G. Pagan, "Comparative efficiency of general and residual parsers", ACM

SIGPLAN Notices, vol. 25, no. 4, p. 59-68, April 1990. The switch from table-drivenLL(1) to recursive descent or from table-driven LR(1) to recursive ascent is viewed as an example of

partial computation. Underlying theory and examples are given.

\Gamma  Boris Burshteyn, "On the modification of the formal grammar at parse time",

ACM SIGPLAN Notices, vol. 25, no. 5, p. 117-123, May 1990. Modifying thegrammar under control of and utilizing information obtained by the parsing process is proposed as a

means of handling context-sensitivity. For example, the recognition of the declaration of an array aaaaaacould cause the introduction of a new grammar rule

expro""aaaaaa[[expr]], (generated from a template), thusallowing forms like
aaaaaa[[33]] to be used. With a correction in the same journal, Vol. 25, No. 8, p 6.

13.2 UNRESTRICTED PS AND CS GRAMMARS
This section also covers some other non-context-free grammar types, excluding VW(two-level) grammars and affix grammars, for which see Section 13.3.

\Gamma  Alfred V. Aho, "Indexed grammars - an extension of context-free grammars", J.

ACM, vol. 15, no. 4, p. 647-671, Oct 1968. In an indexed grammar, each non-terminal Nin a sentential form is followed by zero or more "indices", which govern which of the alternatives for

Nare allowed for this occurrence of
N. The indices propagate according to specific rules.
L (CF) I` L (Indexed) I` L (CS).

\Gamma  William A. Woods, "Context-sensitive parsing", Commun. ACM, vol. 13, no. 7, p.

437-445, July 1970. The paper presents a canonical form for context-sensitive (e-free)derivation trees. Parsing is then performed by an exhaustive guided search over these canonical forms

exclusively. This guarantees that each possible parsing will be found exactly once.

270 Annotated bibliography [Ch. 13

\Gamma  Jacques Loeckx, "The parsing for general phrase-structure grammars", Inform.

Control, vol. 16, p. 443-464, 1970. The paper sketches two non-deterministic parsers for PSgrammars, one top-down, which tries to mimic the production process and one bottom-up, which tries to

find a handle. The instructions of the parsers are derived by listing all possible transitions in the gram-mar and weeding out by hand those that cannot occur. Trial-and-error methods are discussed to resolve
the non-determinism, but no instruction selection mechanisms are given. Very readable, nice pictures.

\Gamma  Daniel A. Walters, "Deterministic context-sensitive languages, Parts I & II",

Inform. Control, vol. 17, no. 1, p. 14-61, 1970. The definition of LR(k) grammars isextended to context-sensitive grammars. Emphasis is more on theoretical properties than on obtaining a

parser.

\Gamma  Z.J. Ghandour, "Formal systems and analysis of context-sensitive languages",

Computer J., vol. 15, no. 3, p. 229-237, 1972. Ghandour describes a formal productionsystem that is in some ways similar to but far from identical to a two-level grammar. A hierarchy of 4

classes is defined on these systems, with Class 1 E^ Class 2 E' Class 3 E' Class 4, Class 1 E^ CS and Class4 = CF. A parsing algorithm for Class 1 systems is given in fairly great detail.

\Gamma  N.A. Khabbaz, "Multipass precedence analysis", Acta Inform., vol. 4, p. 77-85,

1974. A hierarchy of CS grammars is given that can be parsed using multipass precedenceparsing. The parser and the table construction algorithm are given explicitly.

\Gamma  Eberhard Bertsch, "Two thoughts on fast recognition of indexed languages",

Inform. Control, vol. 29, p. 381-384, 1975. Proves that parsing with (tree-)unambiguousindexed grammars is possible in

O(n 2) steps.

\Gamma  Robert W. Sebesta, Neil D. Jones, "Parsers for indexed grammars", Intern. J.

Comput. Inform. Sci., vol. 7, no. 4, p. 344-359, Dec 1978. Very good explanation ofindexed grammars. Three classes of indexed grammars are defined, corresponding to strong-LL, LL and

LR, respectively. It is shown that the flag sets generated by indexed grammars are regular sets.

\Gamma  C.J.M. Turnbull, E.S. Lee, "Generalized deterministic left to right parsing", Acta

Inform., vol. 12, p. 187-207, 1979. The LR(k) parsing machine is modified so that thereduce instruction removes the reduced right-hand side from the stack and pushes an arbitrary number of

symbols back into the input stream. (The traditional LR(k) reduce is a special case of this: it pushes therecognized non-terminal back into the input and immediately shifts it. The technique is similar to that put
forward by Do"mo"lki [CF 1968].) The new machine is capable of parsing efficiently a subset of the Type0 grammars,

DRP grammars (for Deterministic Regular Parsable). Membership of this subset is unde-cidable, but an approximate algorithm can be constructed by extending the LR(

k) parse table algorithm.Details are not given, but can be found in a technical report by the first author.

\Gamma  Kurt Mehlhorn, "Parsing macro grammars top down", Inform. Control, vol. 40,

no. 2, p. 123-143, 1979. Macro grammars are defined as follows. The non-terminals in a CFgrammar are given parameters, as if they were routines in a programming language. The values of these

parameters are strings of terminals and non-terminals (the latter with the proper number of parameters).A parameter can be passed on, possibly concatenated with some terminals and non-terminals, or can be
made part of the sentential form. An algorithm to construct a recursive-descent parser for a macro gram-mar (if possible) is given.

\Gamma  G. Barth, "Fast recognition of context-sensitive structures", Computing, vol. 22,

p. 243-256, 1979. A recording grammar (an RG) is a CF grammar in which each (numbered)production rule belongs to one of three classes: normal, recording and directed. During production, normal rules behave normally and a recording rule records its own occurrence by appending its number to astring called the p-element. When production leaves a "recording" stage, the entire p-element is added to
a set called the \Omega -component, which collects all contexts created so far. When production enters a"directed" stage, an element (a context) is retrieved from the \Omega -component, transferred through a

Sec. 13.2] Unrestricted PS and CS grammars 271
mapping I and used to direct the choice of production rules until the element is exhausted. The expres-sive power of RGs is equal to that of Type 0 grammars.
An LL(k) version of RGs can be defined, based on LL(k)-ness of the underlying CF grammar, plus a fewsimple restrictions on the mapping

I; the resulting property is called RLL(k).For parsing, an LL(
k) parse is performed; during "normal" parsing, nothing special is done, during"recording" parsing the rule numbers are recorded and subsequently added to the \Omega -component; during

"directed" parsing, which is actually "checking" parsing, the rule numbers are checked for consistencywith the \Omega -component, using a simple finite transducer. The parser (+ checker) works in linear time.
It is not clear how convenient RLL(k) RGs are; neither of the two examples provided to demonstrate thepower of RGs is RLL(

k).

\Gamma  G.Sh. Vol'dman, "A parsing algorithm for context-sensitive grammars", Program. Comput. Softw., vol. 7, p. 302-307, 1981. Extends Earley's algorithm first tolength-increasing phrase structure grammars and then to non-decreasing PS grammars (= CS grammars).

The resulting algorithm has exponential time requirements but is often much better.

\Gamma  Lawrence A. Harris, "SLR(1) and LALR(1) parsing for unrestricted grammars",

Acta Inform., vol. 24, p. 191-209, 1987. The notion of an LR(0) item can easily be definedfor unrestricted grammars: "For each item lo""u

1

\Gamma  Xu2 there is a transition on X to the item lo""u1X \Gamma  u2

and an e-transition to any item Xdo""

\Gamma  h", for any symbol X. These items are grouped by subset construction into the usual states, called here preliminary states, since some of them may actually be ineffective.A GOTO function is also defined. If we can, for a given grammar, calculate the FOLLOW sets of all

left-hand sides (undecidable in the general case), we can apply a variant of the usual SLR(1) test andconstruct a parsing table for a parser as described by Turnbull and Lee [PSCS 1979].
To obtain the LALR(1) definition, a look-ahead grammar system is defined that will, for each item ineach state, generate the (unrestricted) language of all continuations after that item. If we can, for a given
grammar, calculate the FIRST sets of all these languages (undecidable in the general case), we can applya variant of the usual LALR(1) test and construct a parsing table for a similar parser. If one of the above
constructions succeeds, a linear-time parser is obtained.The author gives many hand-calculated examples and explores error detection properties. More general
definitions of SLR(1) and LALR(1) are possible, encompassing larger sets of grammars, at the cost of astill further reduced chance of decidability.

13.3 VAN WIJNGAARDEN GRAMMARS AND AFFIX GRAMMARS
Note that van Wijngaarden grammars and two-level grammars are synonyms; affixgrammars are different.

\Gamma  M. Sintzoff, "Existence of a van Wijngaarden syntax for every recursively enumerable set", Annales de la Socie'te' Scientifique de Bruxelles, vol. 81, no. II, p.115-118, 1967.

A relatively simple proof of the theorem that for every semi-Thue system wecan construct a VW grammar that produces the same set.

\Gamma  A. van Wijngaarden, B.J. Mailloux, J.E.L. Peck, C.H.A. Koster, M. Sintzoff,

C.H. Lindsey, L.G.L.T. Meertens, R.G. Fisker, "Revised report on the algorithmiclanguage ALGOL 68", Acta Inform., vol. 5, p. 1-236, 1975.

VW grammars found theirwidest application to date in the definition of ALGOL 68. Section 1.1.3 of the ALGOL 68 Revised

Report contains a very carefully worded description of the two-level mechanism. The report containsmany interesting applications.

\Gamma  C.H.A. Koster, "Affix grammars". In ALGOL 68 Implementation, J.E.L. Peck

(eds.), North-Holland Publ. Co., Amsterdam, p. 95-109, 1971. Context conditions areexpressed inside a context-free grammar by introducing

affixes, which are divided in derived and inherited and which have to fulfill user-defined primitive predicates. If the affix grammar is well-formed, aparser for it can be constructed.

272 Annotated bibliography [Ch. 13

\Gamma  David Crowe, "Generating parsers for affix grammars", Commun. ACM, vol. 15,

no. 8, p. 728-734, Aug 1972. A bounded-context (Floyd productions) parser is extended withaffix manipulation.

\Gamma  A. van Wijngaarden, "The generative power of two-level grammars". In Automata, Languages and Programming, J. Loeckx (eds.), Lecture Notes in ComputerScience #14, Springer-Verlag, Berlin, p. 9-16, 1974.

The generative power of VWgrammars is illustrated by creating a VW grammar that simulates a Turing machine; the VW grammar

uses only one metanotion, thus proving that one metanotion suffices.

\Gamma  Sheila A. Greibach, "Some restrictions on W-grammars", Intern. J. Comput.

Inform. Sci., vol. 3, no. 4, p. 289-327, 1974. The consequences of two easily checkablerestrictions on the form of the rules in a VW grammar are explored in great detail and are found to be

surprising. Although this highly technical paper is not directly concerned with parsing, it is very instruc-tive in that it shows methods of exploring the field.

\Gamma  C.H.A. Koster, "Two-level grammars". In Compiler Construction: An Advanced

Course, F.L. Bauer & J. Eickel (eds.), Lecture Notes in Computer Science #21,Springer-Verlag, Berlin, p. 146-156, 1974.

Easy introduction to two-level (VW)grammars, starting from one-level VW grammars. Examples of practical handling of context in a VW

grammar.

\Gamma  P. Deussen, "A decidability criterion for van Wijngaarden grammars", Acta

Inform., vol. 5, p. 353-375, 1975. The criterion, which is given in detail, can be paraphrasedvery roughly as follows: the language generated by a VW grammar is decidable if (but not only if) there

are no e-rules and either there are no free metanotions (occurring on the right-hand side only) or there areno dummy metanotions (occurring on the left-hand side only).

\Gamma  David A. Watt, "The parsing problem for affix grammars", Acta Inform., vol. 8,

p. 1-20, 1977. A technique is described to convert an affix grammar into a CF grammar called a
head grammar which contains a special kind of non-terminal, copy-symbols. For the head grammar theyare e-rules, but for the affix grammar they effect affix manipulations on the affix stack. Primitive predicates are also e-rules, but do checks on the affixes. Parsing is done by any CF parser, preferably LR(1).The affixes are not used to control the parsing but only to declare an input string erroneous: for the technique to work, the affix grammar must in effect be an attribute grammar.

\Gamma  J. Craig Cleaveland, Robert C. Uzgalis, Grammars for Programming Languages,

Elsevier, New York, p. 154, 1977. In spite of its title, the book is a highly readableexplanation of two-level grammars, also known as van Wijngaarden grammars or VW grammars. After

an introductory treatment of formal languages, the Chomsky hierarchy and parse trees, it is shown towhat extent CF languages can be used to define a programming language. These are shown to fail to
define a language completely and the inadequacy of CS grammars is demonstrated. VW grammars arethen explained and the remainder of the book consists of increasingly complex and impressive examples
of what a VW grammar can do. These examples include keeping a name list, doing type checking andhandling block structure in the definition of a programming language. Recommended reading.

\Gamma  R. Meersman, G. Rozenberg, "Two-level meta-controlled substitution grammars", Acta Inform., vol. 10, p. 323-339, 1978. The authors prove that the uniformsubstitution rule is essential for two-level grammars; without it, they would just generate the CF

languages. This highly technical paper examines a number of variants of the mechanisms involved.

\Gamma  Lutz Wegner, "Bracketed two-level grammars - a decidable and practical

approach to language definition". In Automata, languages and programming, Her-mann A. Maurer (eds.), Lecture Notes in Computer Science #71, Springer-Verlag,

Berlin, p. 668-682, 1979. The metanotions of a VW grammar are partitioned into two blocks,

Sec. 13.3] Van Wijngaarden grammars and affix grammars 273
"synthesized" and "derived"; they are separated in a hyperrule by special markers, "brackets", and aretreated more or less as attributes. Under reasonable conditions parsability can be obtained. The thus restricted VW grammars are very readable.

\Gamma  Lutz Michael Wegner, "On parsing two-level grammars", Acta Inform., vol. 14,

p. 175-193, 1980. The article starts by defining a number of properties a VW grammar mayexhibit; among these are "left(right) bound", "free of hidden empty notions", "uniquely assignable" and

"locally unambiguous". Most of these properties are undecidable, but sub-optimal tests can be devised.For each VW grammar

GVW, a CF skeleton grammar GSK is defined by considering all hypernotions inthe VW grammar as non-terminals of

GSK and adding the cross-references of the VW grammar as pro-duction rules to
GSK. GSK generates a superset of GVW. The cross-reference problem for VW grammars isunsolvable but again any sub-optimal algorithm (or manual intervention) will do. Parsing is now done by

parsing with GSK and then reconstructing and testing the metanotions. A long list of conditions necessaryfor the above to work are given; these conditions are in terms of the properties defined at the beginning.

\Gamma  Dick Grune, "How to produce all sentences from a two-level grammar", Inform.

Process. Lett., vol. 19, p. 181-185, Nov 1984. All terminal productions are derivedsystematically in breadth-first order. The author identifies pitfalls in this process and describes remedies.

A parser is used to identify the hyperrules involved in a given sentential form. This parser is a generalCF recursive descent parser to which a consistency check for the metanotions has been added; it is not
described in detail.

\Gamma  A.J. Fisher, "Practical LL(1)-based parsing of van Wijngaarden grammars", Acta

Inform., vol. 21, p. 559-584, 1985. Fisher's parser is based on the idea that the input stringwas generated using only a small, finite, part of the infinite

strict grammar that can be generated fromthe VW grammar. The parser tries to reconstruct this part of the strict grammar on the fly while parsing

the input. The actual parsing is done by a top-down interpretative LL(1) parser, called the terminal
parser. It is driven by a fragment of the strict grammar and any time the definition of a non-terminal isfound missing by the terminal parser, the latter asks another module, the

strict syntax generator, to try toconstruct it from the VW grammar. For this technique to work, the VW grammar has to satisfy three conditions: the defining CF grammar of each hyperrule is unambiguous, there are no free metanotions, andthe skeleton grammar (as defined by Wegner [VW 1980]) is LL(1). The parser system is organized as a
set of concurrent processes (written in occam), with both parsers, all hyperrule matchers and severalother modules as separate processes. The author claims that "this concurrent organization ... is strictly a
property of the algorithm, not of the implementation", but a sequential, albeit slower, implementationseems quite possible. The paper gives heuristics for the automatic generation of the cross-reference
needed for the skeleton grammar; gives a method to handle general hyperrules, hyperrules that fit allhypernotions, efficiently; and pays much attention to the use of angle brackets in VW grammars.

\Gamma  Jacques Cohen, Timothy J. Hickey, "Parsing and compiling using Prolog", ACM

Trans. Prog. Lang. Syst., vol. 9, no. 2, p. 125-164, April 1987. See same paper [CF1987].

13.4 GENERAL CONTEXT-FREE PARSERS

\Gamma  E.T. Irons, "A syntax-directed compiler for ALGOL 60", Commun. ACM, vol. 4,

no. 1, p. 51-55, Jan 1961. The first to describe a full parser. It is essentially a full backtrackingrecursive descent left-corner parser. The published program is corrected in a Letter to the Editor by B.H.

Mayoh, Commun. ACM, vol. 4, no. 6, p. 284, June 1961.

\Gamma  Itiroo Sakai, "Syntax in universal translation". In Proceedings 1961 International

Conference on Machine Translation of Languages and Applied LanguageAnalysis, Her Majesty's Stationery Office, London, p. 593-608, 1962.

Using aformalism that seems equivalent to a CF grammar in Chomsky Normal Form and a parser that is essentially a CYK parser, the author describes a translation mechanism in which the source language sentence

274 Annotated bibliography [Ch. 13
is transformed into a binary tree (by the CYK parser). Each production rule carries a mark telling if theorder of the two constituents should be reversed in the target language. The target language sentence is
then produced by following this new order and by replacing words. A simple Japanese-to-English exam-ple is provided.

\Gamma  E.T. Irons, "The structure and use of the syntax directed compiler", Annual

Review in Automatic Programming, vol. 3, p. 207-228, 1962. Extended version ofIrons [CF 1961].

\Gamma  E.T. Irons, "An error-correcting parse algorithm", Commun. ACM, vol. 6, no. 11,

p. 669-673, Nov 1963. Contrary to the title, the most interesting part of this paper is the parserit describes, which is essentially Earley's algorithm without look-ahead. The invention of this parser was

prompted by the author's dissatisfaction with the error detection properties of backtracking parsers. Thisone does not backtrack, it keeps all possible parsings in parallel instead. When the set of possible parsings becomes exhausted due to an error in the input, the last non-empty set is analysed for continuations,both terminal and non-terminal, including all successors and alternatives. Then input symbols are discarded until one is found that is a terminal in the continuation set or the beginning of a non-terminal inthat set. Symbols are then inserted to bridge any possible gap thus created, and parsing continues. Note
that this is essentially Ro"hrich's algorithm. The author points out applications for this parser as a patternmatcher.

\Gamma  Sheila A. Greibach, "Formal parsing systems", Commun. ACM, vol. 7, no. 8, p.

499-504, Aug 1964. "A formal parsing system G=(V,u,T,R) consists of two finite disjointvocabularies,

V and T, a many-to-many map, u, from V onto T, and a recursive set R of strings in T calledsyntactic sentence classes" (verbatim). This is intended to solve an additional problem in parsing, which

occurs often in natural languages: a symbol found in the input does not always uniquely identify a termi-nal symbol from the language (for instance,

will (verb) versus will (noun)). On this level, the language isgiven as the entire set
R, but in practice it is given through a "context-free phrase structure generator",i.e. a grammar. To allow parsing, this grammar is brought into what is now known as Greibach Normal

Form: each rule is of the form Zo""aY 1 . . . Ym. Now a directed production analyser is defined which con-sists of an unlimited set of pushdown stores and an input stream, the entries of which are sets of terminal
symbols, derived through u from the lexical symbols. For each consecutive input entry, the machinescans the stores for a top non-terminal

Z for which there is a rule Zo""aY 1 . . . Ym with a in the input set.A new store is filled with a copy of the old store and the top

Z is replaced by Y 1 . . . Ym; if the resultingstore is longer than the input, it is discarded. Stores will contain non-terminals only. For each store that is

empty when the input is exhausted, a parsing has been found. This is in effect non-deterministic top-down parsing with a one-symbol look-ahead. This is probably the first description of a parser that will
work for any CF grammar.A large part of the paper is dedicated to undoing the damage done by converting to Greibach Normal
Form.

\Gamma  T.V. Griffiths, S.R. Petrick, "On the relative efficiencies of context-free grammar

recognizers", Commun. ACM, vol. 8, no. 5, p. 289-300, May 1965. To achieve aunified view of the parsing techniques known at that time, the authors define a non-deterministic twostack machine whose only type of instruction is the replacement of two given strings on the tops of bothstacks by two other strings; the machine is started with the input on one stack and the start symbol on the
other and it "recognizes" the input if both stacks get empty simultaneously. For each parsing techniqueconsidered, a simple mapping from the grammar to the machine instructions is given; the techniques
covered are top-down (called top-down), left-corner (called bottom-up) and bottom-up (called direct-substitution). Next, look-ahead techniques are incorporated to attempt to make the machine deterministic.
The authors identify left-recursion as a trouble-spot. All grammars are required to be e-free. The pro-cedures for the three parsing methods are given in a Letter to the Editor,

Commun. ACM, vol. 8, no. 10,p. 594, Oct 1965.

\Gamma  Susumu Kuno, "The predictive analyzer and a path elimination technique", Commun. ACM, vol. 8, no. 7, p. 453-462, July 1965. The author extends his predictive

Sec. 13.4] General context-free parsers 275
analyser (in modern terminology: an exhaustive top-down parser for grammars in Greibach NormalForm) (see Kuno and Oettinger, reprinted by Grosz, Sparck Jones and Webber [NatLang 1986]) with a
table of well-formed substrings. Through ingenious bit manipulation the table is made to fit in a smallmemory. Time gains are considerable (as expected).

\Gamma  Susumu Kuno, "An augmented predicative analyzer for context-free languages -

its relative efficiency", Commun. ACM, vol. 9, no. 11, p. 810-823, Nov 1966.Converting a CF grammar to Greibach Normal Form often greatly distorts its structure. To keep track of

the structure, the right-hand side of each rule in the CF grammar is prefixed with a marker, a specialnon-terminal which produces e. A conversion algorithm is described that results in rules of the form
Ao""M +aBC . . . , where M + is a non-empty sequence of markers. The Kuno predictive analyser (seeKuno [CF 1965]) is extended with a second stack on which the marker parts of the rules are kept. When a
parsing is found, the marker stack allows easy reconstruction of the parsing according to the original CFgrammar. The parser is compared to two other parsers, using a large number of criteria.

\Gamma  D.H. Younger, "Recognition of context-free languages in time n 3", Inform. Control, vol. 10, no. 2, p. 189-208, Feb 1967. A Boolean recognition matrix R is constructed ina bottom-up fashion, in which

R [i,l,p ] indicates that the segment of the input string starting at position iwith length
l is a production of non-terminal p. This matrix can be filled in O(n 3) actions, where n is thelength of the input string. If

R [0,n, 0] is set, the whole string is a production of non-terminal 0. Many ofthe bits in the matrix can never be used in any actual parsing; these can be removed by doing a top-down

scan starting from R [0,n, 0] and removing all bits not reached this way. If the matrix contains integerrather than Boolean elements, it is easy to fill it with the number of ways a given segment can be produced by a given non-terminal; this yields the ambiguity rate.

\Gamma  S.H. Unger, "A global parser for context-free phrase structure grammars", Commun. ACM, vol. 11, no. 4, p. 240-247, April 1968. The Unger parser (as described inSection 4.1) is extended with a series of tests to avoid partitionings that could never lead to success. For

instance, a section of the input is never matched against a non-terminal if it begins with a token no pro-duction of the non-terminal could begin with. Several such tests are described and ways are given to
statically derive the necessary information (FIRST sets, LAST sets, EXCLUDE sets) from the grammar.Although none of this changes the exponential character of the algorithm, the tests do result in a considerable speed-up in practice. (There is an essential correction to one of the flowcharts given in Commun.
ACM, vol. 11, no. 6, p. 427, June 1968.)

\Gamma  B.A. Chartres, J.J. Florentin, "A universal syntax-directed top-down analyzer", J.

ACM, vol. 15, no. 3, p. 447-464, July 1968. The non-deterministic two-stack top-downparser of Griffiths and Petrick [CF 1965] is extended with a third stack and a status variable. One stack

holds the rest of the input, the second holds the prediction that should match that input and the third holdsa tracing of the outline of the production tree constructed so far; when input and prediction stack are
empty, the third stack holds the completed parse tree. This three-stack mechanism can be run both for-ward and backward; the status variable keeps track of the direction. By properly reacting to the values on
the tops of the stacks and the direction variable, it is possible to make the mechanism perform a fullbacktracking exhaustive search. Much work is spent on handling left recursion and e-rules.

\Gamma  Ba'lint Do"mo"lki, "A universal compiler system based on production rules", BIT,

vol. 8, no. 4, p. 262-275, Oct 1968. The heart of the compiler system described here is aproduction system consisting of an ordered set of production rules, which are the inverses of the grammar rules; note that the notions "left-hand side" (lhs) and "right-hand side" (rhs) are reversed from theirnormal meanings in this abstract. The system attempts to derive the start symbol, by always applying the
first applicable production rule (first in two respects: from the left in the string processed, and in theordered set of production rules). This resolves shift/reduce conflicts in favour of reduce, and
reduce/reduce conflicts by length and by the order of the production rules. When a reduction is found, thelhs of the reducing rule is offered for semantic processing and the rhs is pushed back into the input
stream, to be reread. Since the length of the rhs is not restricted, the method can handle non-CF gram-mars.

276 Annotated bibliography [Ch. 13
The so-called "Syntactic Filter" uses a bitvector technique to determine if, and if so which, productionrule is applicable: for every symbol

i in the alphabet, there is a bitvector B [i ], with one bit for each of thepositions in each lhs; this bit set to 1 if this position contains symbol

i. There is also a bitvector U mark-ing the first symbol of each lhs, and a bitvector
V marking the last symbol of each lhs. Now, a stack ofbitvectors
Qt is maintained, with Q 0 = 0 and Qt = ((Qt -1>>1) U' U) U` B[it], where it is the t-th inputsymbol.
Qt contains the answer to the question whether the last j symbols received are the first j symbolsof some lhs, for any lhs and

j. A 1 "walks" through an lhs part of the Q vector, as this lhs is recognized.An occurrence of a lhs is found if

Q t U` V z' 0. After doing a replacement, t is set back k places, where kis the length of the applied lhs, so a stack of

Qt-s must be maintained. If some Qt = 0, we have an error.An interesting implementation of the Do"mo"lki algorithm is given by Hext and Roberts [CF 1970].

\Gamma  T. Kasami, K. Torii, "A syntax-analysis procedure for unambiguous context-free

grammars", J. ACM, vol. 16, no. 3, p. 423-431, July 1969. A rather complicatedpresentation of a variant of the CYK algorithm, including the derivation of a

O(n 2log n) time bound forunambiguous Chomsky Normal Form grammars.

\Gamma  J. Earley, "An efficient context-free parsing algorithm", Commun. ACM, vol. 13,

no. 2, p. 94-102, Feb 1970. This famous paper gives an informal description of the Earleyalgorithm. The algorithm is compared both theoretically and experimentally with some general search

techniques and with the CYK algorithm. It easily beats the general search techniques. Although theCYK algorithm has the same worst-case efficiency as Earley's, it requires

O(n 3) on any grammar,whereas Earley's requires
O(n 2) on unambiguous grammars and O(n) on bounded-state grammars. Thealgorithm is easily modified to handle Extended CF grammars. (Also reprinted by Grosz, Sparck Jones

and Webber [NatLang 1986])

\Gamma  J.B. Hext, P.S. Roberts, "Syntax analysis by Domo"lki's algorithm", Computer J.,

vol. 13, no. 3, p. 263-271, Aug 1970. Do"mo"lki's algorithm is a bottom-up parser in whichthe item sets are represented as bitvectors. A backtracking version is presented which can handle any

grammar. To reduce the need for backtracking a 1-character look-ahead is introduced and an algorithmfor determining the actions on the look-ahead is given. Since the internal state is recalculated by vector
operations for each input character, the parse table is much smaller than usual and its entries are one biteach. This, and the fact that it is all bitvector operations, makes the algorithm suitable for implementation in hardware.

\Gamma  Bernard Lang, "Parallel non-deterministic bottom-up parsing", ACM SIGPLAN

Notices, vol. 6, no. 12, p. 56-57, Dec 1971. The full breadth-first search of an Earleyparser is limited through the use of weak-precedence relations, in so far as these are unique. Abstract of a

larger technical report.

\Gamma  F.P. Kaminger, "Generation, recognition and parsing of context-free languages

by means of recursive graphs", Computing, vol. 11, no. 1, p. 87-96, 1973. Formaldescription of the use of recursive graphs instead of CF grammars to describe, generate and parse

context-free languages.

\Gamma  Bernard Lang, "Deterministic techniques for efficient non-deterministic parsers".

In Automata, languages and programming, J. Loeckx (eds.), Lecture Notes inComputer Science #14, Springer-Verlag, Berlin, p. 255-269, 1974.

Explores thetheoretical properties of doing breadth-first search to resolve the non-determinism in a bottom-up automaton with conflicts. See Tomita [CF 1986] for a practical realization.

\Gamma  M. Bouckaert, A. Pirotte, M. Snelling, "Efficient parsing algorithms for general

context-free parsers", Inform. Sci., vol. 8, no. 1, p. 1-26, Jan 1975. The authorsobserve that the Predictor in an Earley parser will often predict items that start with symbols that can

never match the first few symbols of the present input; such items will never bear fruit and could as wellbe left out. To accomplish this, they extend the

k-symbol reduction look-ahead Earley parser with a t-symbol prediction mechanism; this results in very general

Mkt parsing machines, the properties of which

Sec. 13.4] General context-free parsers 277
are studied, in much formal detail. Three important conclusions can be drawn. Values of k or t largerthan one lose much more on processing than they will normally gain on better prediction and sharper
reduction; such parsers are better only for asymptotically long input strings. The Earley parser withoutlook-ahead (

M00) performs better than the parser with 1 symbol look-ahead; Earley's recommendation touse always 1 symbol look-ahead is unsound. The best parser is

M01; i.e. use a one symbol predictivelook-ahead and no reduction look-ahead.

\Gamma  L. Valiant, "General context-free recognition in less than cubic time", J. Comput.

Syst. Sci., vol. 10, p. 308-315, 1975. Reduces CYK to bit matrix multiplication and thenapplies Strassen's# algorithm.

\Gamma  C.H.A. Koster, "A technique for parsing ambiguous grammars". In GI-4. Jahrestagung, D. Siefkes (eds.), Lecture Notes in Computer Science #26, Springer-Verlag, New York, p. 233-246, 1975.

Three recursive-descent parsing techniques aredescribed: no backtrack, partial backtrack and full backtrack.

\Gamma  B. Sheil, "Observations on context-free parsing", Statistical Methods in Linguistics, p. 71-109, 1976. The author proves that any CF backtracking parser will have polynomialtime requirements if provided with a

well-formed substring table (WFST), which holds the well-formedsubstrings recognized so far and which is consulted before each attempt to recognize a substring. The

time requirements of the parser is O(n c+1) where c is the maximum number of non-terminals in anyright-hand side. A

2-form grammar is a CF grammar such that no production rule in the grammar hasmore than two non-terminals on the right-hand side; nearly all practical grammars are already 2-form. 2-

form grammars, of which Chomsky Normal Form grammars are a subset, can be parsed in O(n 3). Analgorithm for a dividing top-down parser with a WFST is supplied. Required reading for anybody who
wants to write or use a general CF grammar. Many practical hints and opinions (controversial and other-wise) are given.

\Gamma  Susan L. Graham, Michael A. Harrison, "Parsing of general context-free

languages". In Advances in Computing, Vol. 14, Academic Press, New York, p.77-185, 1976.

The 109 page article describes three algorithms in a more or less unified manner:CYK, Earley's and Valiant's. The main body of the paper is concerned with bounds for time and space

requirements. Sharper bounds than usual are derived for special grammars, for instance, for linear gram-mars.

\Gamma  Jaroslav Kra'l, "A top-down no backtracking parsing of general context-free

languages". In Mathematical Foundations of Computer Science, J. Gruska (eds.),Lecture Notes in Computer Science #53, Springer-Verlag, Berlin, p. 333-341,

1977. The states of a top-down breadth-first general CF parser are combined whenever possible,resulting in an Earley-like parser without the bottom-up component.
\Gamma  G.K. Manacher, "An improved version of the Cocke-Younger-Kasami algorithm", Comput. Lang. (Elmsford, NY), vol. 3, p. 127-133, 1978. This paper discussessome modifications to the CYK algorithm that make it more efficient. First, the "length induction" iteration of CYK is replaced by an iteration that combines sets of non-terminals that derive strings of length

j-1 with sets of non-terminals that derive strings of length k<=j-1. Then, the recognition table of CYK isreplaced by three tables of lists, where each table has a list for each non-terminal/number pair. The first

table maps a non-terminal/length pair to a list of positions, indicating where substrings of this length startthat are derived by this non-terminal. The second table maps a non-terminal/position pair to a list of
lengths, indicating the lengths of the substrings starting at this position that are derived by this non\Gamma  \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma \Delta \Gamma #

Volker Strassen, "Gaussian elimination is not optimal", Numerische Mathematik, vol. 13, p.354-356, 1969. Shows how to multiply two 2*2 matrices using 7 multiplications rather than 8

and extends the principle to larger matrices.

278 Annotated bibliography [Ch. 13
terminal. The third table maps a non-terminal/position pair to a list of lengths, indicating the lengths ofthe substrings ending at this position that are derived by this non-terminal. With these modifications a
time bound O(s(n)) is established for unambiguous grammars, where s(n) is the number of triplets(

A,i, j) for which the non-terminal A derives the substring starting at position i, with length j. This is atworst

O(n 2).

\Gamma  W.L. Ruzzo, "On the complexity of general context-free language parsing and

recognition". In Automata, Languages and Programming, Hermann A. Maurer(eds.), Lecture Notes in Computer Science #71, Springer-Verlag, Berlin, p. 489-

497, 1979. This is an extended abstract, summarizing some time requirement results: it is shownthat parsing strings of length

n is only O(log n) harder than just recognizing them. Also, the time to multiply O"

\Gamma  \Gamma n *O"\Gamma  \Gamma n Boolean matrices is a lower bound on the time needed to recognize all prefixes of a string,

and this, in turn, is a lower bound on the time needed to generate a convenient representation of all parsesof a string (basically the CYK recognition table, but reduced so that a non-terminal only is present in the
recognition table if it can be used to derive the sentence).

\Gamma  S.L. Graham, M.A. Harrison, W.L. Ruzzo, "An improved context-free recognizer", ACM Trans. Prog. Lang. Syst., vol. 2, no. 3, p. 415-462, July 1980. Thewell-formed substring table of the CYK parser is filled with dotted items as in an LR parser rather than

with the usual non-terminals. This allows the number of objects in each table entry to be reduced consid-erably. Special operators are defined to handle e- and unit rules.
The authors do not employ any look-ahead in their parser; they claim that constructing the recognitiontriangle is pretty fast already and that probably more time will be spent in enumerating and analysing the
resulting parse trees. They speed up the latter process by removing all useless entries before starting togenerate parse trees. To this end, a top-down sweep through the triangle is performed, similar to the
scheme to find all parse trees, which just marks all reachable entries without following up any of themtwice. The non-marked entries are then removed (p. 443).
Much attention is paid to efficient implementation, using ingenious data structures.

\Gamma  A. Bossi, N. Cocco, L. Colussi, "A divide-and-conquer approach to general

context-free parsing", Inform. Process. Lett., vol. 16, no. 4, p. 203-208, May 1983.The proposed parsing method yields for a string

T two sets: a set of partial parse trees that may be incom-plete at their left edge (which then coincides with the left end of

T), called L, and a similar right-edge setcalled
R. To parse a string, it is cut in two pieces, each is parsed and the R set of the left-hand piece iscombined with the

L set of the right-hand piece.

\Gamma  Masaru Tomita, Efficient parsing for natural language, Kluwer Academic Publishers, Boston, p. 201, 1986. Tomita describes an efficient parsing algorithm to be used in a"natural-language setting": input strings of some tens of words and considerable but not pathological

ambiguity. The algorithm is essentially LR, starting parallel parses when an ambiguity is found in theLR-table. Full examples are given of handling ambiguities, lexical elements with multiple meanings and
unknown lexical elements.The algorithm is compared extensively to Earley's algorithm by measurement and it is found to be consistently five to ten times faster than the latter, in the domain for which it is intended. Earley's algorithmis better in pathological cases; Tomita's fails on unbounded ambiguity. No time bounds are given explicitly, but graphs show a behaviour better than O(n 3). Bouckaert's algorithm (Bouckaert, Pirotte andSnelling [CF 1975]) is shown to be between Earley's and Tomita's in speed.
MacLisp programs of the various algorithms are given and the application in the Nishida and DoshitaMachine Translation System is described.

\Gamma  Eiichi Tanaka, Mitsuru Ikeda, Kimio Ezure, "Direct parsing", Patt. Recog., vol.

19, no. 4, p. 315-323, 1986. Variants of Unger's and Earley's parser are compared in achromosome recognition situation. The possibility of stopping the Unger parser after the first parsing has

been found is exploited.

\Gamma  Jacques Cohen, Timothy J. Hickey, "Parsing and compiling using Prolog", ACM

Sec. 13.4] General context-free parsers 279

Trans. Prog. Lang. Syst., vol. 9, no. 2, p. 125-164, April 1987. Several methods aregiven to convert grammar rules into Prolog clauses. In the bottom-up method, a rule

Eo""E+Tcorresponds to a clause
reduce ([n (t),t (+),n (e) | X ],[n (e) | X ]) where the parameters represent the stackbefore and after the reduction. In the top-down method, a rule

Tc'o""*FTc' corresponds to a clause
rule (n (tprime),[t (*),n (f ),n (tprime)]). A recursive descent parser is obtained by representing a rule
So""aSb by the clause s (ASB) :- append (A,SB,ASB),append (S,B,SB),a (A),s (S),b (B). which attemptsto cut the input list

ASB into three pieces A, S and B, which can each be recognized as an a, an s and a b,respectively. A fourth type of parser results if ranges in the input list are used as parameters:

s (X 1,X 4):-
link (X 1,a,X 2),s (X 2,X 3),link (X 3,b,X 4) in which link (P,x,Q) describes that the input contains thetoken

x between positions P and Q. For each of these methods, ways are given to limit non-determinismand backtracking, resulting among others in LL(1) parsers.

By supplying additional parameters to clauses, context conditions can be constructed and carried around,much as in a VW grammar (although this term is not used). It should be noted that the resulting Prolog
programs are actually not parsers at all: they are just logic systems that connect input strings to parsings.Consequently they can be driven both ways: supply a string and it will produce the parsing; supply a
parsing and it will produce the string; supply nothing and it will produce all strings with their parsings inthe language.
As a separate topic, it is shown that Prolog is an effective language to do grammar manipulation in: cal-culation of FIRST and FOLLOW sets, etc. As an equally unrelated topic, examples of code generation
in Prolog are shown.

\Gamma  Masaru Tomita, "An efficient augmented-context-free parsing algorithm", Am. J.

Comput. Linguist., vol. 13, no. 1-2, p. 31-46, Jan-June 1987. Tomita's parser [CF1986] is extended with Boolean functions for the non-terminals that decide if a proposed reduce is applicable given the context. A method for deriving these functions in Lisp from more abstract specificationsis given.

13.5 LL PARSING

\Gamma  R. Kurki-Suonio, "On top-to-bottom recognition and left recursion", Commun.

ACM, vol. 9, no. 7, p. 527-528, July 1966. Gives a good account of Greibach's algorithmfor the removal of left-recursion from a grammar. The resulting distortion of the parsing process is countered by leaving (e-producing) markers in the grammar at the original ends of the right-hand sides in aleft-recursive rule. This 2-page paper also gives an algorithm for removing e-rules. Again, these leave
markers behind, which can interfere with the markers from a possibly subsequent removal of left-recursion. Rules for solving this interference are given.

\Gamma  K. C^ ulik II, "Contribution to deterministic top-down analysis of context-free

languages", Kybernetica, vol. 5, no. 4, p. 422-431, 1968. This paper introduces LL(f)grammars where

f is a function mapping strings of terminals to an arbitrary range, always uniquely deter-mining a right-hand side.

f is called a distinctive function.

\Gamma  P.M. Lewis II, R.E. Stearns, "Syntax-directed transduction", J. ACM, vol. 15, no.

3, p. 465-488, 1968. Although this article is about transduction, it is often given as a referencefor LL(

k), because it is one of the first articles discussing the LL(k) property, and it has an appendix onthe recognition of LL(

k) languages.

\Gamma  D. Wood, "The theory of left factored languages, Part I", Computer J., vol. 12,

no. 4, p. 349-356, 1969. A description of a variant of LL(1) grammars and parsing.
\Gamma  R. Kurki-Suonio, "Notes on top-down languages", BIT, vol. 9, p. 225-238, 1969.

Gives several variants of the LL(k) condition. Also demonstrates the existence of an LL(k) languagewhich is not LL(

k-1).

\Gamma  D. Wood, "The theory of left factored languages, Part II", Computer J., vol. 13,

280 Annotated bibliography [Ch. 13

no. 1, p. 55-62, 1970. More results about LL(1) and LL(k) grammars, including a recursive-descent parser in pseudo-Algol 60.
\Gamma  D.J. Rosenkrantz, R.E. Stearns, "Properties of deterministic top-down grammars", Inform. Control, vol. 17, p. 226-256, 1970. Many formal properties of LL(k)grammars are derived and tests for LL(

k) and strong-LL(k) are given.

\Gamma  Donald E. Knuth, "Top-down syntax analysis", Acta Inform., vol. 1, p. 79-110,

1971. A Parsing Machine (PM) is defined, which is effectively a set of mutually recursiveBoolean functions which absorb input if they succeed and absorb nothing if they fail. Properties of the

languages accepted by PMs are examined. This leads to CF grammars, dependency graphs, the null stringproblem, back-up, LL(

k), follow-function, LL(1), s-languages and a comparison of top-down versusbottom-up parsing. The author is one of the few scientists who provide insight in their thinking process.

\Gamma  Paul W. Abrahams, "A syntax-directed parser for recalcitrant grammars", Intern.

J. Comput. Math., vol. A3, p. 105-115, 1972. LL(1) parsing with conflict resolvers, called
oracles.

\Gamma  M. Griffiths, "LL(1) grammars and analyzers". In Compiler Construction: an

advanced course, F.L. Bauer & J. Eickel (eds.), Lecture Notes in Computer Sci-ence #21, Springer-Verlag, New York, p. 57-84, 1974.

A discussion of the LL(1)property, including a decision algorithm and the production of an analyser in the form of executable text.

These lecture notes also discuss some grammar transformations, including elimination of left-recursion,factorization, and substitution. Semantic insertions (or hooks for semantic actions) are also given some
attention.

\Gamma  T. Komor, "A note on left-factored languages", Computer J., vol. 17, no. 3, p.

242-244, 1974. Points out an error in a paper by Wood on left-factored languages [LL 1970],and suggests an extension to Fosters SID [Transform 1968] involving e-rules.

\Gamma  S. Jarzabek, T. Krawczyk, "LL-regular grammars", Inform. Process. Lett., vol. 4,

no. 2, p. 31-37, 1975. Introduces LL-regular (LLR) grammars: for every rule Ao""a1 | . . . | an,a partition (

R 1, . . . ,Rn) of disjoint regular sets must be given such that the rest of the input sentence is amember of exactly one of these sets. A parser can then be constructed by creating finite-state automata

for these sets, and letting these finite state automata determine the next prediction.

\Gamma  A. Nijholt, "On the parsing of LL-regular grammars". In Mathematical Foundations of Computer Science, A. Mazurkiewicz (eds.), Lecture Notes in ComputerScience #45, Springer-Verlag, Berlin, p. 446-452, 1976.

Derives a parsing algorithm forLL-regular grammars with a regular pre-scan from right to left that leaves markers, and a subsequent

scan which consists of an LL(1)-like parser.

\Gamma  D. Wood, "A bibliography of top-down parsing", ACM SIGPLAN Notices, vol.

13, no. 2, p. 71-76, Feb 1978. Contains some 90 literature references up to 1978 ondeterministic top-down parsing and related issues.

\Gamma  J. Lewi, K. de Vlaminck, J. Huens, M. Huybrechts, "The ELL(1) parser generator

and the error-recovery mechanism", Acta Inform., vol. 10, p. 209-228, 1978. Seesame paper [ErrHandl 1978].

\Gamma  V.W. Setzer, "Non-recursive top-down syntax analysis", Softw. Pract. Exper.,

vol. 9, no. 1, p. 237-245, 1979. Compares recursive and non-recursive implementations oftable-driven top-down parsers. The introduction of actions is facilitated by implementing the driver and

the tables as a loop over a case statement (on the states) over case statements (on the input token).

\Gamma  Stephan Heilbrunner, "On the definition of ELR(k) and ELL(k) grammars", Acta

Sec. 13.5] LL parsing 281

Inform., vol. 11, p. 169-176, 1979. Comparison and analysis of various definitions ofextended LL(

k) and extended LR(k), based on the transformations involved.

\Gamma  D.R. Milton, L.W. Kirchhoff, B.R. Rowland, "An ALL(1) compiler generator",

ACM SIGPLAN Notices, vol. 14, no. 8, p. 152-157, Aug 1979. Presents an LL(1)parser generator and attribute evaluator which allows LL(1) conflicts to be solved by examining attribute

values; the generated parsers use the error correction algorithm of Fischer, Milton and Quiring [ErrHandl1980].

\Gamma  D.A. Poplawski, "On LL-regular grammars", J. Comput. Syst. Sci., vol. 18, p.

218-227, 1979. Presents proof that, given a regular partition, it is decidable whether a grammaris LL-regular with respect to this partition; it is undecidable whether or not such a regular partition exists.

The paper then discusses a two-pass parser; the first pass works from right to left, marking each terminalwith an indication of the partition that the rest of the sentence belongs to. The second pass then uses these
indications for its predictions.

\Gamma  V.N. Glushkova, "Lexical analysis of LL(k) languages", Program. Comput.

Softw., vol. 5, p. 166-172, 1979. Examines the reduction of LL(k) grammars to simple-LL(1)grammars by combining terminal symbols into new terminal symbols.

\Gamma  J. Cohen, R. Sitver, D. Auty, "Evaluating and improving recursive descent

parsers", IEEE Trans. Softw. Eng., vol. SE-5, no. 5, p. 472-480, Sept 1979. Derivesformulas which express the execution time of systematically generated recursive descent parsers, and

uses these formulas to estimate the gain of various optimizations, such as the elimination of some routinecalls and merging of common code.

\Gamma  S. Sippu, E. Soisalon-Soininen, "On constructing LL(k) parsers". In Automata,

Languages and Programming, H.A. Maurer (eds.), Lecture Notes in ComputerScience #71, Springer-Verlag, Berlin, p. 585-595, 1979.

Presents a method forconstructing canonical LL(
k) parsers that can be regarded as the dual to the LR(k) technique of items andviable prefixes. In the LL(

k) method we have LL(k) items and viable suffixes. Like in the LR case, theLL(
k) method also has LA(p)LL(k) and SLL(k) variants; the SLL(k) variant coincides with the strong-LL(
k) grammars. Note that, although the S of SLL stands for Simple, this is not the same Simple LL asthe simple LL discussed in chapter 8.

\Gamma  A. Nijholt, "LL-regular grammars", Intern. J. Comput. Math., vol. A8, p. 303-

318, 1980. This paper discusses strong-LL-regular grammars, which are a subset of the LL-regular grammars, exactly as the strong-LL(

k) grammars are a subset of the LL(k) grammars, and derivessome properties.

\Gamma  Seppo Sippu, Eljas Soisalon-Soininen, "On LL(k) parsing", Inform. Control, vol.

53, no. 3, p. 141-164, June 1982. Theoretical background to Sippu and Soisalon-Soininen[LL 1979].

\Gamma  K. John Gough, "A new method of generating LL(1) look-ahead sets", ACM SIGPLAN Notices, vol. 20, no. 6, p. 16-19, June 1985. Presents an efficient method forcomputing the FIRST and FOLLOW sets, using "begun-by", "precedes", and "ends" relations.

\Gamma  Thomas J. Sager, "A technique for creating small fast compiler front ends", ACM

SIGPLAN Notices, vol. 20, no. 10, p. 87-94, Oct 1985. Presents a predictive parser thathas its tables compacted through the use of a minimal perfect hash function, thus making them very

small. An example is given for the Pascal language.

\Gamma  Barry Dwyer, "Improving Gough's LL(1) look-ahead generator", ACM SIGPLAN

Notices, vol. 20, no. 11, p. 27-29, Nov 1985. Refer to Gough [LL 1985]. Improves onGough's algorithm by not computing those FIRST and FOLLOW sets that are not needed for the LL(1)

282 Annotated bibliography [Ch. 13
parser generation.

\Gamma  David R. Hanson, "Compact recursive-descent parsing of expressions", Softw.

Pract. Exper., vol. 15, no. 12, p. 1205-1212, Dec 1985. Discusses recursive descentparsing of expressions by using a precedence table for the operators instead of a parsing routine for each

precedence level. There is for instance only one routine for expressions involving binary operators; theprecedence of the expression to be parsed is a parameter.

\Gamma  Reinhold Heckmann, "An efficient ELL(1)-parser generator", Acta Inform., vol.

23, p. 127-148, 1986. The problem of parsing with an ELL(1) grammar is reduced to findingvarious FIRST and FOLLOW sets. Theorems about these sets are derived and very efficient algorithms

for their calculation are supplied.

\Gamma  Dick Grune, Ceriel J.H. Jacobs, "A programmer-friendly LL(1) parser generator", Softw. Pract. Exper., vol. 18, no. 1, p. 29-38, Jan 1988. Presents a practicalELL(1) parser generator, called

LLgen, that generates fast error correcting recursive descent parsers. Inaddition to the error correction,
LLgen features static as well as dynamic conflict resolvers and a separatecompilation facility. The grammar can be viewed as a program, allowing for a natural positioning of

semantic actions.

\Gamma  Keiichi Yoshida, Yoshiko Takeuchi, "Some properties of an algorithm for constructing LL(1) parsing tables using production indices", J. Inform. Process., vol.11, no. 4, p. 258-262, 1988.

Presents an LL(1) parse table algorithm that, rather than firstcomputing FIRST and FOLLOW sets, computes a so-called FIRST-table and FOLLOW-table, which are

indexed by a (non-terminal, symbol) pair, and deliver a grammar rule number.

\Gamma  H. Dobler, K. Pirklbauer, "Coco-2, - A new compiler compiler", ACM SIGPLAN

Notices, vol. 25, no. 5, p. 82-90, May 1990. The authors present an integrated systemconsisting of a lexical phase using a heavily reduced FS automaton, and a syntactic phase which uses a

table-driven LL(1) parser. Semantic actions are interspersed in the syntactic phase.

13.6 LR PARSING

\Gamma  D.E. Knuth, "On the translation of languages from left to right", Inform. Control,

vol. 8, p. 607-639, 1965. This is the original paper on LR(k). It defines the notion as anabstract property of a grammar and gives two tests for LR(

k). The first works by constructing for thegrammar a regular grammar which generates all possible already reduced parts (= stacks) plus their

look-aheads; if this grammar has the property that none of its words is a prefix to another of its words,the original grammar was LR(

k). The second consists of implicitly constructing all possible item sets (=states) and testing for conflicts. Since none of this is intended to yield a reasonable parsing algorithm,

notation and terminology differs from that in later papers on the subject. Several theorems concerningLR(

k) grammars are given and proved.

\Gamma  A.J. Korenjak, "A practical method for constructing LR(k) processors", Commun. ACM, vol. 12, no. 11, p. 613-623, Nov 1969. The huge LR(1) parsing table ispartitioned as follows. A non-terminal

Z is chosen judiciously from the grammar, and two grammars areconstructed,
G 0, in which Z is considered to be a terminal symbol, and G 1, which is the grammar for Z(i.e. which has

Z as the start symbol). If both grammars are LR(1) and moreover a master LR(1) parsercan be constructed that controls the switching back and forth between

G 0 and G 1, the parser constructionsucceeds (and the original grammar was LR(1) too). The three resulting tables together are much smaller

than the LR(1) table for the original grammar. It is also possible to chose a set of non-terminals
Z 1 . . . Zn and apply a variant of the above technique.

\Gamma  David Pager, "A solution to an open problem by Knuth", Inform. Control, vol. 17,

p. 462-473, 1970. Highly mathematical paper concerning the properties of certain partitions of

Sec. 13.6] LR parsing 283
the states of an LR(1) parser with a view to reducing the size of the LR automaton.

\Gamma  H. Langmaack, "Application of regular canonical systems to grammars translatable from left to right", Acta Inform., vol. 1, p. 111-114, 1971. Different proof of thedecidability of LR(

k).

\Gamma  Franklin L. DeRemer, "Simple LR(k) grammars", Commun. ACM, vol. 14, no. 7,

p. 453-460, July 1971. SLR(k) explained by its inventor. Several suggestions are made on howto modify the method; use a possibly different

k for each state; use possibly different lengths for eachlook-ahead string. The relation to Korenjak's approach [LR 1969] is also discussed.

\Gamma  A.V. Aho, J.D. Ullman, "Optimization of LR(k) parsers", J. Comput. Syst. Sci.,

vol. 6, p. 573-602, 1972. An algorithm is given to determine which entries in an LR(k) tablecan never be accessed; the values of these entries are immaterial (so-called

don't-care entries) and can bemerged with other values. A second algorithm is given to determine which error entries could be merged

with which reduce entry, with the only result that error detection is postponed. Both algorithms and amerging technique are used to reduce table size. It is proved that using these algorithms, one can produce
SLR(1) and LALR(1) tables. It is also proved that SLR(1) is identical to Korenjak's method [LR 1969]with all non-terminals selected. See also Soisalon-Soininen [LR 1982].

\Gamma  David S. Wise, "Generalized overlap resolvable grammars and their parsers", J.

Comput. Syst. Sci., vol. 6, p. 538-572, Dec 1972. See same paper [Precedence 1972].
\Gamma  T. Anderson, J. Eve, J.J. Horning, "Efficient LR(1) parsers", Acta Inform., vol. 2,

p. 12-39, 1973. Coherent explanation of SLR(1), LALR(1), elimination of unit rules and tablecompression, with good advice.

\Gamma  Karel C^ ulik II, Rina Cohen, "LR-regular grammars - an extension of LR(k)

grammars", J. Comput. Syst. Sci., vol. 7, p. 66-96, 1973. The input is scanned fromright to left by a FS automaton which records its state at each position. Next this sequence of states is

parsed from left to right using an LR(0) parser. If such a FS automaton and LR(0) parser exist, the gram-mar is LR-regular. The authors conjecture, however, that it is unsolvable to construct this automaton and
parser. Examples are given of cases in which the problem can be solved.

\Gamma  A.V. Aho, J.D. Ullman, "A technique for speeding up LR(k) parsers", SIAM J.

Computing, vol. 2, no. 2, p. 106-127, June 1973. Describes two detailed techniques toeliminate unit rules, one by recognizing particular stack configurations and one by merging shifts on

non-terminals (GOTO's).

\Gamma  Shoji Sekimoto, Kuniaki Mukai, Masaru Sudo, "A method of minimizing LR(k)

parsers", Systems, Computers and Control, vol. 4, no. 5, p. 73-80, 1973. The statesof an LR(1) parser are grouped into classes by one of several equivalence relations. The parser records

only in which class it is, not in which state. When a reduction is called for, additional computation isrequired to determine which reduction. The tables for the class transitions are much smaller than those
for the state transitions.

\Gamma  Jaroslav Kra'l, Jir^i' Demner, "A note on the number of states of the DeRemer's

recognizer", Inform. Process. Lett., vol. 2, p. 22-23, 1973. Gives a formula for thenumber of states of an SLR(1) parser for an LL(1) grammar.

\Gamma  A.V. Aho, S.C. Johnson, "LR parsing", Computing Surveys, vol. 6, no. 2, p. 99-

124, 1974. LR parsing explained in a readable fashion, by the experts. Required reading.
\Gamma  Matthew M. Geller, Susan L. Graham, Michael A. Harrison, "Production prefix

parsing". In Automata, languages and programming, J. Loeckx (eds.), LectureNotes in Computer Science #14, Springer-Verlag, Berlin, p. 232-241, 1974.

The

284 Annotated bibliography [Ch. 13
items in a non-deterministic LR(0|1) automaton are simplified in that rather than Ao""b

\Gamma  g only b (the production prefix) is recorded. If the corresponding deterministic automaton is free of conflicts and has nocompound states (that is, each state contains only one production prefix) the grammar is a

production
prefix grammar. Table size is proportional to grammar size. Production prefix(1) is between simple pre-cedence and SLR(1) in power.

\Gamma  David Pager, "On eliminating unit productions from LR(k) parsers". In Automata,

languages and programming, J. Loeckx (eds.), Lecture Notes in Computer Sci-ence #14, Springer-Verlag, Berlin, p. 242-254, 1974.

The unit rules (and only the unitrules) of the grammar are collected in a directed graph, which is a set of multi-rooted trees (no cycles

allowed). For each leaf, the states of all its predecessors are contracted.

\Gamma  J.J. Horning, "LR grammars and analyzers". In Compiler Construction, an

Advanced Course, F.L. Bauer & J. Eickel (eds.), Lecture Notes in Computer Sci-ence #21, Springer-Verlag, New York, p. 85-108, 1974.

These lecture notes present aconcise discussion of LR(
k) grammars and LR(0), SLR(1) (more restrictive adding of reduce entries byusing FOLLOW sets), LALR(1) (using shift entries to determine state after reduce), and LR(1) (adding

look-ahead to items) constructor algorithms. Also some attention is given to the representation of LRtables, including some compactification techniques.

\Gamma  Paul Purdom, "The size of LALR(1) parsers", BIT, vol. 14, p. 326-337, 1974.
Experimental size analysis for LALR(1) parsers. Although parser size can be exponential in the grammarsize, it is found in practice to be linear in the grammar size.

\Gamma  Hans H. Kron, Hans-Ju"rgen Hoffman, Gerhard Winkler, "On a SLR(k)-based

parser system which accepts non-LR(k) grammars". In GI-4. Jahrestagung, D.Siefkes (eds.), Lecture Notes in Computer Science #26, Springer-Verlag, New

York, p. 214-223, 1975. For each inadequate state in an LR(0) automaton, a resolution tree isconstructed of maximum depth

k. If this construction succeeds, the grammar is of type FSLR(k). If itfails, a parser is generated that performs breadth-first search to resolve the remaining inadequacies.

Detailed algorithms are given.

\Gamma  A.J. Demers, "Elimination of single productions and merging of non-terminal

symbols in LR(1) grammars", Comput. Lang. (Elmsford, NY), vol. 1, no. 2, p.105-119, April 1975.

The unit rules are used to define subsets of the non-terminals, themembers of which can be treated as equivalent, similar to Aho and Ullman [LR 1973]. Explicit proofs

are given.

\Gamma  Harry B. Hunt III, Thomas G. Szymanski, Jeffrey D. Ullman, "On the complexity of LR(k) testing", Commun. ACM, vol. 18, no. 12, p. 707-716, Dec 1975. Timebounds as a function of the grammar size are derived for testing many properties of grammars. A practical result is that both the LL(k) and the LR(k) properties can be tested in O(n k+2). These and otherbounds given in the paper are upper bounds, and actual testing is often much faster.

\Gamma  O.L. Madsen, B.B. Kristensen, "LR-parsing of extended context-free grammars",

Acta Inform., vol. 7, no. 1, p. 61-73, 1976. The right parts are allowed to contain choices{w
1| . . . |wn} and repetitions {w}*. In addition to the dotted items in the LR sets, there are also markeditems, which have a # rather than a

\Gamma  . The # means one of three things: here starts a repetition, one element of a repetition has just been recognized or one member of a choice has just been recognized. Uponreduction, these marked items will tell how to unstack the entire right-hand side.

\Gamma  R.C. Backhouse, "An alternative approach to the improvement of LR(k) parsers",

Acta Inform., vol. 6, no. 3, p. 277-296, 1976. Traditionally, the field of bottom-up parsingis described in terms of handle-finding automata. The author describes it in terms of left-contexts, in

which a left-context is a set of stack configurations of the LR(k) parser. Other bottom-up techniques areexplained as approximations to these sets.

Sec. 13.6] LR parsing 285

\Gamma  Thomas G. Szymanski, John H. Williams, "Non-canonical extensions of bottomup parsing techniques", SIAM J. Computing, vol. 5, no. 2, p. 231-250, June 1976.Theory of non-canonical versions of several bottom-up parsing techniques, with good informal introduction.

\Gamma  Marc L. Joliat, "A simple technique for partial elimination of unit productions

from LR(k) parsers", IEEE Trans. Comput., vol. C-25, no. 7, p. 763-764, July1976.

A very simple algorithm is given that alters some of the transitions in an LR parse table tobypass unit rules.

\Gamma  M.M. Geller, M.A. Harrison, "Characteristic parsing: a framework for producing

compact deterministic parsers", J. Comput. Syst. Sci., vol. 14, no. 3, p. 265-317,June 1977.

Given a deterministic LR(1) automaton, suppose we add some (arbitrary) items tosome states. This will have two effects: the discriminatory power of the automaton will weaken and its

minimum size will decrease (since now some states will coincide). For a large number of grammarsthere is a

characteristic item addition technique that will minimize automaton size while preserving justenough power. This requires a heavy mathematical apparatus.

\Gamma  Matthew M. Geller, Michael A. Harrison, "On LR(k) grammars and languages",

Theoret. Comput. Sci., vol. 4, p. 245-276, 1977. Theoretical groundwork for the"characteristic parsing technique" of Geller and Harrison [LR June 1977].

\Gamma  D. Pager, "The lane-tracing algorithm for constructing LR(k) parsers and ways of

enhancing its efficiency", Inform. Sci., vol. 12, p. 19-42, 1977. An item Ao""b

\Gamma  Xg in an

LR parser (called a "configuration" here) has in general two kinds of successors: a set of "immediatesuccessors"

Xo""

\Gamma  xn and the "transition successor" Ao""bX \Gamma  g. An item together with a sequence of its

successive successors is called a lane. Lanes are used 1) to collect enough look-ahead context to convertan LR(0) automaton to LALR(1); 2) to determine which LALR(1) states should be split to resolve

remaining LALR(1) conflicts. The required algorithms are of considerable complexity.

\Gamma  Wilf R. LaLonde, "Regular right part grammars and their parsers", Commun.

ACM, vol. 20, no. 10, p. 731-741, Oct 1977. The notion of regular right part grammarsand its advantages are described in detail. A parser is proposed that does LR(

k) parsing to find the rightend of the handle and then, using different parts of the same table, scans the stack backwards using a

look-ahead (to the left!) of m symbols to find the left end; this is called LR(m, k). The correspondingparse table construction algorithm is given by LaLonde [LR 1979].

\Gamma  David Pager, "A practical general method for constructing LR(k) parsers", Acta

Inform., vol. 7, no. 3, p. 249-268, 1977. When during the construction of an LR(1) parser astate has to be added, one can consider merging it with an already existing state, if no conflict can arise

from this. The problem is that it is not easy to tell whether conflicts may arise from a certain merge. Tothis end, the notions

weak compatibility and strong compatibility are defined. Algorithms for the efficientconstruction of conflict-free small full LR(1) parse tables are given.

\Gamma  D. Pager, "Eliminating unit productions from LR(k) parsers", Acta Inform., vol.

9, p. 31-59, 1977. Very detailed description of a unit rule elimination algorithm.
\Gamma  A. Celentano, "Incremental LR parsers", Acta Inform., vol. 10, p. 307-321, 1978.

Very clear exposition of how the Ghezzi and Mandrioli algorithm [LR 1979] can be made to work onparse sequences rather than on parse trees, thus improving efficiency.

\Gamma  Stephen C. Johnson, YACC: yet another compiler-compiler, Bell Laboratories,

Murray Hill, New Jersey 07974, p. 34, 1978. In spite of its title, yacc is one of the mostwidely used parser generators. It generates LALR(1) parsers from a grammar with embedded semantic

actions and features a number of disambiguating and conflict-resolving mechanisms. The generatedparser is in C.

286 Annotated bibliography [Ch. 13

\Gamma  Akifumi Makinouchi, "On single production elimination in simple LR(k)

environment", J. Inform. Process., vol. 1, no. 2, p. 76-80, 1978. An SLR(1) parser isextended with the possibility of specifying grammar rules of the form ~{

Cl}A~{Cr}o"" . . . , which canonly be applied when the symbol before the
A cannot produce a member of {Cl} as its last token, and thetoken after
A is not in {Cr}. Such rules allow some convenient ambiguities to be resolved without loos-ing the generative power of the system.

\Gamma  W.R. LaLonde, "Constructing LR parsers for regular right part grammars", Acta

Inform., vol. 11, p. 177-193, 1979. Describes the algorithms for the regular right partparsing technique explained by LaLonde [LR 1977]. The back scan is performed using so-called

readback tables. Compression techniques for these tables are given.

\Gamma  Stephan Heilbrunner, "On the definition of ELR(k) and ELL(k) grammars", Acta

Inform., vol. 11, p. 169-176, 1979. See same paper [LL 1979].
\Gamma  Otto Mayer, "On deterministic canonical bottom-up parsing", Inform. Control,

vol. 43, p. 280-303, 1979. A general framework is presented for deterministic canonicalbottom-up parsers, from which well-known parsers arise as special cases.

\Gamma  Carlo Ghezzi, Dino Mandrioli, "Incremental parsing", ACM Trans. Prog. Lang.

Syst., vol. 1, no. 1, p. 58-70, July 1979. The authors observe that when a grammar allowsbottom-up parsing using some technique

T and is at the same time RL(k) for any k, then any modificationto the input text can only affect nodes that produce the modified part. By keeping the entire parse tree in

a both left-most and right-most threaded form, these nodes can be located and updated quickly. The caseLR(1) U` RL(1) is treated in full.

\Gamma  Kai Koskimies, Eljas Soisalon-Soininen, "On a method for optimizing LR

parsers", Intern. J. Comput. Math., vol. A7, p. 287-295, 1979. Defines criteria underwhich Pager's algorithm for the elimination of unit rules [LR 1977] can be safely applied to SLR(1)

parsers.

\Gamma  Kuo-Chung Tai, "Noncanonical SLR(1) grammars", ACM Trans. Prog. Lang.

Syst., vol. 1, no. 2, p. 295-320, Oct 1979. A survey of non-canonical parsing methods isgiven and two non-canonical variants of SLR(1) parsing are described.

\Gamma  Gerald A. Fisher Jr., Manfred Weber, "LALR(1) parsing for languages without

reserved words", ACM SIGPLAN Notices, vol. 14, no. 11, p. 26-30, Nov 1979. Aheuristic is given for designing an LALR(1) programming language without reserved words. First design

the LALR(1) language with reserved words, using a non-terminal iiddeennttiiffiieerr for the identifiers. Nowallow

iiddeennttiiffiieerr to also produce all reserved words and modify the grammar (or the language) untilthe grammar is LALR(1) again, using feedback from an LALR(1) parser generator.

\Gamma  Eljas Soisalon-Soininen, "On the space-optimizing effect of eliminating single

productions from LR parsers", Acta Inform., vol. 14, p. 157-174, 1980. Improvementof Pager's unit rule elimination algorithm [LR 1977].

\Gamma  Carlo Ghezzi, Dino Mandrioli, "Augmenting parsers to support incrementality",

J. ACM, vol. 27, no. 3, p. 564-579, 1980. The algorithm of Ghezzi and Mandrioli [LR1979] is extended to all LR(

k) grammars.

\Gamma  Jacek Witaszek, "The LR(k) parser". In Mathematical Foundations of Computer

Science, P. Dembin' ski (eds.), Lecture Notes in Computer Science #88, Springer-Verlag, New York, p. 686-697, 1980.

Three size-reducing transformations on LR(k) tablesare defined that leave the LR(
k) property undisturbed. One is similar to minimising a FS automaton, oneremoves unused look-ahead and one allows delaying error detection. No full algorithms given, but see

Witaszek [LR 1988].

Sec. 13.6] LR parsing 287

\Gamma  Bent Bruun Kristensen, Ole Lehrmann Madsen, "Methods for computing

LALR(k) lookahead", ACM Trans. Prog. Lang. Syst., vol. 3, no. 1, p. 60-82, Jan1981.

The LALR(k) look-ahead sets are seen as the solution to a set of equations, which aresolved by recursive traversal of the LR(0) automaton. Full algorithms plus proofs are given.

\Gamma  R. Kemp, "LR(0) grammars generated by LR(0) parsers", Acta Inform., vol. 15, p.

265-280, 1981. Theoretical analysis of the set of LR(0) grammars that produce a given LR(0)parser.

\Gamma  Theodore P. Baker, "Extending look-ahead for LR parsers", J. Comput. Syst. Sci.,

vol. 22, no. 2, p. 243-259, 1981. A FS automaton is derived from the LR automaton asfollows: upon a reduce to

A the automaton moves to all states that have an incoming arc marked A. Thisautomaton is used for analysing the look-ahead as in an LR-regular parser (

C^ ulik and Cohen [LR 1973]).

\Gamma  Stephan Heilbrunner, "A parsing automata approach to LR theory", Theoret.

Comput. Sci., vol. 15, p. 117-157, 1981. Parsing is explained in terms of item grammars,which describe the stack configurations of the parser. The theory is first developed for LR and then

applied uniformly to LL and LC.

\Gamma  Wilf R. LaLonde, "The construction of stack-controlling LR parsers for regular

right part grammars", ACM Trans. Prog. Lang. Syst., vol. 3, no. 2, p. 168-206,April 1981.

Traditional LR parsers shift each input token onto the stack; often, this shift could bereplaced by a state transition, indicating that the shift has taken place. Such a parser is called a

stackcontrolling LR parser, and will do finite-state recognition without stack manipulation whenever possible.Algorithms for the construction of stack-controlling LR parse tables are given. The paper is complicated

by the fact that the new feature is introduced not in a traditional LR parser, but in an LR parser for regu-lar right parts (for which see LaLonde [LR 1977]).

\Gamma  Charles Wetherell, A. Shannon, "LR - automatic parser generator and LR(1)

parser", IEEE Trans. Softw. Eng., vol. SE-7, no. 3, p. 274-278, May 1981. Thisshort paper discusses a full LR(1) parser generator and parser, written in ANSI 66 Fortran for portability,

and using an algorithm by Pager [LR 1977].

\Gamma  Augusto Celentano, "An LR parsing technique for extended context-free grammars", Comput. Lang. (Elmsford, NY), vol. 6, no. 2, p. 95-107, 1981. The results ofrepetitions or selections are popped off the parsing stack before the entire right-hand side has been recognized. Remarkably, this can be done for any extended LR(1) grammar. Explicit algorithms are given.

\Gamma  Paul W. Purdom, Cynthia A. Brown, "Parsing extended LR(k) grammars", Acta

Inform., vol. 15, p. 115-127, 1981. An LR state is stacked only at the beginning of a right-hand side; all other work is done on a global state. At a reduce, the reduced non-terminal is already on

the top of the stack and needs only to be unstacked. This does not work for all extended LR(k) gram-mars, but any extended LR(

k) can be converted into one for which the method works.

\Gamma  Takehiro Tokuda, "Eliminating unit reductions from LR(k) parsers using

minimum contexts", Acta Inform., vol. 15, p. 447-470, 1981. Very densely writtenanalysis of algorithms for the elimination of unit rules from a special class of LR(

k) parsers.

\Gamma  Colin Burgess, Laurence James, "An indexed bibliography for LR grammars and

parsers", ACM SIGPLAN Notices, vol. 16, no. 8, p. 14-26, Aug 1981. Useful,detailed and structured bibliography containing around 115 entries.

\Gamma  David Spector, "Full LR(1) parser generation", ACM SIGPLAN Notices, vol. 16,

no. 8, p. 58-66, Aug 1981. A heuristic algorithm for enlarging an LR(0) table to full LR(1) isgiven and demonstrated on two examples. With letter of correction (vol. 16, no. 11, Nov 1981, p. 2).

See also Ancona, Dodero and Gianuzzi [LR 1982] and Spector [LR 1988].

288 Annotated bibliography [Ch. 13

\Gamma  M. Ancona, V. Gianuzzi, "A new method for implementing LR(k) tables",

Inform. Process. Lett., vol. 13, no. 4/5, p. 171-176, 1981. For each inadequate statethere is a separate automaton handling that inadequacy by doing a look-ahead of one token. If this automaton has inadequate states the process is repeated. A tables construction algorithm is given.

\Gamma  Eljas Soisalon-Soininen, "Inessential error entries and their use in LR parser

optimization", ACM Trans. Prog. Lang. Syst., vol. 4, no. 2, p. 179-195, April1982.

More sophisticated and general algorithms are given for the techniques described by Ahoand Ullman [LR 1972].

\Gamma  M. Ancona, G. Dodero, V. Gianuzzi, "Building collections of LR(k) items with

partial expansion of lookahead strings", ACM SIGPLAN Notices, vol. 17, no. 5, p.24-28, May 1982.

In addition to the usual terminals, non-terminals are allowed in the look-ahead sets, leading to very substantial savings in the number of states. Only if an inadequate state turns

up the non-terminals are developed as far as needed to resolve the inadequacy. The algorithm will alsowork reasonably for

k >1.

\Gamma  J.C.H. Park, "A new LALR formalism", ACM SIGPLAN Notices, vol. 17, no. 7,

p. 47-61, July 1982. Simplified operators corresponding to Predict and Accept are definedprecisely and applied to LR and LALR parser generation. Difficult to read.

\Gamma  Frank DeRemer, Thomas J. Pennello, "Efficient computation of LALR(1) lookahead sets", ACM Trans. Prog. Lang. Syst., vol. 4, no. 4, p. 615-649, Oct 1982. 1.The LALR(1) look-ahead sets are calculated by four linear sweeps over the LR(0) automaton, calculating

the sets Direct Read, Read, Follow and Look-Ahead, respectively. 2. An obvious simplification leads to"Not Quite LALR(1)",

NQLALR(1), and is shown to be inadequate. 3. The debugging of non-LALR(1)grammars is treated.

\Gamma  Colin Burgess, Laurence James, "A revised index bibliography for LR grammars

and parsers", ACM SIGPLAN Notices, vol. 17, no. 12, p. 18-26, Dec 1982. Arevision of Burgess and James [LR 1981], extending the number of entries to about 160.

\Gamma  Jorma Tarhio, "LR parsing of some ambiguous grammars", Inform. Process.

Lett., vol. 14, no. 3, p. 101-103, 1982. The reduction items in all inadequate states arecollected. The rules in them are extended at the end with "synchronization symbols", to make the

shift/reduce and reduce/reduce conflicts go away. These synchronization symbols are context-dependent;for instance each identifier could be followed by a token indicating its type. The synchronization symbols
are inserted in the input stream by the lexical analyser while parsing.

\Gamma  Rakesh Agrawal, Keith D. Detro, "An efficient incremental LR parser for grammars with epsilon productions", Acta Inform., vol. 19, no. 4, p. 369-376, 1983. Alinear time and space implementation of Celentano's algorithm [LR 1978] is described, which can also

handle e-rules.

\Gamma  Takehiro Tokuda, "A fixed-length approach to the design and construction of

bypassed LR(k) parsers", J. Inform. Process., vol. 6, no. 1, p. 23-30, 1983. The ideaof removing unit reductions is extended to removing

all reductions that do not involve semantic actions;this leads to
bypassed LR(k) parsers. Full algorithms are given. Some of the literature on removing unitrules is analysed critically.

\Gamma  Dashing Yeh, "On incremental shift-reduce parsing", BIT, vol. 23, no. 1, p. 36-48,

1983. The input tokens to an LR parser are stored in a linked list; each node in this list also holdsa pointer to a stack pertinent for the token in the node. These stacks can be merged and are in fact also

stored in the nodes. This arrangement greatly simplifies incremental parsing. Very clear explanation.

\Gamma  Kenzo Inoue, Fukumi Fujiwara, "On LLC(k) parsing method of LR(k)

Sec. 13.6] LR parsing 289

grammars", J. Inform. Process., vol. 6, no. 4, p. 206-217, 1983. Assume an LR(k)grammar. Start parsing using the (full) LL(

k) method, until an LL(k) conflict is encountered, say on non-terminal
A. A is then parsed with the LR(k) method, using the proper predicted look-ahead set. If duringthe LR (sub)parsing the number of items narrows down to one, an LL(

k) (sub-sub)parsing is started; etc.Full algorithms for all tables are given. LLC means "Least Left Corner".

\Gamma  Lothar Schmitz, "On the correct elimination of chain productions from LR

parsers", Intern. J. Comput. Math., vol. 15, no. 2, p. 99-116, 1984. Rigorous proofs ofsome claims about unit-free LR(

k) parsers.

\Gamma  N.P. Chapman, "LALR(1,1) parser generation for regular right part grammars",

Acta Inform., vol. 21, p. 29-45, 1984. Efficient construction algorithm for LALR(1,1) parsetables, which find the right end of the handle by traditional LALR(1) parsing and then scan the stack

backwards using a look-ahead of 1 symbol to find the left end.

\Gamma  Joseph C.H. Park, K.M. Choe, C.H. Chang, "A new analysis of LALR formalisms", ACM Trans. Prog. Lang. Syst., vol. 7, no. 1, p. 159-175, Jan 1985. Therecursive closure operator

CLOSURE of Kristensen and Madsen [LR 1981] is abstracted to an iteratived-operator such that
CLOSURE z^d*. This operator allows the formal derivation of four algorithms for theconstruction of LALR look-ahead sets.

\Gamma  Esko Ukkonen, "Upper bounds on the size of LR(k) parsers", Inform. Process.

Lett., vol. 20, no. 2, p. 99-105, Feb 1985. Upper bounds for the number of states of anLR(
k) parser are given for several types of grammars.

\Gamma  S. Heilbrunner, "Truly prefix-correct chain-free LR(1) parsers", Acta Inform.,

vol. 22, no. 5, p. 499-536, 1985. A unit-free LR(1) parser generator algorithm, rigorouslyproven correct.

\Gamma  Fred Ives, "Unifying view of recent LALR(1) lookahead set algorithms", ACM

SIGPLAN Notices, vol. 21, no. 7, p. 131-135, July 1986. A common formalism is givenin which the LALR(1) look-ahead set construction algorithms of DeRemer and Pennello [LR 1982],

Park, Choe and Chang [LR 1985] and the author can be expressed. See also Park and Choe [LR 1987].

\Gamma  Manuel E. Bermudez, Karl M. Schimpf, "A practical arbitrary look-ahead LR

parsing technique", ACM SIGPLAN Notices, vol. 21, no. 7, p. 136-144, July 1986.To resolve LR(0) conflicts at run time, for each conflict state a FS automaton is developed that will do

arbitrary look-ahead. Grammars for which parsers can be constructed by this technique are called
LAM(m) where m in some way limits the size of the look-ahead FS automata. It can handle some non-LR(

k) grammars. See also Baker [LR 1981].

\Gamma  Thomas J. Pennello, "Very fast LR parsing", ACM SIGPLAN Notices, vol. 21, no.

7, p. 145-151, July 1986. The tables and driver of a traditional LALR(1) parser are replacedby assembler code performing linear search for small fan-out, binary search for medium and a calculated

jump for large fan-out. This modification gained a factor of 6 in speed at the expense of a factor 2 in size.

\Gamma  Ikuo Nakata, Masataka Sassa, "Generation of efficient LALR parsers for regular

right part grammars", Acta Inform., vol. 23, p. 149-162, 1986. The stack of anLALR(1) parser is augmented with a set of special markers that indicate the start of a right-hand side;

adding such a marker during the shift is called a stack-shift. Consequently there can now be ashift/stack-shift conflict, abbreviated to

stacking conflict. The stack-shift is given preference and anysuperfluous markers are eliminated during the reduction. Full algorithms are given.

\Gamma  A.M.M. Al-Hussaini, R.G. Stone, "Yet another storage technique for LR parsing

tables", Softw. Pract. Exper., vol. 16, no. 4, p. 389-401, 1986. Excellent introductionto LR table compression in general. The

submatrix technique introduced in this paper partitions the rows

290 Annotated bibliography [Ch. 13
into a number of submatrices, the rows of each of which are similar enough to allow drastic compressing.The access cost is

O(1). A heuristic partitioning algorithm is given.

\Gamma  Masataka Sassa, Ikuo Nakata, "A simple realization of LR-parsers for regular

right part grammars", Inform. Process. Lett., vol. 24, no. 2, p. 113-120, Jan 1987.For each item in each state on the parse stack of an LR parser, a counter is kept indicating how many

preceding symbols on the stack are covered by the recognized part in the item. Upon reduction, thecounter of the reducing item tells us how many symbols to unstack. The manipulation rules for the
counters are simple. The counters are stored in short arrays, one array for each state on the stack.

\Gamma  Joseph C.H. Park, Kwang-Moo Choe, "Remarks on recent algorithms for LALR

lookahead sets", ACM SIGPLAN Notices, vol. 22, no. 4, p. 30-32, April 1987.Careful analysis of the differences between the algorithms of Park, Choe and Chang [LR 1985] and Ives

[LR 1986]. See also Ives [LR 1987].

\Gamma  Fred Ives, "Response to remarks on recent algorithms for LALR lookahead sets",

ACM SIGPLAN Notices, vol. 22, no. 8, p. 99-104, 1987. Remarks by Park and Choe[LR 1987] are refuted and a new algorithm is presented that is significantly better than that of Park, Choe

and Chang [LR 1985] and that previously presented by Ives [LR 1986].

\Gamma  Nigel P. Chapman, LR Parsing: Theory and Practice, Cambridge University

Press, New York, NY, p. 228, 1987. Detailed treatment of the title subject. Highlyrecommended for anybody who wants to acquire in-depth knowledge about LR parsing. Good on size of

parse tables and attribute grammars.

\Gamma  Eljas Soisalon-Soininen, Jorma Tarhio, "Looping LR parsers", Inform. Process.

Lett., vol. 26, no. 5, p. 251-253, Jan 1988. For some (non-LR) grammars it is true thatthere are ways to resolve the conflicts in an LR parser for them that will make the parser loop on some

inputs (executing an endless sequence of reduces). A test is given to detect such grammars.

\Gamma  Jacek Witaszek, "A practical method for finding the optimum postponement

transformation for LR(k) parsers", Inform. Process. Lett., vol. 27, no. 2, p. 63-67,Feb 1988.

By allowing the LR(k) automaton to postpone error checking, the size of theautomaton can be reduced dramatically. Finding the optimum postponement transformation is, however,

a large combinatorial problem. A good heuristic algorithm for finding a (sub)optimal transformation isgiven.

\Gamma  Dashing Yeh, Uwe Kastens, "Automatic construction of incremental LR(1)

parsers", ACM SIGPLAN Notices, vol. 23, no. 3, p. 33-42, March 1988. Detailedalgorithms for an incremental LR(1) parser that allows multiple modifications and e-rules.

\Gamma  Manuel E. Bermudez, Karl M. Schimpf, "On the (non-)relationship between

SLR(1) and NQLALR(1) grammars", ACM Trans. Prog. Lang. Syst., vol. 10, no.2, p. 338-342, April 1988.

Shows a grammar that is SLR(1) but not NQLALR(1).
\Gamma  Pierpaolo Degano, Stefano Mannucci, Bruno Mojana, "Efficient incremental LR

parsing for syntax-directed editors", ACM Trans. Prog. Lang. Syst., vol. 10, no. 3,p. 345-373, July 1988.

The non-terminals of a grammar are partitioned by hand into sets of"incrementally compatible" non-terminals, meaning that replacement of one non-terminal by an incrementally compatible one is considered a minor structural change. Like in Korenjak's method [LR 1969],for a partitioning in

n sets n +1 parse tables are constructed, one for each set and one for the grammar thatrepresents the connection between the sets. The parser user is allowed interactively to move or copy the

string produced by a given non-terminal to a position where an incrementally compatible one is required.This approach keeps the text (i.e. the program text) reasonably correct most of the time and uses rather
small tables.

Sec. 13.6] LR parsing 291

\Gamma  George H. Roberts, "Recursive ascent: an LR analog to recursive descent", ACM

SIGPLAN Notices, vol. 23, no. 8, p. 23-29, Aug 1988. Each LR state is represented by asubroutine. The shift is implemented as a subroutine call, the reduction is followed by a subroutine

return possibly preceded by a return stack adjustment. The latter prevents the generation of genuine sub-routines since it requires explicit return stack manipulation. A small and more or less readable LR(0)
parser is shown, in which conflicts are resolved by means of the order in which certain tests are done,like in a recursive descent parser.

\Gamma  F.E.J. Kruseman Aretz, "On a recursive ascent parser", Inform. Process. Lett.,

vol. 29, no. 4, p. 201-206, Nov 1988. Each state in an LR automaton is implemented as asubroutine. A shift calls that subroutine. A reduce to

X is effected as follows. X and its length n are storedin global variables; all subroutines are rigged to decrement

n and return as long as n>0, and to call theproper GOTO state of
X when n hits 0. This avoids the explicit stack manipulation of Roberts [LR 1988].

\Gamma  David Spector, "Efficient full LR(1) parser generation", ACM SIGPLAN Notices,

vol. 23, no. 12, p. 143-150, Dec 1988. A relatively simple method is given for extending anLR(0) table to full LR(1). The method isolates the inadequate states, constructs the full look-ahead sets

for them and then splits them (and possible predecessor states). The algorithm is described informally.

\Gamma  Manuel E. Bermudez, George Logothetis, "Simple computation of LALR(1)

look-ahead sets", Inform. Process. Lett., vol. 31, no. 5, p. 233-238, 1989. Theoriginal LALR(1) grammar is replaced by a not much bigger grammar that has been made to incorporate

the necessary state splitting through a simple transformation. The SLR(1) automaton of this grammar isthe LALR(1) automaton of the original grammar.

\Gamma  George H. Roberts, "Another note on recursive ascent", Inform. Process. Lett.,

vol. 32, no. 5, p. 263-266, 1989. The fast parsing methods of Pennello [LR 1986],Kruseman Aretz [LR 1988] and Roberts are compared. A special-purpose optimizing compiler can select

the appropriate technique for each state.

\Gamma  James Kanze, "Handling ambiguous tokens in LR parsers", ACM SIGPLAN

Notices, vol. 24, no. 6, p. 49-54, June 1989. It may not always be possible to infer fromthe appearance of an input symbol the terminal symbol it corresponds to in the parser. In that case a

default assumption can be made and the error recovery mechanism of the parser can be rigged to tryalternatives. A disadvantage is that an LALR parser may already have made reductions (or a strong-LL
parser may have made e-moves) that have ruined the context. An implementation in UNIX's yacc isgiven.

\Gamma  Daniel J. Salomon, Gordon V. Cormack, "Scannerless NSLR(1) parsing of programming languages", ACM SIGPLAN Notices, vol. 24, no. 7, p. 170-178, July1989.

The traditional CF syntax is extended with two rule types: Ao""* B, which means that anysentential form in which

A generates a terminal production of B (with B regular) is illegal, and A-/ B,which means that any sentential form in which terminal productions of

A and B are adjacent, is illegal.The authors show that the addition of these two types of rules allow one to incorporate the lexical phase

of a compiler into the parser. The system uses a non-canonical SLR(1) parser.

\Gamma  J. Heering, P. Klint, J. Rekers, "Incremental generation of parsers", ACM SIGPLAN Notices, vol. 24, no. 7, p. 179-191, July 1989. In a very unconventional approachto parser generation, the initial information for an LR(0) parser consists of the grammar only. As parsing

progresses, more and more entries of the LR(0) table (actually a graph) become required and are con-structed on the fly. LR(0) inadequacies are resolved using Tomita's method. All this greatly facilitates
handling (dynamic) changes to the grammar.

\Gamma  R. Nigel Horspool, "ILALR: an incremental generator of LALR(1) parsers". In

Compiler Compilers and High-Speed Compilation, D. Hammer (eds.), LectureNotes in Computer Science #371, Springer-Verlag, Berlin, p. 128-136, 1989.

292 Annotated bibliography [Ch. 13
Grammar rules are checked as they are typed in. To this end, LALR(1) parse tables are kept and continu-ally updated. When the user interactively adds a new rule, the sets FIRST and NULLABLE are recalculated and algorithms are given to distribute the consequences of possible changes over the LR(0) andlook-ahead sets. Some serious problems are reported and practical solutions are given.

\Gamma  Daniel J. Salomon, Gordon V. Cormack, "Corrections to the paper: Scannerless

NSLR(1) parsing of programming languages", ACM SIGPLAN Notices, vol. 24,no. 11, p. 80-83, Nov 1989.

More accurate time measurements and corrections to thealgorithms are supplied. See same authors [LR July 1989].

\Gamma  Stylianos D. Pezaris, "Shift-reduce conflicts in LR parsers", ACM SIGPLAN

Notices, vol. 24, no. 11, p. 94-95, Nov 1989. It is shown that if an LR(1) parser either hasno shift/reduce conflicts or has shift/reduce conflicts that have been decided to be solved by shifting, the

same parsing behaviour can be obtained from the corresponding LR(0) parser (which will have noreduce/reduce conflicts) in which all shift/reduce conflicts are resolved in favour of the shift. With this
resolution principle, for instance the programming language C can be parsed with an LR(0) parser.

\Gamma  Gregor Snelting, "How to build LR parsers which accept incomplete input", ACM

SIGPLAN Notices, vol. 25, no. 4, p. 51-58, April 1990. When an LR parser finds apremature end-of-file, the incomplete parse tree is completed using some heuristics on the top state of the

stack. The heuristics mainly favour reduction over shift and their application is repeated until the parsetree is complete or further completion would involve too much guessing. The technique is explained in
the setting of a language-based editor.

\Gamma  George H. Roberts, "From recursive ascent to recursive descent: via compiler

optimizations", ACM SIGPLAN Notices, vol. 25, no. 4, p. 83-89, April 1990. Showsa number of code transformations that will turn an LR(1) recursive ascent parser (see Roberts [LR 1988]

and [LR 1989]) for an LL(1) grammar into a recursive descent parser.

13.7 LEFT-CORNER PARSING
This section also covers a number of related techniques: production-chain, LLP(k),PLR(k), etc.

\Gamma  D.J. Rosenkrantz, P.M. Lewis II, "Deterministic left-corner parsing". In IEEE

Conference Record 11th Annual Symposium on Switching and Automata Theory,p. 139-152, 1970.

An LC(k) parser decides the applicability of a rule when it has seen the initialnon-terminal of the rule if it has one, plus a look-ahead of

k symbols. Identifying the initial non-terminalis done by bottom-up parsing, the rest of the rule is recognized top-down. A canonical LC pushdown

machine can be constructed in which the essential entries on the pushdown stack are pairs of non-terminals, one telling what non-terminal has been recognized bottom-up and the other what non-terminal
is predicted top-down. As with LL, there is a difference between LC and strong-LC. There is a simplealgorithm to convert an LC(

k) grammar into LL(k) form; the resulting grammar may be large, though.

\Gamma  Y. Eric Cho, "Simple left-corner grammars". In Proc. Seventh Princeton Conference on Information Sciences and Systems, Princeton, p. 557, 1973. LC parsing issimplified by requiring that each right-hand side be recognizable (after LC reduction) by its first two

symbols and by handling left recursion as a special case. The required tables are extremely small.

\Gamma  David B. Lomet, "Automatic generation of multiple exit parsing subroutines". In

Automata, languages and programming, J. Loeckx (eds.), Lecture Notes in Com-puter Science #14, Springer-Verlag, Berlin, p. 214-231, 1974.

A production chain is achain of production steps
X 0o""X 1a1, X 1o""X 2a2, . . . Xn -1o""ttan, with X 0, . . . ,Xn -1 non-terminals and
tt a terminal. If the input is known to derive from X 0 and starts with tt, each production chain from X 0 to
tt is a possible explanation of how tt was produced. The set of all production chains connecting X 0 to tt

Sec. 13.7] Left-corner parsing 293
is called a production expression. An efficient algorithm for the construction and compression of pro-duction expressions is given. Each production expression is then implemented as a subroutine which contains the production expression as a FS automaton.

\Gamma  Michael Hammer, "A new grammatical transformation into LL(k) form". In

Proceedings Sixth Annual ACM Symposium on Theory of Computing, p. 266-275,1974.

Each left corner in a left-corner parser is described as a FS automaton and implemented asa subroutine. Parsing is then performed by recursive descent using these subroutines. The FS automata

can be incorporated into the grammar to yield an LL(k) grammar.

\Gamma  J. Kra'l, J. Demner, "Parsing as a subtask of compiling". In Mathematical Foundations of Computer Science, J. Bec^va'r^ (eds.), Lecture Notes in Computer Science#32, Springer-Verlag, Berlin, p. 61-74, 1975.

Various considerations that went into thedesign of a variant of left-corner parsing, called
semi-top-down.

\Gamma  E. Soisalon-Soininen, E. Ukkonen, "A characterization of LL(k) grammars". In

Automata, Languages and Programming, S. Michaelson & R. Milner (eds.), Edin-burgh University Press, Edinburgh, p. 20-30, 1976.

Introduces a subclass of the LR(k)grammars called predictive LR(
k) (PLR(k)). The deterministic LC(k) grammars are strictly included inthis class, and a grammatical transformation is presented to transform a PLR(

k) into an LL(k) grammar.PLR(
k) grammars can therefore be parsed with the LL(k) parser of the transformed grammar. A conse-quence is that the classes of LL(

k), LC(k), and PLR(k) languages are identical.

\Gamma  A. Nijholt, "Simple chain grammars". In Automata, Languages and Programming, A. Salomaa & M. Steinby (eds.), Lecture Notes in Computer Science #52,Springer-Verlag, Berlin, p. 352-364, 1977.

A non-terminal X is said to be chainindependent if all production chains (see Lomet [LC 1974]) of X end in a different terminal symbol.Two symbols

X and Y are "mutually chain-independent" if different chains, one starting with X and theother with
Y, end with different symbols. A CF grammar is a simple chain grammar if it satisfies the fol-lowing conditions: (1) all its symbols are chain-independent, (2) if

Ao""aXb and Ao""aYg, then X and Yare mutually chain-independent, and (3) if
Ao""a and Ao""ab then b=e.This class of grammars contains the LL(1) grammars without e-rules, and is a subset of the LR(0) grammars. A simple parser for these grammars is presented.

\Gamma  Jaroslav Kra'l, "Almost top-down analysis for generalized LR(k) grammars". In

Methods of algorithmic language implementation, A.P. Ershov and C.H.A. Koster(eds.), Lecture Notes in Computer Science #47, Springer-Verlag, Berlin, p. 149-

172, 1977. Very well-argued introduction to semi-top-down parsing; see Kra'l [LC 1975].
\Gamma  Jan Pittl, "Exponential optimization for the LLP(k) parsing method". In

Mathematical Foundations of Computer Science, J. Gruska (eds.), Lecture Notesin Computer Science #53, Springer-Verlag, Berlin, p. 435-442, 1977.

The automataby Lomet [LC 1974] are reduced using the "characteristic parsing" technique of Geller and Harrison [LR

1977].

\Gamma  Alan J. Demers, "Generalized left corner parsing". In Fourth ACM Symposium on

Principles of Programming Languages, p. 170-182, 1977. The right-hand side of eachrule is required to contain a marker. The part on the left of the marker is the left corner; it is recognized

by SLR(1) techniques, the rest by LL(1) techniques. An algorithm is given to determine the first admissi-ble position in each right-hand side for the marker.

\Gamma  Eljas Soisalon-Soininen, Esko Ukkonen, "A method for transforming grammars

into LL(k) form", Acta Inform., vol. 12, p. 339-369, 1979. A restricted class of LR(k)grammars is defined, the predictive LR(

k) or PLR(k) grammars, which can be handled by left-cornertechniques. Like LC(
k) grammars, they can be transformed into LL(k) grammars.

294 Annotated bibliography [Ch. 13

\Gamma  Esko Ukkonen, "A modification of the LR(k) method for constructing compact

bottom-up parsers". In Automata, Languages and Programming, Hermann A.Maurer (eds.), Lecture Notes in Computer Science #71, Springer-Verlag, Berlin,

p. 646-658, 1979. An LR(k) parser is extended to do left-corner parsing simultaneously bycompounding the states on the stack. This can be done for

weak-PLR(k) grammars only, which, how-ever, include almost all LR(
k) grammars. The resulting table is gigantic but highly structured, and can becondensed considerably.

\Gamma  Daniel Chester, "A parsing algorithm that extends phrases", Am. J. Comput.

Linguist., vol. 6, no. 2, p. 87-96, April-June 1980. See same paper [NatLang 1980].
\Gamma  Jan Pittl, "On LLP(k) grammars and languages", Theoret. Comput. Sci., vol. 16,

p. 149-175, 1981. See Pittl [LC 1982]. All LR(k) languages have an LLP(k) grammar. LLP(k)lies somewhere between LL(

k) and LR(k).

\Gamma  Jan Pittl, "On LLP(k) parsers", J. Comput. Syst. Sci., vol. 24, p. 36-68, 1982. This

paper first presents a non-deterministic parser using a mixed top-down-bottom-up strategy, and thenexamines the circumstances under which these parsers are deterministic, resulting in the class of LLP(

k)grammars. The parser does not have the correct-prefix property, as the LL(
k) and LR(k) parsers have.

\Gamma  Yuji Matsumoto, Hozumi Tanaka, Hideki Hirakawa, Hideo Miyoshi, Hideki

Yasukawa, "BUP: a bottom-up parser embedded in Prolog", New GenerationComputing, vol. 1, p. 145-158, 1983.

A bottom-up parser for natural language textembedded in Prolog is described, in which each grammar rule corresponds to a Prolog clause. The

parser, which is fact left-corner, can deal with any cycle-free grammar with no e-rules. The dictionary ishandled separately. Explicit rules are given how to convert a grammar into Prolog clauses. A facility for
remembering previous successes and failures is included. A tracing facility is also described.

\Gamma  Kenzo Inoue, Fukumi Fujiwara, "On LLC(k) parsing method of LR(k) grammars", J. Inform. Process., vol. 6, no. 4, p. 206-217, 1983. See same paper [LR 1983].
\Gamma  Susan Hirsh, "P-PATR: a compiler for unification-based grammars". In Natural

Language Understanding and Logic Programming, II, V. Dahl & P. Saint-Dizier(eds.), Elsevier Science Publ., Amsterdam, p. 63-78, 1988.

Left-corner parsing inProlog. How to handle e-rules that hide left recursion (remove them by duplicating the rule).

13.8 PRECEDENCE AND BOUNDED-CONTEXT PARSING

\Gamma  Harold Wolpe, "Algorithm for analyzing logical statements to produce a truth

function table", Commun. ACM, vol. 1, no. 3, p. 4-13, March 1958. The paperdescribes an algorithm to convert a Boolean expression into a decision table. The expression is first fully

parenthesized through a number of substitution rules that represent the priorities of the operators. Parsingis then done by counting parentheses. Further steps construct a decision table.

\Gamma  J.H. Wegstein, "From formulas to computer-oriented language", Commun. ACM,

vol. 2, no. 3, p. 6-8, March 1959. A program that converts from arithmetic expressions tothree-address code is given as a one-page flowchart. The parser is basically operator-precedence, with

built-in precedences.

\Gamma  Robert W. Floyd, "Syntactic analysis and operator precedence", J. ACM, vol. 10,

no. 3, p. 316-333, July 1963. Operator-precedence explained and applied to an Algol 60compiler.

\Gamma  J. Eickel, M. Paul, F.L. Bauer, K. Samelson, "A syntax-controlled generator of

Sec. 13.8] Precedence and bounded-context parsing 295

formal language processors", Commun. ACM, vol. 6, no. 8, p. 451-455, Aug 1963.In this early paper, the authors develop and describe what is basically a (2,1) bounded-context parser.
Reduction rules have to have the form U n^ V or R n^ ST. Such a grammar is called an R-language; it is"unique" if the parse tables can be constructed without conflict. The terminology in the paper differs considerably from today's.

\Gamma  Robert W. Floyd, "Bounded context syntax analysis", Commun. ACM, vol. 7, no.

2, p. 62-67, Feb 1964. For each right-hand side R in the grammar, enough context isconstructed (by hand) so that when

R is found in a sentential form in the right context in a bottom-upparser, it can safely be assumed to be the handle.

\Gamma  Niklaus Wirth, Helmut Weber, "EULER - A generalization of ALGOL and its

formal definition, Part 1/2", Commun. ACM, vol. 9, no. 1/2, p. 13-25/89-99,Jan/Feb 1966.

Detailed description of simple and extended precedence. A table generationalgorithm is given. Part 2 contains the complete precedence table plus functions for the language

EULER.

\Gamma  David F. Martin, "Boolean matrix methods for the detection of simple precedence grammars", Commun. ACM, vol. 11, no. 10, p. 685-687, Oct 1968. Findingthe simple-precedence relations is explained as matrix operations on matrices derived trivially from the

grammar.

\Gamma  James R. Bell, "A new method for determining linear precedence functions for

precedence grammars", Commun. ACM, vol. 12, no. 10, p. 567-569, Oct 1969. Theprecedence relations are used to set up a connectivity matrix. Take the transitive closure and count 1's in

each row. Check for correctness of the result.

\Gamma  Alain Colmerauer, "Total precedence relations", J. ACM, vol. 17, no. 1, p. 14-30,

Jan 1970. The non-terminal resulting from a reduction is not put on the stack but pushed backinto the input stream; this leaves room for more reductions on the stack. This causes precedence relations

that differ considerably from simple precedence.

\Gamma  A. Learner, A.L. Lim, "A note on transforming grammars to Wirth-Weber precedence form", Computer J., vol. 13, p. 142-144, 1970. An algorithm is given totransform any CF grammar to simple precedence form (with possible duplicate right-hand sides).

\Gamma  Jacques Loeckx, "An algorithm for the construction of bounded-context parsers",

Commun. ACM, vol. 13, no. 5, p. 297-307, May 1970. By systematically generating allBC states the parser may encounter.

\Gamma  J. Ichbiah, S. Morse, "A technique for generating almost optimal Floyd-Evans

productions of precedence grammars", Commun. ACM, vol. 13, no. 8, p. 501-508,Aug 1970.

The notion of "weak precedence" is defined in the introduction. The body of thearticle is concerned with efficiently producing good Floyd-Evans productions from a given weak precedence grammar.

\Gamma  A.V. Aho, P.J. Denning, J.D. Ullman, "Weak and mixed strategy precedence

parsing", J. ACM, vol. 19, no. 2, p. 225-243, April 1972. The theory behind and acomparison of various bottom-up (shift/reduce) parsing algorithms.

\Gamma  Shoji Sekimoto, "Extended right precedence grammars and analyzing technique

for them", Inform. Process. Japan, vol. 12, p. 21-25, 1972. In the presence of two rules
Ao""aXb and Bo""b, weak precedence requires that there be no precedence relation between X and B.This requirement is replaced by a more lenient (but more complicated) one, resulting in

right precedenceand is further relaxed to
extended right precedence.

296 Annotated bibliography [Ch. 13

\Gamma  David F. Martin, "A Boolean matrix method for the computation of linear precedence functions", Commun. ACM, vol. 15, no. 6, p. 448-454, June 1972. Detaileddescription of a variant of Bell's method [Precedence 1969].

\Gamma  A.V. Aho, J.D. Ullman, "Linear precedence functions for weak precedence grammars", Intern. J. Comput. Math., vol. A3, p. 149-155, 1972. The entries in aprecedence table have four values: <*, =. , >* and blank. Since precedence functions can only represent three

relations: <, = and >, the blank is sacrificed, to the detriment of error detection. A weak precedencetable holds only three kinds of entries: <=*, >* and blank, which can be mapped onto

<, > and =. The result-ing matrix will normally not allow precedence functions, but it will if a number of the ='s are sacrificed.

An algorithm is given to (heuristically) determine the minimal set of ='s to sacrifice; unfortunately this isdone by calling upon a heuristic algorithm for partitioning graphs.

\Gamma  J. McAfee, L. Presser, "An algorithm for the design of simple precedence grammars", J. ACM, vol. 19, no. 3, p. 385-395, July 1972. An algorithm to construct for anyCF grammar a grammar with conflict-free simple-precedence relations that generates the same language

(with possible duplicate right-hand sides, though).

\Gamma  David Crowe, "Generating parsers for affix grammars", Commun. ACM, vol. 15,

no. 8, p. 728-734, Aug 1972. See same paper [VW 1972].
\Gamma  David S. Wise, "Generalized overlap resolvable grammars and their parsers", J.

Comput. Syst. Sci., vol. 6, p. 538-572, Dec 1972. A CF grammar is Generalized
Overlap-Resolvable (GOR) if the handle in a bottom-up parser can be found deterministically by identi-fying the right-hand side on the top of the stack, preceded on the stack by a token from a set of admissible left-context tokens and by requiring that the next input token belong to a set of admissible right-context tokens. A grammar is

Overlap-Resolvable (OR) if it is GOR and e-free. These grammars arebetween mixed-strategy precedence and SLR(1) in power. A very efficient and flexible implementation

using Do"mo"lki's technique is described.

\Gamma  Rainer Zimmer, "Soft precedence", Inform. Process. Lett., vol. 1, p. 108-110,

1972. A grammar with a conflict-free precedence table in which not all right-hand sides aredifferent, causes reduce conflicts. For each reduce conflict a simple pattern is constructed which resolves

the conflict by checking the parse stack. If for each reduce conflict such a pattern exists, the grammar is
soft precedence. A matrix algorithm to find the patterns if they exist is given.

\Gamma  A.V. Aho, J.D. Ullman, "Error detection in precedence parsers", Math. Syst.

Theory, vol. 7, no. 2, p. 97-113, 1973. The full precedence matrix is split into two copies,one used to decide between shifts and reduces, which contains <=*, >* and blank, and the other to determine

the left end of the handle which contains <*, =. and blank. The techniques of Aho and Ullman [Precedence1972] are now applied to both matrices.

\Gamma  James N. Gray, Michael A. Harrison, "Canonical precedence schemes", J. ACM,

vol. 20, no. 2, p. 214-234, April 1973. The theory behind precedence parsing.
\Gamma  G. Terrine, "Coordinate grammars and parsers", Computer J., vol. 16, p. 232-

244, 1973. A bounded-context parser is made to stack dotted items rather than terminals andnon-terminals. This makes it stronger than bounded-context but still weaker than LR.

\Gamma  M.D. Mickunas, V.B. Schneider, "A parser-generating system for constructing

compressed compilers", Commun. ACM, vol. 16, no. 11, p. 669-676, Nov 1973.Describes a bounded-context parser with transduction facilities. Includes a compression algorithm for

BC tables.

\Gamma  Susan L. Graham, "On bounded right context languages and grammars", SIAM J.

Computing, vol. 3, no. 3, p. 224-254, Sept 1974. Theory of same.

Sec. 13.8] Precedence and bounded-context parsing 297

\Gamma  J.H. Williams, "Bounded-context parsable grammars", Inform. Control, vol. 28,

no. 4, p. 314-334, Aug 1975. A more general non-canonical form of bounded-context, called
bounded-context parsable, is defined which allows, among others, the parsing in linear time of somenon-deterministic languages. Although a parser could be constructed, it would not be practical.

\Gamma  M.R. Levy, "Complete operator precedence", Inform. Process. Lett., vol. 4, no. 2,

p. 38-40, Nov 1975. Establishes conditions under which operator-precedence works properly.
\Gamma  D.S. Henderson, M.R. Levy, "An extended operator precedence parsing algorithm", Computer J., vol. 19, no. 3, p. 229-233, 1976. The relation <* is split into <*1 and<*
2. a<*1b means that a may occur next to b, a<*2b means that a non-terminal has to occur between them.Likewise for =. and >* . This is extended operator-precedence.

\Gamma  M.S. Krishnamurthy, H.R. Ramesha Chandra, "A note on precedence functions", Inform. Process. Lett., vol. 4, no. 4, p. 99-100, Jan 1976. Proves for somesimple-precedence tables that no grammars for them exist.

\Gamma  R.K. Shyamasundar, "A note on linear precedence functions", Inform. Process.

Lett., vol. 5, no. 3, p. 81, 1976. Comments on Krishnamurthy and Ramesha Chandra[Precedence 1976].

\Gamma  M.H. Williams, "Complete operator precedence conditions", Inform. Process.

Lett., vol. 6, no. 2, p. 60-62, April 1977. Revision of the criteria of Levy [Precedence1975].

\Gamma  Eberhard Bertsch, "The storage requirement in precedence parsing", Commun.

ACM, vol. 20, no. 3, p. 192-194, March 1977. Suppose for a given grammar there exists aprecedence matrix but the precedence functions

f and g do not exists. There always exist sets of pre-cedence functions
fi and gj such that for two symbols a and b, comparison of fc (b)(a) and gd (a)(b) yieldsthe precedence relation between

a and b, where c and d are selection functions which select the fi and gjto be compared. An algorithm is given to construct such a system of functions.

\Gamma  R.K. Shyamasundar, "Precedence parsing using Do"mo"lki's algorithm", Intern. J.

Comput. Math., vol. A6, p. 105-114, 1977. Do"mo"lki's algorithm can find a reducibleright-hand-side efficiently but cannot know if it is a handle. Precedence parsing can find the handle

easily but has trouble determining which right-hand side it is. Together they are a perfect match.

\Gamma  I.H. Sudborough, "A note on weak operator precedence grammars", Inform. Process. Lett., vol. 7, no. 5, p. 213-218, 1978. Introduces weak operator-precedence and statesthat
L (SP)=L (WP) and L (SP)E'L (WOP)E'L (OP), where SP is simple precedence, WP is weak pre-cedence,

WOP is weak operator-precedence and OP is operator-precedence, and L (X) is the set oflanguages generatable by

X grammars.

\Gamma  R.K. Shyamasundar, "Precedence-regular grammars", Intern. J. Comput. Math.,

vol. A7, p. 173-186, 1979. Characterization of the class of grammars for which theShyamasundar/Do"mo"lki technique (Shyamasundar [Precedence 1977]) works. Note that whereas in LLand LR-regular it is the rest of the input that is analysed by a FS automaton to resolve a conflict, inprecedence-regular it is the stack that is analysed by a Do"mo"lki-like automaton.

\Gamma  Peter Ruz^ic^ka, "Validity test for Floyd's operator precedence parsing algorithms". In Mathematical Foundations of Computer Science, J. Bec^va'r^ (eds.), Lec-ture Notes in Computer Science #74, Springer-Verlag, Berlin, p. 415-424, 1979.

Additions to the criteria by Levy [Precedence 1975].

\Gamma  M.H. Williams, "Conditions for extended operator precedence parsing", Computer J., vol. 22, no. 2, p. 164-168, 1979. Tighter analysis of extended operator-precedence

298 Annotated bibliography [Ch. 13
than Henderson and Levy [Precedence 1976].

\Gamma  Amiram Yehudai, "A new definition for simple precedence grammars", BIT, vol.

19, p. 282-284, 1979. A weaker definition of simple precedence is given, which is then shownto define the same class.

\Gamma  K.R. Moll, "Left context precedence grammars", Acta Inform., vol. 14, p. 317-

335, 1980. Elaborate and definitely non-trivial refinement of the notion of precedence, to achievethe viable-prefix property.

\Gamma  Wilf R. LaLonde, Jim des Rivieres, "Handling operator precedence in arithmetic

expressions with tree transformations", ACM Trans. Prog. Lang. Syst., vol. 3, no.1, p. 83-103, Jan 1981.

Algorithms that will restructure the parse tree when the operatorprecedences are modified. The algorithm is also used to do parsing: first produce a parse tree in standard

form and then add the precedence information.

\Gamma  David A. Workman, "SR(s,k) parsers: A class of shift-reduce bounded-context

parsers", J. Comput. Syst. Sci., vol. 22, no. 1, p. 178-197, 1981. The look-back over allcombinations of

m symbols on the stack in BC(m,n) parsers is replaced by an LR(m)-like automaton,resulting in an SR(

m,n) parser, if possible. The paper is mainly concerned with theoretical properties ofSR grammars and parsers.

\Gamma  M.H. Williams, "A systematic test for extended operator precedence", Inform.

Process. Lett., vol. 13, no. 4-5, p. 187-190, End 1981. The criteria of Williams[Precedence 1979] in algorithmic form.

\Gamma  M.C. Er, "A note on computing precedence functions", Computer J., vol. 25, no.

3, p. 397-398, 1982. By determining longest paths in a digraph.
\Gamma  Junichi Aoe, Yoneo Yamamoto, Ryosaku Shimada, "A practical method for

reducing weak precedence parsers", IEEE Trans. Softw. Eng., vol. SE-9, no. 1, p.25-30, Jan 1983.

When a weak-precedence parser finds a >* relation and starts a reducesequence, the sequence stops when a <=* is met; all intermediate relations are required to be >* , to continue

the sequence. The authors modify the parser to continue the sequence anyway, until a <=* is found; theintermediate relations are never tested and their values are immaterial. This is exploited to reduce the
parse table.

\Gamma  Piotr Wyrostek, "On the `correct prefix property' in precedence parsers", Inform.

Process. Lett., vol. 17, no. 3, p. 161-165, Oct 1983. Extremely complicatedtransformation of precedence grammars to mixed-strategy grammars which have, for some parsers, the

correct-prefix property. With an erratum in Inform. Process. Lett., vol. 19, no. 2, p. 111, Aug 1984.

\Gamma  Piotr Wyrostek, "Precedence technique is not worse than SLR(1)", Acta Inform.,

vol. 23, p. 361-392, 1986. The thesis in the title is proved by giving an algorithm thattransforms an SLR(1) grammar into a (1,1)-mixed-strategy precedence grammar with the viable-prefix

property (see also Graham [Precedence 1974]). The resulting precedence table is often smaller than theSLR(1) table.

\Gamma  R. Nigel Horspool, Michael R. Levy, "Correctness of an extended operatorprecedence parsing algorithm", Inform. Process. Lett., vol. 24, no. 4, p. 265-273,March 1987.

Establishes conditions under which extended operator-precedence (see Hendersonand Levy [Precedence 1976]) works properly.

Sec. 13.8] Finite-state automata 299
13.9 FINITE-STATE AUTOMATA

\Gamma  M.O. Rabin, D. Scott, "Finite automata and their decision problems", IBM J.

Research and Development, vol. 3, p. 114-125, April 1959. A finite-state automaton isconsidered as the definition of the set of strings it accepts. Many fundamental properties of FS automata

are exposed and proved. The very useful subset construction algorithm can be found in Definition 11.

\Gamma  Ken Thompson, "Regular expression search algorithm", Commun. ACM, vol. 11,

no. 6, p. 419-422, June 1968. The regular expression is turned into a transition diagram,which is then interpreted in parallel. Remarkably, each step generates (IBM 7094) machine code to execute the next step.

\Gamma  Walter L. Johnson, James S. Porter, Stephanie I. Ackley, Douglas T. Ross,

"Automatic generation of efficient lexical processors using finite state tech-niques", Commun. ACM, vol. 11, no. 12, p. 805-813, Dec 1968.

Semantic actions areattached to some rules of a FS grammar. A variant of the subset construction is described that requires

the unique determination of the states in which a semantic action is required.

\Gamma  Franklin L. DeRemer, "Lexical analysis". In Compiler Construction: An

Advanced Course, F.L. Bauer & J. Eickel (eds.), Lecture Notes in Computer Sci-ence #21, Springer-Verlag, Berlin, p. 109-120, 1974.

1. General introduction to lexicalanalysers, hand-written and generated. 2. Simplification of the LR parser generator algorithm for the

case of non-self-embedding CF grammars (which is possible since the latter in fact generate a regularlanguage).

\Gamma  Alfred V. Aho, Margaret J. Corasick, "Efficient string matching: an aid to bibliographic search", Commun. ACM, vol. 18, no. 6, p. 333-340, June 1975. A givenstring embedded in a longer text is found by a very efficient FS automaton derived from that string.

\Gamma  M.E. Lesk, E. Schmidt, "Lex - a lexical analyzer generator". In UNIX Manuals,

Bell Laboratories, Murray Hill, New Jersey, p. 13, 1975. The regular grammar isspecified as a list of regular expressions, each associated with a semantic action, which can access the

segment of the input that matches the expression. Substantial look-ahead is performed if necessary. lex isa well-known and often-used lexical-analyser generator.

\Gamma  D. Langendoen, "Finite-state parsing of phrase-structure languages", Linguistic

Inquiry, vol. 6, no. 4, p. 533-554, 1975. See same author [NatLang 1975].
\Gamma  Roman Krzemien' , Andrzej L/ukasiewicz, "Automatic generation of lexical

analyzers in a compiler-compiler", Inform. Process. Lett., vol. 4, no. 6, p. 165-168, March 1976.

A grammar is quasi-regular if it does not feature nested recursion;consequently it generates a regular language. An algorithm is given that identifies all quasi-regular

subgrammars in a CF grammar, thus identifying the "lexical part" of the grammar.

\Gamma  Thomas J. Ostrand, Marvin C. Paull, Elaine J. Weyuker, "Parsing regular grammars with finite lookahead", Acta Inform., vol. 16, p. 125-138, 1981. Every regular(Type 3) language can be recognized by a finite-state automaton without look-ahead, but such a device is

not sufficient to do parsing. For parsing, look-ahead is needed; if a regular grammar needs a look-aheadof

k tokens, it is called FL(k). FS grammars are either FL(k), FL(e^) or ambiguous; a decision algorithmis described, which also determines the value of

k, if appropriate.A simple parsing algorithm is a FS automaton gouverned by a look-up table for each state, mapping

look-aheads to new states. A second algorithm avoids these large tables by constructing the relevantlook-ahead sets on the fly.

\Gamma  V.P. Heuring, "The automatic generation of fast lexical analysers", Softw. Pract.

Exper., vol. 16, no. 9, p. 801-808, 1986. The lexical analyser is not based directly on a FS

300 Annotated bibliography [Ch. 13
automaton but has a number of built-in analysers for, e.g., identifier, integer, string, which can beparametrized. The lexical analyser is about 6 times faster than UNIX

lex.

\Gamma  Douglas W. Jones, "How (not) to code a finite-state machine", ACM SIGPLAN

Notices, vol. 23, no. 8, p. 19-22, Aug 1988. Small, well-structured and efficient code canbe generated for a FS machine by deriving a single deterministic regular expression from the FS machine

and implementing this expression directly using while and repeat constructions.

\Gamma  Duane Szafron, Randy Ng, "LexAGen: an interactive incremental scanner generator", Softw. Pract. Exper., vol. 20, no. 5, p. 459-483, May 1990. Extensivedescription of an interactive generator for lexical analysers, in Smalltalk-80.

13.10 NATURAL LANGUAGE HANDLING

\Gamma  Hamish P. Dewar, Paul Bratley, James P. Thorne, "A program for the syntactic

analysis of English sentences", Commun. ACM, vol. 12, no. 8, p. 476-479, 1969.The authors argue that the English language can be described by a regular grammar: most rules are regular already and the others describe concatenations of regular sublanguages. The finite-state parser usedconstructs the state subsets on the fly, to avoid large tables. Features (attributes) are used to check consistency and to weed out the state subsets.

\Gamma  W.A. Woods, "Transition networks for natural languages", Commun. ACM, vol.

13, no. 10, p. 591-606, Oct 1970. A recursive-descent parser guided by transition networksrather than by grammar rules.

\Gamma  D. Langendoen, "Finite-state parsing of phrase-structure languages", Linguistic

Inquiry, vol. 6, no. 4, p. 533-554, 1975. A subset of the CF grammars that produces regular(FS) languages is analysed and an algorithm is given to produce a FS parser for any grammar belonging

to this subset. Much attention is paid to the linguistic applicability of such grammars. We advice thereader of this paper to make a list of the abbreviations used in it, to assist in reading.

\Gamma  William A. Woods, "Cascaded ATN grammars", Am. J. Comput. Linguist., vol. 6,

no. 1, p. 1-12, Jan-March 1980. The grammar (of a natural language) is decomposed into anumber of grammars, which are then

cascaded, that is, the parser for grammar Gn obtains as input thelinearized parse tree produced by the parser for

Gn -1. Each grammar can then represent a linguistichypothesis. An efficient implementation is given.

\Gamma  Daniel Chester, "A parsing algorithm that extends phrases", Am. J. Comput.

Linguist., vol. 6, no. 2, p. 87-96, April-June 1980. A variant of a backtracking left-cornerparser is described that is particularly convenient for handling continuing phrases like: "the cat that

caught the rat that stole the cheese".

\Gamma  Harry Tennant, Natural language processing, Petrocelli Books, Inc., Princeton,

N.J., p. 276, 1981. Easy-going introduction to natural language processing; covers syntax,semantics, knowledge representation and dialog with many amusing examples. With glossary.

\Gamma  Philips J. Hayes, George V. Mouradian, "Flexible parsing", Am. J. Comput.

Linguist., vol. 7, no. 4, p. 232-242, Oct-Dec 1981. A directional breadth-first bottom-upparser yields some sets of partial parse trees for segments of the input text. Then several heuristics are

used to combine these into a "top-level hypothesis". The purpose is to be able to parse fragmented orungrammatical natural language input.

\Gamma  Ursula Klenk, "Microcomputers in linguistic data processing: Context-free parsing", Microprocess. Microprogram., vol. 9, no. 5, p. 281-284, May 1982. Shows thefeasibility of the implementation of four general CF parsers on a very small (48 kbytes) PC: breadth-first

Sec. 13.10] Natural language handling 301
top-down, backtracking top-down, bottom-up and Earley's algorithm.

\Gamma  K. Sparck Jones, Y. Wilks, Automatic natural language parsing, Ellis Horwood

Ltd., Chicester, p. 208, 1983. Eighteen short chapters on the application of parsing in NLprocessing, using CF grammars, Augmented Transition Networks, transducers, Generalized Phrase

Structure Grammars and otherwise. Many literature references.

\Gamma  Margaret King (Ed.), Parsing Natural Language, Academic Press, London/New

York, p. 308, 1983. A compilation of twelve tutorials on aspects of parsing in a linguisticsetting. Very readable.

\Gamma  Stuart M. Shieber, "Direct parsing of ID/LP grammars", Linguistics and Philosophy, vol. 7, p. 135-154, 1984. In this very readable paper, the Earley parsing technique isextended in a straightforward way to ID/LP grammars (Gazdar et al. [NatLang 1985]). Practical algorithms are given.

\Gamma  Gerald Gazdar, Ewan Klein, Geoffrey Pullum, Ivan Sag, Generalized phrase

structure grammar, Basil Blackwell Publisher, Ltd., Oxford, UK, p. 276, 1985.The phrase structure of natural languages is more easily and compactly described using

Generalized
Phrase Structure Grammars (GPSGs) or Immediate Dominance/Linear Precedence grammars (ID/LPgrammars) than using conventional CF grammars. Theoretical foundations of these grammars are given

and the results are used extensively in linguistic syntactic theory. GPSGs are not to be confused withgeneral phrase structure grammars, aka Chomsky Type 0 grammars, which are called "unrestricted"
phrase structure grammars in this book.The difference between GPSGs, ID/LP grammars and CF grammars is explained clearly. A GPSG is a
CF grammar, the non-terminals of which are not unstructured names but sets of features with theirvalues; such compound non-terminals are called

categories. An example of a feature is NNOOUUNN, which canhave the values
++ or --; <<NNOOUUNN,,++>> will be a constituent of the categories "noun phrase", "noun", "nounsubject", etc.

ID/LP grammars differ from GPSGs in that the right-hand sides of production rules consist of multisetsof categories rather than of ordered sequences. Thus, production rules (Immediate Dominance rules)
define vertical order in the production tree only. Horizontal order in each node is restricted through (butnot necessarily completely defined by) Linear Precedence rules. Each LP rule is considered to apply to
every node; this is called the Exhaustive Constant Partial Ordering property.

\Gamma  Mary Dee Harris, Natural Language Processing, Reston Publ. Comp, Prentice

Hall, Reston, Virg., p. 368, 1985. A good and slow-paced introduction to natural languageprocessing, with a clear algorithmic view. Lexical analysis including look-up algorithms, phrase structure grammars (actually context-free) and semantic networks are explained and much attention is paid toattaching semantics to the structures obtained.

\Gamma  Veronica Dahl, Patrick Saint-Dizier, Natural language understanding and logic

programming, Elsevier Science Publ., Amsterdam, p. 243, 1985. Seventeen papers onthe application of various grammar types to natural languages.

\Gamma  Glenn Blank, "A new kind of finite-state automaton: Register vector grammar".

In Ninth International Conference on Artificial Intelligence, UCLA, p. 749-756,Aug 1985.

In FS grammars, emphasis is on the states: for each state it is specified which tokensit accepts and to which new state each token leads. In

Register-Vector grammars (RV grammars)emphasis is on the tokens: for each token it is specified which state it maps onto which new state(s). The

mapping is done through a special kind of function, as follows. The state is a (global) vector (array) ofregisters (features, attributes). Each register can be

on or off. For each token there is a condition vectorwith elements which can be
on, off or mask (= ignore); if the condition matches the state, the token isallowed. For each token there is a result vector with elements which can be

on, off or mask (= copy); ifthe token is applied, the result-vector elements specify how to construct the new state. e-moves are incorporated by having tokens (called labels) which have e for their representation. Termination has to be

302 Annotated bibliography [Ch. 13
programmed as a separate register.RV grammars are claimed to be compact and efficient for describing the FS component of natural
languages. Examples are given. Embedding is handled by having a finite number of levels inside thestate.

\Gamma  Barbara J. Grosz, Karen Sparck Jones, Bonnie Lynn Webber, Readings in natural

language processing, Morgan Kaufmann Publishers, Inc., Los Altos, Ca. 94022, p.664, 1986.

Selected papers on NL processing, covering syntactic models, semanticinterpretation, discourse interpretation, language action and intention, NL generation and actual systems.

\Gamma  Walter Goshawke, Ian D.K. Kelly, J. David Wigg, Computer translation of

natural language, Sigma Press, Wilslow, UK, p. 275, 1987. The book consists of threeparts. 1) Overview of progress in Machine Translation. 2) Description of the intermediate code SLUNT

(Spoken Languages Universal Numeric Translation), a stylized numeric language-independent vehiclefor semantics. 3) The International Communicator System, a set of programs to manipulate SLUNT
structures.

\Gamma  Leonard Bolc (Ed.), Natural language parsing systems, Springer-Verlag, Berlin,

p. 367, 1987. A collection of recent papers on parsing in a natural language environment.Among the subjects are Earley and CYK parsers, assigning probabilities to ambiguous parsings, error

recovery and, of course, attaching semantics to parsings.

\Gamma  Jonathan H. Reed, "An efficient context-free parsing algorithm based on register

vector grammars". In Third Annual IEEE Conference on Expert Systems inGovernment, p. 34-40, 1987.

The principles of RV grammars (Blank [NatLang 1985]) areapplied to CF grammars by having a separate RV grammar for each syntactic category, each allowing the

names of syntactic categories as tokens. The Earley parsing algorithm is then adapted to handle thesegrammars. Measurements indicate that the parser is 1 to 3 times faster on small grammars and 5 to 10
times on large grammars.

\Gamma  V. Dahl, P. Saint-Dizier, Natural language understanding and logic programming, II, Elsevier Science Publ., Amsterdam, p. 345, 1988. Eighteen papers and twopanel sessions on programs for natural language understanding, mostly in Prolog.

\Gamma  Glenn D. Blank, "A finite and real-time processor for natural language", Commun. ACM, vol. 32, no. 10, p. 1174-1189, Oct 1989. Several aspects of the register-vector grammars of Blank [NatLang 1985] are treated and extended: notation, center-embedding (3 levels), non-determinism through boundary-backtracking, efficient implementation.

13.11 ERROR HANDLING

\Gamma  W.B. Smith, "Error detection in formal languages", J. Comput. Syst. Sci., vol. 4,

p. 385-405, Oct 1970. A formal paper that examines properties of recognizers that determinewhether the number of substitution errors that has occurred is bounded by some function. Different

language classes and different levels of numbers of errors are examined. It appears that there is littledifference between languages under a constant maximum number of errors and under a constant maximum number of errors per block.

\Gamma  J.E. LaFrance, "Optimization of error-recovery in syntax-directed parsing algorithms", ACM SIGPLAN Notices, vol. 5, no. 12, p. 2-17, Dec 1970. Floyd productionsare divided into groups, and each production in a group is tried in order. If all productions of a group

fail, error recovery takes place, depending on the type(s) of the rules in the group. Apart from localcorrections, in some cases all possible productions are traced three symbols ahead. The result is compared with the next four input symbols, using a set of twenty patterns, each pattern modeling a particularkind of error. If this fails, a FOLLOW-set recovery technique is applied. The implications of

Sec. 13.11] Error handling 303
implementing this error recovery technique in a backtracking recursive descent parser are discussed.

\Gamma  A.V. Aho, T.G. Peterson, "A minimum-distance error-correcting parser for

context-free languages", SIAM J. Computing, vol. 1, no. 4, p. 305-312, 1972. A CFgrammar is extended with error productions so that it will produce \Sigma *; this is effected by replacing each

occurrence of a terminal in a rule by a non-terminal that produces said terminal "with 0 errors" and anyamount of garbage, including e, "with 1 or more errors". The items in an Earley parser are extended with
a count, indicating how many errors were needed to create the item. An item with error count k is addedonly if no similar item with a lower error count is present already.

\Gamma  C.J. Burgess, "Compile-time error diagnostics in syntax-directed compilers",

Computer J., vol. 15, no. 4, p. 302-307, 1972. This paper attempts to define errordiagnostics formally by incorporating them as error productions in the grammar, and examines the extent

to which the positioning of these productions and messages in the grammar can be done automatically.For left-factored grammars it appears to be easy.

\Gamma  E.G. James, D.P. Partridge, "Adaptive correction of program statements", Commun. ACM, vol. 16, no. 1, p. 27-37, Jan 1973. Discusses an error correction technique thatuses artificial intelligence and approximate pattern matching techniques, basing corrections on built-in

statistics, which are adapted continuously.

\Gamma  R.W. Conway, T.R. Wilcox, "Design and implementation of a diagnostic compiler for PL/I", Commun. ACM, vol. 16, no. 3, p. 169-179, 1973. Describes adiagnostic PL/C compiler, using a systematic method for finding places where repair is required, but the

repair strategy for each of these places is chosen by the implementor. The parser uses a separable transi-tion diagram technique (see Conway [Misc 1963]). The error messages detail the error found and the
repair chosen.

\Gamma  G. Lyon, "Syntax-directed least-errors analysis for context-free languages: a practical approach", Commun. ACM, vol. 17, no. 1, p. 3-14, Jan 1974. Discusses a least-errors analyser, based on Earley's parser without look-ahead. The Earley items are extended with an

error count, and the parser is started with items for the start of each rule, in each state set. Earley'sscanner is extended as follows: for all items with the dot in front of a terminal, the item is added to the
same state set with an incremented error count and the dot after the terminal (this represents an insertionof the terminal); if the terminal is not equal to the input symbol associated with the state set, add the item
to the next state set with an incremented error count and the dot after the terminal (this represents areplacement); add the item as it is to the next state set, with an incremented error count (this represents a
deletion). The completer does its work as in the Earley parser, but also updates error counts. Items withthe lowest error counts are processed first, and when a state set contains an item, the same item is only
added if it has a lower error count.

\Gamma  R.A. Wagner, "Order-n correction for regular languages", Commun. ACM, vol.

17, no. 5, p. 265-268, May 1974. Presents an O(n) algorithm which, given a string and afinite-state automaton, can correct the string to an acceptable one with a minimum number of edit operations.

\Gamma  C. Ghezzi, "LL(1) grammars supporting an efficient error handling", Inform. Process. Lett., vol. 3, no. 6, p. 174-176, July 1975. Faced with an erroneous token in anenvironment where empty productions can occur, a strong-LL(1) parser will often do some e-moves

before reporting the error; this makes subsequent error recovery more difficult. This undesirablebehaviour can be avoided by splitting each rule into a number of copies, one for each set of tokens it may
be followed by. An efficient algorithm for this transformation on the grammar is supplied. The resultinggrammar is of type

CRLL(1).

\Gamma  Susan L. Graham, Steven P. Rhodes, "Practical syntactic error recovery", Commun. ACM, vol. 18, no. 11, p. 639-650, Nov 1975. See Section 10.6.1 for a discussion of

304 Annotated bibliography [Ch. 13
this error recovery method.

\Gamma  J.-P. Le'vy, "Automatic correction of syntax errors in programming languages",

Acta Inform., vol. 4, p. 271-292, 1975. When a bottom-up parser encounters an error, partof the stack is pushed back into the input stream (for instance, until a

beacon token is on the top of thestack). Starting from the new state now uncovered on the stack, all possible parsings of the input allowing at most n errors are constructed, using breadth-first search and Lyon's scheme [ErrHandl 1974], untilall parsers are in the same state or all parsers need to assume an

n +1-st error. In the latter case the inputis rejected, otherwise one parse is chosen and parsing continues.

\Gamma  S. Feyock, P. Lazarus, "Syntax-directed correction of syntax errors", Softw. Pract.

Exper., vol. 6, no. 2, p. 207-219, 1976. When an error is detected, the following errorcorrection strategy is applied:

1. A set of correction strings is generated (delete current symbol, insert symbol, replace symbol,interchange with next symbol).
2. This set is filtered (correction syntactically and semantically acceptable?).3. If there is more than one element left, use a heuristic to determine the "best" one. If only one is

left, this is the one. If none are left, back-up one input symbol, and go back to step 1.
\Gamma  David Gries, "Error recovery and correction". In Compiler Construction, an

Advanced Course, Second Edition, F.L. Bauer & J. Eickel (eds.), Springer-Verlag,New York, p. 627-638, 1976.

Mostly an annotated bibliography containing some 35 entries,not all on error handling.

\Gamma  J. Ciesinger, "Generating error recovery in a compiler generating system". In

GI-4 Fachtagung u"ber Programmiersprachen, H.-J. Schneider & M. Nagl (eds.),Lecture Notes in Computer Science #34, Springer-Verlag, New York, p. 185-193,

1976. Proposes an error recovery method using pairs of elements of the alphabet, called "braces",which are used to select part of the input that contains the error and select a goal (non-terminal) to which
this part must be reduced. Some conditions are derived which must be fulfilled by the braces, and it isshown that the braces can be computed automatically, at parser generation time.

\Gamma  K.S. Fu, "Error-correcting parsing for syntactic pattern recognition". In Data

Structure, Computer Graphics and Pattern Recognition, A. Klinger et al. (eds.),Academic Press, New York, p. 449-492, 1977.

Discusses the least-errors analyser of Ahoand Peterson [ErrHandl 1972] in the context of stochastic grammars. Least-errors then becomes maximum likelihood. Many examples are given.

\Gamma  S.Y. Lu, K.S. Fu, "Stochastic error-correcting syntax analysis for recognition of

noisy patterns", IEEE Trans. Comput., vol. 26, no. 12, p. 1268-1276, 1977. Thispaper models deletion, insertion, and replacement errors into a stochastic disformation model: each error

has a probability associated with it. Then, the model is incorporated into the stochastic context-freegrammar, and an Earley parser is modified to look for the most likely error correction. This proves to be
inefficient, so a sequential classification algorithm (SCA) is used. This SCA uses a stopping rule thattells when it has seen enough terminals to make a decision. The authors are interested in pattern recognition rather than in parse trees.

\Gamma  George Poonen, "Error recovery for LR(k) parsers". In Inf. Process. 77, Bruce

Gilchrist (eds.), IFIP, North Holland Publ. Co., Amsterdam, p. 529-533, Aug1977.

A special token, ERRORMARK, is added to the grammar, to represent any incorrect stretchof input. When encountering an error in an LR(1) parser, scan the stack for states having a shift on

ERRORMARK, collect all shift tokens of these states into an acceptable-set, skip the input until anacceptable token is found and unstack until the corresponding accepting state is uncovered.

\Gamma  Jean E. Musinski, "Lookahead recall error recovery for LALR parsers", ACM

SIGPLAN Notices, vol. 12, no. 10, p. 48-60, Oct 1977. Shows how the error recovery of

Sec. 13.11] Error handling 305
a specific LALR(1) parser can be improved by what amounts to the restricted decomposition of symbolson the stack, to increase the acceptable set.

\Gamma  E.-W. Dieterich, "Parsing and syntactic error recovery for context-free grammars

by means of coarse structures". In Automata, Languages and Programming, A.Salomaa & M. Steinby (eds.), Lecture Notes in Computer Science #52, SpringerVerlag, Berlin, p. 492-503, 1977. Proposes a two-level parsing process that separates thecoarse structures from the rest of the grammar. These coarse structures consist of characteristic brackets,
for instance begin and end. Error recovery can then also be applied to these two levels.

\Gamma  S. Sippu, E. Soisalon-Soininen, "On defining error recovery in context-free parsing". In Automata, Languages and Programming, A. Salomaa & M. Steinby(eds.), Lecture Notes in Computer Science #52, Springer-Verlag, Berlin, p. 492-

503, 1977. Uses a grammatical transformation that leads to an LR grammar that incorporatecertain replacement, deletion, or insertion errors.
\Gamma  Charles Wetherell, "Why automatic error correctors fail", Comput. Lang. (Elmsford, NY), vol. 2, p. 179-186, 1977. Shows that there is no hope of building efficientautomatic syntactic error correctors which can handle large classes of errors perfectly. The author argues

that parser writers should instead study the error patterns and work for efficient correction of commonerrors. Language designers must concentrate on ways to make languages less susceptible to common
errors.

\Gamma  D.A. Turner, "Error diagnosis and recovery in one pass compilers", Inform. Process. Lett., vol. 6, p. 113-115, 1977. Proposes an extremely simple(minded) error recoverymethod for recursive descent parsers: when an error occurs, the parser enters a recovering state. While in

this recovering state, error messages are inhibited. Apart from that, the parser proceeds until it requires adefinite symbol. Then, symbols are skipped until this symbol is found or the end of the input is reached.
Because this method can result in a lot of skipping, some fine-tuning can be applied.

\Gamma  Thomas J. Pennello, Frank DeRemer, "A forward move algorithm for LR error

recovery". In Fifth Annual ACM Symposium on Principles of ProgrammingLanguages, p. 241-254, Jan 1978.

Refer to Graham and Rhodes [ErrHandl 1975].Backward moves are found to be detrimental to error recovery. The extent of the forward move is determined as follows. At the error, an LALR(1) parser is started in a state including all possible items. Thethus extended automaton is run until it wants to reduce past the error detection point. The resulting right
context is used in error correction. An algorithm for the construction of a reasonably sized extendedLALR(1) table is given.

\Gamma  Kuo-Chung Tai, "Syntactic error correction in programming languages", IEEE

Trans. Softw. Eng., vol. SE-4, no. 5, p. 414-425, 1978. Presents a technique for syntacticerror correction called

pattern mapping. Patterns model the editing of the input string at the error detec-tion point. These patterns are constructed by the parser developer. The patterns are sorted by a criterion

called the minimum distance correction with k correct look-ahead symbols, and whenever correction isrequired, the first matching pattern is used. If no such pattern is found, error correction fails and another
error recovery method must be applied.

\Gamma  M. Dennis Mickunas, John A. Modry, "Automatic error recovery for LR

parsers", Commun. ACM, vol. 21, no. 6, p. 459-465, June 1978. When an error isencountered, a set of provisional parsings of the beginning of the rest of the input (so-called

condensations) are constructed: for each state a parsing is attempted and those that survive according to certaincriteria are accepted. This yields a set of target states. Now the stack is "frayed" by partly or completely

undoing any reduces; this yields a set of source states. Attempts are made to connect a source state to atarget state by inserting or deleting tokens. Careful rules are given.

\Gamma  J. Lewi, K. de Vlaminck, J. Huens, M. Huybrechts, "The ELL(1) parser generator

306 Annotated bibliography [Ch. 13

and the error-recovery mechanism", Acta Inform., vol. 10, p. 209-228, 1978.Presents a detailed recursive descent parser generation scheme for ELL(1) grammars, and also presents
an error recovery method based on so-called synchronization triplets (a,b,A). a is a terminal fromFIRST(

A), b is a terminal from LAST(A). The parser operates either in parsing mode or in error mode.It starts in parsing mode, and proceeds until an error occurs. Then, in error mode, symbols are skipped

until either an end-marker b is found where a is the last encountered corresponding begin-marker, inwhich case parsing mode resumes, or a begin-marker

a is found, in which case A is invoked in parsingmode. As soon as
A is accepted, error-mode is resumed. The success of the method depends on carefulselection of synchronization triplets.

\Gamma  G. David Ripley, "A simple recovery-only procedure for simple precedence

parsers", Commun. ACM, vol. 21, no. 11, p. 928-930, Nov 1978. When an error(character-pair, reduction or stackability) is encountered, the error is reported and the contents of the

stack are replaced by the one error symbol ????, which has the relation <* to all other symbols. Then theparser is restarted. Subsequent attempts to reduce across the error symbol just result in a reduction to the
error symbol; no semantic routine is called.

\Gamma  Joachim Ciesinger, "A bibliography of error-handling", ACM SIGPLAN Notices,

vol. 14, no. 1, p. 16-26, Jan 1979. Around 90 literature references from 1963-1978.
\Gamma  C.N. Fischer, K.-C. Tai, D.R. Milton, "Immediate error correction in strong

LL(1) parsers", Inform. Process. Lett., vol. 8, no. 5, p. 261-266, June 1979. Astrong-LL(1) parser will sometimes perform some incorrect parsing actions, connected with e-matches,

when confronted with an erroneous input symbol, before signalling an error; this impedes subsequenterror correction. A subset of the LL(1) grammars is defined, the

nullable LL(1) grammars, in whichrules can only produce e directly, not indirectly. A special routine, called before an e-match is done,

hunts down the stack to see if the input symbol will be matched or predicted by something deeper on thestack; if not, an error is signalled immediately. An algorithm to convert any strong-LL(1) grammar into a
non-nullable strong-LL(1) grammar is given. (See also Mauney and Fischer [ErrHandl 1981]).

\Gamma  Susan L. Graham, Charles B. Haley, William N. Joy, "Practical LR error

recovery", ACM SIGPLAN Notices, vol. 14, no. 8, p. 168-175, Aug 1979. Aconsiderable number of techniques is integrated. First-level error recovery does forward-move, restricting the possibilities to one correction only, using a cost function. The backward move is controlled byerror tokens in the grammar. The second level does panic mode error recovery using "beacon tokens";
disaster is prevented by dividing the grammar into sections (like "declarations" or "statement"), whichthe error recovery will not leave.

\Gamma  Ajit B. Pai, Richard B. Kieburtz, "Global context recovery: a new strategy for

syntactic error recovery by table-driven parsers", ACM Trans. Prog. Lang. Syst.,vol. 2, no. 1, p. 18-41, Jan 1980.

A fiducial symbol is a terminal symbol that has the propertythat if it occurs on the top of the stack of an LL(1) parser, it will to a large degree determine the rest of

the stack. Two more explicit definitions are given, the most practical being: a terminal symbol thatoccurs only once in the grammar, in a rule for a non-terminal that occurs only once in the grammar, etc.
Now, if an error occurs that cannot be repaired locally, the input is discarded until a fiducial symbol zappears. Then the stack is popped until

z, or a non-terminal N that produces z, appears. In the latter case nis "developed" until
z appears. Parsing can now continue. If the stack gets empty in this process, the startsymbol is pushed anew; it will produce

z.The paper starts with a very readable introduction to error recovery and a good local error correction

algorithm.

\Gamma  T. Krawczyk, "Error correction by mutational grammars", Inform. Process. Lett.,

vol. 11, no. 1, p. 9-15, 1980. Discusses an error correction method that automatically extendsa grammar by adding certain mutations of grammar rules, so that input with separator and parenthesis

errors can be corrected, while retaining the LR(k) grammar class. The parser delivers the parsing in theform of a list of grammar rules used; the mutated rules in this list are replaced by their originals.

Sec. 13.11] Error handling 307

\Gamma  Steven Pemberton, "Comments on an error-recovery scheme by Hartmann",

Softw. Pract. Exper., vol. 10, no. 3, p. 231-240, 1980. Error recovery in a recursivedescent parser is done by passing to each parsing routine a set of "acceptable" symbols. Upon encountering an error, the parsing routine will insert any directly required terminals and then skip input until anacceptable symbol is found. Rules are given and refined on what should be in the acceptable set for certain constructs in the grammar.

\Gamma  Johannes Ro"hrich, "Methods for the automatic construction of error correcting

parsers", Acta Inform., vol. 13, no. 2, p. 115-139, Feb 1980. See Section 10.7.3 for adiscussion of this error recovery method. The paper also discusses implementation of this method in

LL(k) and LR(k) parsers, using so-called deterministic continuable stack automata.

\Gamma  Seppo Sippu, Eljas Soisalon-Soininen, "A scheme for LR(k) parsing with error

recovery, Part I: LR(k) parsing/Part II: Error recovery/Part III: Error correction",Intern. J. Comput. Math., vol. A8, p. 27-42/107-119/189-206, 1980.

A thoroughmathematical theory of non-deterministic and deterministic LR(
k)-like parsers (which subsumes SLR(k)and LALR(
k)) is given. These parsers are then extended with error productions such that all errors thatare at least
k tokens apart are corrected. It should be noted that the resulting parsers are almost certainlynon-deterministic.

\Gamma  C.N. Fischer, D.R. Milton, S.B. Quiring, "Efficient LL(1) error correction and

recovery using only insertions", Acta Inform., vol. 13, no. 2, p. 141-154, 1980. SeeSection 10.7.4 for a discussion of this error recovery method.

\Gamma  Kuo-Chung Tai, "Predictors of context-free grammars", SIAM J. Computing, vol.

9, no. 3, p. 653-664, Aug 1980. Author's abstract: "A predictor of a context-free grammar Gis a substring of a sentence in

L (G) which determines unambiguously the contents of the parse stackimmediately before (in top-down parsing) or after (in bottom-up parsing) symbols of the predictor are

processed. Two types of predictors are defined, one for bottom-up parsers, one for top-down parsers.Algorithms for finding predictors are given and the possible applications of predictors are discussed."
Predictors are a great help in error recovery.

\Gamma  C.N. Fischer, J. Mauney, "On the role of error productions in syntactic error

correction", Comput. Lang. (Elmsford, NY), vol. 5, p. 131-139, 1980. Presents anumber of examples in a Pascal parser illustrating the use of error productions in cases where an

automatic error corrector would not find the right continuation. Error productions can be added to thegrammar regardless of the error corrector.

\Gamma  Jon Mauney, Charles N. Fischer, "An improvement to immediate error detection

in strong LL(1) parsers", Inform. Process. Lett., vol. 12, no. 5, p. 211-212, 1981.The technique of Fischer, Tai and Milton [ErrHandl 1979] is extended to all LL(1) grammars by having

the special routine which is called before an e-match is done do conversion to non-nullable on the fly.Linear time dependency is preserved by setting a flag when the test succeeds, clearing it when a symbol
is matched and by not performing the test if the flag is set: this way the test will be done at most once foreach symbol.

\Gamma  Stuart O. Anderson, Roland C. Backhouse, "Locally least-cost error recovery in

Earley's algorithm", ACM Trans. Prog. Lang. Syst., vol. 3, no. 3, p. 318-347, July1981.

Parsing and error recovery are unified so that error-free parsing is zero-cost error recovery.The information already present in the Earley items is utilized cleverly to determine possible continuations. From these and from the input, the locally least-cost error recovery can be calculated, albeit atconsiderable expense. Detailed algorithms are given.

\Gamma  Rodney W. Topor, "A note on error recovery in recursive descent parsers", ACM

SIGPLAN Notices, vol. 17, no. 2, p. 37-40, Feb 1982. Followset error recovery isimplemented in a recursive-descent parser by having one parse-and-error-recovery routine which is

308 Annotated bibliography [Ch. 13
passed the actual routine for a rule, its FIRST set and its FOLLOWS set. This reduces the size of theparser considerably and prevents clerical errors in hand-written parsers. Also see subsequent letter by
C.B. Boyd, vol. 17, no. 8, p. 101-103.

\Gamma  Michael G. Burke, Gerald A. Fisher, "A practical method for syntactic error diagnosis and repair", ACM SIGPLAN Notices, vol. 17, no. 6, p. 67-78, June 1982. SeeBurke and Fisher [ErrHandl 1987].

\Gamma  Jon Mauney, Charles N. Fischer, "A forward-move algorithm for LL and LR

parsers", ACM SIGPLAN Notices, vol. 17, no. 6, p. 79-87, June 1982. Upon findingan error, a Graham, Harrison and Ruzzo general CF parser [CF 1980] is started to do a forward move

analysis using cost functions. The general CF parser is run over a restricted piece of the input, allowing
regional least-cost error correction.

\Gamma  F. Jalili, J.H. Gallier, "Building friendly parsers". In 9th Annual ACM Symposium

on Principles of Programming Languages, ACM, New York, p. 196-206, 1982. Aninteractive LALR(1) parser is described that uses forward move error recovery to better prompt the user

with possible corrections. The interactions of the interactive parsing and the forward move algorithm aredescribed in fairly great detail.

\Gamma  S.O. Anderson, R.C. Backhouse, "An alternative implementation of an insertiononly recovery technique", Acta Inform., vol. 18, p. 289-298, 1982. Argues that theFMQ error corrector of Fischer, Milton and Quiring [ErrHandl 1980] does not have to compute a complete insertion. It is sufficient to compute the first symbol. If w = w 1w 2 . . . wn is an optimal insertion forthe error

a following prefix u, then w 2 . . . wn is an optimal insertion for the error a following prefix uw 1.Also, immediate error detection is not necessary. Instead, the error corrector is called for every symbol,

and returns an empty insertion if the symbol is correct.

\Gamma  S.O. Anderson, R.C. Backhouse, E.H. Bugge, C.P. Stirling, "An assessment of

locally least-cost error recovery", Computer J., vol. 26, no. 1, p. 15-24, 1983.Locally least-cost error recovery consists of a mechanism for editing the next input symbol at least cost,

where the cost of each edit operation is determined by the parser developer. The method is compared toWirth's followset method (see Stirling [ErrHandl 1985]) and compares favorably.

\Gamma  Seppo Sippu, Eljas Soisalon-Soininen, "A syntax-error-handling technique and its

experimental analysis", ACM Trans. Prog. Lang. Syst., vol. 5, no. 4, p. 656-679,Oct 1983.

Phrase level error recovery replaces the top m elements from the stack and the next ninput tokens by a single non-terminal such that parsing can continue. The authors explore various search

sequences to determine the values of m and n. Local error recovery can be incorporated by introducingfor each terminal

tt a new production rule TTeerrmm__tt -->> EEmmppttyy tt, and having a production rule EEmmppttyy
-->> ee. This allows both the correction of a phrase (n=0,m=0) to TTeerrmm__tt (i.e. insertion of tt) and of aphrase (

n,m) to EEmmppttyy (i.e. deletion of (n,m)). Experimental results are given.

\Gamma  K. Hammond, V.J. Rayward-Smith, "A survey on syntactic error recovery and

repair", Comput. Lang. (Elmsford, NY), vol. 9, no. 1, p. 51-68, 1984. Divides theerror recovery schemes into three classes:

1. local recovery schemes, such as "panic mode", the followset method, the FMQ method (seeFischer, Milton and Quiring [ErrHandl 1980]), LaFrance's pattern matching method (see LaFrance

[ErrHandl 1970]), and Backhouse's locally least-cost method (see Backhouse et al. [ErrHandl1983]);
2. regional error recovery schemes, such as forward/backward move (see for instance Graham andRodhes [ErrHandl 1975]); and
3. global error recovery schemes, such as global minimum distance error recovery (see for instanceAho and Peterson [ErrHandl 1972] and Lyon [ErrHandl 1974]), and mutational grammars (see for

instance Krawczyk [ErrHandl 1980]).The paper summarizes the advantages and disadvantages of each method.

Sec. 13.11] Error handling 309

\Gamma  Michael Spenke, Heinz Mu"hlenbein, Monika Mevenkamp, Friedemann Mattern,

Christian Beilken, "A language-independent error recovery method for LL(1)parsers", Softw. Pract. Exper., vol. 14, no. 11, p. 1095-1107, Nov 1984.

Presents anerror recovery method using deletions and insertions. The choice between different possible corrections

is made by comparing the cost of the insertion with the reliability of the symbol. A correction is plausibleif the reliability of the first non-skipped symbol is larger than the insert-cost of the insertion. The correction is selected among the plausible corrections, such that the fewest symbols are skipped. Reliabilityand insert-cost of each symbol are tunable.

\Gamma  Colin P. Stirling, "Follow set error recovery", Softw. Pract. Exper., vol. 15, no. 3,

p. 239-257, March 1985. Describes the followset technique for error recovery: at all timesthere is a set of symbols that depends on the parse stack and that will not be skipped, called the

followset.When an error occurs, symbols are skipped until one is found that is a member of this set. Then, symbols

are inserted and/or the parser state is adapted until this symbol is legal. In fact there is a family of errorrecovery (correction) methods that differ in the way the followset is determined. The paper compares
several of these methods.

\Gamma  Pyda Srisuresh, Michael J. Eager, "A portable syntactic error recovery scheme

for LR(1) parsers". In Proc. 1985 ACM Comput. Sc. Conf., W.D. Dominick (eds.),ACM, New Orleans, p. 390-399, March 1985.

Presents a detailed account of theimplementation of an error recovery scheme that works at four levels, each one of a more global nature.

The first and the second level are local, attempting to recover from the error by editing the symbol infront of the error detection point and the error symbol itself. The third level uses error tokens, and the
last level is panic mode.

\Gamma  Helmut Richter, "Noncorrecting syntax error recovery", ACM Trans. Prog. Lang.

Syst., vol. 7, no. 3, p. 478-489, July 1985. See Section 10.8 for a discussion of this method.The errors can be pinpointed better by parsing backwards from the error detection point, using a reverse

grammar until again an error is found. The actual error must be in the indicated interval. Bounded-context grammars are conjectured to yield deterministic suffix-grammars.

\Gamma  Kwang-Moo Choe, Chun-Hyon Chang, "Efficient computation of the locally

least-cost insertion string for the LR error repair", Inform. Process. Lett., vol. 23,no. 6, p. 311-316, 1986.

Refer to Anderson, Backhouse, Bugge and Stirling [ErrHandl 1983]for locally least-cost error correction. The paper presents an efficient implementation in LR parsers,

using a formalism described by Park, Choe and Chang [LR 1985].

\Gamma  Tudor Ba* la* nescu, Serban Gavrila*, Marian Gheorghe, Radu Nicolescu, Liviu

Sofonea, "On Hartman's error recovery scheme", ACM SIGPLAN Notices, vol.21, no. 12, p. 80-86, Dec 1986.

More and tighter acceptable-sets for more grammarconstructions; see Pemberton [ErrHandl 1980].

\Gamma  Michael G. Burke, Gerald A. Fisher, "A practical method for LL and LR syntactic error diagnosis and recovery", ACM Trans. Prog. Lang. Syst., vol. 9, no. 2, p.164-197, April 1987.

Traditional error recovery assumes that all tokens up to the error symbolare correct. The article investigates the option of allowing earlier tokens to be modified. To this end,

parsing is done with two parsers, one of which is a number of tokens ahead of the other. The first parserdoes no actions and keeps enough administration to be rolled back, and the second performs the semantic
actions; the first parser will modify the input stream or stack so that the second parser will never see anerror. This device is combined with three error repair strategies: single token recovery, scope recovery
and secondary recovery. In single token recovery, the parser is rolled back and single tokens are deleted,inserted or replaced by tokens specified by the parser writer. In scope recovery, closers as specified by
the parser writer are inserted before the error symbol. In secondary recovery, sequences of tokensaround the error symbol are discarded. In each case, a recovery is accepted if it allows the parser to
advance a specified number of tokens beyond the error symbol. It is reported that this techniques

310 Annotated bibliography [Ch. 13
corrects three quarters of the normal errors in Pascal programs in the same way as a knowledgeablehuman would. The effects of fine-tuning are discussed.

\Gamma  Jon Mauney, Charles N. Fischer, "Determining the extent of lookahead in syntactic error repair", ACM Trans. Prog. Lang. Syst., vol. 10, no. 3, p. 456-469, July1988.

A correction of an error can be validated by trying it and parsing on until a symbol is foundwith the so-called Moderate Phrase Level Uniqueness. Once such a symbol is found, all minimal corrections of the error are equivalent in the sense that after this MPLU symbol, the acceptable suffixes will beidentical. Measurements indicate that in Pascal the distance between two such symbols is fairly short,
for the most part.

\Gamma  Gordon V. Cormack, "An LR substring parser for noncorrecting syntax error

recovery", ACM SIGPLAN Notices, vol. 24, no. 7, p. 161-169, July 1989. Presents amethod to produce an LR parser for the substrings of a language described by a bounded-context(1,1)

grammar, thereby confirming Richter's [ErrHandl 1985] conjecture that this can be done for BC gram-mars. The resulting parser is about twice as large as an ordinary LR parser.

13.12 TRANSFORMATIONS ON GRAMMARS

\Gamma  J.M. Foster, "A syntax-improving program", Computer J., vol. 11, no. 1, p. 31-34,

May 1968. The parser generator SID (Syntax Improving Device) attempts to remove LL(1)conflicts by eliminating left-recursion, and then left-factoring, combined with inline substitution. If this

succeeds, SID generates a parser in machine language.

\Gamma  Kenichi Taniguchi, Tadao Kasami, "Reduction of context-free grammars",

Inform. Control, vol. 17, p. 92-108, 1970. Considers algorithms to reduce or minimize thenumber of non-terminals in a grammar.

\Gamma  M.D. Mickunas, R.L. Lancaster, V.B. Schneider, "Transforming LR(k) grammars

to LR(1), SLR(1) and (1,1) bounded right-context grammars", J. ACM, vol. 23,no. 3, p. 511-533, July 1976.

The required look-ahead of k tokens is reduced to k -1 byincorporating the first token of the look-ahead into the non-terminal; this requires considerable care. The

process can be repeated until k =1 for all LR(k) grammars and even until k =0 for some grammars.

\Gamma  D.J. Rosenkrantz, H.B. Hunt, "Efficient algorithms for automatic construction

and compactification of parsing grammars", ACM Trans. Prog. Lang. Syst., vol. 9,no. 4, p. 543-566, Oct 1987.

Many grammar types are defined by the absence of certainconflicts: LL(1), LR(1), operator-precedence, etc. A simple algorithm is given to modify a given grammar to avoid such conflicts. Modification is restricted to the merging of non-terminals and possibly themerging of terminals; semantic ambiguity thus introduced will have to be cleared up by later inspection.
Proofs of correctness and applicability of the algorithm are given. The maximal merging of terminalswhile avoiding conflicts is also used to reduce grammar size.

13.13 GENERAL BOOKS ON PARSING

\Gamma  Peter Zilany Ingerman, A Syntax-Oriented Translator, Academic Press, New

York, p. 132, 1966. Readable and realistic (for that time) advice for DIY compilerconstruction, in archaic terminology. Uses a backtracking LC parser improved by FIRST sets.

\Gamma  William M. McKeeman, James J. Horning, David B. Wortman, A Compiler Generator, Prentice Hall, Englewood Cliffs, N.J., p. 527, 1970. Good explanation ofprecedence and mixed-strategy parsing. Full application to the XPL compiler.

\Gamma  Alfred V. Aho, Jeffrey D. Ullman, The Theory of Parsing, Translation and

Sec. 13.13] General books on parsing 311

Compiling, Volume I: Parsing, Prentice Hall, Englewood Cliffs, N.J., p. 542,1972.

The book describes the parts of formal languages and automata theory relevant to parsing ina strict mathematical fashion. Since much of the pertinent theory of parsing had already been developed

in 1972, the book is still reasonably up to date and is a veritable trove of definitions, theorems, lemmataand proofs.
The required mathematical apparatus is first introduced, followed by a survey of compiler constructionand by properties of formal languages. The rest of the book confines itself to CF and regular languages.
General parsing methods are treated in full: backtracking top-down and bottom-up, CYK and Earley.Directional non-backtracking methods are explained in detail, including general LL(

k), LC(k) and LR(k),precedence parsing and various other approaches. A last chapter treats several non-grammatical methods

for language specification and parsing.Many practical matters concerning parser construction are treated in volume II, where the theoretical
aspects of practical parser construction are covered; recursive descent is not mentioned, though.

\Gamma  Frederick W. Weingarten, Translation of Computer Languages, Holden-Day, San

Francisco, Calif., p. 180, 1973. Describes some parsing techniques in an clear and easy style.The coverage of subjects is rather eclectic. A full backtracking top-down parser for e-free non-leftrecursive grammars and a full backtracking bottom-up parser for e-free grammars are described. Theauthor does not explicitly forbid e-rules, but his internal representation of grammar rules cannot represent
them. The Earley parser is described well, with an elaborate example. For linear-time parsers, bounded-context and precedence are treated; a table-construction algorithm is given for precedence but not for
bounded-context. LR(k) is vaguely mentioned, LL(k) not at all. Good additional reading. Contains manyalgorithms and flowcharts similar to Cohen and Gotlieb [Misc 1970].

\Gamma  R.C. Gonzales, M.G. Thomason, Syntactic Pattern Recognition, Addison-Wesley,

Reading, Mass., p. 283, 1978. This book provides numerous examples of syntacticdescriptions of objects not normally considered subject to a syntax. Examples range from simple segmented closed curves, trees and shapes of letters, via bubble chamber events, electronic networks, andstructural formulas of rubber molecules to snow flakes, ECGs, and fingerprints. Special attention is paid
to grammars for non-linear objects, for instance web grammars, plex grammars and shape grammars. Aconsiderable amount of formal language theory is covered. All serious parsing is done using the CYK
algorithm; Earley, LL(k) and LR(k) are not mentioned. Operator-precedence, simple precedence and fin-ite automata are occasionally used. The authors are wrong in claiming that an all-empty row in the CYK
recognition matrix signals an error in the input.Interesting chapters about

stochastic grammars, i.e. grammars with probabilities attached to the produc-tion rules, and about
grammatical inference, i.e. methods to derive a reasonable grammar that will pro-duce all sentences in a representative set

R + and will not produce the sentences in a counterexample set
R -.

\Gamma  John E. Hopcroft, Jeffrey D. Ullman, Introduction to Automata Theory,

Languages, and Computation, Addison-Wesley, Reading, Massachussetts, p. 418,1979.

A must for readers interested in formal language theory and computational(im)possibilities.

\Gamma  Roland C. Backhouse, Syntax of Programming Languages, Prentice Hall, London, p. 290, 1979. Grammars are considered in depth, as far as they are relevant toprogramming languages. FS automata and the parsing techniques LL and LR are treated in detail, and

supported by lots of well-explained math. Often complete and efficient algorithms are given in Pascal.Much attention is paid to error recovery and repair, especially to least-cost repairs and locally optimal
repairs. Definitely recommended for further reading.

\Gamma  A.J.T. Davie, R. Morisson, Recursive Descent Compiling, Ellis Horwood Ltd.,

Chichester, p. 195, 1981. Well-balanced description of the design considerations that go into arecursive descent compiler; uses the St. Andrews University S-algol compiler as a running example.

\Gamma  V.J. Rayward-Smith, A First Course in Formal Languages, Blackwell Scientific,

312 Annotated bibliography [Ch. 13

Oxford, p. 123, 1983. Very useful intermediate between Re've'sz [Books 1985] and Hopcroftand Ullman [Books 1979]. Quite readable (the subject permitting); simple examples; broad coverage. No
treatment of LALR, no bibliography.

\Gamma  Gyo"rgy E. Re've'sz, Introduction to Formal Languages, McGraw-Hill, Singapore,

p. 199, 1985. This nifty little book contains many results and elementary proofs of formallanguages, without being "difficult". It gives a description of the ins and outs of the Chomsky hierarchy,

automata, decidability and complexity of context-free language recognition, including the hardest CF
language. Parsing is discussed, with descriptions of the Earley, LL(k) and LR(k) algorithms, each in afew pages.

\Gamma  William A. Barrett, Rodney M. Bates, David A. Gustafson, John D. Couch, Compiler Construction: Theory and Practice, Science Research Associates, Chicago,p. 504, 1986.

A considerable part (about 50%) of the book is concerned with parsing; formallanguage theory, finite-state automata, top-down en bottom-up parsing and error recovery are covered in

very readable chapters. Only those theorems are treated that relate directly to actual parsing; proofs arequite understandable. The book ends with an annotated bibliography of almost 200 entries, on parsing
and other aspects of compiler construction.

\Gamma  A.V. Aho, R. Sethi, J.D. Ullman, Compilers: Principles, Techniques and Tools,

Addison-Wesley, Reading, Mass., p. 796, 1986. The "Red Dragon Book". Excellent,UNIX-oriented treatment of compiler construction. Even treatment of the various aspects.

\Gamma  Anton Nijholt, Computers and Languages: Theory and Practice, Studies in Computer Science and Artificial Intelligence, 4, North-Holland, Amsterdam, p. 482,1988.

Treats in narrative form computers, natural and computer languages, and artificialintelligence, their essentials, history and interrelationships; for the sophisticated layperson. The account

is interspersed with highly critical assessments of the influence of the military on computers and artificialintelligence. Much global information, little technical detail; treats parsing in breadth but not in depth.

13.14 SOME BOOKS ON COMPUTER SCIENCE

\Gamma  David Harel, Algorithms: The Spirit of Computing, Addison-Wesley, Reading,

Mass, p. 425, 1987. Excellent introduction to the fundamentals of computer science for thesophisticated reader.

\Gamma  Robert Sedgewick, Algorithms, Addison-Wesley, Reading, Mass., p. 657, 1988.
Comprehensive, understandable treatment of many algorithms, beautifully done.

\Gamma  Jeffrey D. Smith, Design and Analysis of Algorithms, PWS-Kent Publ. Comp.,

Boston, p. 447, 1989. Good introductory book, treating list handling, searching, breadth-firstand depth-first search, dynamic programming, etc., etc.

Author index
Page numbers in roman indicate pages referring to the author; page numbers in italicrefer to annotations of works by the author.
Abrahams, P.W., 280Ackley, S.I.,

299Agrawal, R.,

288Aho, A.V., 42, 118, 214, 219,

236, 267, 269, 283, 284,288,

295, 296, 296, 299,
303, 304, 308, 310, 312Al-Hussaini, A.M.M.,

289Al-Hussainin, A.M.M., 204

Ancona, M., 287, 288Anderson, S.O., 246,

307, 308,309

Anderson, T., 214, 283Aoe, J.,

298Auty, D.,

281

Backhouse, R.C., 246, 284,

307, 308, 308, 309, 311Backus, J.W.,

266Baker, T.P.,
287, 289Barrett, W.A.,

312Barth, G., 42,
270Bates, R.M.,
312Bauer, F.L.,
266, 294Beilken, C.,
309Bell, J.R., 192,

295, 296Bermudez, M.E., 221,

289,
290, 291Bertsch, E., 194,

270, 297Birman, A.,
267Blank, G.D., 108,

301, 302,302

Bochmann, G.V., 267Bolc, L.,

302

Bossi, A., 278Bouckaert, M., 161,

276, 278Bratley, P., 108,
300Brooker, R.A.,
266Brown, C.A.,
287Ba*la*nescu, T.,

309Bugge, E.H., 246,

308, 309Burgess, C.J.,
287, 288, 288,
303Burke, M.G.,

308, 308, 309Burshteyn, B.,

269

Carter, L.R., 181, 268Celentano, A.,

285, 287, 288Chang, C.-H., 221, 246,

289,289, 290,
309, 309Chapman, N.P., 220,

289, 290Chartres, B.A.,
275Cherry, L.L., 219,

267Chester, D.,
294, 300Cho, Y.E.,
292Choe, K.-M., 214, 221, 246,

289, 289, 290, 290, 309,309
Chomsky, N., 24, 51, 265Ciesinger, J.,

304, 306Cleaveland, J.C., 45,

272Cocco, N.,
278Cohen, D.J.,

266, 311Cohen, J., 143,

267, 273, 278,
281Cohen, R., 221,

283Colmerauer, A., 227,

295Colussi, L.,
278

Conway, M.E., 266, 303Conway, R.W., 237,

303Corasick, M.J., 118,
299Cormack, G.V., 227, 248,

291,
292, 310Couch, J.D.,

312Crowe, D.,
272, 296
C^ ulik, K., 221, 279, 283

Dahl, V., 301, 302Davie, A.J.T.,

311

de Vlaminck, K., 280, 305
Degano, P., 221, 290Demers, A.J.,

284, 293Demner, J.,
283, 293Dencker, P., 112, 204,

268Denning, P.J.,
295DeRemer, F.L., 218, 221, 240,

283, 288, 289, 299, 305Detro, K.D.,

288Deussen, P., 79,

267, 272Dewar, H.P., 108,

300Dieterich, E.-W.,
305Dobler, H.,
282Dodero, G., 287,

288Do"mo"lki, B., 270,

275Du"rre, K., 112, 204,

268Dwyer, B.,
281

Eager, M.J., 309Earley, J., 15, 149, 206,

267,
276

314 Author index
Eickel, J., 294Er, M.C.,

298Eve, J., 214,

283Ezure, K.,
278

Feldman, J., 266Feyock, S.,

304Finn, G.D.,

268Fischer, C.N., 244, 281,

306,306,
307, 307, 308, 308,
310Fisher, A.J., 72,

273Fisher, G.A.,
286, 308, 308,
309Fisker, R.G.,

271Florentin, J.J.,

275Floyd, R.W.,
266, 294, 295Foster, J.M., 179, 267, 280,

310Fu, K.S.,

304Fujiwara, F.,

288, 294

Gallier, J.H., 308Gavrila*, S.,

309Gazdar, G.,

301, 301Geller, M.M.,

283, 285, 293Gerardy, R.,
269Ghandour, Z.J.,

270Gheorghe, M.,
309Ghezzi, C., 285,

286, 286, 303Gianuzzi, V., 287,

288Glushkova, V.N.,
281Gonzales, R.C.,
311Goshawke, W.,
302Gotlieb, C.C.,
266, 311Gough, K.J.,
281, 281Graham, S.L., 156, 159, 163,

233, 238, 250, 277, 278,
283, 296, 298, 303, 305,
306, 308Gray, J.N.,

296Green, J.,
266Greibach, S.A., 45, 127,

272,
274Gries, D.,

266, 304Griffiths, M.,

280Griffiths, T.V., 79,

274, 275Grosch, J.,
269Grosz, B.J., 275, 276,

302Grune, D., 49, 179, 183,

273,
282Gustafson, D.A.,

312

Haley, C.B., 306Hammer, M.,

293

Hammond, K., 308Hanson, D.R.,

282Harel, D.,
312Harris, L.A.,

271Harris, M.D.,

301Harrison, M.A., 156, 159, 163,

250, 277, 278, 283, 285,293,

296, 308Hayes, P.J.,

300Heckmann, R., 183,

282Heering, J., 221,
291Heilbrunner, S., 220,

280, 286,
287, 289Henderson, D.S.,

297, 298Heuft, J., 112, 204,

268Heuring, V.P.,
299Hext, J.B., 148,

276, 276Hickey, T.J., 143,

273, 278Hirakawa, H.,
294Hirsh, S.,
294Hoffman, H.-J.,

284Hopcroft, J.E., 22, 30, 71, 95,

122, 228, 311, 312Horning, J.J., 214,

283, 284,
310Horspool, R.N., 221,

291, 298Huens, J.,
280, 305Hunt, H.B., 179,

284, 310Hunter, R.B.,
267Huybrechts, M.,

280, 305

Ichbiah, J., 197, 295Ikeda, M.,

278Ingerman, P.Z.,

310Inoue, K.,
288, 294Irons, E.T., 266,

273, 274, 274Ives, F., 221,
289, 290, 290

Jacobs, C.J.H., 179, 183, 282Jalili, F.,

308James, 288

James, E.G., 303James, L.,

287, 288Jarzabek, S.,

280Johnson, S.C., 214, 219,

267,
283, 285Johnson, W.L.,

299Joliat, M.L.,
285Jones, D.W.,

300Jones, N.D.,
270Joy, W.N.,
306

Kaminger, F.P., 276Kanze, J.,

291Kasami, T., 75,

276, 310

Kastens, U., 290Katz, C.,

266Kelly, I.D.K.,

302Kemp, R.,
287Kernighan, B.W., 219,

267Khabbaz, N.A.,
270Kieburtz, R.B.,
306King, M.,
301Kirchhoff, L.W., 179,

281Klein, E.,
301Klenk, U.,

300Klint, P., 221,

291Knuth, D.E., 206, 213,

280,
282Komor, T.,

280Korenjak, A.J., 213,

282, 283,290

Koskimies, K., 286Koster, C.H.A., 42,

271, 272,
277Kra'l, J.,

277, 283, 293, 293Krawczyk, T.,

280, 306, 308Krishnamurthy, M.S.,

297,297

Kristensen, B.B., 284, 287,289
Kron, H.H., 284Kruseman Aretz, F.E.J., 221,

291, 291Krzemien', R., 107,

299Kuno, S.,
274, 275, 275Kurki-Suonio, R., 181,

279

LaFrance, J.E., 302, 308LaLonde, W.R., 220,

285,285,
286, 287, 287, 298Lancaster, R.L., 212,

310Lang, B.,
276Langendoen, D.,

299, 300Langmaack, H.,
283Lazarus, P.,
304Learner, A.,

295Ledgard, H.F.,

267Ledley, R.S.,
266Lee, E.S.,
270, 271Lesk, M.E., 39,

299Le'vy, J.-P.,
304Levy, M.R., 191,

297, 297,
298, 298Lewi, J., 183,

280, 305Lewis II, P.M.,

279, 292Lim, A.L.,
295Lindsey, C.H.,

271Loeckx, J., 198,

270, 295Logothetis, G., 221,

291

Author index 315
Lomet, D.B., 292, 293Lu, S.Y.,

304L/ukasiewicz, A., 107,

299Lyon, G., 236,
303, 304, 308

MacCallum, I.R., 266Madsen, O.L.,

284, 287, 289Mailloux, B.J.,

271Makinouchi, A.,

286Manacher, G.K.,

277Mandrioli, D., 285,

286, 286Mannucci, S., 221,
290Marcotty, M.,
267Martin, D.F.,
295, 296Matsumoto, Y.,

294Mattern, F.,
309Mauney, J., 306,

307, 308, 310Mayer, O.,
286McAfee, J.,

296McCarthy, J.,

266McGettrick, A.D.,

267McKeeman, W.M., 198,

310Meersman, R., 45,
272Meertens, L.G.L.T.,

271Mehlhorn, K.,
270Mevenkamp, M.,

309Mickunas, M.D., 212, 240,

296, 305, 310Milton, D.R., 179, 244,

281,281,
306, 307, 307, 308Miyoshi, H.,

294Modry, J.A., 240,

305Mojana, B., 221,
290Moll, K.R.,
298Morisson, R.,

311Morris, D.,
266Morse, S., 197,

295Mouradian, G.V.,

300Mu"hlenbein, H.,
309Mukai, K.,
283Musinski, J.E.,

304

Nakata, I., 220, 289, 290Naur, P.,

266Ng, R.,
300Nicolescu, R.,

309Nijholt, A., 79,

268, 280, 281,
293, 312

Ostrand, T.J., 113, 299
Pagan, F.G., 269Pager, D.,

282, 284, 285, 286,287

Pai, A.B., 306

Park, J.C.H., 214, 221, 288,

289, 289, 290, 290, 309Partridge, D.P.,

303Patel, R.,
267Paul, M.,
294Paull, M.C., 113,

299Peck, J.E.L.,
271Pemberton, S., 241,

307, 309Pennello, T.J., 218, 221, 240,

288, 289, 289, 291, 305Pereira, F.C.N.,

268Perlis, A.J.,
266Peterson, T.G., 236,

303, 304,308

Petrick, S.R., 79, 274, 275Pezaris, S.D.,

292Pirklbauer, K.,

282Pirotte, A., 161,

276, 278Pittl, J.,
293, 294, 294Poonen, G.,

304Poplawski, D.A.,

281Porter, J.S.,
299Presser, L.,
296Pullum, G.,

301Purdom, P.W.,

284, 287

Quiring, S.B., 244, 281, 307,308

Rabin, M.O., 299Ramesha Chandra, H.R.,

297,297

Rayward-Smith, V.J., 22, 308,

311Reed, J.H.,

302Rekers, J., 221,

291Re've'sz, G.E., 25, 27, 30, 71,

312, 312Rhodes, S.P., 233, 238,

303,305, 308

Richter, H., 247, 309, 310Ripley, G.D.,

306Rivieres, J. des,

298Roberts, G.H., 221,

291, 291,
292, 292Roberts, P.S., 148,

276, 276Rohl, J.S.,
266Ro"hrich, J., 241, 244,

307Rosenkrantz, D.J., 179,

280,
292, 310Ross, D.T.,

299Roth, M.S.,

267Rowland, B.R., 179,

281Rozenberg, G., 45,
272Rutishauser, H.,
266

Ruz^ic^ka, P., 297Ruzzo, W.L., 156, 159, 163,

250, 278, 308
Sag, I., 301Sager, T.J.,

281Saint-Dizier, P.,

301, 302Sakai, I., 75, 88,

273Salomon, D.J., 227,

291, 292Samelson, K.,
266, 294Samet, H.,
268Sassa, M., 220,

289, 290Schimpf, K.M.,

289, 290Schmidt, E., 39,

299Schmitz, L.,
289Schneider, V.B., 212,

296, 310Scott, D.,
299Sebesta, R.W.,

270Sedgewick, R., 75, 118, 265,

312Sekimoto, S., 197,

283, 295Sethi, R., 214,
312Setzer, V.W.,
280Shannon, A.,
287Share, M., 219,

269Sheil, B., 75, 88, 104,

277Shieber, S.M.,
301Shimada, R.,
298Shyamasundar, R.K.,

297, 297Sintzoff, M., 45,
271Sippu, S.,
281, 281, 305, 307,
308Sitver, R.,

281Smith, J.D., 74, 265,

312Smith, W.B.,
302Snelling, M., 161,

276, 278Snelting, G.,
292Sofonea, L.,
309Soisalon-Soininen, E.,

281,281, 283,
286, 288, 290,
293, 305, 307, 308Sparck Jones, K., 275, 276,

301, 302Spector, D.,

287, 287, 291Spenke, M.,

309Srisuresh, P.,

309Stearns, R.E.,

279, 280Stirling, C.P., 241, 246,

308,308,
309, 309Stone, R.G., 204,

289Sudborough, I.H.,

297Sudo, M.,
283Szafron, D.,

300Szymanski, T.G.,

284, 285

316 Author index
Tai, K.-C., 227, 244, 267, 286,

305, 306, 307, 307Takeuchi, Y.,

282Tanaka, E.,
278Tanaka, H.,

294Taniguchi, K.,

310Tarhio, J.,
288, 290Tarjan, R.E.,

268Tennant, H.,
300Terrine, G.,
296Thomason, M.G.,

311Thompson, K., 115,

299Thorne, J.P., 108,
300Tokuda, T.,
287, 288Tomita, M., 79, 153, 157, 222,

276, 278, 279, 279Topor, R.W.,

307Torii, K.,
276Turnbull, C.J.M.,

270, 271Turner, D.A.,
305

Ukkonen, E., 211, 268, 289,

293, 294Ullman, J.D., 22, 30, 71, 95,

122, 214, 219, 228, 267,
283, 284, 284, 288, 295,
296, 296, 310, 311, 312,312

Unger, S.H., 75, 82, 85, 275Uzgalis, R.C., 45,

272

Valiant, L., 76, 277
van Wijngaarden, A., 266,

271, 272

Vauquois, B., 266Vol'dman, G.Sh.,

271

Waddle, V.E., 269Wagner, R.A.,

303Waite, W.M., 181,

268Walters, D.A.,
270Warren, D.H.D.,

268Watt, D.A.,
272Webber, B.L., 275, 276,

302Weber, H.,
295Weber, M.,

286Wegner, L.M.,

272, 273, 273Wegstein, J.H.,

266, 294Weingarten, F.W.,

311Wetherell, C.,
287, 305Weyuker, E.J., 113,

299Wharton, R.M.,
267Wigg, J.D.,
302

Wijngaarden, A. van, 42, 45Wilcox, T.R., 237,

303Wilks, Y.,
301Williams, J.H.,

285, 297Williams, M.H., 191,

297,
298, 298Wilson, J.B.,

266Winkler, G.,
284Wirth, N.,
267, 295Wise, D.S.,

283, 296Witaszek, J.,

286, 286, 290Wolpe, H.,
294Wood, D.,
279, 280, 280Woodger, M.,

266Woods, W.A.,

269, 300Workman, D.A.,

298Wortman, D.B.,
310Wyrostek, P.,
298

Yamamoto, Y., 298Yao, A.C.-C.,

268Yasukawa, H.,

294Yeh, D.,
288, 290Yehudai, A.,

298Yoshida, K.,
282Younger, D.H., 75, 88,

275

Zimmer, R., 296

Index
Page numbers in bold refer to pages where a definition of the indicated term can befound. Two page numbers separated by a dash describe a range of pages on each of
which the term is used; page numbers separated by two dots indicate intermittent use ofthe term over the range.

acceptable-set, 240, 241-244, 304, 309accepting state, 110..113, 117-118, 122,

200..205, 210, 223, 304accessible non-terminal, 97
ad hoc error recovery, 237affix, 46, 271-272
affix grammar, 42, 46, 265, 269..272Aho and Corasick bibliographic search algorithm, 118, 299Algol 60, 35, 266, 273, 280, 294-295
Algol 68, 136, 267, 271alphabet, 17, 20-21, 45, 276, 304
alternative, 25ambiguity, 62, 63-64, 93, 110, 113, 136, 153,

156-157, 219, 222, 227-228, 236,248..251, 267, 277-278, 288, 291, 299,
302, 310ambiguity rate, 275
ambiguity test, 62, 228analysis stack, 123, 125, 130-131
angle brackets, 35, 45, 273attribute, 58, 59, 180, 252, 266, 268, 273, 281
attribute grammar, 46, 58-59, 179, 267, 272, 290
backtracking, 75, 76, 79, 130..147, 180, 232,266-267, 273..279, 300-302, 311
Backus-Naur Form, 35, 45, 266backward move, 238, 239-240, 305..308
beacon token, 304, 306bitvector, 276
blind alley, 26, 48, 56

BNF, see Backus-Naur Formbottom-up parsing, 65
bounded-context, 79, 198, 199, 204-205, 232,248, 272, 294..298, 309-311
bounded-context parsable, 297Boyer-Moore algorithm, 118
bracketed grammar, 272breadth-first production, 48, 273
breadth-first search, 75, 76, 79, 124, 148, 164,200, 222-223, 226, 250, 276, 284, 304,

312bypassed LR(

k) parser, 288

C, 137, 285, 292Cantor, 22
cascaded grammar, 300category (GPSG), 301
chain rule, 89, 220chain-independent, 293
channel algorithm, 214, 220characteristic parsing, 285, 293
character-pair error, 232, 306chart, 105
chart parsing, 105Chomsky hierarchy, 28, 40, 51, 53, 266, 272,

312Chomsky Normal Form, 92, 94, 98..104, 266,
273, 276-277chromosome recognition, 11, 278
closure, 289, 295CNF,

see Chomsky Normal Form

318 Index
Cocke-Younger-Kasami, see CYK parsercondensation phase, 238, 305
conflict resolver, 179, 180..182, 219, 280, 282context-free grammar, 32
context-sensitive grammar, 29, 32, 53, 60, 72,249, 266, 270-271
continuation, 241, 242-244, 307continuation grammar, 242, 243
control mechanism, 68..71, 77-79, 186copy-symbol, 272
coroutine, 266, 268correction phase, 238, 239
correct-prefix property, 175, 229-232, 241, 294,298
counter language, 266CRLL(1), 303
cubic time dependency, 70, 76-77, 149, 155,157, 200, 233, 250, 275-278
CYK parser, 15, 57, 69, 75, 79, 81, 88, 89..92,99, 102, 104, 153..157, 227, 231, 233,

236, 260, 273, 276-278, 302, 311
definite clause, 15, 79, 139, 141, 143, 253, 268depth-first search, 75, 79, 83, 124, 142, 164, 253,

260, 312derived affix, 271
derived attribute, 58derived information, 58
derived metanotion, 273deterministic automaton, 77, 78, 111, 122, 203,

205, 208, 210, 216, 219, 231, 284-285deterministic language, 212, 267, 270, 297
deterministic parser, 15, 57, 164, 167-168, 172,174, 213, 232, 268, 276, 294, 307
Deterministic Regular Parsable, 270directed production analyser, 274
directional parsing method, 74disambiguating rule, 179, 267, 285
document conversion, 11, 219, 268-269don't-care entry, 268, 283
dot, 134, 151, 201, 284-285, 303DRP grammar, 270

Earley deduction, 268Earley item, 151, 152, 201, 236, 303, 307
Earley parser, 15, 69, 76, 79, 149..161, 200-201,206, 227, 232, 236, 246, 250, 268..278,

301..307, 311-312Earley set, 155, 162
eliminating e-rules, 94-95, 101, 128eliminating left recursion, 128-129, 178, 280,

310eliminating unit rules, 94, 96, 102, 128, 220
empty word, 20

end-marker, 23, 29, 125, 130, 137, 165, 172,181, 187, 205, 242
error correction, 230, 234, 244, 246, 281-282,303-309
error detection, 173, 175, 192, 197, 212, 220,229, 230-233, 237..242, 247, 271, 274,

283, 286, 296, 304error detection point, 233, 238-239, 246, 305,
309error production, 236, 237, 303, 307
error recovery, 176-177, 199, 213, 229, 230, 233,238..241, 246-247, 252, 291, 302..311
error repair, 13, 230, 309, 311error symbol, 233, 246-248, 306, 309
error token, 237, 238, 304, 306, 309essential ambiguity, 62, 63
EULER, 295EXCLUDE set, 275
Exhaustive Constant Partial Ordering property,301
exponential explosion, 149exponential time dependency, 70, 71..79, 88, 94,

110, 148, 227, 249-250, 253, 260, 262,271
extended context-free grammar, 35, 36, 183,267, 276
extended LL(1), 183, 282, 306extended LL(

k), 281extended LR(

k), 281, 287extended operator-precedence, 297, 298

extended precedence, 197, 198, 232, 295extended right precedence, 295
extended weak precedence, 197
fast string search, 116, 299feature (GPSG), 301
fiducial symbol, 306FILO list,

see stackfinite-choice grammar, 40, 53

finite-state automaton, 15, 106, 110, 112, 116,118, 122, 200-201, 220-222, 231, 249,

265, 280..303, 311finite-state grammar, 38, 41
FIRST set, 160-162, 168, 169..183, 195, 207,211, 215-216, 241, 243, 245, 271, 275,

279..282, 292, 306, 308, 310FIRST
ALL set, 195FIRST
k set, 168, 181, 182FIRST
OP set, 190, 191FL(k), 299

Floyd production, 199, 266, 272, 295, 302FMQ error correction, 244, 246, 308

FOLLOW set, 162, 173, 172-177, 181, 218, 241,271, 279..284
FOLLOW set error recovery, 241

Index 319
FOLLOWk set, 181, 182formal definition, 24
forward move, 238, 239, 306, 308free of hidden empty notions, 273
FSLR(k), 284full LL(1), 175-176, 182, 232, 244, 252
full LL(k), 182, 268, 289fully parenthesized expression, 187, 294

gap, 144, 146, 151, 199general hyperrule, 273
Generalized Overlap-Resolvable, 296Generalized Phrase Structure Grammar, 301
generative grammar, 18, 24, 272global error handling, 233, 306, 308
GNF, see Greibach Normal FormGOR,

see Generalized Overlap-ResolvableGOTO-action, 204, 271, 283, 291

GPSG, see Generalized Phrase Structure Gram-mar
grammar, 17grammatical inference, 311
Greibach Normal Form, 122, 123, 127, 274-275
handle, 73, 113, 184..206, 220, 222, 227, 232,238, 244, 284-285, 289, 295-297
hardest CF language, 312head grammar, 272
hypernotion, 44, 273hyper-rule, 44

ID/LP grammar, see ImmediateDominance/Linear Precedence grammar
Immediate Dominance/Linear Precedence gram-mar, 301
immediate error detection property, 175, 176,212, 231-233, 244, 306-308
immediate left-recursion, 128immediate successor,

see successorinadequate state, 205, 208, 221-223, 284, 288,

291incremental parser generation, 221, 291
incremental parsing, 221, 285..290, 300indexed grammar, 42, 269, 270
indirect left-recursion, 128, 129, 168infinite ambiguity, 57, 96, 157
infinite symbol set, 45infix notation, 64, 68
inherited affix, 271inherited attribute, 58
inherited information, 58insert-correctable, 244
instantaneous description, 124item, 151, 206
item grammar, 287

keyword, see reserved wordKleene star, 36
known-parsing table, 260
LALR(1), 79, 199, 233, 249..253, 269, 271,283..292, 305, 308
LALR(1) automaton, 213, 216, 218, 291LALR(1,1), 289
LALR(k), 213, 287, 307LAM(

m), 289lane, 285

LAST set, 275LAST

ALL set, 195LAST
OP set, 190LC(k), 292-293, 311

Least Left Corner, 289least-error correction, 233, 234..236, 246-247,

303-304left bound, 273
left priority, 192left-context, 179, 238, 284, 296
left-corner derivation, 64left-corner parsing, 68-69, 79, 264-265, 273-274,

292-294, 300left-factoring, 179, 303, 310
left-hand side, 23left-most derivation, 50, 51, 64, 101, 106, 120,

125, 127, 134-135, 167, 259left-recursion, 79, 127, 128, 135, 149, 168, 178,
253, 260, 274, 279, 294
lex, 39, 269, 299-300lineage, 54

linear grammar, 38, 277linear time dependency, 70, 73, 76-77, 155, 191,

205, 213, 227, 249-252, 271, 297, 307,311
linearized parse tree, 64, 300LL(1), 78, 170, 167..183, 211, 243-245, 252-

253, 267, 269, 273, 279-283, 292-293,306-307
LL(1) conflict, 178, 179, 183, 267, 289, 310LL(2), 182
LLC, 289LL(

f), 279LL(
k), 79, 181, 182, 268, 271, 279-281, 284,292-294, 307, 311-312

LL(k) item, 281LLP(

k), 292-294LLR,

see LL-regularLL-regular, 280-281, 297

local error handling, 233, 240, 306, 308locally least-cost error recovery, 246, 307..311
locally unambiguous, 273loop, 57, 82-83, 86, 103, 128, 222

320 Index
LR(0), 201, 205, 204-206, 209, 212-214, 218,222, 226, 250, 283..293
LR(0) automaton, 203, 204..222, 284..288LR(1), 78, 197, 205..227, 233, 249..252, 267-

269, 272, 282..292, 304, 309-310LR(1) automaton, 206..217, 285
LR(2), 211-212LR(

k), 79, 211-213, 228, 268, 270, 281..294,304..312

LR(m, k), 285LR-regular, 221, 228, 283, 287, 297

macro grammar, 270Manhattan turtle, 27
match, 72maze, 75-77, 110
metagrammar, 45metanotion, 44, 45-46, 272-273
metarule, 44minimum distance error handling, 303, 305, 308
mixed-strategy precedence, 197, 198, 295..298,310
modifying a grammar, 77, 123, 196, 237, 249-252, 267, 310
Modula-2, 131, 137, 252
NCLR(1), 227NDA,

see non-deterministic automatonnon-canonical parsing, 184, 227, 285-286, 291,

297non-deterministic automaton, 68, 69..71, 75, 77,
110-111, 118, 145, 201..218, 284non-directional parsing method, 15, 74, 75-76,
79, 81, 230-231non-productive non-terminal, 57, 96, 94-98, 104
non-reachable non-terminal, 97, 94..102non-terminal, 23
NQLALR(1), 288, 290nullable, 211, 216, 292, 306
nullable grammar, 307
occam, 273operator grammar, 190, 191, 195
operator-precedence, 191, 188..196, 232, 251,267, 269, 294, 297-298, 310-311
OR, see Overlap-Resolvableoriginal symbol, 54
Overlap-Resolvable, 296
panic mode, 240, 241, 306..309parse table, 165, 166..183, 214, 220, 237, 252,

268, 270, 276..298parser generator, 37, 39, 69, 78, 179-180, 183,
204, 208, 214..221, 245, 250-252, 267,269, 281..289, 299-300, 310

partial computation, 269Pascal, 12, 15, 21, 60-61, 131, 133, 136-137,

237, 240, 253, 258, 260, 269, 281, 307,310-311
pattern mapping, 305PDA,

see pushdown automatonphrase, 227

phrase level error handling, 238, 308, 310phrase structure grammar, 25, 26..28, 34, 41, 53,

55, 60-61, 71, 76, 265, 271, 274, 301PL/C, 303
PLR(k), 292-293polynomial time dependency, 70, 88, 253, 260,

263, 277postfix notation, 64
precedence functions, 192, 194, 252, 295-298precedence relation, 187, 188..190, 195..198,

232, 238-239, 276, 295-298, 306precedence table, 187, 188, 196, 282, 311
predict, 72prediction stack, 121, 122..131, 241, 275
predictive analyser, 275predictor, 307
prefix notation, 64prefix-free, 136
preliminary state, 271primitive predicate, 46, 271-272
production chain, 38, 292, 293production expression, 293
production graph, 25, 28, 32, 51production prefix, 284
production rule, 25production step, 25, 66, 73, 168, 242, 292
production tree, 32, 38, 50..74, 120, 148, 269,275, 301
Prolog, 139-143, 253, 268, 279, 294, 302Prolog clause, 141, 279, 294
propagated input, 215-216proper grammar, 97, 242
pumping lemma for context-free languages, 55pumping lemma for regular languages, 56
pushdown automaton, 121, 122-123, 127, 292
quadratic time dependency, 70, 155, 270, 276,278
quasi-regular grammar, 107-108, 299
Rabin-Karp algorithm, 118reachable non-terminal, 97, 98, 102, 114
read-back tables, 286real-time parser, 70, 302
recognition table, 93, 94, 99-105, 231, 236,277-278
recording grammar, 42, 270recursive ascent, 221, 269, 291-292

Index 321
recursive descent, 15, 76..79, 135, 131..137,180..183, 221, 241, 245, 252-253,

267..273, 277..282, 291-293, 300..311reduce, 73
reduce/reduce conflict, 205, 210, 217, 219, 288,292
reduction error, 232, 233, 306reduction look-ahead, 161, 162-163, 276-277
regional error handling, 233, 238, 308regional least-cost error correction, 308
register-vector grammar, 301, 302regular expression, 36, 40, 53, 113-116, 189,

299-300regular grammar, 38
regular language, 56, 107, 114, 299-300, 303,311
regular right part grammar, 36, 220, 285..290reserved word, 40, 74, 286
RG, see recording grammarright bound, 273
right precedence, 295right priority, 192
right-hand side, 23right-most derivation, 51, 64, 101, 106, 144, 184
RL(1), 78, 286R-language, 295
RLL(k), 271RR(1), 78
RRP grammar, see regular right part grammarRV grammar,

see register-vector grammarRV grammar, 302

self-embedding, 266, 299semantic action, 180, 185, 204, 220, 230, 237,

251-252, 266, 280, 282, 285, 288, 299,309
semantic clause, 57, 58-59semi-top-down, 293
sentence, 25sentential form, 25
sequence, 17set, 17
SGML, 33s-grammar, 167
shift, 73shift/reduce conflict, 205, 206, 210, 219, 288,

292simple chain grammar, 293
simple LL(1), 281simple LR(1), 218
simple precedence, 195, 194-196, 232, 238, 284,295..298, 306, 311
single rule, 89, 220skeleton grammar, 273
SLL(1) (simple LL(1)), 167, 170

SLL(1) (Simple LL(1)), 281SLR(1), 79, 218, 219, 222, 226, 233, 250-251,

269, 271, 283..298, 310SLR(
k), 283-284, 307SLUNT, 302

soft precedence, 296space requirements, 155, 204, 213, 251-252, 269,

277, 288spontaneous input, 215-216
spurious ambiguity, 62spurious error message, 230, 246, 248
stack, 121stack alphabet, 121
stack duplication, 223stackability error, 232, 306
stack-controlling LR parser, 287stacking conflict, 289
stacking error, see stackability errorstack-shift, 289
start symbol, 23state, 109
state transition, 109, 220, 283, 287station, 201, 203, 207, 210-211, 215-216
stochastic grammar, 304, 311strict grammar, 273
strict syntax generator, 273strong compatibility, 285
strong-LL(1), 174, 175, 178, 181, 232, 244,251-252, 270, 291, 303, 306-307
strong-LL(k), 181, 182, 270, 280-281, 291Subject-Verb-Object language, 107
submatrix technique, 289subset algorithm, 216
subset construction, 111, 203..206, 271, 299successor, 285
suffix-grammar, 247, 248, 309suffix-language, 247
suffix-parser, 247, 246-248synchronization triplet, 306
syntactic graph, see production graphsyntax, 17, 34, 58, 266-267, 271, 273, 291, 300,

310-311syntax error, 198, 229, 230, 237, 246-248, 304,
308-310syntaxis,

see syntaxsynthesized attribute, 58

synthesized metanotion, 273
table compaction, see table compressiontable compression, 112, 204, 213, 268, 281..296
table-driven, 69, 180-181, 252, 269, 280, 282,306
terminal, 23, 41, 45, 49terminal parser, 273
terminal symbol, 23

322 Index
time complexity, 70time requirements, 70, 251-252, 277-278, 284
token, see terminal symbolTomita notation, 226
Tomita parser, 15, 69, 79, 153, 157, 222, 226,233, 250, 276..279, 291
top-down parsing, 64transduction grammar, 58, 59, 266, 279
transformation rule, 265transformations on grammars, 53, 92..104, 115-

116, 122, 127-128, 167, 170, 212, 251,265, 267, 280, 291, 293, 295, 298, 303,
305, 310transition diagram, 109, 110-111, 266, 299, 303
transition successor, see successorType 1 context-sensitive, 29, 51, 53
Type 1 monotonic, 29, 31typesetting, 11, 219, 267

Unger parser, 15, 75..92, 101, 103, 153, 156,230..236, 250, 253, 275, 278
uniquely assignable, 273unit rule, 89, 90..92, 96, 98, 104, 113, 128, 191,

220, 283-288unreduce, 145, 147
unshift, 145, 146-147useless non-terminal, 57

Valiant parser, 76, 277viable suffix, 281
weak compatibility, 285weak operator-precedence, 297
weak precedence, 196, 197, 232, 252, 276,295-298
weak-PLR(k), 294well-formed affix grammar, 271
well-formed substring table, see recognitiontable
well-formed substring table, 250, 260, 275..278well-tuned Earley/CYK parser, 163
word, 17
yacc, 214, 269, 285, 291
2-form grammar, 277
A`0, 22A`

1, 22e-closure, 114

e-free, 34, 52-53, 96, 269, 274, 296, 311e-move, 113-114, 173, 175, 202, 207, 232, 291,

301, 303

e-rule, 34, 52-53, 57, 73, 82, 85, 90..98, 104,113, 123, 128, 136, 149, 157, 161, 168,

170, 173, 210-211, 216, 234, 244, 266,272, 275, 279-280, 288, 290, 294, 311
e-transition, 113, 201, 271o""

* , 51

o""*l , 51o""

*r , 51

o""* , 291-/ , 291

<*, 187<=*, 197
>* , 187=. , 187Table of contents

Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131.1 Parsing as a craft

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141.2 The approach used

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141.3 Outline of the contents

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151.4 The annotated bibliography

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

2 Grammars as a generating device . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162.1 Languages as infinite sets

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162.1.1 Language
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162.1.2 Grammars

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172.1.3 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182.1.4 Describing a language through a finite recipe

. . . . . . . . . . . . . . . . . 222.2 Formal grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242.2.1 Generating sentences from a formal grammar

. . . . . . . . . . . . . . . . . 252.2.2 The expressive power of formal grammars
. . . . . . . . . . . . . . . . . . . 272.3 The Chomsky hierarchy of grammars and languages

. . . . . . . . . . . . . . . 282.3.1 Type 1 grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282.3.2 Type 2 grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322.3.3 Type 3 grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372.3.4 Type 4 grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402.4 VW grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412.4.1 The human inadequacy of CS and PS grammars

. . . . . . . . . . . . . . . 412.4.2 VW grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422.4.3 Infinite symbol sets

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452.4.4 BNF notation for VW grammars

. . . . . . . . . . . . . . . . . . . . . . . . . . . 452.4.5 Affix grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462.5 Actually generating sentences from a grammar

. . . . . . . . . . . . . . . . . . . 472.5.1 The general case
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472.5.2 The CF case
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492.6 To shrink or not to shrink

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

6 Table of contents

2.7 A characterization of the limitations of CF and FS grammars . . . . . . . . 542.7.1 The uvwxy theorem

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 542.7.2 The uvw theorem
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562.8 Hygiene in grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562.8.1 Undefined non-terminals

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562.8.2 Unused non-terminals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 572.8.3 Non-productive non-terminals

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 572.8.4 Loops
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 572.9 The semantic connection

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 572.9.1 Attribute grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 582.9.2 Transduction grammars

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592.10 A metaphorical comparison of grammar types

. . . . . . . . . . . . . . . . . . . 60

3 Introduction to parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623.1 Various kinds of ambiguity

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623.2 Linearization of the parse tree

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643.3 Two ways to parse a sentence
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643.3.1 Top-down parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653.3.2 Bottom-up parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 663.3.3 Applicability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 673.4 Non-deterministic automata

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 683.4.1 Constructing the NDA
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 693.4.2 Constructing the control mechanism

. . . . . . . . . . . . . . . . . . . . . . . . 693.5 Recognition and parsing for Type 0 to Type 4 grammars

. . . . . . . . . . . . 703.5.1 Time requirements
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 703.5.2 Type 0 and Type 1 grammars

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 703.5.3 Type 2 grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 723.5.4 Type 3 grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 733.5.5 Type 4 grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 743.6 An overview of parsing methods

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 743.6.1 Directionality
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 743.6.2 Search techniques

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753.6.3 General directional methods

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 763.6.4 Linear methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 763.6.5 Linear top-down and bottom-up methods

. . . . . . . . . . . . . . . . . . . . 783.6.6 Almost deterministic methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 793.6.7 Left-corner parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 793.6.8 Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

4 General non-directional methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 814.1 Unger's parsing method

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 824.1.1 Unger's method without e-rules or loops

. . . . . . . . . . . . . . . . . . . . . 824.1.2 Unger's method with e-rules
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 854.2 The CYK parsing method
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 884.2.1 CYK recognition with general CF grammars

. . . . . . . . . . . . . . . . . . 894.2.2 CYK recognition with a grammar in Chomsky Normal Form

. . . . . . 924.2.3 Transforming a CF grammar into Chomsky Normal Form
. . . . . . . . 94

Table of contents 7
4.2.4 The example revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 994.2.5 CYK parsing with Chomsky Normal Form

. . . . . . . . . . . . . . . . . . . 994.2.6 Undoing the effect of the CNF transformation

. . . . . . . . . . . . . . . . 1014.2.7 A short retrospective of CYK
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 1044.2.8 Chart parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105

5 Regular grammars and finite-state automata . . . . . . . . . . . . . . . . . . . . 1065.1 Applications of regular grammars

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 1065.1.1 CF parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1065.1.2 Systems with finite memory

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1075.1.3 Pattern searching
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1085.2 Producing from a regular grammar

. . . . . . . . . . . . . . . . . . . . . . . . . . . 1095.3 Parsing with a regular grammar
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1105.3.1 Replacing sets by states
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1115.3.2 Non-standard notation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1135.3.3 DFA's from regular expressions

. . . . . . . . . . . . . . . . . . . . . . . . . . 1145.3.4 Fast text search using finite-state automata

. . . . . . . . . . . . . . . . . . 116

6 General directional top-down methods . . . . . . . . . . . . . . . . . . . . . . . . . 1196.1 Imitating left-most productions

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1196.2 The pushdown automaton
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1216.3 Breadth-first top-down parsing

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1256.3.1 An example
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1256.3.2 A counterexample: left-recursion

. . . . . . . . . . . . . . . . . . . . . . . . . 1276.4 Eliminating left-recursion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1286.5 Depth-first (backtracking) parsers

. . . . . . . . . . . . . . . . . . . . . . . . . . . 1306.6 Recursive descent
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1316.6.1 A naive approach

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1336.6.2 Exhaustive backtracking recursive descent

. . . . . . . . . . . . . . . . . . 1366.7 Definite Clause grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139

7 General bottom-up parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1447.1 Parsing by searching

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1467.1.1 Depth-first (backtracking) parsing

. . . . . . . . . . . . . . . . . . . . . . . . 1467.1.2 Breadth-first (on-line) parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . 1477.1.3 A combined representation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1487.1.4 A slightly more realistic example

. . . . . . . . . . . . . . . . . . . . . . . . . 1487.2 Top-down restricted breadth-first bottom-up parsing

. . . . . . . . . . . . . . 1497.2.1 The Earley parser without look-ahead
. . . . . . . . . . . . . . . . . . . . . . 1497.2.2 The relation between the Earley and CYK algorithms

. . . . . . . . . . 1557.2.3 Ambiguous sentences
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1567.2.4 Handling e-rules
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1577.2.5 Prediction look-ahead

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1597.2.6 Reduction look-ahead
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161

8 Deterministic top-down methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1648.1 Replacing search by table look-up

. . . . . . . . . . . . . . . . . . . . . . . . . . . 165

8 Table of contents

8.2 LL(1) grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1688.2.1 LL(1) grammars without e-rules

. . . . . . . . . . . . . . . . . . . . . . . . . . 1688.2.2 LL(1) grammars with e-rules
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 1708.2.3 LL(1) versus strong-LL(1)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1748.2.4 Full LL(1) parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1758.2.5 Solving LL(1) conflicts

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1788.2.6 LL(1) and recursive descent

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1808.3 LL(k) grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1818.4 Extended LL(1) grammars

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183

9 Deterministic bottom-up parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1849.1 Simple handle-isolating techniques

. . . . . . . . . . . . . . . . . . . . . . . . . . . 1859.1.1 Fully parenthesized expressions

. . . . . . . . . . . . . . . . . . . . . . . . . . 1869.2 Precedence parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1879.2.1 Constructing the operator-precedence table

. . . . . . . . . . . . . . . . . . 1909.2.2 Precedence functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1929.2.3 Simple-precedence parsing

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1949.2.4 Weak-precedence parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1969.2.5 Extended precedence and mixed-strategy precedence

. . . . . . . . . . 1979.2.6 Actually finding the correct right-hand side
. . . . . . . . . . . . . . . . . . 1989.3 Bounded-context parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1989.3.1 Floyd productions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1999.4 LR methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2009.4.1 LR(0)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2019.4.2 LR(0) grammars

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2059.5 LR(1)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2059.5.1 LR(1) with e-rules

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2109.5.2 Some properties of LR(k) parsing

. . . . . . . . . . . . . . . . . . . . . . . . . 2119.6 LALR(1) parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2139.6.1 Constructing the LALR(1) parsing tables

. . . . . . . . . . . . . . . . . . . 2149.6.2 LALR(1) with e-rules
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2169.6.3 Identifying LALR(1) conflicts

. . . . . . . . . . . . . . . . . . . . . . . . . . . 2179.6.4 SLR(1)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2189.6.5 Conflict resolvers

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2199.7 Further developments of LR methods

. . . . . . . . . . . . . . . . . . . . . . . . . 2199.7.1 Elimination of unit rules
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2209.7.2 Regular right part grammars

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2209.7.3 Improved LALR(1) table construction

. . . . . . . . . . . . . . . . . . . . . 2209.7.4 Incremental parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2219.7.5 Incremental parser generation

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 2219.7.6 LR-regular
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2219.7.7 Recursive ascent

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2219.8 Tomita's parser
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2229.8.1 Stack duplication

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2239.8.2 Combining equal states

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2239.8.3 Combining equal stack prefixes

. . . . . . . . . . . . . . . . . . . . . . . . . . 2269.8.4 Discussion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226

Table of contents 9
9.9 Non-canonical parsers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2279.10 LR(k) as an ambiguity test

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228

10 Error handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22910.1 Detection versus recovery versus correction

. . . . . . . . . . . . . . . . . . . . 22910.2 Parsing techniques and error detection
. . . . . . . . . . . . . . . . . . . . . . . . 23010.2.1 Error detection in non-directional parsing methods

. . . . . . . . . . . . 23010.2.2 Error detection in finite-state automata
. . . . . . . . . . . . . . . . . . . . . 23110.2.3 Error detection in general directional top-down parsers

. . . . . . . . . 23110.2.4 Error detection in general directional bottom-up parsers

. . . . . . . . 23210.2.5 Error detection in deterministic top-down parsers
. . . . . . . . . . . . . 23210.2.6 Error detection in deterministic bottom-up parsers
. . . . . . . . . . . . . 23210.3 Recovering from errors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23310.4 Global error handling
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23310.5 Ad hoc methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23710.5.1 Error productions

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23710.5.2 Empty table slots
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23710.5.3 Error tokens
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23810.6 Regional error handling

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23810.6.1 Backward/forward move

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23810.7 Local error handling
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24010.7.1 Panic mode
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24010.7.2 FOLLOW set error recovery

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24110.7.3 Acceptable-sets derived from continuations

. . . . . . . . . . . . . . . . . . 24110.7.4 Insertion-only error correction
. . . . . . . . . . . . . . . . . . . . . . . . . . . 24410.7.5 Locally least-cost error recovery

. . . . . . . . . . . . . . . . . . . . . . . . . . 24610.8 Suffix parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246

11 Comparative survey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24911.1 Considerations

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24911.2 General parsers

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25011.2.1 Unger
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25011.2.2 Earley
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25011.2.3 Tomita
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25011.2.4 Notes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25111.3 Linear-time parsers

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25111.3.1 Requirements
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25111.3.2 Strong-LL(1) versus LALR(1)

. . . . . . . . . . . . . . . . . . . . . . . . . . . 25111.3.3 Table size
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252

12 A simple general context-free parser . . . . . . . . . . . . . . . . . . . . . . . . . . 25312.1 Principles of the parser

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25312.2 The program
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25812.2.1 Handling left recursion

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26012.3 Parsing in polynomial time
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260

13 Annotated bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264

10 Table of contents

13.1 Miscellaneous literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26513.2 Unrestricted PS and CS grammars

. . . . . . . . . . . . . . . . . . . . . . . . . . . 26913.3 Van Wijngaarden grammars and affix grammars

. . . . . . . . . . . . . . . . . 27113.4 General context-free parsers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27313.5 LL parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27913.6 LR parsing

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28213.7 Left-corner parsing

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29213.8 Precedence and bounded-context parsing

. . . . . . . . . . . . . . . . . . . . . . 29413.9 Finite-state automata
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29913.10 Natural language handling

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30013.11 Error handling
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30213.12 Transformations on grammars

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31013.13 General books on parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31013.14 Some books on computer science

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 312

Author index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317