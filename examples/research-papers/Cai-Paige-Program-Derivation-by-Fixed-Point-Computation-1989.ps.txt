

Program Derivation by Fixed Point Computation

1Jiazhen Cai and Robert Paige

Dept. of Computer ScienceNYU/Courant

251 Mercer StreetNew York, NY 10012/USA

Abstract
This paper develops a transformational paradigm by which nonnumerical algorithms are treated as fixed pointcomputations derived from very high level problem specifications.  We begin by presenting an abstract functional

+problem  specification  language SQ ,  which  is  shown  to  express  any  partial  recursive  function  in  a  fixed  point
normal form.  Next, we give a nondeterministic iterative schema that in the case of finite iteration generalizes the'chaotic  iteration'  of  Cousot  and  Cousot  for  computing  fixed  points  of  monotone  functions  efficiently.    New

techniques  are  discussed  for  recomputing  fixed  points  of  distributive  functions  efficiently.    Numerous  examplesillustrate  how  these  techniques  for  computing  and  recomputing  fixed  points  can  be  incorporated  within  a
transformational programming methodology to facilitate the design and verification of nonnumerical algorithms.

1. Introduction

In  a  recent  survey  article [25]  Martin  Feather  has  said  that  the  current  state  of  the  art  of  program
transformations is still some distance from its ambitious goals - to dramatically improve the construction, reliability,
maintenance, and extensibility of software.  Our paper represents an attempt towards achieving these goals.

Algorithms often follow the same pattern: find an initial approximation of the solution, and then repeatedly
modify  the  approximation  until  it  becomes  the  solution.    We  investigate  a  class  of  such  algorithms  that  are  all
instances  of  a  general  nondeterministic  iterative  algorithm  schema  for  computing  least  or  greatest  fixed  points  of
computable functions.

Various  fixed  point  theorems  due  to  Tarski [75],  Kleene [45],  Cousot  and  Cousot [18],  and  others [8]  have
been  applied  by  Scott  to  program  semantics [69],  have  been  used  by  Cocke  and  Schwartz [13],  Kildall [44],
Tenenbaum [77], and others [36, 42, 16, 67, 43, 74] to specify and implement global program analysis problems, are
important to program verification [16, 17, 20, 23], arise in complexity theory [79, 39, 40, 33, 35, 59], and are used to
support high level program transformations [2, 29, 7, 49, 62, 53, 56, 11, 54, 70, 60, 81, 51].

We are further encouraged by the following facts:

* Any set generated by inductive definitions can also be defined as the least fixed point of a monotonefunction [1];

* Without  a  fixed  point  operator,  a  first  order  language  on  finite  structures  cannot  express  transitiveclosure [2];

* The language of Relational Calculus [14] over a totally ordered finite domain plus least fixed points ofmonotone  operators  precisely  expresses  all queries  computable  in  polynomial  time  (in  the  size  of  the

domain) on a Turing machine [39, 79].

1Part of this work was done while Paige was a summer faculty at IBM Yorktown and while both authors were at Rutgers University. This work
is also partly based upon research supported by the Office of Naval Research under Contract No. N00014-87-K-0461 and by Thomson-CSF/DSEunder an ESPRIT contract.

2
These facts suggest that one reasonable approach to program synthesis is to specify a problem in a least or
greatest fixed point normal form, and then apply various subsequent transformations to compile this normal form
into  an  efficiently  executable  program.    Preliminary  ideas  that  support  this  approach  are  embodied  in  a  working
three-phase  prototype  compiler  that  automatically  translates  abstract  problem  specifications  into  efficient  RAM
code [53, 56].  This compiler, which was implemented by Paige within the RAPTS transformational programming
system,  has  been  used  to  generate  many  efficient  programs  of  moderate  complexity  from  succinct  problem
statements, e.g., graph reachability, cycle detection, live code analysis, and attribute closure.

The current version of RAPTS can manipulate problem specifications of the form,
(1) the s: w I' s | s = f(s) minimizing s
which stands for the smallest set s (with respect to set containment) that contains w and satisfies the equation s =
f(s).  RAPTS can also manipulate the dual form

the s: s I' w | s = f(s) maximizing s

In  the  first  phase  of  compilation,  the  system  solves  these  set  theoretic  equations  by  transforming  them  into
programs  that  compute  least  or  greatest  fixed  points.    These  transformations  introduce  a  minimal  form  of
algorithmic strategy.  In the second phase of compilation, the system uses a generalized finite differencing technique
to  introduce  access  paths  and  basic  invariants  that  serve to  implement the  strategy  efficiently [57, 52, 55].  In  the
final  phase,  the  low  level  set-based  program  that  results  from  the  preceding  transformations  is  compiled  into
conventional code.  Elaboration of the three-step automatic programming scheme just sketched is found in [56].

This article makes the following contributions:

+* A very high level functional problem specification language SQ is presented.  This language contains

conventional expressions over boolean and integer datatypes, mathematical dictions found in finite set+theory, and least and greatest fixed point expressions.  We prove that a subset of SQ with operational

semantics can express all partially recursive functions;

* A  new  nondeterministic algorithm schema  for  computing  least  and  greatest fixed points of monotonefunctions  is  given.    This  schema  generalizes  the  'chaotic  iteration'  found  in  Kildall [44],

Tenenbaum [77], and Cousot and Cousot [17] (restricted to finite iteration), so that it can be adapted ina  wider  range  of  contexts  to  synthesize  efficient  algorithms  and  to  provide  succinct  transformational
correctness  proofs.  Based  on  a  lattice  theoretic  notion  of  abstract  datatype,  our  fixed  pointtransformations can be applied to a diverse assortment of abstract functions and datatypes;

* Broad  sufficient  conditions  are  stated  for  when  specification  (1)  can  be  rewritten  in  the  followingequivalent way,

(2) the s: s = w E` f(s) minimizing s
in  order  to  facilitate  the  mechanical  development  of  regularized  iterative  procedures  with  greatlysimplified  preprocessing  operations.    The  code  that  results  from  (2)  could  be  as  short  as  half  of  that

resulting from (1).

* Conditions are given for when the following least fixed point

the s: w I' s | s = f(s,t) minimizing s
can be recomputed incrementally when either of the parameters w or t increases;

* When f is distributive (i.e., " s, t | f(s E` t) = f(s) E` f(t)), the following least fixed point

the s: w I' s | s = f(s) minimizing s
can be recomputed efficiently when w increases or decreases;

* New techniques are presented for solving the lattice theoretic system of equations

3
the x ,...,x :1 nx

= f (x ,...,x ),1 1 1 n
...x = f (x ,...,x )

n n 1 nminimizing x ,...,x

1 nefficiently;

* Our  use  of  fixed  point  theory  is  shown  to  uncover  new  basic  principles  of  software  engineering  forcombining problem specifications and solving them simultaneously using a small number of 'passes'.

Other  researchers [7, 62, 49]  have  employed  fixed  point  transformations  applied  to  general  recursion
equations.  However, their  transformations  seem less  amenable  to  full  mechanization than  ours,  and  they  foster  a
syntactic bias towards a depth-first or breadth-first search implementation.

Before stating our new results, it is worthwhile giving the reader a broader perspective by stepping through
the three-phase RAPTS problem specification compiler using a simple case study.  (A fuller discussion can be found
in [56].)

Example 1: (Graph  Reachability)  Consider  the  problem  of  finding  the  set  of  vertices s reachable
along paths in a directed graph from an arbitrary set of vertices w.  We represent the graph by a finite set of
edges e (without  multi-edges),  where  each  edge  is  a  pair  of  vertices.    It  is  convenient  to  regard e as  a
multi-valued  mapping,  so  that  for  each  vertex x,  the  term e{x}  denotes  the  set  of  vertices  (called  the
successor or adjacent vertices of x) reachable from x along a single edge.  We also use the image set notation
e[s] to mean E` e{x}.xI^s

The user can formally define the reachability problem using the following specification:
(3) the s: w I' s | e[s] I' s minimizing s
which represents the smallest set s that contains w and satisfies the predicate e[s] I' s.  The predicate w I' s
appearing in specification (3) signifies that the solution set s includes all paths of length 0.  The clause e[s] I'
s means that the solution cannot be extended further.  The restriction to a minimum solution satisfying these
two predicates takes connectivity into account.  Without this restriction the entire set of vertices would be a
solution.

RAPTS will first transform problem specification (3) into the following equational form:
(4) the s: w I' s | s = s E` e[s] minimizing s
which stands for the least fixed point of the expression s E` e[s] in the space of all sets containing w.  Next,
according  to  the  theory  to  be  discussed,  the  fixed  point p can  be  computed  by  executing  the  following
procedure:

(5) p := w $Assign w to p.(while $ x I^ (e[p] - p) )  $Repeatedly augment p

p := p E` {x}  $with a vertex adjacentend $to p.

Further  improvement  in  the  performance  of  code  (5)  can  be  achieved  by  applying  finite
differencing [57, 55] and data structure selection [21, 65, 56].  Finite differencing eliminates a major source
of inefficiency within (5) - the repeated calculation of e[p] - p at the top of the while-loop.  This is achieved
by preserving and exploiting the invariant:

new = e[p] - p
within the while-loop.

Automatic  data  structure  selection  will  subsequently  aggregate  the  program  variables e, p,  and new

4
around the set of graph vertices.  That is, we store an array b of records, one record for each node x in the
graph.  Each record in b contains a node x, a pointer to the set of adjacent nodes e{x}, and a bit denoting
whether x belongs to p or not.  For each node x, e{x} is represented as an array of pointers to records in b
associated with nodes adjacent to x.  Variable new is represented as a queue of pointers to records belonging
to b.  The resulting  program  has  a  worst  case  time  linear  in  the  number  of edges;  its  worst  case  auxiliary
space is linear in the number of vertices. n

This  paper  is  structured  in  the  following  way.    Section 2  presents  basic  definitions,  notational  conventions,

+and a description of the problem specification language SQ .  Section 3 develops basic transformations to compute

fixed  points  of  monotone  computable  functions.    Section  4  describes  transformations  to  compute  fixed  points
dynamically.  Section 5 extends earlier transformations to the problem of solving systems of equations.  Section 6
sketches an implementation design.  The final section surveys related work and discusses open problems.

2. Preliminaries

We  first  review  a  few  basic  definitions  and  concepts  of  lattice  theory  that  underlie  our  main  results.    This
background  material  may  be  found  in  any  introductory  text  on  lattice  theory;  for  example,  Birkhoff [8]  or

+Gratzer [30].  After  that  we  describe  the  problem  specification  language SQ to  be  used  in  illustrating

transformations for computing and recomputing fixed points.

2.1. Definitions

A poset (L,<=)  is  a  reflexive,  transitive,  antisymmetric,  binary  relation <= on  a  set L.  A  poset  (L,<=)  has  a
minimal element y iff " x I^ L |  (x <= y o"" x = y.)  A  poset  (L,<=)  has  a minimum element 0 iff " x I^ L | x >= 0.
Maximal and maximum elements can be defined analogously.  A chain for a poset (L,<=) is a strictly increasing or
decreasing sequence of elements of L.  A poset (L,<=) is said to have an ascending (respectively descending) chain
condition, abbreviated ACC (respectively DCC), iff there are no infinite increasing (decreasing) chains in L.  Let w

+ -I^ L. An element a I^ L is w -finite if the set {x I^ L | w <= x <= a} satisfies ACC.  Similarly, a is w -finite if the set {x

+ -I^ L | a <= x <= w} satisfies DCC. 0 -finite is abbreviated as 0-finite, and 1 -finite is abbreviated as 1-finite, where we

use 1 to represent the maximum element in L.  Let a, b, c I^ L. If a <= c and b <= c, then c is an upper bound for a and
b.  An upper bound c for a and b is said to be the least upper bound if every upper bound x for a and b is greater
than or equal to c.  The least upper bound for a and b is also called the join of a and b and is denoted by a U' b.  If a
U' b is defined and belongs to L for all a, b I^ L, then (L,<=) is called a join semilattice. Lower bounds, greatest lower
bounds (also called meets and denoted by U`), and meet semilattices are defined analogously.  If a poset is both a join
and a meet semilattice, then it is a lattice.  Join and meet operations are each commutative and associative.  If T is a
set, then the set P = {t: t I' T} of all subsets of T is the powerset of T and is denoted by pow(T). P is a powerset
lattice under the relation I' with intersection as meet, union as join, minimum element {}, and maximum element T.

Let f: T o"" Q be a function from a poset (T, <=) into a poset (Q,<=').  We say that the domain of f is the set {x I^
T | ($ y I^ Q | f(x) = y)}; the range of f is the set {y I^ Q | ($ x I^ T | f(x) = y)}.  Function f is said to be partial if there
are  elements  of T outside  its  domain;  otherwise  it  is total.  Function f is  said  to  be monotone (respectively
antimonotone) if for every two elements x, y belonging to its domain such that x <= y, then f(x) <=' f(y) (respectively,
f(x) >=' f(y)).  If  posets  (T,<=)  and  (Q,<=')  are  the  same,  then  following  Gurevich [33],  we  say  that f is inflationary
(respectively, deflationary) at x if f(x) >= x (resp. f(x) <= x).  Function f is said to be inflationary (resp. deflationary) if
it is inflationary (resp.  deflationary) at each point in its domain. For example, function x U' f(x) is inflationary for
any function f.

5
Let f: T o"" Q be a partial function from a poset (T, <=) into a poset (Q,<=').  Suppose that we can identify the

2elements of T and Q with unique finite strings over an alphabet.  We say, informally , that f is partially computable

3iff there exists a Turing Machine M such that for each element x in T, M terminates with output y = f(x) whenever

f(x) is defined and does not terminate otherwise.  If f is total and partially computable, then f is computable.

2.2. Language

+Specification  language SQ is  essentially  a  functional  subset  of  the  SETL  programming  language [66]

+augmented  with  fixed  point  operations.    In  addition  to  conventional  boolean  and  integer  datatypes, SQ includes

finite tuples, sets, and maps, which can be nested to arbitrary depth.  With a few exceptions to be described, most of
the  notations  in  this  language  are  borrowed  from  finite  set  theory [72]  and  conform  to  universally  accepted
mathematical notations.

We  make  use  of  the  overloaded  size  operator  #s in  the  following  way.  If s is  a  set,  then  #s denotes  the
cardinality  of s;  if s is  a  tuple,  it  denotes  the  number  of  components  of s.  The  choice  operation 's denotes  an
arbitrary element selected from the set s.  If s is empty, then the choice operation has the value \Omega , which denotes
undefined.  We regard a map as a finite set of ordered pairs that maps a domain set to a range set.  Thus, a map can
be a single-valued function or a multi-valued binary relation.  The function retrieval term f(x) denotes the value of
function f at domain point x.  If x does not belong to the domain of f or if f contains two or more different pairs with
first component value x, then f(x) is undefined.  We use the image set notation f{x} to denote the set {y: [u, y] I^ f | u

4= x}. If s is a set, then the extended image set f[s] denotes the set E` f{x}.  We use the tuple selection term t(i) toxI^s

threpresent the i component of tuple t.  If t has less than i components, then t(i) is undefined.

If op is a binary, commutative, and associative operation with neutral element e, then it can be applied to all
the elements of a set or tuple t using the notation op/t, where

op/{} = op/[] = e
For example, if t is a set of sets, then E`/t means the same as E` x.  The overloaded binary relational operator <= canxI^t
be  used  to  compare  numbers,  boolean  values  (with false <= true),  sets  (with <= representing I'),  and  user-declared
partial orderings.  Abstract meet (i.e., U`) and join(i.e., U') are used in connection with relation <=.

+We  augment  SETL  with  fixed  point  operations LFP and GFP.  If f(s)  is  an SQ expression,  then  the  least

fixed  point  operation LFP (f(s), s)  denotes  the  minimum  element s (with  respect  to  the  partial  ordering <=)  that<=,w
satisfies the condition w <= s and s = f(s).  The greatest fixed point expression GFP (f(s), s) is defined analogously.<=,w
For convenience, parameter w can be elided when w = 0 for least fixed points and when w = 1 for greatest fixed
points;  parameter <= can  also  be  elided,  whenever  the  partial  ordering  is  clear  within  the  context.  Also,  when
function f has  only  one  argument,  the  least  and  greatest  fixed  point  operators  can  be  abbreviated LFP (f)  and<=,w
GFP (f).<=,w

+We give a more detailed weakly typed definition of SQ expressions as follows:

+1. Every constant (denotations omitted here) and variable is an SQ expression;

2. if

2See Rogers [64] for a more formal definition of computable functions on sets other than natural numbers.
3Turing Machines are defined in Appendix I.
4We do not simplify to {y: [x, y] I^ f}, which in SETL means {y: $ x such that [x, y] I^ f}.

6
x is a variable, +f is a binary relation valued SQ expression;

+t is a tuple valued SQ expression;
+e, e , e ,...,e are SQ expressions,1 2 k

+i, i , i are integer valued SQ expressions,1 2

+k(x), k are boolean valued SQ expressions,0
+s, s , s ,...,s are set valued SQ expressions,1 2 k

+then the expressions defined in table 2-1 are also SQ expressions.

Letaop I^ {+, -}, iop I^ {<, <=, =, >, >=},
rop I^ {I', I`, =, E'}, sop I^ {E`, -, C,},bop I^ {and, or}, mop I^ {min, max},
op I^ {+, E`, C,, and, or, min, max} Q I^ {$, "},
Expression Definition
domain f { x : [x,y] I^ f }range f { y : [x,y] I^ f }

-1f inverse of f
f(e) y, if f{e} = {y}\Omega  (undefined), otherwise

f{e} { y : [u,y] I^ f | u = e }f[s] { y : x I^ s, y I^ f{x} }
t(i) ith component of tuple t#s set cardinality
{e(x ,...,x ):x I^s ,x I^s (x ), set former1 k 1 1 2 2 1
...,x I^s (x ,...,x )| k(x ,...,x )}k k 1 k-1 1 k
{x I^ s | k(x)} abbreviation of {x: x I^ s | k(x)}[e ,...,e ] enumerated tuple

1 k{e ,...,e } enumerated set
1 ke I^ s membership test

's arbitrary choicei aop i arithmetic

1 2i iop i integer comparison
1 2s sop s set union, intersection, and difference

1 2s rop s set comparison
1 2k kop k boolean and, or

1 2not k boolean negation

0i mop i minimum and maximum
1 2Q x I^ s | k(x) boolean valued quantifier (" or $)

op/e e1 op e2 op ... op en, where e is the set {e1,...,en}or tuple [e1,...,en]; evaluates to the

neutral element when n = 0LFP (e(x), x) least fixed point
<=,wGFP (e(x), x) greatest fixed point

<=,w

+Table 2-1: SQ expressions

+ +An SQ expression is an SQ expression with no fixed point operations.  An SQ (respectively, SQ) function is

+a function defined in terms of an SQ (respectively SQ) expression.

+In Appendix I, we show that SQ can express all partial recursive functions.  Nevertheless, it is convenient to

+ +consider expressions syntactically outside of SQ but transformable into SQ .  Two such examples are the following

dual forms of deterministic selection:

(6) the s: w <= s | k(s) minimizing s

(7) the s: s <= w | k(s) maximizing s

7
+where k(s) denotes an arbitrary SQ predicate.  Specification (6) denotes the minimum element s >= w with respect to
partial ordering <= such that predicate k(s) holds.  Specification (7) denotes the maximum element s <= w for which
k(s) holds.  In either case, if there is no unique solution, then the expression value is undefined.  These expressions

+ +are transformable into SQ whenever k(s) can be turned into the form s = f(s), where f(s) is an SQ function.

In dealing with systems of equations it is sometimes convenient to use the following notation

(i) the x ,...,x : w <= x ,...,w <= x1 n1 n 1 1 n n

| x = f (x ,...,x )1 1 1 n

...x = f (x ,...,x )

n n 1 nminimizing x ,...,x

1 n
+ +to  represent  the SQ expression, LFP ([f (x),  ..., f (x)], x),  where f (x), i  =  1,...,n,  are SQ functions  and <= is1 n i<=,w

component-wise comparison (using <= for the ith components, i=1,..,n) of n-tuples.  We also use the notationi

(ii) the x ,...,x : x <= w ,...,x <= w1 n1 n 1 1 n n

| x = f (x ,...,x )1 1 1 n

...x = f (x ,...,x )

n n 1 nmaximizing x ,...,x

1 n
to stand for GFP ([f (x), ..., f (x)], x).  If n-tuple [z ,...,z ] is the value of expression (i), then any n-tuple [y ,...,y ]1 n 1 n 1 n<=,w

that  satisfies  the n inequalities  and  the n equalities  in  (i)  must  also  satisfy z <= y ,  j  =  1,...,n.  Expression  (ii)  isj jj
defined analogously.  If there is no unique minimum (respectively maximum) n-tuple satisfying the inequalities and
equalities in (i) (respectively (ii)), then the expression is undefined.

+We  will  sometimes  employ  several  convenient  abbreviations  for SQ expressions.  Multi-variate  function

application f([x ,...,  x ])  is  abbreviated f(x ,..,x ),  and  multi-variate  multi-valued  map  application f{[x ,...,x ]}  is1 n 1 n 1 n
abbreviated f{x ,...,x }.  It  is  useful  to  abbreviate  set  operations s E` {x}  and s -  {x}  by s with x and s less x,1 n
respectively.  It is sometimes useful to define and apply nonrecursive functions (without procedure parameters) with

+call-by-name semantics.  Thus, if e(x) is an SQ expression that depends on variable x, we can define function f(x) =

e(x), and use f as if it were a finite map; e.g., if s is a set, then we can use image set notation f[s] to abbreviate the set
{e(x): x I^ s}.  The following kind of set former

{x I^ s | k(x) minimizing f(x)}

+can be used to abbreviate the more cumbersome SQ expression

{x I^ s | k(x) and f(x) = min/{f(y): y I^ s | k(y)}}

+We  can  attempt  to  provide SQ with  an  operational  semantics  using  a  lower  level  imperative  language

containing  assignment  statements,  conditional  statements, while-loops,  and  other  control  structures.  Assignment
statements of the form x := x op y can be abbreviated x op:= y.  Hence, set element addition is denoted by s with:= x,
and element deletion is denoted by s less:= x.  We use the basic control structure

(for x I^ s)block(x)

end
to execute block for all values of x belonging to s.  If s is a set, then we execute block for each value of x without
repetition and in any order.  If s is a tuple, then block is executed for every component value of s from the first to the
last component.

8
3. Fixed Point Computation

This section summarizes and generalizes the basic fixed point transformations used in the RAPTS system and
the fixed point theory underlying them.  The purpose of these transformations is to turn functions expressed in fixed
point normal forms into executable programs so that their efficiency can be further improved by finite differencing.
In  the  contexts  to  which  they  apply,  the  fixed  point  transformations  to  be  discussed  provide  a  formal  basis  for
problem solving by iteration.  Illustration of these transformations in numerous examples suggests a wide range of
application.  Although  lacking  a  precise  characterization  of  what  functions  can  be  transformed  into  efficient
computations by these techniques, we can, at least, prove that all partially computable functions can be solved as

+fixed point computations.  The proof is given in Appendix I by expressing a Turing Machine in SQ .

Although  the  theory  and  transformations  to  be  described  in  this  section  allow  us  to  compute  fixed  points
within general posets and semilattices, it is convenient and useful to illustrate many applications with collections of
sets.  This is because

1. the set is one of the simplest and most commonly used aggregate data objects;
2. the  compact  but  powerful  operations  of  set  theory  can  be  used  naturally  to  express  combinatorialalgorithms succinctly;

3. basic set operations frequently satisfy the conditions of these transformations (e.g. monotonicity);
4. it is  often possible to  recognize the transformational conditions automatically; this will be discussedlater.

For succinctness we will develop our theory and give transformations for least fixed points.  To obtain dual
forms of theorems, definitions, conditions, and transformations so that they apply to greatest fixed points, we need
to  switch LFP and GFP,  >  and  <, <= and >=, 0 and 1, U' and U`, ACC and DCC,  inflationary  and  deflationary,

+ -minimizing and maximizing, and w -finite and w -finite.  Explicit transformations for greatest fixed points can be

found in [9].

3.1. Basic Theory

All  of  our  fixed  point  transformations  are  derived  from  the  following  theorem  and  corollary,  which  can  be
derived from Tarski's more general Theorem [75] or its constructive reformulation due to Cousot and Cousot [18]:

Theorem  1: (Paige  and  Henglein [56])  Let  (T,<=)  be  a  poset  with  a  unique  minimum  elementidesignated 0. Let f: T o"" T be a monotone computable function.  Then the set {f (0): i = 0, 1,... } is finite iff

kthere exists an integer k >= 0 such that f (0) = LFP(f).

In the reachability problem (cf. Example 1), what we really need is not the least fixed point, but the least fixed
point  that  contains  the  source  set w.  We  call  this  a conditional  least  fixed  point.  In  general,  we  use  the  term
LFP (f(s), s)  to  denote  the  conditional  least  fixed  point  of f that  is  greater  than  or  equal  to w.  The  following<=,w
Corollary extends Theorem 1 to conditional least fixed points.

Corollary 2: Let (T,<=) be a poset . Let f: T o"" T be a monotone computable function, w I^ T, and wi k<= f(w).  Then the set {f (w): i = 0, 1,... } is finite iff LFP (f) = f (w) for some integer k >= 0.

<=,wProof:
The  subspace T' =  {x I^ T | x >= w}  together  with  relation <= form  a  poset  with  minimumelement w.  Since f maps T' into itself, Theorem 1 applies.

n

Since any function f, as defined in Corollary 2, must be inflationary at 0, Corollary 2 is a generalization of Theorem

9
1.  The inflationary condition w <= f(w) in Corollary 2 is important.  Without that, the conclusion may be incorrect, as
is shown by the following example:

Example 2: Consider a relation f = {[a, a], [a, b], [b, c], [c, c]}.  Let w = {b, c}.  Then LFP (f[s], s)I',w

kmust be s = {a, b, c}.  But f (w) = {c} for any k >= 1. n

In  order  to  design  and  implement  program  transformations  based  on  Corollary  2,  it  is  useful  to  consider

ivarious properties that imply the finiteness of the set {f (w): i=0,1,...}.

Theorem 3: Let (T,<=) be a poset. Let f: T o"" T be a monotone computable function, w I^ T, and w <=if(w).  Then the set {f (w): i = 0,1,...} is finite if any one of the following conditions holds:
1. either of the sets {x I^ T | w <= x} or {x I^ range f | w <= x} is finite;
2. either of the sets {x I^ T | w <= x} or {x I^ range f | w <= x} satisfies ACC;

+3. f has  a w -finite  fixed  point  greater  than  or  equal  to w either  with  respect  to T or  to  the  poset
(range f, <=);

4. The  poset (T, <=) is a join  semilattice and  function f has the form f(x) = x U' g(x),  where the set{g(x): x I^ T | w <= x} is finite.

5. The poset (T, <=) is a join semilattice, function f has the form f(x) = x U' g(x), g is monotone, andthe set {g(x): x I^ T | w <= x} satisfies ACC.

Proof: 1, 2 and 3 are simple. 4 is true because there can be only a finite number of new points thatiresult from taking joins of points in the range of g; i.e., the set {f (w): i = 1, 2,... } = { w U' g(w) U' ... U'
i-1 i i-1g(f (w)): i = 1,2,...} is finite.  For condition 5 monotonicity of g implies that f (w) = w U' g(f (w)), i =

i-11,2,...  Also by monotonicity of g and ACC property, the sequence g(f (w)), i = 1,2,..., can only form a
i-1finite ascending chain.  Hence, the set {w U' g(f (w)): i = 1,2,...} must be finite. n

i +It  is  interesting  to  note  that  the  set  {f (w): i=0,1,...}  can  be  finite  while LFP(f)  is  not w -finite.  It

i i+1might be the case that there is an infinite ascending chain between f (w) and f (w) for some i >= 0.

Example 3:  (Cycle  detection)  A  finite  directed  graph e contains  a  cycle  iff  the  largest  subset s of
vertices  each  containing  a  successor  belonging  to s is  nonempty.    A  more  formal  specification  of  cycle
detection is,

(8) (the s: s I' domain(e) E` range(e) |("x I^ s | e{x} C, s z' {}) maximizing s) z' {}

Since specification (8) is equivalent to the test of whether the greatest fixed point of the monotone expression

s - {x I^ s | e{x} C, s = {} }
(that  is  also  a  subset  of domain(e) E` range(e))  is  nonempty,  we  can  compute  this  fixed  point  efficiently
according to the dual forms of condition 1 of Theorem 3 and Theorem 1. n

Example 4: Consider the problem of graph reachability again. The set of vertices p that are reachable
from w along edges in e can be found by initializing p to w, and storing successive values of p E` e[p] into p
until p = p E` e[p].  Since  the  range  of e is  finite,  condition  4  of  Theorem  3  guarantees  that  after  a  finite
number of steps, p will be the least fixed point of s E` e[s] in the space of all sets that include w. n

Example 5: Kildall's  form  of  the  constant  propagation  problem [44]  satisfies  the  dual  form  of
condition 2 of Theorem 3.  For this example T is an uncountably infinite space of finite functions h: N o"" (F
E` {I}), where N is a finite set of nodes in a directed graph that models control flow, F is the set of partially
defined finite functions f: V o"" R, V is a set of program variables, R represents the set of real numbers, and I
is a special maximum element of F (which is ordered by set containment) with the property that " g I^ F, g

10
I' I.  If T is ordered by node-wise set containment (i.e, " h , h I^ T, h <= h iff " n I^ N, h (n) I' h (n)), then1 2 1 2 1 2

5we see that a DCC holds and that the length of the longest chain in T is 1 + (#N)(#V). n

Many of the algorithms that perform global program optimization were designed and proved correct by first
formulating them as fixed point computations.  In the early development of this field Cocke and Schwartz [13] and
Kildall [44] justified convergence of their algorithms using condition 1 of Theorem 3 and restricted function f to be
distributive;  i.e.,  for  every x,y I^ T, f(x U' y)  = f(x) U' f(y).  (Note  that  distributivity  implies  monotonicity,  but
monotonicity  does  not  imply  distributivity.)    Tenenbaum [77]  and  Kam  and  Ullman [42, 41]  later  designed
algorithms  based  on  the  more  general  condition  2  of  Theorem  3  and  the  more  general  monotonicity  property  for
function f.

3.2. Basic Fixed Point Transformations

According  to  Corollary  2  a  straightforward  algorithm  to  compute LFP (f)  initializes p to w,  and  then<=,w
repeatedly computes a new value of p by assigning f(p) to p until p does not change.  The final value of p is the
solution.  Although such an iterative procedure may be efficient, it may also be highly inefficient in our set theoretic
applications, because of the potentially costly redundancy in the recomputation of f(p) each iteration.  For example,
when applied to the reachability problem (cf. Example 1), the repeated recomputation

(9) p := p E` e[p]
is  unsatisfactory  because  the  new  approximation  of p is  completely  recomputed  and  copied  each  iteration,  even
though it  may differ  only  slightly  from  its old value.  Another shortcoming with the iterative  step (9) is that  it  is
biased towards a breadth-first search strategy.

The following Theorem illustrates two forms of nondeterministic iteration that can overcome both problems.

Theorem 4: Let (T,<=) be a poset.  Let f: T o"" T be a monotone computable function, w I^ T, and w <=f(w).  Let s ,...,s ,... be any sequence such that

0 i1. s = w;
0
2. s I^ {x I^ T | s <= x <= f(s )}, i = 0,1,...  (Note that such sequences always exist for computablei+1 i ifunctions

f that are monotone and inflationary at w.)

Then we conclude the following.
(a) If there exists an integer k >= 0 such that s = f(s ), then s = LFP (f).k k k <=,w

+(b) If LFP (f) is w -finite, and if s < s whenever s z' f(s ), then there exists an integer k >= 0 such thati i+1 i i<=,w
s = f(s ).k k

Proof: (a) We use a simple dominated convergence argument.  By assumption, s is a fixed point ofk
f that  is  greater  than  or  equal  to w.  Let p >= w be  any  other  fixed  point  of f.  Then  by  condition  2  andproperties of f,

iw <= s <= f (w) <= p, i = 0,1,...i
Hence, s = LFP (f).  Part  (b)  follows  immediately  from  the  proof  of  part  (a)  and  the  definition  ofk <=,w

+w -finite. n

Conditions  (a)  and  (b)  of  Theorem  4  imply  two  forms  of  iteration.    In  the  most  general  form  satisfying

5Of course, any algorithm that attempts to solve this problem would only approximate real numbers by using finite representations (as can be
done with rationals).

11
condition (a), we have a nondecreasing sequence s , i=0,1,..., that converges to a fixed point of f after a finite numberi
of steps.  (Note that the sequence does not have to be strictly increasing prior to convergence.)  Thus, in order to
ensure that a sequence satisfies condition (a), we must prove directly that such convergence occurs.  We refer to this
as the 'operational' approach.

Based  on  iteration  according  to  condition  (b)  we  can  develop  a  robust,  and  perhaps  easier,  'algebraic'
approach to fixed point computation.  In this approach sequences are strictly increasing and must converge because

+of w -finiteness.

We formalize the way in which sequences are generated in the algebraic approach as follows.  Let (T, <=) be a
poset, and let S be a nonempty set.  A partially defined function \Delta : T * T o"" pow(S) is called a workset function if
\Delta (q, p) = {} U^ q <= p for all [q,p] I^ domain \Delta .  A partially defined function d: T * S o"" T is called an increment

6function if d(p,z) >= p for all [p,z] I^ domain d.  The two functions \Delta  and d are said to be feasible relative to a partial

function f: T o"" T at a point w I^ T if the following conditions hold:

1. s U' f(s), \Delta (f(s), s), and d(s, z) " z I^ \Delta (f(s), s) are defined;

2. "z I^ \Delta (f(s), s) | s < d(s, z) <= s U' f(s);
for all s belonging to every sequence,

(10) s = w0s

= s if \Delta (f(s ),s ) = {} andi+1 i i i

s I^ {d(s , r): r I^  \Delta (f(s ),s )} otherwise, i = 0,1,...i+1 i i i

If \Delta  and d are feasible relative to function f at point w, then any sequence (10) is said to be generated by \Delta  and d at
w.

Transformation 1: Let f: T o"" T be a monotone computable function, where (T, <=) is a poset.  Let w I^ T

+and w <= f(w).  If LFP (f)  is w -finite  and \Delta  and d are  feasible  relative  to f at w,  then  the  following<=,w

transformation is correct:

p := LFP (f)<=,wTh

(11) p := w(while $ z I^  \Delta (f(p), p))

p := d(p, z)end

Proof: The successive values assigned to p in program (11) form a sequence s , i=0,..., generated byifunctions \Delta  and d, which are feasible relative to f at w.  By definition  of feasibility, such  a sequence  is

+strictly increasing as long as f(s ) z' s ; also s <= f(s ), i=0,...  Since LFP (f) is w -finite, by Theorem 4i i i+1 i <=,w
(b) there exists a finite integer j >= 0 such that s = LFP (f).  Hence, f(s ) = s , which implies that \Delta (f(s ),j j j j<=,w
s ) = {}, since \Delta  is a workset function.  Consequently, code (11) halts after j iterations of the while loop. nj

For a poset (T, <=) and monotone computable function f: T o"" T that is inflationary at w I^ T, we can always
choose S = T and the following functions feasible relative to f at w:

(12)  for all p, q I^ T, where q U' p is defined,\Delta (q, p) = {} if q <= p, and {q} otherwise;

d(p, q) = p U' q;

Using functions (12) Transformation 1 leads to the conventional iteration implied by Corollary 2; that is,

6In its dual formulation for greatest fixed points the feasible functions include a workset function \Delta  that must satisfy the condition \Delta (q, p) = {}
U^ q >= p for all [q,p] I^ domain \Delta .  However, instead of an increment function we use a decrement function d that must satisfy the conditiond(p,z) <= p for all [p,z] I^ domain d.

12
(13) p := w(while f(p) > p)

p := f(p)end

If we redefine \Delta ,

\Delta (q, p) = {z: p < z <= q U' p} if q U' p is defined
then we obtain precisely the nondeterministic iteration given in Theorem 4 (b).

Transformation 1 turns functional programs involving only input and output variables into imperative ones by
introducing an assignment statement and the intermediate variable p.  In the case of (11), the value of p starts out at
w and each transition from an old value of p, p , to a new value of p, p , is governed by the invariant p I^old new new
h(p ), where h(p) = {d(p, z): z I^ \Delta (f(p), p)}.  Since the value of p can be chosen from a set of values, variousold new
transformations can be tailored from (11) to make this choice of values based on highly efficient strategies.

One guiding principle in designing efficient strategies with Transformation 1 is to avoid the potentially costly
computation \Delta (f(p), p).  This can sometimes be achieved using finite differencing to preserve a program invariant
that  keeps  the  value  of \Delta (f(p), p)  stored  at  the  point  where  it  is  needed  within  the while-loop  predicate.    This
approach is profitable whenever the cumulative cost of preserving such an invariant is asymptotically lower than the
cost of computing \Delta (f(p), p) each time through the loop.  Of course, it is also useful to keep the size of the set \Delta (f(p),
p) down to conserve space.  Another more vaguely stated principle is to generate the next element d(p, z) in a way
that  makes  progress  at  minimal  cost.    This  can  sometimes  be  achieved  by  choosing  the  next  element p bynew
augmenting the current element p with a minimal increment dp such that p = d(p ,dp) > p .  Of course, weold new old old
should also exploit any local simplifications or optimizations to implement assignment p := d(p,dp) as efficiently as
possible.

Below we illustrate Transformation 1 with a few computable problems discussed in [3]:

Example 6: (Reachability continued) If we define f(s) = e[s] E` s, then f must be inflationary at w.  It is also
easy to see that workset function \Delta (f(s), s) = f(s) - s and increment function d(s, z) = s with z are feasible
relative to f at any vertex set w.  Hence, by Transformation 1 we can compute the set of vertices p that are
reachable from w along edges in e by initializing p to w and repeatedly augmenting p with a single arbitrary
element selected from e[p] - p, until e[p] - p is empty.  Such a nondeterministic reachability algorithm can
subsequently be refined into a variety of strategies with efficient implementations. n

Example 7: (The  single  source  shortest  path  problem)  Consider  a  graph G =  (V, E),  where V is  a  set  of
vertices and E is a set of ordered pairs representing edges.  Given a source node s I^ V and a weight function

+c that  maps E to  nonnegative  reals R extended  with  a  maximum  element e^,  we  want  to  compute  the

function d = {[i, a]: i I^ V, a is the length of the shortest path from node s to node i}.  If we consider the poset

+of all functions m: V o"" R under the ordering m <= m iff ("i I^ V | m (i) <= m (i)), then the solution d is the1 2 1 2

-1greatest fixed point of the function f(m) = {[i, a]: i I^ V, a = min/{m(i), min/{m(j) + c(j, i): j I^ E {i}}}} that

is less than or equal to {[s, 0]} E` {[i, e^]: i I^ V | i z' s}.  With suitable feasible functions \Delta  and d we could
apply the dual form of Transformation 1 to obtain the following naive algorithm to compute d:

d := {[s, 0]} E` {[i, e^]: i I^ V | i z' s} -1
(while $ [k,a] I^ {[i,q]: i I^ V, q = min/{d(j)+c(j,i): j I^ E {i}}| d(i) > q})

d(k) := aend

The preceding code solves the problem but is too inefficient.  Even after finite differencing is applied
to avoid the cost of computing \Delta , this algorithm would converge too slowly.  However, if we choose

13

-1\Delta (f(d),d) = {[i, a]: i I^ V, a = min/{d(j) + c(j, i): j I^ E {i}}
| d(i) > a minimizing a},

then it will converge within n iterations. n

The next two examples illustrate the two different approaches suggested by Theorem 4 to compute the same
fixed point.  The first example illustrates the algebraic approach, and the second example illustrates the operational
approach.

Example 8: (Single function coarsest partition problem) If s is a finite set, then a partition P of s is a set of
pairwise disjoint subsets of s whose union is all of s.  The elements of P are called its blocks.  If P and Q are
two partitions of s, then Q is a refinement of P (denoted by Q <= P) if every block of Q is contained in a block
of P.  Observe that the space of partitions over s forms a lattice with maximum element {s} and minimum
element {{x}: x I^ s}.

The Single Function Coarsest Partition Problem inputs a finite set s, an initial partition P of s, and a
total function h on s.  It outputs the coarsest (i.e., maximal) refinement Q of P such that " b I^ Q $ d I^ Q |
h[b] I' d.  In [54] this problem is reformulated as computing the greatest fixed point (that is also a refinement
of P) of the following monotone function

-1 -1f(Q) = {b C, h [q]: b I^ Q, q I^ Q | b C, h [q] z' {}}

which maps a partition Q into a refinement of Q.

The first step in a derivation of an efficient algorithm is to apply the dual form of Transformation 1
with the following feasible functions relative to f at P (with a feasibility proof left to the reader):

7\Delta (f(Q),Q) = Q - f(Q)

and

d(Q,q) = (Q - {q}) E` {q - b, b} 8where b I^ {x I^ f(Q) | x I` q and #x <= #q/2}

The  preceding  feasible  functions  combine  Hopcroft's  `choose  the  smaller  half'  strategy  with  the
double partition approach that Paige and Tarjan used to solve the relational coarsest partition problem [58].
Application  of  finite  differencing  to  preserve  the  values  of f(Q)  and Q - f(Q)  incrementally  leads  to  an
algorithm with the same O(nlogn) time bound as Hopcroft's [38]. n

It is interesting to consider an alternative operational derivation of an algorithm to solve the single function
coarsest partition problem based on Theorem 4 (a).

Example 9: According to Example 8, we want to compute GFP (f).  Consider the following function,<=,P

-1 -1split(P,b) = {t C, h [b]: t I^ P | t C, h [b] z' {} }
-1 -1E` {t - h [b]: t I^ P | t - h [b] z' {} }

which is easier to compute than f(P).  Since P >= split(P,b) >= f(P), " b I^ P, then any sequence of partitions,
P = P and P = split(P,b) where b I^ P , i=0,1,..., is nonincreasing and satisfies the general conditions ofi0 i+1
Theorem 4.

Such a sequence of partitions P is generated by an initial partition and the following nondeterministic
code:

7\Delta (f(Q),Q) stores those blocks of Q not belonging to f(Q).
8Note that d(Q,q) is nondeterministic in the sense that block b is any block of refinement f(Q) that has no more than half the elements of block
q of partition Q, where b I` q.

14
(14) Q := {s}(while $ q I^ (Q - P), b I^ P | b I` q and #b <= #q/2)

P := split(P,b)Q := (Q - {q}) E` {q - b, b}
end
Code  (14)  preserves  the  invariants Q >= P and "t I^ Q | P = split(P,t).  This  code  terminates  with Q = P,
because the sequence of Q partitions generated forms a descending chain in a finite lattice.  Since split(P,b) =
P, " b I^ P, iff P is a fixed point of f, then by Theorem 4 (b) and (a) the final value of P is GFP (f). n<=,P

It is interesting to note that application of the split function in code (14) preserves the invariant P = f(Q) U` P ,0
where P is the initial value of P.  Also, the modification to Q occurring within (14) is an efficient implementation0
of d(Q,q) in Example 8.  Hence, the implementations that would be derived in Examples 8 and 9 are much the same.

In  the next  two subsections we tailor  efficient variants of  fixed  point Transformation 1  for  special  kinds of
functions and posets.

3.3. Special Functions

It is useful to refine Transformation 1 to compute fixed points for several particular kinds of functions f(s).

kThese  include  the  'inductive'  form s U' g(s)  and  its  generalization U' g (s),  the  tupling  form  [f (s),...,f (s)],i 1 ni=1

parameterized forms g(s,...,s) with m occurrences of variable s within function g, composition h g, and the fixedo
point form LFP (g(s,t),t).>=,w

Let us first consider a special case of Transformation 1 when f(s) = s U' g(s).

Transformation 2: (Inductive form)
Let f: T o"" T be a monotone computable function f(s) = s U' g(s), where (T, <=, U') is a join semilattice with a

+minimum element 0 and w I^ T.  If LFP (f) is w -finite and functions \Delta  and d are feasible relative to g at w,<=,w

then the following transformation is correct:

p:= LFP (s U' g(s),s)<=,w
Th(15) p := w

(while $ z I^  \Delta (g(p), p))p := d(p, z)
end
Proof: The successive values assigned to p in program (15) form a sequence s , i=0,..., generated byifunctions \Delta  and d,  which are  feasible  relative to

g at w.  By  definition  of feasibility  such a sequence  isstrictly  increasing  as  long  as g(s )  /<= s .  But g(s )  /<= s iff f(s )  = s U' g(s ) z' s .  Also  by  definition  of

i i i i i i i ifeasibility,

"r I^  \Delta (g(s ),s ) | s < d(s , r) <= s U' g(s ) = f(s ), i=0,...i i i i i i i

+so  that s <= f(s ), i=0,...  Finally,  since LFP (f)  is w -finite,  by  Theorem  4  (b)  there  exists  a  finitei+1 i <=,w

integer j >= 0 such that s = LFP (f).  Hence, f(s ) = s , which implies that g(s ) <= s so that \Delta (g(s ), s ) =j j j j j j j<=,w{}, since \Delta  is a workset function.  Consequently, code (15) halts after

j iterations of the while loop. n

In  many  applications  Transformation  2  is  more  convenient  than  Transformation  1,  because  the  inflationary
condition w <= w U' g(w)  holds  automatically  for  inductive  functions f(s)  = s U' g(s).  We  need  only  to  check  the
monotonicity of s U' g(s), which is guaranteed when g(s) is monotone.  Transformation 2 can be used to derive the
reachability solution as presented earlier in Example 1.

The  similarity  in  the  programs  resulting  from  applying  Transformations  1  and  2  suggests  that  under  some

15
conditions, functions f(s) and s U' f(s) have the same fixed point.  Indeed, we can reformulate Theorem 4.1 of Cousot
and Cousot [18] without their condition that T be complete:

Theorem 5: Let (T,<=) be a join semilattice.  Let f: T o"" T be a monotone computable function. Let
c = LFP(w U' f(s),s)1
c = LFP (s U' f(s),s)<=,w2
c = LFP (f)<=,w3

Then

1. If c is defined, then c is defined and c = c ;2 1 1 2

2. If c is defined and T satisfies DCC, then c is defined;1 2
3. If w <= f(w), then w U' f(s) and f have the same set of fixed points that are greater than or equal tow, and thus c = c when they are defined.

1 3Proof: We prove 2 only.  For 1 and 3 see Cousot and Cousot [18].

We show that c is a fixed point of h(x) = x U' f(x), and any other fixed point of h that is greater than1
or equal to w is also greater than or equal to c .  Since c = w U' f(c ), then c >= w, and c = c U' f(c ).  Let1 1 1 1 1 11

2g(x) = w U' f(x).  Since f is monotone, so is g.  If s I^ T and s = s U' f(s) >= w, then s >= g(s) >= g (s) >= ... by
k kmonotonicity  of g.  Since T satisfies DCC,  then  there  exists  an  integer k >= 0  such  that g (s)  = g(g (s)).
kHence, g (s) is a fixed point of g belonging to T.  Since c is the least fixed point of g belonging to T, then1

ks >= g (s) >= c .  Thus, c is the least fixed point of h that is greater than or equal to w, and c = c . n1 1 2 1

-nExample 10: We give an example where c is defined but c is not.  Let T = { 1 - 2 : n = 0, 1, 2,...}3 2
-nE` { 1 + 2 : n = 0, 1, 2,...}, let

-n -(n+1)f(1 - 2 ) =  1 - 2 , n = 0, 1,...
-n -(n+1)f(1 + 2 ) =  1 + 2 , n = 1, 2,...

f(2) = 2

Let <= be the numerical ordering of real numbers. Then " a, b I^ T, a U' b = max(a, b).  Let w = 0. Then f is

-nmonotone and w < f(w). It is easy to check that c = 2.  But for any s = 1 + 2 , n >= 0, s = s U' f(s).  Thus, c is3 2

not defined.  This example does not contradict Theorem 5, because T does not satisfy DCC. n
Theorem 5 provides an opportunity to choose between three equivalent specifications c , c , and c .  Of these,1 2 3
c is often the most desirable choice, because it can lead to greatly simplified preprocessing.  For example, in set1
theoretic  applications,  where  the  empty  set  is 0,  application  of  Transformations  1  or  2  to c results  in  the  simple1
initializing  statement p :=  {}.    Consequently,  the  code  introduced  by  finite  differencing  just  before  such  an
initializing statement can be as little as half the preprocessing code introduced for specifications c or c .2 3

Example 11: (Graph reachability continued) In Example 1 the set of vertices s that are reachable from w
was given by the following specification:

the s: w I' s | s = s E` e[s] minimizing s
which is the same as

(16) LFP (s E` e[s], s)I',w
According to Theorem 5, specification (16) is equivalent to

(17) LFP(w E` e[s], s)
Making use of the same feasible functions,

(18) \Delta (f(s), s) = f(s) - sd(s, z) = s with z

16
as in Example 6 but relative to a more desirable function f(s) = w E` e[s] at {}, we can apply Transformation
1 to implement specification (17) with the following program:

(19) p := {}(while $ x I^ ((w E` e[p]) - p) )

p with:= xend

The preprocessing code that would result from applying finite differencing to program (19) is much simpler
than the code that would arise from applying finite differencing to program (5). n

The  following  example  with  nested  application  of  fixed  point  transformations  illustrates  a  more  substantial
benefit to preprocessing.

Example 12: (Interval partitioning) A flow graph e is a digraph with a unique entry node entry from which
there are paths to every vertex in the graph.  An interval is a smallest subgraph of e that contains a unique
entry  point h,  called  a header node,  and  also  contains  each  node x in  the  graph  whenever  it  contains  all
predecessors of x.  Given a flow graph e with entry node entry, we want to partition its nodes into the unique
set ints of intervals.  We can specify the interval intof(head) with header node head as follows:

-1(20) the int: {head} I' int | (" x I^ e[int] | e {x} /I' int or x I^ int)
minimizing int

which can first be transformed into,

-1LFP (int E` {x I^ e[int] | e {x} I' int}, int)I',{head}

and then by Theorem 5 into,

-1LFP({head} E` { x I^ e[int] | e {x} I' int}, int)

Finally, using feasible functions (18) relative to function

-1f(int) = {head} E` {x I^ e[int] | e {x} I' int}

at {}, we can apply Transformation 1 to obtain the following while-loop:

(21) int := {} -1

(while $ z I^ (({head} E` {x I^ e[int] | e {x} I' int}) - int)int with:= z

end
If we define function
g(ints) = {intof(entry)} E` intof[E`/{e[int] - int: int I^ ints}]

9then the unique interval partition of e can be specified LFP(g) .  Once again using feasible functions (18)

but relative to function g at {}, we can apply Transformation 1 to obtain the following implementation:

(22) ints := {}(while $ z I^ (({intof(entry)} E`

intof[E`/{e[int] - int: int I^ ints}]) - ints))ints with:= z
end
However, if we redefine the feasible functions \Delta  and d differently so that

-1\Delta (g(ints), ints) = intof [g(ints)] - E`/ints

10= ({entry} E` e[E`/ints]) - E`/ints

d(ints, z) = ints with intof(z)

then application of Transformation 1 yields,

9Since function intof maps a vertex into a set of vertices that forms an interval, the image of a set of vertices under intof is a partition  of
intervals.

10This definition is valid, because each interval has a unique header.

17
(23) ints := {}(while $ z I^ ({entry} E` e[E`/ints]) - E`/ints))

ints with:= intof(z)end

Finite differencing and data structure selection will transform (22) or (23) into a program that runs in time
O(#e),  but  (23)  is  better  in  the  sense  that  it  contains  only  one  reference  to intof inside  the while-loop.
Because of another reference to intof within the while-loop predicate of code (22), finite differencing would
introduce  a  whole  bunch  of  code  (which  would  be  absent  in  code  that  results  from  (23))  to  compute  the
interval with header entry as part of the preprocessing code before the while-loop. n

An important generalization of the inductive form just discussed is

k(24) f(x) = U' g (x)i=1 i

where g : T o"" T, i=1,...,k.i

Transformation 3:
Let (T, <=, U') be a join semilattice with a unique minimum element 0.  Consider functions g : T o"" T, i=1,...,k,i

k +such that f = U' g is a monotone computable function.  Let w I^ T and w <= f(w).  If LFP (f) is w -finiteii=1 <=,w

and \Delta  and d are feasible relative to g at every element x, w <= x <= LFP (f), i=1,...,k, then the followingii i <=,w
transformation is correct:

kp:= LFP (U' g )<=,w i=1 i

Th(25) p := w

(while $i=1,...,k $ z I^  \Delta  (g (p), p))i i

p := d (p, z)iend

Proof: The  successive  values  assigned  to p in  program  (25)  form  a  sequence s , i=0,...,  which  isi

kstrictly  increasing  as  long  as $ j =  1,...,k | g (s )  /<= s ,  which  is  true  iff f(s )  = U' g (s )  /<= s .  Also  byj i i i i i ii=1

feasibility, we know that $ j = 1,...,k | s <= g (s ), which implies that s <= f(s ) for i = 0,...  Finally, sincei+1 j i i+1 i

+LFP (f) is w -finite, by Theorem 4 (b) there exists a finite integer j >= 0 such that s = LFP (f).  Hence,j<=,w <=,w

f(s ) = s , which implies that g (s ) <= s so that \Delta  (g (s ), s ) = {}, since \Delta  is a workset function, i = 1,..,k.j j i j j i j ji iConsequently, code (25) halts after

j iterations of the while loop. n

A  further  refinement  of  Transformation  3  can  be  used  to  determine  minimum  solutions  to  systems  of
equations and is discussed in greater depth in Section 5.  The basic idea is stated briefly as follows.  Let (T , <= , 0 )i i i

kbe  a  poset  with  minimum  element 0 , i=1,...,k.  Let T = * T be  the  product  poset  with  minimum  elementii i=1

[0 ,...,0 ].  If x and y belong to T, then x <= y iff x <= y , i=1,...,k.  Let w I^ T, and consider k monotone functions f : Ti i i i1 k
o"" T , where f (w) >= w(i), i=1,...,k.  Define k corresponding monotone functions g : T o"" T, i=1,...,k, as follows:i i ii

g (x)(j) = f (x),  if j = ii i0

,  otherwisej

Transformation 3 can then be used to find the least fixed point of

k(26) f(x) = [f (x),...,f (x)] = U' g (x)i=11 k i

Choose S = T and the following feasible functions relative to each g at every x I^ T, where x >= w, i=1,...,k:i

for all p, q I^ T where p U' q is defined,\Delta (q, p)) = {} if q <= p, and {p U' q} otherwise;

d(p, p U' q) = p U' q,
Assuming that all of the posets (T , <= , 0 ), i = 1,...,k, are the same, we obtain the  'chaotic' iteration described byi i i

18
Tenenbaum [77] and Cousot and Cousot [17] (restricted to finite iteration) to compute least fixed points of systems
of equations x = f (x ,...,x ), i=1,...,k; that is,i i 1 k

p := w(while $i=1,...,k | f (p) > p(i) )

ip(i) := f (p)
iend

More generally, if we redefine \Delta  and d in the following way,

\Delta (q, p) = {[i, x]: i = 1,...,k, p(i) < x <= (q U' p)(i)}d(p,[i, x]) = [p(1),..,p(i-1), x, p(i-1),...,p(k)]

we can replace code (25) appearing in Transformation 3 by,

(27) p := w(while $[i,x] I^  \Delta (f(p), p))

p(i) := xend

We can also obtain a somewhat narrower but, perhaps, more convenient iteration than (27).  If workset \Delta  andi
increment d are feasible relative to functions g at all points x I^ T, x >= w, i = 1,...,k, then the following functions \Delta ii i
and d are feasible relative to f at all points x I^ T, x >= w,

(28) " [x ,...,x ], [y ,...,y ] I^ T,1 k 1 k

\Delta ([x ,...,x ], [y ,...,y ]) = {[i, t]: i = 1,...,k, t I^  \Delta  (x , y )}i1 k 1 k i i

d([x ,...,x ], [i, t]) = [x ,...,x , d (x , t), x ,...,x ]i1 k 1 i-1 i i+1 k
Functions (28) lead to a variant of Transformation 3 in which code (25) is replaced by the code just below:

(29) x := w(while $ [i, z] I^  \Delta (f(x), x))

x := d (x , z)ii iend

Thus we have

+Transformation 4: The w -finite solution of system (26) can be computed by (29). n

Solving  systems  of  equations  leads  to  efficient  solutions  to  two  special  functions.    We  can  compute
LFP(g(x,...,x)) for monotone functions g, where parameter x occurs m times, by substituting m distinct identifiers x ,i
i=1,...,m, for the different occurrences of x within g and finding the minimum solution to the system of m equations
x = g(x ,...,x ), i=1,..,m.  Each variable x has the same solution, which is LFP(g(x,...,x)).  For composition of twoi 1 m i
monotone functions h and g, the solution to LFP(h g) is the same  as the solution to variable s when we take theo
minimum solution to the two equations, t = g(s) and s = h(t).

Transformation 3 can also be used to compute the least common fixed point greater than or equal to w of a
family F of functions, denoted by LFP (F).<=,w

Theorem 6: Let (T,<=,U',0) be a semilattice and w I^ T.  Let g : T o"" T, i=1,...,k, be a family F ofimonotone, inflationary, computable functions.  Let

h be the composition of these k functions in any order,kand let g = U' g .  If T has an ACC, then the least common fixed point LFP (F) exists, and LFP (F)

ii=1 <=,w <=,w= LFP (g) = LFP (h).
<=,w <=,wProof: Since each function g , i=1,...,k, is monotone, inflationary, and computable, so is g.  Then, by

iTheorem 3 and Corollary 2, p = LFP (g) is defined and can be computed.  Since p = g(p) >= g (p) >= p,

j<=,wj=1,...,k, then p is also a common fixed point of F.  Clearly any common fixed point of F that is greater

than or equal to w is also a fixed point of both g and h.  Let q >= w be a fixed point of h.  Without loss ofgenerality, suppose that h = g g ... g .  Because the functions belonging to F are all inflationary, q <=

o o ok k-1 1g (q) <= g (g (q)) <= ... <= h(q) = q.  Hence, q is a common fixed point of F.

n1 2 1

19
Based on Theorem 6 we can show that two seemingly different classical methods of global program analysis
are  identical.  Kildall [44]  introduced  a  fairly  general  method  for  program  analysis  using  iterative  schema  (27).
Kildall's  algorithm  was  later  refined  by  Tenenbaum [77]  and  Kam  and  Ullman [42].  Cousot  and  Cousot  used  a
strategy similar to this algorithm called 'chaotic' iteration for defining least fixed points as the limit of a sequence,
and they applied it in new settings [16, 15].

Let e represent the edges of a program control flow graph with nodes labelled 1, ..., k (cf. Example 5).  For
each flow graph node j = 1, ..., k, let x be a variable storing some program fact at node j, and let g be a monotonej j
flow function defined at node j.  In the flow analysis frameworks of Tenenbaum and Kam and Ullman, the goal is to
solve the least fixed point of the following system of equations:

(30) x = U' g (x ), i = 1, ..., ki j I^ e{i} j j

According to Theorem 5, the least fixed point of system (30) can be rewritten equivalently as the least fixed
point of

x = x U' (U' g (x )), i = 1, ..., ki i j I^ e{i} j j
which is the same as the least fixed point of

(31) x = U' (x U' g (x )), i = 1, ..., ki j I^ e{i} i j j
Finally, by Theorem 6, the least fixed point of the system of equations (31) is equivalent to the least fixed point of
the following system

(32) x = x U' g (x ), i = 1, ..., k, j I^ e(i)i i j j
which is precisely the system that Kildall used.

The next example illustrates a more interesting application of Theorem 6 to algorithm derivation.

Example 13: (Many  function  coarsest  partition  problem)  The  Many  Function  Coarsest  Partition  Problem
inputs a finite set s, an initial partition P of s, and a family of total functions h on s, i=1,...,k.  It outputs thei
coarsest  (i.e.,  greatest)  refinement Q of P such  that " b I^ Q " i=1,...,k $ d I^ Q | h [b] I' d.  It  isi
straightforward  to  reformulate  this  problem  as  computing  the  greatest  common  fixed  point  (that  is  a
refinement of P) of the family of functions

-1 -1g (Q) = {b C, h [q]: b I^ Q, q I^ Q | b C, h [q] z' {}}i i i

ki=1,...,k.  Since each function g is monotone and deflationary, the solution is to compute GFP(P U` (U` g ))i ii=1

according to the dual forms of Theorem 6 and Theorem 5.

kLet f = P U` (U` g ).  The first step in a derivation of an efficient algorithm is to apply the dual form ofii=1

Transformation 3 with the following feasible functions relative to f at P:

\Delta (f(Q), Q) = Q - f(Q)
and

d(Q,q) = (Q - {q}) E` {q - b, b}where b I^ {x I^ f(Q) | x I` q and #x <= #q/2}

This leads to the algorithm just below,

(33) Q := {s}(while $ q I^ (Q - f(Q)) $ b I^ f(Q) | b I` q and #b <= #q/2)

Q := (Q - {q}) E` {q - b, b}end

Finite differencing can be used to improve code (33) by preserving the invariant P = f(Q) appearing in the
condition of the while-loop.  If we define for i=1,...,k,

20
-1 -1split (P,b) = {t C, h [b]: t I^ P | t C, h [b] z' {} }i i i
-1 -1E` {t - h [b]: t I^ P | t - h [b] z' {} }i i

then the improved code is

Q := {s}(while $ q I^ (Q - P) $ b I^ P | b I` q and #b <= #q/2)

(for i = 1,...,k)P := split (P,b)

iend

Q := (Q - {q}) E` {q - b, b}end

Further applications of finite differencing and appropriate data structuring will yield an algorithm that runs in
O(nklogn)  time  with O(n)  auxiliary  space.    Note  that  Hopcroft's  algorithm  had  a  similar  time  bound  but
required \Omega (nk) space in the worst case [38].

We  can  also  consider  an  alternative  derivation  that  leads  to  an  algorithm  with  the  same  asymptotic
worst  case  time  but  with  fewer  split  operations  and O(nk)  space.    Using  the  method  of  parameterized
functions, we can rewrite the arguments of function f with k distinct parameters, so that

kf(Q ,...,Q ) = P U` (U` g (Q ))i=11 k i i

and  solve  the  resulting  systems  of  equations  using  feasible  functions  as  before.    The  result  appears  just
below:

(34)  (for i=1,...,k)Q := {s}

iend

(while $ i=1,...,k $ q I^ (Q - f(Q ,...,Q ))i 1 k$

b I^ f(Q ,...,Q ) | b I` q and #b <= #q/2)1 k
Q := (Q - {q}) E` {q - b, b}i i
end

As  in  the  previous  derivation,  code  (34)  can  be  improved  by  using  the split functions  to  preserve  the
invariant P = f(Q ,...,Q ); that is,1 k

(35)  (for i=1,...,k)Q := {s}

iend

(while $ i=1,...,k $ q I^ (Q - P)i

$ b I^ P | b I` q and #b <= #q/2)P := split (P,b)
iQ := (Q - {q}) E` {q - b, b}
i iend

Further  applications  of  finite  differencing  and  data  structure  selection  are  straightforward  to  complete  the
derivation.  The  resulting  algorithm  was  suggested  in [58]  and  comes  closer  to  Hopcroft's  original
algorithm [38].  Gries  gave  a  more  complete  but  lower  level  top-down  (almost  transformational)  proof  of
Hopcroft's algorithm [31]. n

Computing fixed points for functions of the form f(w,s) = LFP (g(s,t),t) are discussed later.>=,w

3.4. Special Data Types

It is also worthwhile to refine Transformation 1 with respect to different data types.  We have already given
examples of fixed point computations on set lattices, partition lattices, function posets, and so forth.  In this section
we consider a simple hierarchy of semilattices and corresponding methods for computing fixed points.

21
Let (L, <=, 0) be a join semilattice with a unique minimum element 0.  A non-0 element a I^ L is called an atom
if " b I^ L, b <= a implies b = 0 or b = a.  For example, in the powerset lattice over a finite set s, the atoms are the
singleton sets {x}, for all x belonging to s.  In the lattice of partitions over a finite set s (cf. Example 8), the atoms
are the partitions

{{x}: x I^ (s-{a, b})} E` {{a,b}},
for all doubleton sets {a,b} I' s.

For each element w I^ L, we define the decomposition of w to be
dec(w) = {a <= w | a is an atom}.
If for all elements w I^ L,

11w = U'/dec(w)

then we say  that L is decomposable.  The  Shortest Path  Problem  in  Example 7  is  defined  on a  nondecomposable
lattice.

If L is decomposable, then the following properties hold for all a, b I^ L:
1. a <= b U^ dec(a) I' dec(b)
2. a U' b = U'/(dec(a) E` dec(b))

If L is decomposable and dec(w) is finite for all w I^ L, then we say that L is finitely decomposable (abbr. FD).
Kildall's  Constant  Propagation  Problem  from  Example  5  illustrates  a  decomposable  lattice  that  is  not  finitely
decomposable because of the maximum element I.

If L is FD, then the following additional property holds:
3. a U` b = U'/(dec(a) C, dec(b))
and therefore L is a lattice.  We also know that

4. dec(a U` b) = dec(a) C, dec(b)
For FD lattices we can define difference,

5. a - b = U'/(dec(a) - dec(b))
with the property that

6.  (a U` b) U' (a - b) = a
For  FD  lattices  the  set  theoretic  arbitrary  selection  operation ' generalizes  to  atom  selection.  Of  particular
importance, if a is an atom and a I" dec(T), then T U' a > T.  However, if a is an atom and a I^ dec(T), it does not
follow that T - a < T.

If L is FD and the dual of property 4 holds; i.e.,
7. dec(a U' b) = dec(a) E` dec(b),
then for any finite set A of atoms, we have

dec(U'/A)= E`/{dec(t): t I^ A}

= E`/{{t}: t I^ A}= A

Consequently, each element  belonging to L is uniquely representable by the join of a finite set of atoms.  Such a
lattice is said to be uniquely finite decomposable (abbr. UFD).  The partition lattice used in Example 8 is FD but not
UFD.

For UFD lattices, difference a - b defined in (5.) above is the unique minimum element d such that (a U` b) U' d
= a, and if a is an atom and a I^ dec(T), then T - a < T.  UFD lattices also have the following property,

11We define U'/{} = 0.

22
8. dec(a - b) = dec(a) - dec(b)
If L is a UFD lattice and A = {a I^ L | a is an atom of L}, it follows from the preceding discussion that dec is an
isomorphic map from (L, <=, U', U`, -, 0) to ({x I' A | x is finite}, I', E`, C,, -, {}); and for any set A, ({x I' A | x is
finite}, I', E`, C, ) forms a UFD lattice.  Thus, we have the following useful transformation for UFD lattices:

Transformation 5:  Let f: T o"" T be  a  monotone  computable  function  defined  on  UFD  lattice  (T, <=)  with

+minimum  element 0.  Let w I^ T such  that w <= f(w).  If LFP (f)  is w -finite,  then  the  following>=,w

transformation is correct:

p := LFP (f)>=,w
Th(36) p := 0

(while $ z I^ dec((w U' f(p)) - p)p U':= z
end

n

Powerset lattices are important examples of UFD lattices.  If s is a finite set and T = pow(s), then functions
(37) \Delta (A, B) = A - B (set difference)d(A, x) = A with x (element addition)

are feasible at w I^ T relative to any monotone computable function f: T o"" T that is inflationary at w.  An inductive
form  of  Transformation  5  (as  in  Transformation  2)  justifies  the  treatment  of  the  reachability  problem  found  in
Example 11 and interval partitioning found in Example 12.

Transformation 5 is exceedingly useful for efficient computation of least fixed points of functions defined on
UFD lattices.  If we assume that f(p) can be computed in polynomial time and each atom uses O(1) space, then all
specifications  to  which  Transformation  5  may  be  applied  can  be  implemented  in  a  most  rudimentary  way  with
running  times  exponential  in  the  size  of  the  search  space dec((U'/range f) U' w).  Transformation  5  effectively
translates these exponential time specifications into procedural forms with greedy strategies and polynomial running
times.  Since p grows  one  element  at  a  time,  if  the  value  of  the  expression dec((w U' f(p))  - p)  does  not  change
dramatically from iteration to iteration, then it may be possible to compute the new value of dec((w U' f(p)) - p) from
its  old value more efficiently than  to  compute  it  from scratch.   Transformation 5  also provides  an  accurate upper
bound on the iteration count for the while-loop in code (36).  The bound is #dec(p) at the final value of p.  Such
complexity information provided by Transformation 5 is exploited in [11] to develop a syntactic characterization of
a  class  of  set  theoretic  fixed  point  expressions  that  can  be  computed  in  linear  time  and  space  with  respect  to  the
input/output space.

To compute greatest fixed points in UFD lattices, we can apply the following dual form of Transformation 5:

Transformation 6:  Let f: T o"" T be  a  monotone  computable  function  defined  on  UFD  lattice  (T, <=)  with

-maximum  element 1.  Let w I^ T such  that w >= f(w).  If GFP (f)  is w -finite,  then  the  following<=,w

transformation is correct:

p := GFP (f)<=,w
Th(38) p := 1

(while $ z I^ dec(p - (w U` f(p)) )p -:= z
end

n

Transformation 6 can be used to derive an efficient cycle testing algorithm (cf. Example 3).

23
Now consider a general FD lattice L.  In this case Transformation 6 does not apply, because the iterative atom
deletion step in code (38) may make no progress.  Nevertheless, we can sometimes map greatest fixed points into
equivalent least fixed points that can be computed using Transformation 5.

Suppose an FD lattice L has a maximum element 1.  A non-1 element a I^ L is called a dual atom if " b I^ L, b
>= a implies b = 1 or b = a.  For each element w I^ L, we define the dual decomposition of w to be

ddec(w) = {a >= w | a is a dual atom}.
We say that L is dual-decomposable, if for all elements w I^ L,

w = U`/ddec(w).
If L is a lattice, then the dual lattice L' is formed from L by reversing the ordering <= (where the reverse ordering is
denoted  by <='),  and  interchanging  meet  and join  and 1 and 0.  In  reasoning  about  the  dual lattice we  would also

- +interchange dec and ddec, and w -finite and w -finite.

If L is FD and dual decomposable, then L' is FD also.  Hence, we have

Transformation 7:  Let f: T o"" T be a monotone computable function defined on a dual decomposable FD
lattice (T, <=) with minimum element 0 and maximum element 1.  Let w I^ T such that w >= f(w).  If GFP (f)<=,w

-is w -finite,  then GFP (f)  = LFP (f).  Hence,  we  can  compute  the  greatest  fixed  point  by  applying<=,w <=',w

Transformation 5 to the equivalent least fixed point in the dual lattice.

n

Although Transformation 5 still applies for FD lattices, the potentially excessive number of atoms contained
in dec((w U' f(p)) - p), appearing in code (36), can often make this transformation too costly.  For instance, in the
lattice of partitions over an n-element set s (cf. Example 8), the maximum element {s} can be formed from the join
of n-1  atoms,  even  though  #dec({s})  = n(n-1)/2.  Fortunately,  we  can  sometimes  overcome  this  problem  using  a
simple, fairly general data compression technique.

Let A be the set of atoms in an FD lattice L.  For any x I^ L and t I' A, we say that t is a representation of x if x
= U' / t.  A function r: L o"" pow(A) is a representation function of L, if " x I^ L, r(x) is a representation of x.  If r is a
representation function of L, then we can obtain feasible functions

(39) \Delta (a, b) = r(a) - dec(b)
and

(40) d(b, q) = b U' q
relative to any monotone computable function f: L o"" L that is inflationary at w I^ L.  Consequently, when #r(x) is
much smaller than #dec(x), we can replace the set dec((w U' f(p) - p) within code (36) with r(w U' f(p)) - dec(p) in
order to obtain better performance out of Transformations 5.  Definitions (39) and (40) can also be used to improve
Transformations 1, 2, 3, and 4.

To  illustrate  both  the  use  of  representation  functions  and  Transformation  7,  consider  greatest  fixed  point
computations on the  lattice L of  partitions  over  a finite  set s (cf. Examples 8, 9,  and 13).   Observe that L is  dual
decomposable and FD, but not UFD.

If we define datm(x) = {x, s - x} for all x I' s, then the set of atoms in the dual lattice L' is
{datm(x): x I' s | x z' {} and x z' s}
If we also define r(X) = {datm(x): x I^ X} for all X I^ L, then r is a representation function for L', since X = U`/r(X).

In  order  to  solve  greatest  fixed  points  in L by  computing  least  fixed  points  in L',  we  reformulate  feasible
functions (39) and (40) for L' as follows,

24
r(B ) - ddec(B )1 2
= {datm(b ): b I^ B |($ b I^ B | b C, b z' {} and b - b z' {})}1 1 1 2 2 1 2 2 1

which leads to the following simplified feasible functions:

(41) \Delta (B , B )1 2

= {b I^ B | ($ b I^ B | b C, b z' {} and b - b z' {})}1 1 2 2 1 2 2 1

d(B , b) = B U` datm(b)2 2
= {x C, b: x I^ B | x C, b z' {} } E` {x - b: b I^ B | x - b z' {} }2 2

relative to any monotone computable function f: L' o"" L' that is inflationary at w I^ L'.  If B >= B , then functions2 1
(41) further simplify to

(42) \Delta (B , B ) = {b I^ B | b I" B }1 2 1 2

d(B , b) = {x I^ B | x C, b = {} } E` {b} E` {x - b: x I^ B |b I' x}2 2 2

Example 14: (Relational coarsest partition problem) Let Q be a partition of the finite set s, e be a binary

-1 -1relation over s, and a I' s. Q is stable with respect to a if for all blocks b I^ Q, either b I' e [a] or b C, e [a]

= {}. Q is stable if it is stable with respect to each of its blocks. Let Q be an initial partition.  Then the0
Relational Coarsest Partition Problem is to find the maximum stable partition Q <= Q .0

Let #s = n and #e = m.  Two algorithms are presented in [58]:  one is a general algorithm with O(mn)
time complexity, and the other uses the 'smaller half' strategy to achieve a lower time complexity O(mlogn).
Both  of  these  two  algorithms  can  be  formally  derived  from  abstract  specifications.    If f(Q)  is  the  coarsest
refinement  of Q that  is  stable  with  respect  to  each  block  of Q,  then  the  solution  to  the  relational  coarsest
partition problem can be specified as

(43) GFP (f)<=,Q0
from which the O(mn) general algorithm can be derived.

Let g(Q) = f(Q) U` Q .  We can incorporate a 'smaller half' strategy like the one used in Example 8 into0
the following feasible functions relative to g at Q based on (42):0

\Delta (g(Q), Q) = {b I^ g(Q) | ($ b I^ Q | b I' b and #b <= #b /2)}1 2 1 2 1 2

d(Q,b) = Q U` datm(b)
When  the  preceding functions  are  used  in  connection  with  Transformation  7,  we  can  derive  the O(mlogn)
algorithm described in [58]. n

To compute fixed points of functions defined on lattices that are not FD, we offer no general method other
than what has been suggested in the previous section on special functions or on the classical iteration (13) implied
by Corollary 2.  For example, fixed point computation for functions defined on the real numbers, which is certainly
nondecomposable, is a whole subject outside the scope of this paper.  Whenever a lattice is partly FD, in the sense
that  some  of  the  lattice  elements can  be  represented  by  a  finite  join  of  atoms,  then  we  can  still often  employ  the
techniques discussed in this section.  The next example illustrates this idea.

Example 15: In  the  constant  propagation  algorithm  given  by  Reif  and  Lewis [61, 80],  each  assignment
statement A is associated with either bottom (means undefined), top (means non-constant), or a real number
that can result from the execution of A.  Let R denote the set of real numbers.  Under the ordering,

bottom <=' any real number <=' top;real numbers are incomparable

the set {bottom, top} E` R forms a lattice in which

25
bottom U' x = xtop U' x = top
x U' y = top " x, y I^ R | x z' y
Let val be  the  set  of  all  statement-value  pairs  for  the  given  program.  Let assign be  the  set  of  assignment
statements of the given program. Then val is a total function from assign to {bottom,top} E` R.  Let val and1
val be two such functions. We define val <= val iff val (s) <=' val (s) " s I^ assign.  Under this ordering, all2 1 2 1 2
such functions form a lattice F.  The maximum function in this lattice is val1 = {[s,top]: s I^ assign} and the
minimum is val0 = { [s, bottom]: s I^ assign}. " s I^ assign and v I^ R, the function {[s,v]} E` {[x, bottom]: x
I^ assign | x z' s} is an atom.  Because of top, lattice F is decomposable but not FD.  But we can still use rule
(28) to obtain the following feasible functions:

\Delta (val , val ) = {[s, v] I^ val | val (s) \<= val (s)}1 2 1 1 2
and

d(val,[s,v]) = (val - {[s, val(s)]}) E` {[s, val(s) U' v]} n
relative to any monotone computable function f: F o"" F and any element w I^ F where f is inflationary.

4. Fixed Point Recomputation

The preceding section showed that efficient computation of least fixed points depends, in large part, on finite
differencing  to  avoid  costly  recomputation  of  expressions  embedded  within  feasible  functions \Delta  and d.  In  this
section we investigate application of finite differencing to fixed point expressions themselves, e.g.,

(44) g(u,w) = LFP (f(s,u),s)<=,w
whose input parameters u and w can be modified.  We also show how to compute fixed points of functions g(u, w)
and, hence, nested fixed points.

In general, the need for recomputing problems arises naturally in several contexts.  For example, the Cornell
Synthesizer [76]  is  a  syntactic  editing  system  that  uses  an  attribute  grammar  to  implement  program  semantics.
Whenever  a  program  is  modified  using  the  synthesizer,  the  program's  semantic  information  must  be  updated  to
reflect  the  editing  changes.    The  attribute  reevaluation  algorithm  of  Reps, et.  al [63]  is  an  efficient  incremental
algorithm  in  the  sense  that  it  recomputes  the  new  semantics  from  the  old  in  an  optimal  way  -  performing
asymptotically better than an algorithm that just recomputes the new semantics from scratch.

Incremental algorithms can also be used to solve repeated subtasks of a problem efficiently.  For example, a
selection sort, which repeatedly performs a linear time search for the minimum value of a set, can be turned into a
faster heap sort by using a 'dynamic' heap data structure to compute the minimum set value with only a log factor
cost each time [55].

A third context is where a problem P(s) can be solved incrementally by computing P at an initial point s and0
then at successive points s = g(s ), i = 1, .., n, where g is an inexpensive incremental calculation and s = s.ni i-1

Finite  differencing  and  stream  processing [28]  are  program  transformations  that  provide  a  formal  basis  for
studying the three kinds of recomputations just described.  However, previous investigations avoided consideration
of fixed point expressions.  In this section we extend that earlier work in finite differencing by presenting rules for
efficient recomputation of fixed point expressions (44).

26
4.1. Finite Differencing

In  this  section  we  give  a  brief  introduction  to finite  differencing [57, 55],  a  technique  that  can  improve  the
performance  of  programs  generated  by  our  fixed  point  transformations.  The  basic  goal  of  this  technique  is  to
replace  direct  calculations  of  costly  expressions f(x ,...,x )  in  a  program  region B by  less  expensive  incrementalm1
calculations.  We explain the technique by example after first presenting some definitions and convenient notational
conventions.

If a variable e always stores the value of an n-variate function f(x , ..., x ) at a program point p, we say thatm1
equality e = f(x , ..., x ) is invariant at p.  Consequently, any occurrence of expression f(x , ..., x ) at p is said to bem m1 1
redundant and can be replaced by variable e.  Let dx be a modification to a variable x on which f depends.  Thej j

- +predifference and postdifference of e with respect to dx , denoted by u"" e <dx > and u"" e <dx > respectively, are twoj j j

single-entry single-exit code blocks with the following properties.

1. If e = f(x , ..., x ) is invariant just before executing codem1

-(45) u"" e <dx >j

dxj

+u"" e <dx >j

it is invariant immediately after (45) is executed.
2. The  predifference  and  postdifference  code  blocks  can  only  modify  variable e and  variables  local  tothese blocks.

Consider  a  collection  of  equalities e =  f ,  j  =1,...,n,  in  which  each  expression f depends  only  on  variablesj j j
v ,...,v ,e ,...,e .  Suppose we want to maintain and exploit all of these equalities as invariants within a single-entry1 k 1 j-1
program region B.  The differential of e , ..., e with respect to B, denoted by u"" {e , ..., e } <B>, is a new code block1 n 1 n
formed from B by recursively applying the following rules.

1. Replace each modification dx occurring in B with

- +(46) u"" {e ,...,e } <u"" e <dx> dx u"" e <dx>>2 1 1n

where  no  new  occurrences  of f are  introduced  within  difference  code  associated  with  invariants1

- +e ,...,e .  Also,  all  occurrences  of f appearing  in u"" e <dx> dx u"" e <dx>  must  be  redundant  and  aren2 1 1 1

replaced  by e .  If  the  preceding  conditions  are  met,  we  refer  to e as  a minimal  invariant for  the1 1
differential u""{e ,...,e }<dx>.1 n

2. Substitute all occurrences of f by e , j=1,...,n, within the rest of B.j j

Based  on  the  differential,  we  obtain  the  following  general  chain  rules  for  collective  predifference  and
postdifference code blocks:

- -u"" {e , ..., e } <dx > = u"" {e ,...,e } <u"" e <dx >>21 n j n 1 j

-u"" {e ,...,e } <dx >2 n j
and+ +u"" {e , ..., e } <dx > = u"" {e ,...,e } <dx >

21 n j n j+u"" {e ,...,e } <u"" e <dx >>
2 n 1 j
where e is minimal.1

Example 16: Two simple examples of difference code are illustrated below.

1. Let c = 5x. Then

-u"" c < x +:= 1 >  = c +:= 5
+ 12u"" c < x +:= 1 >  = l

12We use l to denote the empty code block

27
Since  the  predifference  code  for c does  not  involve x,  it  could  also  be  regarded  aspostdifference code; i.e.,

-u"" c < x +:= 1 >  = l
+u"" c < x +:= 1 >  = c +:= 5

2. Let c = { x I^ s | k(x) } where k is a computable boolean expression that does not depend on s.Then

-u"" c < s with:= z >
=if k(z) then

c with:= zend

and

+u"" c < s with:= z >  = l

For this example, also, the predifference code could just as easily be shifted to postdifferencecode.

We can see that, in general, difference code is not unique. n
Difference code for products like 5x are included in the strength reduction transformations of most optimizing
compilers  and  are  used  to  replace  certain  products  with  less  expensive  sums [12].  Difference  code  for  a  more
general  class  of  expressions  including  the  set  former  above  are  central  to  finite  differencing  and  can  be  found  in

[22, 26, 57].  The following example adopted from [53] shows how finite differencing is used in combination with
the fixed point transformations to yield efficient programs:

Example 17: (Graph  reachability  continued)  The  program  derived  in  Example  11  contains  an  expensive
expression w E` e[p]  - p in  the while-loop.  However,  its  costly  computation  can  be  avoided  by  applying
finite differencing.

Bottom up parsing decomposes this expression as follows:
e = e[p]1
e = w E` e2 1
e = e - p3 2

Next, we replace the while-loop in (19) by its collective differential

u"" {e , e , e } <(while $ z I^ (w E` e[p] - p) )1 2 3 p with

:= zend >

which is equivalent to

(while $ z I^ e )3u""

{e , e , e } <p with:= z>1 2 3end

Among the three invariants e , e , and e , we know that e is minimal, and within the remaining two, e is1 2 3 1 2
minimal. Therefore we have

u"" {e , e , e } <p with:= z>1 2 3
=u"" e < u"" e <u"" e <p with:= z> > >

3 2 1=

u"" e < u"" e < (for x I^ e{z})3 2 e with

:= x1end

p with:= z >  >

28
=
u"" e < (for x I^ e{z})3 e with

:= x2e with
:= x1end

p with:= z >
=

(for x I^ e{z})if x I" p then

e with:= x3end
e with:= x2e with

:= x1end

e less:= z3p with

:= z

The initialization code is simple:

e := {}1e

:= w2e
:= w3

Consolidating all of the preceding code, we obtain the following procedure for computing graph reachability:

e = {}1e

= w2e
= w3p
:= {}

(while $ z I^ e )3

(for x I^ e{z})if x I" p then

e with:= x3end

e with:= x2e with

:= x1end

e less:= z3
p with:= zend

Analysis for useless code determines that variables e and e are never used.  After all assignments to e and1 2 1
e are eliminated, we obtain the following much improved code:2

(47) e = w3p

:= {}

(while $ z I^ e )3

(for x I^ e{z})

if x I" p thenif x I" e then

3e with:= x
3end

endend

e less:= z3p with

:= zend

29
Note  that  in  the  final  phase,  we  guarded  the  operation e with:= x with  the  condition x I" e so  that x is3 3
always added to set e .  This program can be implemented with suitable data structures to have a time and3
space complexity linear to the size of the input graph. n

4.2. Incremental Recomputation of Fixed Points

We now derive difference code for the fixed point expression
g(u, w) = LFP (f(s,u),s)<=,w
with  respect  to  modifications  to u and w.  In  Section  3  we  showed  different  ways  of  computing g according  to
properties of f.  In the next lemma we also show how properties of g, which are useful in deriving difference code
for g, depend on properties of f.

Lemma 7: Let f: S * T o"" S be a computable function monotone in each of its arguments, where (S,<=)  and  (T, <=)  are  posets,  and w I^ S.  Consider  function g: T * S o"" S with  the  rule g(u,  w)  =
LFP (f(s,u),s).  Then<=,w

1. g(u, w) is monotone and inflationary in w.

2. If g(u, w) is defined for [u, w] I^ T * S, then for any w' I^ S such that w <= w' <= g(u, w), g(u, w') isalso defined, and g(u, w') = g(u, w).

+3. g(u, w) is monotone in u over the subset of T for which w <= f(w, u) and g(u, w) is w -finite.
4. If (S, <=) = (T, <=), and f(s,u) is inflationary in u for all s I^ S, then g(u, w) is inflationary in u.

Proof:
1. Suppose that w, w' I^ S, w' >= w, and g(u, w), g(u, w') are defined.  By definition, g(u, w) is thesmallest solution to f(s, u) = s greater than or equal to w. Since g(u, w') >= w' >= w and since g(u,

w') is another solution to f(s, u) = s greater than or equal to w, then g(u, w') >= g(u, w).
2. g(u, w) is the smallest x I^ S satisfying the conditions w' <= x and x = f(x, u).

+3. Let f (s) = f(s, u).  Let u, u' I^ {x I^ T | w <= f (w) and g(x,w) is w -finite} and u <= u'.  Then f (x) <=u x u
k kf (x) " x I^ S.  Since f (s) and f (s) are monotone in s, then f (x) <= f (x) " x I^ S, " k = 0, 1, ...  .u' u u' u u'
k kHence, for some integer k >= 1, g(u, w) = f (w) <= f (w) = g(u', w) by Corollary 2 and Theoremu u'
3(3).

4. If g(u,w) = s', then s' = f(s',u) >= u. n

Based on Lemma 7, we can modify the least fixed point g(u, w) incrementally with respect to modifications in
u and w.  In the following discussion, we use x and x to represent the value of the variable x before and afterold new
modifications respectively, and use f (x) to represent the function of x defined by f(x, y).y

Theorem 8: Let p = g(u, w) be defined as in Lemma 7. Assume that " [x, y] I^ T * S, g(x, y) is+y -finite.  Let d and d be increment functions.

1 21. Let \Delta  and d be functions feasible relative to f at p .  If w <= f(w, u ), then

u old oldnew
+u"" p <u := d (u,z)>1

=

(48)  (while $ x I^  \Delta (f(p,u), p))p := d(p,x)

end
The  predifference code is empty.  (Note that postdifference and not predifference  code is used,because (48) references the new values of u.)

2. If  (S, <=, U')  is  a  join  semilattice, p U'w <= f(p U'w , u), \Delta '  and d'  are  functions  feasibleold new old new

relative to w at p , and \Delta  and d are functions feasible relative to f at p U' w , thennew old u old new

30
+u"" p <w := d (w, z)>2
=

(49) (while $ y I^  \Delta '(w, p))p := d'(p, y)

end(while $ x I^  \Delta (f(p,u),  p))

p := d(p, x)end

The predifference code is empty.
Proof:

+1. Because of the feasibility of \Delta  and d, and the y -finiteness of g(x, y) for all [x, y] I^ T * S, the

successive values assigned to p in the while-loop forms a sequence s , s , ..., s with s = p =0 1 k 0 old
g(u ,  w)  and s = g(u ,  p ).  We  need  to  show  that g(u ,  p )  = g(u ,  w).  Since f isold new old new old newk
monotone in u, and u = d (u , z) >= u , then f(w, u ) >= f(w, u ) >= w.  Since for all x I^ T,old new oldnew 2 old

+g(x,  w)  is w -finite,  then  by  Lemma  7(3) w <= p <= g(u ,  w).  Therefore g(u ,w)  =newold new

g(u ,p ) by Lemma 7(2).new old

+2. The proof is similar to 1. " [x, y] I^ T * S, the condition that g(x, y) is y -finite implies that g(x, y)
is defined, and thus y <= g(x, y).  By Lemma 7(1), g(u, w ) <= g(u, w ).  Thus w <= p U'old new new old
w <= g(u, w ), where p = g(u, w ). Then by Lemma 7(2), g(u, w ) = g(u, p U' w ),new old new oldnew old new
which is computed by (49).  Note that at the end of the first while-loop of (49), p = p U' w ,old new
and the condition p <= f(p, u) is satisfied. n

It is interesting to consider various ways in which code (48) and (49) can be improved.  One obvious approach
to speed up this code is to avoid computing \Delta (f(p, u), p) by maintaining equality e = \Delta (f(p, u), p) as an invariant
together with p.  In the case of (48) the chain rule applied to both p and e leads to the following difference code
blocks:

- -u"" {e, p} <u := d (u,z)> = u"" e <u := d (u,z)>1 1

and

+ +u"" {e, p} <u := d (u,z)> = u"" e <u := d (u,z)>1 1

+u"" e <u"" p <u := d (u,z)>>1

Under the same conditions, a similar improvement is possible for (49).

One drawback with code (48) is that it can only be used as postdifference code (if we are to avoid copying
parameter u),  since  it  references  the  new  value  of u.  Such  inflexibility  is  undesirable,  because  it  can  preclude
opportunities  for  further  optimization.  However,  we  can  overcome  this  problem  whenever  the  collective

+ +postdifference code u"" {e, p} <u := d (u,z)> involves no occurrences of u.  This fortunate situation arises when u"" e1

+<u := d (u,z)> is empty and u"" e <u"" p <u := d (u,z)>> involves no free occurrences of u.  In this case we can use an1 1
+empty postdifference block u"" {e, p} <u := d (u,z)> and the following predifference code:1
- -u"" {e, p} <u := d (u,z)> = u"" e <u := d (u,z)>1 1

+u"" e <u"" p <u := d (u,z)>>1

-(50)  = u"" e <u := d (u,z)>1
(while $ x I^ e)u"" e <p := d(p,x)>

end
Since code (50) involves no occurrences of u, it can be used as either pre- or postdifference code.

We can exploit contexts where predifference code (50) is correct in two ways:
1. by uncovering alternative ways of computing (44) and

31
2. by space optimizations.
If d: T * X o"" T is an increment function, u I' X is a set such that u can be computed by executing0

u := 0(for z I^ u )

0u := d(u, z)

end
then  one  alternative  way  to  establish p = g(u,  w)  is  by  the  following  incremental  calculation  involving  a  search
through u

-u"" {e, p} <u := 0>
(for z I^ u )0

-u"" {e, p} <u := d(u, z)>

end

This  implementation  may  be  useful  when  a  search  through u is  unavoidable  because  of  global  computational
requirements.  Note that a similar incremental approach to compute g(u, w) based on a search through w and using
predifference code (49) is also possible.

In  computing  nested  expressions  such  as g(h(x), w)  or  in  maintaining  the  value  of  such  expressions  across
modifications  to x,  we  can  often  avoid  evaluation  or  maintenance  of  subexpression h(x).  To  see  how  this  is
achieved, consider the three equalities u = h(x), p = g(u, w), and e = f(p, u)  - p.  If we can calculate u = h(x) by
executing

-u"" u <x := 0>
(for z I^ x )0

-u"" u <x := d(x, z)>

end

then we can also calculate p = g(h(x), w) by executing the following block:

-u"" {e, p, u} <x := 0>
(for z I^ x )0

-u"" {e, p, u} <x := d(x, z)>

end

- -Whenever the difference code blocks u"" {e, p, u} <x := 0> and u"" {e, p, u} <x := d(x, z)> contain no occurrences of u

except for modifications to u, then maintenance of the equality u = h(x) is unnecessary; all modifications to u can be
eliminated.  Goldberg and Paige called this technique vertical loop fusion [28].

The following theorem and corollary can sometimes be used to introduce opportunities for vertical fusion.

Theorem 9: Let (L, <=, 0) be a meet semilattice with a unique minimum element 0; let f: L o"" L be acomputable  monotone  function;  let c I^ L be  a  constant  such  that f(s U` c) U` c = f(s) U` c.  If LFP(f)  is
0-finite, then LFP(f) U` c = LFP(f(s) U` c, s).

i iProof: Let h(s) = f(s) U` c.  We prove by induction that for all integers i >= 1, h (0) = f (0) U` c.  For i
= 1, it is trivial.  Assume this holds for i < k, where k > 1.  Then

kh (0)

k-1= h(h (0))
k-1= f(f (0) U` c) U` c
k= f (0) U` c n

Corollary 10: Let (L, <=, 0) be a meet semilattice with a unique minimum element 0; let f, g: L o"" Lbe two computable monotone functions.  If LFP(f) and LFP(g) are 0-finite, and f(s U` LFP(g)) U` LFP(g) =
f(s) U` LFP(g), then LFP(f) U` LFP(g) = LFP(f U` LFP(g)). n

Example 18: (Sink-source problem) Consider a directed graph represented as a binary edge relation e.  Let

32
sources and sinks be two sets of nodes.  We want to find out all the nodes in e occurring within any path
from sources to sinks. Let

f1(s) = sources E` e[s]-1f2

(s) = sinks E` e [s]

Then we can solve this problem by first computing the set

reach = LFP(f1)
of all nodes reachable from sources and the set

access = LFP(f2)
of all nodes that can reach sinks. Then the required solution is simply

output = reach C, access
It is easy to verify

f1(s C, access) C, access = f1(s) C, access
and

f2(s C, reach) C, reach = f2(s) C, reach
which leads to the new specifications

(51) output = LFP(f1(s) C, access)  = LFP(f2(s) C, reach)
by corollary 10.  Either of these specifications is desirable, since it can be computed in one pass through e.

n

Example 19: (Elimination of dead code and unreachable code)  A program statement is called dead if it
makes no contribution to the output of the program; a statement is called unreachable if it cannot be reached
along any path in the flow graph of the program beginning from the entry statement.  Based on Theorem 9
we can derive code that eliminates both dead code and unreachable code in one pass.

Consider a program representation in which variable prints is the set of print statements, variable iuses
maps  each program  statement to  the variable  uses it  contains, usetodef is a binary  relation  that maps  each
variable use to the variable definitions that can reach it, instof is a function mapping each variable occurrence
to  the  statement  immediately  enclosing  it,  and compound is  a  function  that  maps  each  statement  to  the
compound  statement  immediately  enclosing  it  (i.e., if or while statement).  In [52]  an  efficient  dead  code
elimination procedure was derived from the following specification:

(52) live = LFP(f )3
where f (s) = prints E` instof[usetodef[iuses[s]]] E` compound[s], which determines live statements from data3
and control flow considerations.

Unreachable statements can be eliminated by applying the graph reachability algorithm of Example 1
to the program flow graph:

(53) reachable = LFP(f )4
where f (s) = {entry} E` succ[s], entry is the only entry statement of the program, and succ{x} is the set of all4
the possible successors of statement x.  The set of statements that are both live and reachable is

useful = live C, reachable
The reader can verify that

f (s C, reachable) C, reachable = f (s) C, reachable33
Hence, by Theorem 9, we have

useful = LFP(f (s) C, reachable, s)3
from which a one pass program can be derived.  Note that we do not have

33
useful = LFP(f (s) C, live, s)4
because in general

f (s C, live) C, live z' f (s) C, live n4 4

4.3. Recomputation of Fixed Points of Distributive Functions

Although  we  can  recompute  least  fixed  points  incrementally  under  some  conditions,  the  decremental
recomputation of least fixed points is much more difficult.  In [43], Kaplan and Ullman specify their solution to a
general weak type analysis problem as GFP(\Psi  \Phi (s)), where \Psi (s) = LFP(s U` B(x), x), \Phi (s) = LFP(s U` F(x), x), ando
B and F are two monotone functions representing backward and forward type analysis respectively.  They compute
this greatest fixed point as the limit of the decreasing sequence 1 = s , s , ..., where s = \Psi  \Phi (s ) is recomputedo0 1 i+1 i
from scratch for each s in this sequence.  We have also failed to find a general efficient decremental method fori
computing least fixed points.

However, least fixed points can still be recomputed efficiently under some restricted conditions. One special
case observed in [46] is when a function has a unique fixed point.  In this case, the fixed point can be treated as both
a least and a greatest fixed point, and thus can be recomputed incrementally and decrementally.

In this section we show that if f is distributive, i.e., f(x U' y) = f(x) U' f(y), then the following expression
(54) g(w) = LFP(w U' f(x), x)
can  be  modified  efficiently  when w is  incremented  or  decremented.  We  assume  that f is  a  monotone  computable
function  on  a  finite UFD lattice  (L, <=, U', 0)  (  cf.  Section  3.4  ).    We  show  that  this  problem  is  reducible  to  the
problem of graph reachability.

Let r = r(w, e) be the set of vertices s reachable from a source set w along paths in a directed graph e.  We
have shown that r is the least fixed point of the function f (x) = w E` e[x].w

Lemma 11: If the directed graph e is acyclic, then the function f (x) has a unique fixed point.w
Proof: Let x = LFP(f ), and x be any fixed point of f .  Then x I' x .  To show also that x I' x ,w w0 1 0 1 1 0
let y = x - e[x ].  Since x = w E` e[x ],  then y I' w.  Since e is  acyclic,  then x is  the  set  of  vertices1 1 1 1 1reachable from

y along paths in e.  Therefore x = LFP(f ).  Since by Lemma 7(3) LFP(f ) is monotone iny t1
t, then x I' x . n1 0

Therefore, by Theorem 8(1) and its dual, if e is acyclic, we have

+u"" r <w with:= z>
=(55) (while $ t I^ (w E` e[r] - r))

r with:= tend

and

+u"" r <w less:= z>
=(56) (while $ t I^ (r - (w E` e[r])))

r less:= tend

Note that the inflationary condition of Theorem 8(1) and the deflationary condition of its dual are satisfied, since all
functions are inflationary at 0 and deflationary at 1.

The next theorem shows how the efficiency of (55) and (56) can be further improved by finite differencing.

+ + + --Lemma 12: If r = r - r , r = r - r , e = {[x, y] I^ e | x I^ r }, and e = {[x, y] I^ e | x I^new old old new
- + -r }, then we can compute code (55) in O(1 + #e ) steps and the code (56) in O(1 + #e ) steps.

34

-1Proof: Let new1 = w E` e[r] - r, new2 = r - w E` e[r], numpred = {[x, #(e {x} C, r)]: x I^ domain e
E` range e}. Analysis of the following two difference code blocks yields the result.

+u"" {r, new1, numpred} <w with:= z>
=if z I" r then

new1 with:= zend
(while $ t I^ new1)(for x I^ e{t})

numpred(x) +:= 1if x I" r then

new1 with:= xend
endnew1 less:= t
r with:= tend

and

+u"" {r, new2, numpred} <w less:= z>
=if numpred(z) = 0 then

new2 with:= zend
(while $ t I^ new2)(for x I^ e{t})

numpred(x) -:= 1if numpred(x) = 0 and x I" w then

new2 with:= xend
endnew2 less:= t
r less:= tend

n

Let C be  the  set  of  strongly-connected  components  for  a  directed  graph e.  It  is  well  known [73]  that  the
directed graph G = {[u, v] I^ C * C | (u z' v) and ($ [m, n] I^ e | m I^ u and n I^ v)} is acyclic.  For all vertices v I^
domain e E` range e, let component(v) be the strongly-connected component that contains v.  Then the set C and
map component can be computed in O(#e) time [73] (see also [3]).

Hence, we can conclude,

+Lemma  13: For  directed  graphs  in  general, r can  be  maintained  in O(1  +  #e )  steps  when w
-changes  by  an  element  addition  and  in O(1  +  #e )  steps  when w changes  by  an  element  deletion  (c.f.

Lemma 12).

Proof: If X = component[w], and if Y is the set of strongly-connected components reachable from Xin G, then

r = E` / Y
X can be modified in unit time when w changes by deletion or addition. To see this, let refcount(x) = #(x C,w) for each component x I^ C.  Then

-u"" {X, refcount} <w with:= z>
=if refcount(component(z)) = 0 then

X with:= component(z)end
refcount(component(z)) +:= 1

35
-u"" {X, refcount} <w less:= z>
=if refcount(component(z)) = 1 then

X less:= component(z)end
refcount(component(z)) -:= 1
Whenever X is modified in the preceding difference code, we can modify Y by the difference code given inthe proof of Lemma 12.

n

Finally, consider the expression
g(w) = LFP(w U' f(x), x)
where f is a monotone, distributive, computable function on a finite UFD lattice (L, <=, U', 0).  Since L is UFD, we
can represent w and g(w) by their decompositions.  Let dw = dec(w) and dg = dec(g(U'/dw)).  We discuss how to
maintain the invariant dg with respect to the modifications dw with:= z and dw less:= z, where z is an atom.

Let W = dw with 0. Then

-u"" W <dw with:= z>
=W with:= z

and

-u"" W <dw less:= z>
=W less:= z

Let A be the set of all the atoms of L, and V = A E` {0}.  Let e = {[v, u]: v I^ V, u I^ dec(f(v))}.  Let F(x) = w U'
f(x) and F'(x) = W E` e[x]. We have

Lemma 14: For all s I' V such that s z' {}, F(U'/s) = U'/F'(s).
Proof:

F(U'/s)= w U' f(U'/s)

= w U'  U'/{f(x): x I^ s} since f is distributive= w U'  U'/{U'/dec(f(x)): x I^ s} since L is UFD
= w U'  U'/{U'/e{x}: x I^ s} by the definition of e= w U'  U'/e[s] where e[s] = E`/{e{x}: x I^ s}
= U'/(W E` e[s])= U'/F'(s)

n

By induction we can prove further that

k kCorollary 15: F (w) = U'/F' (W) for all k = 0, 1, 2, ... . n

If r = LFP(e[s]E`W, s), then by Corollary 15 LFP(wU'f(x), x) = U'/r, which implies that dg = r .  (Remember
that dg = dec(g(w)) = dec(LFP(wU'f(x), x)). ) Since we know how to modify r with respect to the modifications W
with:= z and W less:= z, then we know how to modify dg respect to the modifications dw with:= z and dw less:= z.
Thus we have

+Theorem  16: The  invariant dg can  be  maintained  in O(1  +  #e )  steps  when dw changes  by  an
-element addition and in O(1 + #e ) steps when dw changes by an element deletion (c.f. Lemma 12).

36
4.4. Nested Least Fixed Points

Efficient ways to recompute least  fixed points  are especially useful in contexts  where least fixed points  are
composed with other functions.  However, in the special case where the least fixed point operator is composed with
itself, it is sometimes best to avoid an incremental least fixed point calculation in favor of a more direct approach
described as follows.

Consider again the function
g(u, w) = LFP (f(s,u),s)<=,w
where f: S * S o"" S is a computable function monotone and inflationary in each of its arguments, (S, <=) is a poset,
and w I^ S.  Then g(u,w) is a function on S monotone in u and w and we can further consider the least fixed point of
g(u,w) with respect to either u or w. The following theorem tells when these fixed points are defined and how they
can be computed.

Theorem  17: Let f (x)  = f(x,x).  For  any  function z(x, y),  let z (x)  = z(x,y).  If f: S * S o"" S is  a1 y
computable function monotone and inflationary in each of its arguments, w, q I^ S, and w U' q is defined,then

1. LFP (g(u,w), w) = g(u, q)<=,q

i2. Let a = LFP (g(u,w), u) and b = LFP (f (u), u).  Let h = {f (wU'q): i = 0, 1, ...}, and h =1 2<=,q <=,qU'w 1 1
i{g (q): i = 0, 1, ...}.w
Then both a and b are defined and a = b if either of the following conditions are satisfied:

i. h is finite;1

iii. h is finite, and {f (w): i = 0, 1, ...} is finite for all x I^ h .2 2x

Proof:
1. Trivial.

2. Suppose h is finite.  By Corollary 2, b is defined and there exists an integer t >= 0 such that1

k k(57) " k = t, t+1, ..., b = f (w U' q) = f (b)1 1

Since f is monotone and inflationary in each of its arguments, we can prove by induction that

i i+1 i+1(58) " i = 0, 1, ..., f (w U' q) <= f (w) <= f (b)1 b 1

From (57) and (58), we have

k" k = t+1, t+2, ..., b = f (w) = LFP (f(s,b), s) = g(b,w)b <=,w

This means that b is a fixed point of g (x) greater than or equal to q.  On the other hand, if c iswany fixed point of

g (x) greater than or equal to q, thenw

(59) c = g (c)w

= LFP (f(s,c), s)<=,w
= f(c,c)>= w

Thus c is also a fixed point of f greater than or equal to w U' q. Since b is the least fixed point of1
f greater than or equal to w U' q, we have c >= b.  Thus b = LFP (g (x)) = a.1 <=,q w

iSuppose h is  finite and {f (w): i =  0, 1,  ...} is  finite for all x I^ h .  By  Corollary 2, a is2 2x

defined. As shown in (59), a is a also a fixed point of f greater than or equal to w U' q.  Let c >= w1

tU' q be any fixed point of f (x).  We prove by induction that c >= g (q) for any t = 0, 1, ...  .  For t =1 w

i0, the assertion c >= q is trivially true.  Assume that c >= g (q) for some integer i >= 0. Then for somew

integer k >= 0,

37
kc = f (c)1

k i>= f (w U' g (q)) by induction hypothesis1 w
k i>= f (w U' g (q)) since f is monotone and inflationaryiwU'g (q) w

w in each of its arguments

k>= f (w)ig (q)

wi+1= g (q)

w
tSince a = g (q) for some integer t >= 0, then c >= a.  Hence a is the least fixed point of f (x) greaterw 1

than or equal to w U' q, i.e., a = b. n

5. Systems of Equations

In this section we return to the topic of systems of equations introduced in Section 3.3.  Let (T , <= , 0 ) be ai i i

nposet with minimum element 0 , i=1, ..., n.  Let T = * T be the product poset with minimum element 0 = [0 , ...,ii i=1 1

0 ].  If x and y belong to T, then x <= y iff x <= y , i=1, ..., n.i i in

Consider the following system of equations:
(60) the x , ..., x :1 n

x = f (x , ..., x ),1 1 1 n
...,x = f (x , ..., x )

n n 1 nminimizing x , ..., x

1 n
where functions f : T o"" T , i=1, ..., n, are computable and monotone in each of their parameters.  System (60) can bei i

solved by such classical methods as Jacobi iteration, Gauss-Seidel iteration, or Gaussian elimination.  But in many
nonnumerical application, these approaches are not efficient.  In Section 3.3 several efficient iterative methods were
discussed (cf.  Transformation 4).

In  this  section  we  derive  potentially  efficient  elimination  and  hybrid  methods.    But  before  doing  this,  it  is
interesting  to  note  that  the  iterative  methods  of  Section  3.3  for  solving  systems  of  equations  can  be  obtained  by
straightforward refinements of Transformation 1.  To see this, let X = [x , ..., x ] and F(X) = [f (x , ..., x ), ..., f (x ,1 n i n1 n 1
..., x )].  Then specification (60) is equivalent to LFP(F).  If LFP(F) has a 0-finite solution, and \Delta  and d are feasiblen
relative to F at 0, then it can be solved as follows by Transformation 1:

(61) X := 0(while $ Z I^  \Delta (F(X), X))

X := d(X, Z)end

5.1. Elimination Method

Now we derive an elimination method for solving system (60).  Let g (x , ..., x ) = LFP(f (x , ..., x ), x ).n n n n1 n-1 1
Replacing x in the first n-1 equations by g (x , ..., x ), we get the following new system with n-1 equations andn n 1 n-1
variables:

(62) the x , ..., x :1 n-1

1x = f (x , ..., x ),1 1 1 n-1

..., 1x = f (x , ..., x ),

n-1 n-1 1 n-1minimizing x , ..., x

1 n-1where

1f (x , ..., x ) = f (x , ..., x , g (x , ..., x ))i 1 n-1 i 1 n-1 n 1 n-1
It is not difficult to show that if n-tuple [x , ..., x ] is the value of expression (60), then (n-1)-tuple [x , ..., x ] is the1 n 1 n-1

38
value of (62).  Moreover, if [x , ..., x ] is the value of expression (62), then [x , ..., x , g (x , ..., x )] is the value1 n-1 1 n-1 n 1 n-1
of (60).

Applying this elimination step repeatedly, we will eventually get a one-equation system

n-1the x : x = f (x ) minimizing x1 1 1 1 1

which can be solved by Transformation 1:

(63) x := 011

n-1(while $ z I^  \Delta  (f (x ), x ))1 1 1 1

x := d (x , z)11 1end

If B represents code block (63), then we can maintain the invariants

n-2x = LFP(f (x , x'), x'),1 2 22 2

...,x = LFP(f (x , ..., x , x'), x'),

n n 1 n-1 n nwithin
B according to the following differential code:

(64) u"" x <u"" x ... <u"" x < B > >...>n n-1 2

Transformation 8: The 0-finite solution of the system (60) can be computed by (64). n
The nested differential (64) can be expanded using Transformation 1 and Theorem 8.  For example, when n =
2, (64) can be expanded into

(65) x := 011x

:= 022
(while $ z I^  \Delta  (f , x ))22 2 2

x := d (x , z )22 2 2end

(while $ z I^  \Delta  (f ,  x ))11 1 1x

:= d (x , z )11 1 1

(while $ z I^  \Delta  (f ,  x ))22 2 2x

:= d (x , z )22 2 2end

end
The  length  of  the  expanded  code  from  Transformation  8  grows  quickly  with n.  This  fact  limits  the  use  of
Transformation 8 as the main transformation scheme for system (60). However, a hybrid transformation, discussed
in  the  next  section,  overcomes  this  problem  by  combining  both  the  iterative  method  of  transformation  4  and  the
elimination method of transformation 8.

5.2. Minimizing the Number of Worksets

Let us partition the n variables x , i = 1,..,n, appearing in the system of equations (60) into two setsi
e = {x : i = 1,..,p}1 i
and

e = {x : i = p+1,..,n}2 i
By eliminating the variables in e from system (60), we get a new system2

(66) the x , ..., x :1 px

= f'(x , ..., x ),1 1 1 p
...,x = f'(x , ..., x ),

p p 1 pminimizing x , ..., x

1 p
Applying Transformation 4 to this new system, we get the following program

39
(67) x := 0 , i = 1,..,pii

(while $ i = 1, ..., p, $ z I^  \Delta  (f',  x ))i i ix

= d (x , z)ii iend

Let f'(x , ..., x ) = LFP(f (x , ..., x ), x ) and f'(x , ..., x ) = LFP(f' (x , ..., x ), x ), i = n-1, ..., p+1.  Byn 1 n-1 n 1 n n i 1 i-1 i+1 1 i i
preserving invariants

(68) x = f'(x , ..., x ), i = p+1, ..., ni 1 i-1i
within code (67), we have the following hybrid solution for system (60):

(69) u"" {x , ..., x }p+1 n

<x := 0 , i = 1, ..., pii

(while $ i = 1, ..., p, $ z I^  \Delta  (f , x ))i i i

x := d (x , z)ii iend
>

Transformation 9: The 0-finite solution of the system (60) can also be solved by (69). n
If the variables in e are chosen in such a way that f does not depend on x for i = p+1, ..., n and j = i, i+1, ...,2 i j
n, then the invariants in (68) can be maintained without resorting to fixed point iteration.  In this case, we save the
time and space of maintaining the worksets \Delta  for i = p+1, ..., n.  Furthermore, if (69) contains no occurrences of xii
for  some i=p+1,  ..., n except  for  the  modifications  to x ,  then  the  variable x needs  not  be  maintained  at  all  (cf.i i
vertical loop fusion, section 4.2.)

The  set e with  the  above  property  can  be  found  using  Algorithm  5  below.    It  first  creates  the dependency2
graph of (60)

(70) e = {[i, j]: i, j I^ {1, ..., n} | f directly depends on x }i j
and then repeatedly chooses a node from e with no self-loop and merges it into each of its predecessors until the
graph is stable.  The nodes eliminated from the graph correspond to e .2

Algorithm 5:

e := {1,..,n}1e

:= {}2

e := {[i,j]: i,j I^ {1,..,n} | f directly depends on x }i j
(while $ i I^ e | i I" e{i})1

e less:= i1e with

:= i2

-1(for k I^ e (i))
e less:= [k,i](for t I^ e(i))

e with:=  [k,t]end
end(for t I^ e(i))

e less:=  [i,t]end
end n

Lemma 18: In Algorithm 5, let e be the initial value of e, e' be the final value of e and e' be the0 1 1 2
final value of e .  Then the subgraph e' = {[a, b] I^ e | a, b I^ e'} is acyclic.2 0 2

Proof: Let [a , ..., a ] be a (nonempty) cycle in e .  Since Algorithm 5 does not delete self-loops and1 k 0
does not break cycles, then {a , ..., a } C, e' z' {}.  Thus {a , ..., a } C, e' z' {a , ..., a }. n1 k 1 1 k 2 1 k

40
Since no worksets are needed for the variables in e , we want the size of e to be as small as possible.  But2 1

13according to [27], e is a feedback set of e, and the problem of finding the minimum feedback set is NP-complete.1

Thus,

Theorem 19: The problem of minimizing the number of equations in system (60) by substitution isNP-complete.

n

Example 20: (Grammar  transformation.)    Let G be  a  context  free  grammar  with  grammar  symbols V,
terminals T, and start symbol a.  For each production x I^ G, let lhs(x) be the nonterminal symbol in the left
hand side of x, and let rhs{x} be the set of grammar symbols appearing in the right hand side of x.  We can
apply two transformations on G:

trans1(G) = LFP({x I^ G | rhs{x} I' T E` lhs[s]}, s)
restricts G to all productions that derive strings of terminals and

trans2(G) = LFP({x I^ G | lhs(x) I^ ({a} E` rhs[s])}, s)
restricts G to all productions that are derivable from the start symbol a.

We can find a grammar equivalent to G, but restricted so that each production is derivable from the
start symbol a and each nonterminal derives strings of terminals by first applying trans1 and then trans2 to
G.  (But  not  the  other  way  around!)    Thus  the  required  grammar  is  simply trans2(trans1(G)).  Let G =1
trans1(G) and G = trans2(G ), then Transformation 8 gives the following program:2 1

G := {}1
G := {}2
(while $ z I^ ({x I^ G | rhs{x} I' (T E` lhs[G ])} - G ))1 1

u"" G <G with:= z>2 1end

n

More complicated grammar transformations such as those used to turn a positive context free grammar into
Greibach  normal  form [19]  can  also  be  expressed  as  fixed  point  specifications  and  significant  loop  fusion  of  the
same kind has been observed.

Example 21: Consider  the  following  interprocedural  analysis  problem.    Suppose  we  are  given  a  set  of
procedure names procs of some input program P.  For each procedure f, let params(f) be the tuple of formal
parameters of f, and let calls{f} be the set of call-instructions occurring within f.  For each call-instruction i
I^ calls{f}, let called(i), which can be either a procedure name or a parameter of f, be the procedure called by
i, and let args(i) be the tuple of arguments passed in procedure call i.  For example, suppose P contains the
following procedure declaration:

proc f(x , ..., x )1 n

..

i: call g(y , ..., y )1 m

..

end
Then, f I^ procs, params(f) = [x , ..., x ], i I^ calls{f}, called(i) = g, and args(i) = [y , ..., y ].  Note that an1 n 1 m
argument in a procedure call can be a procedure name or a procedure parameter.

From the above described information, we want to find

13Given a graph G(V, A), a subset V' I' V is called a feedback set of G if every cycle in G includes at least one vertex form V'

41
1. the set pparams{f} of procedure parameters for each procedure f;
2. the set assoc{x, f} of procedures that at runtime might be associated with formal parameter x ofprocedure f;

3. the set maycall{f} of procedures that might call f at runtime;
4. the set callsto{f} of call instructions that might be made to procedure f at runtime.

+An SQ specification of the these four sets is:

the maycall, pparams, callsto, assoc:-1maycall

= {[f,calls (i)]: [f,i] I^ callsto},pparams = {[f,x]: [x,f] I^ domain assoc},

callsto = {[called(i),i]: [f,i] I^ calls | called(i) I^ procs} E`{[g,i]: [f,i] I^ calls, g I^ assoc{called(i),f}},
assoc = {[[params(g)(j),g],args(i)(j)]: [g,i] I^ callsto,j = 1,..,#args(i) | args(i)(j) I^ procs} E`

{[[params(g)(j),g],f]: [g,i] I^ callsto, -1j = 1,..,#args(i), f I^ assoc{args(i)(j),calls (i)}},
minimizing maycall, pparams, callsto, assoc
Algorithm  5  will  compute e =  {maycall, pparams, callsto}  and e =  {assoc},  and  application  of1 2
Transformation 9 will produce the following one-pass program:

u"" {maycall, pparams, callsto}< assoc := {}

(while $ z I^ (f - assoc))assoc with:= z
end >
where

f = {[[params(g)(j),g],args(i)(j)]: [g,i] I^ callsto,j = 1,..,#args(i) | args(i)(j) I^ procs} E`

{[[params(g)(j),g],f]: [g,i] I^ callsto,j = 1,..,#args(i),-1f I^ assoc{args(i)(j),calls (i)}}

n

6. Implementation Issues

We have presented general theorems and transformations for computing and recomputing fixed points.  These
transformations are defined semantically in terms of  program properties that are usually undecidable.   In order to
incorporate these transformations as part of an effective mechanical program development system, we need to define
stronger syntactically defined decidable properties that imply these undecidable semantic properties.  Our approach,
which  is similar  to Sintzoff's  method  of  valuations [71],  is  to  specify  properties using  a  formal  system  of  pattern
directed  inductive  definitions.    Definitions  for  the  following  properties  are  essential  and  have  been  implemented
within RAPTS:  computable, well-typed, monotone, inflationary, and finite.

+In a user-friendly implementation we would also want to use convenient specifications outside of SQ (cf (6)

+or (7)) but transformable into SQ .  Some of these problems will be addressed in this section.

6.1. Inflationary and Deflationary

Inflationary and deflationary conditions must be checked when we use Corollary 2 and Theorem 8.  Although

+these properties are undecidable, a recursive class of inflationary and deflationary SQ expressions can be generated

by composition from a predefined collection of basic inflationary and deflationary functions (see Table 6-1) by the
following rules:

42
f  g  f go--------------------------------------------
inflationary  inflationary  inflationarydeflationary  deflationary  deflationary

and these rules can also be implemented using  an S-attributed grammar.  Note that the function f(s)  = s U' g(s) is
always inflationary, and the function f(s) = s U` g(s) is always deflationary.

6.2. Finiteness and Finite Chain Conditions

Our  transformations  and theorems also  frequently  require conditions such  as ACC, DCC, existence  of 1,  or
existence of 0.  These properties can be provided in or deduced from data type declarations.  For example, ACC and
DCC are implied by a finite poset; the existence of 0 is implied by a UFD lattice.

Transformation 2 may require the condition #range(g) < e^.  This condition holds if g is any subset of an set
theoretical  expression  involving  only 0, 1,  and  input  parameters.  Compile  time  analysis  of  inclusion  and
membership relations have been studied before by Schwartz [68, 67].

6.3. Monotone and Antimonotone

Although  monotonicity  is  undecidable [32],  in  practice  we  can  recognize  a  large  subclass  of  computable
monotone and antimonotone functions as described below:

1. Define basic computable monotone and antimonotone functions as are shown in table 6-1;

2. If  we  know  whether  two  functions f and g are  monotone  or  antimonotone,  then  we  can  determinewhether their composition f g is monotone or antimonotone by the following table:

o
Monotonicity of the composition function f go

f  g  f go---------------------------------------------
monotone  monotone  monotoneantimonotone  monotone  antimonotone
monotone  antimonotone  antimonotoneantimonotone  antimonotone  monotone

3. If f(x, y) is monotone in each of its parameters x and y, then the function g(x) = f(x, x) is monotone in x;if f(x, y) is antimonotone in each parameter x and y, then the function h(x) = f(x, x) is antimonotone in

x;
4. If f(x, y) is a function with a finite range, is monotone in each parameter x and y, and is inflationary inx, then the functions g(w, y) = LFP (f(x, y), x) and h(w, y) = GFP (f(x, y), x) are monotone in w and y

w wand have finite ranges.

The  preceding  rules  define  a  decidable  sublanguage  of  computable  monotone  (respectively  antimonotone)
+SQ functions,  which  we  call positive (respectively negative)  functions  that  can  be  recognized,  say,  by  an  S+attributed attribute grammar (see [4]).  For example, the SQ function

f(x) = s - (t - x)
is recognized as positive, because it is the composition of two negative basic functions f1(e) = s - e and f2(x) = t - x.
But the monotone function

f(x) = x - (x C, s)
is not, because we have no rules for the difference of two positive functions.

We  are  currently  developing  a  transformational  programming  system  in  which  these  and  other  properties
defined by inductive definitions are implemented.  Our results will be reported in a subsequent paper.

43
Let s, t be sets, and let f be a map (set of pairs), and let k, p be predicates
Expression Parameters that satisfying conditions

Monotone Anti- infla- deflat-monotone tionary ionary

x I^ s sy I^ f{x} f
f{x} ff[s] s and f

-1f f
domain f frange f f

#s s{xI^s|k(x)} s and k s
s C, t s and t s and ts - t s t s
s E` t s and t s and ts * t s and t
$xI^s|k(x) s and k"xI^s|k(x) k s
~k kk and p k and p k and p
k or p k and p k and p

Table 6-1: Basic monotone, antimonotone, inflationary, and deflationary functions

6.4. Equational Form

We  have  mentioned  problem  specifications  that  can  sometimes  be  expressed  more  conveniently  outside  of
+SQ in either of the two forms:

(71) the s: 0 <= s | k(s) minimizing s
or

(72) the s: 1 >= s | k(s) maximizing s
Before  any  of  our  fixed  point  transformations  can  be  applied,  we  must  first  transform k(s)  into  an  equivalent
equational form

s = f(s)
in which f is monotone.  We have no general method to do this, but the following transformations have proved to be
very useful:

1. the s: " x I^ s | k(x) maximizing s Th GFP(s - {x I^ s | not k(x)}, s)

2. the s: {x I^ s | k(x)} = {} maximizing s Th GFP(s - {x I^ s | k(x)}, s)
3. f(s) <= s Th s = s U' f(s)
4. s <= f(s) Th s = s U` f(s)
5. negation elimination and De Morgan's rules

Other useful rules for deriving normal form specifications for (71) and (72) include:

44
1. the s: s >= 0 | s = f(s) and s = g(s) minimizing sU^

LFP(h (s) U' h (s), s)1 2
U^LFP(h h )

o2 1U^

LFP(h h )o1 2
where h (x) = LFP (f) and h (x) = LFP (g).21 <=, x <=, x

2. the s: s >= 0 | s = f(s) or s = g(s) minimizing sU^

the s: s = min(LFP(f), LFP(g))
where we assume that both LFP(f) and LFP(g) are defined.

3. the s, t: s >= v, t >= 0 | s = f(t) and t = g(s) minimizing s, tU^

LFP (f(g(s)), s)<=,v
4. the s: s >= v and s >= v | s = f(s) minimizing s1 2U^

LFP (f)<=, v U' v1 2
The above rules are merely a preliminary ad hoc collection.

7. Conclusion
7.1. A Survey of Related Work

Earlier  in  the  paper  the  important  connection  between  fixed  point  computation  and  global  program
optimization  was  mentioned.    More  recently,  fixed  point  computation  has  also  had  a  strong  impact  on  relational
databases.  In  1979  Aho  and  Ullman [2]  noted  that  Codd's  Relational  Calculus  is  unable  to  express  transitive
closure,  and  they  suggested  extending  Relational  Calculus  with  fixed  point  operators.    Their  paper  triggered  an
extensive study into the expressive power of languages with fixed point constructs.

In  the  theoretical  direction  logicians  have  made  rapid  progress.    In  1982  Immerman [39]  and  Vardi
[79] proved that, in the presence of a linear order (<=), every relational query computable by a Turing Machine in
polynomial  time with  respect to  the size  of its  input  is  expressible  in  first order  logic (FO)  extended  with a least
fixed  point  operator  (LFP).  Conversely,  every FO(<=)  + LFP query  is  computable  by  a  Turing  Machine  in
polynomial  time  with  respect  to  the  cardinality  of  the  given  structure.    Immerman  also  showed  that  any  query
expressible with nested fixed points can be expressed with a single least fixed point application.

One  problem  is  that  fixed  point  operations  can  be  computed  most  easily  for  monotone  formula,  but
monotonicity is undecidable [32].  Also, not every formula monotone in its parameter P is equivalent to a formula
positive in P [5].  Since positivity is decidable, it is fortunate that Gurevich and Shelah [34] proved that, for every
monotone formula y, there is a positive formula y' such that y and y' have the same least fixed point.

In the pragmatic direction, database researchers have been looking for efficient ways of computing the least
fixed point defined by a set of recursive logical queries. They all noticed that the repeated computation of expensive
expressions in the fixed point iteration is one of the main sources of inefficiency.  Numerous strategies have been
proposed to solve this problem.

In 1981 McKay and Shapiro [47] presented a rule-based inference system that can handle recursive rules. In
their  system,  queries  are  implemented  as processes representing  nodes  in  a  graph  with  edges  reflecting

45
producer-consumer relationships between processes.  Each process consumes information from its child processes
and produces information to its parent processes until the whole system reaches a fixed point.  No data is transmitted
more than once along the same path.  While their method is AI oriented, their one-element-at-a-time strategy is quite
close to the principle of our finite differencing.

In  1984,  Henschen  and  Naqvi [37]  suggested  that  a  recursive  query  could  be  expanded  into  a  set  of
nonrecursive ones that could be evaluated iteratively. The expansion terminates when no more solutions are found.
Their method works well when the query dependency graph forms a single cycle, but in more general situations the
control structures of the generated programs would be very complicated.  In 1986, Naughton [50] showed that under
some conditions, this expansion can be done at compile time.

In 1985, Ullman [78] presented a more general approach.  Instead of finding a single solution, he suggested
the  use  of  different  strategies  (capture  rules)  for  different  queries.    This  idea  is  fully  developed  in  the  design  of
NAIL! [48].  One interesting strategy used in NAIL! is to divide the whole system of queries into subsystems, with
each subsystem corresponding to a strongly connected component in the dependency graph. These subsystems are
solved one by one in a topologic order. This strategy further reduces the number of worksets.

In 1987, Kyu-Young Whang [81] introduced a rudimentary form of fixed point iteration together with finite
differencing to evaluate recursive logic queries efficiently.  He noticed that when this technique is applied to linear
recursive  queries,  a  one  pass  algorithm  can  be  obtained.    In  the  same  article,  he  also  raised  the  question  of
minimizing the number of worksets.

7.2. Future Work and Conclusions

Although much work remains to be done before we can say how useful our fixed point transformations are in
practice,  we  have  presented  numerous  examples  where  they  could  be  applied  effectively  to  nontrivial  problems,

+specified conveniently in SQ .  Besides the examples presented here, we have also shown that a significant fragment

of an optimizing compiler [11] and even the problem of planarity testing [10] are amenable to our transformational
methodology.

Several open problems and further research arise from this work.  We mention some of these briefly below:
1. Are there more general conditions under which the least and greatest fixed points can be recomputedefficiently?

2. Can our previous work characterizing a class of linear time set theoretic fixed point expressions [11]be generalized to fixed points over more general lattices and semilattices?
3. Can our transformations be used to implement Prolog queries efficiently?
4. Can  they  be  applied  to  implement  circular  attribute  grammars  efficiently  along  the  lines  ofFarrow [24]?

Conventional software development methodology can be derived from the following informal schema:
(converge)problem formulation;

algorithm design;programming;
debugging;end

Problem formulation and algorithm design involve the most conceptual work, and programming and debugging are
the  most  tedious.    We  have  shown  in  this  paper  rudimentary  theoretical  underpinnings  for  a  computer  assisted
program  development  system  in  which  low  level  debugging  is  unnecessary,  programming  is  highly  simplified,
algorithm design is facilitated by a small collection of powerful transformations, and the major effort is in problem
specification (where errors can still be introduced).

46
Backus [6] has criticized conventional programming languages for 'their close coupling of semantics to state
transitions, their division of programming into a world of expressions and a world of statements, their inability to
effectively  use  powerful  combining  forms  for  building  new  programs  from  existing  ones,  and  their  lack  of  useful
mathematical properties for reasoning about programs'.  We hope that our approach can overcome some of these
weaknesses.

References
1. Aczel, P.  An Introduction to Inductive Definitions.  In Handbook of Mathematical Logic, Barwise, J., Ed., North-Holland,1977, pp. 739-782.

2. Aho, A.V. and Ullman J.D.  Universality of Data Retrieval Language.  Proc. 6th ACM Symp. on Principles of ProgrammingLanguages, 1979, pp. 110-117.
3. Aho, A., Hopcroft, J., and Ullman, J. Design and Analysis of Computer Algorithms. Addison-Wesley, 1974.
4. Aho, A., Sethi, R., and Ullman, J. . Compilers. Addison-Wesley , 1986.
5. Ajtai, M and Gurevich, Yuri.  "Monotone versus Positive". JACM 34, 4 (Oct. 1987), 1004-1015.
6. Backus, J.  "Can Programming Be Liberated from the von Neumann Style?  A Functional Style and Its Algebra of Programs".CACM 21, 8 (Aug 1978), 613-641.

7. Bauer, F. L. and Wossner, H. Algorithmic Language and Program Development. Springer-Verlag, Berlin, 1982.
8. Birkhoff, G. Lattice Theory. American Mathematical Society, Providence, 1966.
9. Cai, J. Fixed Point Computation and Transformational Programming.  Ph.D. Th., Rutgers University, May 1987. appears inRutgers Technical Report DSC-TR-217.

10. Cai, J.  An iterative Version of Hopcroft and Tarjan's Planarity Testing Algorithm.  Tech. Rept. 324, NYU/Courant Insitute,Oct., 1987.
11. Cai, J. and Paige, R.  Binding Performance at Language Design Time.  Proc. 14th ACM Symp. on Principles ofProgramming Languages, Munich, West Germany, January, 1987, pp. 85-97.
12. Cocke, J. and Kennedy, K.  "An Algorithm for Reduction of Operator Strength". CACM 20, 11 (Nov 1977), 850-856.
13. Cocke, J. and Schwartz, J. T. Programming Languages and Their Compilers. CIMS, New York University, 1969.  LectureNotes.

14. Codd, E.  "A Relational Model of Data for Large Shared Data Banks". CACM 13, 6 (June 1970), 377-387.
15. Cousot., P.  Semantic Foundations of Program Analysis.  In Program Flow Analysis, Muchnick, S., Jones, N., Eds., PrenticeHall, 1981, pp. 303-342.

16. Cousot, P. and Cousot, R.  Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs by Constructionor Approximation of Fixed points.  Proc. 4th ACM Symp. on Principles of Programming Languages, Los Angeles, CA, Jan,
1977, pp. 238-252.
17. Cousot, P. and Cousot, R.  "Automatic Synthesis of Optimal Invariant Assertions:  Mathematical Foundations". SIGPLANNotices 12, 8 (Aug 1977), 1-12.

18. Cousot, P. and Cousot, R.  "Constructive Versions of Tarski's Fixed Point Theorems". Pacific J. Math 82, 1 (May 1979),43-57.
19. Davis, M. and Weyuker, E. Computability, Complexity, and Languages. Academic Press, 1983.
20. de Bakker, J. Mathematical Theory of Program Correctness. Prentice-Hall, 1980.
21. Dewar, R., Grand, A., Liu S. C., Schwartz, J. T., and Schonberg, E.  "Program by Refinement, as Exemplified by the SETLRepresentation Sublanguage". TOPLAS 1, 1 (July 1979), 27-49.

22. Earley, J.  "High Level Iterators and a Method for Automatically Designing Data Structure Representation". Journal ofComputer Languages 1, 4 (1976), 321-342.
23. Emerson, E.A. and Lei, C. L.  Model Checking in the Propositional Mu-Calculus.  Tech. Rept. 86-06, University of Texas atAustin, Department of Computer Sciences, 1986.

47
24. Farrow R.  Automatic generation of fixed-point-finding evaluators for circular, but well-defined, attribute grammars.  Proc.ACM Symp. on Compiler Construction, 1986, pp. 85-99.
25. Feather, Martin S.  A survey and classification of Some Program Transformation Approaches and Techniques.  In IFIP TC 2WG2.1 Working Conference on Program Specification and Transformation, Bad Tolz, FRG, 1986, L. G. L. T. Meertens, Ed.,
North-Holland, 1987, pp. 165-196.
26. Fong, A. and Ullman, J.  Induction Variables in Very High Level Languages.  Proc. 3rd ACM Symp. on Principles ofProgramming Languages, Jan, 1976, pp. 104-112.

27. Garey, M. and Johnson, D. Computers and Intractability. Freeman and Johnson, 1979.
28. Goldberg, A. and Paige, R.  Stream Processing.  Proc. ACM Symp. on LISP and Functional Programming, Aug, 1984, pp.53-62.

29. Gordon, M., Milner, A., and Wadsworth, C. Edinburgh LCF. Springer-Verlag, 1979.
30. Gratzer, G. General Lattice Theory. Birkhauser, 1978.
31. Gries, D.  "Describing an Algorithm by Hopcroft". Acta Informatica 2 (1973), 97-109.
32. Gurevich, Y.  Toward Logic Tailored for Computational Complexity.  Lecture Notes in Math:  Computation and ProofTheory, 1984, pp. 175-216.

33. Gurevich, Y.  Logic and the Challenge of Computer Science.  In Current Trends in Theoretical Computer Science, EgonBoerger, Ed., Computer Science Press, 1987.
34. Gurevich, Y. and Shelah, S.  Fixed-point Extensions of First-order Logic.  26th Annual Symposium on Foundation ofcomputer Science, 1985, pp. 346-353.
35. Harel, D. and Kozen, D.  "A Programming Language for the Inductive Sets, and Applications". Information and Control 63,2 (1985), 1-27.
36. Hecht, M. and Ullman, J.  "A Simple Algorithm for Global Data Flow Analysis Problems". SIAM J. Comput. 4, 4(December 1975), 519-532.
37. Henschen,L.J. and Naqvi,S.A.  "On Compiling Queries in Recursive First-order Database". CACM 31 (1984), 47-85.
38. Hopcroft, J.  An n log n Algorithm for Minimizing States in a Finite Automaton.  In Theory of Machines and Computations,Kohavi and Paz, Ed., Academic Press, New York, 1971, pp. 189-196. Proc. Intl. Symp. on Theory of Machines and Computation.

39. Immerman, N.  Relational queries computable in polynomial time.  Proc. 14th ACM Symp. on Theory of Computing, May,1982, pp. 147-152.
40. Immerman, N.  Languages Which Capture Complexity Classes.  Proc. 15th ACM Symp. on Theory of Computing, April,1983, pp. 347-354.
41. Kam, J. and Ullman, J.  "Global Data Flow Analysis and Iterative Algorithms". JACM 23, 1 (1976), 158-171.
42. Kam, J. and Ullman, J.  "Monotone Data Flow Analysis Frameworks". Acta Informatica 7 (1977), 305-317.
43. Kaplan, M. and Ullman, J.  "A Scheme for the Automatic Inference of Variable Types". JACM 27, 1 (1980), 128-145.
44. Kildall, G.  A Unified Approach to Global Program Optimization.  Proc. 1st ACM Symp. on Principles of ProgrammingLanguages, Oct, 1973, pp. 194-206.

45. Kleene. Introduction to Meta-Mathematics. Van Nostrand, Princeton, NJ, 1952.
46. Marlowe, T., Paull, M.C. and Ryder, B.  Applicability Of Incremental Iterative Algorithms.  Tech. Rept. DCS-TR-159,Rutgers University, Aug, 1985.

47. McKay, D. and Shapiro, S.  Using Active Connection Graphs for reasoning with resursive rules.  Proceedings 7th IJCAI,1981, pp. 368-374.
48. Morris, K., Ullman, J. D., and Gelder, A. V.  Design Overview of the NAIL! System.  Tech. Rept. STAN-CS-86-1108,Stanford University, May, 1986.
49. Naqvi, S. and Henschen, L.  Synthesizing Least Fixed Point Queries into Non-recursive Iterative Programs.  IJCAI 83, Aug,1983, pp. 25-28.
50. Naughton, J. F.  Optimizing Function-Free Recursive Inference Rules.  Tech. Rept. STAN-CS-86-1114, Stanford University,1986.

48
51. O'Keefe, R..  Finite Fixed-Point Problems.  Logic Programming, 1987, pp. 729-743. Proc. 4th Intl. Conf. on Logic Prog..
52. Paige, R.  Transformational Programming -- Applications to Algorithms and Systems.  Proc. 10th ACM Symp. on Principlesof Programming Languages, Jan, 1983, pp. 73-87.

53. Paige, R.  Supercompilers - Extended Abstract.  In Program Transformation and Programming Environments, P. Pepper,Ed., Springer Verlag, 1984, pp. 331-340.
54. Paige, R., Tarjan, R., and Bonic, R.  "A Linear Time Solution to the Single Function Coarsest Partition Problem". TCS 40, 1(Sep 1985), 67-84.
55. Paige, R.  "Programming With Invariants". IEEE Software 3, 1 (Jan 1986), 56-69.
56. Paige, R. and Henglein, F.  "Mechanical Translation of Set Theoretic Problem Specifications Into Efficient RAM Code - ACase Study". Journal of Symbolic Computation 4, 2 (Aug. 1987), 207-232.

57. Paige, R. and Koenig, S.  "Finite Differencing of Computable Expressions". ACM TOPLAS 4, 3 (July 1982), 402-454.
58. Paige, R. and Tarjan, R.  "Three Partition Refinement Algorithms". SIAM J. Comput. 16, 6 (Dec 1987), 973-989.
59. Papadimitriou, C.  "A Note On The Expressive Power of PROLOG". Bullitin of EATCS 26 (1985), 21-23.
60. Paull, M. Algorithm Design - A recursion Transformation Framework. Wiley, 1988.
61. Reif, J. H. and Lewis, H. R.  Symbolic evaluation and the global value graph.  Proc. 4th ACM Symp. on Principle ofProgramming Languages. , Jan, 1977, pp. 104-118.

62. Reif, J. and Scherlis, W.  Deriving Efficient Graph Algorithms.  Carnegie-Mellon U., 1982. Technical Report.
63. Reps, T., Teitelbaum, T., and Demers, A.  "Incremental Context-Dependent Analysis for Language-Based Editors". ACMTOPLAS 5, 3 (July 1983), 449-477.

64. Rogers, H., Jr. Theory of Recursive Functions and Effective Computability. McGraw Hill, 1967.
65. Schonberg, E., Schwartz, J. T., and Sharir, M.  "An Automatic Technique for Selection of Data Representations in in SETLPrograms". ACM TOPLAS 3, 2 (Apr 1981), 126-143.

66. Schwartz, J. T. On Programming: An Interim Report on the SETL Project, Installments I and II. CIMS, New York Univ.,New York, 1974.
67. Schwartz, J. T.  "Optimization of Very High Level Languages, Parts I, II". J. of Computer Languages 1, 2,3 (1975),161-218.
68. Schwartz, J.T.  "Automatic Data Structure Choice in a Language of Very High Level". CACM 18, 12 (Dec 1975), 722-728.
69. Scott, D. S.  "Data Types as Lattices". SIAM J. Comptng 5 (1976), 522-587.
70. Sharir, M.  "Some Observations on Formal Differentiation". ACM TOPLAS 4, 2 (Apr. 1982), 196-225.
71. Sintzoff, M.  "Calculating Properties of Programs by Valuations on Specific Models". ACM SIGPLAN Notices 7, 1 (1972),203-207.

72. Suppes, P. Axiomatic Set Theory. Dover, 1972.
73. Tarjan, R. E.  "Depth first search and linear graph algorithms". SIAM J. Computing 1, 2 (1972), 146-160.
74. Tarjan, R.  Iterative Algorithms for Global Flow Analysis.  In Algorithms and Complexity - New Directions and RecentResults, J. Traub, Ed., Academic Press, 1976, pp. 71-101.

75. Tarski, A.  "A Lattice-Theoretical Fixpoint Theorem and its Application". Pacific Journal of Mathematics 5 (1955),285-309.
76. Teitelbaum, T. and Reps, T.  "The Cornell Program Synthesizer: a syntax-directed programming environment". CACM 24, 9(Sep 1981), pp. 563-573.
77. Tenenbaum, A. Type Determination for Very High Level Languages.  Ph.D. Th., New York University, Dept. of ComputerScience, Oct 1974. appears in Courant Computer Science Report 3.
78. Ullman, J.D.  "Implementation of Logical Query Languages for Databases". ACM TODS 10, 3 (September 1985), 289-321.
79. Vardi, M.  Complexity of Relational Query Languages.  Proc. 14th ACM Symp. on Theory of Computing, May, 1982, pp.137-146.

49
80. Wegman, M. N. and Zadeck, F. K.  Constant Propagation with Conditional Branches.  Proc. 12th ACM Symp. on Principlesof Programming Languages, Jan, 1985, pp. 291-299.
81. Whang, K., and Navathe, S. B.  An Extended Disjunctive Normal Form Approach for Processing Recursive Logic Queries inLoosely Coupled Environments.  Proc. 13th Intl. Conf. on Very Large Data Bases, Sept., 1987, pp. 275-287.

Appendix I. Turing Machine Simulation
In 1982, Immerman [39] and Vardi [79] proved that every relational query over a finite linearly ordered set D
and computable by a Turing Machine in polynomial time with respect to the cardinality of D is expressible in first
order logic extended with a least fixed point operator.  Papadimitriou [59] proved similar results for PROLOG.  The

+following theorem indicates the expressive power of SQ .

Theorem  20: Every  partially  computable  function f: N o"" N over  the  natural  numbers N can  be+expressed in SQ .
Proof: Let f(n) be any partially computable function.  Then there is a deterministic Turing machinen f(n)TM that inputs the string 1 and outputs the string 1 if f(n) is defined, and will never stop otherwise.

+From this machine, we can construct an SQ expression for f(n) as follows:
We assume that TM has a finite, but ever-growing tape that initially contains only the input in theform of n 1's with one blank cell at each end.  After each move of the read-write head, the tape grows by

one blank cell at its right end.  Note that this tape is equivalent to a one-way infinite tape.We represent the next-move function d of TM as a set of quintuples [19] of the form:

[q , s , s , +1, q ]i j k lor
[q , s , s , -1, q ]i j k l
The first quintuple signifies that when the machine is in state q scanning s it will print s and then movei j kto the right going into state

q .  The second quintuple is the same, except that the motion is to the left.lWe use a 6-tuple [t, i, d, q, h, c] to represent the fact that the content of cell i is d just before step t of

the computation, when the machine is in state q, and the read-write head is scanning cell h with content c.We use a set comp to collect all the 6-tuples for all tape cells and all computation steps starting from the
initial state.  Then comp is finite if and only if f(n) is defined.  Let D be the alphabet of TM, and Q be theset of states of TM.

Now we discuss what should be contained in comp.
1. Just  before  the  computation  (at  step  0),  the  machine  is  in  state  0,  the  head  is  scanning  cell  0,which contains a blank (B), and cells 1 through n of the tape contain 1's.  The last cell (cell n+1)

is blank.  Therefore, the set

A(n) = {[0, 0, B, 0, 0, B], [0, n+1, B, 0, 0, B]}E` {[0, i, 1, 0, 0, B]: i = 1, ..., n}

must be a subset of comp.
2. If comp contains a 6-tuple for the cell i at step t, it must also contain a 6-tuple for cell i at step t+1if t is not the last step. So comp must also contain the following set:

B(comp)=
{[t+1,i,d,q',h+e,c']: [t,i,d,q,h,c], [t,x,c',q,h,c] I^ comp,[q,c,s,e,q'] I^  d | i z' h and x = h+e} E`
{[t+1,h,s,q',h+e,c']: [t,x,c',q,h,c]I^comp, [q,c,s,e,q']I^d| x = h+e}

3. Since the tape grows by one blank cell after each step of the computation, comp must also containthe following set:

C(n,comp) = {[t,n+t+1,B,q,h,c]: [t,i,d,q,h,c] I^ comp}
Nothing else will be contained in comp.When the computation stops, the state q of the machine and the symbol d scanned by the head must

satisfy the following condition:

50
" [q1,d1,d2,e,q2] I^  d | (q1 = q o"" d1 z' d)
from which the output F, which is the singleton set containing f(n), can be obtained:

F = #{i: [t,i,x,q,h,d] I^ comp | x = 1and (" [q1,d1,d2,e,q2] I^  d | (q1 = q o"" d1 z' d))}

+Thus, the required SQ specification for f(n) is,
(73) the comp, F:comp = A(n) E` B(comp) E` C(n,comp)

F = #{i: [t,i,1,q,h,d] I^ comp |" [q1,d1,d2,e,q2] I^  d | (q1 = q o"" d1 z' d)}
minimizing comp, Foutput F

Specification (73) has a finite solution if and only if TM halts. n

In [39], almost the same argument is used to prove that all polynomial time computable relational queries can
be  expressed  in FO(>=)  + LFP.  The  main  difference  is  that  the  arithmetic  operation x +  1  in FO(>=)  + LFP is
restricted  to  a  range  whose  size  is  polynomial  to  the  size  of  the  input  domain.  It  is  this  restriction  that  prevents
FO(>=) + LFP from simulating a Turing machine that stops in more than a polynomial number of steps.

Since Gurevich [32] has shown that monotonicity is undecidable, we cannot provide an operational semantics

+ + +for SQ .  However, we can provide operational semantics for a subset of SQ called SQ1 in which any expression

LFP (f)  (respectively, GFP (f))  requires  that  function f must  be  positive  and  inflationary  (respectively,<=,w <=,w
deflationary) at w (cf. Section 6.3 for how positive and inflationary functions can be defined).  We can then define
the meaning of expressions LFP (f) and GFP (f) to be the final value of s computed by the following code:<=,w <=,w

s := w(while f(s) z' s)

s := f(s)end

+Corollary 21: Every partially computable function f: N o"" N can be expressed in SQ1 .
Proof: By  the  rules  in Section  6.3 we  can  recognize that  the  right-hand-side  of  both  equations in(73) are positive SQ expressions monotone in comp and F and inflationary at the empty set.

n

With  a  more  sophisticated  encoding  scheme  for  the  input  and  output,  we  can  generalize  the  preceding
Corollary to set valued functions:

Corollary 22: Let f: D * .. * D o"" D be a partially computable function, where D , ..., D , D are1 n f 1 n f

+sets of finite sets of natural numbers. Then f(x , ..., x ) can be expressed in SQ1 with at most one LFP1 n

operation. n

+Further generalization to functions over the full range of SQ datatypes is straightforward.
+The fact that SQ1 can specify a Turing machine itself gives rise to another undecidable problem:

Corollary 23: The problem of whether a positive inflationary SQ function has a finite fixed point isundecidable in general.

Proof: Otherwise,  we  could  solve  the  halting  problem  for  Turing  machines  by  translating  Turing+machines into SQ1 specifications.

n

Acknowledgement We  are  grateful  to  Eric  Allender,  Barbara  Osofsky,  Shmuel  Sagiv,  and  Dan  Yellin  for
their valuable comments.  Special thanks go to Fritz Henglein for his thorough reading of our paper and for many
helpful observations that led to substantial improvements.

i
Table of Contents1. Introduction 1
2. Preliminaries 42.1. Definitions 4

2.2. Language 53. Fixed Point Computation 8
3.1. Basic Theory 83.2. Basic Fixed Point Transformations 10
3.3. Special Functions 143.4. Special Data Types 20
4. Fixed Point Recomputation 254.1. Finite Differencing 26

4.2. Incremental Recomputation of Fixed Points 294.3. Recomputation of Fixed Points of Distributive Functions 33
4.4. Nested Least Fixed Points 365. Systems of Equations 37
5.1. Elimination Method 375.2. Minimizing the Number of Worksets 38
6. Implementation Issues 416.1. Inflationary and Deflationary 41

6.2. Finiteness and Finite Chain Conditions 426.3. Monotone and Antimonotone 42
6.4. Equational Form 437. Conclusion 44
7.1. A Survey of Related Work 447.2. Future Work and Conclusions 45

ii
List of Tables+Table 2-1: SQ expressions 6
Table 6-1:  Basic monotone, antimonotone, inflationary, and deflationary functions 43