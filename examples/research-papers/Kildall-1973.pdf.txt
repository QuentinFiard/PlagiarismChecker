

A UNIFIED APPROACH TO GLOBAL

PROGRAM OPTIMIZATION

Gary A. Kildall
Computer Science Group
Naval Postgraduate School

Monterey, California

Abstract
A technique is presented for global analysie of program structure in order to perform compile time
optimization of object code generated for expressions. The global expression optimization presented

includes constant propagation, common subexpression elimination, elimination of redundant register load
operations, and live expression analysis. A general purpose program flow analysis algorithm is developed
which depends upon the existence of an "optimizing function." The algorithm is defined formally using a

directed graph model of program flow structure, and is shown to be correct, Several optimizing functions
are defined which, when used in conjunction with the flow analysis algorithm, provide the various forms of
code optimization. The flow analysis algorithm is sufficiently general that additional functions can easily
be defined for other forms of globa~ cod: optimization.

execution-time.1.
INTRODUCTION of the graph represent program control flow DossiA number of techniques have evolved for the bilities between the nodes at
compile-time analysis of program structure in order

to locate redundant computations, perform constant
computations, and reduce the number of store-load
sequences between memory and high-speed registers.
Some of these techniques provide analysis of only
straight-line sequences of instructions [5,6,9,14,
17,18,19,20,27,29,34,36,38,39,43,45 ,46], while
others take the program branching structure into
account [2,3,4,10,11,12,13,15,23,30, 32,33,35].
The purpose here is to describe a single program

flow analysis algorithm which extends all of

these straight-line optimizing techniques to include branching structure. The algorithm is presented formally and is shown to be correct. Implementation of the flow analysis algorithm in a

practical compiler is also discussed.

The methods used here are motivated in the
section which follows.

2. CONSTANT PROPAGATION

A fairly simple case of program analysis
and optimization occurs when constant computations
are evaluated at compile-time. This process is
referred to as "constant propagation," or "folding."
Consider the following skeletal ALGOL 60 program:

= integer i,a, b,c, d,e;

a:=l; c:=O; . . .
~ i:=l steu 1 until 10 @

-bb:=z; " . ":- ;...

e:=b+c; . . .

c:= 4; . . .
end--
end--

This program is represented by the directed graph

shown in Figure 1 (ignoring calculations which control the for-loop). The nodes of the directed
graph represent sequences of instructions containing no alternate program branches, while the edges

o
entry' ~:=j

iii

B &()

c b=2

Dd= a+b

e=btc
F C:=4

Figure 1. A program graph corresponding to

an ALGOL 60 program containing one loop.

For purposes of constant propagation, it is
convenient to associate a "pool" of propagated

constants with each node in the graph, The pool
is a set of ordered pairs which indicate variables
which have constant values when the node is encountered. Thus, the pool of constants at node B, denoted by PB, consists of the single element (a,l)

since the assignment a:=l at node A must occur
before node B is encountered during execution of

the program.

The fundamental global analysis problem is
that of determining the pool of propagated constants
for each node in an arbitrary program graph. By

inspection of the graph of Figure 1, the pool of
constants at each node is

194

pA=@ PD = {(a,l), (b,2)}
PB = {(a,l)} PE = {(a,l), (b,2), (d,3)}

Pc = {(a,l)} PF = {(a,l),(b,2),(d,3)} .

In the general case, PN could be determined for
each node N in the graph as follows. Consider each
path (A,p ,p2,. ..,Pn, N) from the entry node A tothen.de~. Apply constant propagation throughout

this path to obtain a set of propagated constantsat node N for this path only. The intersection
of the propagated constants determined for eachpath to N is then the set of constants which can
be assumed for optimization purpoees, since it isnot known which of the paths will be taken at

execution-time.

The pool of propagated constants at node D of
Figure 1, for example, can be determined as follows.
A path from the entry node A to the node II is

(A, B,c,D). Considering only this path, the "first
approximation" to PD is

P1 = {(a,l), (b,2), (c,0)}D

A longer path from A to D is (A,B,C,D,E,F,C,D)
which results in the pool

P; = {(a,l),(b,2), (c,4), (cl,3), (e,2) }
corresponding to this particular path to D. successively longer patha from A to D can be evaluated, resulting in Pa, pi, . . ..pfi for arbitrarily

large n. The pool of propagated constants which
can be assumed no matter which flow of control,
occurs is the set of constants common to all P%;

that is,

PD = n i`D

Vi

This procedure, however, is not effective
since the number of such paths may have no finite
bound in the case of an arbitrary directed graph.
Hence, the procedure would not necessarily halt.
The purpose of the algorithm of the following

section is to compute this intersection in a
finite number of steps.

3. A GLOBAL ANALYSIS ALGORITHM

The analysis of the program graph of Figure
1 suggests a solution to the global constant propagation problem. Considering node C, the first
approximation to P is given by propagating

constants along th~ path (A, B, C), resulting in

1
`c = {(a,l), (c, O)}.

Based upon this approximate pool, the first

approximations to subsequent nodes can be determined:

P1 = {(a,l),(c,o) >(bsz)}~

D

P1 = {(a,l),(c,0),(b,2),(d,3)}E

d = {(a,l),(c,0),(b,2),(d,3),(e,2) }.F

Using P;, the constant pool resulting from node F

entering node C is

P = {(a,l),(b,2),(d,3),(e,2),(c ,4)}.
Note, however, that since n

i
`c = `cvi

it follows that Pc ~ P: n P: . Thus, rather than

2 = P, the second approximation to Pc isassuming Pc
taken as

P: =P~nP=P~n
{(a,l),(b,2), (d,3),(e,2),(c,4)} = {(a,l)}.

Using P;, the circuit through the looP Past C is

traced once again. The next approximation at subs~quent nodes can then be determined based upon

P; n {(a,l), (b,2)} = {(a,l), (b,2)},
P; n {(a,l),(b,2),(d,3)}

{(a,l),(b,2),(d,3)} ,
p; n {(a,l),(b,2), (d,3)}

{(a,l), (b,2), (d,3)}.

Continuing around the loop once again from node F

to node C, the third approximate pool P; ia
determined as

p: = p: n {(a,l),(b,2), (d,3)} = {(a,l)}.
Clearly, no changes to subsequent approximate pools
W-ll o cur if the circuit is2traversed again since35

Pc = Pc, and the effect of Pc on the pools in the

circuit has already been investigated. Thus, the
analysis stops, and the last approximate pools at
each node are taken as the final constant pools.
Note that these last approximations correspond to

the constant pools determined earlier by inspection.

Based upon these observations, it is possible
to informally state a global analysis algorithm.

a.

b.

c.

d.

Start with an entry node in the program graph,
along with a given entry pool corresponding to

this entry node. Normally, there is only one
entry node, and the entry pool is empty.

Process the entry node, and produce optimizing

information (in this case, a set of propagated
constants) which is sent to all immediate

successors of the entry node.

Intersect the incoming optimizing pools with

that already established at the successor nodes

(if this is the first time the node is encountered, assume the incoming pool as the first
approximation and continue processing),

Considering each successor node, if the amount
of optimizing information is reduced by this

intersection (or if the node has been encoun-

tered for the first time) then process the
successor in the same manner as the initial
entry node (the order in which the successor

195

nodes are processed is unimportant).

In order to generalize the above notions, it
is useful to define an "optimizing function" f
which maps an "input" pool, along with a particular

node, to a new "output" pool. Given a particular

set of propagated constants, for example, it is
possible to examine the operation at a particular

node and determine the set of propagated constants
which can be assumed after the node is executed,

In the case of constant propagation, the function
can be informally stated as follows. Let V be
a set of variables, let C be a set of constants,
and let ~ be the set of nodes in the graph being

analyzed, The set U = V X C represents ordered
pairs which may appear in any constant pool. In

fact, all constant pools are elements of the power
set of U (i.e., the set of all subsets of U),
denoted by P(U). Thus,

f: Ix P(u) + P(u),
where (v,c) c f(N,P) <=>

a.

b.

(v, c) c P and the operation at node N does not
assign a new value to the variable v, or

the operation at node N assigns an expression
to the variable v, and the expression evaluates
to the constant c, based upon the constants in
P.

Consider the graph of Figure 1, for example.
The optimizing function can be applied to node A
with an empty constant pool resulting in

f(A,@) = {(a,l)}.
Similarly, the function f can be applied to node
B with {(a,l)} as a constant pool yielding

f(B, {(a,l)}) = {(a,l), (c, O)}.
Note that given a particular path from the entry
node A to an arbitrary node N e ~, the optimizing

pool which can be assumed for this path is determined by composing the function f up to the last
node of the path. Given the path (A,B,c,D), for

example,

f(C,f(B,f(A,@))) = {(a,l),(c,O), (b,2)}
is the constant pool at D for this path.

The pool of optimizing information which
can be assumed at an arbitrary node N in the graph
being analyzed, independent of the path taken at

execution time, can now be stated formally as

PN = n xXEI?

N
where

FN = {f(pn,f(Pn_la ..o,f(P1,p)). .,)1

(P1>P2, . ..>Pn) N) is a path from an entry node PI
with corresponding entry pool P to the node N}.

Before formally stating the global analysis
algorithm, it is necessary to clarify the fundamental notions.

A finite directed graph G = <~, E> is an
arbitrary finite set of "nodes" N an~ "edges"
EcNxN. A "path" from A to B-in G, for A,B c ~,

~s-a--seq~ence of nodes (p1,P2, . ..Pk)3 PI = A and

Pk= B, where (pi,p +1)6 ~ Vi, 1 < i . k. The

t"length" of a path p1,p2,. ..,Pk) is k-1.

A "program graph" is a finite directed graph
G along with a non-empty set of "entry nodes"
c SY such that given N c YH a path (Pi,...,Pn)

3 p1c8 and pn = N (i.e., there is a path to every

node in the graph from an entry node),

The set of "immediate successors" of a node N
is given by

I(N) = {N' elil 3 (N,N') c~}.

Similarly, the set of "immediate predecessors" of
N is given by

I-l(N) = {N' l ~lLI(N',N) e E}.

Let the finite set ~ be the set of all possi-
ble optimizing pools for a given application (e.g.,
~ = P(u) in the constant propagation case, where

U = V x C), and A be a "meet" operation with the
properties

A: P X P +~,--.
x A y = y A x (commutative),
x A (y A Z) = (x A y)A z (associative),

where x, y, z c ~. The set P and the A operation

define a finite meet-semilat~ice.

The A operation defines a partial ordering on
~ given by

x < y <=> x A y= xVX,Y E P.--

Similarly,

x < y <=> x s y S,nd X# y.

Given X c P Athe generalized meet operation x ~ ~-- --,

is defined simply as the pairwise application of
A to the elements of X. ~ is assumed to contain

a "zefo element" Q3 ~ s x Vx e P. An augmented

set ~ is constructed from P by adding a "unit
element" ~ with the propert~es ~ ~ ~ and ~ A x =

xvxc~;~'=gu {l}. It follows that--x < lVX c P,

-- --

An "optimizing function" f is defined

f: N x P +P---- --
and must have the homomorphism property:

f(N,x A y) = f(N,x) A f(N,y), N 6 ~, X,y c ~.
Note that f(N,x) < ~ VN e ~ and x ~ ~.

The global analysis algorithm is now stated:
Algorithm A. Analysis of each particular program

graph G depends upon an "entry pool set" ~~~ x ~,
where (e, x) 6 ~ if e c C is an entry node with

196

corresponding entry optimizing pool x e ~.
Al[initialize] L+6
A2[terminate ?] If L = @ then halt.
A3[select node] Let L' ~ L, L' = (N$Pj-) for

some N < N and Pi c ~,
L-+L-{~t}

A4[traverse?] I,et PN be the current approximate pool of optimizing information associated with the node
N (initially, PN = ~).

If PN s Pi then go to step A2.

A5[set pool] p+p Ap., L+Lu

{~N',f~N,~j)lN' E I(N)}.

A6[loop] Go to step A2.

For purposes of constant propagation,
~ = P(u), where U = V x C, as before. The meet

operation is n, and the less-than-or-equal relation ia c. Note that the zero element in this
case is T e P(U), The unit element in P(U) is U
itself. The algorithm requires a new unit element,
however, which is not in P(u). The new unit

element is constructed as follows: let 6 be a
symbol not in U, and let ~= U u {6}. It follows

that~n x= XVX c P(u) and~~ P(U). Thus,
~' .1 u {y} is obtained from ~ by adding a unit

element ~. Aa demonstrated in the proof in Theorem

2, the addition of the symbol 6 to U causes the
algorithm A to consider each node in the program
graph at least once.

Appendix A shows the analysis of the program
graph of Figure 1 using the entry pool aet
~= {(A,@)}.

Theorem 1. The algorithm A is finite.
Proof. The algorithm A terminates when L = 0.

Each evaluation of step A3 removes an element

from L, and elements are added to L only in step
A5 . Thus, A is finite if the number of evaluations of step A5 is finite. Informally, each
evaluation of step A5 reduces the "size" of the
pool PN at some node N. Since the size cannot

be less than ~, the process must be finite.

Formally, step A5 ia performed only when

PN ~ pN A pi.

`Ut `pNAp$)p~%~Np~~ip;~~.PN A Pi s PN, and PN A pi
Thus, the approximate pool PN at node N can be

reduced at most to ~ since P + P A P..N y _ Further,

since the first approximation to isll and the

lattice is finite, it follows thatNstep A5 can
be performed only a finite number of times. Thus
A is finite.

An upper bound on the number of steps in the
algorithm A can easily be determined. Let n be

the cardinality of ~ and h(~') be a function of
P' (which, in turn, may be a function of n) providing the maximum length of any chain between ~

and ~ in ~'. Step A5 can be executed a maximum of
h(~') times for any given node. Since there are n

nodes in the program graph, step A5 can be performed no more than n l h(~') times.

In the case of constant propagation, for
example, let u be the cardinality of U. The size
of U varies directly with the number of nodes n.
In addition, the maximum length of any chain

`lY"2Y-. .,uk such that u 1 = U and Uk = fl, where
ul~ U2 ~ U3 . ..~llkisll. Thus, h@(U)) = U;

and the theoretical bound is n . u. Since u varies
directly with n, it follows that the order of the

algorithm A is no worse than n2.

The correctness of the algorithm A is guaranteed by the following theorem.

Theorem 2, Let FN = {f(Pn,f(Pn_l ,...$
f(Pl,p)). .)l(Pl, . . ..Pn.N) is a path from an entrynode pl with corresponding entry pool P to the

node N}. Further, let

A
XN=X,F;

corresponding to a particular program graph G, set
P' , and optimizing function f, which satisfy the
~onditions of the algorithm A, If PN is the final

approximate pool associated with node N when A
halts, then PN =~VNcN.

Theorem 2 thus relates the final output of the
algorithm to the intuitive results which were developed earlier. The proof of Theorem 2 is given

in Appendix B.

An interesting side-effect of Theorem 2 is
that the order of choice of elements from L in step
A3 is arbitrary, as given in the following corollary.

Corollary 1. The final pool PN associated
with each node N c ~ upon termination of the algorithm A ia uniquely determined, independent of the
order of choice of L' from L in step A3.
Proof. This corollary follows immediately, since

the proof of Theorem 2 in Appendix B is independent
of the choice of L'@

Since the choice of L' from L in step A3 is
arbitrary, it is interesting to investigate the
effects of the selection criteria upon the algorithm. The number of steps to the final solution

is clearly affected by this choice, No selection
method has been established, however, to maximize

this convergence rate. One might also notice that
by treating accesses to L as critical sections in

steps A3 and A5, the elements of L can be processedin parallel. That is, independent processes can be

started in step A3 to analyze all elements of L.

It is important to note at this point that the
algorithm A allows one to ignore the global analysis, and concentrate upon development of straightline code optimizing functions. That is, if an
optimizing function f can be constructed for optimizing a sequence of code containing no alternative
branches, then the algorithm A can be invoked to

perform the branch analysis, as long aa f satisfies

the conditions of the algorithm.

4. COMMONSUBEXPRESSION ELIMINATION

Global common subexpression elimination involves the analysis of a program's structure in

order to detect and eliminate calculations of re-
dundant expressions. A fundamental assumption is

that it requires less execution time to store the
result of a previous computation and load this
value when the redundant expression is encountered.

197

As an example, consider the simple sequence of
expressions:

. . . r:=a+b; . . . r+x . . . (a+b)+x . . .
which could occur as part of an ALCOL 60 program.

Figure 2 shows this sequence written as a directed
graph. Note that the redundant expression (a+b)
at node v is easily recognized. The entire expres-
sion (a+b)+x at node v is redundant, however, since
r has the same value as a+b at node U, and rtx is
computed at node U ahead of node V. It is only
necessary to describe an optimizing function f
which detects this situation for straight-line

code; the algorithm A will make the function globally applicable.

entry:T

B

r:.a+b
u r+x

v

kl+b)+x

w

Figure 2. An acyclic program graph representing

a simple computation sequence.

A convenient representation for the optimizing
pool in the case of common subexpression elimination is a partition of a set of expressions. The
expressions in the partition at a particular node
are those which occur before the node is encoun-

tered at execution-time.

The optimizing function for common subexpression elimination manipulates the equivalence classes of the partition. Two expressions
are placed into the same class of the partition

if they are known to have equivalent values, Considering Figure 2, for example, the set of expressions which are evaluated before node T is encountered is empty; thus, PT = 0. The expressions
evaluated before node U are exactly those which
occur at node T, including partial computations.
The set of (partial) computations at node T is

{a,b,a+b,r}. Since r takes the value of a+b at
node T, r is known to be equivalent to a+b.
Thus, P ={alb/a+b,r}, where "/" separates the

equival~nce classes of the pool. Similarly,
Pv = {a/b/a+b,rlxlr+x} and PW =

{a/bla+b,rlx/r+x/(a+b)+x}. The expression
a+b at node V is redundant since a+b is in the
pool Pv.

Note, however, that the redundant expression
(a+b)+x at node V is not readily detected. Thisis due to the fact that r+x was computed at node U

and, as noted above, the evaluation of r+x is the

same as evaluation of (a+b)+x at node U. In order
to account for this in the output optimizing pool,

(a+b)+x is added to the same class as r+x. Thus,
PV becomes

{albla+b,rlxlr+x, (a+b)+x}.
This process is called "structuring" an optimizing

pool . Structuring consists of adding any expressions to the partition which have operands equivalent to the one which occurs at the node being
considered. The entire expression (a+b)+x at node
V is then found to be redundant since the structured pool Pv contains a class with (a+b)+x.

An optimizing function fl(N,P) for common subexpression elimination can now be informally stated.

1. Consider each partial computation e in the expression at node N < ~.

2. If the computation e is in a class of P then e

is redundant; otherwise

3. create a new class in P containing e and add

all (partial) computations which occur in the
program graph and which have operands equivalent to e (i.e., structure the pool P).

4. If N contains an assignment d:=e, remove from

P all expressions containing d as a subexpression. For each expression e' in P containing
e as a subexpression, create e" with d substituted for e, and place e" in the class of e'.

The meet operation A of the algorithm A must
be defined for common subexpression elimination.

Since the optimizing pools in ~' are partitions of

expressions, the natural interpretation is as
intersection by classes, denoted by ?!. That is,
given Pl,P2 c ~', P = PI h P2 is defined as follows.

Let

and P(c) = PI(c) n P2(c) Vc E C.
C is the set of expressions common to both PI and
P2, while PI(c) and P2(c) are the classes of c in

PI and P2, respectively. Thus , the class of each

c c C in the new partition P is derived from PI
and P2 by intersecting the classes P (c) and P2(c).
For example, if P

{a,cld,f,g} then ~ ~ {a, bld,e,f} and1p2 =- {a,d,f} andP1 fip2 = {a\d,f}.

It is easily shown that H has the properties
required of the meet operation; hence, a "refinement" relation is defined:

That is, PI ~ p2 if and only if PI is a refinement

of P2. The refinement relation provides the ordering required on the set ~' for the algorithm A.

The function fl can be stated formally, and
shown to have the homomorphism property requiredby the global analysis algorithm [33]:

fl(N,pl * P2) = fl(N,P1) K f1(N,P2).
Before considering an example of the use of
fl with the algorithm A, the function fl is extended to combine constant propagation with common subexpression elimination,

5. CONSTANT PROPAGATION AND COMMON SUBEXPRRSSION
ELIMINATION

198

The common subexpression elimination optimizing
function fl of Section 4 can easily be extended to

include constant propagation. Consider, for
example, the following segment of an ALGOL 60 program:

. . . U:=20; .,. V:=30' , . . . U+v . . . X:=lo;
. . . y:=40; . . . X+y . .. y-x . . .

Figure 3 shows a program graph representing this

segment. Assume the entry pool is empty; i.e.,
PB = 0. The analysis proceeds up to node E as

before, resulting in

PE = {u,201v,30}.

Note that u and v are both propagated constants inPE since they are both in classes containing constants. If the expression u-i-v at node E is pro-cessed

as in fl, the output pool is

{u,201v,301u+v}.
Noting that u and v are in classes with constants,

then U+V must be the propagated constant 20+30 = 50.
Hence, the constant 50 is placed into the class of

U+V in the resulting partition. Thus ,

`F = {u,201v,301u+v,50}.
The analysis continues as before up to node H,

resulting in

PH= {u,2OIV,3OIU+V,5OIX,1OIY,4O} .
In the case of the f optimizing function, the

iexpression x+y at no e H is placed into a distinct
class. The operands x and y, however, are propagated constants since they are equivalent to 10
and 40, respectively. The expression x+y is

equivalent to 50 which is already in the partition. Thus, x+y is added to the class of 50,
resulting in

PI = {u,2OIV,3OIU+V,5O,X+YIX,1OIY,4O} .
Similarly, the output pool from node I is

{u,2OIV,3O,Y-XIU+V,5O,X+YIX,1O ly,40}.
The analysis above depends upon the ability
to recognize certain expressions as constants and
the ability to compute the constant value of an
expression when the operands are all propagated
constants. It is also implicit that no two
differing constants are in the same class.

An optimizing function f2 which combines
constant propagation with common subexpression
elimination can be constructed from fl by altering

step (3) as follows:

3a. create a new class in P containing e and add

all (partial) computations which occur in the
program graph and which have operands equivalent to those of e (structure the pool as
before).

3b . If e does not evaluate to a constant value--.

based upon propagated constant operands, then

no further processing is required (same asstep (3) of fl); otherwise let z be the

constant value of e. If z is already in the paKtition P then combine the class of z with the class
of e in the resulting partition. If z is not in

the partition P, then add z to the class of e. The
expression e becomes a propagated constant in either

case.

The function f2 is stated formally and its
properties are investigated elsewhere [33].

~= {u,201v,3~lu+v,50)a

F X=lo

~= {u,201v~l~+v,501x,10}

0'yz4
E= {u,201v,301u+~,501x,101y,40}O

H X+y

~ = {u,201v,301u+vj50,x+ylx,101y,40}

6
I y-x

Figure 3. A program graph demonstrating the

effects of constant propagation.

6. EXPRESSION OPTIMIZATION

Expression optimization, as defined earlier,
includes common subexpression elimination, constant
propagation, and register optimization. The first

two forms of optimization are covered by the f2
optimizing function; only register optimization

needs to be considered. It will be shown below

that f2 also provides a simple form of register
optimization.

In general, global register optimization involves the assignment of high speed registers

(accumulators and index registers) throughout a
program in such a manner that the number of storefetch sequences between the high-speed registers
and central memory is minimized. Ths store-fetch

sequences arise in two ways. The first form involves redundant fetches from memory. Consider

the sequence of expressions

a:=b+c; d:=a+e. .
for example. A straight-forward translation of
these statements for a machine with multiple
general-purpose registers might be

r :=b; r :=c; r :=r +r ; a:=r .1 2 1 12 1'

r :=a; r :=e; r :=r +r . d:=r1 2 1 1 2' 1"

199

Note, however, that the operation r :=a is not1
necessary since r 1 contains the value of the variable a hefore the operation. McKeeman [38] discusses a technique called "peephole optimization"
which eliminates these redundant fetches within a

basic block.

Figure 4 shows a program corresponding to the
register operations above. The f2 optimizing function is applied to each successive node in the
graph, resulting in the optimizing pools shown in

the Figure. In particular, note that

PE - {a,r11blr2,c}.
The operation at node E assigns the variable a to

the register rl. Since a is already in the class
of r

it

however, the operation is redundant and can
be e lminated. Hence, the f2 optimizing function

can be used to generalize peephole optimization.
Further, the algorithm A extends f to allow global

elimination of redundant register load operations.

Figure 4. Elimination of redundant
register load operations.

The second source of store-fetch sequences
arises when registers are in use and must be
released temporarily for another purpose. The
contents of the busy register is stored into a

central memory location and restored again at a
later point in the program. An optimal register
allocation scheme would minimize the number of

temporary stores. This form of register optimization has been treated on a local basis, including
algorithms which arrange arithmetic computations

in order to reduce the total number of registers
required in the evaluation [5,27,36,39,43,45,46].
Global register allocation has also been formulated
as an integer programming problem by Day [14],

given that register interference and cost of data
displacement from registers is known. No complete

solution to the global register allocation problem

is known by the author at this time.

A solution to the global register allocation
problem will be aided by the analysis of "live"

and "dead" variables at each node in the program
graph. A variable v is live at a node N if v could
possibly be referenced in an expression subsequent

to node N. The variable v is dead otherwise. Recent work has been done by Kennedy [32] using interval analysis techniques to detect live and dead
variables on a global basis.

An optimizing function f3 can be constructed
which produces a set of live expressions at each

node in the graph. The detection of live expressions requires the analyais to proceed from the
end of the program toward the beginning. Figure
5 shows the graph of Figure 4 with the direction
of the edges reversed. The live expressions at

the beginning of the graph correspond to the live
expressions at the end of program execution; hence,
PH = @ (there are no live expressions at the end of

execution) , The expression d:=rl at node H refers

to the expression r . Thus, rl is live ahead of
node H. This fact is recorded by including rl

in PG,

PG = {rl}.
Since rl is assigned a new value at node G, it becomes a dead expression, but, since r 1 is also
involved in the expression r1+r2, it mediately
becomes a live expression again. Thus,

PF = {r1,r2,r1+r2}.
The analysis continues , producing the optimizing
pools associated with each node in Figure 5. The

expressions which are live at node C, for example,
are pB = {e,r~!r~,r~+r21.

~ Lg

o
H ~.r(

~= {r,}

6G :=f+~

Figure 5. Detection of live expressions in a

reversed program graph.

200

The optimizing function f3(N,P) which provides
li,ve expression analysis can be informally stated
as follows:

1. If the expression at node N involves an assign-

ment to a variable, let d be the destination

of the assignment; setP+P - {eld is ~ subexpression in e}(d and all expressions containing d become dead expressions).

2. Consider each partial computation e at node N.

Set P + P u {e} (e becomes a live expression).
The value of f3(N,P) is the altered value of P.

The algorithm A can then be applied to the
reversed program graph using the optimizing func-

tion f .

;

The exit nodes of the original graph become t e entry nodes of the reversed graph. In
addition, the meet operation of the algorithm A is

the set union operation u. The union operation
induces the partial ordering given by

PI < P2 <=> p 1 u P2 = `1 `=> `1 ~ `2' `1' `2 `~"

where P is the set of (partial) computations which

occur ~n the program graph. Note that ~ = P' and
~ = @ in this case. Thus, all initial appr~ximate

pools in the algorithm A are set to 0.

There is a simple generalization of detection
of live expressions to "minimum distance analysis"
where each live expression is accompanied by the

minimum distance to an occurrence of the expression. The optimizing pools in this case are sets
of ordered pairs (e,d), where e is a live expression and d is the minimum distance (in program
steps) to an occurrence of e, The optimizing
function extends live expression analysis by

tabulating a distance measure as the live expres-
sion analysis proceeds. In addition, the meet
operation consists of both set union and a comparison of the distances corresponding to each

live expression. This minimum distance infor-
mation can then be used in the register replacement decision: whenever all registers are busy

and contain live expressions, the register containing the live expression with the largest
distance to its occurrence is displaced.

Examples are given in the section which
follows demonstrating the fz and f3 optimizing

functions when used in conjunction with the
algorithm A.

7. A TABULAR FORM FOR THE ALGORITHM A

The processing of the algorithm A can be
expressed in a tabular form. The tabular form
allows presentation of a number of examples, andprovides an intuitive basis for implementing the

optimizing techniques. In particular, this form
allows representation of the approximate optimizing
pools at each node, the elements of L, and the node

traversing decision. As shown in Table I, the
column labeled "N" contains the current node beingprocessed (i. e., the N in L' = (N, P.) in step A5).

The column labeled "PN + PN A P." S*OWS the change

in the approximate pool at node \ when the node istraversed in step A5. The column marked "f(N, PN)"

contains the output optimizing pool produced by

traversing the node N (the set bracee are omitted
for convenience of notation). The last column,

marked "L," represents the set of nodes remaining

to be prncessed (the set L of the algorithm A).

is
1.

2.

3.
4.

is

Paraphrasing the algorithm A, the tabular form
processed as follows.

List all entry nodes and entry pools vertically

in the right-hand columns, with entry node ei
in column L, and associated entry pool xi in
column f(N,PN). Normally, there is only one
entry node, with the null set as an entry pool,

Select an L' from L as follbws, Choose any
node from column L, say node N. If there are

no elements remaining in L then the algorithm
halts. The line where N was added to L contains the associated output pool Pi in the
column f(N,PN). Eliminate L' from L by crossing out N from column L.

Using L' = (N, Pi) from step 2, scan the table

from the bottom upward to the first occurrence
of node N in column N. The current approximate
pOOl PN is adjacent in the column PN + P A Pi.

If node N has not appeared in column N, ~hen
aesume the first approximation to PN = ~ (andhence, pN i- 1 A Pi = pi).

If PN S Pi then go to step 2. Otherwise, write

the node name N in column N and the value of
the new approximate pool determined by PN A Pi
in the column marked PN + PN A Pi. Compute

the output pool based upon the new approximate
pOOl PN in the column f(N,PN), and write the

names of the immediate successors of N in

column L. Go back to step 2.

Upon termination of this algorithm, the table
scanned from bottom to top; the first occurrence
each node N 6 N is circled. The pool associatedof
with each circled--node in column PN ~ PN A Pi is

the final DOO1 for that node. Anv nodes of N which
do not app~ar in column N cannot be reached ~rom
an entry node, and can be eliminated from the program graph,

Table I shows the analysis of the program
graph given in Figure 1, using the f
function. 2 `ptimizin~The entry node set for th~s analysis is
L= {(A,@)}, as before. L is treated as a stack;

elements are removed from the lower right position
of column L in step 2. After processing the graph,

the final pools at each node are listed in the
table opposite the circled nodes. The final pool
at node E, for example, is

PE = {a,llb,21d,a+b,3}.
The final pools determined by the algorithm corre-..

spend to those determined previously in Section 2,

TABLE I
step N `N- PNAPI f[N, PN)

i 0
2 @ ~

3 @ a,l

4 c a,l[c, O

5 D a,llc,01b,2
6 E a,l]c,01b,2 [d, a+b,3
7 F a.11c,01b,2, e, b+cld. a+b, 3

8 @ a,l

9 0 a,l\b,210 @ a,llb,21d, a+b,3

u @ a,l]b,2

a,i
a,llc, O
a,llc,01b,2
a,llc,01b,21d, a+b,3
a,llc,0[b,2, e, btcld, a+b, 3

.a,llb,2, eld, Mb,31c,4
a,llb,2
a,l/b,2
a,llb,21d, a+b,31b+c, e

a,llb,21c,4

201

Figure 6 shows a program graph with two parallel feedback loops. The analysis of this program

graph is given in Table II, using the f2 optimizing
function. Note that in step (8),

PF = {lo]ylx,5,u}.
Applying f2@,PF), the resulting output pool is

{1OIY]X,5,U]U"Y,X"Y}.
The expression X*Y is placed into the class of U*Y
when the partition is structured. That is, x-y ia

an expression which occurs in the program, and x-y

is operand equivalent to u"y. Thus, x,y must be
added to the class of U*Y in the output pool. The
redundant expression x"y is detected at node G

since the final pool PG contains X*Y.

A

&

entry' x:=KI

B
F X"YU"Y

Cx
E U=x D G

X:=5 x-y

Figure 6. A program graph with two parallel

feedback loops.

TABLE 11
tep N `N- `NAPI

1

2 @ @3 @ X,lo

4 c X,lolylx.y

5 G X,lolylx. y
6 D X,lolylx. y
7 @ lo/ylx,5

8 @ lolylx,,,u

9 @ Xllolylx"y10 @ XI1OIYIX.Y

11 @ XI1OIYIX.Y

f(N, PN)
@X,lo

X,lo[ylx. y
X,lolylx. y

X,lolylx. y

10jylx. ylx,5

lolylx,5, u

101YIX,5, UIU-Y, X.Y
Xllolylx.y
X[lolylx. y
x,5110/y

Global live expression analysis can be per-
formed on the program graph of Figure 6 by reversing the gr~ph~ as ~ho& in Figure 7. Given

that node C is the exit node of the original graph,
node C becomes the entry node of the reversed

graph. Thus, ~ = {(C,@)} in the analysis shOwn in
Table 111, using the f optimizing function.

3

For
example, the final poo

`A = {X,y,x"y}
indicates that the expressions x, y, and x.y are
live immediately following node A in the original

graph. G D

w

X"Y X:.5 Ec U.x
entry: x F

B U"YX.y

AbX:=lo
Fieure 7. The reversed graph corresponding to

J1-----

3 Gx
4 @ X,y, x.y

5 G X, Y, X-Y
6 @ X, Y,X. Y

7 @ X, Y, X. Y, U,U. Y
8 @ X, Y, X-Y
9 @ X. Y.X, Y
10 x8Y8~.Y

X,Y!X.Y
X,y, x.y
x,Y.~,Y
X,Y,X. Y,U,U. Y

X, YIX.Y
YX.y,

x.y

This tabular form can be used for processing
any program graph using an optimizing function
which satisfies the conditions of the algorithm A.

8. IMPLEMENTATION NOTES

Implementation of the above optimizing techniques in a practical compiler is considered below.

In particular, the optimizer operates upon an intermediate form of the program, such as tree structures or Polish [24], augmented by branching information. The control flow analyzer accepts the intermediate form and calls the various optimizing
functions to process each basic block, roughly
paralleling the tabular form given previously. A

single stack can be used to list uninvestigated
basic blocks, corresponding to "L" of the tabular

form. Pool information must be maintained for
each basic block corresponding to the "P + P A Pi"
column, but may be discarded and replace ! if ~he
node is encountered again in the analysis (i.e. ,

the node reappears in column "N"). The output
optimizing pools found in column "f(N,PN), " however,
can be intersected with all immediate successors

as they are produced, and thus need not be maintained during analysis. The final optimizing pools

(determined by "scanning" the tabular form) are

simply the current pools attached to each basic
block.

The optimizing functions and corresponding
meet operations are generally simple to implement

using bit strings for sets, and lists for ordered
pairs. Common subexpression elimination, however,
requires further consideration since direct representation and manipulation of structured partitions
is particularly unwieldy.

One approach to handling structured partitions
allows direct representation of the classes, but
limits the number of expressions which appear. A
list of all (sub)expressions is constructed by
prescanning the program (an optimizing function
which always returns @ ia useful for this scan).
When a partition is structured, only those expressions which occur in the expression list are in-
eluded. The set of eligible expressions can be

further reduced by first performing live expression

analysis. The expressions which appear in a partition are limited to the live expressions at the
point the partition is generated. The use of live

expression analysis before common subexpression
elimination will generally reduce partition size
and improve the convergence rate of the analysis
algorithm.

A second approach to
tured partitions involves

representation of structhe assignment of "value

the program graph of Fig~re 6.

202

numbers" to the expressions in the optimizing pools

[13,24,33,34]. A value number is a unique integer
assigned to all elements of the same class. The

sequence of statements

a:=b+c; d:=b; s:=a.
results in the structured partiti~n

PI ={b,d I c I b+c,d-tc,a,e}.
Next, assign the value numbers 1, 2, and 3 to the

three classes, and replace the expressions b+c and
d+c by (l)+(2), representing the addition of elements
of class (1) and class (2). PI can now be written

Similarly, the sequence of assignments

a:=d; b:=c; e:=b+c;
produces the structured partition represented by

P2 = {a,dlb,c\(5)+(5),e }.

(4) (5) (6)
which expands to

P2 = {a,dlb,clb+c,b+b,c+b,c+c,e}

Thus , the assignment of value numbers provides a

data structure whose size is linear in the number
of expressions in the basic block. In addition,

the value number representation is particularly
easy to construct and use in the detection of

common subexpresaions.

Given two partitions P1 and P2 in value number
form, the meet operation P = P

i

H P2 can be iteratively computed. The computat on proceeds as
follows . Construct a list C consisting of the
number of occurrences of each value number in P
The elements of C thus provide a count of the 1"
number of elements in each class of P . This

count is decremented whenever an elem~nt of the
class is processed, until the count goes to zero
indicating the entire class is exhausted,

A list R is also maintained which gives a
mapping of the class numbers in P and P2 to the

resulting class numbers in P. Th~ elements of R

are of the form r(rl,r2), indicating that value
number rl from Pl and value number r from P2 map

to value number r in the resulting p?irtition P.
R is built during the construction of P.

The elements of PI are scanned and processed
until the classes of P are exhausted.1 Suppose q

is an identifier in PI with value number v .1 The
count corresponding to VI in the list C is first

decremented. If q does not occur in P2 then the
next element of PI is selected. Otherwise, let V2

be the value number corresponding to q in P . Ris scanned for an element V(V1, V2) ; if not

$ound,a new value number v is assigned, and V(V1,V2) is

added to R. The identifier q is placed into Pwith value number v.

If the element selected from PI is not an
identifier, then it ia an expression of the form

(nl) 9 (ml) with value number VI, where nl and mlare value numbers in PI (assuming all operations

+3 are binary). If the count of either class (nl)

or (ml) is non-zero in C, defray the processing of

this expression; otherwise, decrement the count for
class (v ) in C, as abOve.1 Examine R for pairs of

elements n(nl,n2) and m(m
value numbers in P2, ISm2) where n and m are$For each such pa r, sea~ch
p2 for an entry (n ) 9 (m2). If found, let V2 be

thevalue number o!this matched expression. Scan
R for an element of the form V(vl,v ), and make a

new entry if not found, as above. ?he expression

(n) Q (m) with value number v is then placed into
the intersection P.

As an example, consider the class intersection
of the partitions PI and P2 given previously. These
partitions are represented by the value number tables

d (1) d (4)

b (5)
(1):(2) ::1

a (3) (5):(5) {21
e (3) e (6)

The class count list C for the partition PI is

initially

val# _count
m 2

(2) 1

(3) 3

The identifiers b, d, and c are processed first,

reducing the class counts for (1) and (2) to zero

in C, The class mapping list at this point is

R= {7(1,5), 8(1,4), 9(2,5)}
The identifiers b, d, and c are placed into P with
value numbers 7, 8, and 9, respectively. The expression (l)+(2) with value number (3) is then
processed from PI) since the class counts for both

(1) and (2) are zero. Based upon the mappings inR, P is searched for an occurrence of (5)+(5) or

(4)+?5) . Since (5)+(5) occurs in P2 with value
number (6) , R is scanned for an element of the form
v(3,6), and, since no such element is found, 10(3,6)

is added to R. The expression (7)+(9) with value
number (10) is included in P. The identifier a is

then processed, resulting in another mapping 11(3,4)
in R; a is added to P with value number (11). Finally, the identifier e from PI with value number

(3) is processed. A match is found in P2 with
value number (6). Since the element 10(3,6) is

already in R, e is added to P with value number

(lo) . The final value of the class list is

R= {7(1,5), 8(1,4), 9(2,5), 10(3,6), 11(3,4)}
which can now be discarded. The value of the re-

suiting partition P is

d (8)

(9)
(7):(9) (lo)

a (11)e (lo)

which represents the structured partition

{bld\clb+c,ela}
Note that the predicate P2 2 PI is easily computed

203

during this process.

The control flow analysis algorithm has been
implemented as a general-pugpose optimizing module,
including several optimizing functions. The implementation is described in some detail elsewhere

[33].

9. CONCLUSIONS

An algorithm haa been presented which, in
conjunction with various optimizing functions,
provides global program optimization, Optimizing

functions have been described which provide constant propagation, common subexpression elimination, and a degree of register optimization.

The functions which have been given by no
means exhaust those which are useful for optimization. Simplifying formal identities such as O+x =
O-I-X = x can be incorporated to further coalesce
equivalence classes at each application of the f2
optimizing function. In addition, it may be possible to develop functions which extend live ex-
pression analysis to completely solve the global

register allocation problem.

REFERENCES
1. Aho, A., Sethi, R., and Unman, J. A formal

approach to code optimization. Proceedingsof a Symposium on Compiler Optimization.

University of Illinois at Urbana-Champaign,
July, 1970.

2. Allen, F. Program optimization. In Annual

Review in Automatic Programming, Pergamon
Press, 5(1969), 239-307.

3. --- A basis for program optimization. IFIP

Congress 71, Ljubljana, August, 1971, 64-68.

4. --- Control flow analysis. Proceedings of a

Symposium on Compiler Optimization, University of Illinois at Urbana-Champaign, July,

1970.

5. Aoderson, J. A note on some compiling algori thins. Comm. ACM 7, 3 (March 1964),

149-150.

6. Arden, B. Galler, B., and Graham, R. An

algorithm for translating boolean expres-sions. Jour. ACM 9, 2(April 1962)$ 222-239.

7. Bachmann, P. A contribution to the problem ofthe optimization of programs. IFIP Congress

71, Ljubljana, August, 1971, 74-78.
8. Ballard, A., and Tsichritzis, D. Transforma-

tions of programs. IFIP Congress 71,
Ljubljana, August, 1971, 89-93.

9. Breuer, M. Generation of optimal code for expressions via factorization. Comm. ACM 12,

6(June 1970), 333-340.

10. Busam, V., and Englund, D. Optimization ofexpressions in FORTMN. Comm. ACM 12,

~2(Dec. 1969), 666-674.
11. Cocke, J. Global common aubexpre.ssion elimination. Proceedings of a Sybposium on Compiler Optimization. University of Illinois

at Urbana-Champaign, July, 1970.

12. ---, and Miller, R. Some analysis techniques
13.

14,

15.

16,

17.

18.
19.

20.

21.
22.

23.

24.

25.

26.
27.

28.

29.

for optimizin~ computer programs. Proc~

Second International Conference of System
Sciences, Hawaii, January, 1969, 143-146.

---, and Schwartz, J. Programming Languages

and Their Compilers: Preliminary Notes.----
Courant Institute of Mathematical Sciences,
New York University, 1970.

Day, W. Compiler assignment of data items to

registers. IBM Systems Journal, 8, 4(1970),

281-317.

Earnest, C., Balke, K,, and Anderson, J.

Analysis of graphs by ordering nodes. Jour.

ACM 19, l(Jan.' 1972), 23-42.

Elaon, M., and Rake, S. Code generation

technique for large language compilers.
IBM Systems Journal 3(1970), 166-188.

Fateman, R. Optimal code for serial and

parallel computation. Comm. ACM 12, 12(Dec.

1969), 694-695.

Finkelstein, M. A compiler optimization

technique. The Computer Review (Feb. 1968),
22-25.

Floyd, R. An algorithm for coding efficient

arithmetic operations. Comm. ACM 4, l(Jan.
1961), 42-51.

Frailey, D. Expression Optimization using

unary complement operators. Proceedings of
a Symposium on Compiler Optimization, University of Illinois at Urbana-Champaign,
July, 1970.

---, A study of optimization using a general

purpose optimizer. (PhD Thesis) Purdue
University, Lafayette, Ind., January 1971.

Freiburghouse, R. The MULTICS PL/I compiler.

AFIPS Conf. Proc. FJCC (1969), 187-199.

Gear, C. High speed compilation of efficient

object code. Comm. ACM 8, 8(Aug. 1965),

483-488.

Gries, D. Compiler Construction for Dipital

Computers. John Wiley and Sons ~nc., New
York, 1971.

Hill, V., Langmaack, H., Schwartz, H., and

Seegumuller, G. Efficient handling of subscripted variables in ALGOL-60 compilers.
Proc. Symbolic Languages in Data Processing,
Gordon and Breach, New York, 1962, 331-340.

Hopkins, M. An optimizing compiler deaign.

IFIP Congress 71, Ljubljana, August, 1971,

69-73.

Horowitz, L., Karp, R., Miller, R., and

Winograd, S. Index register allocation.
Jour. ACM 13, l(Jan. 1966), 43-61.

Huskey, H., and Wattenberg, W. Compiling

techniques for boolean expressions and
conditional statements in ALGOL-60. Comm.
ACM 4, l(Jan. 1961), 70-75.

Huskey, H. Compiling techniques for algebraic

expressions. Computer Journal 4, 4(April

1961), 10-19.

204

30.

31.

32.
33.

34.

35.
36.

37.

38.

39,

40.

41.

42.

43.

44.

45.

46.

47.

Huxtable, D. On writing an optimizing translator for ALGOL-60. In Introduction~
System Programming, Academic Press, Inc.,
New York, 1964.

IBM System/360 Operating System, FORTRAN IV

(G and H) Programmer's Guide. c28-6817-1,
International Business Machines, 1967,

174-179.

Kennedy, K. A global flow analysis algorithm.

Intern. J. of Computer Mathematics, Section
A, Vol. 3, 1971, 5-15.

Kildall, G. Global expression optimization

during compilation. Technical Report No.
TR# 72-06-02, University of Washington Computer Science Group, University of Washington,

Seattle, Washington, June, 1972.

--- A code synthesis filter for basic block

optimization. Technical Report No. TR# 72-

01-01, University of Washington Computer
Science Group, University of Washington,

Seattle, Washington, January, 1972.

Lowry, E., and Medlock, C. Object code optimization. Comm. ACM 12, l(Jan. 1969), 13-22.

Luccio, F. A comment on index register allocation. Comm. ACM 10,9 (Sept. 1967), 572-

572-574.

Maurer, W. Programming-An Introduction to

Computer Language Techn~ue. Holden-DaZ

San Francisco, 1968, 202-203.

McKeeman, W. Peephole optimization. Comm.

ACM 8, 7(July 1965), 443-444.

Nakata, 1, On compiling algorithms for

arithmetic expressions. COmm. ACM19,

8(Aug, 1967), 492-494.

Nievergelt, J. On the automatic simplification of computer programs. Comm.
ACM 8, 6(June 1965), 366-370.

Painter, J. Compiler effectiveness. Proceedings of a Symposium on Compiler
Optimization, University of Illinois at

Urbana-Champaign, July, 1970.

Randell, B., and Russell, L. ALGOL 60--.

Implementation. Academic Press, Inc.,

New York, 1964.

Redziejowski, R. On arithmetic expressions

and trees. COmm. ACM 12, 2(Feb. 1969),

81-84.

Ryan, J. A direction-independent algorithm for

determining the forward and backward compute
points for a term or subscript during compilation. Computer Journal 9, 2(Aug. 1966),

157-160.

Schnieder, V. On the number of registers

needed to evaluate arithmetic expressions.

BIT 11(1971), 84-93.

Sethi, R., and Unman, J. The generation of

optimal code for arithmetic expressions.

Jour. ACM17, 4(Oct. 1970), 715-728.

Wagner, R. Some techniques for algebraic

optimization with application to matrix
arithmetic expressions. Thesis, CarnegieMellon University, June, 1968.

48. Yershov, A. On programming of arithmetic

operationa. Comm. ACM 1, 8(Aug. 1958), 3-6.

49. --- ALPHA-an automatic programming system of

high efficiency. Jour. ACM 13, l(Jan. 1966),

L7-24 .

1

2

3

4

5

6

7
8

9
10

11

12

13

14

15

APPENDIX A
Al: L = {(A,PI)}

A3 : ~1 =
A4 : `N =

pA +

A5: PA =

A3: L' =

A5: PB =
A3 : L! =

A5 : `c =

PA=~,Pi=O, PAi4Pi,
pAApi=pi=@

0, L= {(B,{(a,l) })}

(B, {(ail)}), L = ~
{(a,l)}, L= {( C,{(a,l), (c, O)})}

(C, {(a,l), (c, O)}), L = @

[(a,l),(c,O)},
L = {(D,{(a,l),(c,0),(b,2)})}
A3 : L' = (D,{(a,l),(c,0),(b,2)}), L = @
A5 : `D = {(a,l),(c,0),(b,2)},

L = {(E,{(a,l),(c,0),(b,2),(d,3) })}
A3 : L' = (E,{(a,l),(c,0),(b,2),(d,3) }), L = @
A5 : `E = {(a,l),(c,0),(b,2),(d,3)},

L = {(F,{(a,l),(c,O), (b,2),(d,3) ,(e,2)})}
A3 : L' = (F,{(a,l),(c,O), (b,2),(d,3) ,(e,2) }),

L=@

A5 : PF = {(a,l),(c,0),(b,2),(d,3), (e,2)},

L = {(C,{(a,l),(c,4),(b,2), (d,3) ,(e,2)})}
A3 : L' = (C, {(a,l), (c,4), (b,2), (d,3), (e,2) }),

L=fl

A5 : `c = {(a,l)}, L= {(D,{(a,l), (b,2)})}

A3 : L' = (D,{(a,l),(b,2)}), L = @
16
17

18 A5:

19 A3:

20 A5:

21 A3:

22 A5:

23 A3:

`D = {(a,l),(b,2)},

L = {(E,{(a,l),(b,2),(d,3)})}

L' = (E,{(a,l),(b,2),(d,3)}), L = @

PE = {(a,l), (b,2), (d,3)},
L = {(F,{(a,l),(b,2),(d,3)})}

L' = (F,{ (a,l),(b,2),(d,3)}), L = @
PF = {(a,l),(b,2), (d,3)},

L = {(C,{ (a,l),(b,2),(d,3),(c,4) })}

L' = (C,{(a,l),(b,2), (d,3),(c,4) }), halt.

APPENDIX B
The proof of Theorem 2 is given below. First
note that given a program graph G with multiple

entry nodes, an augmented graph G' can be constructed with only one entry node with entry pool

The construction is as follows. Let ~=
f~1,e2, . . . ek} be the entry node set and ~ =

`(el$x~), (e2,x~), . ..$ (ek,x~) } be the en~ry poolset corresponding to a particular analysis. COnsider the augmented graph G' = <~',~'> where

205

E1=E"----
{(v, vi), (v, v2),. ... (v,vk), (vi, el), (vk,ek)}k)}.
The augmented graph G' has a single entry mode v

and entry node set~' = {v}. The functional value
of f is defined for these nodes as

f(v,P) = ~ VP c ~, and
f(vi,P) = xiVP c ~, l<i<k.
Hence, the analysis proceeds as if there is only a
Fingle entry node with entry pool O_; i.e.,
~' = {(v,g)}.--

Lemma 1. If f(N,P AP ) = f(N,P ) A f(N,P ) then
`1SP2 => f@, P1)sf?N,l,), VN c i, P~,P, 2P--.

Proof. The proof is immediate since P SP2 =>

f(N, P1AP2) = f(N, P1) =( f(N, P1) Af(N, P2j)=>
f(N,pl) s f(N,P2)o

Lemma 2. Let X ~ ~, if f(N,P1AP2) =

f(N,P1)Af(N,P2) VN < ~, P1,P2 e ~ then

f(N, ~xx) =x~Xf(N,x).
Proof. The proof proceeds by induction on the

cardinalit of X, denoted by C(X). If C(X) = 1

&then f (N, xcx ) = f(N, x) and the lemma is trivially
true. If G(X) = k, k>l, assume lema is true
for all X'~(X') i k. Let y~x and X! = x -{y}.

f(N,&x) = f(N,yA(~x,)) = f(N,y)Af(N,@x,) .
f(N, y) A(x~x, f(N, x)) = ~~xf(N, x)O
Proof of Theorem 2. It will first be shown by

induction on the path length that

PN5~vNe~.
Consider the following proposition on n:

pN s f(pn,f(pn_l,. ..,f(p1,9) )...) fOr all
final POOIS PN and paths of length n from the entry
node pl with entry pool ~ to node N, QN E N.--

The trivial case is easily proved. The only
node which can be reached by a path of length O

from the entry node PI is PI itself. Hence, it is
only necessary to show that P s o. This isP1 -

immediate, however, since (p ,0) is initially
placed into L in step Al, an~ ~xtracted in step A3

as L' = (P~,~). But, Ppl is initially~, and hence
P~1 * Pi =~in step A4. Thus, Ppl +P AO=OPl--

in step AS. Thus , it follows that P =os0P1 - -"

Suppose the proposition is true for all n<k,
for k>O. That is, PN s f(p ,,. .,f(p ,0)) ..,)~-
for all paths of length less than k rom PI

to node N, for each node N c N.--

p denote one such predecessor, and let T =

?f pk_~,. ... f(pl,g)) ...). By inductive hypothesis,
PPk s T. It will be shown that PK s f(pk,T).

Since P is the final approximation to the
Pool at pk, [~,f(pk,pp )) must have been added to L

in step A5. But, Ppk~T=> f(pk,pp ) ~ f(Pk,T) by

Lemma 1. The pair ( ,f(pk, PPk)) mu!$t be processed

in step A3 before the algorithm halts, Thus, either
pK<f (pk, ppk) in Step A4, or PKi-PK A f(pk,ppk).

In either case, p~ ~ f(pk,ppk). But ,

pK s `(pk,ppk) S f(pk,T) `> PK S f(pk,T)

=> P < `(pk,f(pk_l, ...,K- f(P1,Q)) ...).
Thus , since the proposition holds for paths of

length k, it follows by induction that the proposition is true for all paths from the entry node to
node N, for all N c N,--

The following claim will be proved in Order to
show that XN < P for all N < N: ,.
the processing .! G by the alg~ri;~ ~g~~ ;
has not been encountered in step A5, or

where P

!

% < Pljais the current approximate pool associated
with no e N, for all N l N. The proof proceeds by--

induction on the number of times step A5 has been
executed. Suppose step A5 has been executed only
once. Then L' = (P1,Q) and the only n~de encountered

in step A5 is the entry node PI. The entry pool ~

corresponds to a path of length zero from PI tO PI.

Thus, ~ < FPI " `P1 = ~ and the proposition is

trivially true since XPI -`o~pPl=Q"

Suppose that either N has not been encountered
in step A5, or ~spN VN6Nwhen step A5 has been

executed n<k times, k>l. Con=ider the kth execution
of step A5. Let L' = (N,T) where T = f(N',PN,) for

some N' s I-l(N). The pair (N,T) was added to L
when the node N' was processed in the nth execution

of step A5, for n<k. Hence,
hypothesis. ? t ~ pNt by inductiveBut, using Lemma ,A

%s v paths f(N'>f(pts """, f(P1,~)) ...) =

(Pi>...,pt,,N)N)

f(N', A f(Ptyf(Pt_l, . . ..f(P1.Q)) . ..)V Paths

(P; , . . ..pt.N')

= f(N',XNT).

~, ~pN, andthus~~f(N',~, ) =>
~ s f(N',P N,) = T, using Lemma 1.

If this step is the first occurrence of node
N inA5, then PN+~AT = T since f(Nf,p) + I

for any N' c ~, P E p. In this case, ~ <PN-=T
after step A5. Othe--%ise, suppose this is not the
first occurrence of node N in step A5. ~<PN
and~<T=> ~sPNAT=> XN<PN+PNATafter

step A5 is executed. Hence, the proposition holds
for each execution of step A5. In particular,

Let K c ~~3a path (Pi,...,pk,K) of length k. It N c N upon termination of the algorithmXN ~ PN Q _
will be shown that P~ s f(pk,f(pk_l,. ..,f(pl ,g)). ..). A. Hence, the theorem is proved since

Consider each immediate predecessor in I-l(K). Let PN<~and~SPN=~XN=PNVN<~e

206