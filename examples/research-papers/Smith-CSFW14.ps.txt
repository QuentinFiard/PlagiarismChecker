

A New Type System for Secure Information Flow

Geoffrey SmithSchool of Computer Science
Florida International UniversityMiami, Florida 33199, USA

smithg@cs.fiu.edu

Abstract
With the variables of a program classified as L (low,
public) or H (high, private), we wish to prevent the program from leaking information about H variables into Lvariables. Given a multi-threaded imperative language

with probabilistic scheduling, the goal can be formalized
as a property called probabilistic noninterference. Previ-ous work identified a type system sufficient to guarantee

probabilistic noninterference, but at the cost of severe re-strictions: to prevent timing leaks,

H variables were disallowed from the guards of while loops. Here we present a
new type system that gives each command a type of the form
o/1 cmd o/2; this type says that the command assigns only to
variables of level o/1 (or higher) and has running time that
depends only on variables of level o/2 (or lower). Also weuse types of the form

o/ cmd n for commands that terminate
in exactly n steps. With these typings, we can prevent tim-ing leaks by demanding that no assignment to an

L variable
may sequentially follow a command whose running time depends on H variables. As a result, we can use H variablesmore flexibly; for example, under the new system a thread

that involves only H variables is always well typed. The
soundness of the type system is proved using the notion ofprobabilistic bisimulation.

1 Introduction

In this paper, as in [10] and [13], we consider a simplemulti-threaded imperative programming language in which
each variable is classified either as L (low, public) or H(high, private). Our goal is to develop a static analysis that
ensures that a program cannot "leak" the values of H vari-ables. Of course, the possible ways of leaking information
depend on what is observable. If we can observe the runningprogram from the

outside, seeing running time or the usageof various system resources, then controlling leaks is very

difficult, because leaks can be based on very low-level implementation details, such as caching behavior. Hence ourfocus, as in previous work, is on controlling

internal leaks,in which information about
H variables is somehow trans-mitted to
L variables. This makes the task more tractable,because we can control what is observable by the running

program--for example, we can deny it access to a real-timeclock.

More precisely, we wish to achieve noninterferenceproperties, which assert that changing the initial values of
H variables cannot affect the final values of L variables.Given the nondeterminism associated with multi-threading,
and our assumption that thread scheduling is probabilistic,we require more precisely that changing the initial values of
H variables cannot affect the joint probability distributionof the possible final values of

L variables; this property iscalled
probabilistic noninterference.
A type system that guarantees a weaker property, called
possibilistic noninterference, was given in [10]. Building onthat work, a type system for probabilistic noninterference

was given in [13]. The restrictions imposed by that systemcan be summarized as follows:

1. An expression e is H if it contains any H variables;otherwise it is

L.

2. Only L expressions can be assigned to L variables.
3. A guarded command with H guard cannot assign to Lvariables.

4. The guard of a while loop must be L.
5. An if with H guard must be protected, so that it ex-ecutes atomically, and can contain no while loops

within its branches.
Restrictions 2 and 3 prevent what Denning [3] long agocalled

direct and indirect flows, respectively. In a languagewithout concurrency, restrictions 2 and 3 are sufficient to

guarantee noninterference--see for example [15]. Restric-tions 4 and 5 were introduced to prevent timing-based flows

1

in multi-threaded programs; unfortunately, they restrict theset of allowable programs quite severely.

A recent paper by Honda, Vasconcelos, and Yoshida [6]explores secure information flow in the

ss-calculus, showingin particular that the system of [10] can be embedded into

their system. Most interestingly, they propose enriching theset of command types of [10] from

ffl H cmd, for commands that assign only to H variablesand are guaranteed to terminate; and
ffl L cmd, for commands that assign to L variables ormight not terminate,
to

ffl o/ cmd +, for commands that assign only to variablesof type

o/ (or higher) and are guaranteed to terminate;and

ffl o/ cmd *, for commands that assign only to variablesof type

o/ (or higher) and might not terminate.

They then argue that in some cases H variables can be usedin the guards of while loops without sacrificing possibilistic

noninterference.Inspired by this suggestion, we can observe that the command typings used in [10] and [13] conflate two distinct is-sues: what does a command

assign to, and what is the com-mand's
running time. This leads us to propose commandtypes with two parameters to address these two issues separately. More precisely, our new system will make use of thefollowing command types:

ffl o/1 cmd o/2, for commands that assign only to variablesof type

o/1 (or higher) and whose running time dependsonly on variables of type

o/2 (or lower); and

ffl o/ cmd n, for commands that assign only to variablesof type

o/ (or higher) and which are guaranteed to ter-minate in exactly

n steps.

With these typings, we can impose more accurate restric-tions to prevent flows based on timing. In particular, we can

replace restrictions 4 and 5 above with the following rule:

A command whose running time depends on Hvariables cannot be followed sequentially by a

command that assigns to L variables.
Let's now consider some examples informally, assumingthat

x : H and y : L.

1. x := 0 : H cmd 1
2. y := 0 : L cmd 1
3. if x = 0 then x := 5 else skip : H cmd 2

4. while y = 0 do skip : H cmd L
5. while y = 0 do y := y \Gamma  1 : L cmd L
6. if x = 0 thenwhile

y = 0 do skipelse

skip : H cmd H
7. y := 5; while x + 1 do skip : L cmd H
8. (while x = 0 do skip); y := 5 : illegal
Example 3 shows that an if can have a H guard and never-theless have a known running time; such a command can be

sequentially followed by a L assignment without any prob-lem.

Example 8, on the other hand, is illegal because the run-ning time of the while loop depends on the

H variable x, sowe can't follow it sequentially with an assignment to the

Lvariable
y. This example would be dangerous, because an-other thread could reliably determine whether

x is 0 or not,simply by waiting for a while (to give the thread scheduler

a chance to run all threads) and then seeing whether y is 5.

Our typings also satisfy interesting subtyping rules. Asusual, we have

L ` H. Furthermore, command types arecontravariant in their first position, and covariant in their

second position. Also, o/ cmd n ` o/ cmd L, because ifa command always halts in

n steps, then its running timedoesn't depend on the values of

H variables. This subtypingrule implies that example 3 above also has type

H cmd L.
The rest of the paper is organized as follows. Sec-tion 2 reviews the definition of our multi-threaded language,

which is the same as the language of [13], and Section 3 de-fines our type system precisely. The soundness of the type
system is then proved in Section 4, which argues that ev-ery well-typed multi-threaded program satisfies probabilistic noninterference. Finally, Section 5 concludes and men-tions some future directions.

2 The Multi-Threaded Language

Threads are written in the simple imperative language:

(phrases) p ::= e j c
(expressions) e ::= x j n j e1 + e2 j

e1 \Lambda  e2 j e1 = e2 j : : :

(commands) c ::= x := e jskip

jif
e then c1 else c2 jwhile

e do c j
c1; c2 jprotect

c

2

In our syntax, metavariable x ranges over identifiers and nover integer literals. Integers are the only values; we use 0
for false and nonzero for true. We assume that expressionsare

free of side effects and are total. The command protect ccauses

c to be executed atomically; this is important onlywhen concurrency is considered.

Programs are executed with respect to a memory _,which is a mapping from identifiers to integers. Also, we
assume for simplicity that expressions are evaluated atom-ically; thus we simply extend a memory

_ in the obviousway to map expressions to integers, writing

_(e) to denotethe value of expression
e in memory _.We define the semantics of commands via a sequential

transition relation \Gamma ! on configurations. A configuration
C is either a pair (c; _) or simply a memory _. In the firstcase,

c is the command yet to be executed; in the secondcase, the command has terminated, yielding final memory

_. The sequential transition relation is defined by the fol-lowing (completely standard) structural operational semantics:

(UPDATE) x 2 dom(_)

(x := e; _)\Gamma !_[x := _(e)]

(NO-OP) (skip; _)\Gamma !_
(BRANCH) _(e) 6= 0

(if e then c1 else c2; _)\Gamma !(c1; _)

_(e) = 0
(if e then c1 else c2; _)\Gamma !(c2; _)

(LOOP) _(e) = 0

(while e do c; _)\Gamma !_

_(e) 6= 0
(while e do c; _)\Gamma !(c; while e do c; _)

(SEQUENCE) (c1; _)\Gamma !_0

(c1; c2; _)\Gamma !(c2; _0)

(c1; _)\Gamma !(c01; _0)
(c1; c2; _)\Gamma !(c01; c2; _0)

(ATOMICITY) (c; _)\Gamma !\Lambda _0

(protect c; _)\Gamma !_0

In rule (ATOMICITY), note that (as usual) \Gamma !\Lambda  denotes thereflexive transitive closure of

\Gamma !.Note that our sequential transition relation

\Gamma ! is deterministic and total (if some obvious restrictions are met):

Lemma 2.1 If every identifier in c is in dom(_) and no sub-command involving while is protected in

c, then there is a
unique configuration C such that (c; _)\Gamma !C.

Also, the behavior of sequential composition is charac-terized by the following two lemmas:
Lemma 2.2 If (c1; _) \Gamma !i _0 and (c2; _0) \Gamma !j _00, then
(c1; c2; _) \Gamma !i+j _00.

Lemma 2.3 If (c1; c2; _)\Gamma !j_0, then there exist i and _00
such that 0 ! i ! j, (c1; _)\Gamma !i_00, and (c2; _00)\Gamma !j\Gamma i_0.

The multi-threaded programs that we consider here con-sist simply of a set of commands (the threads) running concurrently under a shared memory _. We model this set as a
thread pool O, which is a finite function from thread iden-tifiers (

ff, fi, . . . ) to commands. A pair (O; _), consistingof a thread pool and a shared memory, is called a global

configuration.A multi-threaded program is executed in an interleaving
manner, by repeatedly choosing a thread to run for a step.We assume that the choice is made probabilistically, with
each thread having an equal probability of being chosen ateach step--that is, we assume a

uniform thread scheduler.We formalize this by defining a global transition relation

p
=) on global configurations:

(GLOBAL) O(ff) = c

(c; _)\Gamma !_0
p = 1=jOj

(O; _)

p
=)(O \Gamma  ff; _0)

O(ff) = c
(c; _)\Gamma !(c0; _0)
p = 1=jOj

(O; _)

p
=)(O[ff := c0]; _0)

(f g; _)

1
=)(f g; _)

The judgment (O; _)

p
=)(O0; _0) asserts that the probabil-ity of going from global configuration

(O; _) to (O0; _0) is
p. Note that O \Gamma  ff denotes the thread pool obtained by re-moving thread

ff from O, and O[ff := c0] denotes the threadpool obtained by updating the command associated with

ffto
c0. The third rule (GLOBAL), which deals with an emptythread pool, allows us to view a multi-threaded program as

a discrete Markov chain [4]. The states of the Markov chainare global configurations and the transition matrix is governed by

p
=).

3 The Type System

Our type system is based upon the following types:

(data types) o/ ::= L j H(

phrase types) ae ::= o/ j o/ var j o/1 cmd o/2 j

o/ cmd n

3

(R-VAL) fl(x) = o/ var

fl ` x : o/

(INT) fl ` n : L
(SUM) fl ` e1 : o/; fl ` e2 : o/

fl ` e1 + e2 : o/

(ASSIGN) fl(x) = o/ var; fl ` e : o/

fl ` x := e : o/ cmd 1

(SKIP) fl ` skip : H cmd 1
(IF) fl ` e : o/

fl ` c1 : o/ cmd n
fl ` c2 : o/ cmd n
fl ` if e then c1 else c2 : o/ cmd n + 1

fl ` e : o/1
o/1 ` o/2
fl ` c1 : o/2 cmd o/3
fl ` c2 : o/2 cmd o/3
fl ` if e then c1 else c2 : o/2 cmd o/1 . o/3

(WHILE) fl ` e : o/1

o/1 ` o/2
o/3 ` o/2
fl ` c : o/2 cmd o/3
fl ` while e do c : o/2 cmd o/1 . o/3

(COMPOSE) fl ` c1 : o/ cmd m

fl ` c2 : o/ cmd n
fl ` c1; c2 : o/ cmd m + n

fl ` c1 : o/1 cmd o/2
o/2 ` o/3
fl ` c2 : o/3 cmd o/4
fl ` c1; c2 : o/1 ^ o/3 cmd o/2 . o/4

(PROTECT) fl ` c : o/1 cmd o/2

c contains no while loops
fl ` protect c : o/1 cmd 1

Figure 1. Typing rules

4

(BASE) L ` H
(CMD) o/ 01 ` o/1; o/2 ` o/ 02

o/1 cmd o/2 ` o/ 01 cmd o/ 02

o/ 0 ` o/
o/ cmd n ` o/ 0 cmd n

o/ cmd n ` o/ cmd L
(REFLEX) ae ` ae
(TRANS) ae1 ` ae2; ae2 ` ae3

ae1 ` ae3

(SUBSUMP) fl ` p : ae1; ae1 ` ae2

fl ` p : ae2

Figure 2. Subtyping rules

The rules of the type system are given in Figures 1 and 2.In the rules (

IF), (WHILE), and (COMPOSE), . denotes join(
least upper bound) and ^ denotes meet (greatest lowerbound). The rules allow us to prove typing judgments of

the form fl ` p : ae as well as subtyping judgments of theform

ae1 ` ae2. Here fl denotes an identifier typing, mappingidentifiers to phrase types of the form

o/ var. As usual, wesay that phrase
p is well typed under fl if fl ` p : ae for some
ae. Similarly, thread pool O is well typed under fl if eachthread in

O is well typed under fl.As an example, let's show the derivation of the typing of

example 7 from the Introduction:

fl ` y := 5; while x + 1 do skip : L cmd H;
assuming that fl(x) = H var and fl(y) = L var. We have

fl ` 5 : L (1)
by rule (INT). Then we get

fl ` y := 5 : L cmd 1 (2)
by rule (ASSIGN) on (1), and

fl ` y := 5 : L cmd L (3)
by rule (SUBSUMP) on (2) using the third rule (CMD). Nextwe have

fl ` x : H (4)
from rule (R-VAL) and

fl ` 1 : L (5)
by rule (INT), which gives

fl ` 1 : H (6)

by rule (SUBSUMP) on (5) using rule (BASE), and

fl ` x + 1 : H (7)
by rule (SUM) on (4) and (6). Next

fl ` skip : H cmd 1 (8)
by rule (SKIP), and hence

fl ` skip : H cmd L (9)
by rule (SUBSUMP) on (8) using the third rule (CMD).Hence we get

fl ` while x + 1 do skip : H cmd H (10)
by rule (WHILE) on (7) and (9), since H ` H (by rule(

REFLEX)) and L ` H (by rule (BASE)), and since H .L =
H. And finally, we get

fl ` y := 5; while x + 1 do skip : L cmd H (11)
by the second rule (COMPOSE) on (3) and (10), since L `
H, L ^ H = L, and L . H = H.We now give some discussion of the typing rules.

The first (IF) rule says that an if statement takes n + 1steps if both its branches take

n steps. This rule can some-times be used to "pad" a command to eliminate timing

leaks, as in the transformation approach proposed by JohanAgat [1]. For example, if

x : H and y : L, then the thread

if x = 0 then

x := x \Lambda  x; x := x \Lambda  xelse

x := x + 1;
y := 0

5

is dangerous, because the time at which y is assigned 0 de-pends on the value of

x. And this program is not well typedunder our rules--the then branch of the if has type

H cmd 2and the else branch has type
H cmd 1, which means thatthe first (
IF) rule does not apply. Instead we must coercethe two branches to type

H cmd L and use the second (IF)rule, which gives the if type

H cmd H. But this makes itillegal (under the second rule (

COMPOSE)) to sequentiallycompose the if with the assignment

y := 0, which has type
L cmd 1, and H 6` L. To make the program well typed, wecan pad the else branch to

x := x + 1; skip, which has type
H cmd 2. Now we can type the if using the first (IF) rule,giving it type

H cmd 3, and then we can give the threadtype
L cmd 4, using the first rule (COMPOSE). It shouldbe noted, however, that Agat's transformation approach is

more general that what we can achieve here.

The second rule (IF) is rather complex. One might hopethat we could exploit subtyping to simplify the rule, but this

is not possible here. We would not want to coerce the typeof

e up to o/2, because then it would appear that the executiontime of the if depends on

o/2 variables. Nor would we wantto coerce the types of
c1 and c2 to o/1 cmd o/3, because thenit would appear that the if can assign to

o/1 variables.
We can, however, specialize the second rule (IF) to a pairof rules in the case where

L and H are the only securitylevels; the same specialization can be done to rule (

WHILE)and the second rule (
COMPOSE). The specialized typingrules are shown in Figure 3.

The constraint o/3 ` o/2 in rule (WHILE) is perhaps sur-prising,1 but the typing rules are unsound without it. The
problem is that while e do c implicitly involves sequentialcomposition of

c and the entire loop, as shown in the sec-ond rule (
LOOP). As a result, if c's running time dependson
H variables, then c must not assign to L variables. Forexample, if

x is H and y is L, then without the constraint
o/3 ` o/2 in rule (WHILE), the following program would bewell typed:

while 1 do(

y := y + 1; while x do skip)

Note that y := y + 1 has type L cmd L and while x do skiphas type

H cmd H, so the loop body has type L cmd H.Hence, without the constraint

o/3 ` o/2, the while loop canbe given type
L cmd H. But the loop is dangerous--if x =
0, then y is incremented only once, and if x 6= 0, then y isincremented repeatedly.

Finally, we note that protect c takes a command that isguaranteed to terminate and makes it appear to run in just
one step. This gives another way of dealing with the exam-ple program discussed above; rather than padding the else
branch, we can just protect the if (or just its then branch),

1Indeed, I did not originally notice the need for it.

thereby masking any timing differences resulting from dif-ferent values of

x.

4 Properties of the Type System

In this section, we formally establish the properties ofthe type system. We begin with a lemma that shows that the
type system does not restrict a thread at all unless the threadinvolves both

L and H variables:

Lemma 4.1 Any command involving only L variables hastype

L cmd L. Any command involving only H variables
has type H cmd H.

Note this is certainly not the case for the type system of[13], since (for example) that system disallows

H variablesin the guards of while loops.

Now we establish the soundness of the type system.
Lemma 4.2 (Simple Security) If fl ` e : L, then contains
only L variables.

Proof. By induction on the structure of e.
Lemma 4.3 (Confinement) If fl ` c : H cmd o/ , then cdoes not assign to any

L variables.

Proof. By induction on the structure of c.
Lemma 4.4 (Subject Reduction) Suppose that (c; _)\Gamma !
(c0; _0). If fl ` c : o/1 cmd o/2, then fl ` c0 : o/1 cmd o/2.
And if fl ` c : o/ cmd n then fl ` c0 : o/ cmd (n \Gamma  1).

Proof. By induction on the structure of c.

The result holds vacuously if c is of the form x := e,skip, or protect

c0.
If c is of the form if e then c1 else c2, then c0 is either c1or
c2. Now, if c has type o/ cmd n, then it must be typedby the first rule (

IF), which implies that both c1 and c2 havetype
o/ cmd (n \Gamma  1). And if c has type o/1 cmd o/2, then itis typed either with the first rule (

IF) (using the fact that
o/1 cmd m ` o/1 cmd o/2), or with the second rule (IF). Inthe first case,

c1 and c2 have type o/1 cmd m, which impliesthat they also have type

o/1 cmd o/2. In the second case, c1and
c2 have type o/1 cmd o/3, for some o/3 with o/3 ` o/2. Sothey have type

o/1 cmd o/2 as well, by rule (SUBSUMP).If
c is of the form while e do c1, then c0 is of the form
c1; c. In this case, c cannot have type o/ cmd n; it musthave type

o/1 cmd o/2 by rule (WHILE). Hence c1 has type
o/1 cmd o/3 for some o/3 with o/3 ` o/2 and o/3 ` o/1. Therefore,by the second rule (

COMPOSE), c1; c has type o/1 cmd o/2.2Finally, if
c is of the form c1; c2, then c0 is either c2 (if
(c1; _)\Gamma !_0) or c01; c2 (if (c1; _)\Gamma !(c01; _0)). If c has type

2Note that this last step would fail without the constraint o/

3 ` o/1.

6

(IF) fl ` e : L

fl ` c1 : o/1 cmd o/2
fl ` c2 : o/1 cmd o/2
fl ` if e then c1 else c2 : o/1 cmd o/2

fl ` e : H
fl ` c1 : H cmd H
fl ` c2 : H cmd H
fl ` if e then c1 else c2 : H cmd H

(WHILE) fl ` e : L

o/2 ` o/1
fl ` c : o/1 cmd o/2
fl ` while e do c : o/1 cmd o/2

fl ` e : H
fl ` c : H cmd H
fl ` while e do c : H cmd H

(COMPOSE) fl ` c1 : o/1 cmd L

fl ` c2 : o/1 cmd o/2
fl ` c1; c2 : o/1 cmd o/2

fl ` c1 : o/ cmd H
fl ` c2 : H cmd H
fl ` c1; c2 : o/ cmd H

Figure 3. Typing rules specialized to L and H

o/ cmd n, then it must be typed by the first rule (COMPOSE)which means that

c1 has type o/ cmd k and c2 has type
o/ cmd l for some k and l with k + l = n. If c0 is c2, thenwe must have

k = 1, so c2 has type o/ cmd (n \Gamma  1). If c0is
c01; c2, then by induction c01 has type o/ cmd (k \Gamma  1), andtherefore

c0 has type o/ cmd (k \Gamma  1 + l) = o/ cmd (n \Gamma  1).And if
c has type o/1 cmd o/2, then it must be typed bythe second rule (

COMPOSE) which means that c1 has type
o/3 cmd o/4 and c2 has type o/5 cmd o/6, for some o/3, o/4,
o/5, and o/6 satisfying the subtyping constraints o/4 ` o/5,
o/4 ` o/2, o/6 ` o/2, o/1 ` o/3, and o/1 ` o/5. Now, if c0is

c2, then by rule (SUBSUMP) it also has type o/1 cmd o/2,since

o/5 cmd o/6 ` o/1 cmd o/2. And if c0 is c01; c2, then byinduction

c01 has type o/3 cmd o/4, and therefore c0 has type
o/1 cmd o/2.

Lemma 4.5 If fl ` c : o/ cmd 1 and dom(_) = dom(fl),
then (c; _)\Gamma !_0 for some _0.

Proof. The only commands with type o/ cmd 1 are x := e,skip, and protect

c1. The result is immediate in the firsttwo cases; in the case of protect

c1 we note that if c1 iswell typed and free of while loops, then

c1 is guaranteed toterminate.

Definition 4.1 Memories _ and * are equivalent with respect to fl, written _,fl*, if _, *, and fl have the same do-main and

_ and * agree on all L variables.

We now explore the behavior of a well-typed command
c when run under two equivalent memories. We begin witha Mutual Termination lemma for while-free programs:

Lemma 4.6 (Mutual Termination) Let c be a command
containing no while loops. If c is well typed under fl, _,fl*,and

(c; _)\Gamma !\Lambda _0, then there is a *0 such that (c; *)\Gamma !\Lambda *0
and _0,fl*0.

Proof. Similar to Lemma 5.6 of [13].

In the context of multi-threaded programs, however, it isnot enough to consider only the final memory resulting from
the execution of c (as in the Mutual Termination lemma); wemust also consider timing. The key property that lets us establish probabilistic noninterference is this: if a well-typedcommand

c is run under two equivalent memories, it makesexactly the same sequence of assignments to

L variables, atthe same times. Hence the two memories will remain equivalent after every execution step. (Note however that c mayterminate faster under one memory than the other; but then
the slower execution will not make any more assignmentsto

L variables.)

7

Here's an example that illustrates some of the working ofthe type system. Suppose that

c is a well-typed commandof the form

(if e then c1 else c2); c3:
What happens when c is run under two equivalent mem-ories

_ and *? If e : L, then _(e) = *(e) by SimpleSecurity, and hence both executions will choose the same

branch. If, instead, e : H, then the two executions maychoose different branches. But if the if is typed using the
first rule (IF), then both branches have type H cmd n forsome

n. Therefore neither branch assigns to L variables,by Confinement, and both branches terminate after

n steps,by Subject Reduction. Hence both executions will reach

c3at the same time, and the memories will still be equivalent.

And if the if is typed using the second rule (IF), then it getstype

H cmd H, which is also the type given to each branch.Again, neither branch assigns to

L variables, by Confine-ment. Now, in this case the two branches may not terminate

at the same time--indeed, one may terminate and the othermay not. But the entire command

(if e then c1 else c2); c3will have to be typed by the second rule (

COMPOSE), whichmeans that
c3 : H cmd H. Hence c3 makes no assignmentsto
L variables, which means that, as far as L variables areconcerned, it doesn't matter when (or even whether)

c3 isexecuted.

To formalize these ideas, we need to define a notion ofequivalence on configurations; then we can argue that

\Gamma !takes equivalent configurations to equivalent configurations.

But first we make some observations about sequential com-position. Any command

c can be written in the standard
form

(: : : ((c1; c2); c3); : : :); ck

for some k * 1, where c1 is not a sequential composition(but

c2 through ck might be sequential compositions). If weadopt the convention that sequential composition associates

to the left, then we can write this more simply as

c1; c2; c3; : : : ; ck:
Now, if c is executed, it follows from the (SEQUENCE) rulesthat the first execution step touches only

c1; that is, we haveeither

(c1; c2; c3; : : : ; ck; _)\Gamma !(c2; c3; : : : ; ck; _

0

);

if (c1; _)\Gamma !_0, or else

(c1; c2; c3; : : : ; ck; _)\Gamma !(c

0

1; c2; c3; : : : ; ck; _

0

);

if (c1; _)\Gamma !(c01; _0). Now we define our notion of equiva-lence on commands:

Definition 4.2 We say that commands c and d are equiva-lent with respect to

fl, written c,fld, if c and d are both well
typed under fl and either

ffl c = d,
ffl c and d both have type H cmd o/ , or
ffl c has standard form c1; c2; c3; : : : ; ck, d has standard

form d1; c2; c3; : : : ; ck, for some k, and c1 and d1 bothhave type

H cmd n for some n.

We extend the notion of equivalence to configurations bysaying that configurations

C and D are equivalent, written
C,flD, if any of the following four cases applies:

ffl C is of the form (c; _), D is of the form (d; *), c,fld,and

_,fl*.

ffl C is of the form (c; _), D is of the form *, c has type

H cmd o/ , and _,fl*.

ffl C is of the form _, D is of the form (d; *), d has type

H cmd o/ , and _,fl*.

ffl C is of the form _, D is of the form *, and _,fl*.
(In effect, we are saying that a command of type H cmd o/is equivalent to a terminated command.)

Theorem 4.7 (Sequential Noninterference) Suppose that
(c; _),fl(d; *), (c; _)\Gamma !C0, and (d; *)\Gamma !D0. Then
C0,flD0.

Proof. We begin by dealing with the case when c and d bothhave type

H cmd o/ , for some o/ . In this case, by the Con-finement Lemma, neither

c nor d assigns to any L variables.Hence the memories of
C0 and D0 remain equivalent. Now,if
C0 is of the form (c0; _0) and D0 is of the form (d0; *0),then by the Subject Reduction Lemma,

c0 and d0 both havetype
H cmd o/ , which gives c0,fld0. The cases when C0 isof the form

_0 and/or D0 is of the form *0 are similar.Now consider the case where

c and d do not both havetype
H cmd o/ . Let c have standard form c1; c2; c3; : : : ; ck.We can see from the definition of

,fl that either c = d or dhas standard form
d1; c2; c3; : : : ; ck, where c1 and d1 bothhave type
H cmd n for some n.In the latter case, we have by the Confinement Lemma

that neither c1 nor d1 assigns to any L variables. Hencethe memories of

C0 and D0 are equivalent. And if n ? 1,then by the Subject Reduction Lemma,

C0 and D0 are of theform
(c01; c2; c3; : : : ; ck; _0) and (d01; c2; c3; : : : ; ck; *0), re-spectively, where

c01 and d01 both have type H cmd (n \Gamma  1).Thus
C0,flD0. And if n = 1, then C0 and D0 are ofthe form

(c2; c3; : : : ; ck; _0) and (c2; c3; : : : ; ck; *0), respec-tively.3 So again

C0,flD0.We are left, finally, with the case where

c = d. Let themhave standard form
c1; c2; c3; : : : ; ck and consider in turneach of the possible forms of

c1:
3Actually, if k = 1 then C0 and D0 are just _0 and *0 here. We'll
ignore this point in the rest of this proof.

8

Case x := e. In this case, we have that C0 is

(c2; c3; : : : ; ck; _[x := _(e)])
and D0 is

(c2; c3; : : : ; ck; *[x := *(e)]):
Now if x is H, then _[x := _(e)],fl*[x := *(e)].And if

x is L, then by rule (ASSIGN) e : L. Hence
_(e) = *(e) by Simple Security, so again

_[x := _(e)],fl*[x := *(e)]:
Therefore C0,flD0.
Case skip. In this case C0 is (c2; c3; : : : ; ck; _) and D0 is

(c2; c3; : : : ; ck; *). So C0,flD0.

Case if e then c11 else c12. If e : L, then by Simple Secu-rity

_(e) = *(e). Hence C0 and D0 both choose thesame branch. That is, either

C

0

= (c11; c2; c3; : : : ; ck; _)

and

D

0

= (c11; c2; c3; : : : ; ck; *);

or else

C

0

= (c12; c2; c3; : : : ; ck; _)

and

D

0

= (c12; c2; c3; : : : ; ck; *):

So C0,flD0.
If e doesn't have type L, then if c1 is typed by the firstrule (

IF), then we have that c11 and c12 both have type
H cmd n for some n. Therefore, whether or not C0and

D0 take the same branch, we have C0,flD0.

And if c1 is typed by the second rule (IF), than it getstype

H cmd H. But, by rule (COMPOSE), this meansthat
c1; c2 also has type H cmd H. This in turn impliesthat

c1; c2; c3 has type H cmd H, and so on, until weget that

c has type H cmd H. So, since c = d here,this case has already been handled.

Case while e do c11. As in the case of if, if e : L, thenby Simple Security

_(e) = *(e). Hence the twocomputations stay together. That is, either

C0 =
(c2; c3; : : : ; ck; _) and D0 = (c2; c3; : : : ; ck; *), or else

C

0

= (c11; while e do c11; c2; c3; : : : ; ck; _)

and

D

0

= (c11; while e do c11; c2; c3; : : : ; ck; *):

So C0,flD0.
If e doesn't have type L, then by rule (WHILE) we havethat

c1 : H cmd H. As in the case of if, this impliesthat
c : H cmd H, so this case has again already beenhandled.

Case protect c11. By rule (PROTECT), c11 contains nowhile loops. Hence this case follows from the Mutual

Termination lemma.

We remark here that if c is well typed and _,fl*, then
(c; _),fl(c; *). Hence, by applying the Sequential Non-interference Theorem repeatedly, we see that, for all

k,the configuration reached from
(c; _) after k steps will beequivalent to the configuration reached from

(c; *) after ksteps. Hence, as we claimed above, the two executions

make exactly the same assignments to L variables, at thesame times.

Now we change our focus from the execution of an in-dividual thread

c, which is deterministic, to the executionof a
pool of threads O, which is a Markov chain. Ourfirst thought, given the Sequential Noninterference Theorem, may be that if O is well typed and _,fl*, then (O; _)and

(O; *) give rise to the same (i.e. isomorphic) Markovchains. But this isn't quite right. For example, suppose that

O is fff : (x := x \Lambda  x); fi : (x := x + 1)g. If x is
H, then memories fx = 0g and fx = 1g are equivalent.But running

O from fx = 0g gives a Markov chain with4 states:
(O; fx = 0g), (ffi : (x := x + 1)g; fx = 0g),
(fff : (x := x \Lambda  x)g; fx = 1g), and (fg; fx = 1g); andrunning

O from fx = 1g gives a Markov chain with 5states:
(O; fx = 1g), (ffi : (x := x + 1)g; fx = 1g),
(fff : (x := x \Lambda  x)g; fx = 2g), (fg; fx = 2g), and
(fg; fx = 4g). Note however that the last two states,
(fg; fx = 2g) and (fg; fx = 4g) should be consideredequivalent; thus we might feel that the two Markov chains

are basically the same after all.Formally, what we need is to construct a

quotient Markovchain. That is, given a Markov chain with state set

S andan equivalence relation
, on S, we'd like to form a newMarkov chain
S=, whose states are the equivalence classesof
S under ,. But when is this possible? Kemeny andSnell, who refer to the issue as "lumpability", identified the

needed condition long ago [7, p. 124]:

If s1 , s2, then for each equivalence class A, theprobability of going from

s1 to a state in A is thesame as the probability of going from

s2 to a statein
A: X

a2A

ps1a =

X

a2A

ps2a

(Here ps1a denotes the probability of going in one step from
s1 to a in the original Markov chain.) In this case, we candefine the transition probabilities for

S=, as follows: theprobability of going from equivalence class

A to equiva-lence class
B, denoted pAB, is

P

b2B pab, where a is any el-ement of
A; the above condition says exactly that the choiceof

a makes no difference. Within computer science, this

9

idea later appeared in the work of Larsen and Skou [8], who(noting the analogy to bisimulation) introduced the name
probabilistic bisimulation for such an equivalence relation
,. In more recent work, Hermanns [5] includes a lengthydiscussion of probabilistic bisimulation, and Sabelfeld and

Sands [9] apply it to probabilistic noninterference.The key property of

S=, is this [5, p. 49]:

Theorem 4.8 Suppose that , is a probabilistic bisimula-tion. Then for each starting state

s, equivalence class A,
and integer k, the probability that S, starting from s, willend up in a state in

A after k steps is equal to the probability that S=,, starting from [s] (the equivalence class of s),
will end up in state A after k steps.

Now, returning to our specific system, we want to de-fine a probabilistic bisimulation, which we'll still call

,fl,on the set of global configurations
(O; _). Under the (morerestrictive) type system of [13], it would suffice to define

(O1; _),fl(O2; *) iff O1 = O2 and _,fl*. But here weneed a looser notion, since (as discussed above) the executions of a well-typed command c under two equivalentmemories

_ and * can be quite different. Roughly, we wantto have
(O1; _),fl(O2; *) if _,fl* and O1(ff),flO2(ff) forall
ff. But actually O1 and O2 could have different domains,since changing the values of

H variables can affect the run-ning time of well-typed commands. So we'd like to say that

O1 can have extra threads in it, so long as they have type
H cmd o/ , (and similarly for O2). Unfortunately, this won'twork within our framework of probabilistic bisimulation,

because the transition probabilities will not be the same inthis case. For example, if

O1 = fff : (y := 1); fi : skipgand
O2 = fff : (y := 1)g, where y is L, then (O2; fy = 0g)goes to

(fg; fy = 1g) with probability 1, but (O1; fy = 0g)goes to the equivalent configuration

(ffi : skipg; fy = 1g)with probability only
1=2.4So to get a probabilistic bisimulation, we need a stronger

definition that requires equality of domains:
Definition 4.3 (O1; _),fl(O2; *) if dom(O1) = dom(O2),
O1(ff),flO2(ff) for all ff 2 dom(O1), and _,fl*.

And, regrettably, we have to change our rule (GLOBAL) toprevent the thread pool domain from ever changing:

(GLOBAL) O(ff) = c

(c; _)\Gamma !_0
p = 1=jOj

(O; _)

p
=)(O[ff := skip]; _0)

O(ff) = c
(c; _)\Gamma !(c0; _0)
p = 1=jOj

(O; _)

p
=)(O[ff := c0]; _0)

4Note that intuitively there is no information leakage here; it's just a

question of one program running more slowly than the other, which is notobservable internally.

The new semantics says, in effect, that a completed threadremains alive, wasting processor time.5 With this change,
we can now prove what we want:
Theorem 4.9 ,fl is a probabilistic bisimulation on the setof global configurations.

Proof. Suppose that (O; _),fl(O0; *) and suppose that
dom(O) and dom(O0) are fff1; ff2; : : : ; ffng. Then (O; _)and

(O0; *) can each go to n (not necessarily distinct)global configurations,

(O1; _1), (O2; _2), . . . , (On; _n),and
(O01; *1), (O02; *2), . . . , (O0n; *n), each with proba-bility

1=n, where (Oi; _i) and (O0i; *i) denote the globalconfigurations reached by choosing thread

ffi. Now, since
O(ffi),flO0(ffi), we have by the Sequential Noninterfer-ence Theorem that

(Oi; _i),fl(O0i; *i) for all i. This im-plies that the probabilities of reaching any equivalence class

from (O; _) or from (O0; *) are the same.

Finally we can argue that well-typed programs satisfyprobabilistic noninterference: if

O is well typed and _,fl*,then
(O; _),fl(O; *). Hence, by Theorem 4.8, the proba-bility that the

L variables have certain values after k steps isthe same when starting from

(O; _) as when starting from
(O; *).

5 Conclusion

The new type system allows probabilistic noninterfer-ence to be guaranteed for a much larger class of programs
than previously permitted. In particular, there seems to behope that the system might not be too hard to accommodate
in practice, since any threads that involve only H variablesor only

L variables are automatically well typed.
As in previous work, we have assumed that program ex-ecution is observable only internally; with external observations of running time, timing leaks are certainly possi-ble, because protected commands won't really execute in
one time step. However, if a program is well typed with-out the use of protect, then it seems possible in principle to
allow external observations, except that our simple timingmodel is unrealistic. For example, our semantics specifies
that x := x\Lambda x and skip each execute in 1 step, which seemsto require an implementation that wastes a lot of time. Agat
[2] has recently attempted to tackle external timing leaks inthe realistic setting of Java byte-code programs, which requires the consideration of a host of delicate timing issues,such as caching.

On the other hand, if we are concerned only with inter-nal timing leaks, then we don't really need such precise

5Also note that we actually have to do some summing in our p=) relation, since now it may be possible for (O; _) to reach (O0; _0) in morethan one way; see [9, p. 202].

10

timings. In particular, if doesn't matter to us how muchtime

x := x \Lambda  x and skip actually take, so long as rule(
GLOBAL) is implemented faithfully, which means simplythat the scheduler randomly picks a new thread after each

computation step. Of course, doing scheduling with suchfine granularity would appear to involve high overhead; it
remains to be seen whether acceptable performance couldbe achieved with such a scheduler. This needs more study.

In other future work, it would be desirable to find aweaker notion of probabilistic bisimulation that would allow our original rule (GLOBAL) to be used. Also, it wouldbe useful to investigate type inference for the new system,
presumably using the approach of [12]. Finally, it wouldbe valuable to integrate cryptography into the framework
of this paper by using a weaker notion of noninterferencebased on computational complexity; some preliminary steps
in this direction have been made [14, 11].

6 Acknowledgments

This work was partially supported by the National Sci-ence Foundation under grant CCR-9900951. I am grateful to Andrei Sabelfeld, David Sands, Dennis Volpano, andthe anonymous reviewers for their useful comments on this
work.

References

[1] J. Agat. Transforming out timing leaks. In Proceedings

27th Symposium on Principles of Programming Languages,
pages 40-53, Boston, MA, Jan. 2000.
[2] J. Agat. Type Based Techniques for Covert Channel Elimination and Register Allocation. PhD thesis, Chalmers University of Technology, G"oteborg, Sweden, Dec. 2000.
[3] D. Denning and P. Denning. Certification of programs for

secure information flow. Commun. ACM, 20(7):504-513,
1977.
[4] W. Feller. An Introduction to Probability Theory and Its Applications, volume I. John Wiley & Sons, Inc., third edition,
1968.
[5] H. Hermanns. Interactive Markov Chains. PhD thesis, University of Erlangen-N"urnberg, July 1998.
[6] K. Honda, V. Vasconcelos, and N. Yoshida. Secure information flow as typed process behaviour. In Proceedings 9th European Symposium on Programming, volume 1782 of Lecture Notes in Computer Science, pages 180-199, Apr. 2000.
[7] J. Kemeny and J. L. Snell. Finite Markov Chains. D. Van

Nostrand, 1960.
[8] K. G. Larsen and A. Skou. Bisimulation through probabilistic testing. Information and Computation, 94(1):1-28, 1991.
[9] A. Sabelfeld and D. Sands. Probabilistic noninterference

for multi-threaded programs. In Proceedings 13th IEEE
Computer Security Foundations Workshop, pages 200-214,
Cambridge, UK, July 2000.

[10] G. Smith and D. Volpano. Secure information flow in

a multi-threaded imperative language. In Proceedings
25th Symposium on Principles of Programming Languages,
pages 355-364, San Diego, CA, Jan. 1998.[11] D. Volpano. Secure introduction of one-way functions.

In Proceedings 13th IEEE Computer Security Foundations
Workshop, pages 246-254, Cambridge, UK, June 2000.[12] D. Volpano and G. Smith. A type-based approach to program security. In Proc. Theory and Practice of Software
Development, volume 1214 of Lecture Notes in Computer
Science, pages 607-621, Apr. 1997.[13] D. Volpano and G. Smith. Probabilistic noninterference

in a concurrent language. Journal of Computer Security,
7(2,3):231-253, 1999.[14] D. Volpano and G. Smith. Verifying secrets and relative

secrecy. In Proceedings 27th Symposium on Principles of
Programming Languages, pages 268-276, Boston, MA, Jan.
2000.[15] D. Volpano, G. Smith, and C. Irvine. A sound type system for secure flow analysis. Journal of Computer Security,
4(2,3):167-187, 1996.

11