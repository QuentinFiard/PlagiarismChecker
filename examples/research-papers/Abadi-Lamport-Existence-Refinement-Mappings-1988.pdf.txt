

29
The Existence of Refinement

Mappings

Mart'in Abadi and Leslie Lamport

August 14, 1988

Systems Research Center
DEC's business and technology objectives require a strong research program.
The Systems Research Center (SRC) and three other research laboratories
are committed to filling that need.

SRC began recruiting its first research scientists in l984--their charter, to
advance the state of knowledge in all aspects of computer systems research.
Our current work includes exploring high-performance personal computing,
distributed computing, programming environments, system modelling techniques, specification technology, and tightly-coupled multiprocessors.

Our approach to both hardware and software research is to create and use
real systems so that we can investigate their properties fully. Complex
systems cannot be evaluated solely in the abstract. Based on this belief,
our strategy is to demonstrate the technical and practical feasibility of our
ideas by building prototypes and using them as daily tools. The experience
we gain is useful in the short term in enabling us to refine our designs, and
invaluable in the long term in helping us to advance the state of knowledge
about those systems. Most of the major advances in information systems
have come through this strategy, including time-sharing, the ArpaNet, and
distributed personal computing.

SRC also performs work of a more mathematical flavor which complements
our systems research. Some of this work is in established fields of theoretical
computer science, such as the analysis of algorithms, computational geometry, and logics of programming. The rest of this work explores new ground
motivated by problems that arise in our systems research.

DEC has a strong commitment to communicating the results and experience
gained through pursuing these activities. The Company values the improved
understanding that comes with exposing and testing our ideas within the
research community. SRC will therefore report results in conferences, in
professional journals, and in our research report series. We will seek users
for our prototype systems among those with whom we have common research
interests, and we will encourage collaboration with university researchers.

Robert W. Taylor, Director

The Existence of Refinement Mappings
Mart'in Abadi and Leslie Lamport
August 14, 1988

iii
cflDigital Equipment Corporation 1988
This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment
of fee is granted for nonprofit educational and research purposes provided
that all such whole or partial copies include the following: a notice that
such copying is by permission of the Systems Research Center of Digital
Equipment Corporation in Palo Alto, California; an acknowledgment of the
authors and individual contributors to the work; and all applicable portions
of the copyright notice. Copying, reproducing, or republishing for any other
purpose shall require a license with payment of fee to the Systems Research
Center. All rights reserved.

iv

Author's Abstract
Refinement mappings are used to prove that a lower-level specification correctly implements a higher-level one. We consider specifications consisting
of a state machine (which may be infinite-state) that specifies safety requirements, and an arbitrary supplementary property that specifies liveness
requirements. A refinement mapping from a lower-level specification S1 to
a higher-level one S2 is a mapping from S1's state space to S2's state space.
It maps steps of S1's state machine to steps of S2's state machine and maps
behaviors allowed by S1 to behaviors allowed by S2. We show that, under reasonable assumptions about the specifications, if S1 implements S2,
then by adding auxiliary variables to S1 we can guarantee the existence of
a refinement mapping. This provides a completeness result for a practical,
hierarchical specification method.

Capsule Review
This report deals with the problem of proving that implementations satisfy
their specifications. Suppose, for example, that a client asks a circuit fabricator to build a box with S inside and with certain external signals (inputs
and outputs). The circuit fabricator returns later with an epoxy brick that
has an appropriate number of wires sticking out for the external signals, but
that actually contains not S but some other ciruit I. In order to guarantee
that the client cannot detect the substitution without breaking open the
brick (and thereby voiding the warranty), the fabricator must be sure that
for any possible behavior of I there corresponds at least one behavior of S
that produces identical external signals.

In some cases, such a correspondence between behaviors can be proved
using a refinement mapping, a function that maps states of I to states of
S and that satisfies certain conditions. The refinment mapping technique
reduces a problem of proving something about arbitrary behaviors to one of
proving something about single state transitions. Unfortunately there are
many cases in which a correct implementation I cannot be related to its
specification S by a refinement mapping.

This report shows that it is possible in a very large class of cases to
augment a legal implementation with some extra state components (history
and prophecy variables) in a way that places no constraints on the behavior
of the implementation but that makes it possible to produce an appropriate
refinement mapping to the specification. This result broadens considerably
the domain of applicability of the refinement mapping technique.

Jim Saxev

Contents
1 Introduction 1

1.1 Specifications : : : : : : : : : : : : : : : : : : : : : : : : : : : 1
1.2 Proving That One Specification Implements Another : : : : : 3

2 Preliminaries 6

2.1 Sequences : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6
2.2 Properties : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 7
2.3 Specifications : : : : : : : : : : : : : : : : : : : : : : : : : : : 8
2.4 Refinement Mappings : : : : : : : : : : : : : : : : : : : : : : 11

3 Finite Invisible Nondeterminism 12
4 Safety Properties 16
5 Auxiliary Variables 20

5.1 History Variables : : : : : : : : : : : : : : : : : : : : : : : : : 20
5.2 Simple Prophecy Variables : : : : : : : : : : : : : : : : : : : : 22
5.3 Prophecy Variables That Add Stuttering : : : : : : : : : : : : 25

6 Internal Continuity 28
7 The Completeness Theorem 30
8 Whence and Whither? 35
References 38

vi

1 Introduction
1.1 Specifications
A system may be specified at many levels of abstraction, from a description
of its highest-level properties to a description of its implementation in terms
of microcode and circuitry. We address the problem of proving that a lowerlevel specification is a correct implementation of a higher-level one.

Unlike simple programs, which can be specified by input/output relations, complex systems can be adequately specified only by describing their
behaviors--that is, their possible sequences of inputs and outputs. We consider specification methods in which a behavior is represented by a sequence
of states and a system is specified by a set of permitted behaviors. Input
and output are represented in the state--for example, by including a keyboard state describing which keys are currently depressed and a screen state
describing what is currently displayed.

A specification should describe only the externally visible components
of a system's state. However, it is often helpful to describe its behavior in
terms of unobservable internal components. For example, a natural way to
specify a queue includes a description of the sequence of elements currently
in the queue, and that sequence is not externally visible. Although internal
components are mentioned, the specification prescribes the behavior of only
the externally visible components. The system may exhibit the externally
visible behavior

hhe0; e1; e2; : : : ii

where ei is a state of the externally visible component, if there exist states
yi of the internal component such that the complete behavior

hh(e0; y0); (e1; y1); (e2; y2); : : : ii
is permitted by the specification. (We use hh ii to denote a sequence.)

A specification may allow steps in which only the internal state component changes--for example, a sequence

hh(e0; y0); (e1; y1); (e1; y

0

1); (e1; y

00
1); (e2; y2); : : : ii

1

Such internal steps are not externally visible, so the sequence of external
states hhe0; e1; e1; e1; e2; : : :ii should be equivalent to the sequence hhe0; e1;
e2; : : : ii obtained by removing the "stuttering" steps from e1 to e1. Let
\Gamma hhe0; e1; : : :ii be the set of all sequences obtained from hhe0; e1; : : :ii by repeating states and deleting repeated states--that is, by adding and removing finite amounts of stuttering. We consider only specifications in which
a sequence hhe0; e1; : : :ii is allowed only if all sequences in \Gamma hhe0; e1; : : :ii are
allowed. Such specifications are said to be invariant under stuttering.

The behaviors permitted by a specification can be described as the set
of sequences satisfying a safety and a liveness property [AS86, Lam77]. Intuitively, a safety property asserts that something bad does not happen and
a liveness property asserts that something good does eventually happen.
In specifying a queue, the safety property might assert that the sequence
of elements removed from the queue is an initial prefix of the sequence of
elements added to the queue. The liveness property might assert that an
operation of putting an element into the queue is eventually completed if
the queue is not full, and an operation of removing an element from the
queue is eventually completed if the queue is not empty. (What operations
are in progress and what elements they are adding to or have removed from
the queue would be described by the externally visible state.)

We are concerned with specifications in which the safety property is described by an "abstract" nondeterministic program; a behavior satisfies the
property if it can be generated by the program. Liveness properties are
described either directly by writing axioms or indirectly by placing fairness
constraints on the abstract program. In a specification of a queue, the program describes the sequence of actions by which an element is added to or
removed from the sequence of queued elements, ensuring the safety property
that the correct elements are removed from the queue. Additional fairness
constraints assert that certain actions must eventually occur, ensuring the
liveness property that operations that should complete eventually do complete.

Many proposed specification methods involve writing programs and
fairness conditions in this way [LS84, Lam83, LT87]. (Some methods do
not consider liveness at all and just specify safety properties with
programs.)

To describe specifications formally, we represent a program by a state
machine (whose set of states may be infinite) and we represent the fairness
constraints by an arbitrary supplementary condition. For our results, it does
not matter if the supplementary condition specifies a liveness property.

2

1.2 Proving That One Specification Implements Another
A specification S1 implements a specification S2 if every externally visible
behavior allowed by S1 is also allowed by S2. To prove that S1 implements
S2, it suffices to prove that if S1 allows the behavior

hh(e0; z0); (e1; z1); (e2; z2); : : : ii
where the zi are internal states, then there exist internal states yi such that
S2 allows

hh(e0; y0); (e1; y1); (e2; y2); : : : ii

In general, each yi can depend upon the entire sequence hh(e0; z0); (e1; z1);
(e2; z2); : : :ii, and proving the existence of the yi may be quite difficult. The
proof is easier if each yi depends only upon ei and zi, so there exists a
function f such that (ei; yi) = f (ei; zi). To verify that hhf (e0; z0); f (e1; z1);
f (e2; z2); : : :ii satisfies the safety property of S2, it suffices to show that
f preserves state machine behavior--that is, it maps executions of S1's
state machine to executions (possibly with stuttering) of S2's state machine. Proving that f preserves state machine behavior involves reasoning
about states and pairs of states, not about sequences. Verifying that f
preserves liveness--meaning that hhf (e0; z0); f (e1; z1); f (e2; z2); : : : ii satisfies
the liveness property of S2--usually also requires only local reasoning, with
no explicit reasoning about sequences. A mapping f that preserves state
machine behavior and liveness is called a refinement mapping.

In the example of a queue, the internal state yi of specification S2 might
describe the sequence of elements currently in the queue, and the internal
state zi of specification S1 might describe the contents of an array that implements the queue. To prove that S1 implements S2, one would construct
a refinement mapping f such that f (ei; zi) = (ei; yi), where yi describes the
state of the queue that is represented by the contents of the array described
by state zi.

Several methods for proving that one specification implements another
are based upon finding a refinement mapping [LS84, Lam83]. In practice,
if S1 implements S2, then these methods usually can prove that the implementation is correct--usually, but not always. The methods fail if the
refinement mapping does not exist. Three reasons why the mapping might
not exist are:

ffl S2 may specify an internal state with "historical information" not

needed by S1. For example, suppose S2 requires that the system

3

display up to three of the least-significant bits of a three-bit clock.
This specification is implemented by a lower-level specification S1 that
alternately displays zero and one, with no internal state. A refinement
mapping does not exist because there is no way to define the internal
state of a three-bit clock as a function of its low-order bit.

ffl S2 may specify that a nondeterministic choice is made before it has

to be. For example, consider two specifications S1 and S2 for a system that displays ten nondeterministically chosen values in sequence.
Suppose S2 requires that all values be chosen before any is displayed,
while S1 requires each value to be chosen as it is displayed. Both
specifications describe the same externally visible behaviors, so each
implements the other. However, S2 requires the internal state to contain all ten values before any is displayed, while S1 does not specify
any internal state, so no refinement mapping is possible.

ffl S2 may "run slower" than S1. For example, let S1 and S2 both specify

clocks in which hours and minutes are externally visible and seconds
are internal. Suppose that in S2 each step increments the clock by
one second, while in S1 each step increments the clock by ten seconds.
Both specifications allow the same externally visible behaviors. To
show that S2 implements S1, we can use the refinement mapping f
that rounds the time down to the nearest multiple of ten seconds. For
any complete behavior hhs0; s1; s2; : : :ii allowed by S2, the behavior
hhf (s0); f (s1); f (s2); : : :ii is a complete behavior allowed by S1 that
contains nine "stuttering" steps for every step that changes the state.

On the other hand, a complete behavior hhs0; s1; s2; : : :ii specified by
S1 may produce an externally visible change every six steps. For any
mapping f , the sequence hhf (s0); f (s1); f (s2); : : :ii may also produce
an externally visible change every six steps. This is not allowed by S2,
which requires fifty-nine internal steps for every externally visible one.
Hence, no refinement mapping can prove that S1 implements S2.

If a refinement mapping does not exist, it can often be made to exist
by adding auxiliary variables to the lower-level specification. An auxiliary
variable is an internal state component that is added to a specification without affecting the externally visible behavior. The three situations described
above in which refinement mappings cannot be found are handled as follows:

ffl Historical information missing from the internal state specified by S1

4

can be provided by adding a history variable--a well-known form of
auxiliary variable that merely records past actions [Owi75].

ffl If S2 requires that a nondeterministic choice be made before it has to

be, then S1 can be modified so the choice is made sooner by adding
a prophecy variable. A prophecy variable is a new form of auxiliary
variable that is the mirror image of a history variable--its formal definition is almost the same as the definition of a history variable with
past and future interchanged, but there is an asymmetry due to behaviors having a beginning but not necessarily an end.

ffl If S2 runs slower than S1, then an auxiliary variable must be added

to S1 to slow it down. We will define prophecy variables in such a way
that they can perform this slowing.

Our main result is a completeness theorem. It states that, under three
hypotheses about the specifications, if S1 implements S2 then one can add
auxiliary history and prophecy variables to S1 to form an equivalent specification Shp1 and find a refinement mapping from Shp1 to S2. The three
hypotheses, and their intuitive meanings, are:

S1 is machine closed. Machine closure means that the supplementary property (the one normally used to specify liveness requirements) does not
specify any safety property not already specified by the state machine.
In other words, the state machine does as much of the specifying as
possible.

S2 has finite invisible nondeterminism. This denotes that, given any finite

number of steps of an externally visible behavior allowed by S2, there
are only a finite number of possible choices for its internal state component.

S2 is internally continuous. A specification is internally continuous if, for

any complete behavior that is not allowed, we can determine that it is
not allowed by examining only its externally visible part (which may
be infinite) and some finite portion of the complete behavior.

We will show by examples why these three hypotheses are needed.

We will prove that any safety property has a specification with finite
invisible nondeterminism, any specification of a safety property is internally
continuous, and any property has a machine-closed specification. Therefore,

5

our completeness theorem implies that if the specifications are written in a
suitable form and S2 specifies only a safety property then one can ensure
that a refinement mapping exists. We will also show that, even when S2
is not internally continuous, a refinement mapping exists to show that S1
satisfies the safety property specified by S2. Therefore, by writing suitable
specifications, refinement mappings can always be used to prove the safety
property of a specification if not its liveness property. We do not know if
anything can be said about proving arbitrary liveness properties.

Throughout this report, proofs are written in a self-explanatory structured format. The format permits very careful proofs that can be read to
any desired level of detail by ignoring lower-level statements. Writing proofs
in this format helped us to eliminate many errors and greatly increased our
confidence in the correctness of the results.

A glossary/index of notations and conventions appears at the end of this
report, along with an index. We hope they will help the reader cope with
the formalism.

2 Preliminaries
2.1 Sequences
We now define some useful notations for sequences. In these definitions,
oe denotes the sequence hhs0; s1; s2; : : :ii and o/ denotes the sequence hht0; t1;
t2; : : :ii. These sequences may be finite or infinite. If oe is finite, we let koek
denote its length and last(oe) denote its last element, so khhs0; : : : ; sm\Gamma 1iik =
m and last(hhs0; : : : ; sm\Gamma 1ii) = sm\Gamma 1. An infinite sequence is said to be
terminating iff (if and only if) it is of the form hhs0; s1; : : : ; sn; sn; sn; : : :ii--
in other words, if it reaches a final state in which it stutters forever.

As usual, a mapping on elements is extended to a mapping on sequences
of elements by defining g(oe) to equal hhg(s0); g(s1); : : :ii, and to a mapping
on sets of elements by defining g(S) to equal fg(s) : s 2 Sg.

The sequence oe is said to be stutter-free if, for each i, either si 6= si+1
or the sequence is infinite and si = sj for all j * i. Thus, a nonterminating
sequence is stutter-free iff it never stutters, and a terminating sequence is
stutter-free iff it stutters only after reaching its final state. We define "oe
to be the stutter-free form of oe--that is, the stutter-free sequence obtained
by replacing every maximal finite subsequence si; si+1; : : : ; sj of identical

6

elements with the single element si. For example,

"hh0; 1; 1; 2; 2; 2; 3; 3; 3; 3; 4 : : :ii = hh0; 1; 2; 3; 4; : : : ii
We define oe ' o/ to mean that "oe = "o/ , so oe ' o/ iff oe and o/ are equivalent
up to stuttering, and we define \Gamma oe to be the set fo/ : o/ ' oeg. If S is a set
of sequences, \Gamma (S) is the set fo/ : 9oe 2 S:o/ 2 \Gamma oeg. A set of sequences S is
closed under stuttering if S = \Gamma (S). Thus, S is closed under stuttering iff
for every pair of sequences oe, o/ with oe ' o/ , if oe 2 S then o/ 2 S.

We use "\Delta " to denote concatenation of sequences--that is, if koek = m,
then oe \Delta  o/ = hhs0; : : : ; sm\Gamma 1; t0; t1; : : :ii. If koek * m, we let oejm denote
hhs0; s1; : : : ; sm\Gamma 1ii, the prefix of oe of length m.

For any set \Sigma , let \Sigma ! denote the set of all infinite sequences of elements
in \Sigma . An infinite sequence hhoe0; oe1; oe2; : : :ii of sequences in \Sigma ! is said to
converge to the sequence oe in \Sigma ! iff for all m * 0 there exists an n * 0
such that oeijm = oejm for all i * n. In this case, we define lim oei to be oe.
This definition of convergence gives rise to a topology on \Sigma !. We now recall
some other definitions.

Let oe be an element of \Sigma ! and let S be a subset of \Sigma !. We say that oe is
a limit point of S iff there exist elements oei in S such that lim oei = oe. The
set S is closed iff S contains all its limit points. The closure of S, denoted
S, consists of all limit points of S; it is the smallest closed superset of S.

2.2 Properties
We can only say that one specification implements another if we are given
a correspondence between the externally visible states of the two specifications. For example, if S2 asserts that the initial value of a particular register
is the integer \Gamma 3 and S1 asserts that the register's initial value is the sequence of bits 1111100, then we can't say whether or not S1 implements
S2 without knowing how to interpret a sequence of bits as an integer. In
general, to decide if S1 implements S2, we must know how to interpret an
externally visible state of S1 as an externally visible state of S2. Given such
an interpretation, we can translate S1 into a specification with the same
set of externally visible states as S2. Thus, there is no loss of generality in
requiring that S1 and S2 have the same set of externally visible states.

We therefore assume that all specifications under consideration have the
same fixed set \Sigma E of externally visible states. A state space \Sigma  is a subset
of \Sigma E \Theta  \Sigma I for some set \Sigma I of internal states. We let \Pi E be the obvious

7

projection mapping from \Sigma E \Theta  \Sigma I onto \Sigma E. The set \Sigma E itself is considered
to be a state space for which \Pi E is the identity mapping.

If \Sigma  is a state space, then a \Sigma -behavior is an element of \Sigma !. A \Sigma Ebehavior is called an externally visible behavior. A \Sigma -property P is a set
of \Sigma -behaviors that is closed under stuttering. A \Sigma E-property is called
an externally visible property. If P is a \Sigma -property, then \Pi E(P ) is a set
of externally visible behaviors but is not necessarily an externally visible
property because it need not be closed under stuttering. The externally
visible property induced by a \Sigma -property P is defined to be the set \Gamma (\Pi E(P )).

If \Sigma  is clear from context or is irrelevant, we use the terms behavior
and property instead of \Sigma -behavior and \Sigma -property. We sometimes add the
adjective "complete", as in "complete behavior", to distinguish behaviors
and properties from externally visible behaviors and properties.

A property P that is closed (P = P ) is called a safety property. Intuitively, a safety property is one asserting that something bad does not
happen. To see that our formal definition of a safety property as a closed
set captures this intuitive meaning, observe that if something bad happens,
then it must happen within some finite period of time. Thus, P is a safety
property iff, for any sequence oe not in P , one can tell that oe is not in P by
looking at some finite prefix oeji of oe. In other words, oe 62 P iff there exists
an i such that for all o/ if o/ ji = oeji then o/ 62 P . Hence, oe 2 P iff for all i
there exists a o/i 2 P such that o/iji = oeji. But lim o/i = oe, which implies that
oe 2 P ; thus, oe 2 P iff oe 2 P . Therefore, P satisfies the intuitive definition
of a safety property only if P = P .

Even though we do not use the formal definition, it is interesting to note
that a \Sigma -property L can be defined to be a liveness property iff it is dense in
\Sigma !--in other words, if L = \Sigma !. This means that L is a liveness property iff
any finite sequence of elements in \Sigma  can be extended to a behavior in L. In
a topological space, every set can be written as the intersection of a closed
set and a dense set, so any property P can be written as M " L, where M
is a safety property and L is a liveness property. Moreover, M can be taken
to be P .

2.3 Specifications
A state machine is a triple (\Sigma ; F; N ) where

ffl \Sigma  is a state space. (Recall that this means \Sigma  ` \Sigma E \Theta  \Sigma I for some set

\Sigma I of internal states.)

8

ffl F , the set of initial states, is a subset of \Sigma .
ffl N , the next-state relation, is a subset of \Sigma  \Theta  \Sigma . (Elements of N are

denoted by pairs of states enclosed in angle brackets, like hs; ti.)

The (complete) property generated by a state machine (\Sigma ; F; N ) consists of
all infinite sequences hhs0; s1; : : :ii such that s0 2 F and, for all i * 0, either
hsi; si+1i 2 N or si = si+1. This set is closed under stuttering, so it is a
\Sigma -property. The externally visible property generated by a state machine is
the externally visible property induced by its complete property.

We now show that the complete property P generated by a state machine
is a safety property. This requires proving that if lim oei = oe and each oei 2 P ,
then oe 2 P . For any behavior o/ = hhs0; s1; : : :ii and any j * 0, let o/ j be
the terminating behavior hhs0; s1; : : : ; sj; sj; sj; : : :ii. Then o/ is in P iff each
o/ j is in P . Since lim oei = oe, each oej equals (oei)j for some i. Since each oei
is in P , each (oei)j is in P , which implies that oe is also in P . Hence, P is
closed, so the complete property generated by a state machine is a safety
property. However, we will show in Section 3 that the externally visible
property generated by a state machine need not be a safety property.

A state machine (\Sigma ; F; N ) is a familiar type of nondeterministic automaton, where F is the set of starting states and N describes the possible state
transitions. (However, remember that \Sigma  may be an infinite set.) The set of
sequences generated (or accepted) by such an automaton is usually defined
to be the set A of all sequences starting with a state in F and progressing by
making transitions allowed by N . However, we also allow stuttering transitions, so we have defined the property generated by the state machine to be
\Gamma (A) together with all terminating sequences obtained from finite prefixes
of behaviors in \Gamma (A) by infinite stuttering.

A specification S is a four-tuple (\Sigma ; F; N; L), where (\Sigma ; F; N ) is a state
machine and L is a \Sigma -property, called the supplementary property of the
specification. The property M generated by the state machine (\Sigma ; F; N )
is called the machine property of S. The (complete) property defined by
S is defined to be M " L, and the externally visible property defined by S
is defined to be \Gamma (\Pi E(M " L)), the externally visible property induced by
M " L.

State machines are easier to work with than arbitrary sets of sequences,
so one would like to specify a property purely in terms of state machines.
However, the complete property generated by a state machine is a safety
property. The supplementary property of a specification is needed to introduce liveness requirements. However, if we were to place no additional

9

requirement on our specifications, we could use the supplementary property
to do all the specifying. To see why this leads to trouble, let S2 be a specifi-
cation consisting of any arbitrary state machine that generates an externally
visible safety property O together with the trivial supplementary property
that contains all behaviors. Define S1 to be the specification with state space
\Sigma E whose state machine is the trivial one that generates all \Sigma E-behaviors
and whose supplementary property is O. Obviously S1 implements S2.
The existence of a refinement mapping from S1 to S2 implies that S1's
state machine implements S2's state machine. However, S1 has the trivial
state machine and no internal state. As we will see, auxiliary variables are
added to a specification's state machine without affecting or being affected
by the supplementary property. (This is what makes the addition of auxiliary variables practical.) No sound method of adding auxiliary variables
can transform the trivial machine into one that implements an arbitrary
state machine. Therefore, we need some constraint on the supplementary
property.

In practice, we specify a desired property P by writing P as the intersection M " L of a safety property M and a liveness property L. We try to
construct L so that it does not specify any safety property, meaning that
it does not rule out any finite behavior. More precisely, we try to choose L
to be a liveness property such that any finite sequence of states generated
by the state machine is the prefix of a behavior in P . For our results, it
is not necessary that L be a liveness property; we need only require that
L does not specify any safety property not implied by M . To express this
requirement formally, we say that a specification S having machine property
M and supplementary property L is machine closed iff M = M " L.

The following lemma implies that, for a machine-closed specification, we
can ignore the supplementary property and consider only the state machine
when we are interested in finite portions of behaviors.

Lemma 1 If M = P , then every prefix of a behavior in M is the prefix of
a behavior in P .

Proof of Lemma 1
Given: A1. M = P .

A2. oe 2 M .
A3. m * 0.
Prove: C1. There exists o/ 2 P such that o/ jm = oejm.
Pf : 1. Choose oei 2 P such that lim oei = oe.

10

Pf : A1, A2, and the definition of P .
2. Choose n * 0 such that, for all i * n, oeijm = oejm.

Pf : Definition of convergence.
3. C1 holds.

Pf : Let o/ be oen.
End Proof of Lemma 1

The converse of this lemma is also true when M is generated by a state
machine, but we will not need it.

2.4 Refinement Mappings
A specification S1 implements a specification S2 iff the externally visible
property induced by S1 is a subset of the externally visible property induced
by S2. In other words, S1 implements S2 iff every externally visible behavior
allowed by S1 is also allowed by S2.

A refinement mapping from a specification S1 = (\Sigma 1; F1; N1; L1) to a
specification S2 = (\Sigma 2; F2; N2; L2) is a mapping f : \Sigma 1 ! \Sigma 2 such that

R1. For all s 2 \Sigma 1: \Pi E(f (s)) = \Pi E(s). (f preserves the externally visible

state component.)

R2. f (F1) ` F2. (f takes initial states into initial states.)
R3. If hs; ti 2 N1 then hf (s); f (t)i 2 N2 or f (s) = f (t). (A state transition

allowed by N1 is mapped by f into a [possibly stuttering] transition
allowed by N2.)

R4. f (P1) ` L2, where P1 is the property defined by S1. (f maps behaviors allowed by S1 into behaviors that satisfy S2's supplementary
property.)

Conditions R1-R3 are local, meaning that they can be checked by reasoning
about states or pairs of states rather than about behaviors. Condition R4 is
not local, but checking it is simplified by the fact that f is not an arbitrary
mapping on sequences, but is obtained from a mapping on states. Thus, one
can apply local methods like well-founded induction to prove R4.

Proposition 1 If there exists a refinement mapping from S1 to S2, then
S1 implements S2.

11

Proof of Proposition 1.
Given: A1. Si = (\Sigma i; Fi; Ni; Li), for i = 1; 2.

A2. Mi is the machine property of Si, for i = 1; 2.
A3. f is a refinement mapping from S1 to S2.
A4. j 2 \Gamma (\Pi E(M1 " L1))
Prove: C1. j 2 \Gamma (\Pi E(M2 " L2)).
Pf : 1. f (M1) ` M2.

Given: A1.1. oe = hhs0; s1; : : :ii 2 M1.
Prove: C1.1. f (oe) 2 M2.
Pf : 1.1. f (s0) 2 F2.

Pf : By A1.1, A2, the definition of machine property (which permits stuttering), A3, and property R2 in the definition of
refinement mapping.
1.2. For all i * 0: hf (si); f (si+1)i 2 N2 or f (si) = f (si+1).

Pf : By A1.1, A2, the definition of machine property, A3, and

property R3.
1.3. C1.1 holds.

Pf : By 1.1, 1.2, the definition of f (oe) (it equals hhf (s0);

f (s1); : : :ii), A2, and the definition of machine property.
2. f (M1 " L1) ` M2 " L2.

Pf : By 1, A3, and R4, since g(S " T ) ` g(S) " g(T ) for any sets S

and T and any mapping g.
3. Choose oe = hhs0; s1; : : :ii 2 M1 " L1 such that \Pi E(oe) ' j.

Pf : Such a oe exists by A4 and the definition of \Gamma .
4. \Pi E(f (oe)) = \Pi E(oe).

Pf : By A3 and R1.
5. \Pi E(f (oe)) ' j.

Pf : By 3 and 4.
6. \Pi E(f (oe)) 2 \Pi E(M2 " L2).

Pf : By 3 and 2.
7. C1 holds.

Pf : By 5, 6, and the definition of \Gamma .
End Proof of Proposition 1.

3 Finite Invisible Nondeterminism
The machine property M of a specification is a safety property. However, the
property that is really being specified by the specification's state machine

12

is the externally visible property \Gamma (\Pi E(M )) induced by M . The following
example shows that this externally visible property is not necessarily a safety
property.

Let \Sigma E be the set N of natural numbers, and define the state machine
(\Sigma ; F; N ) by:

ffl \Sigma  equals \Sigma E \Theta  N.
ffl F equals f(0; 0)g.
ffl N is the union of the following two sets:

- fh(0; 0); (1; n)i : n 2 Ng,
- fh(m; n + 1); (m + 1; n)i : m; n 2 Ng.

A stutter-free behavior of this machine starts in state (0; 0), goes to state
(1; n) for some arbitrary n * 0, then goes through the sequence of states
(2; n\Gamma 1), (3; n\Gamma 2), : : : , (n\Gamma i+1; i) for some i * 0, and terminates (stutters
forever) in the state (n \Gamma  i + 1; i).

The set of externally visible behaviors induced by this state machine
consists of all sequences obtainable by stuttering from a sequence oen of
the form hh0; 1; 2; : : :; n; n; n; : : :ii. This set is not closed, because lim oen =
hh0; 1; 2; 3; : : :ii, and hh0; 1; 2; 3; : : :ii is not in the set. The externally visible
property specified by this state machine is the conjunction of two properties:

1. The set of all behaviors that start in state 0 and change state only by

adding 1 to the previous state.

2. The set of terminating behaviors.
The first property is a safety property, but the second is a liveness property;
their intersection is neither a safety nor a liveness property.

The purpose of a specification is to specify an externally visible property.
We feel that the externally visible property specified by a state machine
should be a safety property, so we want to restrict the class of allowed state
machines.

The reason the externally visible property defined by the state machine
in our example is not a safety property can be traced to the existence of
infinitely many state transitions h(0; 0); (1; n)i that correspond to the same
externally visible transition h0; 1i. It is this type of infinite invisible nondeterminism that allows the introduction of liveness into the externally visible

13

property of a state machine. To ensure that a state machine specifies only
safety properties, we must restrict it to having finite invisible nondeterminism.

Instead of defining the concept of finite invisible nondeterminism for a
state machine, it is more general to define it for a property. A state machine
is defined to have finite invisible nondeterminism iff the property it generates
does.

Definition 1 Let P be a property and O its induced externally visible property \Gamma (\Pi E(P )). We say that P is fin (for finitely invisibly nondeterministic)
iff for all j 2 O and all n * 0, the set

f"(oejm) : (m ? 0) ^ (oe 2 P ) ^ (\Pi E(oejm) ' jjn)g
is finite. We say that a specification is fin iff the complete property of the
specification is fin.

In other words, property P is fin iff every finite prefix jjn of any externally visible behavior j is the projection of only finitely many inequivalent
(under ') finite prefixes oejm of complete behaviors oe in P .

If a property M is fin then every stronger property P is also fin. (Property P is stronger than property M iff P ` M .) In our main theorem,
instead of requiring that the state machine of S2 is fin, we make the weaker
assumption that S2 is fin. This is strictly weaker only if S2 is not machine
closed, since a machine-closed specification is fin iff its state machine is fin.

The following proposition asserts that the externally visible property
of a fin state machine is a safety property. It is a simple corollary of the
subsequent lemma, which will be used later as well.

Proposition 2 If a safety property P is fin, then the externally visible property \Gamma (\Pi E(P )) that it induces is also a safety property.

Lemma 2 (Closure and nondeterminism) Let property P be fin and let
O be the externally visible property that it induces. If ffi is a limit point of O
then there is a limit point ae of P such that \Pi E(ae) ' ffi.

Proof of Lemma 2
Given: A1. P is fin.

A2. O = \Gamma (\Pi E(P )).
A3. ffi is a limit point of O.

14

Prove: C1. There exists ae such that:

C1a. ae is a limit point of P .
C1b. \Pi E(ae) ' ffi.
Pf : 1. Let \Theta n equal f"(oejm) : (m ? 0) ^ (oe 2 P ) ^ (\Pi E(oejm) ' ffijn)g. For

all n, the set \Theta n is finite. (\Theta n is the set of stutter-free prefixes of
behaviors in P that are externally equivalent to ffijn.)
Pf : By A3, we can choose j 2 O such that jjn = ffijn. Statement 1

then follows from A1 and Definition 1.
2. For all n, the set \Theta n is nonempty.

Pf : 2.1. Choose j 2 O such that jjn = ffijn.

Pf : A3 implies the existence of j.
2.2. Choose oe 2 P such that \Pi E(oe) ' j.

Pf : A2 and definition of \Gamma  imply the existence of oe.
2.3. There exists m such that \Pi E(oejm) ' jjn.

Pf : 2.2 and the definition of '.
2.4. "(oejm) 2 \Theta n, so \Theta n is nonempty.

Pf : oe 2 P (by 2.2), and \Pi E(oejm) ' ffijn (by 2.3 and 2.1), so 2.4

follows from 1 (the definition of \Theta n).
3. For finite sequences oe and o/ , let oe _ o/ iff there is a (possibly empty)

sequence O/ such that o/ = oe \Delta  O/. For all n and all ` 2 \Theta n+1 there
exists `

0 2 \Theta n such that `0 _ `.

Pf : By 1 (the definition of \Theta n), since if o/ jm ' ffijn+1, then there

exists m

0 ^ m such that o/ jm

0 ' ffijn.

4. There is an infinite sequence ae1 _ ae2 _ ae3 _ : : : with each aei 2 \Theta i.

Pf : By 1, 2, 3 and K"onig's Lemma [Knu73, pages 381-383].
5. For all i, choose ae

0i such that:

5a. ae

0i ' aei.

5b. kae

0ik * i.

5c. ae

01 _ ae02 _ ae03 _ : : : .

Pf : The existence of the ae

0i is proved by induction using 4, where

the length of ae

0i is increased by stuttering the last element when

necessary.
6. Let baei be an element of P such that ae

0i is a prefix of

baei.

Pf : Since aei 2 \Theta i (by 4), the definition of \Theta i (1) implies that there

exists a stutter-free sequence i 2 P such that aei is a prefix of i.
By 5a and the assumption that P (like all properties) is invariant
under stuttering, baei can be obtained by adding stuttering to i.
7. Let ae equal lim baei.

Pf : ae exists by 6 (ae

0i a prefix of

baei), 5b, and 5c.

15

8. C1a holds.

Pf : Follows immediately from 7, 6 ( baei 2 P ), and the definition of

limit point.
9. For every i there exists an m * i such that \Pi E( baeijm) ' ffiji.

Pf : 5a, 5b, 6, 4 (aei 2 \Theta i), and 1 (the definition of \Theta i).
10. lim \Pi E( baei) ' ffi.

Pf : Follows immediately from 9.
11. C1b holds.

Pf : By 6 ( baei 2 P ), 7, and 10, since lim ,i = , implies lim \Pi E(,i) =

\Pi E(,) .
End Proof of Lemma 2

For a state machine to be fin, it may not make an infinite nondeterministic choice unless all but a finite part of that choice is immediately revealed
in the externally visible state. We can weaken our definition by requiring
only that the choice eventually be revealed. Formally, this means defining a
property P with induced externally visible property O to be fin iff for every
j in O and n * 0 there exists an n

0 * n such that the set

f"(oejm) : (m ? 0) ^ (oe 2 P ) ^ (\Pi E(oejm) ' jjn)

^ 9m

0 : (\Pi E(oejm

0) ' jjn0)g

is finite. However, using this weaker definition of finite invisible nondeterminism would require somewhat more powerful prophecy variables and
would complicate our proofs, so we will stick with our original definition.

4 Safety Properties
Alpern and Schneider [AS87] and others have observed in the finite-state
case that there is a correspondence between state machines and externally
visible safety properties. We extend their results to the infinite-state case
for state machines with finite invisible nondeterminism. We also prove a
result that allows us to apply our completeness theorem to safety properties
even when the internal continuity hypothesis defined later is not satisfied.

Proposition 2 implies that the externally visible property generated by
a fin state machine is a safety property. We now prove the converse.

Proposition 3 Every externally visible safety property can be generated by
a state machine with finite invisible nondeterminism.

16

Proof of Proposition 3
Given: A1. O is a \Sigma E-property.

A2. O = O.
Prove: C1. There exists a state machine (\Sigma ; F; N ) generating a (complete)

property M such that

C1a. M is fin.
C1b. O ` \Gamma (\Pi E(M )).

C1c. \Gamma (\Pi E(M )) ` O.
Pf : 1. Define the state machine (\Sigma ; F; N ) as follows:

ffl \Sigma  = f(last(`jn); `jn) : n * 1 ^ ` 2 Og. (\Sigma  consists of all pairs

(ei; hhe0; e1; : : : ; eiii) such that hhe0; e1; : : : ; eiii is a prefix of a
sequence in O.)
ffl F = f(e; hheii) 2 \Sigma g. (The starting states are ones whose internal components have length one.)
ffl N = fh(e; h); (e

0; h \Delta  hhe0ii)i 2 \Sigma  \Theta  \Sigma g (The machine can go from

state (ei; hhe0; : : : ; eiii) only to state (ei+1; hhe0; : : :; ei; ei+1ii) for
some ei+1.)
2. A stutter-free sequence hh(e0; h0); (e1; h1); : : :ii is in M iff, for all i * 0,

hi = hhe0; e1; : : :; eiii and there exists ji 2 O such that hi = jiji+1.
Pf : Follows easily by induction from the definition of the state machine (\Sigma ; F; N ) and of the property that it generates.
3. C1a holds.

Pf : By Definition 1, we must show that for any j 2 O and all n * 0

the set

f"(oejm) : (m ? 0) ^ (oe 2 M ) ^ (\Pi E(oejm) ' jjn)g
is finite. However, it follows from 2 that if j = hhe0; e1; : : : ii then
this set contains only the single element

hh(e0; hhe0ii); (e1; hhe0; e1ii); : : :; (en\Gamma 1; hhe0; : : : ; en\Gamma 1ii)ii
4. C1b holds.

Pf : For any j = hhe0; e1; : : :ii in O, statement 2 implies that oe = hh: : : ;

(ei; jji+1); : : :ii is in M , and obviously \Pi E(oe) = j.
5. \Pi E(M ) ` O.

Given: A5.1. hh(e0; h0); (e1; h1); : : :ii 2 M .
Prove: C5.1. hhe0; e1; : : :ii 2 O.
Pf : 5.1. For all i * 0 choose ji 2 O such that jiji+1 = hhe0; : : : ; eiii.

Pf : By 2, the ji exist.
5.2. lim ji = hhe0; e1; : : :ii.

Pf : Follows immediately from 5.1.

17

5.3. C5.1 holds.

Pf : By 5.1 (which asserts that ji 2 O), 5.2, A2, and the definition of O.
6. C1c holds.

Pf : By 5 and the assumption that O is a property (A1), so \Gamma (O) = O.
End Proof of Proposition 3

If specification S2 is not internally continuous, it is possible for it to be
implemented by a specification S1 without there being a refinement mapping
from S1 to S2. (Internal continuity was mentioned in the introduction and
will be defined formally in Section 6.) However, since safety properties are
internally continuous, we would expect to be able to prove that, whenever S1
implements S2, the externally visible machine property of S1 implements
the externally visible machine property of S2. Combined with our main
theorem, the following result shows that this is always possible if S1 is
machine closed and the machine property of S2 is fin.

Theorem 1 (Separate safety proofs) Let P1 = M1 " L1 and P2 = M2 "
L2, where the Li are arbitrary properties and the Mi are safety properties;
and let Oi and OMi be the externally visible properties induced by Pi and Mi,
respectively. If M1 = P1, M2 is fin, and O1 ` O2, then OM1 ` OM2 .

Proof of Theorem 1
Given: A1. For i = 1; 2:

A1a. Pi = Mi " Li.
A1b. Mi closed.

A1c. Oi = \Gamma (\Pi E(Pi)).
A1d. OMi = \Gamma (\Pi E(Mi)).
A2. M1 = P1.
A3. M2 is fin.
A4. O1 ` O2.
Prove: C1. OM1 ` OM2 .
Pf : 1. For any set Q of behaviors \Gamma (Q) ` \Gamma (Q).

Given: A1.1. oe 2 \Gamma (Q).
Prove: C1.1. oe 2 \Gamma (Q).
Pf : 1.1. There exists oe

0 2 Q such that oe0 ' oe.

Pf : A1.1 and the definition of \Gamma .
1.2. There exists a function r such that, for all i * 0, oeji ' oe

0jr(i).

Pf : 1.1 and the definition of '.

18

1.3. For all i * 0 there exists o/

0i 2 Q such that o/ 0i jr(i) = oe0jr(i).

Pf : Definition of Q and 1.1.
1.4. oeji ' o/

0i jr(i).

Pf : 1.2 and 1.3.
1.5. For each i, let o/

0i = hhti;0; ti;1; : : :ii and define o/i to equal oeji \Delta 

hhti;r(i); ti;r(i)+1; : : :ii. Then o/i ' o/

0i.

Pf : 1.4.
1.6. o/i 2 \Gamma (Q).

Pf : o/i ' o/

0i (by 1.5), o/ 0i 2 Q (by 1.3), and the definition of \Gamma .

1.7. lim o/i = oe.

Pf : By 1.5 and the definition of convergence.
1.8. C1.1 holds.

Pf : 1.6, 1.7, and the definition of closure.
2. For any set Q of behaviors \Pi E(Q) ` \Pi E(Q).

Given: A2.1. j 2 \Pi E(Q).
Prove: C2.1. j 2 \Pi E(Q).
Pf : 2.1. There exists oe 2 Q such that j = \Pi E(oe).

Pf : A2.1.
2.2. For all i * 0 choose o/i in Q such that o/iji = oeji.

Pf : 2.1 and the definition of Q.
2.3. For all i * 0, \Pi E(o/i)ji = jji.

Pf : 2.1 and 2.2, since \Pi E(ji) = (\Pi E())ji for any sequence .
2.4. \Pi E(o/i) 2 \Pi E(Q).

Pf : By 2.2 (o/i 2 Q).
2.5. C2.1 holds.

Pf : By 2.3, which implies lim \Pi E(o/i) = j, and 2.4.
3. OM1 ` O1.

Pf : 3.1. OM1 = \Gamma (\Pi E(P1)).

Pf : A2 and A1d.
3.2. O1 = \Gamma (\Pi E(P1)).

Pf : A1c.
3.3. \Pi E(P1) ` \Pi E(P1).

Pf : 2.
3.4. OM1 ` \Gamma (\Pi E(P1))

Pf : 3.1, 3.3, and monotonicity of \Gamma .
3.5. \Gamma (\Pi EP1) ` \Gamma (\Pi E(P1)).

Pf : 1.
3.6. 3 holds.

19

Pf : 3.4, 3.5, and 3.2.
4. O1 ` O2.

Pf : A4 and monotonicity of the closure operation.
5. O2 ` OM2 .

Pf : A1a, A1c, A1d, and the monotonicity of \Pi E and \Gamma .
6. O2 ` OM2 .

Pf : 5 and monotonicity of closure.
7. OM2 = OM2 .

Pf : A1b, A1d, A3, and Proposition 2.
8. C1 holds.

Pf : 3, 4, 6, and 7.
End Proof of Theorem 1

5 Auxiliary Variables
Although in practice refinement mappings usually exist, they do not always
exist. To construct a refinement mapping, it may be necessary to add auxiliary variables. We now formally define two types of auxiliary variables: the
well-known history variable and the new prophecy variable. These auxiliary
variables are added to a specification's state machine; the supplementary
property is essentially left unchanged.

5.1 History Variables
Adding a history variable means augmenting the state space with an additional component \Sigma H and modifying the state machine in such a way
that this additional component records past information but does not affect the behavior of the original state components. Formally, a specification Sh = (\Sigma h; F h; N h; Lh) is said to be obtained from the specification
S = (\Sigma ; F; N; L) by adding a history variable iff the following five conditions are satisfied. In these conditions, we identify (\Sigma E \Theta  \Sigma I) \Theta  \Sigma H with
\Sigma E \Theta  (\Sigma I \Theta  \Sigma H) (so H1 implies that \Sigma h is a state space), and we let \Pi [H]
be the obvious projection mapping from \Sigma  \Theta  \Sigma H onto \Sigma . (In the intuitive
explanation, we say that a \Sigma h-behavior oe simulates the \Sigma -behavior \Pi [H](oe).)

H1. \Sigma h ` \Sigma  \Theta  \Sigma H for some set \Sigma H.
H2. \Pi [H](F h) = F . (A state in \Sigma  is an initial state of S iff it is the first

component of an initial state of Sh.)

20

H3. If h(s; h); (s

0; h0)i 2 N h then hs; s0i 2 N or s = s0. (Every step of

Sh's state machine simulates a [possibly stuttering] step of S's state
machine.)

H4. If hs; s

0i 2 N and (s; h) 2 \Sigma h then there exists h0 2 \Sigma H such that

h(s; h); (s

0; h0)i 2 N h. (From any state, Sh's state machine can simulate any possible step of S's state machine.)

H5. Lh = \Pi 

\Gamma 1

[H](L). (A \Sigma 

h-behavior is in Lh iff the \Sigma -behavior that it

simulates is in L.)

The following result shows that adding a history variable leaves an implementation essentially unchanged.

Proposition 4 (Soundness of history variables) If Sh is obtained from
S by adding a history variable, then the two specifications define the same
externally visible property.

Proof of Proposition 4
Given: A1. S = (\Sigma ; F; N; L), Sh = (\Sigma h; F h; N h; Lh), and H1-H5 hold.

A2. M and M h are the machine properties of S and Sh, respectively.
A3. P = M " L and P h = M h " Lh.
A4. O = \Gamma (\Pi E(P )) and Oh = \Gamma (\Pi E(P h)).
Prove: C1. Oh ` O.

C2. O ` Oh.
Pf : 1. \Pi [H](M h) ` M .

Pf : Follows from A2, A1 (conditions H2 and H3), and the definition

of the machine property of a specification.
2. \Pi [H](P h) ` P .

Pf : From A3, 1, and H5, since g(S"T ) ` g(S)"g(T ) for any function

g and sets S and T .
3. C1 holds.

Pf : From 2, A4, and the fact that \Pi E(\Pi [H](s)) = \Pi E(s) for any

s 2 \Sigma h.
4. P ` \Pi [H](P h).

Given: A4.1. oe = hhs0; s1; : : :ii in P .
Prove: C4.1. There exists o/ 2 P h such that \Pi [H](o/ ) = oe.
Pf : 4.1. s0 2 F and, for all i * 0, hsi; si+1i 2 N .

Pf : A3 and the definition of machine property.

21

4.2. For all i * 0 choose hi inductively such that (s0; h0) 2 F h and

h(si; hi); (si+1; hi+1)i 2 N h.
Pf : The existence of h0 follows from 4.1 (s0 2 F ) and H2; for

i * 0, the existence of hi+1 follows from 4.1 (hsi; si+1i 2 N )
and H4.
4.3. Let o/ = hh(s0; h0); (s1; h1); : : :ii. Then o/ 2 M h.

Pf : 4.2, A2, and the definition of machine property.
4.4. \Pi [H](o/ ) = oe.

Pf : By definition of o/ (4.3).
4.5. o/ 2 Lh.

Pf : 4.4, H5, and A4.1.
4.6. C4.1 holds.

Pf : 4.5, 4.3, and A3, which imply that o/ 2 P h, and 4.4.
5. C2 holds.

Pf : From 4, A4, the monotonicity of \Gamma  and \Pi E, and the fact that

\Pi E(\Pi [H](s)) = \Pi E(s) for any s 2 \Sigma h.
End Proof of Proposition 4

5.2 Simple Prophecy Variables
A prophecy variable is the dual of a history variable; its definition is almost
that of a history variable with time running backwards. Intuitively, whereas
a history variable records past behavior, a prophecy variable guesses future
behavior. Using notation similar to that used in defining history variables,
we define a specification Sp = (\Sigma p; F p; N p; Lp) to be obtained from S =
(\Sigma ; F; N; L) by adding a prophecy variable iff the following conditions are
satisfied. (Conditions P2

0 and P40 will be replaced in Section 5.3.)

P1. \Sigma p ` \Sigma  \Theta  \Sigma P for some set \Sigma P .
P2

0. F p = \Pi \Gamma 1

[P ](F ). (This is the expected correspondence between the

initial states of the two specifications.)

P3. If h(s; p); (s

0; p0)i 2 N p then hs; s0i 2 N or s = s0. (Every step of

Sp's state machine simulates a [possibly stuttering] step of S's state
machine.)

P4

0. If hs; s0i 2 N and (s0; p0) 2 \Sigma p then there exists p 2 \Sigma P such that

h(s; p); (s

0; p0)i 2 N p. (From every state in \Sigma p, the state machine of

Sp can take a backwards step that simulates any possible backwards

22

step of S's state machine. This is the time-reversed version of condition
H4.)

P5. Lp = \Pi 

\Gamma 1

[P ](L). (The supplementary property of S

p is the set of behaviors that simulate behaviors in the supplementary property of S.)

P6. For all s 2 \Sigma , the set \Pi 

\Gamma 1

[P ](s) is finite and nonempty. (To every state

of S there corresponds some nonzero finite number of states of Sp.)

Condition P6 is the only one not corresponding to any condition for history
variables. It is needed because time reversal is asymmetric--all behaviors
have initial states but only terminating behaviors have final states. The
second example below indicates why it is needed.

We now give two examples to illustrate the definition of prophecy variables. We mention only the state machines; the supplementary property can
be taken to be the trivial one containing all behaviors.

For our first example, we take a state machine that nondeterministically
generates an integer between 0 and 9. To do this, the machine counts up by
one until it either decides to stop or else reaches 9, at which point it stutters
forever. The set \Sigma E of externally visible states is the set N of natural
numbers, and the internal state component is a Boolean that becomes true
when the final value is reached. (The Boolean values are written t and f .)

ffl \Sigma  = N \Theta  ft; fg.
ffl F = f(0; f)g.
ffl N is the union of the following two sets:

- fh(i \Gamma  1; f); (i; f)i : 0 ! i ! 10g,
- fh(i; f); (i; t)i : i 2 Ng.

The set of stutter-free behaviors generated by this state machine consists of
all sequences of the forms

hh(0; f); (1; f); : : :; (n; f); (n; t); (n; t); (n; t); : : :ii
and

hh(0; f); (1; f); : : : ; (n; f); (n; f); (n; f); : : :ii

with 0 ^ n ! 10.

We now add a prophecy variable whose value is a natural number. This
variable "predicts" the maximum number of nonstuttering steps that the
state machine will take. The precise definition of the new state machine is:

23

ffl \Sigma p is the union of the following two sets:

- f(i; f; j) : 0 ^ i; 0 ^ j; and i + j ! 10g,
- f(i; t; 0) : 0 ^ i ! 10g.

ffl F p = f(0; f; j) 2 \Sigma pg.
ffl N p is the union of the following two sets:

- fh(i \Gamma  1; f; j + 1); (i; f; j)i 2 \Sigma p \Theta  \Sigma pg,
- fh(i; f; 0); (i; t; 0)i 2 \Sigma p \Theta  \Sigma pg.

The reader can check that the conditions P1-P4

0 and P6 given above are

satisfied. (Condition P5 is satisfied if L and Lp are the trivial properties
that contain all behaviors.) Observe that although condition P4

0 is satisfied,

condition H4 is not. The state machine can take a backwards step from the
state (6; f; 0) but not a forward step.

The only stutter-free behaviors of (\Sigma p; F p; N p) starting from the state
(0; f; n) are of the forms

hh(0; f; n); (1; f; n \Gamma  1); : : :; (n; f; 0); (n; t; 0); (n; t; 0); : : :ii
and

hh(0; f; n); (1; f; n \Gamma  1); : : :; (i; f; n \Gamma  i); (i; f; n \Gamma  i); : : :ii

with 0 ^ i ^ n. The set of externally visible behaviors generated by the
two state machines is the same; the stutter-free behaviors have the form
hh0; 1; : : :; n; n; n; : : :ii for some n less than 10. State machine (\Sigma ; F; N ) decides nondeterministically when it is going to stop counting, while in state
machine (\Sigma p; F p; N p) this choice is made by the initial value of the prophecy
variable.

As our second example, replace "10" by "1" in the definitions of the
two state machines. Conditions P1-P4

0 still hold, but P6 does not; for each

state (i; f) of \Sigma  there are an infinite number of states (i; f; j) in \Sigma p. The
externally visible stutter-free behaviors of (\Sigma p; F p; N p) consist of sequences
of the form hh0; 1; : : : ; n; n; n; : : :ii for any natural number n. The state machine (\Sigma ; F; N ) generates all these behaviors plus the additional behavior
hh0; 1; 2; 3; : : :ii that never terminates. Because the finiteness condition P6
is not satisfied, adding the auxiliary variable changed the specification by
ruling out this nonterminating behavior--effectively adding a liveness condition.

24

We can use our last example to indicate why we need the hypothesis
of finite invisible nondeterminism for our completeness theorem. Let S2
be the specification consisting of the state machine (\Sigma p; F p; N p) we just
constructed (the one with "10" replaced by "1") and the trivial supplementary property containing all \Sigma p-behaviors. Let S1 be the specification
with state machine (\Sigma ; F; N ) and supplementary property L consisting of
all terminating behaviors. Both specifications define the same set of externally visible behaviors--all behaviors obtainable by stuttering from ones of
the form hh0; 1; : : : ; n; n; nii. To construct a refinement mapping, we would
have to add to S1 a prophecy variable that "guesses" the value of the last
component of a state of \Sigma p. However, no such prophecy variable can be
constructed that satisfies P6, since for any starting state of S1 there are an
infinite number of corresponding starting states of S2.

The complete property P2 defined by this specification S2 is a safety
property, and we will see that this implies that S2 is internally continuous.
Moreover, specification S1 is machine closed. Nevertheless, adding auxiliary
variables to S1 will not allow us to construct a refinement mapping to prove
that it implements S2. Our completeness theorem does not apply because
P2 is not fin.

In this example, the prophecy variable we wanted to add would not
satisfy P6. However, the supplementary property happened to ensure that
adding the prophecy variable did not change the externally visible behavior.
If we were to replace P6 by the weaker requirement that Sp have the same
externally visible property as S, then we could find a refinement mapping.
However, this requirement is precisely what we had to prove in the first
place--namely, that S1 implements S2.

5.3 Prophecy Variables That Add Stuttering
We now generalize our definition of a prophecy variable to allow it to introduce stuttering. Condition P2

0 asserts that a state (s; p) 2 \Sigma p is an initial

state of Sp's state machine iff s is an initial state of S's state machine. We
relax this condition by requiring only that such a state (s; p) be reachable
from an initial state by steps that simulate stuttering steps. Formally, we
replace P2

0 by:

P2. (a) \Pi [P ](F p) ` F .

(b) For all (s; p) 2 \Pi 

\Gamma 1

[P ](F ) there exist p0; p1; : : : ; pn = p such that

(s; p0) 2 F p and, for 0 ^ i ! n, h(s; pi); (s; pi+1)i 2 N p.

25

Similarly, we relax condition P4

0 by allowing Sp's state machine to simulate the step in S's state machine from state s to state s

0 by a sequence

of n + 1 steps, the last n of which simulate stuttering steps. The precise
condition that replaces P4

0 is:

P4. If hs; s

0i 2 N and (s0; p0) 2 \Sigma p then there exist p, p00, : : : , p0n

\Gamma 1, p

0n = p0

such that h(s; p); (s

0; p00)i 2 N p and, for 0 ^ i ! n, h(s0; p0i); (s0; p0i+1)i 2

N p.

As with history variables, the addition of prophecy variables leaves an
implementation essentially unchanged.

Proposition 5 (Soundness of prophecy variables) If Sp is obtained
from S by adding a prophecy variable, then the two specifications define
the same externally visible property.

Proof of Proposition 5
Given: A1. S = (\Sigma ; F; N; L), Sp = (\Sigma p; F p; N p; Lp), and P1-P6 hold.

A2. M and M p are the machine properties of S and Sp, respectively.
A3. P = M " L and P p = M p " Lp.
A4. O = \Gamma (\Pi E(P )) and Op = \Gamma (\Pi E(P p)).
Prove: C1. Op ` O.

C2. O ` Op.
Pf : 1. C1 holds.

Pf : The proof is identical to the proof of the corresponding condition

for history variables in Proposition 4.
2. P ` \Pi [P ](P p).

Given: A2.1. oe = hhs0; s1; : : :ii 2 P .
Prove: C2.1. There exists o/ 2 P p such that \Pi [P ](o/ ) ' oe.
Pf : 2.1. Let G be the directed graph with

Nodes: the set \Sigma p \Theta  N.
Edges: there is an edge between ((si; p); i) and ((sj; p

0); j) iff

j = i + 1 and either (si; p) = (si+1; p

0) or there exist

p0; p1; : : : ; pn = p

0 in \Sigma P such that h(si; p); (si+1; p0)i 2 N p

and, for all 0 ^ k ! n, h(si+1; pk); (si+1; pk+1)i 2 N p.
Let G

0 be the subgraph of G reachable from nodes of the form

((s0; p); 0). Then G

0 is acyclic, with finite branching and a finite

set of sources.
Pf : It is obviously acyclic, since there is an edge from ((s; p); i)

to ((s

0; p0); i0) only if i0 = i + 1. Its sources are all the nodes

26

of the form ((s0; p); 0). For each j, P6 implies that there are
only a finite set of p such that (sj ; p) 2 \Sigma p, so G

0 has a finite

set of sources and is finitely branching.
2.2. For all n * 0 and all (sn; pn) 2 \Sigma p there exist elements p0, : : : ,

pn\Gamma 1 in \Sigma P such that hh((s0; p0); 0); : : :; ((sn; pn); n)ii is a path in
G

0.

Pf : The proof is by induction on n. The case n = 0 is trivial.

For n ? 0, condition P4 implies the existence of the required
pn\Gamma 1, and the induction hypothesis provides p0, : : : , pn\Gamma 2.
2.3. Choose elements pi 2 \Sigma P such that hh((s0; p0); 0); ((s1; p1); 1);

: : :ii is an infinite path in G

0.

Pf : The existence of this path follows from 2.1, 2.2, and K"onig's

Lemma.
2.4. Let ae = hh(s0; p0); : : :; (si; pi); : : :ii. Choose a sequence ae

0 =

hh(s

00; p00); : : : ; (s0i; p0i); : : :ii such that:

2.4a. \Pi [P ](ae

0) ' oe.

2.4b. For all i * 0: h(s

0i; p0i); (s0i+1; p0i+1)i 2 N p or (s0i; p0i) =

(s

0i+1; p0i+1).

2.4c. (s

00; p00) = (s0; p0).

Pf : Let ae

0 be the supersequence of ae obtained by inserting

between (si; pi) and (si+1; pi+1) the sequence hh(si+1; p

00);

(si+1; p

01); : : : ; (si+1; p0

k\Gamma 1)ii of elements in \Sigma 

p whose existence is guaranteed by 2.3 and the definition of edges in
G

0 (2.1). (Recall that oe = hhs0; s1; : : :ii.)

2.5. Choose o/ = hh(t0; q0); (t1; q1); : : :ii such that:

2.5a. \Pi [P ](o/ ) ' oe.
2.5b. For all i * 0: h(ti; qi); (ti+1; qi+1)i 2 N p or (ti; qi) =

(ti+1; qi+1).
2.5c. (t0; q0) 2 F p.
Pf : By A2.1, we have s0 2 F . By P2, there exists a finite

sequence hh(s0; p

000); : : :; (s0; p00n)ii of elements in \Sigma p such that

(s0; p

000) 2 F p, each h(s0; p00i ); (s0; p00i+1)i 2 N p, and p00n = p0.

Let o/ = hh(s0; p

000); : : : ; (s0; p00n

\Gamma 1)ii \Delta  ae

0.

2.6. o/ 2 M p.

Pf : By A2, 2.5b, and 2.5c.
2.7. o/ 2 P p.

Pf : By A3, 2.6, and P5.
2.8. C2.1 holds.

Pf : 2.7, 2.5a.

27

3. C2 holds.

Pf : From 2, A1-A4, and the fact that \Pi E(\Pi [P ](t)) = \Pi E(t) for any

t 2 \Sigma p.
End Proof of Proposition 5

6 Internal Continuity
We now define internal continuity, which appears in the third hypothesis of
our main theorem. But first, we give an example that indicates why the
hypothesis is needed for our completeness theorem.

Let \Sigma E = N, let ji be the terminating sequence hh0; 1; : : :; i; i; i; : : :ii,
and let j be the nonterminating sequence hh0; 1; 2; : : :ii. Let hhe0; e1; : : :ii \Theta  x
denote the sequence hh(e0; x); (e1; x); : : :ii. We construct a specification S2
that defines the property whose stutter-free sequences consist of all sequences
ji \Theta  t together with the sequence j \Theta  f . Formally, S2 = (\Sigma 2; F2; N2; L2),
where

ffl \Sigma 2 = N \Theta  ft; f g. (The internal component is a Boolean.)
ffl F2 = f(0; t); (0; f)g. (Behaviors start with their visible components

equal to 0.)

ffl N2 = fh(i; b); (i + 1; b)ig. (The external component is incremented by

1 and the internal component remains constant.)

ffl L2 consists of all behaviors except ones of the form oe \Theta  f with oe

terminating, and oe \Theta  t with oe nonterminating.

The externally visible property O2 defined by S2 consists of the behaviors
ji, the behavior j, and all behaviors obtained from them by stuttering.
Specification S2 is fin and machine closed.

The externally visible property O2 is also defined by the simpler specifi-
cation S1 = (\Sigma 1; F1; N1; L1), where

ffl \Sigma 1 = \Sigma E = N. (There is no internal component.)
ffl F1 = f0g. (All behaviors start at 0.)
ffl N1 = fhi; i + 1ig. (The state is incremented by 1.)
ffl L1 = \Sigma !1 (the trivial property that allows all behaviors).

28

Obviously, S1 implements S2. Let Sp1 = (\Sigma p1; F p1 ; N p1 ; Lp1) be any specification obtained from S1 by adding a prophecy variable. We now show that
there does not exist a refinement mapping from Sp1 to S2; in fact there does
not exist any mapping from \Sigma p1 to \Sigma 2 that proves that Sp1 implements S2.

Let P p1 be the property defined by Sp1 . We show by contradiction that
there does not exist any mapping f : \Sigma p1 ! \Sigma 2 such that (i) \Pi E(f (i; p)) = i
and (ii) f (P p1 ) ` P2. For each i let j

0i 2 P p1 be a behavior with \Pi [P ](j0i) ' ji.

Moreover, P5 implies that we can choose j

0i to have no repeated nonfinal

states, meaning that for j ! i and k ? 1, there is no segment hh(j; p1); (j; p2);
: : : ; (j; pk)ii of j

0i with p1 = pk. By (i), we then have that for every i and

m with i ! m there is an l such that \Pi E(j

0mjl) ' jiji+1. Moreover, P6

and the absence of repeated nonfinal states imply that for each i there is an
integer ss(i) ? i such that l ^ ss(i) for all such m. We can choose ss so that
ss(i + 1) * ss(i) for all i.

For any n, the set fj

0jjss(n)g is finite (by P6). Therefore, we can inductively construct the sequence `n of length ss(n) such that `n is a prefix of
infinitely many of the j

0j and is also a prefix of `n+1. Let j0 = lim `n; then

\Pi E(j

0) ' j. Since each `n is a prefix of some j0j, clearly j0 is in the machine

property of Sp1 . Property P5 then implies that j

0 2 P p1 . By definition of j0i,

assumption (ii) implies that f (j

0i) ' ji \Theta  t, which implies that f (j0) ' j \Theta  t.

We then have j

0 2 P p1 and f (j0) 62 P2, which contradicts assumption (ii).

This proof can be extended to the case where S1 is replaced by any
specification Sh1 obtained from it by adding a history variable. We just
replace j with any behavior allowed by Sh1 that simulates it, and replace ji
with an initial prefix of this new j. Thus, first adding a history variable still
does not allow one to construct the refinement mapping.

The problem with specification S2 is that j \Theta  t is not in P2 even though
\Pi E(j \Theta  t) is in O2 and any finite portion of j \Theta  t is the same as the corresponding portion of some behavior ji \Theta  t in P2. The sequence j \Theta  t is
not in P2 even though we cannot tell that it isn't by looking either at its
externally visible component or at any finite part of the complete behavior.
To rule out this possibility, we must add to our completeness theorem the
hypothesis that P2 is internally continuous.

Definition 2 A \Sigma -property P with induced externally visible property O
is internally continuous iff, for any \Sigma -behavior oe, if \Pi E(oe) 2 O and oe 2
P , then oe 2 P . A specification is internally continuous iff the (complete)
property it defines is internally continuous.

29

Suppose P = M " L and M = P . Then lim oei = oe for oei 2 P iff
oe 2 M . It follows from this that, for a machine-closed specification, internal
continuity is equivalent to the condition that a complete behavior is allowed
iff it is generated by the state machine and its externally visible component
is allowed. In particular, safety properties are internally continuous.

Since the machine property M is closed, if lim oei = oe for oei 2 M " L,
then oe 2 L iff oe 2 M " L. This implies that if L is internally continuous,
then M " L is internally continuous. Hence, for any specification, if the
supplementary property is internally continuous, then the specification is
internally continuous. The converse is not true, since if M is the empty
property, then M " L is internally continuous for any L.

Any specification can be made internally continuous by adding to L all
sequences oe in M such that \Pi E(oe) 2 O. Expanding L in this way obviously adds no new externally visible behaviors, so the resulting specification
is equivalent to the original one. The expansion could introduce infinite
internal nondeterminism, but not if M is fin.

7 The Completeness Theorem
We can now prove our main result.
Theorem 2 (Completeness) If the machine-closed specification S1 implements the internally continuous, fin specification S2, then there is a specification Sh1 obtained from S1 by adding a history variable and a specification

Shp1 obtained from Sh1 by adding a prophecy variable such that there exists
a refinement mapping from Shp1 to S2.

Proof of Theorem 2
Given: A1. For i = 1; 2: Si = (\Sigma i; Fi; Ni; Li), Mi is the machine property

of Si, Pi = Mi " Li, and Oi = \Gamma (\Pi E(Pi)).
A2. O1 ` O2.
A3. S1 is machine closed.
A4. S2 is fin.
A5. S2 is internally continuous.
Prove: C1. There exist specifications Sh1 and Shp1 such that:

C1a. Sh1 is obtained from S1 by adding a history variable.

C1b. Shp1 is obtained from Sh1 by adding a prophecy variable.

C1c. There exists a refinement mapping f from Shp1 to S2.

30

Pf : 1. Let Sh1 equal (\Sigma h1; F h1 ; N h1 ; Lh1), where

ffl \Sigma h1 = f(last(oejn); oejn) : n ? 0 and oe 2 P1g. (The history

component h of any state (s; h) is a finite prefix of a behavior
in P1 that ends in state s.)
ffl F h1 = f(s; h) 2 \Sigma h1 : khk = 1g.
ffl N h1 = fh(s; h); (s

0; h0)i 2 \Sigma h1 \Theta  \Sigma h1 : h0 = h \Delta  hhs0iig. (A step of

Sh1's state machine simulates a step of S1's state machine and
adds the new state to the history component.)
ffl Lh1 = \Pi 

\Gamma 1

[H](L1). (As required by H5.)Then C1a holds.

Pf : 1.1. H1, H3, and H5 hold.

Pf : Follows immediately from the definition of Sh1 .
1.2. \Pi [H](F h1 ) ` F1.

Pf : Immediate from the definition of F h1 .
1.3. F1 ` \Pi [H](F h1 )

Pf : For any s 2 F1, the sequence hhs; s; s; : : :ii 2 M1. Therefore,

A3 and Lemma 1 imply that hhsii is a prefix of a behavior
in P1, so (s; hhsii) 2 F h1 and s = \Pi [H]((s; hhsii)).
1.4. H2 holds.

Pf : 1.2 and 1.3.
1.5. H4 holds.

Pf : For any hs; s

0i 2 N1 and (s; h) 2 \Sigma h1, let h0 = h \Delta  hhs0ii. Then

A3 and Lemma 1 imply that h

0 is the prefix of a behavior in

P1, so (s

0; h0) 2 \Sigma h1 by definition of \Sigma h1, and h(s; h); (s0; h0)i 2

N h1 by definition of N h1 .
2. Let Shp1 equal (\Sigma hp1 ; F hp1 ; N hp1 ; Lhp1 ), where

ffl \Sigma hp1 equals the set of triples (s; h; "(oejm)) with (s; h) 2 \Sigma h1,

oe 2 P2, m ? 0, and \Pi E(oejm) ' \Pi E(h), where we write (s; h; p)
instead of ((s; h); p). (The prophecy component p of (s; h; p) is
an initial stutter-free prefix of a behavior in P2 such that p and
h are externally equivalent.)
ffl F hp1 = f(s; h; p) 2 \Sigma hp : (s; h) 2 F h1 and kpk = 1g. (Note that

this implies s 2 F1 and p = hhtii with t 2 F2.)
ffl N hp1 is the set of pairs h(s; h; p); (s

0; h0; p0)i in \Sigma hp1 \Theta  \Sigma hp1 such

that either

(a) p

0 = p \Delta  hhlast(p0)ii and either h(s; h); (s0; h0)i 2 N h1 or

(s; h) = (s

0; h0), or

31

(b) p

0 = p and h(s; h); (s0; h0)i 2 N h1 .

(A step of Shp1 's state machine either increases the length of
the prophecy component by one and simulates a [possibly stuttering] step of Sh1 's state machine, or else leaves the prophecy
component unchanged and simulates a nonstuttering step of
Sh1's state machine.)
ffl Lh1 = \Pi 

\Gamma 1

[H](L1). (As required by P5.)Then C1b holds.

Pf : 2.1. P1, P3, and P5 hold.

Pf : Immediate from the definition of Shp1 .
2.2. \Pi [P ](F hp1 ) ` F h1 .

Pf : Immediate from the definitions of F hp1 and F h1 .
2.3. For all (s; h; p) 2 \Pi 

\Gamma 1

[P ](F

h1 ) there exist p0; p1; : : : ; pn = p such that

(s; h; p0) 2 F hp1 and, for 0 ^ i ! n, h(s; h; pi); (s; h; pi+1)i 2 N hp1 .
Pf : 2.3.1. Let (s; h; p) 2 \Pi 

\Gamma 1

[P ](F

h1 ), and let p = hht0; t1; : : : ; tnii. Then

h = hhsii and \Pi E(p) ' \Pi E(hhsii).
Pf : By definitions of F h1 and \Sigma hp1 .
2.3.2. Let pi = hht0; : : :; tiii. Then \Pi E(pi) ' \Pi E(h).

Pf : By 2.3.1.
2.3.3. (s; h; p0) 2 F hp1 and h(s; h; pi); (s; h; pi+1)i 2 N hp1 for 0 ^

i ! n.
Pf : By 2.3.2 and the definitions of F hp1 and N hp1 .
2.4. P2 holds.

Pf : By 2.2 and 2.3.
2.5. P4 holds.

Given: A2.5.1. h(s; h); (s

0; h0)i 2 N h1 and (s0; h0; p0) 2 \Sigma hp1 .

Prove: C2.5.1. There exist p, p

00, : : : , p0n

\Gamma 1, p

0n = p0 in \Sigma P such

that h(s; h; p); (s

0; h0; p00)i 2 N hp1 and, for 0 ^ i ! n,

h(s

0; h0; p0i); (s0; h0; p0i+1)i 2 N hp1 .

Pf : 2.5.1. p

0 = "(oejm) for some oe 2 P2, and \Pi E(p0) ' \Pi E(h0).

Pf : By A2.5.1 and the definition of \Sigma hp1 .
2.5.2. h

0 = h \Delta  hhs0ii.

Pf : By A2.5.1 (h(s; h); (s

0; h0)i 2 N h1 ) and the definition

of N h1 .
2.5.3. Let p be the longest prefix of p

0 such that \Pi E(p) ' \Pi E(h).

Pf : The existence of p follows from 2.5.1 and 2.5.2.
2.5.4. p

0 = p \Delta  hht0; : : : ; tnii where \Pi E(ti) ' \Pi E(s0) for 0 ^ i ^ n.

32

Pf : By 2.5.3, 2.5.1, and 2.5.2.
2.5.5. Let p

0i = p \Delta  hht0; : : : ; tiii. Then (s0; h0; p0i) 2 \Sigma hp1 for 0 ^ i ^

n.
Pf : By 2.5.4 and 2.5.1, we have \Pi E(p

0i) ' \Pi E(h0). The

result then follows from the definition of \Sigma hp1 , since
A2.5.1 implies (s

0; h0) 2 \Sigma h1.

2.5.6. C2.5.1 holds.

Pf : Follows easily from 2.5.5, 2.5.1, and the definition of

N hp1 .
2.6. P6 holds.

Given: A2.6.1. (s; h) 2 \Sigma h1.
Prove: C2.6.1. fp : (s; h; p) 2 \Sigma hp1 g is finite.

C2.6.2. There exists p 2 \Sigma P such that (s; h; p) 2 \Sigma hp1 .
Pf : 2.6.1. Choose  2 P1 such that h = jn, and let j = \Pi E().

Pf :  exists by A2.6.1 and the definition of \Sigma h1.
2.6.2. C2.6.1 holds.

Pf : By definition of \Sigma hp1 and j (in 2.6.1), A4, and Definition 1.
2.6.3. Choose oe 2 P2 such that \Pi E(oe) ' j.

Pf : Such a oe exists since j 2 O1 (by 2.6.1) and O1 ` O2

(by A2).
2.6.4. C2.6.2 holds.

Pf : By 2.6.3 and the definition of j (in 2.6.1), we can

choose m such that \Pi E(oejm) ' \Pi E(h). Let p =
"(oejm). The definition of \Sigma hp1 implies that (s; h; p) 2
\Sigma hp1 .
3. Define f : \Sigma hp1 ! \Sigma 2 by f ((s; h; p)) = last(p). Then f is a refinement

mapping.
Pf : 3.1. f satisfies R1.

Pf : By definition of \Sigma hp1 , if (s; h; p) 2 \Sigma hp1 then (s; h) 2 \Sigma h1

and \Pi E(p) ' \Pi E(h). But (s; h) 2 \Sigma h1 implies s = last(h)
(by definition of \Sigma h1), so \Pi E(p) ' \Pi E(h) implies \Pi E(s) =
\Pi E(last(p)).
3.2. f satisfies R2.

Pf : By definition of F hp1 , its elements are of the form (s;

hhsii; hhtii) where t 2 F2, so f ((s; hhsii; hhtii)) = t 2 F2.
3.3. f satisfies R3.

Given: A3.3.1. h(s; h; p); (s

0; h0; p0)i 2 N hp1 .

33

Prove: C3.3.1. hlast(p); last(p

0)i 2 N2 or last(p) = last(p0).

Pf : By definition of N hp1 , A3.3.1 implies p

0 = hht0; : : :; tnii for

some infinite sequence hht0; t1; : : :ii 2 P2, and either p = p

0,

in which case C3.3.1 is immediate, or p = hht0; : : : ; tn\Gamma 1ii.
In the latter case, we must prove htn\Gamma 1; tni 2 N2. However,
this follows immediately from the fact that hht0; t1; : : :ii 2
P2 ` M2 and the definition of the machine property of a
specification.
3.4. f satisfies R4.

Given: A3.4.1. o/ = hh(s0; h0; p0); (s1; h1; p1); : : :ii 2 P hp1 .
Prove: C3.4.1. f (o/ ) = hhlast(p0); last(p1); : : :ii 2 L2.
Pf : 3.4.1. Let oe = hhs0; s1; : : :ii. Then \Pi E(oe) = \Pi E(o/ ).

Pf : Follows immediately from R1 (by 3.1).
3.4.2. \Pi E(oe) 2 O1.

Pf : C1a (proved in 1), C1b (proved in 2), and Propositions 4 and 5 imply that \Pi E(o/ ) 2 O1, so 3.4.2 follows
from 3.4.1.
3.4.3. For all n * 0, f (o/ )jn ' pn.

Pf : By A3.4.1, h(si; hi; pi); (si+1; hi+1; pi+1)i 2 N hp1 or

(si; hi; pi) = (si+1; hi+1; pi+1) for all i * 0. By definition of N hp1 , this implies pi+1 = pi or pi+1 =
pi \Delta  hhlast(pi)ii for all i. A simple induction proof then
shows that pn ' hhlast(p0); : : :; last(pn)ii.
3.4.4. For all n * 0 there exists n 2 P2 such that njn = f (o/ )jn.

Pf : By definition of \Sigma hp1 , there exists a sequence OEn such

that pn \Delta  OEn 2 P2. Let n = f (o/ )jn \Delta  OEn. By 3.4.3,
n ' pn \Delta  OEn, so n is in P2.
3.4.5. C3.4.1 holds.

Pf : 3.4.4 implies that lim n = f (o/ ) and n 2 P2. By

3.4.1, 3.4.2, R1 (proved in 3.1), and A2, we have
\Pi E(f (o/ )) 2 O2. Since S2 is internally continuous
(by A5) and the n are in P2 (by 3.4.4), Definition 2
implies that f (o/ ) 2 P2. This proves C3.4.1, since
P2 ` L2 (by A1).
End Proof of Theorem 2

The converse of this completeness theorem is not true. For instance, no
matter how pathological a specification is, we can use the identity refinement
mapping to prove that it implements itself.

34

The hypotheses of the internal continuity and finite invisible nondeterminism of S2 can be removed from our completeness theorem by generalizing the definition of a prophecy variable--namely, by replacing condition
P6 with the explicit requirement that the externally visible behaviors of Sp
be the same as those of S. This result is proved by defining Sh1 as in the

proof of Theorem 2, and defining Shp1 such that

ffl \Sigma hp1 is the set of 4-tuples (s; h; n; o/ ) with (s; h) 2 \Sigma h1 , o/ 2 P2, and

\Pi E(h) ' \Pi E(o/ jn).

ffl F hp1 is the set of all states of the form (s; h; 1; o/ ).
ffl N hp1 is the set of pairs h(s; h; n; o/ ); (s

0; h0; n + 1; o/ )i with either h(s; h);

(s

0; h0)i 2 N h1 or (s; h) = (s0; h0).

ffl The refinement mapping is defined by letting f ((s; h; n; o/ )) be the nth

element of o/ .

However, the condition that replaces P6 asserts that specification Sp implements S, which is precisely the type of condition we are trying to prove
in the first place. This generalization of Theorem 2 is therefore of little
practical value, so we will not bother to state it and prove it formally.

There is one simple way to strengthen the completeness theorem that
is of some interest. The specification S2 is fin and internally continuous
iff the property P2 that it defines is fin and internally continuous. We can
weaken the hypothesis by requiring only that there exist a fin and internally
continuous property P

02 contained in P2 that induces the same externallyvisible property as P2. Let S

0

2 be the specification obtained from S2 byreplacing

L2 with L2 " P

02. The correctness of this result follows easily from

Theorem 2 by replacing S2 with S

02.

8 Whence and Whither?
Refinement mappings are not new. They form the basis of the methods
advocated by Lam and Shankar [LS84] and by us [Lam83], and they are
used by Lynch and Tuttle [LT87] to prove that one automaton implements
another. However, none of this work addresses the issue of completeness.
Jonsson [Jon87] and Stark [Sta88] did prove completeness results similar to
ours, but for smaller classes of specifications.

35

Complete methods for checking that a program implements a specification, without constructing refinement mappings, have been developed. Some
of the most general are those of Alpern and Schneider [AS87], Manna and
Pnueli [MP87], and Vardi [Var87]. Their methods differ from our approach
in at least two important ways:

ffl They do not consider behaviors with different amounts of "stuttering" to be equivalent, so their definition of what constitutes a correct
implementation is weaker than ours.

ffl They require constructing the negation of specifications. In practice,

the negation of a specification may be hard to find and hard to understand.

Because of these differences, the methods may not offer practical alternatives
to the use of refinement mappings for proving correctness.

Our exposition has been purely semantic. We have considered specifi-
cations, but not the languages in which they are expressed. We proved the
existence of refinement mappings, but said nothing about whether they are
expressible in any language. We do not know what languages can describe
the necessary auxiliary variables and resulting refinement mappings.

Our results also raise the question of what properties can be described
by specifications that are fin and internally continuous. If the specification language is expressive enough, then all properties can be defined by
specifications without internal state, which are trivially fin and internally
continuous. At the other extreme, one can easily invent artificially impoverished languages that do not allow any fin or internally continuous specifi-
cations. The question becomes interesting only for interesting specification
languages, such as various forms of temporal logic. In addition, recall that
the hypotheses of our completeness theorem can be weakened by requiring
only that S2's complete property be equivalent to a fin and internally continuous subproperty. This raises the more general question of what expressible
properties have equivalent fin and continuous subproperties.

Acknowledgements
The first example we saw that demonstrated the inadequacy of history variables is due to Herlihy and Wing [HW87]. The introduction of prophecy
variables was based on a suggestion of Jim Saxe, who also provided many
useful suggestions for improving the exposition. We wish to thank Pierre

36

Wolper for making clear whence our ideas came and Gordon Plotkin for
making clear that whither they will lead is not an easy question, since he
couldn't answer it on the spot.

37
References
[AS86] Bowen Alpern and Fred B. Schneider. Recognizing Safety and Liveness. Technical Report TR86-727, Department of Computer Science, Cornell University, January 1986.

[AS87] Bowen Alpern and Fred Schneider. Proving boolean combinations

of deterministic properties. In Proceedings of the Second Symposium on Logic in Computer Science, pages 131-137, IEEE, June
1987.

[HW87] M.P. Herlihy and J.M. Wing. Axioms for concurrent objects. In

Proceedings of the Fourteenth Annual ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages, pages 13-
26, ACM, Munich, January 1987.

[Jon87] Bengt Jonsson. Compositional Verification of Distributed Systems.

PhD thesis, Uppsala University, 1987.

[Knu73] Donald E. Knuth. Fundamental Algorithms. Volume 1 of The

Art of Computer Programming, Addison-Wesley, Reading, Massachusetts, second edition, 1973.

[Lam77] Leslie Lamport. Proving the correctness of multiprocess programs. IEEE Transactions on Software Engineering, SE-3(2):125-
143, March 1977.

[Lam83] Leslie Lamport. Specifying concurrent program modules. ACM

Transactions on Programming Languages and Systems, 5(2):190-
222, April 1983.

[LS84] Simon S. Lam and A. Udaya Shankar. Protocol verification via

projections. IEEE Transactions on Software Engineering, SE10(4):325-342, July 1984.

[LT87] Nancy Lynch and Mark Tuttle. Hierarchical correctness proofs

for distributed algorithms. In Proceedings of the Sixth Symposium
on the Principles of Distributed Computing, pages 137-151, ACM,
August 1987.

[MP87] Zohar Manna and Amir Pnueli. Specification and verification of

concurrent programs by 8-automata. In Proceedings of the Four38

teenth Symposium on the Principles of Programming Languages,
pages 1-12, ACM, January 1987.

[Owi75] S. Owicki. Axiomatic Proof Techniques for Parallel Programs. PhD

thesis, Cornell University, August 1975.

[Sta88] Eugene W. Stark. Proving entailment between conceptual state

specifications. Theoretical Computer Science, 56(1):135-154, January 1988.

[Var87] Moshe Vardi. Verification of concurrent programs: the automatatheoretic framework. In Proceedings of the Second Symposium on
Logic in Computer Science, pages 167-176, IEEE, June 1987.

39

Glossary/Index of Notations and Conventions
e The externally visible component of a state, 7
f A refinement mapping, 11
h A history variable, 20
p A prophecy variable, 22
s, t States, 7
y, z Internal components of states, 7
F The set of initial states of a state machine, 9
L A supplementary property, 9
M A property, usually generated by a state machine, 9
N The next-state relation of a state machine, 9
O An externally visible property, 8
P A complete property, 8
S A set--typically a set of sequences, 7
S A specification, 9
ffi, j, ` Externally visible behaviors, 8
ae, oe, o/ ,  Sequences, usually representing complete behaviors, 8
\Gamma oe The set of all behaviors equivalent to oe up to stuttering, 7
\Gamma S The set of all behaviors equivalent to behaviors in S up to stuttering, 7

\Pi E The projection from states onto their external components, 7
\Pi [X] The projection from states that eliminates the X component, 20
\Sigma  A set of states, 7
\Sigma E A set of externally visible states, 7

40

\Sigma I A set of internal states, 7
\Sigma ! The set of all infinite sequences of elements of \Sigma , 7
hhs1; s2; : : :ii The sequence whose first element is s1, whose second element

is s2, etc., 1

"oe The stutter-free form of oe, 6
' Equivalence of sequences up to stuttering, 7
oe \Delta  o/ The concatenation of the sequences oe and o/ , 7
oejm The prefix of sequence oe of length m, 7
hs; ti A pair of states that is an element of the next-state relation of a

state machine, 9

S The closure of the set S, 7

41