

A Universal Top-Down Fixpoint Algorithm

Baudouin Le Charlier1
Pascal Van Hentenryck2

Technical Report No. CS-92-25

May 1992

1University of Namur 21 rue Grandgagnage, B-5000 Namur Belgium
2Brown University Computer Science Box 1910, Providence, RI 02912

A Universal Top-Down Fixpoint Algorithm
Baudouin Le Charlier
University of Namur,
21 rue Grandgagnage, B-5000 Namur (Belgium)
Email: ble@info.fundp.ac.be

Pascal Van Hentenryck
Brown University,
Box 1910, Providence, RI 02912 (USA)
Email: pvh@cs.brown.edu

Abstract

Computing fixpoints and postfixpoints of transformations has numerous applications in computer science. In this paper, we present a universal top-down fixpoint algorithm based on the weak
assumption that the transformation is defined by an effective procedure. Given the procedure and a
value ff, the universal algorithm computes the value of ff in the least fixpoint of the transformation
and has the property of focusing on the subset of elements necessary to compute the value of ff.
The total correctness of the algorithm is proven and requires some weak (but rather technical) assumptions. These conditions can be relaxed further if postfixpoints suffice for the application. As a
consequence, the algorithm generalizes and abstracts numerous algorithms and provides a versatile
alternative to bottom-up algorithms. It has been used to derive specific abstract interpretation
algorithms for Prolog, which are practical both in terms of efficiency and accuracy and are, to our
knowledge, the fastest algorithms available.

1 Introduction
The computation of least fixpoints and/or postfixpoints of transformations (or functionals) has
numerous applications in computer science, including programming languages (e.g. abstract interpretation), data bases (e.g. dependencies), and artificial intelligence (e.g. expert systems). Many
fixpoint algorithms were proposed in the past; most of them, however, are either specific to a given
problem, restricted to a syntactic class of transformations (e.g. equations), or they impose some
strong restrictions on the domains of the transformations (e.g. finiteness). Moreover, many algorithms are bottom-up, which complicates them substantially for infinite domains and/or when the
application only requires a subset of the fixpoint or postfixpoint.

In this paper, we propose a universal top-down fixpoint algorithm, based upon the weak assumption that the transformation, say o/ , is defined by an effective procedure tau (e.g. a procedure
in a programming language), which, given a functional parameter f and a value ff, returns the value
o/ f ff. The algorithm, given tau and ff, returns the value of ff in the least fixpoint of o/ . It has the
property of focusing on the subset of elements necessary to compute the value of ff and explores few,
if any, other elements in the practical applications we have considered [15]. Its partial correctness
and termination are proven and require some additional weak (but rather technical) assumptions.
These assumptions are mostly specific monotonicity conditions and they can be relaxed further

when the application only requires a postfixpoint. In particular, widening techniques [7] allow
termination even on infinite domains and the monotonicity conditions imposed for partial correctness can be dropped at the cost of a stronger assumption for termination. As a consequence, the
algorithm generalizes and abstracts numerous existing algorithms, removes many restrictions, and
provides a versatile alternative to bottom-up algorithms. The algorithm has proven useful to derive
several abstract interpretation algorithms for Prolog, based on various semantics [14, 15, 16, 9].
These algorithms (to our knowledge, the fastest available) are practical both in terms of efficiency
and accuracy.

The rest of the paper is organized as follows. Section 2 presents some mathematical preliminaries. Section 3 presents the universal top-down fixpoint algorithm. Section 4 outlines the proofs of
partial correctness and termination and formulates the necessary assumptions. Section 5 indicates
how to weaken further the conditions when only a postfixpoint is required. Section 6 discusses
how several optimizations can be included in the algorithm. Section 7 discusses related works on
fixpoint algorithms, abstract interpretation, and deductive data bases, and suggests several possible
improvements for specific application areas.

2 Preliminaries
The basic computation domains considered in this paper are complete partial orders (cpos). Recall
that a cpo is noetherian if it contains no infinite strictly increasing chains. Cpos are denoted by
A, A0 in the following. We use A 7! A0 to represent the set of monotone functions from A to A0,
where A, A0 are cpos and A0 is endowed with a lub operation. Similarly, (A 7! A0) 7! (A 7! A0) is
the set of monotonic functionals (or transformations). Transformations are denoted by the letter o/
in the following. The least fixpoint of a transformation is denoted by _(o/ ) or _o/ when there is no
ambiguity. A postfixpoint of a transformation o/ is a function f such that f * o/ (f ).

We also consider partial functions and use A 6! A0 to represent the set of partial functions from
A to A0. We write f (x) = undef if the partial function f is undefined for some value x 2 A. The
domain of a partial function f , dom(f ), is the set of ff 2 A such that f (ff) 6= undef . A 6! A0 is
endowed with a structure of cpo by the traditional inclusion relation on partial functions:

f ` g iff dom(f ) ` dom(g) & f (ff) = g(ff) for all ff 2 dom(f ):
Let S ` A. The restriction of a partial function f to S, say fjS , is the partial function satisfying
dom(fjS) = dom(f ) " S and fjS (ff) = f (ff) 8ff 2 dom(fjS). Let o/ + : (A 6! A0) ! (A 6! A0),
f 2 A 6! A0, and ff 2 A. We say that ff is based in f (wrt o/ +) iff o/ +f ff 6= undef . f 2 A 6! A0
is a partial fixpoint (resp. postfixpoint) iff 8ff 2 dom(f ) : ff is based in f & f ff = o/ +f ff (resp.
f ff * o/ +f ff).

The set of monotone partial functions is denoted A 67! A0. We also define a partial order ^ on
monotone partial functions:

f ^ g iff dom(f ) ` dom(g) & f (ff) ^ g(ff) for all ff 2 dom(f ):

3 The Problem
We consider a monotone transformation o/ : (A 7! A0) 7! (A 7! A0) described by a computable
procedure tau(f:A!A';ff:A):A' written in any usual programming language (e.g. Pascal). Pro1

cedure tau computes the transformation o/ (i.e. tau(f,ff)) returns o/ f ff, calls f finitely often, and
terminates when the functional parameter computes a (total) monotone function. The second requirement can be formalized, using denotational semantics [25], by requiring that procedure tau
computes a continuous transformation o/ + : (A 6! A0) ! (A 6! A0), i.e. 8f 2 (A 7! A0) : o/ +f = o/ f .
In the following, we overload o/ and o/ + and use o/ only.

The purpose of this paper is to present an algorithm to compute _(o/ )ff, given procedure tau
and ff. The algorithm computes a subset of the least fixpoint for practical (A may be infinite)
and efficiency reasons. As can be noticed, the algorithm is very general since it only assumes the
existence of procedure tau. It is fact necessary to add some weak (but rather technical) assumptions
to prove partial correctness and termination. They are discussed later in the paper.

4 The Algorithm
4.1 Informal Presentation
The goal of our algorithm is to avoid computing the entire table of _(o/ ) for practical and efficiency
reasons, since, on the one hand, A may be infinite, and, on the other hand, only a small subset
of _(o/ ) is sufficient in general to compute _(o/ )ff. As a consequence, the result of our algorithm
is the table of a partial function f satisfying f (ff) = _(o/ )ff. Of course, the elements necessary to
compute _(o/ )ff are not known in advance and it is the purpose of the top-down strategy to avoid
considering non-relevant elements. How the algorithm computes this table is the key issue. Recall
that the only tool at our disposal is the procedure tau(f:A!A';ff:A):A'.

Our algorithm builds the partial table by simulating the execution of procedure tau on ff. The
simulation is straightforward as long as function f is not used. When function f is applied to a
parameter fi, the algorithm suspends its simulation (we say that ff is suspended) and initiates a
sub-simulation on fi. On return of the sub-simulation, the algorithm updates the partial table and
resumes the initial simulation, using, in both cases, the sub-simulation result. The sub-simulation
itself proceeds as the initial simulation except when function f is applied to ff. In this case, no
sub-simulation is initiated and the current value of ff in the partial table is used instead. Since the
value of ff may be approximated, the algorithm needs to redo the sub-simulation on fi if the value
of ff is refined.

The table built by the algorithm corresponds in fact to a sequence of partial functions f0; . . . ; fn
satisfying f0 ^ f1 ^ . . . ^ _(o/ ): f0 is the function undefined everywhere and the algorithm moves
from a function fi to the next by considering a new element fi (i.e. fi =2 dom(fi) & fi 2 dom(fi+1))
or refining an existing element fi (i.e. fi(fi) ! fi+1(fi0)).

4.2 Operations on Partial Tables
The algorithm can be viewed as building a partial function represented by its table. Tables are
denoted by pt in the following. Also, because of the correspondence between tables and partial
functions, we do not distinguish the two when convenient. In particular, when we write about the
domain of a table, or the application of a table to a value, we mean the appropriate operations of
the partial function.

The algorithm manipulates partial tables through two operations which are used respectively
to introduce a new element in pt and to update pt with the result fi for ff. Their specification are

2

as follows (in the postconditions, a parameter subscripted by 0 denotes the value of the parameter
at call time):

Extend pt(pt,ff)

Pre: ff =2 dom(pt).
Post: pt= pt0 [f(ff; fi)g where fi = lubfpt0(ff0) : ff0 ^ ff and ff0 2 dom(pt0)g

Adjust pt(pt,ff; fi)

Pre: ff 2 dom(pt0)
Post: pt= pt1 [ pt2 where pt1 = pt0 n f(ff0; fi0) 2 pt0 j ff0 * ffg and

pt2 = f(ff0; lubfpt0(ff0); fig j ff0 * ff & ff0 2 dom(pt0)g.

In the algorithm, we use in fact a slightly more general version that returns also the set of
ff0 2 dom(pt) such that pt(ff0) has been modified by the operation.

4.3 The Dependency Graph
Our algorithm includes a dependency graph to avoid redundant computations. The main idea is
to avoid restarting simulations of procedure tau that would not produce any information. Dependencies enable to improve the worst case complexity of the algorithm for many instantiations (see
[14]).

Definition 1 A dependency graph dg is a set of elements (ff; ffi1; . . . ; fing) such that

(ff; S) 2 dg & (ff; S0) 2 dg ) S = S0:
The domain of dg, dom(dg), is the set of ff such that (ff; S) 2 dg for some S. The codomain of dg,
codom(dg), is the set of all fi such that there exists (ff; S) 2 dg and fi 2 S. If ff 2 dom(dg), dg(ff)
denotes the set S such that (ff; S) 2 dg.

Intuitively, dg(ff) represents all elements which ff depends upon. The value of ff needs to be reconsidered if any of these elements are refined. More precisely, two situations must be distinguished:

ffl If ff is not suspended, ff 2 dom(dg) means that pt(ff) cannot be refined by a new simulation.
ffl If ff is suspended, ff 2 dom(dg) means that, so far, the computation of ff need no further

simulation. In addition, dg(ff) contains the set of fi called directly by the simulation of ff.

Definition 2 Let dg be a dependency graph such that ff 2 dom(dg). The set dg+(ff) is the smallest
subset of codom(dg) closed by the following two rules:

1. fi 2 dg(ff) ) fi 2 dg+(ff).
2. fi 2 dg(ff) & fi 2 dom(dg) & fl 2 dg+(fi) ) fl 2 dg+(ff):
The reflexive and transitive closure of dg, noted dg\Lambda (ff), is simply dg+(ff) [ fffg.

The operations on dependency graphs (to remove, extend, and add dependencies) can be specified
as follows:

3

Remove dg(dg,ffi1; . . . ; fing)

Post: dg = dg0 n f(ff; S) 2 dg0 j ffi1; . . . ; fing " dg+0 (ff) 6= ;g.

Extend dg(dg,ff)

Pre: ff =2 dom(dg);
Post: dg = dg0 [ f(ff; ;)g.

Add dg(dg,ff,fi)

Pre: ff 2 dom(dg);
Post: dg = dg0 n f(ff; dg0(ff))g [ f(ff; dg0(ff) [ ffig)g.

Note also that if dg+(ff) does not contain any suspended element, then pt(ff) = _(o/ )ff.

4.4 The Algorithm
The algorithm is depicted in Figure 1. The top-level procedure is compute fixpoint which receives
two inputs, the functional parameter tau and an element ff of A, and returns two outputs, the
partial table pt and the dependency graph dg. The procedure expected as functional parameter
is a syntactic variant of the procedure tau given by the user. The modification, which can be
automated directly, amounts to replacing each call f(fi) in tau(f,ff) by f(fi; ff). This simple
change allows us to simulate procedure tau on an unknown function. The partial table returned
as result satisfies pt(ff) = _(o/ )ff for all ff 2 dom(dg), where o/ is the transformation described by
tau. In addition to the parameters, compute fixpoint makes use of a local set ics representing
the set of initiated simulations (or computations), a local procedure, and a local function. Its body
initializes pt, dg, ics to empty sets and calls procedure repeat computation.

Procedure repeat computation starts by testing if its parameter ff is in the domain of the
dependency graph or in the set of initiated computations. In the first case, no information can be
gained by executing the remaining instructions, since no element which ff depends upon has been
refined. In the second case, a sub-computation has already been started for ff. In all other cases,
the procedure extends the partial table if necessary and updates ics to include ff. It then enters
a repeat loop which, informally speaking, computes a local fixpoint for ff given the values of the
suspended elements in ics. This local fixpoint is obtained by successive executions of procedure tau
and terminates when ff is in the domain of the dependency graph (which means that an additional
iteration would produce no new information). Each iteration computes a value fi (possibly) used
to update the partial table. The dependency graph is also extended with ff before the call and is
possibly updated after the call to remove elements depending upon elements in modified. Note
that procedure tau has the local function pretended f as argument.

When procedure tau applies its functional argument, function pretended f is used. It receives
two parameters: fi is the argument to which the function should be applied while ff is the value
which required the value of fi. To obtain the value for fi, the function calls recursively procedure
repeat computation on fi. It then updates the dependency graph if ff is still in its domain;
otherwise ff needs an additional iteration and no update takes place. Finally, the procedure returns
the value pt(fi).

4

procedure compute fixpoint(in tau(f:A!A';ff:A):A',ff:A; out pt,dg);

var ics: set of A;
procedure repeat computation(in ff:A);

var fi:A';
begin

if ff 62 dom(dg) [ ics then

begin
if ff 62 dom(pt) then Extend pt(pt,ff);
ics := ics [ fffg;
repeat

Extend dg(dg,ff);
fi := tau(pretended f,ff);
Adjust pt(pt,ff,fi,modified);
Remove dg(dg,modified)
until ff 2 dom(dg);
ics := ics \Gamma  fffg
end
end;
function pretended f(in fi,ff,:A):A';

begin

repeat computation(fi);
if ff 2 dom(dg) then Add dg(dg,ff,fi);
pretended f := pt(fi)
end;
begin

pt := fg; dg := fg; ics := fg; repeat computation(ff)
end

Figure 1: The Universal Top-down Fixpoint Algorithm

5

5 Correctness
5.1 Partial Correctness
Partial correctness mainly amounts to proving the condition ptjdom(dg) = (_o/ )jdom(dg) whenever
the algorithm terminates. We outline the main ideas here; most of the details are in Appendix 1.
The proof is split into two steps:

Condition 1: 8x 2 dom(dg) : ptx * _o/ x:
Condition 2: 8x 2 dom(dg) : ptx ^ _o/ x:
Condition 1 is proven by showing the property

8x 2 dom(dg) : ( ptx * o/ ptx;x is based in pt

jdom(dg):

The condition means that ptjdom(dg) is a partial postfixpoint for o/ . The property follows from
operations Adjust pt and Remove dg which ensure ptff * o/ ptff on termination of the repeat
loop of procedure repeat computation. Moreover, conditions ff is based in ptjdg(ff) and dg(ff) `
codom(dg) ` dom(dg) are also fulfilled.

The proof of condition 2 requires some additional hypotheses about procedure tau. Informally
speaking, the information produced by a call f (ffi) should be "recorded monotonically" inside the
internal store of tau. To express this condition mathematically, we assume, without loss of
generality, that the execution of tau depends on some internal state oe. In other words, the execution
from any computation point is completely determined by the current internal state oe and the actual
functional parameter f . This implies the existence of a continuous function ` : (A 6! A0) ! (\Sigma  6!
A0), where \Sigma  is the set of internal states, satisfying `f oe = o/ f ff for any possible intermediate
internal state oe occurring during execution of tau(f; ff). Clearly, ` is a continuation in the sense
of denotational semantics.

\Sigma  is endowed with an order relation (denoted ^) which is intended to reflecting, in some sense,
the orderings on A and A0. Intuitively, if ff1 ^ ff2 and oe1 and oe2 are the internal states at
corresponding execution points for two calls tau(f; ff1) and tau(f; ff2), then oe1 ^ oe2. In other
words, the internal state contains a store of the information received in the past and the greater
the value of ff, the greater the value of the store. This can be formalized as follows:

Hypothesis 1: `f is a monotone function of oe (when execution terminates) or, in symbols:

8f 2 (A 7! A0) : 8oe1; oe2 2 \Sigma  : oe1 ^ oe2 ) `f oe1 ^ `f oe2:

For partial correctness, we assume, for convenience, that `f oe1 ^ `f oe2 holds whenever `f oe1 or `f oe2
is not defined. Finally, we need a "recording" function r : \Sigma  ! A0 7! \Sigma  such that roeff0 defines the
new internal state resulting from the internal state oe before the call and the value ff0 returned by
the call. Mathematically,

`f oe = `f (roe(f ff))

holds for any partial function f such that f (ff) is defined and any internal state o/ before a call
f(ff). The recording function should satisfy the following property:

6

Hypothesis 2: The value returned by f is recorded monotonically in the store or, in symbols,

8ff01; ff02 2 A0 : 8oe 2 \Sigma  : ff01 ^ ff02 ) roeff01 ^ roeff02:

Note also that the monotonicity of ` wrt f can be proven from the hypotheses on ` and r.

Now the proof that condition 2 holds on termination follows from the invariant pt ^ _o/ which
always holds during the computation. The invariant imposes that the result fi of any call to
procedure tau(pretended f,ff) satisfies fi ^ _o/ ff; which, in turn, follows from the property that
`(_o/ )oe ^ _o/ ff holds for any state oe occurring during the call. The proof of the last property uses
the hypotheses on ` and r. More precisely,

ffl `(_o/ )oe = _o/ ff holds before the first call to pretended f by definition of ` and o/ (_o/ ) = _o/ .
ffl Let oe; oe0 denote respectively internal states after and before a call pretended f(ff; ffi).

`(_o/ )oe0 ^ _o/ ff holds by hypothesis. Let fii be the value returned by pretended f(ff,ffi).
By definition of r, oe = roe0fii. But fii ^ _o/ ffi because fii = ptffi and pt ^ _o/ by hypothesis.
Therefore

`(_o/ )oe = `(_o/ )(roe0fii) (definition of r)

^ `(_o/ )(roe0(_o/ ffi)) (monotonicity of r; _o/ and `(_o/ ))
= `(_o/ )oe0 (property of r and `)
^ _o/ ff (hypothesis)

5.2 Termination
We give sufficient conditions for the termination of the algorithm. Other (weaker) conditions
are possible for particular transformation classes. Moreover, these conditions are relaxed further
for upper approximations of least fixpoints in the next section. Proving termination amounts to
showing the absence of following three situations:

1. There is an infinite sequence of imbricated calls to procedure repeat computation.
2. The repeat until loop in procedure repeat computation is executed infinitely often.
3. Execution of fi := tau(pretended f; ff) contains an internal loop or calls pretended f infinitely often. This situation may arise since the algorithm does not use tau with a constant
(functional) argument and is the most difficult one to preclude.

5.2.1 Sufficient Conditions for Termination
Hypothesis 3: A is finite.

Hypothesis 3 precludes situation 1 since the test ff 62 dom(dg) [ ics ensures that imbricated calls
have different input values and thus bounds the number of imbricated calls with #A.

Hypothesis 4: A 67! A0 is a noetherian cpo.
Hypothesis 4 precludes situation 2, since operations Extend pt and Adjust pt return a partial
table greater or equal to pt and hence ensures that the partial table pt is increasing.

7

Situation 3 requires some additional assumptions about tau. An obvious condition is to require
tau to terminate for any terminating functional parameter when triggered at any point in procedure
for any internal state oe. This condition is satisfied for many applications but is still too strong a
restriction as it would reject the procedure:

function tau(f:A!A; ff:A):A;

var oe: A;
begin

oe := ?;
while f(oe) !? oe do oe := f(oe);
tau := oe
end.

If A is noetherian, the procedure terminates for any monotone function f but may loop on non
monotone functions. An example is A = f?; ?g, f (?) = ?, f (?) = ?. Moreover, the procedure
may even loop for some monotone f if triggered in some "bad" states. For instance, assuming
A = f?; 0; 1g where ? ^ 0; ? ^ 1; 0 6^ 1; 1 6^ 0 and f (?) = ?, f (0) = 1, f (1) = 0, procedure tau
loops if triggered at the beginning of the while loop with oe = 0.

A weaker condition can in fact be imposed.

Hypothesis 5.a: Procedure tau terminates when started in a state "below the least fixpoint" or,

in symbols,

8f 2 (A 7! A0) : 8oe; oe0 2 \Sigma  : f ^ _o/ & oe ^ oe0 & 9ff 2 A : `(_o/ )oe0 = _o/ ff ) `f oe 6= undef :

This somewhat unnatural hypothesis is implied by the more natural
Hypothesis 5.b:

8f1; f2 2 (A 7! A0) : 8oe1; oe2 2 \Sigma  : (f1 ^ f2 & oe1 ^ oe2) ) (`f2oe2 6= undef ) `f1oe1 6= undef ):

Note the relationship of this condition with the monotonicity condition for partial correctness.
Note also that, in practical applications, it is often possible to modify the code of procedure tau to
ensure termination for arbitrary states and arbitrary total functional parameters without changing
the value of the least fixpoint. Consider our example again and assume that A is endowed with a
lub operation. The previous procedure can be modified to give:

function tau(f:A!A; ff:A):A;

var oe: A;
begin

oe := ?;
while f(oe) !? oe ? do oe := lub(oe,f(oe));
tau := oe
end.

8

We now prove that hypothesis 5.a precludes situation 3. Let oe be an internal state during a
call tau(pretended f,ff). The corresponding partial table pt can be embedded in a monotone
function f (pt ` f ) such that f ^ _o/ . Hence `f oe 6= undef . It follows that no internal loop may
occur in tau until the next call to pretended f. By induction on execution length, no internal loop
occurs. It remains to prove that no execution of tau calls pretended f infinitely often. Consider
the execution point where pt has reached its maximal value. Let f be a total monotonic function
such that (pt ` f ) and f ^ _o/ . All further calls pretended f(ff; ffi) return the value pt(ffi), i.e.
f (ffi). Therefore, for any further state oe reached in a call to tau, `f oe 6= undef holds, implying
termination.

6 Extensions
In many applications such as abstract interpretation, it is acceptable to compute an upper approximation of the least fixpoint. We now show how to broaden the applicability of the algorithm for
these problems. In particular, A can now be infinite and some of the monotonicity conditions can
be relaxed.

6.1 Infinite Domains
Definition 3 [7] Let A be a poset. A widening operator on A is a function: r : A \Theta  A ! A
satisfying

1. for each infinite sequence x0; x1; . . . ; xi; . . . (xk 2 A), the sequence y0; y1; . . . ; yi; . . . is increasing and stationary, where y0 = x0 & yi+1 = yirxi+1 (i * 0).

2. 8x; y 2 A : x; y ^ xry.

Widening operators can be used to prevent the set ics of initiated computations from growing
infinitely when A is an infinite set. It suffices to modify the universal algorithm to work on a
finite number of increasing sequences, each of which constructed with a widening operator. When
repeat computation(ff) is called, the first instruction of procedure repeat computation now
becomes ff := y r ff where y is the last element in one of the sequences in the set ics. The rest
of the algorithm is left unchanged. Now the condition ptjdom(dg) * (_o/ )jdom(dg) holds when the
modified algorithm terminates. Alternatively, widening techniques are also useful to speed up the
algorithm when convergence is slow.

6.2 Non Monotone Transformations
In abstract interpretation, it is sometimes difficult, if not almost impossible, to prove monotonicity
of transformation o/ . (See [11, 21].) Fortunately, monotonicity can be dropped since the first
part of the partial correctness proof does not use the hypotheses on o/; ` and r. The algorithm
then computes a partial postfixpoint of o/ (assuming termination), which is still useful in abstract
interpretation, since it is an upper approximation of the least fixpoint in the concrete domain.
However, stronger conditions are needed for termination: procedure tau must terminate for any
internal state oe and total function parameter f .

9

Widening techniques can be applied in the case of non monotone transformation to broaden
further the applicability. As suggested by the previous example, they can be also applied "inside"
procedure tau to ensure its termination.

7 Instantiations and Optimizations
7.1 Existing Instantiations
The universal top-down fixpoint algorithm is a generalization of several specific algorithms that we
developed for the abstract interpretation of Prolog.

The generic algorithm described in [15, 14, 21] is an instantiation of the universal algorithm
where the transformation o/ is a simple input/output abstract semantics based on SLD-resolution,
the standard operational semantics for Prolog (see [17]). In this instantiation, o/ is specific to a
given program and the cpos A and A0 are the same. The worst case complexity (i.e. the number of
iterations in the repeat loop of procedure repeat computation) was analyzed for several interesting
program classes and behavioral assumptions. The worst case complexity is O(n2hs2) where n is
the size of the analyzed program, and h and s are the height and the size of the cpo A. However,
under reasonable assumptions satisfied by many programs or subprograms, the complexity reduces
to O(nhs2), O(nhs), and O(nh) for several classes of recursive programs. The algorithm was
evaluated experimentally on real programs using a complex infinite abstract domain containing
modes, sharing, and structural information. The number of iterations is h0n in the average and
bounded by 3h0n (where h0 is the length of the longest explored increasing sequence), showing the
practicability of the approach. In addition, the algorithm explores at most 15% of unnecessary
elements and none for most programs.

The reexecution algorithm described in [16] is also an instantiation of the universal algorithm
for a much more complex semantics exploiting referential transparency to improve accuracy of
the analysis. Although the semantics was much more complex, the derivation algorithm and its
correctness proof were substantially simplified because of the availability of the universal algorithm.

7.2 Optimizations
In [16], we have described two general optimizations for fixpoint algorithms. Both of them produced
about 30% improvement on the first instantiation. The second optimization was also integrated
in the second instantiations and its effect should be even more important (although no explicit
comparison was made).

The first optimization (the so-called prefix optimization) applies when procedure tau is reexecuted on an input value ff 2 A. Rather than restarting the execution of tau from scratch, it is
more efficient to restart the execution from the first call to the functional parameter which gives a
value different from the previous execution. This idea can be implemented with a data structure
keeping, for each value ff, the internal states oe1; . . . ; oen of procedure tau before the calls to the
functional parameter with input values ff1; . . . ; ffn. This data structure, which can be seen as a dependency graph with a finer granularity, needs to be updated each time pt(ffj) is refined to remove
oej+1; . . . ; oen. The recomputation starts from the internal state with the largest k still in the data
structure at reexecution time. More sophisticated optimizations are possible when procedure tau
can be decomposed in several independent subparts, avoiding reexecution of non-affected subparts

10

even when an earlier call has an improved value. Deriving the algorithm automatically is however
more complicated in this case since it is necessary to consider the internal structure of tau.

The second optimization is based on the fact that reexecution on the same value redoes many
internal operations with the same input arguments. These operations can in fact be cached so
that they are only computed once. Although this optimization is rather systematic, it also requires
considering the internal structure of tau.

Finally, a last optimization that may turn useful amounts to detecting when an element ff has
reached its final value, i.e. ptff = _(o/ )ff. This is the case when procedure tau on ff does not make
use (directly or indirectly) of any element in the set ics. All subsequent calls to tau with ff can
thus be avoided. This optimization does not require considering the internal structure of tau.

8 Related Work and Discussion
8.1 Bottom-up Fixpoint Algorithms
In this paper, we view the computation of the least fixpoint of a monotone transformation as a
generalization of computing the values of a function from its recursive definition. This leads to a
top-down approach in contrast to the more usual approach inspired by the Kleene's sequence. We
now compare our algorithm with a representative bottom-up algorithm proposed by O'Keefe [24].

O'Keefe's algorithm solves finite sets of equations of the form xi = expri (1 ^ i ^ n) where
x1; . . . ; xn are distinct variables, ranging on lattices of finite depth T1; . . . ; Tn, and expr1; . . . ; exprn
are well-typed monotone expressions possibly involving x1; . . . ; xn. The algorithm computes the
least fixpoint of the transformation

o/ : T1 \Theta  . . . \Theta  Tn 7! T1 \Theta  . . . \Theta  Tn

hx1; . . . ; xni ; hexpr1; . . .; exprni

It proceeds as follows. Variables x1 to xn are initialized to ? and pushed onto a stack. Then
variables are popped from the stack until it becomes empty. Each time a variable xi is popped its
value is recomputed. If the new value is greater than the previous one, all variables that depends
on xi and are not on the stack are pushed on it.

Our algorithm is obviously applicable to the same class of problems (with A = f1; . . .; ng and
A0 = T1 + . . .+ Tn). It can also be shown that both algorithms have the same worst case complexity
on this class of problems. Moreover, none of the algorithms outperforms the other. For example,
our algorithm is optimal for equations of the form:

xji = exprjihxj1 ; . . . ; xji\Gamma 1i (1 ^ i ^ n)
where j1; . . . ; jn is a permutation of 1; . . .; n, since each expression is computed at most once while
O'Keefe's algorithm is not optimal.

Example 4 Consider the following system:

x2 = x1+1.

..

xn = xn\Gamma 1+1

x1 = 1

11

where T1 = . . . = Tn = f0; . . .; ng and x+y = min(n; x + y). O'Keefe's algorithm requires at least
2(n \Gamma  1) additions and (n \Gamma  1)(n \Gamma  2)=2 in the worst case depending on how x2; . . . ; xn are put onto
the stack.

Conversely, there are also situations where O'Keefe's algorithm gives better results.
Example 5 Consider the following system (n * 3):

x1 = max(x2; xn\Gamma 2)

...

xn\Gamma 3 = max(xn\Gamma 2; xn\Gamma 2)
xn\Gamma 2 = if xn\Gamma 1 = n then n else 0
xn\Gamma 1 = xn+1
xn = max(x1; xn\Gamma 1)

Computation of xn with our algorithm requires n(n + 1) operations while O'Keefe's algorithm only
requires 7n \Gamma  11.

As far as applicability is concerned, it is clear that O'Keefe's algorithm addresses a considerably
more restrictive class than ours. The two main restrictions are the following:

1. A reduces to the finite set f1; . . .; ng, ordered pointwise. (A0 is the disjoint product T1 + . . . +

Tn.)

2. The functional transformation o/ is not defined by an arbitrary algorithm but is constrained

to be defined by a system of n equations.

It is nevertheless possible to adapt O'Keefe's algorithm to deal with more general problems.
However, O'Keefe's algorithm becomes substantially more complicated compared to ours. As an
example, consider the problem of solving recursive equations of the form

f (x) = exprhf; xi
on a finite lattice T , where expr is an expression built from f; x and constant symbols denoting
monotone functions. To compute f (v) where v 2 T , we can partially evaluate exprhf; vi and
identify other subexpressions, say f (w) for which we iterate the process. We obtain a finite set of
equations and O'Keefe's algorithm is applicable. The problem is more difficult when f appears in
subexpressions of expressions of the form f (expr1; . . . ; exprn). In order to evaluate the subexpressions we can initialize f (v) to the bottom element of T for each v 2 T . So a first set of equations
can be derived and solved. The solution is used to derive a new set of equations which is then
solved, and so on until no greater values are obtained for f . This complicated process should be
contrasted with the application of our algorithm to a procedure tau defined as follows:

function tau(f:T!T; ff:T):T;

begin tau:=exprhf,ffi end.

The previous discussion can also be related to the minimal function graph semantics discussed in
the paper and to bottom-up computation of recursive functions (see [3]).

12

8.2 Chaotic Iteration Algorithms
Chaotic iteration was introduced by P. Cousot in [7] as a very general class of methods for finding
the least solution of a set of equations of the form xi = expri (1 ^ i ^ n): A chaotic iteration
strategy for such a set of equations defines a finite sequence i1; . . . ; im such that the "loop":

(x1; . . .; xn):=(?; . . .; ?);
for j:= 1 to m do xij := exprij !x1; . . . ; xn?

compute the least solution of the system. The best strategy minimizes the amount of computation
(roughly speaking, the number m of iterations). Clearly, this is very problem dependent. O'Keefe's
algorithm provides a standard (bottom-up) chaotic iteration strategy while ours provides a topdown one. However, chaotic iteration seems difficult to generalize to the class of problems considered
in this paper.

8.3 Deductive Data Bases
Fixpoint computation algorithms have been extensively studied to solve efficiently queries in deductive data bases (e.g. [5, 27, 28]). Bottom-up and top-down approaches are both used and have been
related by means of rewriting techniques (e.g. magic sets [1, 27]). Our algorithm should be related
to OLDT-resolution [26]. However, our algorithm is inefficient in this context because elements
of A and A0 denote sets of values (facts) instead of "individual" values. As a consequence, each
iteration redoes all computations performed by the previous ones. Our algorithm can be adapted
to this type of problems to derive an efficient and general version of OLDT. The idea is to replace
"single" valued functions by "multi" valued ones, delivering values one at a time, and to use the
prefix optimization cleverly. We will investigate this idea in future research.

8.4 Abstract Interpretation
Abstract interpretation is a general methodology for building static analyses of programs. It was
introduced by P. and R. Cousot in [7]. The original idea was subsequently adapted, reformulated,
and applied to many programming paradigms. (Approximate) computation of fixpoints is a main
issue in abstract interpretation. In the following we relate our algorithm to several aspects and
approaches in this field. We stress its usefulness in such a context both from a practical and a
methodological point of view.

Operational Frameworks The operational approach to abstract interpretation reduces to the
idea of executing the program on a non standard (abstract) domain. This leads either to specific
algorithms (e.g. [8, 2, 10]) or to the so-called operational frameworks [4, 6]. Those approaches are
difficult to prove correct, to implement, and to modify, since they mix semantic and algorithmic
aspects. A more systematic approach consists in separating both issues. Defining a new class of
static analyses then reduces to the definition of a non standard fixpoint semantics with respect to
which the universal algorithm can be instantiated (the semantics maps each program to a procedure
tau). Hence, a general algorithm parametrized on the abstract domain is obtained. The advantages
of the approach are twofold. Most optimizations can be handled at the algorithmic level and the
semantics can be changed without requiring any substantial modification of the algorithm.

13

Denotational Frameworks The denotational approach to abstract interpretation was first introduced by F. Nielson [22]. It was then further developed by many authors including [13, 19, 29].
In this approach, the elegant notations of denotational semantics and the mathematical framework
of the Cousot's are put together to provide high level descriptions of static analyses. Authors relying on this approach are mostly concerned with semantic issues. They assume the existence of
well-accepted and efficient algorithms for fixpoint computation. O'Keefe algorithm is often referred
to as well as algorithms designed for deductive data bases and OLDT-resolution. Our universal
algorithm provides a versatile alternative to O'Keefe's algorithm to implement the denotational
approach given its wide applicability. Moreover, OLDT-resolution is not especially adequate for
abstract interpretation, since many results are produced for each single input leading to an exponential algorithm.

Minimal Function Graph and Query Directed Semantics To overcome the applicability
problem of O'Keefe's algorithm (and more generally of any bottom-up evaluation), many authors
design "instrumental" semantics introducing supplementary results needed only by the algorithm
for fixpoint computation. They are called minimal function graph semantics in [12, 29] and query
directed semantics in [18]. The basic idea is that each iteration provides the set of input values for
the next iteration in addition to the output values corresponding to the previous inputs. Such an
instrumental semantics is no longer needed with our approach (see the comparison with O'Keefe's
algorithm). However, the additional results provided by those derived semantics are sometimes
useful for applications. For example, the minimal function graph semantics is useful for program
specialization [29]. The same information is obtained by instantiating our algorithm to a simple
input/output semantics, also called total function graph semantics. Note that ptjdom(dg) is the
required minimal function graph.

Collecting Semantics Instrumental semantics designed for bottom-up computation purposes
must be distinguished from collecting semantics. Collecting semantics characterizes explicitly properties that are present only implicitly in a simpler abstract semantics. An obvious example is given
by an abstract semantics collecting information at each program point as opposed to a simpler
input/output semantics (for loops and procedures). Oddly enough, it happens sometimes that the
minimal function graph and collecting semantics collapse. In these cases, our universal algorithm
can be instantiated to the simple input/output semantics and provides for free the results of the
collecting one. In the general case (the collecting semantics does not reduce to the minimal function
graph), it is more efficient to compute the needed information in two steps. First, the universal
algorithm is applied to the simple semantics. Then, the equations defining the collecting semantics
are executed once, using the results of the first step.

Granularity Granularity is a useful criterium to compare Abstract Interpretation algorithms
(and frameworks). It is specified by giving the "program points" where information about all possible executions is considered. It is finer if more program points are considered, coarser otherwise.
It is static if the pieces of information corresponding to all execution paths leading to the same
point are lumped together. Static granularity has the advantage that a finite set of equations can
be associated with the analyzed program. Dynamic granularity is achieved when different pieces of
information can be associated with the same program point, possibly corresponding to different execution paths. For a fixed abstract domain, a finer granularity is likely to give more precise results

14

at the price of a higher computation cost. Similarly, dynamic granularity is a priori more accurate
and costly than static granularity. Usefulness of granularity to compare abstract interpretation
frameworks can be illustrated by the works of C. Mellish [20], U. Nilsson [23] and M. Bruynooghe
[4], which proposed frameworks for the abstract interpretation of (pure) Prolog.

The earlier work is due to Mellish. It has static granularity and only two program points are
considered for each procedure: procedure entry and procedure exit. The abstract semantics can be
expressed as system of 2n equations with 2n variables where n is the number of procedures. Since
this system is small, the naive bottom-up method based on the Kleene's sequence can be used as
done in Mellish's algorithm. O' Keefe's paper [24] was motivated by Mellish's work and provides a
faster method.

The framework of Nilsson has static but finer granularity: all program points are considered
(clause entry and exit, and any point between two calls). This results in a system of m equations
with m variables where m is the number of program points. The algorithm of is essentially O'Keefe's
one and it is easy to exhibit programs where it gives more precise results than Mellish's algorithm.

Bruynooghe's framework uses the same program points as Nilsson but with dynamic granularity.
We have presented in [14] a generic abstract interpretation algorithm for Prolog wich can be seen as a
precise implementation of Bruynooghe's framework. Alternatively it can be seen as an instantiation
of the universal algorithm of this paper to a simple input/output semantics for Prolog.

It is possible to derive abstract interpretation algorithms with the same granularity as Mellish
and Nilsson by instantiating the universal algorithm to appropriate (but more complicated) instrumental semantics. An other approach to investigate in the future is the definition of universal
algorithms that should automatically provide a coarser granularity when instantiated to a simple
(input/output) abstract semantics. Those algorithms should use widening techniques and/or introduce "hooks" in the tau procedure to control the granularity level. Hence, we conclude that
semantic and algorithmic issues in abstract interpretation are in some sense dual. Tuning the
granularity can be done at both levels just as a matter of convenience.

Acknowledgements
Henry Leroy pointed out that our previous algorithms would work in a much more general setting.
This research was partly supported by the Belgian National Incentive-Program for fundamental
Research in Artificial Intelligence (Baudouin Le Charlier) and by the National Science Foundation
under grant number CCR-9108032 and by the Office of Naval Research and the Defense Advanced
Research Projects Agency under Contract N00014-91-J-4052 (Pascal Van Hentenryck).

References

[1] F. et al. Bancilhon. Magic Sets and Other Strange Ways to Implement Logic Programs. In Proceedings of Fifth ACM Symposium on Principles of Database Systems, pages 1-15, Cambridge,
Massachusetts, 1986.

[2] A. Bansam and L. Sterling. An abstract interpretation scheme for logic programs based on

type expression. In Proc. Int. Conf. on Fifth Generation Computer Systems, pages 422-429,
Tokyo, 1988.

15

[3] G. Berry. Bottom-up Computation of Recursive Programs. In RAIRO-rouge 10(3), pages

47-82. March 1976.

[4] M. Bruynooghe. A Practical Framework for the Abstract Interpretation of Logic Programs.

Journal of Logic Programming, 10:91-124, 1991.

[5] F. Bry. Query Evaluation in Recursive Databases. In Proc. First Int. Conf. on Deductive and

Object-Oriented Databases, pages 20-39, Kyoto, Japan, 1989.

[6] P. Codognet and G. Fil`e. Computations, Abstractions and Constraints in Logic Programs.

In Proceedings of the fourth International Conference on Programming languages (ICCL'92),
Oakland, U.S.A., April 1992.

[7] P Cousot and R. Cousot. Abstract Interpretation: A unified Lattice Model for Static Analysis

of Programs by Construction or Approximation of Fixpoints. In Conf. Record of Fourth ACM
Symposium on POPL, pages 238-252, Los Angeles, CA, 1977.

[8] S. Debray. Efficient Dataflow Analysis of Logic Programs. In Proc. of 15th Annual Symposium

on POPL, pages 260-273, San Diego, CA, 1988.

[9] V. Englebert, B. Le Charlier, D. Roland, and P. Van Hentenryck. Generic Abstract Interpretation Algorithms for Prolog: Two Optimization Techniques and Their Experimental Evaluation. In Fourth International Symposium on Programming Language Implementation and
Logic Programming (PLILP-92), Leuven (Belgium), August 1992.

[10] M. Hermenegildo, R. Warren, and S. Debray. Global Flow Analysis as a Practical Compilation

Tool. Journal of Logic Programming, 1991. (To appear).

[11] G Janssens. Deriving Run Time Properties Of Logic Programs By Means of Abstract Interpretation. PhD thesis, Katholieke Universiteit Leuven, Department Computerwetenschappen,
Leuven (Belgium), 1990.

[12] N.D. Jones and A. Mycroft. Dataflow Analysis of Applicative Programs using Minimal Function Graphs. In Proceedings of 13th ACM symposium on Principles of Programming Languages,
pages 123-142, St. Petersburg, Florida, 1986.

[13] N.D. Jones and H. Sondergaard. A Semantics-Based Framework for the Abstract Interpretation

of Prolog, volume Abstract Interpretation of Declarative Languages, pages 123-142. Ellis
Horwood, 1987.

[14] B. Le Charlier, K. Musumbu, and P. Van Hentenryck. A Generic Abstract Interpretation Algorithm and its Complexity Analysis (Extended Abstract). In Eighth International Conference
on Logic Programming (ICLP-91), Paris (France), June 1991.

[15] B. Le Charlier and P. Van Hentenryck. Experimental Evaluation of a Generic Abstract Interpretation Algorithm for Prolog. In Fourth IEEE International Conference on Computer
Languages (ICCL'92), San Fransisco, CA, April 1992.

[16] B. Le Charlier and P. Van Hentenryck. Reexecution in Abstract Interpretation of Prolog.

Technical Report CS-92-12, CS Department, Brown University, 1992. (44 pages).

16

[17] J. W. Lloyd. Foundations of Logic Programming. Springer-Verlag, New York, 1984.
[18] K. Marriott and H. Sondergaard. Semantics-based Dataflow Analysis of Logic Programs. In

Information Processing-89, pages 601-606, San Fransisco, CA, 1989.

[19] K. Marriott and H. Sondergaard. Abstract Interpretation of Logic Programs: the Denotational

Approach, June 1990. To appear in ACM Transaction on Programming Languages.

[20] C. Mellish. Abstract Interpretation of Prolog Programs, volume Abstract Interpretation of

Declarative Languages, pages 181-198. Ellis Horwood, 1987.

[21] K. Musumbu. Interpretation Abstraite de Programmes Prolog. PhD thesis, University of Namur

(Belgium), September 1990.

[22] F. Nielson. A Denotational Framework for Data Flow Analysis. Acta Informatica, 18:265-287,

1982.

[23] U. Nilsson. Systematic Semantic Approximations of Logic Programs. In Proceedings of PLILP

90, pages 293-306, Linkoeping, Sweeden, August 1990.

[24] R.A. O'Keefe. Finite Fixed-Point Problems. In J-L. Lassez, editor, Fourth International

Conference on Logic Programming, pages 729-743, Melbourne, Australia, 1987.

[25] J. Stoy. Denotational Semantics: The Scott-Stratchey Approach to Programming Language

Theory . MIT Press, Cambridge Mass., 1977.

[26] H Tamaki and T. Sato. OLD-resolution with Tabulation. In Third International Conference

on Logic Programming, pages 84-98, London, July 1986.

[27] J.D. Ullman. Bottom-up Beats Top-Down for Datalog. In Proc. ACM SIGACT-SIGMODSIGART Symp. on Principles of Database Systems, pages 140-149, 1989.

[28] L. Vieille. Recursive Axioms in Deductive Databases : the Query/Subquery Approach. In

Proceedings of the First International Conference on Expert Databases Systems, pages 179-
193, Charleston, South Carolina, April 1986.

[29] W.H. Winsborough. A minimal function graph semantics for logic programs. Technical Report

TR-711, Computer-Science Department, University of Wisconsin at Madison, August 1987.

17

9 Appendix 1: Proof of Partial Correctness
9.1 Invariant Conditions
An Invariant Condition is a condition that hold "almost" always during the execution of the algorithm. To reuse them extensively, invariant conditions are parametrized on the values of the used
objects: dg, ics, pt. When used without parameters, their original textual form is intended. We
use the following invariant conditions:

Inv1hdg; icsi : codom(dg) ` dom(dg) [ ics
Inv2hdg; ics; pti : dom(dg); ics ` dom(pt)

Inv3hdg; ics; pti : 8x 2 dom(dg) \Gamma  ics : ( x is based in ptjdg(x);ptx * o/ ptx
Inv4hpti : pt ^ _(o/ )
Invhdg; ics; pti :

4^

i=1

Invi

9.2 General Post Conditions
General Post Conditions relates objects values at two different execution times called initial and
final. Those conditions are intended to be used frequently in postconditions of the procedures. We
use the following general post conditions:

GP1hics0; icsi : ics = ics0
GP2hdg0; dg; ics0i : dom(dg) " ics0 ` dom(dg0)
GP3hpt0; pti : pt0 ^ pt
GP4hdg0; ics0; dgi : U (dg0; ics0; dg) ` dom(dg)

GP5hdg0; ics0; pt0; dg; pti : 8x 2 U (dg0; ics0; dg) : ( dg(x) = dg0(x);8y 2 dg+(x) : pty = pt

0y

GPhdg0; ics0; pt0dg; ics; pti :

5^

i=1

GPi

The new notations used by the post conditions are defined as follows. Let dg be a dependency
graph and ff 2 dom(dg). dg(ff) is the subset of dg "reachable" from ff, i.e.

dg(ff) = f(fi; S) 2 dg j fi 2 dg\Lambda (ff)g:
Intuitively, U (dg0; ics0; dg) contains the values whose elements they depend upon are left unchanged.
Formally, it is defined by

U (dg0; ics0; dg) = [

x2dom(dg) " ics0

dg\Lambda 0(x):

9.3 Specifications of the Procedures
We now turn to the specifications of the procedures.

18

procedure repeat computation(in ff:A);

var fi:A';
begin

if ff 62 dom(dg) [ ics then
begin

if ff 62 dom(pt) then Extend pt(pt,ff);
ics := ics [ fffg;
repeat fA1g

Extend dg(dg,ff) fA2g;
fi := tau(pretended f,ff) fA3g;
Adjust pt(pt,ff,fi,modified) fA4g;
Remove dg(dg,modified) fA5g
until ff 2 dom(dg);
ics := ics \Gamma  fffg
end
end

Figure 2: The annotated repeat computation Procedure
Specification 6 frepeat computation(ff)g

Pre : Inv
Post : Inv & GP & ff 2 dom(dg)

Specification 7 ffi := tau(pretended f,ff)g

Pre : Inv & ff 2 ics & dg(ff) = f! ff; fg ?g
Post : Inv & GP & fi ^ _o/ ff &

ff 2 dom(dg) ) ( ff is based in ptjdg(ff)fi = o/ ptff )

Specification 8 fcompute fixpoint(ff,pt,dg)g

Pre : true
Post : ff 2 dom(dg) & codom(dg) ` dom(dg) &

8x 2 dom(dg) : x is based in ptjdg(x) &
ptjdom(dg) = _(o/ )jdom(dg)

9.4 Partial Correctness of repeat computation(ff)

Partial correctness of Procedure repeat computation relies on five intermediate assertions A1,
. . . ,A5 that hold where indicated in the annotated version of the procedure (see Figure 2). The
assertions are depicted in Figure 3.

We prove statements of the form fAig S fAi+1g. In the proofs, dgi, icsi and pti denote the
values of dg, ics and pt before execution of S. To denote the final values, the symbols dg, ics and

19

A1 : ( Inv & ff 62 ics0 & ics = ics0 [ fffg &GP

2::5 & ff 62 dom(dg)

A2 : ( Inv & ff 62 ics0 & ics = ics0 [ fffg &GP

2::5 & ff 2 dom(dg) & dg(ff) = fg

A3 :

8?????!

?????:

Inv & ff 62 ics0 & ics = ics0 [ fffg &
GP2::5 &

ff 2 dom(dg) ) ( ff is based in ptjdg(ff)fi = o/ ptff ) &
fi ^ _o/ ff

A4 :

8???????
??????!

??????????
???:

Inv1::2;4 & ff 62 ics0 & ics = ics0 [ fffg &
8x 2 dom(dg) \Gamma  ics : (dg+(x) " modified = fg) ) ( x is based in ptjdg(x)ptx * o/ ptx ) &

ff 2 dom(dg) ) ( ff is based in ptjdg(ff)ptff * o/ ptff ) &
GP2::4 &
8x 2 U (dg0; ics0; dg) : ( dg(x) = dg0(x);8y 2 dg+(x) \Gamma  modified : pty = pt

0y )

A5 : 8???!???:

Inv & ff 62 ics0 & ics = ics0 [ fffg &
ff 2 dom(dg) ) ( ff is based in ptjdg(ff)ptff * o/ ptff ) &
GP2::5

Figure 3: Assertions for repeat computation

20

pt are used. To simplify notation, we introduce some notational conventions for assertions. Let
As be an assertion containing possibly occurrences of the symbols dg, ics, pt, dg0, ics0 and pt0.
We denote As(i) the assertion As where dgi, icsi and pti have been substituted to dg, ics and pt.
Similarly we denote As[i] the assertion As where dgi, icsi and pti have been substituted to dg0, ics0
and pt0.

The proof proceeds by symbolic execution, using the specification of fi := tau(pretended f,ff).
Since it is rather tedious, we focus on the less obvious parts.

9.4.1 fA1g Extend dg(dg,ff) fA2g
Straightforward consequence of the specification of Extend dg.

9.4.2 fA2g fi := tau(pretended f,ff) fA3g
Due to the specification of fi := tau(pretended f,ff), conditions Inv, ff 62 ics0, ics = ics0 [ fffg
and GP2::5 are maintained. This is obvious except for GP3, GP4 and GP5. The proof of GP3 uses

the fact that GP(2)3 and GP[2]3 hold due to A2 and the specification of fi := tau(pretended f,ff).
Proofs of GP4 and GP5 are similar.

9.4.3 fA3g Adjust pt(pt,ff,fi,modified) fA4g
Conditions Inv1::2 are maintained because operation Adjust pt does not use dg nor ics, and does
not change dom(pt). Condition Inv3 is replaced by a weaker one, i.e.

8x 2 dom(dg) \Gamma  ics : (dg+(x) " modified = fg) ) ( x is based in ptjdg(x)ptx * o/ ptx ) ;
due to the fact that o/ ptx can have been increased if dg+(x)"modified 6= fg. Otherwise ptx * pt3x
due to the specification of Adjust pt. Therefore: ptx * pt3x * o/ pt3x = o/ ptx.
Condition

ff 2 dom(dg) ) ( ff is based in ptjdg(ff)ptff * o/ ptff )

holds because of the third subcondition of A(3)3 and because ptff * fi due to the specification
of Adjust pt. Conditions ff 62 ics0 & ics = ics0 [ fffg, GP2 and GP4 are maintained since
Adjust pt(pt,ff,fi,modified) does not use ics nor dg.

Condition Inv4 holds due to Inv(3)4 , fi ^ _o/ ff and because

8f; g 2 (A 67! A0) : 8 ! x; y ?2 A \Theta  A0 : f ^ g & y ^ gx ) Adjust pt(f; x; y) ^ gx:
Finally the condition

8x 2 U (dg0; ics0; dg) : ( dg(x) = dg0(x);8y 2 dg+(x) \Gamma  modified : pty = pt

0y )

is a consequence of GP(3)5 , dg = dg3 and of the fact that pty = pt3y if y 62 modified.

21

9.4.4 fA4g Remove dg(dg,modified) fA5g
The proof uses condition A(4)4 , the fact that Remove dg(dg,modified) does not use ics nor pt and
the following consequences of the specification of Remove dg.

dom(dg) ` dom(dg4) ;
8x 2 dom(dg) : dg+(x) = dg+4 (x):

9.4.5 Proof of Partial Correctness
Let dg0, ics0 and pt0 denote the values of dg, ics and pt at procedure entry. Correctness is
straightforward if ff 2 dom(dg0) [ ics0. Otherwise it is easy to see that the body of the repeat
until instruction is executed under the precondition A1. (Only pt will be possibly increased by the
call Extend pt(pt,ff).) A5 holds after each execution of the body. Reexecution of the body takes
place when ff 62 dom(dg). Then A1 is true once again since (A5 & ff 62 dom(dg)) ) A1. Finally
termination of the repeat until instruction happens when (A5 & ff 2 dom(dg)) holds. This clearly
implies that the postcondition of the specification holds at procedure exit.

9.5 Partial Correctness of fi := tau(pretended f,ff)
Assuming termination, procedure tau calls pretended f finitely often, say n(* 0) times. Let ffi
denote the first actual parameter of the i-th call (1 ^ i ^ n). We prove that assertion Iphii below
holds before the first call (i = 0), between the calls i \Gamma  1 and i (1 ^ i ! n), and after the last call
if any (i = n). Assertion Iphii is given together with four auxiliary assertions Ip1, . . . , Ip4. Recall
that oe denotes the internal state of tau and ` its associated continuation function.

Ip1 : dg(ff) = fff1; . . .; ffig

Ip2 : dg(ffj) = dgj(ffj) (8j : 1 ^ j ^ i)
Ip3 : 8x 2 dg+(ffj) : ptx = ptjx (8j : 1 ^ j ^ i)
Ip4 : 8j : 1 ^ j ^ i : ptffj = ptjffj

Iphii : 8?!?:

Inv & GP & `(_o/ )oe ^ _o/ ff &

ff 2 dom(dg) )

4^

k=1

Ipk

The proof is by induction on i.
Base case (i = 0) Execution of tau does not use dg, ics nor pt (except through calls to
pretended f). So all conditions hold trivially because they hold initially. Condition

`(_o/ )oe ^ _o/ ff
holds because pretended f was not used. Therefore the current state is the same as for a call
tau(f,ff) where f computes _o/ .

22

Induction step (1 ^ i ^ n) Assume that Iphi \Gamma  1i holds before the i-th call to pretended f.
The call repeat computation(ffi) is valid, since Inv holds. Therefore it establishes the following
condition:

Inv & GP[i\Gamma 1] & ffi 2 dom(dg):

Moreover conditions Inv(i\Gamma 1) and GP(i\Gamma 1) also hold. Putting all conditions together allows to deduce
that Inv & GP holds after the call pretended f(ff,ffi).
Now we prove that condition

`(_o/ )oe ^ _o/ ff

holds. Let oe0 denote the internal state before the call. `(_o/ )oe0 ^ _o/ ff holds by hypothesis. Let fii
be the value returned by the call pretended f(ff,ffi). By definition of r, oe = roe0fii. But fii ^ _o/ ffi
because fii = ptffi and pt ^ _o/ due to the specification of repeat computation. Therefore

`(_o/ )oe = `(_o/ )(roe0fii) (definition of o/ )

^ `(_o/ )(roe0(_o/ ffi)) (monotonicity of r; _o/ and `)
= `(_o/ )oe0 (property of r and `)
^ _o/ ff ( hypothesis )

Partial correctness As Iphni holds at completion, Inv & GP holds. To show fi ^ _o/ ff, consider
the final state oe. Then:

fi = `(_o/ )oe (`f oe depends on oe, not on f , for final states.)

^ _o/ ff (Iphni)

For the last condition, suppose ff 2 dom(dg). Conditions Ip1 and Ip3 imply that any call tau(f; ff)
where f (ffj) = pt(ffj) (8j : 1 ^ j ^ n) has the same sequence of internal states as the completed
call to tau. Hence it returns fi. Therefore

ff is based in ptjdg(ff) & fi = o/ ptff:

9.6 Partial Correctness of compute fixpoint(ff,pt,dg)
Applying Specification 4 with dg0 = ics0 = pt0 = fg results in the postcondition:

codom(dg) ` dom(dg) ` dom(pt);
8x 2 dom(dg) : ( x is based in ptjdg(x);ptx * o/ ptx;
pt ^ _(o/ ):
which obviously implies:

8x 2 dom(dg) : ptx ^ _o/ x;

To show the converse, consider the transfinite Kleene's sequence (f-)-^ffl defined as follows:

f0 = ?
f- = o/ f-\Gamma 1 if - is not a limit ordinal

= o/ (F-0!- f-0 ) if - is a limit ordinal

23

with ffl being the smallest ordinal such that fffl = o/ fffl = _o/ . Condition

8x 2 dom(dg) : ptx * f-x
is proven by transfinite induction on -:

ptx * ? = f0x

ptx * o/ ptx

* o/ f-\Gamma 1x (induction hypothesis, monotonicity of o/ and

x is based in ptjdom(dg))
= f-x

when - is a non limit ordinal. The proof is similar for limit ordinals. It follows that

8x 2 dom(dg) : ptx * ffflx = _o/ x:

Note Using transfinite induction seems unnatural here but is technically simpler because

1. o/ is only required to be monotone wrt ^ and not continuous;
2. Only partial correctness is of interest to us.
Of course, termination is possible only for values reachable in a finite number of steps (except when
widening operations are used).

24