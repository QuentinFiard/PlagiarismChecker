

Is Continuation-Passing Useful for

Data Flow Analysis?

Amr Sabry\Lambda  Matthias Felleisen\Lambda 

Department of Computer Science

Rice University
Houston, TX 77251-1892y

Abstract
The widespread use of the continuation-passing style
(CPS) transformation in compilers, optimizers, abstract interpreters, and partial evaluators reflects a
common belief that the transformation has a positive
effect on the analysis of programs. Investigations by
Nielson [13] and Burn/Filho [5, 6] support, to some degree, this belief with theoretical results. However, they
do not pinpoint the source of increased abstract information and do not explain the observation of many
people that continuation-passing confuses some conventional data flow analyses.

To study the impact of the CPS transformation on
program analysis, we derive three canonical data flow
analyzers for the core of an applicative higher-order
programming language. The first analyzer is based on
a direct semantics of the language, the second on a
continuation-semantics of the language, and the last
on the direct semantics of CPS terms. All analyzers
compute the control flow graph of the source program
and hence our results apply to a large class of data flow
analyses. A comparison of the information gathered by
our analyzers establishes the following points:

1. The results of a direct analysis of a source program

are incomparable to the results of an analysis of
the equivalent CPS program. In other words, the
translation of the source program to a CPS version may increase or decrease static information.

\Lambda Supported in part by NSF grant CCR 89-17022 and by Texas
ATP grant 91-003604014. Also partially supported by Arpa
grant 8313, issued by ESD/AVS under Contract No. F196228-
91-C-0168 under the direction of Robert Harper and Peter Lee.

yTemporary address: School of Computer Science, Carnegie

Mellon University, Pittsburgh, PA 15213.

The gain of information occurs in non-distributive
analyses and is solely due to the duplication of the
analysis of the continuation. The loss of information is due to the confusion of distinct procedure
returns.

2. The analyzer based on the continuation semantics

produces more accurate results than both direct
analyzers, but again only in non-distributive analyses due to the duplication of continuations along
every execution path. However, when the analyzer
explicitly accounts for looping constructs, the results of the semantic-CPS analysis are no longer
computable.

In view of these results, we argue that, in practice, a
direct data flow analysis that relies on some amount of
duplication would be as satisfactory as a CPS analysis.

1 Compiling with CPS
Many compilers for higher-order applicative languages
(Scheme, ML, Common Lisp) map source programs to
programs in continuation-passing style (CPS) [1, 10,
11, 17]. Compiler writers believe that the intermediate
representation based on CPS eases the production of
code and facilitates optimizations. Numerous people
also argue that the CPS transformation increases the
precision of the data flow analysis that is necessary for
advanced optimizations [2, 3, 5, 6, 16].1

Even though CPS programs are widely accepted as
an advantageous intermediate representation, few compiler writers can pinpoint the advantages of the CPS
representation over other intermediate representations.

1Researchers often express this view in informal discussions
as opposed to formal papers. We discussed and re-confirmed
this idea with, among others, Charles Consel, and Olivier Danvy
at LFP '92 [June 92], following the presentation of our paper on
equational reasoning about programs in CPS [14]; in an email exchange with Geoffrey Burn and Juarez Filho [July 92]; in further
discussions at POPL '93 with Daniel Weise; in email discussions
with Kelsey [July 93] and Shivers [May 93]; and in discussions
with Burn at FPCA '93 [June 93].

Page 1

In an attempt to understand the principles of compiling
with continuations and to determine its crucial properties, we recently found two important results:

1. The theory of equational manipulations of programs in CPS based on the fi rule of the lambda
calculus has a simple counterpart for source programs; in other words, whatever optimizations
are expressible via fi-steps on CPS programs,
can equally well be formulated for source programs [14].

2. The code generation phase of a non-optimizing

CPS compiler only requires that the intermediate
representation be normalized according to simple
transformations. It is unnecessary to perform a
full CPS translation [7].

Prompted by conversations with G. Burn [July 92] and
with H. Boehm [August 92], and by the observation
that the CPS transformation obscures some obvious
properties of programs, we started to investigate the
exact effect of the CPS program representation on the
data flow analysis of programs. Our answer rejects the
common belief that the CPS transformation is useful
for practical analyses.

The organization of the rest of this paper is as follows. The next section introduces the syntax and semantics of a typical higher-order language. Section 3
presents the CPS transformation. In Section 4, we derive various data flow analyzers from the standard semantics using the method of abstract interpretation.
Section 5 presents all the formal theorems, which are
discussed from a practical perspective in Section 6. For
full proofs, we refer the reader to the technical report
version of our paper [15].

2 \Lambda : Syntax and Semantics
Our source language is a simple extension of the language \Lambda  of the *-calculus. It corresponds to the core of
typical higher-order languages like Scheme, Lisp, and
ML. The set of terms includes values, applications, letexpressions, and conditionals; the set of syntactic values contains numerals (n 2 Z), variables (x 2 Var),
primitive procedures, and user-defined procedures:

M ::= V j (M M ) j (let (x M ) M ) j (if0 M M M )

V ::= n j x j add1 j sub1 j (*x:M )

Informally, the semantics of the language is as follows. All procedures are call-by-value, the evaluation
of (let (x M1) M2) computes the value of M1 and binds
it to x in M2, and the conditional if0 branches to the
second or third subexpression depending on whether
the first subexpression evaluates to 0 or not. Though
the language is overly simple, it is rich enough for the
primary purpose of the paper.

The analysis of programs assumes that every intermediate result is named, and that all bound variables
in a program are unique. The restricted subset is the
following language:

M ::= V

j (let (x V ) M )
j (let (x (V V )) M )
j (let (x (if0 V M M )) M )
V ::= n j x j add1 j sub1 j (*x:M )

The assumptions simplify the semantics and the derivation of the data flow analyzers without restricting the
set of valid programs: every term in \Lambda  has a semantically equivalent normal form in the restricted subset.
For example, the code fragment (f (let (x 1) (g x)))
becomes:

(let (x1 1) (let (x2 (g x1)) (let (x3 (f x2)) x3))):
In general, the normalization process uses the reductions that we identified in previous work as the Areductions [7, 14].2

The semantics of the restricted subset of \Lambda  is specified by the two predicates M and app defined in Figure 1. It is straightforward to show that M is a partial
function from terms, environments, and stores to answers. The environment is a finite table that maps the
free variables to locations; the store is a finite table
that maps locations to values.3 An answer is a pair
that consists of a run-time value and a store. The set
of run-time values consists of numbers and closures.
A closure is one of the procedure tags inc and dec,
or a data-structure that contains the text of a userdefined procedure and the environment at the point of
the creation of the closure. When applying a closure
hcl x; M; aei to a value u, we extend the enclosed environment at x with a new location and extend the
store with the value u at the new location. Thus, the
bound variable of a procedure or a block is related to
different locations, one for each invocation of the procedure. The function new takes a variable x, a store

2The normalization process does not affect the results of
the data flow analyzers. Intuitively, the first phase of Anormalization gives every subexpression a name to which the
data flow analyzer can associate information about the expression. Without A-normalization, the analyzer would typically associate a "label" with every expression and attach the information about each expression at the corresponding label [8, 13, 16].
The two treatments are identical but the replacement of labels by variables simplifies the analyzers. The second phase
of A-normalization re-orders the expressions to reflect the order in which the interpreters will traverse them. For example, an expression (add1 (let (x V ) 0)) would be rewritten as
(let (x V ) (add1 0)). Again, the change is transparent to the
(abstract) interpreters since they evaluate both expressions in
the same manner.

3The formulation of the semantics does not require a store,

but the presence of the store simplifies the derivation of data
flow analyzers.

Page 2

Domains: Auxiliary Function:
Ans = Val \Theta  Sto
Env = Var \Gamma !ffi Loc

Sto = Loc \Gamma !ffi Val
Val = Num + Clo
Clo = (Var \Theta  \Lambda  \Theta  Env) + inc + dec

OE : \Lambda (V ) \Theta  Env \Theta  Sto ! Val

OE(n; ae; s) = n

OE(x; ae; s) = s(ae(x))
OE(add1; ae; s) = inc

OE(sub1; ae; s) = dec
OE((*x:M); ae; s) = hcl x; M; aei

M : (\Lambda  \Theta  Env \Theta  Sto) ! Ans

u = OE(V; ae; s)
hV; ae; si M hu; si

u = OE(V; ae; s) hM; ae[x := new(x)]; s[new(x) := u]i M A

h(let (x V ) M); ae; si M A

u1 = OE(V1; ae; s) u2 = OE(V2; ae; s) hu1; u2; si app hu3; s3i hM; ae[x := new(x)]; s3[new(x) := u3]i M A

h(let (x (V1 V2)) M); ae; si M A

u0 = OE(V0; ae; s) hMi; ae; si M hu1; s1i hM; ae[x := new(x)]; s1[new(x) := u1]i M A

h(let (x (if0 V0 M1 M2)) M); ae; si M A i = 1 if u

0 = 0; i = 2 otherwise:

app : (Val \Theta  Val \Theta  Sto) ! Ans

hinc; n; si app h(n + 1); si hdec; n; si app h(n \Gamma  1); si

hM; ae[x := new(x)]; s[new(x) := u]i M A

hhcl x; M; aei; u; si app A

Figure 1: Direct (Store) Interpreter

s, and returns a new location ` from which it is possible to recover x, i.e., new (x; s) = ` 62 dom(s) and
x = new\Gamma 1(`). (For brevity, we will often omit the
second argument to new.)

3 Continuation-Passing Style
A continuation-passing style transformation may be
applied to the interpreter or to the source program.
We distinguish the two approaches by referring to the
first transformation as the semantic-CPS transformation and to the second as the syntactic-CPS transformation. We discuss both possibilities in this section.

3.1 Continuations
A continuation is the control state of an evaluator. For
example, during the evaluation of the procedure call
((*a:N1) 5) in (let (x ((*a:N1) 5)) N2), the evaluator
must remember the evaluation context (let (x [ ]) N2)
of the call as well as the environment ae in which to
evaluate N2. Typically, this information is packaged
in a frame and added to the continuation prior to the
procedure call. The evaluation of the body of the procedure N1 may itself push frames on the control stack.
Thus the continuation ^ can in general be represented
as a list of frames where each frame consists of an evaluation context and an environment [7]:

^ = hE1; ae1i :: \Delta  \Delta  \Delta  :: nil where Ei = (let (xi [ ]) Mi)

3.2 Semantic-CPS Transformation
The CPS interpreter (see Figure 2) maps expressions,
environments, continuations, and stores to answers. It
employs two auxiliary functions: appk , which is the
CPS counterpart of app, and appr, which corresponds
to the "return" operation of an abstract machine. The
latter operation binds the return value to a variable,
restores the environment, pops the control stack, and
jumps to the next instruction.

The direct interpreter and the semantic-CPS interpreter produce the same output.

Lemma 3.1 Let M 2 \Lambda , then hM; ae; si M A iff
hM; ae; nil; si C A.

3.3 Syntactic-CPS Transformation
The transformation of a source program to a program
in CPS uses two mutually recursive functions: F to
transform terms and V to transform values. The function F takes an additional argument k, which is a variable that represents the current continuation.

Definition 3.2. (Syntactic-CPS Transformation)
Let cps(\Lambda ) be the language:

P ::= (k W )

j (let (x W ) P )
j (W W (*x:P ))
j (let (k *x:P ) (if0 W P P ))
W ::= n j x j add1k j sub1k j (*xk:P )

Page 3

Domains:

Ans = Val \Theta  Sto

Env = Var \Gamma !ffi Loc

Sto = Loc \Gamma !ffi Val

Con = (\Lambda (E) \Theta  Env) :: Con + nil

Val = Num + Clo
Clo = (Var \Theta  \Lambda  \Theta  Env) + inc + dec

C : (\Lambda  \Theta  Env \Theta  Con \Theta  Sto) ! Ans

u = OE(V; ae; s) h^; hu; sii appr A

hV; ae; ^; si C A

u = OE(V; ae; s) hM; ae[x := new(x)]; ^; s[new(x) := u]i C A

h(let (x V ) M); ae; ^; si C A

u1 = OE(V1; ae; s) u2 = OE(V2; ae; s) hu1; u2; h(let (x [ ]) M); aei :: ^; si appk A

h(let (x (V1 V2)) M); ae; ^; si C A

u0 = OE(V0; ae; s) hMi; ae; h(let (x [ ]) M); aei :: ^; si C A

h(let (x (if0 V0 M1 M2)) M); ae; ^; si C A i = 1 if u

0 = 0; i = 2 otherwise.

appk : (Val \Theta  Val \Theta  Con \Theta  Sto) ! Ans

h^; h(n + 1); sii appr A

hinc; n; ^; si appk A

h^; h(n \Gamma  1); sii appr A

hdec; n; ^; si appk A

hM; ae[x := new(x)]; ^; s[new(x) := u]i C A

hhcl x; M; aei; u; ^; si appk A

appr : (Con \Theta  Ans) ! Ans

hM; ae[x := new(x)]; ^; s[new(x) := u]i C A

hh(let (x [ ]) M); aei :: ^; hu; sii appr A hnil; Ai appr A

Figure 2: Semantic-CPS Interpreter

where x 2 Vars, k 2 KVars and KVars " Vars = ;.
The CPS transformation uses the functions F and V:

F : \Lambda  ! cps(\Lambda )
Fk[[V ]] = (k V[[V ]])
Fk[[(let (x V ) M )]] = (let (x V[[V ]]) Fk[[M ]])
Fk[[(let (x (V1 V2)) M )]] = (V[[V1]] V[[V2]] *x:Fk[[M ]])
Fk[[(let (x (if0 V0 M1 M2)) M )]] =

(let (k0 *x:Fk[[M ]]) (if0 V[[V0]] Fk0[[M1]] Fk0[[M2]]))

V : \Lambda (V ) ! cps(\Lambda )(W )
V[[n]] = n
V[[x]] = x
V[[add1]] = add1k
V[[sub1]] = sub1k
V[[(*x:M )]] = (*xk:Fk[[M ]])

The evaluation for CPS programs is defined by Mc,
a specialized version of the direct interpreter M [7].
It handles procedures of two arguments and manipulates a larger set of run-time values than the direct interpreter that includes continuations of the form
hco x; P; aei (see Figure 3). The larger set of run-time
values reflects the salient aspect of the CPS transformation: it reifies the continuation of the evaluator to
an object that the program explicitly manipulates.4

4In principle, we could use the direct interpreter M to evaluate CPS programs. However, this choice forces continuations to
be represented as procedures, which is (unrealistic and) unnecessarily confusing for data flow analyzers.

To establish the formal relationship between the behavior of a direct term and the behavior of its CPStransform, we define the function ffi that relates direct
run-time values to their CPS counterparts:

ffi(n) = n
ffi(inc) = inck
ffi(dec) = deck
ffi(hcl x; M; aei) = hcl xk; Fk[[M ]]; aei

We extend ffi to work on stores by applying it to the
value at each location and to answers by applying it to
both the value component and the store component.

The following lemma describes the precise relationship between the semantic-CPS interpreter and
the syntactic-CPS interpreter. The interpreters yield
answers related by ffi; the store resulting from the
syntactic-CPS interpreter will contain additional entries that correspond to continuations.

Lemma 3.3 Let M 2 \Lambda , then:

hM; ae; nil; si C hu1; s1i iff

hFk[[M ]]; ae[k := new(k)]; ffi(s)[new(k) := stop]i
Mc hffi(u1); ffi(s1)[new(k1) := ^1; new(k2) := ^2; . . .]i:

Together with Lemma 3.1, this result also relates the
syntactic-CPS interpreter to the direct one.

Page 4

Domains: Auxiliary Function:
Ans = Val \Theta  Sto
Env = Var \Gamma !ffi Loc

Sto = Loc \Gamma !ffi Val
Val = Num + Clo + Con
Clo = (Var \Theta  KVar \Theta  cps(\Lambda ) \Theta  Env)

+ inck + deck
Con = (Var \Theta  cps(\Lambda ) \Theta  Env) + stop

OEc : cps(\Lambda )(W ) \Theta  Env \Theta  Sto ! Val

OEc(n; ae; s) = n

OEc(x; ae; s) = s(ae(x))
OEc(add1k; ae; s) = inck

OEc(sub1k; ae; s) = deck
OEc((*xk:P ); ae; s) = hcl xk; P; aei

Mc : (cps(\Lambda ) \Theta  Env \Theta  Sto) ! Ans

^ = s(ae(k)) u = OEc(W; ae; s) h^; hu; sii apprc A

h(k W ); ae; si Mc A

u = OEc(W; ae; s) hP; ae[x := new(x)]; s[new(x) := u]i Mc A

h(let (x W ) P ); ae; si Mc A

u1 = OEc(W1; ae; s) u2 = OEc(W2; ae; s) hu1; u2; hco x; P; aei; si appc A

h(W1 W2 (*x:P )); ae; si Mc A

u0 = OEc(W0; ae; s) hPi; ae[k := new(k)]; s[new(k) := hco x; P; aei]i Mc A

h(let (k *x:P ) (if0 W0 P1 P2)); ae; si Mc A i = 1 if u

0 = 0; i = 2 otherwise:

appc : (Val \Theta  Val \Theta  Val \Theta  Sto) ! Ans

h^; h(n + 1); sii apprc A

hinck; n; ^; si appc A

h^; h(n \Gamma  1); sii apprc A

hdeck; n; ^; si appc A

hP; ae[x := new(x); k := new(k)]; s[new(x) := u; new(k) := ^]i Mc A

hhcl xk; P; aei; u; ^; si appc A

apprc : (Val \Theta  Ans) ! Ans

hP; ae[x := new(x)]; s[new(x) := u]i Mc A

hhco x; P; aei; hu; sii apprc A hstop; Ai apprc A

Figure 3: Syntactic-CPS Interpreter

4 Constant Propagation by Abstract Interpretation

Using well-known ideas from the area of abstract interpretation [4, 8, 13, 16], we now derive a data flow
analyzer from each of the three interpreters. The first
step in the derivation is to associate one location with
each variable that holds the potentially infinite set of
values to which the variable is bound during the evaluation of the program. Second, we approximate these sets
of values so that each label is associated with a finite
number of values. Finally, we modify the interpreters
to detect and recover from all loops when computing
over the universe of approximate values.

4.1 Abstracting Procedures
The first step in the derivation of the data flow analyzers is to limit the number of locations that can be
created during the evaluation of a given program. One
of the simple approximations, known as 0CFA analysis [16], is to associate one location for each variable
and to collect all the values to which the variable is
bound at that location. Formally, we approximate environments ae to ae and stores s to s as follows:

ffl Since each variable is associated with exactly one

location, we can choose that location to be the
variable itself. Thus, if ae = fx1 7! new(x1); . . .g,
ae = fx1 7! x1; . . .g. This approximation of
the environment does not provide any information, that is, we can drop it completely. Thus,
a closure hcl x; M; aei becomes an abstract closure hcl\Psi  x; M i, a continuation hE1; ae1i :: . . . ::
nil becomes E1 :: . . . :: nil, and a continuation hco x; M; aei becomes an abstract continuation hco\Psi  x; M i. It follows that, for each source
program, the sets of abstract closures and abstract
continuations are finite.

ffl For stores s = fnew(x1) 7! u1; . . .g, we first recover the variable associated with each of the locations: fx1 7! u1; . . .g. Then, to obtain the store

Page 5

s, we merge all entries of the form x 7! u1; x 7!
u2; . . . for some x into one entry x 7! fu1; u2; . . .g.

A "collecting semantics" like the above associates a
set with each variable. Intuitively, the larger the set
the less information is available at compile time about
the variable. To formalize this notion of "precision,"
we note that the sets of collected values form a complete lattice ordered by set-inclusion; the least upper
bound operation is set-union. Thus, the relation "is
more precise than" coincides with the lattice ordering.

4.2 Abstracting Integers
Despite the approximations of environments, stores,
closures, and continuations, the "collecting semantics"
still associates an unbounded set of values with a variable because the lattice of collected values contains infinite chains of elements of decreasing precision, e.g.,
; ` f0g ` f0; 1g ` f0; 1; 2g . . .. Since these infinite
chains may cause the analysis to diverge, we approximate sets of numbers to abstract numbers [9]:

; = ?; fng = n; and fn1; n2; . . .g = ?:
At this point, the universe of abstract values consists of abstract numbers and abstract closures. It remains to impose an order v on the abstract values
similar to the order ` on collected values that coincides with the relation "is more precise than." For the
direct and semantic-CPS interpreters, we organize the
abstract values in a lattice that is the product of two
lattices: the first is the traditional lattice N?? for constant propagation [9], and the second is the power set
of abstract closures (ordered by the subset relation)
for control flow analysis [16]. The ordering relation
v and the least upper bound operation t are defined
component-wise. It is easy to check that, if S1 and
S2 are sets of collected values, then S1 ` S2 implies
S1 v S2.

For the syntactic-CPS collecting interpreter, the sets
of values include abstract continuations as well. In that
case, we use a lattice of abstract values that consists
of the product of three lattices: the constant propagation lattice, the power set of abstract closures, and the
power set of abstract continuations.

The ordering of abstract values induces an ordering
on stores. If oe1 and oe2 are abstract stores, then oe1 v oe2
if for every variable x in the domain of oe1, oe1(x) v
oe2(x). The latter ordering induces a component-wise
ordering on abstract answers.

We can now specify collecting interpreters that manipulate abstract values. The interpreters replace add1
and sub1 by add1 \Psi  and sub1 \Psi :

add1 \Psi (?) = ?

add1 \Psi (n) = (n + 1)
add1 \Psi (?) = ?

sub1 \Psi (?) = ?

sub1 \Psi (n) = (n \Gamma  1)
sub1 \Psi (?) = ?

Figures 4, 5, and 6 contain the direct abstract collecting interpreters, the semantic-CPS abstract collecting
interpreter, and the syntactic-CPS abstract collecting
interpreter respectively.

4.3 Correctness
The correctness criterion of an abstract collecting interpreter is that its results approximate the actual execution of the program. For example, if the variable
x gets bound to 5 along any actual execution path,
the abstract collecting interpreter should associate an
abstract value u w h5; ?i with the variable x.

Lemma 4.1 If s v oe, fu1g v u2 and s1 v oe2, then:

1. If hM; ae; si M hu1; s1i, then hM; oei M\Psi  hu2; oe2i.
2. If hM; ae; ^; si C hu1; s1i, then hM; ^; oei C\Psi 

hu2; oe2i.

3. If hP; ae; si Mc hu1; s1i, then hP; oei M\Psi c hu2; oe2i.

4.4 Termination
Interpreted na"ively, the specifications for the abstract
collecting interpreters define partial functions that diverge on some inputs, and hence are not data flow algorithms. However, this is not a problem, since it is
possible to detect all loops in the derivations. More
precisely, assume we have the following fragment of a
derivation tree:

. . .
hMj; oeji M\Psi  ?
. . .
hMi; oeii M\Psi  ?
. . .
hM1; oe1i M\Psi  ?

The evaluation of M1 in store oe1 requires the value of
Mi in store oei, which in turn requires the value of Mj in
store oej, and so on. If the above fragment of the derivation is indeed infinite, then one of the M 's must be repeated infinitely often as the abstract syntax tree of the
program has only a finite number of subtrees. Thus,
without loss of generality, let M1 = Mi = Mj = M . By
inspection of the direct abstract collecting interpreter,
oe1 . . . v oei . . . v oej . . .. As the lattice of abstract stores
does not have any infinite ascending chains, one of the
oe's in the sequence must be repeated: oei = oej = oe.
Thus, all loops will result in two identical proof goals.

Having detected a loop, we return the least precise
value paired with the current store. Thus, if the arguments hM; oei have already been considered, the direct interpreter returns the answer hh?; CL?i; oei where
CL? is the set of all abstract closures in the program. Similarly, the semantic-CPS interpreter returns

Page 6

Domains: Auxiliary Functions:
Ans = Val \Theta  Sto

Sto = Var \Gamma !ffi Val
Val = Num \Theta  P(Clo)
Clo = (Var \Theta  \Lambda ) + inc + dec

OE\Psi  : \Lambda (V ) \Theta  Sto ! Val

OE\Psi (n; oe) = hn; ;i

OE\Psi (x; oe) = oe(x)
OE\Psi (add1; oe) = h?; fincgi

OE\Psi (sub1; oe) = h?; fdecgi
OE\Psi ((*x:M); oe) = h?; fhcl\Psi  x; Migi

M\Psi  : (\Lambda  \Theta  Sto) ! Ans

u = OE\Psi (V; oe)
hV; oei M\Psi  hu; oei

u = OE\Psi (V; oe) hM; oe[x := oe(x) t u]i M\Psi  A

h(let (x V ) M); oei M\Psi  A

u1 = OE\Psi (V1; oe) u2 = OE\Psi (V2; oe) hu1; u2; oei app\Psi  hu3; oe3i hM; oe3[x := oe3(x) t u3]i M\Psi  A

h(let (x (V1 V2)) M); oei M\Psi  A

u0 = OE\Psi (V0; oe) hMi; oei M\Psi  hu1; oe1i hM; oe1[x := oe1(x) t u1]i M\Psi  A

h(let (x (if0 V0 M1 M2)) M); oei M\Psi  A i = 1 if u

0 = h0; ;i; i = 2 if h0; ;i 6v u0:

h0; ;i @ u0 = OE\Psi (V0; oe) hM1; oei M\Psi  hu1; oe1i hM2; oei M\Psi  hu2; oe2i hM; (oe1 t oe2)[x := (oe1 t oe2)(x) t (u1 t u2)]i M\Psi  A

h(let (x (if0 V0 M1 M2)) M); oei M\Psi  A

app\Psi  : (Val \Theta  Val \Theta  Sto) ! Ans

hcl1; u; oei app1 \Psi  A1 . . . hcln; u; oei app1\Psi  An

hhn; fcl1; . . . ; clngi; u; oei app\Psi  Fi=1;n Ai

app1\Psi  : (Clo \Theta  Val \Theta  Sto) ! Ans

u = hadd1\Psi (n); ;i
hinc; hn; CLi; oei app1\Psi  hu; oei

u = hsub1\Psi (n); ;i
hdec; hn; CLi; oei app1\Psi  hu; oei

hM; oe[x := oe(x) t u]i M\Psi  A

hhcl\Psi  x; Mi; u; oei app1 \Psi  A

Figure 4: Direct Abstract Collecting Interpreter

hh?; CL?i; oei to the continuation ^. If the syntacticCPS interpreter detects that the arguments hP; oei have
already been considered, it returns hh?; CL?; K ?i; oei
where K? is the set of all abstract continuations
hco\Psi  x; P i in the program.

In the remainder of the paper, we will use "abstract
collecting interpreter" or "data flow analysis" to refer
to the terminating versions of the interpreters that detect loops as above.

5 Formal Relationships
After deriving the data flow analyzers, we turn our attention to the relationship between them. For the connection between the direct and syntactic-CPS analyses,
we need an abstract version of the function ffi that maps

abstract direct values to abstract CPS values:

ffi\Psi (hn; fcl1; . . . ; cligi) = hn; fV\Psi (cl 1); . . . ; V\Psi (cl i)g; ;i
V\Psi (hcl\Psi  x1; M1i) = hcl\Psi  x1k1; Fk1[[M1]]i
V\Psi (inc) = inck
V\Psi (dec) = deck

The application of ffi\Psi  to stores and answers is pointwise and component-wise respectively.

5.1 Direct vs Syntactic-CPS
The first theorem establishes that the direct analysis
of M may be more precise than the analysis of Fk[[M ]].

Theorem 5.1 There exists M 2 \Lambda  and oe 2 Sto such
that:

ffl hM; oei M\Psi  hu1; oe1i,
ffl hFk[[M ]]; ffi\Psi (oe)[k := h?; ;; fstopgi]i M\Psi c hu2; oe2i,

Page 7

Domains:

Ans = Val \Theta  Sto

Sto = Var \Gamma !ffi Val

Con = \Lambda (E) :: Con + nil

Val = Num \Theta  P(Clo)
Clo = (Var \Theta  \Lambda ) + inc + dec

C\Psi  : (\Lambda  \Theta  Con \Theta  Sto) ! Ans

u = OE\Psi (V; oe) h^; hu; oeii appr\Psi  A

hV; ^; oei C\Psi  A

u = OE\Psi (V; oe) hM; ^; oe[x := oe(x) t u]i C\Psi  A

h(let (x V ) M); ^; oei C\Psi  A

u1 = OE\Psi (V1; oe) u2 = OE\Psi (V2; oe) hu1; u2; (let (x [ ]) M) :: ^; oei appk\Psi  A

h(let (x (V1 V2)) M); ^; oei C\Psi  A

u0 = OE\Psi (V0; oe) hMi; (let (x [ ]) M) :: ^; oei C\Psi  A

h(let (x (if0 V0 M1 M2)) M); ^; oei C\Psi  A i = 1 if u

0 = h0; ;i; i = 2 if h0; ;i 6v u0:

h0; ;i @ u0 = OE\Psi (V0; oe) hM1; (let (x [ ]) M) :: ^; oei C\Psi  A1 hM2; (let (x [ ]) M) :: ^; oei C\Psi  A2

h(let (x (if0 V0 M1 M2)) M); ^; oei C\Psi  A1 t A2

appk\Psi  : (Val \Theta  Val \Theta  Con \Theta  Sto) ! Ans

hcl1; u; ^; oei appk1\Psi  A1 . . . hcln; u; ^; oei appk1\Psi  An

hhn; fcl1; . . . ; clngi; u; ^; oei appk\Psi  Fi=1;n Ai

appk1\Psi  : (Clo \Theta  Val \Theta  Con \Theta  Sto) ! Ans

u = hadd1\Psi (n); ;i h^; hu; oeii appr\Psi  A

hinc; hn; CLi; ^; oei appk1\Psi  A

u = hsub1\Psi (n); ;i h^; hu; oeii appr\Psi  A

hdec; hn; CLi; ^; oei appk1\Psi  A

hM; ^; oe[x := oe(x) t u]i C\Psi  A
hhcl\Psi  x; Mi; u; ^; oei appk1\Psi  A

appr\Psi  : (Con \Theta  Ans) ! Ans

hM; ^; oe[x := oe(x) t u]i C\Psi  A
h(let (x [ ]) M) :: ^; hu; oeii appr\Psi  A hnil; Ai appr\Psi  A

Figure 5: Semantic-CPS Abstract Collecting Interpreter

ffl ffi\Psi (u1) v u2, and for each variable in the domain

of oe1, ffi\Psi (oe1(x)) v oe2(x).

Proof. Let M be (let (a1 (f 1)) (let (a2 (f 2)) a2)),
and let:

oe = fa1 7! h?; ;i;

a2 7! h?; ;i;
f 7! h?; fhcl\Psi  x; xigi;
x 7! h?; ;ig:

It is straightforward to calculate that the result of the
direct abstract collecting interpreter is A1 = hu1; oe1i
where:

u1 = h?; ;i
oe1 = fa1 7! h1; ;i;

a2 7! h?; ;i;
f 7! h?; fhcl\Psi  x; xigi;
x 7! h?; ;ig:

For the analysis of the CPS version, we have that:

Fk[[M ]] = (f 1 (*a1:(f 2 (*a2:(k a2)))))

oe0 = fa1 7! h?; ;; ;i;

a2 7! h?; ;; ;i;
f 7! h?; fhcl\Psi  xk1; (k1 x)ig; ;i;
x 7! h?; ;; ;i;
k 7! h?; ;; fstopgig:

The syntactic-CPS abstract collecting interpreter produces the answer A2 = hu2; oe2i where:

u2 = h?; CL?; K?i
oe2 = fa1 7! h?; ;; ;i;

a2 7! h?; ;; ;i;

Page 8

Domains: Auxiliary Function:
Ans = Val \Theta  Sto

Sto = Var \Gamma !ffi Val
Val = Num \Theta  P(Clo) \Theta  P(Con)
Clo = (Var \Theta  KVar \Theta  cps(\Lambda ))

+ inck + deck
Con = (Var \Theta  cps(\Lambda )) + stop

OE\Psi c : cps(\Lambda )(W ) \Theta  Sto ! Val

OE\Psi c (n; oe) = hn; ;; ;i

OE\Psi c (x; oe) = oe(x)
OE\Psi c (add1k; oe) = h?; finckg; ;i

OE\Psi c (sub1k; oe) = h?; fdeckg; ;i
OE\Psi c ((*xk:P ); oe) = h?; fhcl\Psi  xk; P ig; ;i

M\Psi c : (cps(\Lambda ) \Theta  Sto) ! Ans

^ = oe(k) u = OE\Psi c (W; oe) h^; hu; oeii appr\Psi c A

h(k W ); oei M\Psi c A

u = OE\Psi c (W; oe) hP; oe[x := oe(x) t u]i M\Psi c A

h(let (x W ) P ); oei M\Psi c A

u1 = OE\Psi c (W1; oe) u2 = OE\Psi c (W2; oe) hu1; u2; h?; ;; hco\Psi  x; P ii; oei app\Psi c A

h(W1 W2 (*x:P )); oei M\Psi c A

u0 = OE\Psi c (W0; oe) hPi; oe[k := oe(k) t h?; ;; hco\Psi  x; P ii]i M\Psi c A

h(let (k *x:P ) (if0 W0 P1 P2)); oei M\Psi c A i = 1 if u

0 = h0; ;; ;i; i = 2 if h0; ;; ;i 6v u0:

h0; ;; ;i @ u0 = OE\Psi c (W0; oe)
hP1; oe[k := oe(k) t h?; ;; hco\Psi  x; P ii]i M\Psi c A1 hP2; oe[k := oe(k) t h?; ;; hco\Psi  x; P ii]i M\Psi c A2

h(let (k *x:P ) (if0 W0 P1 P2)); oei M\Psi c A1 t A2

app\Psi c : (Val \Theta  Val \Theta  Val \Theta  Sto) ! Ans

hcl1; u; ^; oei app\Psi 1c A1 . . . hcln; u; ^; oei app\Psi 1c An

hhn; fcl1; . . . ; clng; Ki; u; ^; oei app\Psi c Fi=1;n Ai

app\Psi 1c : (Clo \Theta  Val \Theta  Val \Theta  Sto) ! Ans

u = hadd1\Psi (n); ;; ;i h^; hu; oeii appr\Psi c A

hinck; hn; CL; Ki; ^; oei app\Psi 1c A

u = hsub1\Psi (n); ;; ;i h^; hu; oeii appr\Psi c A

hdeck; hn; CL; Ki; ^; oei app\Psi 1c A

hP; oe[x := oe(x) t u; k := oe(k) t ^]i M\Psi c A

hhcl\Psi  xk; P i; u; ^; oei app\Psi 1c A

appr\Psi c : (Val \Theta  Ans) ! Ans

h^1; hu; oeii appr\Psi 1c A1 . . . h^n; hu; oeii appr\Psi 1c An

hhn; CL; f^1; . . . ; ^ngi; hu; oeii appr\Psi c Fi=1;n Ai

appr\Psi 1c : (Con \Theta  Ans) ! Ans

hP; oe[x := oe(x) t u]i M\Psi c A
hhco\Psi  x; P i; hu; oeii appr\Psi 1c A hstop; Ai appr\Psi 1c A

Figure 6: Syntactic-CPS Abstract Collecting Interpreter

f 7! h?; fhcl\Psi  xk1; (k1 x)ig; ;i;
x 7! h?; ;; ;i;
k1 7! h?; ;; f hco\Psi  a1; (f 2 (*a2:(k a2)))i;

hco\Psi  a2; (k a2)igi
k 7! h?; ;; fstopgi]

The analysis of the source program is more precise since
it determines that the variable a1 is constant (=1),
while the analysis of the CPS program fails to produce
any information about a1.

Page 9

Our second theorem states that the direct analysis
of a program may also give less information than the
syntactic-CPS analysis. Together with the previous
theorem, the result establishes that the direct analysis
of a source program is incomparable to the syntacticCPS analysis.

Theorem 5.2 There exists a term M 2 \Lambda  such that:

ffl hM; oei M\Psi  hu1; oe1i,
ffl hFk[[M ]]; ffi\Psi (oe)[k := h?; ;; fstopgi]i M\Psi c hu2; oe2i,
ffl ffi\Psi (u1) w u2, and for each variable x in the domain of oe1, ffi\Psi (oe1(x)) w oe2(x).

Proof. To illustrate the principle, we present two cases
in which the analysis of the CPS program yields more
information than the direct analysis of the source program.

For the first case, take:

M = (let (a1 (if0 x 0 1))

(let (a2 (if0 a1 (+ a1 3) (+ a1 2)))

a2))
oe = fa1 7! h?; ;i; a2 7! h?; ;i; x 7! h?; ;ig

where (+ a1 3) and (+ a1 2) are the obvious abbreviations. The direct analysis of the term, not knowing
which branch to take, merges the abstract values of 0
and 1 at the variable a1 and hence loses all information
about a2. In contrast, the analysis of:

Fk[[M ]] =
(let (k0 *a1:Fk[[(let (a2 (if0 a1 (a1 + 3) (a1 + 2))) a2)]])

(if0 x (k0 0) (k0 1)))

analyzes both (k0 0) and (k0 1) in a store that maps
k0 7! hco\Psi  a1; . . .i. The analysis of each execution
path determines that the abstract value of a2 is h3; ;; ;i,
which improves on the direct analysis.

For the second case, take M :

M = (let (a1 (f 3))

(let (a2 (if0 a1 5 (if0 (sub1 a1) 5 6)))

a2))
oe = fa1 7! h?; ;i

a2 7! h?; ;i;
f 7! h?; fhcl\Psi  d0; 0i; hcl\Psi  d1; 1igi
d0 7! h?; ;i;
d1 7! h?; ;ig

where we have not named the results of (sub1 a1) and
(if0 (sub1 a1) 5 6) to avoid clutter. The direct analysis
of M begins by applying both closures bound to f to
the abstract value of 3. The analysis then combines
the results of these two applications associating h0; ;it

h1; ;i = h?; ;i with a1. As a consequence, the analysis
loses all information about the value of a2. In contrast,
the analysis of the CPS version:

(f 3 (*a1:Fk[[(let (a2 (if0 a1 5 (if0 (sub1 a1) 5 6))) a2)]]))
duplicates the continuation (*a1: . . .) when evaluating
the application of each of the closures bound to f. The
analysis determines that the value of a2 is h5; ;; ;i along
each execution path and hence improves on the analysis
of the source program.

5.2 Direct vs Semantic-CPS
The character of the relationship between the direct
and the semantic-CPS analysis depends on a key property of analyses.

Definition 5.3. (Distributivity) An analysis is
distributive5 if for all ^, Ai, and n:

h^; Fi=1;n Aii appr \Psi  Af iff

h^; A1i appr \Psi  B1 . . . h^; Ani appr\Psi  Bn
and Af = Fi=1;n Bn:

When Distributivity does not hold, e.g., for constant
propagation [9], the semantic-CPS data flow analyzer
may gain information by duplicating the continuation
along every execution path as in the right hand side of
the condition. Otherwise, the results of the analyses
are identical.

Theorem 5.4 Let M 2 \Lambda , then hM; ^; oei C\Psi  A1 if
and only if:

ffl hM; oei M\Psi  A2, and h^; A2i appr \Psi  A3,

and A1 v A3, or

ffl if the Distributivity condition holds,

hM; oei M\Psi  A2, and h^; A2i appr\Psi  A1.

5.3 Syntactic-CPS vs Semantic-CPS
The semantic-CPS analyzer may yield more precise results than the syntactic-CPS analyzer since the latter
may confuse the continuations collected at a given label.

Theorem 5.5 Let M 2 \Lambda , then:

hM; nil; oei C\Psi  A1 iff
hFk[[M ]]; ffi\Psi (oe)[k := h?; ;; fstopgi]i M\Psi c A2

where ffi\Psi (A1) v A2.

5In the traditional framework, the lattice is usually inverted
and the Distributivity condition is stated using the greatest lower
bound u. In our case, the condition should be called Continuity
but, to avoid confusion, we use the standard terminology.

Page 10

6 Discussion of the Results
In summary, our theorems show that:

1. The syntactic-CPS analyzer may confuse some

continuations, i.e., may analyze an infeasible path,
and also duplicates the analysis of continuations
along every execution path, i.e., may gather
more information than the direct analyzer in nondistributive analyses.

2. The semantic-CPS analyzer does not suffer from

the false return problem and increases the collected information in non-distributive analyses by
the duplication of the analysis of continuations.

In the remainder of this section, we discuss each of the
properties of the CPS analyzers in detail.

6.1 False Returns
In practice, many analyses do indeed confuse continuations when applied to CPS programs. For example,
Shivers's 0CFA analysis of CPS programs [16] merges
distinct control paths unnecessarily. Shivers did not
relate the problem to CPS but his example [16:p.33] is
essentially the example for Theorem 5.1.

Given our result, we can explain how the CPS transformation confuses some data flow analyzers that associate (approximate) information with program points.
Because the CPS transformation reifies the continuation to a value that the program manipulates explicitly, the analysis of a CPS program is obligated to collect, at each variable k, the set of continuations that
k may refer to during the execution of the program.
Thus, when considering a return, i.e., a call (k W ),
the analysis applies each of the continuations bound to
k and merges the results. In contrast, the analysis of
the source program and the semantic-CPS analysis do
not collect continuations, but only consider the current
continuation at any program point.

6.2 Duplication
The gain of information in semantic-CPS analyzers is
folklore knowledge. Nielson [13] proved that, for a
small imperative language, the semantic-CPS analysis computes the MOP (meet over all paths) solution
and the direct analysis computes the less precise MFP
(maximum fixed point) solution; Filho and Burn [6]
improved the abstract interpretations of typed call-byname languages using the CPS transformation. Our
result shows that the gain in all cases is entirely due
to the duplication of the analysis of the continuation
along different execution paths. In the remainder of
this section, we consider the impact of this duplication
on the computability and cost of the analysis.

Intuitively, the difference between the direct semantics and the CPS analyses is that the former merges all
the values of an expression before analyzing the continuation and the latter apply the continuation to each
of the values of an expression and merge the results.
Therefore, the duplication of the analysis of the continuation depends on the number of values an expression
may have.

Thus far, every expression in our language had only
a finite number of values: the analysis of a conditional
expression may proceed along two paths, and the analysis of a procedure call may proceed along some finite
number of paths, one for each abstract closure that the
term in function position evaluates to. Consequently,
at each conditional and at each call site, the continuation may be duplicated along each of the possible
paths, at an overall exponential cost in the analysis.

In a realistic language, the duplication of the continuation causes the computation of the result of the
CPS analysis to become undecidable. To illustrate this
point, we assume an extension of the language with an
explicit looping construct and a sufficiently rich set of
primitives.

Let the construct loop be an infinite loop whose exact collecting semantics returns the infinite set of values f0; 1; 2; . . .g.6 The extensions of the direct and
semantic-CPS analyzers are:

ui = hi; ;i Ai = hui; oei
hloop; oei M\Psi  (Fi=0;1 Ai)

ui = hi; ;i Ai = hui; oei h^; Aii appr\Psi  Bi

hloop; ^; oei C\Psi  (Fi=0;1 Bi)

In the direct interpreter, each ui is an abstract number
hi; ;i and the least upper bound of the set fui j i * 0g is
h?; ;i. In the semantic-CPS case, the computation ofF

i=0;1 Bi is undecidable. The proof is an adaptationof Kam and Ullman's proof [9] that it is undecidable

to compute the MOP solution for a general program in
an arbitrary monotone framework.

6.3 Conclusion
In conclusion, a practical analysis based on the CPS
transformation should not perform any duplication
when the analysis is distributive since the duplication would not yield more precise answers. In nondistributive cases, a CPS analysis should limit the
amount of duplication for both computability and efficiency reasons. When the analysis of a CPS program does not perform any duplication, the net effect
of transforming the program to CPS is to obscure the
fact that there is only one control stack at any point

6The construct loop corresponds to the following program
fragment `x := 0; while true x := x + 1'.

Page 11

during a computation. Hence a more practical alternative is to combine heuristic in-lining algorithms with a
direct-style analysis.

Acknowledgements
We thank Hans Boehm for discussions about the undecidability of the semantic-CPS analysis, and Geoffrey
Burn, Bruce Duba, Juarez Filho, Cormac Flanagan,
John Greiner, Bob Harper, Nevin Heintze, Peter Lee,
and Frank Pfenning for comments on an earlier version
of this paper and for discussions of the results.

References

[1] Appel, A. Compiling with Continuations. Cambridge University Press (1992).

[2] Bondorf, A. Improving binding times without explicit CPS-conversion. In Proceedings of the ACM
Conference on Lisp and Functional Programming
(1992) 1-10.

[3] Consel, C. and Danvy, O. For a better support

of static data flow. In Proceedings of the Conference on Functional Programming Languages and
Computer Architecture (1991) 496-519.

[4] Cousot, P. and Cousot, R. Abstract interpretation: A unified lattice model for static analysis
of programs by construction of approximation of
fixpoints. In Conference Record of the 4th ACM
Symposium on Principles of Programming Languages (1977) 238-252.

[5] Filho, J. Muylaert. Improving abstract interpretations with CPS-translation. (1993). Unpublished
Manuscript.

[6] Filho, J. Muylaert and Burn, G. Continuation

passing transformation and abstract interpretation. In Burn, G., Gay, S., and Ryan, M., editors, Proceedings of the First Imperial College,
Department of Computing, Workshop on Theory
and Formal Methods (1993).

[7] Flanagan, C., Sabry, A., Duba, B. F., and

Felleisen, M. The essence of compiling with continuations. In Proceedings of the ACM Sigplan
Conference on Programming Language Design and
Implementation (1993) 237-247.

[8] Hudak, P. and Young, J. Collecting interpretations of expressions. ACM Transactions on Programming Languages and Systems, 13, 2 (April
1991) 269-290.

[9] Kam, J.B. and Ullman, J. D. Monotone data flow

analysis frameworks. Acta Informatica, 7 (1977)
305-317.

[10] Kelsey, R. and Hudak, P. Realistic compilation by

program transformation. In Conference Record of
the 16th ACM Symposium on Principles of Programming Languages (1989) 281-292.

[11] Kranz, D., Kelsey, R., Rees, J., Hudak, P.,

Philbin, J., and Adams, N. Orbit: An optimizing compiler for Scheme. In Proceedings of the
ACM Sigplan Symposium on Compiler Construction, Sigplan Notices, 21, 7 (1986) 219-233.

[12] Marlowe, T.J. and Ryder, B.G. Properties of data

flow frameworks: A unified model. Acta Informatica (1990).

[13] Nielson, F. A denotational framework for data

flow analysis. Acta Informatica, 18 (1982) 265-
287.

[14] Sabry, A. and Felleisen, M. Reasoning about

programs in continuation-passing style. Lisp and
Symbolic Computation, 6, 3/4 (1993) 289-360.
Also in Proceedings of the ACM Conference on
Lisp and Functional Programming, 1992, and
Technical Report 92-180, Rice University.

[15] Sabry, A. and Felleisen, M. Is ContinuationPassing Useful for Data Flow Analysis? Technical
Report TR94-223, Rice University (1994).

[16] Shivers, O. Control-Flow Analysis of HigherOrder Languages or Taming Lambda. PhD thesis,
Carnegie Mellon University (1991).

[17] Steele, G. L. Rabbit: A Compiler for Scheme. MIT

AI Memo 474, Massachusetts Institute of Technology (1978).

Page 12