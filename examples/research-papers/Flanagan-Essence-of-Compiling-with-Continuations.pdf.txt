

The Essence of Cornpiling with Continuations
Cormac Flanagan* Amr Sabry'k Bruce F. Duba Matthias Felleisen*

Departmentof Computer Science

Rice University
Houston, TX 77251-1892

Abstract
In order to simplify the compilation process, many compilers for higher-order languages use the continuationpassing style (CPS) transformation in a first phase to
generate an intermediate representation of the source
program. The salient aspect of this intermediate fcmm
is that all procedures take an argument that represents

the rest of the computation (the "continuation"). Since
the naive CPS transformation considerably increases
the size of programs, CPS compilers perform reductions

to produce a more compact intermediate representation.
Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat

continuations specially in order to optimize the interpretation of continuation parameters.

A thorough analysis of the abstract machine for CPS
terms shows that the actions of the code generator 2nvert the naive CPS translation step. Put differently,
the combined effect of the three phases is equivalent
to a source-to-source transformation that simulates the
compaction phase. Thus, fully developed CPS compilers do not need to employ the CPS transformation but

can achieve the same results with a simple source-level
transformation.

1 Compiling with Continuations
A number of prominent compilers for applicative higherorder programming languages use the language of

*Supported in part by NSF grants CCR 89-17022 and ~CCR
91-22518 and Texas ATP grant 91-003604014.

Permission to copy without fee all or part of this material is
granted provided that the copias are not made or distributed for
direct commercial advantage, the ACM copyright notice and the
title of the publication and its date appear, and notice is given
that copying is by permission of the Association for Computing

Machinery. To copy otherwisa, or to rapublish, requires a fee
and/or specific permission.
ACM-SlGPLAN-PLDl-6 /93/Albuquerque, N.M.
01993 ACM 0-89791 -598 -41931000610237 ...$1 .50

continuation-passing style (CPS) terms as their intermediate representation for programs [2, 14, 18, 19]. This
strategy apparently offers two major advantages. First,

Plotkin [16] showed that the A-value calculus based on
the ~-value rule is an operational semantics for the
source language, that the conventional jull A-calculus

is a semantics for the intermediate language, and, most
importantly, that the A-calculus proves more equations
between CPS terms than the &-calculus does between

corresponding terms of the source language. ~anslated
into practice, a compiler can perform more transformations on the intermediate language than on the source
language [2:4-5]. Second, the language of CPS terms is
basically a stylized assembly language, for which it is
easy to generate actual assembly programs for different
machines [2, 13, 20]. In short, the CPS transformation
provides an organizational principle that simplifies the

construction of compilers.

To gain a better understanding of the role that the
CPS transformation plays in the compilation process,
we recently studied the precise connection between the

Au-calculus for source terms and the Lcalculus for CPS
terms. The result of this research [17] was an extended

A.-calculus that precisely corresponds to the A-calculus of the intermediate CPS language and that is still
semantically sound for the source language. The extended calculus includes a set of reductions, called the
A-reductions, that simplify source terms in the same
manner as realistic CPS transformations simplify the
output of the naive transformation. The effect of these

reductions is to name all intermediate results and to
merge code blocks across declarations and conditionals.

Direct compilers typically perform these reductions on
an ad hoc and incomplete basis.l

The goal of the present paper is to show that the true
purpose of using CPS terms as an intermediate represent ation is also achieved by using A-normal forms. We
base our argument on a formal development of the abstract machine for the intermediate code of a CPS-based

compiler. The development shows that this machine is

1Personal communication: H. Boehm (also [4]), K. D ybvig,
R. Hieb (April 92).

237

M .._,.--

I ~et (z Ml) M,)
I (ifO MI M, M,)
I(MM, . . . Jfn)

I(ofkf, . . . &fn)

V C Values

c c constants
x c Variables
O C Primitive Operations

v ..--..-- CIZI(AZ1. .,%M)

Figure 1: Abstract Syntax of Core Scheme ( CS)

identical to a machine for A-normal forms. Thus, the
back end of an A-normal form compiler can employ the
same code generation techniques that a CPS compiler

uses. In short, A-normalization provides an organizational principle for the construction of compilers that

combines various stages of fully developed CPS compilers in one straightforward transformation.

The next section reviews the syntax and semantics of
a typical higher-order applicative language. The following section analyses CPS compilers for this language.
Section 4 introduces the A-reductions and describes Anormal form compilers. Section 5 proves the equiva,
lence between A-normal form compilers and realistic

CPS compilers. The benefits of using A-normal form
terms as an intermediate representation for compilers is
the topic of Section 6. The appendix includes a linear
A-normalization algorithm.

2 Core Scheme
The source language is a simple higher-order applicative

language. For our purposes, it suffices to consider the
language of abstract syntax trees that is produced by
the lexical and syntactic analysis module of the compiler: see Figure 1 for the context-free grammar of this
language. The terms of the language are either values or non-values. Values include constants, variables,

and procedures. Non-values include let-expressions

(blocks), conditionals, function applications and primitive operations.2 The sets of constants and primitive

procedures are intentionally unspecified. For our purposes, it is irrelevant whether the language is statically
typed like ML or dynamically typed like Scheme.

The language Core Scheme has the following contextsensitive properties, which are assumed to be checked

by the front-end of the compiler. In the procedure

(Ax, . . . zn, iM) the parameters Z1, . . . . Zn are mutually
distinct and bound in the body M. Similarly, the expression (let (z Ml) Mz) binds z in M2. A variable that

2The language is overly simple but contains all ingredients that
are necessary to generate our result for full ML or Scheme. In particular, the introduction of assignments, and even control operators, is orthogonal to the analysis of the CPS-based compilation
strategy.

is not bound by a A or a let is free; the set of free variables in a term M is FV(M). Like Barendregt [3:ch 2,3],
we identify terms modulo bound variables and we assume that free and bound variables of distinct terms do
not interfere in definitions or theorems.

The semantics of the language is a partial function
from programs to answers. A program is a term with

no free variables and an answer is a member of the
synt attic category of constants. Following conventional

tradition [1], we specify the operational semantics of

Core Scheme with an abstract machine. The machine
we use, the CEK machine [10], has three components: a

control string C, an environment E that includes bindings for all free variables in C, and a continuation 1{
that represents the `(rest of the computation".

The CEK machine changes state according to the
transition function in Figure 2. For example, the state

transition for the block (let (z Ml) ib12 ) starts the evaluation of Ml in the current environment E and modifies
the continuation register to encode the rest of the computation (It x, Mz, E, K). When the new continuation
receives a value, it extends the environment with a value
for z and proceeds with the evaluation of Mz. The remaining clauses have similarly intuitive explanations.

The relation -* is the reflexive transitive closure
of the transition function. The function y constructs
machine values from syntactic values and environments.

The notation E(z) refers to an algorithm for looking up
the value of x in the environment E. The operation

E[xl := V;, . . ..$m := V;] extends the environment E

such that subsequent lookups of Zi return the value Vi".
The object (cl xl . . .xn, M, E) is a closure, a record that

contains the code for M and values for the free variables
of (Axl . . . Zn .M). The partial function 6 abstracts the
semantics of the primitive operations.

The CEK machine provides a model for designing direct compilers [6, 11, 15]. A compiler based on the CEK
machine implements an efficient representation for environments, e.g., displays, and for continuations, e.g., a
stack.3 The machine code produced by such a compiler

3The machine also characterizes compilers for first-order languages, e.g., Fortran. In this case, the creation and deletion of

the environment and continuation components always follows a
stack-like behavior. Hence the machine reduces to a traditional
stack machine.

238

Semantics: Let M G CS,

e?dd(kf) = c if (M, 0, stop)%' (stop, c).

Data Specifications:

S ~ Stated = 6'S X Eravd X 6'Odd I Codci X Vdbed (machine states)
E G Envd = Varhbles ++ Vdtted (environments)v" c valued=

cl(clzl. ..z~, M,E) (machine values)
~ e ~Odd = stop I (ap (..., V*, *, M), E,K)E, K) I (lt z, M, E,K) (continuations)

[ (if J41, M., E,K) I (pr O,(..., V*, @,M,), E,K), K)

Transition Rules:

(V, E, K) +---+ (K, -Y(V, E))
((let (s MI) MQ), E, K) x (Jfl, E, (lt ~, Jfz, E, K))
((ifO,Ml Lfz Jfs), E, K) ++ (MI, E, (if M,, Ms, E, K))
((MM, . . . Mn), E,K) +-+ (M, E, (ap (c, Ml,..., M~), E, K))((OM, M, . ..

Mn), E,K) +--+ (M1, E, (pr 0,(0,M2,..., ~n))E)~))

((it z, M, E, K), V*) _ (M, E[z := V*], K)
((if MI, M2, E, K),0) _ (MI, E,K)
((if MI, Mz, E, K), V") + (M,, E, K) where V* # O
((ape..., ~*, e, M,... ), E, K), T&,) +--+ (M, E,(ap (..., K*, Ul,, C), E,~)), ~))

((ap V*, ~*,..., l), E, K),V;) + (M', .~'[$1 := VI*, . . . . Zn := v;], ~) if V* = (cl ZI . .. T~. Jf', E')
((pr O,(..., V*, o,M ,...), E, K), K$I) t---+ (M, E,(pr 0,(..., K*, K:,, ), E,K)), K))

((pr O,(V*,.. .,c), E, K"), V:) * (K,6(0, Vi*,. ... V;)) if 6(0, VI*, . . . . V:) is defined

Converting syntactic values to machine values:

7(c, E) = C
T(z, E) = E(x)
7((AX1 . ..xn.M). E) = (CIZ,L . ..zn. M,E)

Figure 2: The CEK-machine

realizes the abstract operations specified by the CEK
machine by manipulating these concrete representations
of environments and continuations.

3 CPS Compilers
Several compilers map source terms to a CPS intermediate representation before generating machine code. The
function f [12] in Figure 3 is the basis of CPS transformations used in various compilers [2, 14, 19]. It uses

special A-expressions or continuations to encode the rest
of the computation, thus shifting the burden of maintaining control information from the abstract machine
to the code. The notation (~z. . . .) marks the adntinistrattve A-expressions introduced by the CPS transformation. The primitive operation 0' used in the CPS
language is equivalent to the operation O for the source
language, except that O' takes an extra continuation argument, which receives the result once it is computed.

The transformation f introduces a large number of
administrative ~-expressions. For example, f maps the

code segment

IV~(+
into the CPS term

(+2 2) (let (z 1) (f z)))

((I/c,. ((~k,. (~ 2))

(At,. ((M4. (kq 2))

(Xtz.(+' h f,~z))))))(Xts.((m~

(Qk,. (k, 1))

(Atq.(letJz t~)

((M,. ((II%$.(~ f))

(x,. ((M,, (k,z))

(Ro. (tslb t~))))))
k,)))))
(X7. (+' k t3h))))))

By convention, we ignore the context (M.[ ]) enclosing

all CPS programs.

To decrease the number of administrative ~-

abstractions, realistic CPS compilers include a simplification phase for compacting CPS terms [2:68-69, 14:5-
6, 19:49-51]. For an analysis of this simplification phase,

239

its optimality, and how it can be combined with Y, we
refer the reader to Danvy and Filinski [9] and Sabry

and Felleisen [17]. This phase simplifies administrative
redexes of the form ((~z .P) Q) according to the rule:

((~z.P) Q) + P[z := Q] (P)
The term P[z := Q] is the result of the capture-free
substitution of all free occurrences of z in P by Q; for
example, (Az.zz)[z := (Ay. x)] = (Au.u(~y. z)), Applying the reduction @ to all the administrative redexes in
our previous example produces t]
form term:

cps(N) = (+' (Itl. (let (z 1) (j

2 2)

e following ~-normal
Xtz.(+' k i~f~))z)))
The reduction ~ is strongly-normalizing on the language of CPS terms [17]. Hence, the simplification
phase of a CPS compiler can remove all ~-redexes from

the output of the translation 7.4 After the simplification phase, we no longer need to distinguish between regular and administrative J-expressions, and use
the notation (~. ) for both classes of A-expression.
With this convention, the language of ~-normal forms,

CPS(CS), is the following [17]:

P ::= (k w) (return)

\ (let (z w) P) (btnd)
I (ifO W PI P,) (branch)
[(wkw, . ..wn) (ta~l call)
I (w (A$.P) w, . . . w.) (call)
I(o'kw, . ..wn) (pn?n-clp)
\ (0' (AZ.P) w, . . . Wn) (pram-q)

4The CPS translation of a conditional expression contains
two references to the continuation variable k. Thus, the ~-
normalization phase can produce exponentially larger output.

Modifying the CPS algorithm to avoid duplicating k removes the
potential for exponential growth. The rest of our technical development can be adapted mutatis mutandts.

w ::= c I z I (Akz?l. ..8P)P) (values)
Indeed, this language is typical of the intermediate representation used by CPS compilers [2, 14, 19].

Naive CPS Compilers The abstract machine that
characterizes the code generator of a naive CPS compiler is the C.P,E machine. Since terms in CPS(CS)

contain an encoding of control-flow information, the
machine does not require a continuation component

(K) to record the rest of the computation. Evaluation proceeds according to the state transition function in Figure 4. For example, the state transition
for the tail call (W k W1 Wn) computes a closure

(cl k'xl . . ~n, P', E') corresponding to W, extends E'
with the values of k, WI, . . . . Wn and starts the interpretation of P'.

Realistic CPS Compilers Although the CCP,E machine describes what a na~ve CPS compiler would do,
typical compilers deviate from this model in two regards.

First, the naive abstract machine for CPS code represents the continuation as an ordinary closure. Yet, realistic CPS compilers "mark" the continuation closure as

a special closure. For example, Shivers partitions procedures and continuations in order to improve the data
flow analysis of CPS programs [18:sec 3.8.3]. Also, in

both Orbit [14] and Rabbit [19], the allocation strategy
of a closure changes if the closure is a continuation. Similarly, Appel [2: 114-124] describes various techniques for

closure allocation that treat the continuation closure in
a special way.

In order to reflect these changes in the machine, we
tag continuation closures with a special marker `(ar"
that describes them as activation records.

Second, the CPS representation of any user-defined
procedure receives a continuation argument. However,

Steele [19] modifies the CPS transformation with a

"continuation variable hack" [19:94] that recognizes instances of CPS terms like ((~kl . . . .P) k2 . . .) and trans240

Semantics: Let P G CPS(CS),

ev&(P) = c if (P, o[k := (cl x, (k $), O[k := stop])]) +--+* ((k Z), o[z := c, k := stop]).
Data Specifications:

s. C State. =

E c Envn =
W" g Value. =

Transition Rules:

((k W), E) w
((let (z W) P), E) w

((ifO W P, Pz), E) _

((W k W, . . . Wn), E) "~
((W (Ax.P) W, . . . Wn), E) +--+

((0' k W, . . . Wn), E) _
((0' (Az.P) W, . W.)j E) _

CPS(CS) x Envn (machine states)

Variables +-+ Value. (environments)
Cl(clkxl . . . x~, P, E) I (cl o, P, E) I stop (machine values)

(P', E'[x := ~(W, E)]) where E(k)= (cl z, P', E')
(P, E[z := @V, E)])
(Pi, E) where P(W, E) = O

(Pz, E) where ,u(W, E) # O
(P', E'[k' := E(k), q := W:,.. .,x~ := W;])
where P(W, E) = (cl k'zI . . . x~, P', E') and for 1< i < n, W,* = p(W,, E)

(P', E'[k' := (cl z, P, E), q := W;,.. .,xn := W;])
where p(W, E) = (cl k'zl . ..z~. P', E') and for 1 < z < n, W,* = IJ(W, ,E)

(P', E'[z := &( O', W;, . . . . W;)]) if 6.(0', W;, . . ., W;) is defined,
where E(k) = (cl z, P', E') and for 1 < z < n, W,* = p(W,, E)

(P, E[Z := 8.(0', w;, . . . . w:)]) if 6C(0', W?, ..., W;) is defined,
and for 1< i < n, W," = I.J(W,, E)

Converting syntactic values to machine values:

P(C, E) = C
P(:z, E) = E(x)
P((NW1 ...zn.P).E) = (cl kzl . ..zn. P,E)

Figure 4: The naive CPS abstract machine: the C.p, E machine.

forms them to ((A. ~~.P[/cl := kz]) . .). This "optimization" eliminates "some of the register shuffling" [19:94]

during the evaluation of the term. Appel [2] achieves
the same effect without modifying the CPS transformation by letting the variables kl and kz share the same

register during the procedure call.

In terms of the CPS abstract machine, the optimization corresponds to a modification of the operation E'[k' := E(k), xl := W;, . . .,zn := W:] to

J!7[lrl: =w; ,...,z n:= W;] such that E and E' share
the binding of k. In order to make the sharing explicit,
we split the environment into two components: a component Ek that includes the binding for the continuation, and a component E- that includes the rest of
the bindings, and treat each component independently.
This optimization relies on the fact that every control
string has exactly one free continuation variable, which

implies that the corresponding value can be held in a
special register.5

Performing these modifications on the naive abstract
machine produces the realistic CPS abstract machine

5This fact also holds in the presence of control operators as
there is always one identifiable current continuation.

in Figure 5. The new CCP,EK machine extracts the

information regarding the continuation from the CPS
terms and manages the continuation in an optimized
way. For example, the state transition for the tail

call (W k W1 . . . W.) evaluates W to a closure

(cl kc, . . . X., P', E;), extends E; with the values ofWI,...,

Wn and starts the execution of P'. In particular, there is no need to extend E; with the value of k as
this value remains in the environment component Ek.

4 A-Normal Form Compilers
A close inspection of the C=P,EK machine reveals that
the control strings often contain redundant information

considering the way instructions are executed. First, a

return instruction, i.e., the transition WC, dispatches
on the term (k W), which informs the machine that the

"return address" is denoted by the value of the variable
k. The machine ignores this information since a return instruction automatically uses the value of register
Ek as the "return address". Second, the call instructions, i.e., transitions ~, and ~., invoke closures

that expect, among other arguments, a continuation k.

241

Semantics: Let P G CPS(CS),

ew&(P) = c if (P, O, (ar z, (k z), O, stop)) w: ((k z), O[z := c], stop).
Data Specifications:

SC E Statec =
E- E Envc =
W* G Valuee =

Ek E Contc =

Transition Rules:

((k W), E-, Ek) @+c (P'

CPS(CS) x Envc x Cont. (machine states)

Variables m Valuec (environments)
c I (CllCZI... Z~, P,)-) (machine values)
stop I (ar z, P, E-, Ek) (continuations)

E;[z := p(W, E-)], Ef) where Ek = (ar x, P', El-, E!)
((let (z W) P), E-, E') F@+= (P, E-[z := ,u(W, E-)], E')
{(if'O W PI .P2), E-, Ek) ~c (Pi, E-, E') where K(W,.E-) = O

or (Pz, E-, E') where K(W, E-) # O
((wkw, .. . W~), E-, Ek) w= (P', E~[z] := W:, . . ..z. := WJ, Ek)

where v(W, E-) = (cl k'zl . . .xn, P', El-) and for 1 5 t < n, W," = P(W,, E-)
((w (AS.P) w, . . . Wn), E-, E~) ~c (P', E~[zl := WY,. ... zn := W:], (ar z, P, E-, Ek))

where p(W, E-) = (cl k'zl . ..~n. P', E1-) and for 1< i < n, W,* = p(W; ,E-)
((0' k W, . . Wn), E-, Ek) WC (P', Efl[z := &( O', W:,..., W;)], E;)

where Ek = (ar x, P', E;, E!) and for
((o' (/kz.P) w, . . . W~), E-, Ek) UC (P, E-[x := &( O', W:,..., W~)], Ek)

and for 1 < i < n, W,* = p(W,, E-)

if 8C(0', W~, . . . . W;) is defined,
1< i < n, W? = ,U(W,, E-)

if ISC(O', W:, . . . . W;) is defined,

Figure 5: The realistic CPS abstract machine: the C,-P, EK machine.
Again, the machine ignores the continuation parameter
in the closures and manipulate the "global" register Ek
instead.

Undoing CPS The crucial
elimination of the redundant

CCP,EK machine corresponds to

insight is that the
information from the

an inverse CPS transformation [7, 17] on the intermediate code. The function Z./ in Figure 6 realizes such an inverse [17]. The inverse transformation formalizes our intuition about the
redundancies in the CCP.EK machine. It eliminates the

variable k from return instructions as well aa the parameter k from procedures. The latter change implies that

continuations are not passed as arguments in function
calls but rather become contexts surrounding the calls.
For example, the code segment cps (IV) in Section 3 becomes:

A(iV) = (let (f~ (+ 2 2))

(let (z 1)

(let (t,(f %))

(+ -t,t2))))

Based on the above argument, it appears
compilers perform a sequence of three steps:

that CPS

(7s $3 CPS b o

II

Al @normalization

I
I
I t

A(C'S) i un-CPS e CPS(CS)
The diagram naturally suggests a direct translation A

that combines the effects of the three phases. The identification of the translation A requires a theorem relating ~-reductions on CPS terms to reductions on the
source language. This correspondence of reductions was

the subject of our previous paper [17]. The resulting set
of source reductions, the A-reductions, is in Figure 7.6

Since the A-reductions are strongly normalizing, we can

characterize the translation A aa any function that aPplies the A-reductions to a source term until it reaches

a normal form [17: Theorem 6.4].

The definition of the A-reductions refers to the concept of evaluation contexts. An evaluation context is a
term with a "hole" (denoted by [ ]) in the place of one
subterm. The location of the hole points to the next

6Danvy [8] and Weise [21] also recognize that the compaction
of CPS terms can be expressed in the source language, but do not
explore this topic systematically.

L-t/&

--
The inverse CPS transformation:

I?J: CPS(CS) +

U[(k w)] =
U[(let (z W) P)] =
U[(if'O W P1 Pz)] =
Z.f[(w k WI . . . w.)] =
U[(W (AZ.P) WI . . . w.)] =

U[(o' k- WI . . . Wn)] =
q(o' (XZ.P) WI . . . Wn)] =

V2:w+

W[c] =
!qx] =
qMx] . ..%. q =

A(CS)

V[w]

(let (z iU[W]) U[P])
(if'(l Q[W] UIP1] U[l%])

(w[w] `3qw,] . . . V[wn])
(let (z (V[W] Ui[Wl] . . . If[W.])) U[P])
(o W[w,] . . . W[wn])
(let (z (O VIW1] . . . f17[Wn])) L/[P])

v
c

x
Axl . ..xn.u[itq

The language A(CS)

M ::= V

I (let (z V) M)
l(if13V MM)
I(VK . . . Vn)

I (let (x (V K Vn)) M)
I(ovl... vn)
\ (let (x (O Vi . . . V~)) M)

v ::= clxl(kc]. ..x M)M)
Figure 6: The inverse CPS transformation and its output

(return)

(bind)
(branch)
(tad call)

(call)
(prira-op)
(prim-op)

(values)

Evaluation Contexts:

& ::= [1
The A-reductions:

l(let(z$)M) l(ifO~MM) I(FV. V8M.. .M) where F= Vor F=O

~[(let (X M) AT)] + (let (x M) S[N]) where&# [ ], z @ FV(f)
t[(i~ v M, M,)] -+ (ifO V SIMI] S[MZ]) where ~ # [ ]

t[(F VI . . . Vn)] + (let (t (F VI . . . V~)) t[t])

where F = V or F = O,: # ~'[(let (Z []) M)], $ # [ ],t C FV(~)

Figure 7: Evaluation contexts and the set of A-reductions

(AI)

(Az)
(A)

subexpression to be evaluated according to the CEK se- The A-reductions transform programs in a natumantics. For example, in an expression (let (% Ml) flfz), ral and intuitive manner. The first two reductions

the next reducible expression must occur within Ml, merge code segments across declarations and condihence the definition of evaluation contexts includes the t ionals. The last reduction Iifis redexes out of evalclause (let (z ~) M). uation contexts and names intermediate results. Using evaluation contexts and the A-reductions, we can

243

Semantics: Let M G A(CS),

ewda(M) = c if (M, 0, (ar z, z, O,stop)) ++; (x, O[z := c], stop).
Data Specifications:

s. C State. = A(CS) x Enva x Conta (machine states)

E G Enva = Variables++ Vakea (environments)v* E

Valuea = c I (cl Z1 . . . z~, M, E) (machine values)
K E Conta = stop I (ar z, M, E, K) (continuations)

Transition Rules:

(~ E, K") Q. (M', E'[z := Y(V, E)], K') where K = (ar z, M', E', K')

((let (z V) M), E, K) @+a (M, E[z := T(V, E)], K)
((ifll V Ml Mz), E, K) u. (Ml, E, K) where 7(V, E) = O

or (Mz, E, K) where ~(~ E) # O

((vu . . . V~), E, K) ~. (M', E'[q := V;, . ... xn := V:], K)

where T(V, E) = (cl ZI .z~, M', E') and for 1< t < n, ~" = ~(~,E)

((let (z (V VI . . . Vn)) M)j E, K) ~. (M', E'[zl := ~", ,z~ := V~], (ar z, M, E, A'))

where Y(V, E) = (cl ZI . . .z~, M', E') and for 1 < z < n, V,* = y(V,, E)

((OU . . . V~), E, K) ma (M', E'[z := c$(O, V;,..., V:)],)') if 6(0, VI*, . . . . V;) is defined,

where K = (ar z, M', E'j K') and for 1 < i < n, V,* = -y(V,, E)

((let (z (O VI . . Vn)) M), E, A-) Ua (M, E[z := 8(0, ~", ,V~)], K) if 8(0, VI*, . . . . VJ) is defined,

and for 1 < i"< n, V,* = -y(V, jE)

Figure 8: The C.EK machine

rewrite our sample code segment IV in Section 3 as follows. For clarity, we surround the reducible term with
a box:

N = I(+(+22) (let (x 1) (~ z))) I

- (let (t, (+2 2)) (A,)

I (+ t, (let (z 1) (j z))) I )

+ (let (t, (+2 2)) (AI)

(let (x 1)

~))
-- (let (t, (+2 2)) (A,)

(let (x 1)

(let (t, (f x))

(+ ~1 b))))

The appendix includes a linear algorithm that maps

Core Scheme terms to their normal form with respect
to the A-reductions.

Compilers In order to establish that the Areductions generate the actual intermediate code of CPS
compilers, we design an abstract machine for the language of A-normal forms, the C. EK machine, and prove

that this machine is "equivalent" to the CPS machine
in Figure 5.

The C.EK machine is a CEK machine specialized to
the subset of Core Scheme in A-normal form (Figure 6).
The machine (see Figure 8) has only two kinds of continuations: the continuation stop, and continuations of

the form (ar z, M, E, K). Unlike the CEK machine,
the C. EK machine only needs to build a continuation
for the evaluation of a non-tail function call. For example, the transition rule for the tail call (V VI . . . Vn )
evaluates V to a closure (cl xl . . . ~n, M', E'), extends
the environment E' with the values of VI, . . . . Vn and

continues with the execution of M'. The continuation
component remains in the register K. By comparison,
the CEK machine would build a seperate continuation
for the evaluation of each sub-expression V, VI, . . . . Vm.

5 Equivalence of Compilation

Strategies

A comparison of Figures 5 and 8 suggests a close
relationship between the CCP,EK machine and the

CaEK machine. In fact, the two machines are identical modulo the syntaz of the control strings, as corresponding state transitions on the two machines perform the same abstract operations. Currently, the transition rules for these machines are defined using pattern

L-t-l

matching on the syntax of terms. Once we reformulate
these rules using predicates and selectors for abstract
syntax, we can see the correspondence more clearly.

For example, we can abstract the transition rules
(5) (5)-a and I---+C from the term syntax as the higher-order

functional 75:

T5[ca11-var, call-body, cal!?, call- args, ca!l-fn] =

(c, E,K) -. if ca//?(C)

where = ca/Lvar(C)

~ = call-body(C)

V = ca/Lfn(C)
Vl, . . ..vn = ca!l-argsl(C)

The arguments to 75 are abstract-syntax functions for

manipulating terms in a syntax-independent manner.
Applying 75 to the appropriate functions produces either the transition rule ~. of the C. EK machine or
the rule WC of the C.P, EK machine, i.e.,

ma = T5[A-ca/Lvar, . . . . A-calLfn]

&Zc = T~~cps-call-var, . . . . cps-call-fn]

Suitable definitions of the syntax-functions for the
language A(CS) are:

A-calLvar[(let (z (V VI . . . V~)) M)] = x
A-call-body [(let (z (V V1 . . . Vn)) M)] = M

. . .
A-call-fn[(let (z (V VI . . . Vn)) M)] = V

Definitions for the language CPS(CS) follow a similar
pattern:

cps-calLvar[(W (kc. P) WI . . . IVn)] = x
cps-calLbody[(W (Az. P) W1 . . . Wn)] = P

. . .
cps-ca/Lfn[(W (~z.P) W1 . . . Wn)] = W

In the same manner, we can abstract each pair of transition rules tia and WC as a higher-order functional T..

Let S. and $C be abstract-syntax functions appropriate for A-normal forms and CPS terms, respectively.
Then the following theorem characterizes the relationship between the two transition functions.

Theorem 5.1 (Machine Equivalence) For 1< n <
7,~a = 7n[Sa]and @c = 7n[SC].

The theorem states that the transition functions of the

CaEK and CCP,EK machines are identical modulo syntax. However, in order to show that the evaluation of an

A-normal form term M and its CPS counterpart on the
respective machines produces exactly the same behavior, we also need to prove that there exists a bijection M
between machine states that commutes with the transition rules.

Definition 5.2. (M, 7?, V, and K)

M : Statec - Statea

M((P, E-, Ek)) = (U[P], R(E-), K(Ek))

EnvC + Enva

XI(E-) = E where E($) = V(E- (x))

Value. - Valuea

v(c) = c
V((cl klzl . . .~n, P, E-)) =

(cl z,. . .rn,U[P],7?(E-)) -

Conic * Cont.

)qstop) = stop
K((ar z, P, E-, Ek)) =

(ar LZ,U[P], R(E-), K(E'))

Intuitively, the function M maps C.P,EK machine states
to C. EK machine states, and l?, V and K perform a
similar mapping for environments, machine values and

continuations respectively. We can now formalize the
previously stated requirement that O and 0' behave in

the same manner.

Requirement For all W;, . . . . W: E Vaiuec,

V(6. (0', W:,..., w;)) = 6(0, V(W; ), . . . . V(wq)).
The function M commutes with the state transition
functions.

Theorem 5.3 (Commutativity Theorem)

Let S E Statec: S ~c S' if and only zf JU(S) @+.
M(S').

s fic s,A A

I I

I I
M(i') ma M(s)

Proofi The inverse CPS transformations U is bijective [17]. Hence by structural induction, the functions
M, 7?, V and K are also bijective. The proof proceeds
by case analysis on the transition rules. ~

Intuitively, the evaluation of a CPS term P on the
CCP,EK machine proceeds in the same fashion as the
evaluation of U [P] on the CaEK machine. Together
with the machine equivalence theorem, this implies that
both machines perform the same sequence of abstract

operations, and hence compilers based on these abstract
machines can produce identical code for the same input.
The A-normal form compiler achieves its goal in fewer

passes.

245

6 A-Normal Forms as an Intermediate Language

Our analysis suggests that the language of A-normal
forms is a good intermediate representation for compilers. Indeed, most direct compilers use transformations

similar to the A-reductions on an ad hoc and incomplete
basis. It is therefore natural to modify such compilers to
perform a complete A-normalization phase, and analyze

the effects. We have conducted such an experiment with
the non-optimizing, direct compiler CAML Light [15].
This compiler translates ML programs into bytecode via

a A-calculus based intermediate language, and then interprets this bytecode. By performing A-normalization
on the intermediate language and rewriting the interpreter as a C. EK machine, we achieved speedups of between 50'?ZOand 100'% for each of a dozen small benchmarks. Naturally, we expect the speedups to be smaller
when modifying an optimizing compiler.

A major advantage of using a CPS-based intermediate representation is that many optimizations can be
expressed as sequences of /3 and q reductions. For
example, CPS compilers can transform the non-tail

call (W' (Az.kx) WI . . w'~ ) to the tail-recursive call

(Wkw, . . . W.) using an q-reduction on the continuation [2]. An identical transformation [17] on the
language of A-normal forms is the reduction f?,d:

(let (x (V V, . . Vn)) %)+ (V V, . . . Vn),
where V, Vi,..., V. are the A-normal forms corresponding to W, W1, . . . . Wn respectively. Every other optimization on CPS terms that corresponds to a sequence
of ~~-reductions is also expressible on A-normal form
terms [17].

The A-reductions also expose optimization opportunities by merging code segments across block declarations and conditionals. In particular, partial evaluators
rely on the A-reductions to improve their specialization phase [5]. For example, the addition operation and
the constant O are apparently unrelated in the following
term:

(addl (let (3 (~ 5)) O))
The A-normalization phase produces:

(let (z (f 5)) (addl O)),
which specializes to (let (x (f 5)) 1).

In summary, compilation with A-normal forms characterizes the critical aspects of the CPS transformation
relevant to compilation. Moreover, it formulates these

aspects in a way that direct compilers can easily use.
Thus, our result should lead to improvements for both

traditional compilation strategies.

A Linear A-Normalization
The linear A-normalization algorithm in Figure 9 is

written in Scheme extended with a special form match,
which performs pattern matching on the syntax of program terms. It employs a programming technique for

CPS algorithms pioneered by Danvy and Filinski [9].
To prevent possible exponential growth in code size, the

algorithm avoids duplicating the evaluation context enclosing a conditional expression. We assume the frontend uniquely renames all variables, which implies that

the condition z @ FV(S) of the reduction Al holds.

Acknowledgments We thank Olivier Danvy, Preston
Briggs, and Keith Cooper for comments on an early
version of the paper.

References

[1]

[2]

[3]

[4]

[5]
[6]

[7]

[8]
[9]

AHO) A., SETHI, R., AND ULLMAN, J.

Compile rs--Prmciples, Techniques, and Took.
Addison-Wesley, Reading, Mass., 1985.

AIWIZL) A. Compiling with Continuations. Cambridge University Press, 1992.

BARENDREGT, H. The Lambda Calcu!us: Its Syntax and Semantics, revised ed. Studies in Logic
and the Foundations of Mathematics 103. NorthHolland, 1984.

BOEHM, H.-J., AND DEMERS, A. Implementing Russel. In Proceedings of the ACM SIGPLAN 1986 Sympostum on Comptler Construction
(1986), vol. 21(7), Sigplan Notices, pp. 186-195.

BONDORF, A. Improving binding times without
explicit CPS-conversion. In Proceedings of the 1992

ACM Conference on LMp and Functional Programming (1992), pp. 1-10.

CLINGER, W. The Scheme 311 compiler: An exercise in denotational semantics. In Proceedings of

the 1984 ACM Conference on Lzsp and Functional
Programmmg (1984), pp. 356-364.

DANVY, O. Back to direct style. In Proceedings

of the ith European Symposium on Programming

(Rennes, 1992), Lecture Notes in Computer Science, 582, Springer Verlag, pp. 130-150.

DANVY, O. Three steps for the CPS transformation. Tech. Rep. CIS-92-2, Kansas State University,

1992.

DANVY, O., AND FILINSKI, A. Representing control: A study of the CPS transformation. Mathematical Structures in Computer Science, 4 (1992),
361-391.

246

(define na-rndize-terrn(lambda (M) (w-make M (lambda (z) z))))
(define normalize

(lambda (M k)

(match M

('(lambda ,paranu ,My) (k `(lambda ,pararns ,(norrnalize-terrn ZIody)))]
~(let (,z ,MI) ,Mz) (rwrrmdize Ml (lambda (Nl) `(let (,z ,Nl) ,(normalize M, k))))]
~(ifO ,MI ,Mz ,M3) (normalize-name Ml (lambda (t)(k `(ifO ,t,(normalize-term M2) ,(norrmdize-term Ms)))))]
~(,Fn ,M*) (if (PrirnOp? Fn)

(normcdwe-name* M* (lambda (t*) (k `(,Fn . ,t"))))

(normalize-name Fn (lambda (t)(normalize-nom.' M* (lambda (t*) (k `(jt . ,t")))))))][v (k v)])))

(define normalize-name

(lambda (M k)

(rzormahze M (lambda (N) (if ( Value? N) (k N) (let([t (neuwar)]) `(let (,t ,N) ,(k t))))))))

(define normaiize-name"

(lambda (M* k)

(if (null? M*)

(k `())
(normalize-name (car M*) (lambda (t) (normalize-name" (cxtr M*) (lambda (t*)(k `(,t . at')))))))))

Figure 9: A linear-tilme A-normalization algorithm

[10]

[11]
[12]

[13]

[14]

FELLEISEN, M., AND FRIEDMAN, D. Control operators, the SECD-machine, and the A-calculus. [n

Formal Description of Programmmg Concepts III
(Amsterdam, 1986), M. Wirsing, Ed., Elsevier Science Publishers B.V. (North-Holland), pp. 19;3-

217.

FESSENDEN, C., CLINGER, W., FRIEDMAN,
D. P., AND HAYNES, C. T. Scheme 311 version 4
reference manual. Computer Science Technical R,eport 137, Indiana University, Bloomhy.$on, Indiana, Feb. 1983.

FISCHER, M. Lambda calculus schemata. In Proceedings of the ACM Conference on Proving Assertions About Programs (1972), vol. 7(l), Sigplim
Notices, pp. 104-109.

KELSEY, R., AND HUDAK, P. Realistic compilation by program transformation. In Conference Record of the 16th Annuai ACM Symposium
on Principles of Programming Languages (Austin,
TX, Jan. 1989), pp. 281-292.

KRANZ, D., KELSEY, R., REES, J., HUDAK, E'.,

PHILBIN, J., AND ADAMS, N. Orbit: An optimizing compiler for Scheme. In Proceedings of

the ACM SIGPLAN 1986 Symposium on Compi/er

Construction (1986), vol. 21(7), Sigplan Notices,
pp. 219-233.

[15]
[16]
[17]

[18]

[19]
[20]

[21]

LEROY, X. The Zinc experiment: An economical
implementation of the ML language. Tech. Rep.

117, INRIA, 1990.

PLOTKIN, G. Call-by-name, call-by-value, and the
A-calculus, Theoretical Computer Science 1 (1975),

125-159.

SAE+RY, A., AND FELLEISEN, M. Reasoning about
programs in continuation-passing style. In Proceedings of the 1992 ACM Conference on Lisp
and Functional Programming (1992), pp. 288-298.
Technical Report 92-180, Rice University.

SHIVERS, O. Control-Flow Analysis of HigherOrder Languages or Tamtng Lambda. PhD thesis,
Carnegie-Mellon University, 1991.

STEELE, G. L. RABBIT: A compiler for Scheme.
MIT AI Memo 474, Massachusetts Institute of
Technology, Cambridge, Mass., May 1978.

WAND, M. Correctness of procedure representations in higher-order assembly language. In Proceedings of the 1991 Conference on the Mathematical Foundations of Programing Semantics (1992),
S. Brookes, Ed., vol. 598 of Lecture Notes an Computer Scaence, Springer Verlag, pp. 294-311.

WEISE, D. Advanced compiling techniques.
Course Notes at Stanford University, 1990.

247