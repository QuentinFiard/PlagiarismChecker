

This  first  volume  of  a  comprehensive, 
self-contained,  and  up-to-date  treat- 
ment  of  compiler  theory  emphasizes 
parsing and its theoretical  framework. 
Chapters  are devoted to mathematical 

preliminaries,  an  overview  of  compil- 

ing,  elements  of  language  theory, 
theory  of  translation,  general  parsing 
methods,  one-pass  no-backtrack  pars- 
ing,  and  limited  backtrack  parsing 

algorithms. 

Also included are valuable appendices 

on  the  syntax  for  an  extensible  base 
language,  for  SNOBOL4  statements, 
for  PL860  and  for  a  PAL  syntax 
directed  translation  scheme. 

Among  the  features: 

*  Emphasizes  principles  that  have 

broad  applicability  rather  than  de- 
tails  specific  to  a  given  language  or 
machine. 

*  Provides  complete  coverage  of  all 

important  parsing  algorithms,  in- 

eluding  the  LL,  LR,  Precedence 

and  Earley's  Methods. 

*  Presents  many  examples  to  illus- 

trate  concepts  and  applications. 
Exercises  at the  end  of each  section 

(continued on back jlap) 

THE  THEORY OF 

PARSING,  TRANSLATION, 
AND  COMPILING 

Prentice-Hall Series in Automatic  Computation 

George Forsythe, editor 

AHO AND ULLMAN,  The Theory of Parsing, Translation,  and Compiling, Volume  I: Parsing; Volume  II:  Compiling 
(ANDREE),3  Computer Programming:  Techniques, Analysis, and Mathematics ANSELONE,  Collectively Compact Operator Approximation  Theory 

and Applications to Integral Equations ARBIB,  Theories of Abstract Automata 
BATES AND DOUGLAS,  Programming Language/One,  2nd  ed. BLUMENTHAL,  Management Information Systems 
BOBROW AND SCHWARTZ,  Computers and the Policy-Making Community BOWLES, editor,  Computers in Humanistic Research 

BRENT,  Algorithms for  Minimization without Derivatives CESCHINO AND KUNTZMAN,  Numerical Solution of Initial  Value Problems 

CRESS, et  al.,  FORTRAN IV with  WATFOR  and  WATFIV DANIEL,  The Approximate Minimization of Functionals 
DESMONDE,  A  Conversational Graphic Data Processing System DESMONDE,  Computers and Their  Uses, 2nd  ed. 
DESMONDE,  Real-Time Data Processing Systems DRUMMOND,  Evaluation and Measurement Techniques for  Digital Computer Systems 

EVANS, et  al.,  Simulation  Using Digital Computers FIKE,  Computer Evaluation of Mathematical Functions 

FIKE,  PL/1 for Scientific Programers rORSYTHE AND MOLER,  Computer Solution of Linear Algebraic Systems 
GAUTHIER  AND  PONTO,  Designing Systems Programs GEAR,  Numerical lnital  Value Problems in Ordinary Differential Equations 

GOLDEN,  FORTRAN IV Programming and Computing GOLDEN AND LEtCHUS,  IBM~360 Programming and Computing 

GORDON,  System Simulation GREENSPAN,  Lectures on the Numerical Solution of Linear,  Singular, and 

Nonlinear Differential Equations GRUENBERGER, editor,  Computers and Communications 
GRUENBERGER, editor,  Critical Factors in Data Management GRUENBERGER, editor,  Expanding  Use of Computers in the 70's 
GRUENBERGER, editor,  Fourth  Generation  Computers HARTMANIS AND STEARNS,  Algebraic Structure Theory of Sequential Machines 
HULL,  Introduction to  Computing JACOBY, et  al.,  Iterative Methods for  Nonlinear  Optimization Problems 
JOHNSON,  System  Structure in Data, Programs and Computers KANTER,  The  Computer and the Executive 

KIVIAT, et  al.,  The SIMSCRIPT H Programming Language LORIN,  Parallelism in Hardware and Software:  Real and Apparent  Concurrency 
LOUDEN AND LEDIN,  Programming  thelBM 1130, 2nd  ed. MARTIN,  Design of Real-Time  Computer Systems 
MARTIN,  Future Developments in Telecommunications MARTIN,  Man-Computer Dialogue 

MARTIN,  Programming Real-Time  Computing  Systems MARTIN,  Systems Analysis for  Data Transmission 
MARTIN,  Telecommunications  and the  Computer MARTIN,  Teleprocessing Network  Organization 

MARTIN AND NORMAN,  The  Computerized Society 
MATHISON AND WALKER,  Computers  and Telecommunications:  Issues  in Public Policy MCKEEMAN, et  al.,  A  Compiler  Generator 

MEYERS,  Time-Sharing  Computation  in the Social Sciences MINSKY,  Computation:  Finite  and Infinite  Machines 
MOORE,  Interval Analysis PLANE AND MCMILLAN,  Discrete  Optimization:  Integer Programming and 

Network Analysis for Management Decisions PRITSKER AND KIVIAT,  Simulation  with GASP II: 
a FORTRAN-Based Simulation  Language PYLYSHYN,  editor,  Perspectives  on the Computer Revolution 
RICH,  Internal Sorting Methods:  Illustrated with PL/1 Program RffSTIN, editor,  Computer  Networks 

RUSTIN, editor,  Debugging  Techniques in Large Systems RUSTIN, editor,  Formal Semantics  of Programming Languages 
SACKMAN AND CITRENBAUM, editors,  On-line Planning: To wards Creative Problem-Solving 
SALTON, editor,  The SMART Retrieval System:  Experiments in Automatic  Document Processing 

SAMMET,  Programming Languages:  History and Fundamentals SCHULTZ,  Digital Processing:  A  System  Orientation 
SCHULTZ,  Finite Element Analysis 

SCHWARZ, et  al.,  Numerical Analysis of Symmetric  Matrices SHERMAN,  Techniques in  Computer Programming 

SIMON  AND  SIKLOSSY,  Representation  and Meaning:  Experiments with Information Processing Systems 

SNYDER,  Chebyshev  Methods in Numerical Approximation STERLING AND 

POLLACK,  Introduction  to Statistical  Data Processing STOUTMEYER,  PL/1 Programming for Engineering  and Science 

STROUD,  Approximate  Calculation  of Multiple Integrals STROUD AND SECREST,  Gbussian Quadrature Formulas 
TAVISS, editor,  The  Computer  Impact 
TRAUn,  Iterative Methods for  the Solution  of Equations UHR,  Pattern Recognition,  Learning,  and Thought 

VAN TASSEL,  Computer  Security  Management VARGA,  Matrix Iterative Analysis 

VAZSONYI,  Problem  Solving  by Digital  Computers  with PL/1 Programming WAITE,  Implementing Software for Non-Numeric  Application 
WILKINSON,  Rounding  Errors in Algebraic Processes ZIEGLER,  Time-Sharing Data Processing  Systems 

THE  T H E O R Y   OF 

P A R S I N G ,   T R A N S L A T I O N ,  
A N D   C O M P I L I N G  

VOLUME  I:  PARSING 

A L F R E D   V.  A H O  

Bell Telephone Laboratories,  Inc. 

Murray Hill,  N.J. 

J E F F R E Y   D.  U L L M A N  

Department of Electrical  Engineering 
Princeton  University 

PRENTICE-HALL,  INC. 
E N G L E W 0 0D   C L I F F S ,   N . J .  

(C)  1972 by Bell Telephone Laboratories, Incorporated, and J. D.  Ullman 
All rights reserved. No part of this book may be reproduced in any form or by any means 
without permission in writing from the publisher, 

1 0 9 8  
ISBN:  0-13-914556-7 Library of Congress Catalog Card No.  72-1073 

Printed in the United States of America 

PRENTICE-HALL  INTERNATIONAL,  INC., London PRENTICE-HALL  OF  AUSTRALIA,  PTY.  LTD.,  Sydney 
PRENTICE-HALL  OF  CANADA,  LTD., Toronto PRENTICE-HALL  OF  INDIA  PRIVATE  LIMITED, New Delhi 

PRENTICE-HALL  OF  JAPAN,  INC., Tokyo 

For Adrienne  and Holly 
PREFACE 
This  book  is  intended  for  a  one  or  two  semester  course  in  compiling 
theory  at  the  senior  or  graduate  level.  It is  a  theoretically  oriented  treatment 

of a  practical  subject.  Our  motivation  for .making it so is threefold. 

(1)  In  an  area  as  rapidly  changing  as  Computer  Science,  sound  pedagogy 
demands  that  courses  emphasize  ideas,  rather  than  implementation  details. 
It  is  our  hope  that  the  algorithms  and  concepts  presented  in  this  book  will 

survive  the  next  generation  of  computers  and  programming  languages,  and 
that  at  least  some  of  them  will  be  applicable  to  fields  other  than  compiler 
writing. 

(2)  Compiler  writing  has progressed  to  the  point  where  many portions  of 
a  compiler  can  be  isolated  and  subjected  to  design  optimization.  It  is  im- 
portant  that  appropriate  mathematical  tools  be  available  to  the  person 

attempting  this  optimization. 

(3)  Some  of  the  most  useful  and  most  efficient  compiler  algorithms, 
e.g.  LR(k)  parsing,  require  a  good  deal  of mathematical  background  for  full 

understanding.  We expect,  therefore,  that  a good  theoretical  background  will 
become  essential  for  the  compiler  designer. 

While  we  have  not  omitted  difficult  theorems  that  are  relevant  to  com- 
piling,  we  have  tried  to  make  the  book  as  readable  as  possible.  Numer- 

ous  examples  are  given,  each  based  on  a  small  grammar,  rather  than  on 
the  large  grammars  encountered  in  practice.  It  is  hoped  that  these  examples 
are  sufficient  to  illustrate  the  basic  ideas,  even  in  cases  where  the  theoretical 

developments  are  difficult  to  follow  in  isolation. 

Use of the Book 

The notes  from  which  this  book  derives  were used  in courses  at  Princeton 
University  and  Stevens  Institute  of  Technology  at  both  the  senior  and 
graduate levels. Both one and two semester courses have been taught from this 

book.  In  a  one  semester  course,  the  course  in  compilers  was  preceded  by  a 

*  PREFACE 
course  covering finite  automata  and  context-free  languages.  It was therefore 
unnecessary to cover Chapters 0, 2 and 8. Most of the remaining chapters were 
covered  in  detail. 

In  a  two  semester  sequence,  most  of  Volume  I  was  covered  in  the  first 
semester  and  most of Volume II,  except for  Chapter  8,  in  the  second.  In  the 
two  semester  course  more  attention  was  devoted  to  proofs  and  proof  tech- 
niques than in the  one  semester  course. 

Some  sections  of the  book  are  clearly  more  important  than  others,  and 
we would like to give the reader some brief comments regarding our estimates 

of the  relative  importance  of various  parts  of Volume  I.  As  a  general  com- 
ment,  it is probably wise to  skip most of the proofs.  We include proofs  of all 
main  results  because we  believe  them  to  be  necessary  for  maximum  under- 

standing of the subject.  However, we suspect  that many courses in compiling 
do  not  get this  deeply  into  many  topics,  and  reasonable  understanding  can 

be  obtained  with  only a  smattering  of proofs. 

Chapters0  (mathematical  background)  and  1  (overview  of  compiling) 
are almost all essential material, except possibly for Section  1.3, which covers 
applications  of  parsing  other  than  to  compilers. 

We  believe  that  every  concept  and  theorem  introduced  in  Chapter  2 
(language  theory)  finds  use  somewhere  in  the  remaining  nine  chapters. 
However,  some  of  the  material  can  be  skipped  in  a  course  on  compilers. 
A  good  candidate  for  omission  is  the  rather  difficult  material  on  regular 

expression equations  in Section 2.2.1.  One  is then  forced  to  omit  some of the 
material on right linear  grammars in Section 2.2.2.  (although the equivalence 
between  these  and  finite  automata  can  be  obtained  in  other  ways)  and  the 
material  on  Rosenkrantz's  method  of  achieving  Greibach  normal  form  in 

Section  2.4.5. 

The  concepts  of Chapter  3  (translation)  are  quite  essential  to  the  rest  of 
the book.  However, Section 3.2.3,  on the hierarchy of syntax,directed transla- 
tions,  is  rather  difficult and  can  be  omitted. 

We believe that Section 4.1 on backtracking methods of parsing is less vital 
than  the  tabular  methods  of Section 4.2. 

Most  of Chapter  5 (single-pass  parsing)  is  essential.  We  suggest  that  LL 
grammars  (Section  5.1),  LR  grammars  (Section  5.2),  precedence  grammars 

(Sections  5.3.2  and  5.3.4)  and  operator  precedence  grammars  (Section  5.4.3) 
receive  maximum priority.  Other  sections could be  omitted if necessary. 

Chapter  6  (backtracking  algorithms)  is  less  essential  than  most  of 
Chapter  5 or  Section 4.2.  If given a choice,  we would cover Section 6.1  rather 
than  6.2. 

Organization of the Book 

The entire work The Theory of Parsing, Translation, and Compiling appears 
in  two  volumes,  Parsing (Chs.  0-6)  and  Compiling (Chs.  7-11).  (The  topics 
covered in the second volume are parser  optimization, theory of deterministic 

PREFACE  xi 
parsing,  translation,  bookkeeping,  and code  optimization.)  The two volumes form  an  integrated  work,  with  pages  consecutively  numbered,  and  with  a 
bibliography  and  index  for  both  volumes  appearing  in  Volume  II. 

Problems  and  bibliographical  notes  appear  at  the  end  of  each  section 
(numbered  i.j).  Except  for  open  problems  and  research  problems,  we  have used  stars  to  indicate  grades  of  difficulty.  Singly  starred  problems  require 

one  significant  insight  for  their  solution.  Doubly  starred  exercises  require more  than  one  such  insight. 

It is recommended that a course  based on this book  be accompanied  by a 
programming  laboratory  in  which  several  compiler  parts  are  designed  and implemented. At the end of certain sections of this book appear programming 

exercises,  which  can  be  used  as  projects  in  such a  programming  laboratory. 
Acknowledgements 

Many  people  have  carefully  read  various  parts  of  this  manuscript  and 
have  helped  us  significantly  in  its  preparation.  Especially,  we  would  like 
to thank  David  Benson,  John  Bruno,  Stephen  Chen,  Matthew  Geller,  James 

Gimpel,  Michael  Harrison,  Ned  Horvath,  Jean  Ichbiah,  Brian  Kernighan, Douglas  Mcllroy,  Robert  Martin,  Robert  Morris,  Howard  Siegel,  Leah 

Siegel,  Harold  Stone,  and  Thomas  Szymanski,  as  well  as  referees  Thomas Cheatham,  Michael Fischer, and William McKeeman.  we  have also received 
important comments from many of the students who used these notes,  among them  Alan  Demers,  Nahed  El  Djabri,  Matthew  Hecht,  Peter  Henderson, 
Peter Maika, Thomas Peterson, Ravi Sethi, Kenneth Sills, and Steven Squires. 

Our  thanks  are  also  due  for  the  excellent  typing  of the  manuscript  done by  Hannah  Kresse  and  Dorothy  Luciani.  In  addition,  we  acknowledge  the 

support  services  provided  by  Bell Telephone  Laboratories  during  the  pre- 
paration  of the  manuscript.  The  use  of  UNIX,  an  operating  system  for the PDP-11  computer  designed  by  Dennis  Ritchie  and  Kenneth  Thompson, 

expedited  the  preparation  of certain  parts  of this manuscript. 

ALFRED V.  AHO 
JEFFREY D.  ULLMAN 

CONTENTS 
PREFACE 
O  MATHEMATICAL  PRELIMINARIES 

0.1  Concepts from Set Theory 

0.1.1  Sets  1 
0.1.2  Operations on Sets  3 0.1.3  Relations  5 

0.1.4  Closures of Relations  7 0.1.5  Ordering Relations  9 
0.1.6  Mappings  10 

Exercises  11 

0.2  Sets of Strings 

0.2.1  Strings  15 
0.2.2  Languages  16 
0.2.3  Operations on Languages  17 

Exercises  18 

0.3  Concepts from Logic 

0.3.1  Proofs  19 
0.3.2  Proof by Induction  20 
0.3.3  Logical  Connectives  21 

Exercises  22 
Bibliographic  Notes  25 

0.4  Procedures and Algorithms 

0.4.1  Procedures  25 
0.4.2  Algorithms  27 

,,, XIII 

15 
19 

25 

xiv 
| 

2 

CONTENTS 
0.5 

0.4.3  Recursive Functions  28 
0.4.4  Specification  of Procedures  29 0.4.5  Problems  29 

0.4.6  Post's Correspondence Problem 

Exercises  33 
Bibliographic Notes  36 

Concepts from Graph Theory 
0.5.1 
0.5.2 
0.5.3 
0.5.4 

0.5.5 0.5.6 

0.5.7 0.5.8 

32 
Directed Graphs  37 

Directed Acyclic Graphs  39 
Trees  40 

Ordered Graphs  41 
Inductive Proofs Involving Dags  43 
Linear Orders from Partial Orders  43 Representations for Trees  45 

Paths Through a Graph  47 
Exercises  50 Bibliographic Notes  52 

AN  INTRODUCTION  TO  COMPILING 

1.1 

1.2 

1.3 

Programming Languages 

1.1.1  Specification of Programming Languages  53 1.1.2r Syntax and Semantics  55 

Bibliographic Notes  57 
An Overview of Compiling 

1.2.1  The Portions of a Compiler  58 
1.2.2  Lexical Analysis  59 1.2.3  Bookkeeping  62 

1.2.4  Parsing  63 
1.2.5  Code Generation  65 1.2.6  Code Optimization  70 

1.2.7  Error Analysis and Recovery  72 
1.2.8  Summary  73 Exercises  75 

Bibliographic Notes  76 
Other Applications of Parsing and Translating Algorithms 
1.3.1  Natural Languages  78 
1.3.2  Structural Description of Patterns  79 Bibliographic Notes  82 

ELEMENTS  OF  LANGUAGE  THEORY 
2.1  Representations  for Languages 

2.1.1  Motivation  84 

37 
53 

53 

58 

77 
83 

83 

CONTENTS  xv 
2.2 

2.1.2 

2.1.3 

2.1.4 

Grammars  84 
Restricted Grammars  91 
Recognizers  93 
Exercises  96 
Bibliographic Notes  102 

Regular Sets, Their Generators, and Their Recognizers 
2.2.1  Regular Sets and Regular  Expressions  103 2.2.2  Regular  Sets  and Right-Linear Grammars  110 

2.2.3  Finite Automata  112 2.2.4  Finite Automata  and Regular  Sets  118 
2.2.5  Summary  120 

Exercises  121 

Bibiliographic Notes  124 

103 

2.3  Properties of Regular Sets 

2.3.1  Minimization  of Finite Automata  124 
2.3.2  The Pumping Lemma for Regular  Sets  128 
2.3.3  Closure Properties of Regular  Sets  129 

2.3.4  Decidable  Questions About  Regular  Sets  130 Exercises  132 

Bibliographic Notes  138 

124 

2.4  Context-free Languages 

2.4.1  Derivation  Trees  139 
2.4.2  Transformations  on  Context-Free  Grammars  143 

2.4.3  Chomsky  Normal Form  151 
2.4.4  Greibach Normal Form  153 2.4.5  An Alternative  Method of Achieving Greibach  Normal Form 

Exercises  163 
Bibliographic Notes  166 

159 

138 

2.5  Pushdown Automata 

2.5.1  The Basic  Definition  167 2.5.2  Variants of Pushdown  Automata  172 

2.5.3  Equivalence of PDA  Languages and  CFL's  176 

2.5.4  Deterministic Pushdown Automata  184 

Exercises  190 Bibliographic Notes  192 

2.6  Properties of Context-Free Languages 

2.6.1  Ogden's Lemma  192 
2.6.2  Closure  Properties of CFL's  196 
2.6.3  Decidability  Results  199 2.6.4  Properties of Deterministic  CFL's  201 

2.6.5  Ambiguity  202 

Exercises  207 Bibliographic Notes  21I 

167 
192 

xvi  CONTENTS 

3  THEORY  OF  TRANSLATION 

3.1  Formalisms for Translations 

3.1.1  Translation and Semantics  213 
3.1.2  Syntax-Directed  Translation  Schemata  215 
3.1.3  Finite  Transducers  223 
3.1.4  Pushdown  Transducers  227 

Exercises  233 

Bibliographic Notes  237 

3.2  Properties of Syntax-Directed Translations 

3.2.1  Characterizing Languages  238 3.2.2  Properties of Simple  SDT's  243 

3.2.3  A  Hierarchy of SDT's  243 

Exercises  250 
Bibliographic  Notes  251 

3.3  Lexical Analysis 

3.3.1  An Extended Language for Regular  Expressions 
3.3.2  Indirect Lexical Analysis  254 
3.3.3  Direct Lexical Analysis  258 
3.3.4  Software  Simulation  of Finite  Transducers  260 

Exercises  261 
Bibliographic  Notes  263 

252 

3.4  Parsing 

3.4.1  Definition of Parsing  263 3.4.2  Top-Down Parsing  264 

3.4.3  Bottom-Up Parsing  268 3.4.4  Comparison  of Top-Down  and Bottom-Up Parsing  271 
3.4.5  Grammatical  Covering  275 

Exercises  277 Bibliographic Notes  280 

212 

213 

238 

251 

263 

4  GENERAL  PARSING  METHODS 

4.1  Backtrack Parsing 

4.1.1  Simulation  of a PDT  282 4.1.2  Informal  Top-Down Parsing  285 

4.1.3  The  Top-Down Parsing Algorithm  289 4.1.4  Time and Space  Complexity  of the  Top-Down Parser 

4.1.5  Bottom-UpParsing  301 

Exercises  307 
Bibliographic Notes  313 

297 

281 

282 

4.2  Tabular Parsing  Methods 

4.2.1  The  Cocke- Younger-Kasami Algorithm 

4.2.2  The ParsingMethodofEarley  320 

Exercises  330 
Bibliographic Notes  332 

314 

CONTENTS  xvii 

314 

5  ONE-PASS  NO  BACKTRACK  PARSING 

5.1  LL(k) Grammars 

5.1.1 
5.1.2 
5.1.3 
5.1.4 

5.1.5 
5.1.6 

Definition  of LL(k) Grammar  334 Predictive Parsing Algorithms  338 
Implications  of the LL(k)  Definition Parsing LL(1)  Grammars  345 
Parsing LL(k) Grammars  348 Testing for the LL(k)  Condition  356 

Exercises  361 Bibliographic Notes  368 

342 

5.2  Deterministic Bottom-Up  Parsing 

5.2.1  Deterministic  Shift-Reduce Parsing  368 5.2.2  LR(k)  Grammars  371 

5.2.3  Implications  of the LR(k)  Definition  380 5.2.4  Testing for the LR(k)  Condition  391 
5.2.5  Deterministic Right Parsers for LR(k) Grammars 5.2.6  Implementation of LL(k) and LR(k) Parsers  396 

Exercises  396 
Bibliographic Notes  399 

5.3  Precedence Grammars 

5.3.1 

5.3.2 
5.3.3 

5.3.4 

Formal Shift-Reduce Parsing Algorithms 

Simple Precedence Grammars  403 
Extended Precedence Grammars  410 Weak Precedence Grammars  415 

Exercises  424 Bibliographic Notes  426 

400 

5.4  Other Classes  of Shift-Reduce  Parsable Grammars 

5.4.1  Bounded-Right-Context Grammars  427 
5.4.2  Mixed Strategy Precedence Grammars  435 
5.4.3  Operator Precedence Grammars  439 

5.4.4  Floyd-Evans Production Language  443 
5.4.5  Chapter Summary  448 Exercises  450 

Bibliographic Notes  455 

392 

333 

334 

368 
399 
426 

ooo XVIll  CONTENTS 
6  LIMITED  BACKTRACK  PARSING  ALGORITHMS 

6.1  Limited Backtrack Top-Down Parsing 

6.1.1  TDPL  457 
6.1.2  TDPL and Deterministic Context-Free Languages 6.1.3  A  Generalization of TDPL  469 

6.1.4  Time  Complexity of GTDPL Languages  473 
6.1.5  Implementation of GTDPL Programs  476 

Exercises  482 
Bibliographic Notes  485 

6.2  Limited Backtrack Bottom-Up Parsing 

6.2.1  Noncanonical Parsing  485 6.2.2  Two-StackParsers  487 

6.2.3  Colmerauer Precedence Relations  490 
6.2.4  Test forColmerauerPrecedence  493 

Exercises  499 Bibliographic Notes  500 

466 

456 

456 

485 
APPENDIX 

A.1  Syntax for an Extensible Base Language  501 A.2  Syntax of SNOBOL4 Statements  505 
A.3  Syntax forPL360  507 A.4  A  Syntax-Directed Translation Scheme for PAL  512 

501 

BIBLIOGRAPHY 
INDEX  TO  LEMMAS,  THEOREMS, 

AND  ALGORITHMS 

519 
531 
INDEX  TO  VOLUME  i  533 

0 

MATHEMATICAL 
PRELIMINARIES 

To  speak  clearly  and  accurately  we  need  a  precise  and  well-defined  lan- 
guage.  This  chapter  describes  the  language  that  we  shall  use  to  discuss  pars- 

ing, translation, and the other topics to be covered in this book. This language 
is  primarily  elementary  set  theory  with  some  rudimentary  concepts  from 

graph  theory  and  logic  included.  For  readers  having  background  in  these 
areas,  Chapter  0  can  be  easily  skimmed  and  treated  as  a  reference  for  nota- 

tion  and  definitions. 

0.1.  CONCEPTS  FROM  SET  THEORY 

This  section will  briefly review  some  of the  most  basic  concepts  from  set 
theory:  relations,  functions,  orderings,  and  the  usual  operations  on  sets. 

0.1.1.  * Sets 

In  what  follows,  we  assume  that  there  are  certain  objects,  referred  to  as 
atoms.  The  term  atom  wilt  be  a  rudimentary  concept,  which  is just  another 
way of saying that  the  term  atom  will be  left undefined,  and  what  we choose 
to  call  an  atom  depends  on  our  domain  of discourse.  Many  times  it  is  con- 

venient  to  consider  integers  or  letters  of an alphabet  to  be  atoms. 

We  also  postulate  an  abstract  notion  of membership.  If a  is  a  member  of 
A,  we  write  a  ~  A.  The  negation  of  this  statement  is  written  a  ~  A.  We 

assume  that  if a  is  an  atom,  then  it has  no  member;  i.e.,  x  ~  a  for  all  x  in 

the  domain  of discourse. 

We  shall  also  use  certain  primitive  objects,  called  sets,  which  are  not 
atoms.  If A  is  a  set,  then  its  members  or  elements  are  those  objects  a  (not 

2  MATHEMATICAL PRELIMINARIES  CHAP.  0 

necessarily  atoms)  such  that  a  E  A.  Each  member  of a  set  is  either  an  atom 

or  another  set.  We  assume  each  member  of  a  set  appears  exactly  once  in 
that  set.  If A  has  a  finite  number  of members,  then  A  is  a finite set, and  we 

often  write  A  =  {al,  a2,.  * *,  an}, if a l , . . . ,   an are  all  the  members  of A  and 
a~ ~  a j,  for  i  ~  j.  Note  that  order  is  unimportant.  We  could  also  write 
A  =  [ a n , . . . ,   al},  for  example.  We  reserve  the  symbol  ~  for  the  empty set, 
the  set  which  has  no  members.  Note  that  an  atom  also  has  no  members, 
but  ~  is  not  an  atom,  and  no  atom  is  ~ .  

The  statement  @A  =  n means  that  set A  has  n  members. 

Example 0.1 

Let  the  nonnegative  integers  be  atoms.  Then  A  =  {1, {2, 3}, 4}  is  a  set. 
A's  members  are  l,  {2, 3},  and  4.  The  member  {2, 3}  of  A  is  also  a  set.  Its 
members  are  2  and  3.  However,  the  atoms  2  and  3  are  not  members  of 
A  itself.  We  could  equivalently  have  written  A  =  {4,  1, {3, 2}}. Note  that # A = 3 .   D 

A  useful  way  o f   defining  sets  is  by  means  of  a  predicate,  a  statement 
involving  one  or  more  unknowns  which  has  one  of two  values,  true or false. 

The  set  defined  by  a  predicate  consists  of  exactly  those  elements  for  which 
the  predicate  is true.  However,  we  must  be careful  what  predicate  we choose 
to define a set, or we may attempt  to define a  set that could not possibly exist. 

Example  0.2 

The  phenomenon  alluded  to  above  is  known  as  Russell's paradox.  Let P(X) 

be  the  predicate  " X   is  not  a  member  of  itself";  i.e.,  X  ~  X.  Then  we 
might  think  that  we  could  define  the  set  Y of all  X  such  that  P(X) was  true; 
i.e.,  Y  consists  of  exactly  those  sets  that  are  not  members  of  themselves. 

Since most common sets seem not to be members of themselves,  it is tempting 
to  suppose  that  set  Y exists. 

But if  Y exists,  we should be  able to answer the  question,  "Is  Y a  member 
of itself?"  But  this  leads  to  an  impossible  situation.  If  Y ~  Y,  then  P(Y)  is 
false,  and  Y  is  not  a  member  of  itself,  by  definfi:ion  of  Y.  Hence,  it  is  not 
possible  that  Y ~  Y.  Conversely,  suppose  that  Y ~  Y.  Then,  by  definition 

of  Y  again,  Y ~   Y.  We  see  that  Y ~   Y  implies  Y ~   Y  and  that  Y ~  Y 
implies  Y ~  Y.  Since  either  Y  ~  Y  or  Y ~  Y  is  true,  both  are  true,  a  situa- 
tion  which  we  shall  assume  is  impossible.  One  "way  out"  is  to  accept  that 

set  Y does  not  exist.  [Z] 

The  normal way to  avoid  Russell's  paradox  is to  define  sets  only by those 
predicates P(X) of the  form  "X is  in  A  and  Pi(X),"  where A  is  a  known  set 

and  P1  is an arbitrary predicate.  If the set A is understood,  we shall just write PI(X) 

for  "X is in  A  and  Pi(X)." 

SEC. 0.1  CONCEPTS FROM SET THEORY  3 

If  P(X)  is  a  predicate,  then  we  denote  the  set  of  objects  X  for  which P(X) 

is  true  by  IX] P(X)]. 

Example 0.3 

Let  P(X)  be  the  predicate  "X  is  a  nonnegative  even  integer."  That  is, P(X) 

is  "X  is  in  the  set  of  integers  and  Pi(X),"  where  PI(X)  is  the  predi- 
cate  "X  is  even."  Then  A----[X]P(X)}  is  the  set  which  is  often  written 

{0, 2, 4,  . . . ,   2 n , . . . ) .   Colloquially,  we  can  assume  that  the  set  of  nonnega- 
tive  integers  is  understood,  and  write  A  --  IX I X  is  even).  D 

We  have  glossed  over  a  great  deal  of  development  called  axiomatic  set 
theory.  The  interested  reader  is  referred  to  Halmos  [1960]  or  Suppes  [1960] 
(see the  Bibliography at  the  end  of Volume  1) for a  more complete  treatment 
of this  subject. 

DEFINITION 

We  say that  set A  is  included in set B,  written  A  ___ B,  if every element  of 
A  is  also  an  element  of  B.  Sometimes  we  say  that  B  includes A,  written 
B  ~  A,  if A  ~  B.  In  either  case,  A is said to be a subset of B, and B a superset 

of A. 

If  B  contains  an  element  not  contained  in  A  and  A  ___ B,  then  we  say 
that  A  is properly included  in  B,  written  A  ~  B  (or  B  properly  includes  A, 
written  B  ~  A).  We  can  also  say that  A  is  a  proper  subset  of B  or  that  B  is 

a  proper  superset  of A. 

Two sets A and B are equal if and  only if A  ~  B and B  _  A. 

A  picture  called  a  Venn diagram is  often  used  to  graphically  describe  set 
membership  and inclusion.  Figure 0. i  shows a  Venn diagram for the relation A~_B. 

Fig.  0.1  Venn  diagram  of set  inclusion: 
A ~ B .  

0.1.2.  Operations  on  Sets 

There  are  several  basic  operations  on  sets which  can  be  used to  construct 
new  sets. 

4  MATHEMATICAL PRELIMINARIES  CHAP.  0 

DEFINITION 

Let  A  and  B  be  sets.  The  union  of  A  and  B,  written  A  U  B,  is  the  set 
containing  all  elements  in  A  together  with  all  elements  in  B.  Formally, 
A  u B = ~ x I x E A   o r x ~ B } . t  

The  intersection  of  A  and  B,  written  A  ~  B,  is  the  set  of  all  elements 
that  are  in  both  A  and  B.  Formally,  A  n  B  =  {x Ix  ~  A  and  x  6  B}. 

The  difference  of A a n d   B,  written  A  --  B,  is  the  set  of all  elements  in  A 
that  are  not  in  B.  If  A  =  Umthe  set  of all  elements  under  consideration  or 
the  universal  set,  as  it  is  sometimes  called--then  U -   B  is  often  written  ./~ 

and  called  the  complement  of B. 

Note  that  we  have  referred  to  the  universal  set  as  the  set  of  all  objects 
"under  consideration."  We  must  be  careful  to  be  sure  that  U  exists.  For 

example,  if  we  choose  U  to  be  "the  set  of  all  sets,"  then  we  would  have 

Russell's  paradox  again.  Also,  note  that  /t  is  not  well  defined  unless  we 
assume  that  complementation  with  respect  to  some  known  universe  is 
implied. 

In  general  A  B  =  A  n  /t.  Venn  diagrams  for  these  set  operations  are 

shown  in  Fig.  0.2. 

(a)  (b)  (c) 
AUB  A n B   A - B  

Fig. 0.2  Venn diagrams of set operations. 

If A  (~  B  --  ;~,  then  A  and  B  are  said  to  be  disjoint. 
DEFINITION 

If I  is  some  (indexing)  set  such  that  A t is a  known  set for  each  i i n/,   then 
we  write  U  A~ for  {X] there  exists  i  ~  I  such  that  X  ~  At}. Since  I  may  not 

iEI be finite,  this definition  is an extension  of the  union  of two  sets.  If I  is defined 

by predicate P(i),  we  sometimes  write  U  At  for  U  At.  For  example,  " U   At" P(i)  iEI  i>2 
means  A3  u  A,  U  As  U . . .  

tNote that we may not have a set guaranteed to include A W B, so this use of predi- cate  definition appears  questionable.  In  axiomatic  set  theory,  the  existence of A w  B is 

taken to be an axiom. 

SEC. 0.1  CONCEPTS  FROM  SET  THEORY  5 

DEFINITION 

Let  A  be  a  set.  The power  set  of  A,  written  ~P(A) or  sometimes  2 A, is  the 
set  of all  subsets  of A.  That  is,  6'(A)  =  [B! B  ~  A}.t 

Example 0.4 

Let  A  =  {1,  2}.  Then  ~P(A) =  ( ~ ,   {1}, {2}, {1,  2}}.  As  another  example, 
6,(~)  =  {~].  13 

In  general,  if A  is  a  finite  set  of m  members,  CP(A) has  2 m members.  The 
empty  set  is  a  member  of 6'(A)  for  every  A. 

We  have  observed  that  the  members  of  a  set  are  considered  to  be  un- 
ordered.  It  is  often  convenient  to  have  ordered  pairs  of  objects  available  for 
discourse.  We  thus  make  the  following  definition. 

DEFINITION 

Let  a  and  b  be  objects.  Then  (a,  b)  denotes  the  ordered pair  consisting  of 
a  and  b  in  that  order.  We  say  that  (a,  b)  =  (c,  d)  if  and  only  if  a  =  c  and 
b  =  d.  In  contrast,  [a,  b}  =  {b,  a}. 

Ordered  pairs  can  be  considered  sets  if  we  define  (a,  b)  to  be  the  set 
[a,  [a,  b}}. It  is left  to  the  Exercises  to  show  that  [a,  [a,  b]}  =  [c, [c,  d]}  if and 

only  if  a  =  c  and  b  =  d.  Thus  this  definition  is  consistent  with  what  we 
regard  to  be  the  fundamental  property  of ordered  pairs. 

DEFINITION 

The  Cartesian product  of  sets  A  and  B,  denoted  A  x  B,  is 

{(a,  b) la  ~  A  and  b  ~  B}. 

Example 0.5 

Let  A  =  [ 1, 2}  and  B  =  {2,  3,  4}.  Then 

A  x  B  =  ((1,  2),  (I,  3),  (1,  4),  (2,  2),  (2,  3),  (2,  4)}.  D 

0.1.3.  Relations 

M any  c om m o n  mathematical  concepts,  such  as  membership,  set  inclu- 
sion,  and  arithmetic  "less  than"  ( < ) ,   are  referred  to  as  relations.  We  shall 
give  a  formal  definition  of  the  concept  and  see  how  common  examples  of 
relations  fit  the  formal  definition. 

~The  existence  of  the  power  set  of any  set  is  an  axiom  of set  theory.  The  other  set defining  axioms,  in  addition  to  the  power  set  axiom  and  the  union  axiom  previously 
mentioned,  are: (1)  If A is  a  set  and P  a  predicate,  then  [XIP(X)  and  X ~ A} is  a  set. 

(2)  If  X  is an  atom  or  set,  then  [X}  is  a  set. (3)  If A  is  a  set,  then  {XI for  some  Y, we have  X ~  Y and  Y ~ A} is  a  set. 

6  MATHEMATICAL  PRELIMINARIES  CHAP. 0 

DEFINITION 

Let  A  and  B  be  sets.  A  relation from  A  to  B  is  any  subset  of  A  x  B.  If 
A  =  B,  we  say  that  the  relation  is  on  A.  If  R  is  a  relation  from  A  to  B,  we 
write  a  R  b  whenever  (a,  b)  is  in  R.  We  call  A  the  domain  of  R,  and  B  the 

range  of R. 

Example 0.6 

Let  A  be  the  set  of  integers.  The  relation  <  is  {(a, b) la  is  less  than  b}. 
We  thus  write  a  <  b  exactly  when  we  would  expect  to  do  so.  [[] 

DEFINITION 

The  relation  {(b, a)[ (a,  b ) ~   R}  is  called  the  inverse  of  R  and  is  often 
denoted  R- 1. 

A  relation  is a  very general  concept.  Often  a  relation  may  possess  certain 
properties  to  which  special  names  have  been  given. 

DEFINITION 

Let A  be  a  set  and  R  a  relation  on  A.  We  say that  R  is 
(1)  Reflexive  if a  R  a  for  all  a  in  A, 
(2)  Symmetric  if "a  R  b"  implies  "b R  a"  for  a,  b  in  A,  and 

(3)  Transitive  if  "a R  b  and  b  R  c"  implies  "a R  c"  for  a,  b,  c  in  A.  The 
elements  a,  b,  and  c need  not  be  distinct. 

Relations  obeying  these  three  properties  occur  frequently  and  have  addi- 
tional  properties  as  a  consequence.  The  term  equivalence  relation  is  used  to 
describe  a  relation  which  is  reflexive,  symmetric,  and  transitive. 

An  important  property  of  equivalence  relations  is  that  an  equivalence 
relation  R  on  a  set  A  partitions  A  into  disjoint  subsets  called  equivalence 

classes.  For  each  element  a  in  A  we  define  [a],  the  equivalence  class  of a,  to 
be  the  set  {b l a R  b}. 

Example 0.7 

Consider  the  relation  of  congruence  modulo  N  on  the  nonnegative  inte- 
gers.  We  say  that  a  _= b  mod  N  (read  "a  is  congruent  to  b  modulo  N")  if 
there  is  an  integer  k  such  that  a -   b  =  kN.  As  a  specific  case  let  us  take 

N  =  3.  Then  the  set  {0, 3,  6 , . . . ,   3 n , . . . }   forms  an  equivalence  class,  since 

3n  ~  3m mod  3 for  all  integer  values  of m  and  n.  We  shall  use  [0]  to  denote 
this  class.  We  could  have  used  [3]  or  [6]  or  [3n],  since  any  element  of  an 

equivalence  class  can  be  used  as  a  representative  of  that  class. 

The  two  other  equivalence  classes  under  the  relation  congruence  modulo 
3  are  [1]  =  {1, 4,  7 , . . . ,   3n  -t-  1 , . . . }  

[2]  =  {2,  5,  8 , . . . ,   3n  -q- 2 , . . . }  

SEC. 0.1  CONCEPTS FROM SET THEORY  7 
The union of the three sets [0], [ 1] and  [2] is the set of all nonnegative  integers. 
Thus  we  have  partitioned  the  set  of  all  nonnegative  integers  into  the  three 
disjoint  equivalence  classes  [0], [1], and  [2] by means  of the  equivalence  rela- 
tion  congruence  modulo  3 (Fig.  0.3).  D 

[1] 

Set of all nonnegative 

integers 

Fig. 0.3  Equivalence classes for congruence modulo 3. 
The  index of an  equivalence  relation  on  a  set  A  is  the  number  of equiva- 
lence  classes into which A  is partitioned. 

The  following theorem  about  equivalence  relations  is  left  as  an  exercise. 

THEOREM  0.1 

Let  R  be  an  equivalence  relation  on  A.  Then  for  all  a  and  b  in  A,  either 
[a]  =  [b]  or  [a]  and  [b] are  disjoint. 

Proof.  Exercise.  D 

0.1.4.  Closures  of  Relations 

Given  a  relation  R,  we  often  need  to  find  another  relation  R',  which 
includes  R  and has certain  additional  properties,  e.g.,  transitivity.  Moreover, 

we would generally like R'  to be  as  "small"  as possible, that is, to be a  subset 

of  every  other  relation  including  R  which  has  the  desired  properties.  Of 
course, the  "smallest"  relation may not be unique if the additional  properties 
are  strange.  However,  for  the  common  properties  mentioned  in the  previous 
section,  we  can  often  find  a  unique  superset  of  a  given  relation  wffh  these 
as  additional  properties.  Some  specific  cases  follow. 

DEFINITION 

The  k-fold product of a  relation  R  (on  A),  denoted  R k,  is  defined  as  fol- 
lows" 

(1)  a  R I b   if and  only if a  R  b; 

(2)  a  R t b  if and  only  if there  exists  c  in  A  such  that  a  R  c  and  c R ~- 1 b 
fo ri   >  1. 

This  is  an  example  of  a  recursive  definition,  a  method  of  definition  we 
shall  use  many  times.  To  examine  the  recursive  aspect  of  this  definition, 

8  MATHEMATICAL PRELIMINARIES  CHAP.  0 

suppose  that  a  R* b.  Then  by  (2)  there  is  a  c~  such  that  a  R  c~  and  c 1 R 3 b. 
Applying  (2)  again  there  is  a  c 2  such  that  c 1 R  c 2  and  c 2 R 2 b.  One  more 
application  of  (2)  says  that  there  is  a c   3 such  that  c z R  c 3 and  c 3 R ~ b.  No w  
we  can  apply  (1)  and  say  that  c3  R  b. 

Thus,  if a  R 4 b, then there  exists a  sequence  of elements  c 1, c 2, c 3 in A  such 

that  a  R  cl,  cl  R  c2,  c2  R  c3,  and  c a R  b. 

The  transitive  closure  of  a  relation  R  on  a  set  A  will  be  denoted  R /.  We 
define  a  R / b  if  and  only  if  a  R ~ b  for  some  i  ~  1.  We  shall  see  that  R /  is 
the  smallest  transitive  relation  that  includes  R. 

We  could  have  alternatively  defined  R /  by  saying  that  a  R / b  if  there 
exists  a  sequence  cl,  c2 . . . .   , c,  of  zero  or  more  elements  in  A  such  that 
a  R  c~,  c~ R  c 2,  . . . ,   c,_ 1 R  c,,  c,  R  b.  If n  --  0,  a  R  b  is  meant. 

The  reflexive  and transitive  closure  of  a  relation  R  on  a  set  A  is  denoted 
R*  and  is  defined  as  follows" 

(1)  a  R*  a  for  all  a  in  A; 
(2)  a  R*  b  if a  R / b; 
(3)  Nothing  is  in  R*  unless  its  being  there  follows  from  (1)  or  (2). 

If we  define  R o  by  saying  a  R o b  if and  only  if a  =  b,  then  a  R*  b  if and  only 
if a  R l b  for  some  i  ~  0. 

The  only  difference  between  R +  and  R*  is  that  a  R*  a  is  true  for  all  a  in 
A  but  a  R / a  may  or  may  not  be  true.  R*  is  the  smallest  reflexive and  transi- 
tive  relation  that  includes  R. 

In  Section  0.5.8  we  shall  examine  methods  of  computing  the  reflexive 
and  tralasitive  closure  of  a  relation  efficiently.  We  would  like  to  prove  here 
that  R /  and  R*  are  the  smallest  supersets  of  R  with  the  desired  properties. 

TI-IEOR~ 0.2 

If  R /  and  R*  are,  respectively,  the  transitive  and  reflexive-transitive 
closure  of R  as  defined  above,  then 

(a)  R /  is  transitive;  if  R'  is  any  transitive  relation  such  that  R  _~  R', 
then  R +  ~  R'. 

(b)  R*  is  reflexive  and  transitive;  if  R'  is  any  reflexive  and  transitive 
relation  such  that  R  ~  R',  then  R*  ~  R'. 

Proof  We  prove  only  (a);  (b)  is  left  as  an  exercise.  First,  to  show  that 
R /  is  transitive,  we  must  show  that  if  a R+b  and  b  R / c,  then  a  R / c. 

Since  a R  / b,  there  exists  a  sequence  of  elements  d l , . . . ,   d,  such  that 
dl  R  d 2 , . . . ,   d,_~  R  d,,  where  dl  =  a  and  d~  =  b.  Since  b  R + c,  we  can  find 

e l , . . . ,   e ,   such  that  el  R  e2 . . . .   , em_l R  era, where  el  =  b  =  d,  and  em =  c. 
Applying  the  definition  of  R + m  /  n  times,  we  conclude  that  a  R + c. 

No w we shall  show  that  R + is the  smallest.transitive  relation  that  includes 
R.  Let  R'  be  any  transitive  relation  such  that  R  ~  R'.  We  must  show  that 

R +  ~_  R'.  Thus  let  (a,  b)  be  in  R+;  i.e.,  a  R / b.  Then  there  is  a  sequence 

SEC. 0.1  CONCEPTS  FROM SET THEORY  9 

c l , . . . ,   c,  such  that  a  =  c 1,  b  =  c,,  and  c tRct+ 1  for  1 ~  i  <  n.  Since 
R  ~  R',  we have  c~ R'  c~+ 1 for  1 _ <  i <  n.  Since R' is transitive, repeated  appli- 
cation  of the  definition  of transitivity  yields  c~ R'  c,;  i.e.,  a R'  b.  Since  (a,  b) 
is  an  arbitrary  member  of  R +,  we  have  shown  that  every  member  of  R +  is 
also  a  member  of R'.  Thus,  R + ~  R',  as  desired.  D 

0.1.5.  Ordering  Relations 

An  important  class  of relations  on  sets  arc  the  ordering  relations.  In  gen- 
eral,  an  ordering  on  a  set  A  is  any  transitive  relation  on  A.  In  the  study  of 
algorithms,  a  special  type  of  ordering,  called  a  partial  order,  is particularly 
important. 

DEFINITION 

A  partial  order  on  a  set  A  is  a  relation  R  on  A  such  that 

(1)  R  is  transitive,  and 

(2)  For  all a  in A,  a  R  a  is false.  (That  is, R  is irreflexive.) 

From  properties  (i)  and  (2)  of a  partial  order it follows that  if a  R  b, then 
b  R  a  is false.  This  is called  the  asymmetric  property. 

Example 0.8 

An  example  of  a  partial  order  is  proper  inclusion  of  sets.  For  example, 
let  S  =  {ei . . . .   , e,}  be  a  set  of  n  elements  and  let  A  =  6'(S).  There  are  2" 
elements  in  A.  Then  define  a  R  b  if and  only  if a  ~  b  for  all  a,  b  in  A.  R  is 
a  partial  order. 

If  S  =  {0,  1, 2},  then  Fig.  0.4  graphically  depicts  this  partial  order.  Set 
$1  properly  includes  set  $2  if  and  only  if  there  is  a  path  downward  from 
S~  to  $2.  D 

In  the  literature  the  term  partial  order  is  sometimes  used  to denote  what 
we  call  a  reflexive  partial  order. 

0,  1,2~ 

Fig. 0.4  A partial order. 

2} 

10  MATHEMATICAL PRELIMINARIES  CHAP. 0 
DEFINITION 

A  reflexive partial order on  a  set  A  is  a  relation  R  such  that 

(1)  R  is  transitive, 
(2)  R  is  reflexive,  and 
(3)  If a  R  b  and  b R  a,  then  a  =  b.  This  property  is  called  antisymmetry. 

An  example  of a  reflexive partial  order  would  be  (not  necessarily  proper) 
inclusion  of sets. 

In  Section  0.5  we  shall  show  that  every  partial  order  can  be  graphically 
displayed  in  terms  of  a  structure  called  a  directed  acyclic  graph. 

An  important  special  case  of  a  partial  order  is  linear  order  (sometimes 
called  a  total  order). 

DEFINITION 

A  linear order R  on  a  set  A  is  a  partial  order  such  that  if a  and  b  are  in 
A,  then  either  a  R  b,  b  R  a,  or  a  =  b.  If  A  is  a  finite  set,  then  one  conven- 
ient  representation  for  the  linear  order  R  is  to  display  the  elements  of  A 
as  a  sequence  a 1, a 2 , . . .   , a,  such  that  at R  aj  if  and  only  if  i  <  j,  where 
A  =  [ a l , . . . ,   a,}. 

We  can  also  define  a  reflexive  linear  order  analogously.  That  is,  R  is a  reflexive  linear order 

on  A  if R  is  a  reflexive  partial  order  such  that  for  all 
a  and  b in A,  either  a  R  b  or  b  R  a. 

For  example,  the  relation  <  (less  than)  on  the  nonnegative  integers  is 
a  linear  order.  The  relation  <  is  a  reflexive  linear  order. 

0.1.6.  Mappings 

One  important  kind  of  relation  that  we  shall  be  using  is  known  as  a 
mapping. 

DEFINITION 

A  mapping  (also function  or  transformation)  M  from  a  set  A  to  a  set  B  is 
a  relation  from  A  to  B  such  that  if (a,  b)  and  (a,  c)  are  in  M,  then  b  =  c. 

If  (a, b)  is  in  M,  we  shall  often  write  M ( a ) =   b.  We  say  that  M(a)  is defined 

if there  exists  b  in  B  such  that  (a, b)  is  in  M.  If  m(a) is  defined  for 
all  a  in  A,  we  shall  say  that  M  is  total.  If we  wish  to  emphasize  that  M  may 
not  be  defined  for  all  a  in  A,  we  shall  say  that  M  is  a partial mapping  (func- 

tion)  from  A  to  B.  In  either  case,  we write  M  : A  ~  B.  We  call A  and  B  the domain 

and  range  of M,  respectively. 
If  M"   A  ~  B  is  a  mapping  having  the  property  that  for  each  b  in  B 
there  is  at  most  one  a  in  A  such  that  M(a)~-  b,  then  M  is  an  injection  (one- to-one  mapping) 

from  A  into  B.  If  M  is  a  total  mapping  such  that  for  each 
b  in  B  there  is  exactly  one  a  in  A  such  that  M(a)  =  b,  then  M  is  a  bijection (one-to-one  correspondence) 

between  A  and  B. 
If  M"   A  ~  B  is  an  injection,  then  we  can  find  the  inverse  mapping 

EXERCISES  11 
M-1  : B  ---~ A  such  that  M-l(b)  =  a  if and  only  if M(a)  =  b.  If  there  exists 
b  in  B  for  which  there  is  no  a  in  A  such  that  M(a)  =  b,  then  M-~  will  be  a 
partial  function. 

The  notion  of a  bijection  is  used  to  define  the  cardinality  of a  set,  which, 
informally  speaking,  denotes  the  number  of  elements  the  set  contains. 

DEFINITION 

Two  sets  A  and  B  are  of equal  cardinality  if  there  is  a  bijection  M  from A t o B .  

Example 0.9 

{0,  1, 2}  and  {a, b,  c}  are  of  equal  cardinality.  To  prove  this,  use,  for 
example,  the  bijection  M  =  {(0, a),  (1,  b),  (2,  c)}.  The  set  of  integers  is 
equal  in  cardinality  to  the  set  of  even  integers,  even  though  the  latter  is 
a  proper  subset  of the  former.  A  bijection  we can use  to prove  this would  be 
{(i, 2i)1i is  an integer}.  E] 

We  can  now  define  precisely  what  we  mean  by  a  finite  and  infinite  set.I 
DEFINITION 

A  set S  isfinite  if it is equal  in cardinality  to  the  set { 1, 2 , . . . ,   n} for  some 
integer  n.  A  set  is  infinite  if  it  is  equal  in  cardinality  to  a  proper  subset  of 

itself.  A  set  is  countable  if  it  is  equal  in  cardinality  to  the  set  of  positive 
integers.  It  follows  from  Example  0.9  that  every  countable  set  is  infinite. 

An  infinite  set  that  is  not  countable  is  called  uncountable. 

Examples  of countable  sets  are 
(1)  The  set  of  all  positive  and  negative  integers, 
(2)  The  set  of even  integers,  and 

(3)  {(a, b) la  and  b  are  integers}. 

Examples  of uncountable  sets  are 

(1)  The  set  of real  numbers, 
(2)  The  set  of all mappings  from  the integers  to  the  integers,  and 

(3)  The  set  of all  subsets  of the  positive  integers. 

E X E R C I S E S  
0.1.1.  Write  out  the  sets  defined  by  the  following  predicates.  Assume  that A  =  {0, 1,2,  3, 4, 5, 6}. 

(a)  {XI X is in A  and  X  is even}. 
tWe have used these terms previously, of course, assuming that their intuitive meaning was clear.  The formal  definitions  should  be  of some  interest,  however. 

12  MATHEMATICAL PRELIMINARIES  CHAP.  0 

(b)  {XI X  is  in  A  and  X  is  a  perfect  square}. 
(c)  {XI X  is in  A  and  X  >_  X 2 +  1 }. 

0.1.2.  Let  A  =  {0,  1, 2} and  B  =  {0, 3, 4}.  Write  out 

(a)  a  U  B. (b)  A N B .  

(c)  A - - B .  (d)  6'(a). 
(e)  A  *  B. 
0.1.3.  Show  that  if A  is  a  set  with  n  elements,  then  (P(A) has  2"  elements. 
0.1.4.  Let  A  and  B  be  sets  and  let  U  be  some  universal  set  with  respect  to 

which  complements  are  taken.  Show  that (a)  A u B = ~ n a .  

(b)  A n B = A   UB. 
These  two  identities  are  referred  to  as  De Morgan's  laws. 

0.1.5.  Show  that  there  does  not  exist  a  set  U  such  that  for  all  sets  A,  A  ~  U. Hint: 

Consider  Russell's  paradox. 

0.1.6.  Give  an  example  of a  relation  on  a  set which  is 

(a)  Reflexive but  not  symmetric  or  transitive. 
(b)  Symmetric  but  not  reflexive or  transitive. 
(c)  Transitive  but  not  reflexive or  symmetric. 
In  each  case  specify the  set  on  which  the  relation  is  defined. 

0.1.7.  Give  an  example  of a  relation  on  a  set  which  is 

(a)  Reflexive  and  symmetric  but  not  transitive. 
(b)  Reflexive  and  transitive  but  not  symmetric. 
(c)  Symmetric  and  transitive  but  not  reflexive. Warning: 

Do  not be  misled into believing that  a  relation  which  is  sym- 
metric  and  transitive  must  be  reflexive  (since  a  R  b  and  b R  a  implies aRa). 

0.1.8.  Show  that  the  following  relations  are  equivalence  relations" (a)  {(a, a) l a  ~  A }. 

(tO  Congruence  on  the  set  of triangles. 
0.1.9.  Let  R  be  an  equivalence  relation  on  a  set  A.  Let  a  and  b  be  in  A. 

Show  that 
(a)  [a]  =  [b]  if and  only  if a  R  b. 

(b)  [a]  n  [b]  =  Z~ if and  only  if a  R  b  is  false.? 

0.1.10.  Let A  be  a  finite  set. What  equivalence relations  on  A  induce  the largest 

and  smallest  number  of equivalence  classes ? 

0.1.11.  Let  A  =  {0,  1, 2)  and  R  =  [(0,  1), (1, 2)}.  Find  R*  and  R +. 

0.1.12.  Prove  Theorem  0.2(b). 

tBy  "a R b is false,"  we mean that  (a,  b) ~ R. 

EXERCISES  13 
0.1.13.  Let  R  be  a  relation  on  A.  Show  that  there  is  a  unique  relation  Re  such 

that 

(I)  R  ~  R,, 

(2)  Re  is  an  equivalence  relation  on  A,  and 

(3)  If  R'  is  any  equivalence  relation  on  A  such  that  R  ~  R',  then 
Re  ~  R'. 

Re  is  called  the  least  equivalence  relation  containing  R. 
DEFINITION 

A  well  order  on  a  set  A  is  a  reflexive  partial  order  R  on  A  such  that 
for  each  nonempty  subset  B  ___ A  there  exists  b  in  B  such  that  b  R  a 
for  all  a  in  B  (i.e.,  each  nonempty  subset  contains  a  smallest  element). 

0.1.14.  Show  that  ~  (less  than  or  equal  to)  is  a  well  order  on  the  positive 

integers. 

DEFINITION 

Let  A  be  a  set.  Define 

(1)  A 1 --  A,  and 
(2)  A"  =  A '-~  *  A,  for  i  >  1. 

Let  A + denote  U  A;. i>_ 1 

0.1.15.  Let  R  be  a  well  order  on  A.  Define  /~  on  A +  by" 

(al . ....  am)?  R  (bl .... .  b,)  if and  only  if either 

(1)  For  some  i  <  m,  a i  =  b i  for  1  _  j  <  i,  ac  ~  b~ and  as Rb~,  or 
(2)  m~ n ,   and  a~  --  b~  for  all  i,  1 ~i ~m .  

Show  that  /~  is  a  well  order  on  A +.  We  call  /~  a  lexicographie  order 
on  A +.  (The  ordering  of  words  in  a  dictionary  is  an  example  of  a 
lexicographic  order.) 

0.1.16.  State  whether  each  of  the  following  are  partial  orders,  reflexive  partial 

orders,  linear  orders,  or  reflexive  linear  orders" 

(a)  ~  on  6'(A). 
(b)  ~  on  6~(A). 
(c)  The  relation  R1  on  the  set  H  of h uman  beings  defined  by  a  R1  b 
if  and  only  if a  is  the  father  of b. 

(d)  The  relation  Rz  on  H  given  by  a  R  b  if  and  only  if  a  is  an 
ancestor  of  b. 

(e)  The  relation  R s  on  H  defined  by  a  R3  b  if and  only  if a  is  older 
than  b. 

0.1.17.  Let  R1  and  Rz  be  relations.  The  composition  of  Ri  and  Rz,  denoted 

Ri  o Rz  is  {(a,  b)[for  some  c,  a  R1  c  and  c  Rz  b}.  Show  that  if  R1  and 
R2  are  mappings,  then  R1  o R2  is  a  mapping.  Under  what  conditions 
will  R  i  o R2  be  a  total  mapping ?  An  injection ? A  bijection ? 

* J'Strictly  speaking,  (al . . . . .   am)  means  (((...  (al,  a2), a3) . ...  ), am),  according  to  the definition  of A m . 

14  MATHEMATICAL PRELIMINARIES  CHAP.  0 

0.1.18.  Let  A  be  a  finite  set  and  let  B  ~  A.  Show  that  if M"  A  ---~ B  is  a  bijec- 

tion,  then  A  =  B. 

0.1.19.  Let  A  and  B  have  m  and  n  elements,  respectively.  Show  that  there  are 

n m  total  functions  from  A  to   B.  How  many  (not  necessarily  total) 

functions  from  A  to  B  are  there ? 

'0.1.20.  Let  A  be  an  arbitrary  (not  necessarily  finite)  set.  Show  that  the  sets 

&(A)  and  {MI M  is  a  total  function  from  A  to  {0,  1}} are  of equal  car- 
dinality. 

0.1.21.  Show  that  the  set  of  all  integers  is  equal  in  cardinality  to 

(a)  The  set  of primes. 

(b)  The  set  of pairs  of integers. 
Hint:  Define  a  linear  order  on  the  set  of  pairs  of  integers  by 

(il,jl)  R  (i2,J2)  if  and  only  if  il  + j l   <  i2  + j 2   or  il  + j l   =  i2  + j 2  
and  il  <  i2. 

0.1.22.  Set  A  is  "larger  than"  B  if  A  and  B  are  of  different  cardinality  but  B 

is equal in cardinality  to  a subset of A.  Show that the set of real numbers 
between  0  and  1,  exclusive,  is  larger  than  the  set  of  integers.  Hint: 
Represent  real  numbers  by  unique  decimal  expansions.  In  contradic- 
tion,  suppose  that  the  two  sets  in  question  were  of  equal  cardinality. 
Then  we  could  find  a  sequence  of  real  numbers  rl,  r 2 , .. ,  which 

included  all  real  numbers  r,  0  <  r  <  1.  Can  you  find  a  real  number  r 
between  0  and  1 which  differs  in  the  ith  decimal  place  from  rt  for  all  i? 

"0.1.23.  Let  R  be  a  linear  order  on  a  finite set A.  Show that  there  exists a  unique 

element  a  ~  A  such  that  a  R  b  for  all  b  ~  A  --{a}.  Such  an  element 

a  is  called  the  least  element.  If  A  is  infinite,  does  there  always  exist  a 
least  element ? 

"0.1.24.  Show  that  [a,  [a,  b}]  =  {c, [c,  d}} if and  only  if a  =  c and  b  =  d. 

0.1.25.  Let  R  be  a  partial  order  on  a  set  A.  Show  that  if  a  R  b,  then  b  R  a  is 

false. 

"0.1.26.  Use  the  power  set  and  union  axioms  to  help  show  that  if A  and  B  are 

sets,  then  A  *  B  is  a  set. 

*'0.1.27.  Show  that  every set  is  either finite  or  infinite,  but  not  both. 

"0.1.28.  Show  that  every countable  set  is  infinite. 
"0.1.29.  Show  that  the  following  sets  have  the  same  cardinality: 

(1)  The  set  of real  numbers  between  0  and  1, 
(2)  The  set  of all  real  numbers, 

(3)  The  set  of all  mappings  from  the  integers  to  the  integers,  and 
(4)  The  set  of all  subsets  of the  positive  integers, 

*'0.1.30.  Show  that  6~(A) is  always  larger  than  A  for  any  set  A. 

0.1.31.  Show  that  if  R  is  a  partial  order  on  a  set  A,  then  the  relation  R'  given 

by  R'  =  R  w  {(a, a)[a  ~  A}  is  a  reflexive  partial  order  on  A. 

SEC. 0.2  SETS OF STRINGS  15 

0.1.32.  Show  that  if R  is  a  reflexive  partial  order  on  a  set A, then the relation R'  =  R  -- [(a, a)la  ~  A} is  a  partial  order on A. 

0.2.  SETS  OF  STRINGS 

In  this  book  we  shall  be  dealing  primarily  with  sets  whose  elements  are 
strings  of symbols.  In  this  section  we  shall  define  a  number  of terms  dealing 
with  strings. 

0.2.1.  Strings 

First  of all we need  the  concept  of an  alphabet.  To  us  an  alphabet  will  be 
any  set  of  symbols.  We  assume  that  the  term  symbol  has  a  sufficiently  clear 
intuitive  meaning  that  it  needs  no  further  explication. 

An  alphabet  need  not  be  finite  or  even  countable,  but  for  all  practical 
applications  alphabets  will  be  finite.  Two  examples  of  alphabets  are  the  set 

of 26  upper-  and  26  lowercase  Roman  letters  (the  Roman  alphabet)  and  the 
set  {0,  1 }, which  is  often called  the  binary  alphabet. 

The  terms  letter  and  character  will  be  used  synonymously with symbol  to 
denote  an  element  of  an  alphabet.  If  we  put  a  sequence  of  symbols  side  by 

side,  we  have  a  string  of  symbols.  For  example,  01011  is  a  string  over  the 
binary  alphabet  [0,  1}.  The  terms  sentence  and  word  are  often  used  as  syno- 
nyms  for  string. 

There  is  one  string  which  arises  frequently  and  which  has  been  given 
a  special  denotation.  This  is  the  empty  string  and  it  will  be  denoted  by  the 
symbol  e.  The  empty  string  is  that  string  which  has  no  symbols. 

CONVENTION 

We  shall  ordinarily  use  capital  Greek  letters  for  alphabets.  The  letters 
a,  b,  c,  and  d  will  represent  symbols  and  the  letters  t,  u,  v,  w, x, y,  and  z 
generally  represent  strings.  We  shall  represent  a  string  of  i  a's  by  a ~.  For 
example,  a t  =  a,t  a z  =  aa,  a 3 =  aaa,  and  so  forth.  Then,  a o is  e,  the  empty 

string. 

DEFINITION 

We  formally  define  strings  over  an  alphabet  E  in  the  following  manner: 
(1)  e  is  a  string  over  ~. 

(2)  If x  is  a  string  over  Z  and  a  is  in  E,  then  xa  is  a  string  over  Z. 

(3)  y  is a  string  over  E  if and  only if its being  so follows from  (1) and  (2). 

There  are  several  operations  on  strings  for  which  we  shall  have  use  later 
on.  If x  and  y  are  strings,  then  the  string  xy  is  called  the  concatenation  of x 

tWe  thus  identify the  symbol a  and the  string consisting of a  alone. 

16  MATHEMATICAL  PRELIMINARIES  CHAP.  0 
and  y.  For  example,  if x  =  ab  and  y  =  cd,  then  xy  =  abed.  For  all  strings 
X,  xe  -~-  ex  =  x. 

The  reversal  of  a  string  x,  denoted  x R,  is  the  string  x  written  in  reverse 
order;  i.e.,  if  x  :  a~  * ..  a,,  where  each  a~ is  a  symbol,  then  x R :  a , . . . a , .  
Also,  e R :   e. 

Let  x,  y,  and  z  be  arbitrary  strings  over  some  alphabet  E.  We  call  x  a 
prefix  of  the  string  xy  and  y  a  suffix  of  xy.  y  is  a  substring  of  xyz.  Both  a 

prefix  and  suffix  of a  string  x  are  substrings  of  x.  For  example,  ba  is  both 

a  prefix  and  a  substring  of  the  string  bac.  Notice  that  the  empty  string  is 

a  substring,  a  prefix,  and  a  su~x  of every  string. 

If x  :/:  y  and x  is a  prefix (suffix)  of some  string y, then  x  is called  a proper 
prefix  (suffix)  of y. 

The  length  of  a  string  is  the  number  of  symbols  in  the  string.  That  is,  if 
x  ---- a~  . . .   a,,  where  each  at  is  a  symbol,  then  the  length  of x  is n.  We  shall 

denote  the  length  of  a  string  x  b y ] x   1. For  example,  l a a b l=   3  a n d ] e l =   0. 
All  strings  which  we  shall  encounter  will  be  of finite  length. 

0.2.2.  Languages 

DEFINITION 

A  language  over  an  alphabet  Z  is  a  set  of  strings  over  E.  This  definition 
surely  encompasses  almost  everyone's  notion  of  a  language.  FOR TRAN, 
ALGOL,  PL/I,  and  even  English  are  included  in  this  definition. 

Example 0.10 

Let  us  consider  some  simple  examples  of  languages  over  an  alphabet  Z. 
The  empty  set  ~  is  a  language.  The  set  [e}  which  contains  only  the  empty 

string  is  a  language.  Notice  that  ~  and  {~e} are  two  distinct  languages.  D 

DEFINITION 

We  let  E*  denote  the  set  containing  all  strings  over  E  including  e.  For 
example,  if A  is  the  binary  alphabet  {0,  1],  then 

E*  =  [e, 0,  1, 00, 01,  10,  11,000,  0 0 1 , . . . ] .  
Every  language  over  E  is  a  subset  of  E*.  The  set  of  all  strings  over  E  but 
excluding  e  will  be  denoted  by  E+. 

Example 0.11 

Let  us  consider  the  language  L 1 containing  all  strings  of  zero  or  more 
a's.  We  can  denote  L 1 by  [atl i ~  0}.  It  should  be  clear  that  L1  =  {a}*.  [Z] 

SEC. 0.2  SETS OF STRINGS  17 

CONVENTION 

When  no  confusion  results  we  shall  often  denote  a  set  consisting  of  a 
single  element  by  the  element  itself.  Thus  according  to  this  convention  a* =  {a}*. 

DEFINITION 

A  language L  such  that  no  string  in  L  is  a  proper  prefix  (suffix)  of  any 
other  string  in  L  is  said  to  have  the prefix  (suffix)  property. 

For  example,  a*  does  not  have  the  prefix  property but  (a~b[i  ~  O} does. 

0.2.3.  Operations  on  Languages 

We shall often be concerned with various operations applied  to languages. 
In  this  section,  we  shall  consider  some  basic  and  fundamental  operations  on 
languages. 

Since a  language  is just  a  set,  the  operations  of union,  intersection,  differ- 
ence,  and  complementation  apply to languages.  The  operation  concatenation 

can  be  applied  to  languages  as  well  as  strings. 

DEFINITION 

Let L1  be  a  language  over  alphabet  El  and  L z a  language  over  E2.  Then 
L1L2,  called  the  concatenation  or  product  of  L a  and  L z,  is  the  language 
{xy[x  ~  L~  and  y  ~  L2]. 

There  will  be  occasions  when  we  wish to  concatenate  any  arbitrary  num- 
ber  of  strings  from  a  language.  This  notion  is  captured  in  the  closure  of 

a  language. 

DEFINITION 

The  closure  of L,  denoted L*,  is  defined  as  follows" 
(1)  L o  =  (e}. 
(2)  L"  =  LL"-1  for  n  .~  1. 

(3)  L * =   U  L". n>_0 

The positive  closure  of L,  denoted  L +,  is  [,_J L".  Note  that  L + =  LL*  =  L*L 

n~l and  that  L*  =  L +  U  (e}. 

We  shall  also  be  interested  in  mappings  on  languages.  A  simple  type  of 
mapping  which  occurs  frequently  when  dealing  with  languages  is  homo- 

morphism.  We  can  define  a  homomorphism  in  the  following  way. 

DEFINITION 

Let  E~  and  E2  be  alphabets.  A  homomorphism  is a  mapping  h"  E~  ~  E*. 
We  extend  the  domain  of  the  homomorphism  h  to  ~*  by  letting  h (e )=  e 
and  h(xa)  =  h(x)h(a)  for  all  x  in  ~*,  a  in  E,. 

18  MATHEMATICAL PRELIMINARIES  CHAP. 0 

Applying  a  homo mo rph ism  to  a  language  L,  we  get  another  language h(L), 

which  is  the  set  of  strings  (h(w) lw  ~  L]. 

Example 0.12 

Suppose  that  we  wish  to  change  every  instance  of  0  in  a  string  to  a  and 
every  1  to  bb.  We  can  define  a  homo morp hism  h  such  that  h ( 0 ) =   a  and 
h(1)  =  bb.  Then  i f L   is  the  language  (0"l"[n  ~  1}, h ( L ) =   {a"b2"ln  ~  1}.  [Z 

Although  homomorphisms  on  languages  are  not  always  one-to-one 
mappings,  it  is  often  useful  to  talk  about  their  inverses  (as  relations). 

DEFINITION 

If h"  E~  --~  Ez* is  a  homomorphism,  then  the  relation  h-1.  E2* ---~ 6~(E~*), 
defined  below,  is  called  an  inverse  homomorphism.  If y  is  in  E2*, then  h-~(y) 
is  the  set  of  strings  over  E1  which  get  mapped  by  h  to  y.  That  is,  h-~(y)  = (xlh(x) 

=  y}.  I f L   is  a  language  over  E2,  then  h-I(L)is  the  language  over  Ex 
consisting  of those  strings  which get mapped  by h into  a  string in L.  Formally, 
h - '( L)   =  U  h-'(y)  =  (xlh(x)  ~  L]. yEL 

Example 0.13 

Let  h  be  a  homomor phism  such  that  h(0)  =  a  and  h(1)  =  a.  It  follows 
that  h-~(a)  =  {0,  1}  and  h-~(a *)  =  [0,  1}*. 

As  a  second  example,  suppose  that  h  is  a  homomorphism  such  that 
h(0)  =  a  and  h(i)  =  e.  Then  h- l(e)  =  1"  and  h- l(a)  =  1'01".  Here  1"01" 

denotes  the  language  [ 1 i01J I i, j  ~  0],  which  is consistent  with  our  definitions 
and  the  convention  which  identifies  a  and  [a].  [~] 

E X E R C I S E S  
0.2.1.  Give  all  the  (a)  prefixes,  (b)  suffixes,  and  (c)  substrings  of  the  string  abe. 

0.2.2.  Prove  or  disprove:  L + =  L*  --  {el. 
0.2.3.  Let h be the homomorphism  defined by h(0)  =  a, h(1)  =  bb, and h(2)  =  e. What  is h(L), where L  =  {012]* ?t 

0.2.4.  Let  h  be  as  in Exercise  0.2.3.  What  is h-l({ab]*)? 
*0.2.5.  Prove  or  disprox, e  the  following" (a)  h-l(h(L)).'=  L. 

(b)  h(h-~(L))  =  L. 
0,2,6,  Can  L*  or  L +  ever  be  ~  ?  Under  what  circumstances  are  L*  and  L + finite ? 

tNote  that  {012}* is not  {0, 1, 2}*. 

SEC. 0.3  CONCEPTS FROM LOGIC  19 

*0.2.7.  Give well orders  on  the  following languages: 

(a)  (a, b}*. (b)  a*b*c*. 

(c)  (w[ w ~  (a, b}*  and  the  number  of a's  in  w equals  the  number  of  b's). 
0.2.8.  Which  of  the  following  languages  have  the  prefix  (suffix)  property? ( a ) ~ .  

(b)  {e). (c)  (a"b"ln ~  1~. 
(d)  L*, if L  has  the prefix property. (e)  (wl w ~  {a, b}*  and  the  number  of a's  in  w equals  the  number  of  b's~}. 

0.3.  CONCEPTS  FROM  LOGIC 

In  this  book  we  shall  present  a  number  of  algorithms  which  are  useful 
in  language-processing  applications.  For  some  functions  several  algorithms 

are  known,  and  it is  desirable  to  present  the  algorithms  in  a  common  frame- 
work  in  which  they  can  be  evaluated  and  compared. 

Above  all,  it  is  most  desirable  to  know  that  an  algorithm  performs  the 
function  that  it  is  supposed  to  perform.  For  this  reason,  we  shall  provide 

proofs  that  the  various  algorithms  that  we  shall  present  function  as  adver- 

tised.  In  this  section,  we shall  briefly  comment  on  what  is  a  proof  and  men- 
tion  some  useful  techniques  of proof. 

0.3.1.  Proofs 

A  formal  mathematical  system  can  be  characterized  by  the  following 
basic  components" 

(1)  Basic  symbols, 
(2)  Formation  rules, 
(3)  Axioms,  and 
(4)  Rules  of inference. 

The  set  of basic  symbols would  include  the  symbols  for  constants,  operators, 
and  so  forth.  Statements  can  then  be  constructed  from  these  basic  symbols 
according  to  a  set  of formation  rules.  Certain  primitive  statements  can  be 
defined,  and  the  validity  of these  statements  can  be  accepted  without  justifi- 
cation.  These  statements  are  known  as  the  axioms  of the  system. 

Then  certain  rules  can  be  specified  whereby  valid  statements  can  be  used 
to  infer  new  valid  statements.  Such  rules  are  called  rules  of inference. 

The  objective  may be  to  prove  that  a  certain  statement  is true  in  a  certain 
mathematical  system.  A  proof  of  that  statement  is  a  sequence  of statements 
such  that 

(1)  Each  statement is either  an  axiom  or can  be created  from  one  or more 
of  the  previous  statements  by  means  of  a  rule  of  inference. 

20  MATHEMATICAL PRELIMINARIES  CHAP.  0 

(2)  The  last  statement  in  the  sequence  is  the  statement  to  be  proved. 
A  statement  for  which  we  can  find  a  proof  is  called  a  theorem  of  that 
formal  system.  Obviously,  every  axiom  of  a  formal  system  is  a  theorem. 

In  spirit  at  least,  the  proof  of  any  mathematical  theorem  can  be  formu- 
lated  in  these  terms.  However,  going  to  a  level  of detail  in  which  each  state- 
ment  is either  an  axiom  or  follows  from  previous  statements  by  rudimentary 
rules  of inference  makes  the  proofs  of all but  the  most  elementary  theorems 
too  long.  The  tasl~  of  finding  proofs  of  theorems  in  this  fashion  is  in  itself 
laborious,  even  for  computers. 

Consequently,  mathematicians  invariably  employ  various  shortcuts  to 
reduce  the  length  of  a  proof.  Statements  which  are  previously  proved  theo- 
rems  can  be  inserted  into  proofs.  Also,  statements  can  be  omitted  when  it  is 
(hopefully)  clear  what  is  being  done.  This  technique  is  practiced  virtually 
everywhere,  and  this  book  is  no  exception. 

It  is  known  to  be  impossible  to  provide  a  universal  method  for  proving 
theorems.  However,  in the  next  sections  we  shall  mention  a  few  of the  more 
commonly  used  techniques. 

0.3.2.  Proof  by Induction 

Suppose  that  we  wish  to  prove  that  a  statement  S(n)  about  an  integer  n 
is  true  for  all  integers  in  a  set  N. 

If  N  is  finite,  then  one  method  of  proof  is  to  show  that  S(n)  is  true  for 
each  value  of  n  in  N.  This  method  of  proof  is  sometimes  called  proof  by perfect  induction 

or  proof  by  exhaustion. 
If N  is  an  infinite  subset  of  the  integers,  then  we  may  use  simple  mathe- matical  induction. 

Let  no  be  the  smallest  value  in  N.  To  show  that  S(n)  is 
true  for  all n  in  N,  we  may  equivalently  show  that 

(1)  S(no) is  true.  (This  is  called  the  basis  of the  induction.) 

(2)  Assuming  that  S(m)  is true  for  all  m  <  n in  N,  show  that  S(n) is also 
true.  (This  is  the  inductive step.) 

Example 0.14 

Suppose  then  that  S(n) is  the  statement 

1-q-3+5+...+2n--  1  =n  2 
That is, the  sum  of odd  integers  is a perfect  square.  Suppose  we wish to  show 
that  S(n)  is  true  for  all  positive  integers.  Thus  N  =  {~1, 2,  3, . . . } .  Basis. 

F o r n =   l w e h a v e l   =  12. Inductive Step. 

Assuming  S ( 1 ) , . . . ,   S(n) are  true  [in particular,  that  S(n) 

SEe. 0.3  CONCEPTS FROM LOGIC  21 

is  true],  we  have 

1 - t - 3 - J r - 5 + . . - + [ 2 n - -   l ] + [ 2 ( n +   1)--  1 ] : n   2 + 2 n +   1 

:  (n  +  1)2 

so  that  S(n  +  1)  must  then  also  be  true. 

We  thus  conclude  that  S(n)  is  true  for  all  positive  integers. 

The  reader  is referred  to  Section  0.5.5  for  some  methods  of induction  on 
sets  other  than  integers. 

0.3.3.  Logical  Connectives 

Often  a  statement  (theorem)  may  read  "P  if  and  only  if  Q"  or  "P  is 
a  necessary  and  sufficient  condition  for  Q,"  where  P  and  Q  are  themselves 
statements.  The  terms/f,  only  if,  necessary,  and  sufficient  have  precise  mean- 
ings  in  logic. 

A  logical  connective  is  a  symbol  that  can  be  used  to  create  a  statement 
out  of  simpler  statements.  For  example,  and,  or,  not,  implies  are  logical 
connectives,  not being  a  unary  connective  and  the  others  binary  connectives. 

If P  and  Q  are  statements,  then  P  and  Q, P  or  Q, not P,  and P  implies  Q  are 
also  statements. 

The  symbol  A  is  used  to  denote  and,  V  to  denote  or,  ,,~  to  denote  not, 
and  ~  to  denote  implies. 

There  are  well-defined  rules  governing  the  truth  or  falsehood  of  a  state- 
ment  containing  logical  connectives.  For  example,  the  statement P  and  Q  is 
true  only  when  both  P  is  true  and  Q  is  also  true.  We  can  summarize  the 
properties  of  a  logical  connective  by  a  table,  called  a  truth  table,  which  dis- 

plays  the  value  of a  composite  statement  in  terms  of the  values  of  its compo- 

nents.  Figure  0.5  shows  the  truth  table  for  the  logical  connectives  and,  or, 
not  and  implies. 

P  Q  PAQ  PVQ  ,.~P  P---~Q 

F  F  F  F  T  T F  T  F  T  T  T 

T  F  F  T  F  F T  T  T  T  F  T 

Fig. 0.5  Truth tables for and, or, not, and implies. 
From  the  table  (Fig.  0.5)  we  see that  P  ~  Q  is  false  only  when  P  is  true 
and  Q  is  false.  It  may  seem  a  little  odd  that  if P  is  false,  then  P  implies  Q 

22  MATHEMATICAL  PRELIMINARIES  CHAP.  0 
is  always  true,  regardless  of  the  value  of  Q.  But  in  logic  this  is  customary; 
from  falsehood  as  a  hypothesis,  anything  follows. 

We  can  now  return  to  consideration  of  a  statement  of the  form P  if and only  if Q. 

This  statement  consists  of two  parts:  P / f   Q  and  P  only  if Q.  It  is 
more  common  to  state  P / f   Q  as  if  Q  then P,  which  is  only  another  way  of 
saying  Q  implies P. 

In  fact  the  following  five  statements  are  equivalent: 

(1)  P  implies  Q. 
(2)  l f  P  then a. 
(3)  P  only  if Q. 
(4)  Q is a necessary  condition for P. 
(5)  P  is a sufficient  condition for  Q. 

To  show  that  the  statement P  if and only  if Q is  true  we  must  show  both 
that  Q implies P and  that P  implies  Q. Thus,  P  if and only  if Q is true  exactly 
when  P  and  Q  are  either  both  true  or  both  false. 

There  are  several  alternative  methods  of  showing  that  the  statement  P implies  Q 

is  always  true.  One  method  is  to  show  that  the  statement  not  Q implies  not  Pt 

is  always  true.  The  reader  should  verify  that  not  Q  implies not P 
has  exactly  the  same  truth  table  as  P  implies  Q.  The  statement  not  Q implies not P 

is  called  the  contrapositive  of P  implies  Q. 
One  important  technique  of  proof  is  proof by  contradiction,  sometimes 
called  the  indirect proof or  reductio  ad absurdum.  Here,  to  show  that P implies 

Q  is  true,  we  show  that  not  Q and P  implies falsehood is  true.  That  is  to  say, 
we  assume  that  Q  is  not  true,  and  if assuming  that  P  is  true  we  are  able  to 

obtain  a  statement  known  to  be false,  then P  implies  Q must be true. 

The  converse  of  the  statement  if P  then Q  is  if  Q  then P.  The  statement P  if and  only  if  Q 

is  often  written  if P  then  Q  and  conversely.  Note  that  a 
statement  and  its  converse  do  not  have  the  same  truth  table. 

E X E R C I S E S  
DEFINITION 

Propositional  calculus is a  good  example of a  mathematical system.  Formally, propositional  calculus  can  be  defined  as  a  system  ~  consisting  of 

(1)  A  set  of primitive symbols, 
(2)  Rules  for generating well-formed statements, 
(3)  A  set of axioms,  and 
(4)  Rules  of inference. 

tWe  assume  "not"  takes  precedence  over  "implies."  Thus  the proper phrasing  of the sentence is (not P) implies (not Q). In general, "not" takes precedence over "and," which 
takes precedence  over  "or,"  which takes  precedence  over "implies." 

EXERCISES  23 
(1)  The  primitive  symbols  of g  are  ( , ) ,   ---~,  ~ ,   and  an  infinite  set  of statement 
letters  al,  a2,  a3 . . . . .   The  symbol ~  can  be  thought  of  as  implies  and  ~  as  not. 

(2)  A  well-formed  statement  is  formed  by  one  or  more  applications  of  the 
following  rules: 

(a)  A  statement  letter  is  a  statement. 

(b)  If  A  and  B  are  statements,  then  so  are  (~   A)  and  (A  ~  B). 
(3)  Let  A,  B,  and  C  be  statements.  The  axioms  of S  are 

A I : 
A2: 

A3: 

(A ~  ( n - - ,   A)) 

((A  ~  (n  ~  c))  ~  ((A  ~  B)  ~  (a  ~  C))) 
((,-, B  ---~  ~  A)  ~  ((~  B  ~  A)  ~  a)) 

(4)  The  rule  of  inference  is  modus  ponens,  i.e.,  from'  the  statements  (A  -----~ B) 
and  A  we  can  infer  the  statement  B. 

We  shall  leave  out  parentheses  wherever  possible.  The  statement  a  ---~ a  is  a 
theorem  of  S  and  has  as  proof  the  sequence  of  statements 

(i)  (a  ~  ((a  ---~ a)  ~  a))  ~  ((a  ---~ (a  ---~ a))  --~  (a  --~  a))  from  A2  with 
A  = a ,   B = ( a~ a ) ,   and  C = a.  

(ii)  a  ~  ((a  ~  a)  ~  a)  from  A1. 
(iii)  (a  ~  (a  ~  a))  ---~ (a  ~  a)  by  modus ponens  from  (i)  and  (ii). 

(iv)  a  ~  (a  ~  a)  from  A1. 

(v)  a  ---~ a  by  modus ponens  from  (iii)  and  (iv). 

"0.3.1.  Prove  that  (~  a  --~  a)  ~  a  is  a  theorem  of  S. 

0.3.2.  A  tautology  is  a  statement  that  is  true  for  all  possible  truth  values  of 

the  statement  variables.  Show  that  every  theorem  of  S  is  a  tautology. Hint: 

Prove  the  theorem  by  induction  on  the  number  of  steps  necessary 
to  obtain  the  theorem. 
**0.3.3.  Prove the converse of Exercise 0.3.2,  i.e., that every tautology is a theorem. 

Thus  a  simple  method  to  determine  whether  a  statement  of propositional 
calculus  is  a  theorem  is  to  determine  whether  that  statement  is  a  tau- 

tology. 
0.3.4.  Give  the  truth  table  for  the  statement  if P  then  if  Q  then  R. 

DEFINITION 

Boolean  algebra  can  be  interpreted  as  a  system  for  manipulating 
truth-valued  variables  using  logical  connectives  informally  interpreted 

as  and,  or,  and  not.  Formally,  a  Boolean  algebra  is  a  set  B  together with 
operations  * (and),  +  (or),  and  -  (not).  The  axioms  of  Boolean  algebra 

are  the  following:  For  all  a,  b,  and  c  in  B, 

(1)  a + ( b + c) = ( a + b ) + c 

a - ( b .   c)  =  (a-  b).  c. 
(2)  a  + b   = b . + a  

a . b   = b . a .  
(3)  a .   (b  +  c)  =  ( a- b )   +  (a.  c) a  + ( b . c )  

= ( a   +  b ) . ( a   +  c). 

(associativity) 
(commutativity) 

(distributivity) 

24  MATHEMATICAL PRELIMINARIES  CHAP.  0 

In  addition,  there  are  two  distinguished  members  of  B,  0  and  1  (in 
the  most  common  Boolean  algebra,  these  are  the  only  members  of  B, 
representing  falsehood  and  truth,  respectively),  with  the  following  laws: 

(4)  a  +  0  =  a 

a . l = a .  
(5)  a + a = l  

a . d   = 0 .  

The  rule  of inference  is  substitution  of equals  for  equals. 
*0.3.5.  Show that the following statements  are theorems in any Boolean  algebra: 

(a)  0  =  i. 
(b)  a + ( b . a ) = a + b .  
(c)  a = a .  
What  are  the  informal  interpretations  of these theorems ? 

0.3.6.  Let  A  be  a  set.  Show  that  (P(A)  is  a  Boolean  algebra  if  + , . ,   and  -  are 

u ,   N,  and  complementation with  respect  to  the  universe  A. 

**0.3.7.  Let  B  be  a  Boolean  algebra where  ~ B   =  n.  Show  that  n  =  2 m for  some 

integer m. 

0.3.8.  Prove  by  induction  that 

l + 2 + . . . + n = ~  
0.3.9.  Prove  by  induction  that 

n(n  +  1) 

(1  + 2 + . . .   + n )   z  =1 3   + 2   3  + . . .   +  n s 
"0.3.10.  What  is  wrong  with  the  following? 

THEOREM 

All marbles  have  the  same  color. 
Proof.  Let  A  be  any  set  of  n  marbles,  n  >  1.  We  shall  "prove"  by 
induction  on  n  that  all  marbles  in  A  have  the  same  color. 

Basis.  If  n  =  1,  all  marbles  in  A  are  clearly  of  the  same  color. 
Inductive  Step.  Assume  that  if  A  is  any  set  of  n  marbles,  then  all 
marbles in A  are the same color.  Let A'  be a  set of n  +  1 marbles,  n  >  1. 
Remove one marble from A'.  We  are then left with  a  set A"  of n  marbles, 
which,  by  the  inductive  hypothesis,  has  marbles  all  the  same  color. 

Remove  from  A"  a  second  marble  and  then  add  to  A"  the  marble 
originally  removed.  We  again  have  a  set  of  n  marbles,  which  by  the 
inductive  hypothesis  has  marbles  the  same  color.  Thus  the  two  marbles 
removed  must  have  been  the  same  color  so  that  the  set  A'  must  contain 
marbles  all  the  same  color.  Thus,  in  any  set  of  n  marbles,  all  marbles 
are  the  same  color.  D 

"0.3.11.  Let  R  be  a  well  order  on  a  set  A  and  S(a)  a  statement  about  a  in  A. 

Assume that if S(b) is true  for  all b  ~  a  such  that  b R  a,  then  S(a)  is true. 

SEC. 0.4  PROCEDURES  AND  ALGORITHMS  25 

Show that then S(a) is  true  for  all a  in  A.  Note  that  this  is  a  generaliza- 
tion  of the  principle  of simple induction. 

0.3.12.  Show that  there are only four unary logical connectives.  Give their truth 

tables. 

0.3.13.  Show  that  there  are  16  binary  logical  connectives. 
0.3.14.  Two  logical  statements  are  equivalent if they  have  the  same  truth  table. 

Show that (a)  ~ (P A Q) is equivalent  to  ~ p  v  ~  Q. 

(b)  ~ (P V Q)  is equivalent to  ~ P  A ~  Q. 
0.3.15.  Show that P  --~  Q  is equivalent  to  ~  Q ~  ~ P. 
0.3.16.  Show that P  ~  Q  is equivalent to P  A ~  Q ~  false. 
"0.3.17.  A  set  of  logical  connectives  is  complete if  for  any  logical  statement  we 

can  find  an  equivalent  statement  containing  only  those  logical  connec- 
tives.  Show  that  [A,  ~]  and  [V,  ~}  are  complete  sets  of  logical  con- 

nectives. 

B I B L I O G R A P H I C   N O T E S  
Church  [1956] and  Mendelson  [1968] give  good  treatments  of  mathematical 
logic.  Halmos  [1963] gives  a  nice  introduction  to  Boolean  algebras. 

0.4  PROCEDURES  AND  ALGORITHMS 

The  concept  of  algorithm  is  central  to  computing.  The  definition  of 
algorithm  can  be  approached  from  a  variety  of directions.  In  this  section  we 
shall  discuss  the  term  algorithm  informally  and  hint  at  how  a  more  formal 
definition  can  be  obtained. 

0.4.1.  Procedures 

We  shall  begin  with  a  slightly  more  general  concept,  that  of a  procedure. 
Broadly  speaking,  a  procedure  consists  of a  finite  set  of instructions  each  of 
which  can  be  mechanically  executed  in  a  fixed  amount  of  time  with  a  fixed 

amount  of effort.  A  procedure  can  have  any  number  of  inputs  and  outputs. 

To  be  precise,  we  should  define  the  terms  instruction,  input,  and  output. 
However,  we  shall  not  go  into  the  details  of such  a  definition  here  since  any 
"reasonable"  definition  is  adequate  for  our  needs. 

A  good example  of a  procedure  is a  machine  language computer  program. 
A  program  consists  of  a  finite  number  of  machine  instructions,  and  each 

instruction  usually  requires  a  fixed  amount  of  computation.  However,  pro- 

cedures  in  the  form  of  computer  programs  may  often  be  very  difficult  to 
understand,  so  a  more  descriptive  notation  wit!  be  used  in  this  book.  The 

26  MATHEMATICAL PRELIMINARIES  CHAI'. 0 
following  example  is  representative  of the  notation  we  shall  use  to  describe procedures  and  algorithms. 

Example 0.15 

Consider  Euclid's  algorithm  to  determine  the  greatest  common  divisor of two  positive integers p  and q. 

Procedure  1.  Euclidean  algorithm. lnput,  p 

and q,  positive integers. Output.  g, 

the  greatest  common  divisor  of p  and  q. Method. 
Step  1:  Let  r  be  the  remainder  of p/q. 
Step  2:  If r  =  0,  set g  =  q  and  halt.  Otherwise  set p  =  q,  then 

q  =  r,  and  go  to  step  1.  D 

Let  us  see  if procedure  1  qualifies  as  a  procedure  under  our  definition. 
Procedure  1  certainly  consists  of  a  finite  set  of  instructions  (each  step  is 
considered as  one instruction)  and  has  input  and  output.  However,  can  each 
instruction be mechanically executed with a  fixed amount  of effort ? 

Strictly  speaking,  the  answer  to  this  question  is  no,  because  i f p   and  q 
are  sufficiently  large,  the  computation  of the  remainder  of p/q  may  require 
an  amount  of effort  that  is proportional  in  some way to  the  size  of p  and q. 

However,  we could replace  step  1 by a  sequence  of steps  whose net  effect 
is  to  compute  the  remainder  of p/q,  although  the  amount  of  effort  in  each 

step would be fixed and independent  of the size  ofp  and q. (Thus the number 
of times each step is executed is an increasing function of the size ofp  and q.) 
These  steps,  for  example,  could  implement  the  customary  paper-and-pencil 
method  of doing integer  division. 

Thus  we  shall  permit  a  step  of  a  procedure  to  be  a  procedure  in  itself. 
So under this liberalized notion of procedure,  procedure  1 qualifies  as a  pro- cedure. 

In  general,  it is  convenient  to  assume  that  integers  are  basic  entities,  and 
we shall do so. Any integer can be stored in one memory cell, and any integer 
arithmetic  operation can be perforr~ed in one  step.  This  is  a  fair  assumption 

only if the  integers  are  less  than  2 k, where  k  is the  number  of bits  in  a  com- 
puter  word,  as  often  happens  in  practice.  However,  the  reader  should  bear 
in  mind.  the  additional  effort  necessary  to  handle  integers  of  arbitrary  size 
when  the  elementary  steps  handle  only integers  of bounded  size. 

We  must  now  face  perhaps  the  most  important  consideration--proving 
that  the  procedure  does what  it is  supposed  to  do.  For  each  pair  of integers 
p  and  q,  does  procedure  1  in  fact  compute  g  to  be  the  greatest  common 

divisor  of p  and  q ? The  answer  is  yes,  but  we  shall  leave  the  proof  of this 

particular  assertion  to  the  Exercises. 

We  might  note  in  passing,  however,  that  one  useful  technique  of proof 

SEC. 0.4  PROCEDURES  AND  ALGORITHMS  27 

for  showing  that  procedures  work  as  intended  is  induction  on  the  number 

of steps  taken. 

0.4.2.  Algorithms 

We  shall now  place  an  all-important  restriction  on  a  procedure  to  obtain 
what  is  known  as  an  algorithm. 

DEFINITION 

A  procedure  halts on  a  given input  if there  is  a  finite  number  t  such theft 
after  executing  t  (not  necessarily  different)  elementary  instructions  of  the 
procedure,  either  there  is  no  instruction  to  be  executed  next  or  a  "halt" 
instruction  was  last  executed.  A  procedure  which  halts  on  all inputs  is called 
an algorithm. 

Example 0.16 

Consider  the  procedure  of Example  0.15.  We  observe  that  steps  1 and  2 
must  be  executed  alternately.  After  step  1,  step  2  must  be  executed.  After 

step  2,  step  1 may be  executed,  or  there  may be  no  next  step;  i.e.,  the  proce- 
dure  halts.  We  can  prove  that  for  every  input  p  and  q,  the  procedure  halts 

after  at  most  2q  steps,t  and  that  thus  the  procedure  is  an  algorithm.  The 
proof  turns  on  observing  that  the  value  r  computed  in  step  1  is  less  than 

the  value  of q,  and  that,  hence,  successive  values  of q when  step  1 is executed 

form  a  monotonically  decreasing  sequence.  Thus,  by  the  qth  time  step  2  is 

executed,  r,  which cannot  be negative  and  is less than  the  current  value  of q, 

must  attain  the  value  zero.  When  r  =  0,  the  procedure  halts.  [3 

There are several reasons why a procedure may fail to halt on some inputs. 
It  is  possible  that  a  procedure  can  get  into  an  infinite  loop  under  certain 
conditions.  For  example,  if  a  procedure  contained  the  instruction 

Step  I:  If x  =  0,  then  go  to  Step  1,  else  halt, 
then  for  x  =  0  the  procedure  would  never  halt.  Variations  on  this  situation 

are  countless. Our  interest  will  be  almost  exclusively  in  algorithms.  We  shall  be  inter- 

ested  not  only in  proving that  algorithms  are  correct,  but  also  in  evaluating 
algorithms. The two main criteria for evaluating how well algorithms perform 
will  be 

(1)  The  number  of elementary mechanical  operations  executed  as  a  func- 
tion  of  the  size  of the  input  (time  complexity),  and 

(2)  How  large  an  auxiliary  memory  is  required  to  hold  intermediate 

tin  fact, 4 logs q is an upper  bound  on the number of steps executed for q >  1. We leave this as an exercise. 

28  MATHEMATICAL PRELIMINARIES  CHAP.  0 

results  that  arise  during  the  execution,  again  as  a  function  of  the  size  of the 
input  (space  complexity). 

Example  0.17 

in  Example 0.16 we saw that the number  of steps of procedure  1 (Example 
0.15)  that  would  be  executed  with  input  (p, q)  is  bounded  above  by  2q. 
The  amount  of memory used is one  cell for  each  of p, q,  and  r,  assuming  that 
any  integer  can  be  stored  in  a  single  cell.  If we  assume  that  the  amount  of 
memory  needed  to  store  an  integer  depends  on  the  length  of  the  binary 
representation  of that  integer,  the  amount  of memory  needed  is proportional 
to  log 2 n,  where  n  is  the  maximum  of inputs p  and  q.  [Z] 

0.4.3.  Recursive  Functions 

A  procedure  defines  a  mapping  from  the  set  of  all  allowable  inputs  to 
a  set  of  outputs.  The  mapping  defined  by  a  procedure  is  called  a  partial recursivefunc-tion 

or  recursivefunction.  If the  procedure  is  an  algorithm,  then 
the  mapping  is  called  a  total recursive function. 

A  procedure  can  also  be  used  to  define  a  language.  We  could  have  a  pro- 
cedure  to  which  we  can  present  an  arbitrary  string  x.  After  some  computa- 
tion,  the  procedure  would  output  "yes"  when  string  x  is  in  the  language. 

If x  is not  in  the  language,  then  the  procedure  may  halt  and  say  "no"  or  the 
procedure  may  never  halt. 

This  procedure  would  then  define  a  language L  as  the  set  of input  strings 
for  which  the  procedure  has  output  "yes."  The  behavior  of  the  procedure 

on  a  string  not  in  the  language  is  not  acceptable  from  a  practical  point  of 
view.  If the  procedure  has  not  halted  after  some  length  of time  on  an  input 
x,  we  would  not  know  whether  x  was  in  the  language  but  the  procedure  had 
not  finished  computing  or  whether  x  was  not  in  the  language  and  the  proce- 

dure  would  never  terminate. 

If  we  had  used  an  algorithm  to  define  a  language,  then  the  algorithm 
would  halt  on all inputs.  Consequently,  patience  is justified  with an algorithm 
in  that  we  know  that  if we  wait  long  enough,  the  algorithm  will  eventually 
halt  and  say  either  "yes"  or  "no." 

A  set  which  can  be  defined  by  a  procedure  is  said  to  be  recursively enu- merable. 

A  set which  can  be  defined  by an  algorithm  is called  recursive. 
If  we  use  more  precise  definitions,  then  we  can  rigorously  show  that 
there  are  sets  which  are  not  recursivety  enumerable.  We  can  also  show  that 
there  are  recursively  enumerable  sets  which  are  not  recursive. 

We  can  state  this  in  another  way.  There  are  mappings  which  cannot  be 
specified  by  any  procedure.  There  are  also  mappings  which  can  be  specified 
by  a  procedure  but  which  cannot  be  specified  by  an algorithm. 

We  shall  see  that  these  concepts  have  great  underlying  significance  for 

SEC. 0.4  PROCEDURES AND ALGORITHMS  29 

a  theory  of  programming.  In  Section  0.4.5  we  shall  give  an  example  of 

a procedure  for which it can be  shown that  there is no equivalent  algorithm. 

0.4.4.  Specification  of  Procedures 

In the previous section we informally defined what we meant by procedure 
and  algorithm.  It  is  possible  to  give  a  rigorous  definition  of  these  terms  in 
a  variety of formalisms.  In fact there  are  a  large number  of formal  notations 

for  describing  procedures.  These  notations  include 

(1)  Turing  machines  [Turing,  1936-1937]. 
(2)  Chomsky type 0 grammars  [Chomsky,  1959a and  1963]. 

(3)  Markov  algorithms  [Markov,  1951]. 
(4)  Lambda  calculus  [Church,  1941]. 
(5)  Post  systems  [Post,  1943]. 
(6)  Tag  systems  [Post,  1965]. 
(7)  Most  programming  languages  [Sammet,  1969]. 

This  list  can  be  extended  readily.  The  important  point  to  be  made  here 
is that it is possible  to  simulate  a procedure  written  in one  of these notations 

by  means  of  a  procedure  written  in  any  other  of  these  notations.  In  this 

sense  all these  notations  are  equivalent. 

Many  years  ago  the  logicians  Church  and  Turing  hypothesized  that  any 
computational  process  which  could  be  reasonably  called  a  procedure  could 

be  simulated  by a  Turing  machine.  This  hypothesis  is known  as the  Church- Turing thesis 

and  has  been  generally  accepted.  Thus  the  most  general  class 
of sets  that we would wish to  deal with in a practical  way would be included 
in the  class  of recursively enumerable  sets. 

Most  programming  languages,  at  least  in  principle,  have  the  capability 
of  specifying  any  procedure.  In  Chapter  11  (Volume  2)  we  shall  see  what 
consequences  this  capability  produces  when  we  attempt  to  optimize  pro- 
grams. 

We  shall  not  discuss  the  details  of these  formalisms  for procedures  here, 
although  some  of them  appear  in  the  exercises.  Minsky  [1967] gives  a  very 
readable  introduction  to  this  topic. 

In  our  book  we  shall  use  the  rather  informal  notation  for  describing 
procedures  and  algorithms  that  we  have  already  seen. 

0.4.5.  Problems 

We shall use the word problem in a rather  specific  way in this book. 
DEFINITION 

A problem (or  question) is  a  statement  (predicate)  which  is  either  true  or 
false,  depending  on  the  value  of  some  number  of  unknowns  of  designated 

30  MATHEMATICAL PRELIMINARIES  CHAP.  0 
type  in  the  statement.  A  problem  is  usually  presented  as  a  question,  and 
we  say  the  answer  to  the  problem  is  "yes"  if the  statement  is  true  and  "no" 

if the  statement  is  false. 

Example 0.18 

An  example  of a  problem  is  "x is less than y,  for integers  x  and y."  More 
colloquially,  we  can  express  the  statement  in  question  form  and  delete  men- 
tion  of the  type  of x  and y:  "Is  x  less  than  y ?" 

DEFINITION 

An  instance  of  a  problem  is  a  set  of  allowable  values  for  its  unknowns. 
For  example,  the  instances  of the  problem  of  Example  0.18  are  ordered pairs  of integers. 

A  mapping  from  the  set  of instances  of  a  problem  to  ~yes, no}  is  called 
a  solution  to  the  problem.  If this  mapping  can  be  specified  by  an  algorithm, 
then  the  problem  is  said  to  be  (recursively)  decidable  or  solvable.  If  no 
algorithm  exists  to  specify  this  mapping,  then  the  problem  is  said  to  be 

(recursively)  undecidable  or  unsolvable. 

One  of  the  remarkable  achievements  of  twentieth-century  mathematics 
was  the  discovery  of problems  that  are  undecidable.  We  shall  see  later  that 

undecidable  problems  seriously  hamper  the  development  of a  broadly  appli- 
cable  theory  of computation. 

Example 0.19 

Let  us  discuss  the  particular  problem  "Is  procedure  P  an  algorithm ?" 
Its  analysis  will  go  a  long  way  toward  exhibiting  why  some  problems  are 
undecidable.  First,  we must  assume  that  all procedures  are  specified  in  some 
formal  system  such  as  those  mentioned  earlier  in  this  section. 

It  appears  that  every formal  specification  language  for procedures  admits 
only  a  countable  number  of procedures.  While  we  cannot  prove  this  in  gen- 
eral,  we  give  one  example,  the  formalism  for  representing  absolute  machine 

language  programs,  and  leave  the  other  mentioned  specifications  for  the 

Exercises.  Any  absolute  machine  language  program  is  a  finite  sequence  of 
O's  and  l's  (which  we  imagine  are  grouped  32,  36,  48,  or  some  number  to 

a  machine  word). 

Suppose  that  we  have  a  string  of O's and  l's  representing  a  machine  lan- 
guage  program.  We  can  assign  an  integer  to  this  program  by  giving its  posi- 
tion  in  some  well  ordering  of  all  strings  of  O's  and  l's.  One  such  ordering 

can  be  obtained  by  ordering  the  strings  of O's  and  l's  in  terms  of increasing 

length  and  lexicographically  ordering  strings  of equal  length  by treating each 

string  as  a  binary  number.  Since  there  are  only  a  finite  number  of strings  of 
any  length,  every  string  in  {~0, 1~* is  thus  mapped  to  some  integer.  The  first 

SEC. 0.4 
few  pairs  in  this  bijection  are 

PROCEDURES  AND  ALGORITHMS  31 

Integer  String 

1  e 
2  0 

3  1 
4  00 

5  01 
6  10 
7  11 

8  000 
9  001 

In  this  fashion  we  see  that  for  each  machine  language  program  we  can  find 

a  unique  integer  and  that  for  each  integer  we  can  find  a  certain  machine 
language  program. 

It seems that no matter what formalism for  specifying procedures  is taken, 
we  shall  always  be  able  to  find  a  one-to-one  correspondence  between  pro- 

cedures  and  integers.  Thus  it  makes  sense  to  talk  about  the  ith  procedure  in 

any  given  formalism  for  specifying  procedures.  Moreover,  the  correspon- 
dence  between  procedures  and  integers  is  sufficiently  simple  that  one  can, 
given  an  integer  i,  write  out  the  ith  procedure,  or  given  a  procedure,  find  its 
corresponding  number. 

Let  us  suppose  that  there  is  a  procedure  Pj  which  is  an  algorithm  and 
takes  as  input  a  specification  of  a  procedure  in  our  formalism  and  returns 

the answer  "yes" if and  only if its input is an algorithm.  All known formalisms 
for  procedure  specification  have  the  property  that  procedures  can  be  com- 
bined  in  certain  simple  ways.  In  particular,  given  the  hypothetical  procedure 

(algorithm)  Pj, we  could  construct  an  algorithm  Pk to  work  as  follows: 

ALGORITHM Pk 

lnput.  Any  procedure  P  which  requires  one  input. Output. 

(1)  "No"  if  (a)P  is  not  an  algorithm  or  (b)P  is  an  algorithm  and P(P) 

=  "yes." 
(2)  "Yes"  otherwise. 

The  notation  P(P)  means  that  we  are  applying  procedure  P  to  its  own 
specification  as  input. Method. 

(1)  If Pj(P) =  "yes," then  go to  step (2).  Otherwise  output  "no"  and  halt. 
(2)  If  the  input  P  is  an  algorithm  and  P  takes  a  procedure  specification 

32  MATHEMATICAL PRELIMINARIES  CHAP.  0 

as input and gives "yes" or  "no"  as output,  Pk applies P  to itself (P) as input. 
(We assume that procedure  specifications are  such that these questions about 
input  and  output  forms  can  be  ascertained  by  inspection.  The  assumption is true  in  known  cases.) 

(3)  Pk gives output  "no"  or  "yes" if P  gives output  "yes" or  "no,"  respec- tively. 

We see that Pk is an algorithm,  on the assumption that P~ is an algorithm. 
Also  Pk  requires  one  input.  But  what  does  PE  do  when  its  input  is  itself? 
Presumably,  Pj  determines  that  Pk  is  an  algorithm  [i.e.,  Py(Pk)=  "yes"]. Pk 

then  simulates  itself on  itself.  But  now  Pk  cannot  give  an  output  that  is 
consistent.  If  Pk  determines  that  this  simulation  gives  "yes"  as  output,  Pk 
gives "no"  as output.  But Pk just  determined  that  it gave "yes"  when  applied to  itself.  A  similar paradox  occurs  if Pk finds that  the  simulation gives  "no." 

We must conclude  that it is fallacious  to assume  that  the  algorithm Pj exists, 

and  thus  the  question  "Is  P  an  algorithm ?"  is  not  decidable  for  any  of the known  procedure  formalisms.  [1 

We  should  emphasize  that  a  problem  is  decidable  if and  only  if there  is 
an  algorithm  which  will  take  as  input  an  arbitrary  instance  of that  problem 
and  give  the  answer  yes  or  no.  Given  a  specific  instance  of  a  problem,  we 
are  often  able  to  answer  yes  or  no  for  that  specific  instance.  This  does  not 
necessarily  make  the  problem  decidable.  We  must  be  able  to  give a  uniform 

algorithm  which  will  work  for  all  instances  of  the  problem  before  we  can say that  the  problem  is  decidable. 

As  an  additional  caveat,  we  should  point  out  that  the  encoding  of  the 
instances  of a  problem is vitally important.  Normally a  "standard"  encoding 

(one  that  can  be  mapped  by  means  of an  algorithm  into  a  Turing  machine 
specification)  is  assumed.  If nonstandard  encodings  are  used,  then  problems 
which  are  normally  undecidable  can  become  decidable.  In  such  cases,  how- 
ever,  there  will  be  no  algorithm  to  go  from  the  standard  encoding  to  the nonstandard.  (See  Exercise  0.4.21.) 

0.4.6.  Post's  Correspondence  Problem 

In  this  section we  shall introduce  one  of the  paradigm  undecidable  prob- 
lems,  called  Post's  correspondence  problem.  Later  in  the  book  we  shall  use 
this  problem  to  show  that  other  problems  are  undecidable. 

DEFINITION 

An  instance  of Post's  correspondence problem  over alphabet E  is  a  finite 
set  of  pairs  in  E /  *  E /  (i.e.,  a  set  of  pairs  of  nonempty  strings  over  E). 
The  problem  is  to  determine  if  there  exists  a  finite  sequence  of  (not  neces- 

sarily  distinct)  pairs  (xl, yl),  (x2, Y 2) , ... ,  (xm, Ym) such  that  x~x2...x=  = 

EXERCISES  33 
YlYz  "'"  Ym" We  shall  call  such  a  sequence  a  viable  sequence  for  this  instance 

of Post's  correspondence  problem.  We  shall  often  use  xlx2...xm  to  represent 
the  viable  sequence. 

Example 0.20 

Consider  the  following  instance  of  Post's  correspondence  problem  over 
{a, b}: 

{(abbb,  b),  (a,  aab),  (ba,  b)} 

The  sequence  (a,  aab),  (a,  aab),  (ba,  b),  (abbb,  b)  is  viable,  since (a)(a)(ba)(abbb)  =  (aab)(aab)(b)(b). 

The  instance  {(ab,  aba),  (aba,  baa),  (baa,  aa)}  of  Post's  correspondence 
problem  has  no  viable  sequences,  since  any  such  sequence  must  begin  with 

the  pair  (ab,  aba),  and  from  that  point  on,  the  total  number  of  a's  in  the 

first components  of the pairs  in the sequence  will always be less than  the num- 
ber  of a's  in  the  second  components.  D 

There  is  a  procedure  which  is in  a  sense  a  "solution"  to  Post's  correspon- 
dence  problem.  Namely,  one  can  linearly  order  all  possible  sequences  of 
pairs  of strings  that  can  be constructed  from  a  given  instance  of the  problem. 

One  can  then  proceed  to  test  each  sequence  to  see  if that  sequence  is  viable. 

On  encountering  the  first  viable  sequence,  the  procedure  would  halt  and 
report  yes.  Otherwise  the  procedure  would  continue  to  operate  forever. 

However,  there  is  no  algorithm  to  solve  Post's  correspondence  problem, 
for  one  can  show  that  if there  were  such  an  algorithm,  then  one  could  solve 
the  halting  problem  for  Turing  machines  (Exercise  0.4.22)--but  the  halting 

problem  for  Turing  machines  is  undecidable  (Exercise  0.4.14).  ' 

E X E R C IS E S  
0.4.1.  A  perfect  number  is  an  integer  which  is  equal  to  the  sum  of  all  its 

divisors  (including  1  but  excluding  the  number  itself).  For  example, 6 =   1 + 2 + 3   and  2 8 =   1 + 2 - t - 4 - t - 7   +  14  are  the  first  two  per- 

fect  numbers. (The  next  three  are  496,  8128, and  33550336.)  Construct 

a  procedure  which has  input  i  and  output  the  ith  perfect  number.  (At present  it is not  known whether there  are  a  finite  or  infinite  number  of 

perfect numbers.) 
0.4.2.  Prove  that  the  Euclidean  algorithm  of Example  0.15  is  correct. 
0.4.3.  Provide  an  algorithm  to  add  two  n-digit  decimal  numbers.  How  much 

time  and  space  does  the  algorithm  require  as  a  function  of  n ?  (See Winograd  [1965] for  a  discussion  of the  time complexity of addition.) 

0.4.4.  Provide  an  algorithm  to  multiply  two  n-digit  decimal  numbers.  How much time and space  does the algorithm require ? (See Winograd [1967] 

34  MATHEMATICAL PRELIMINARIES  CHAP. 0 

0.4.5. 
0.4.6. 

0.4.7. 

0.4.8. 

and  Cook  and  Aanderaa  [1969]  for  a  discussion  of  the  time  com- 
plexity  of multiplication.) 

Give  an  algorithm  to  multiply  two  integer-valued  n  by  n  matrices. 
Assume  that  integer  arithmetic  operations  can  be  done  in  one  step. 

What  is  the  speed  of  your  algorithm ?  If  it  is  proportional  to  n 3 steps, 
see  Strassen  [1969]  for  an  asymptotically  faster  one. 

Let  L  ~  [a,b}*.  The  characteristic  function  for  L  is  a  mapping 
.fL:Z---~  {0,  1 },  where  Z  is  the  set  of  nonnegative  integers,  such  that 

.fL  (i)  =  1  if  the  i th  string  in  {a,  b}*  is  in  L  and .fL  (i)  =  0  otherwise. 

Show  that  L  is  recursively  enumerable  if  and  only  if  .fL  is  a  partial 
recursive  function. 

Show  that L  is a  recursive  set if and  only  if both L  and  L  are  recursively 
enumerable. 

Let  P  be  a  procedure  which  defines  a  recusively  enumerable  set 
L  ~_ {a, b}*.  From  P  construct  a  procedure  P'  which  will  generate  all 

and  only  all  the  elements  of  L.  That  is,  the  output  of  P'  is  to  be  an 
infinite string of the form  xl  ~:  xz  ~  x3  ~  . . . ,   where  L  =  {xl,  x z , . .. ] .  
Hint:  Construct  P'  to  apply  i  steps  of  procedure  P  to  the jth  string  in 

[a,  b}*  for  all  (i, j),  in  a  reasonable  order. 

DEFINITION 

A  Turing  machine  consists  of  a  finite  set  of states  (Q),  tape  symbols 
(F),  and  a  function  ~  (the  next  move function,  i.e.,  program)  that  maps 
a  subset  of  Q  *  F  to  Q  *  F  *  [L,  R].  A  subset  ~  ~  F  is  designated 
as  the  set  of input symbols  and  one  symbol  in  I"  -  ~  is  designated  the 
blank.  One  state,  q0,  is  designated  the  start  state.  The  Turing  machine 
operates  on  a  tape,  one  square  of  which  is  pointed  to  by  a  tape  head. 
All  but  a  finite  number  of  squares  hold  the  blank  at  any  time.  A  con- 
figuration  of a  Turing  machine  is  a  pair  (q,  ~  F fl),  where  q  is  the  state, 

* fl  is  the  nonblank  portion  of  the  tape,  and  r  is  a  special  symbol, 
indicating  that  the  tape  head  is  at  the  square  immediately  to  its  right. 
(F does  not  occupy  a  square.) 

The  next  configuration  after  configuration  (q,  0cFfl)  is  determined 
by  letting  A  be  the  symbol  scanned  by  the  tape  head  (the  leftmost 
symbol  of  fl,  or  the  blank  if fl  =  e)  and  finding  ~(q,  A).  Suppose  that 

~(q, A)  =  (p,  A',  D),  where p  is  a  state,  A'  a  tape  symbol,  and  D  =  L 

or  R.  Then  the  next  configuration  is  (p,  0c' Ffl'),  where  ~'fl'  is  formed 
from  ~  r fl  by  replacing  the  A  to  the  right  of r  by  A'  and  then  moving 
the  symbol  in  direction  D  (left  if  D  =  L,  right  if  D  =  R).  It  may  be 
necessary  to  insert  a  blank  at  one  end  in  order  to  move  F. 

The  Turing  machine  can  be  thought  of  as  a  formalism  for  defining 
procedures.  Its  input  may  be  any  finite  length  string  w  in  E*.  The 

procedure  is executed by starting  with  configuration  (q0, r w) and  repeat- 
edly  computing  next  configurations.  If  the  Turing  machine  halts,  i.e., 

it  has  reached  a  configuration  for  which  no  move  is  defined  (recall  that 

may  not  be  specified  for  all  pairs  in  Q  x  F),  then  the  output  is  the 
nonblank  portion  of  the  Turing  machine's  tape. 

*0.4.9. 
'0.4.10. 

*'0.4.11. 

'0.4.12. 
'0.4.13. 

*'0.4.14. 

*'0.4.15. 

"0.4.16. 

0.4.17. 

0.4.18. 

0.4.19. 

0.4.20. 

EXERCISES  35 
Exhibit  a  Turing  machine  that,  given  an  input  w in  {0,  1}*, will  write 
YES  on  its  tape  if  w  is  a  palindrome  (i.e.,  w =  w R)  and  write  NO 
otherwise,  halting  in either case. 

Assume that  all Turing  machines  use  a  finite  subset  of some  countable 
set  of symbols,  ala2 .. ..  for  their  states  and  tape  symbols.  Show  that 

there  is  a  one-to-one  correspondence  between  the  integers  and  Turing 
machines. 

Show  that  there  is  no  Turing  machine  which  halts  on  all  inputs  (i.e., 
algorithm)  and  determines,  given integer  i written in binary  on its tape, 
whether  the  ith  Turing  machine  halts.  (See  Exercise 0.4.10.) 

Let  al, a2 . . . .   be  a  countable  set  of  symbols.  Show  that  the  set  of 
finite-length  strings  over  these  symbols  is  countable. 

Informally  describe  a  Turing  machine  which  takes  a  pair  of  integers 
i  and j  as-input,  and  halts  if  and  only  if the  ith  Turing  machine  halts 
when given the jth  string (as in Exercise 0.4.12)  as input.  Such  a  Turing 

machine  is  called  universal. 

Show  that  there  exists  no  Turing  machine  which  always  halts,  takes 
input  (i,])  a  pair  of  integers,  and  prints  YES  on  its  tape  if  Turing machine  i  halts  with  input  j  and  NO  otherwise. 

Hint:  Assume  such 
a  Turing  machine  existed,  and  derive  a  contradiction  as  in  Example 
0.19.  The  existence  of  a  universal  Turing  machine  is  useful  in  many 

proofs. 

Show  that  there  is no  Turing  machine  (not  necessarily  an  algorithm) 
which determines whether an arbitrary Turing machine is an algorithm. 

Note  that  this  statement  is  stronger  than  Exercise  0.4.14,  where  we 
essentially  showed  that  no  such  Turing  machine  which  always  halts 

exists. 

Show that it is undecidable whether a given Turing machine halts when 
started with blank tape. 

Show that the problem of determining whether a statement is a theorem 
in  propositional  calculus  is  decidable.  Hint:  See  Exercises  0.3.2  and 

0.3.3. 

Show  that  the  problem  of  deciding  whether  a  string  is  in  a  particular 
recursive set is decidable. 

Does  Post's  correspondence  problem  have  a  viable  sequence  in  the 

following instances ? 

(a)  (01,011),  (10, 000),  (00, 0). (b)  (1,  11), (11,101),  (101,011),  (011,  1011). 

How  do  you  reconcile  being  able  to  answer  this  exercise  with  the  fact 
that  Post's  correspondence  problem  is undecidable ? 

Show  that  Post's  correspondence  problem  with  strings  restricted  to  be 
over  the  alphabet  [a}  is  decidable.  How  do  you  reconcile  this  result 
with  the  und~idability  of Post's  correspondence  problem ? 

36  MATHEMATICAL PRELIMINARIES  CHAP.  0 

"0.4.21.  Let  P1,  Pz .. ..  be  an  enumeration  of  procedures  in  some  formalism. Define  a  new  enumeration  P~  P' 

,  z , . . .   as  follows 

(1)  Let  P~t-1  be  the  ith  of  P1, Pz . . . .   which  is  not  an  algorithm. 
(2)  Let  Pat  be  the  ith  of Pi,  Pz,.  * * which  is  an  algorithm. 

Then  there  is  a  simple  algorithm  to  determine,  given j,  whether P~, 
is  an  algorithm--just  see  whether j  is  even  or  odd.  Moreover,  each  of 
P1, Pz . . . .   is P~  for  some j.  How  do  you  reconcile  the existence of this 

one-to-one  correspondence  between  integers  and  procedures  with  the claims  of Example  0.19 ? 

* *0.4.22.  Show  that  Post's  correspondence  problem  is  undecidable.  Hint:  Given 

a  Turing  machine,  construct  an  instance  of  Post's  problem  which  has 
a  viable  sequence  if and  only  if the  Turing  machine  halts  when  started with  blank  tape. 

* 0.4.23.  Show  that  the  Euclidean  algorithm  in  Example  0.15  halts  after  at  most 

4  logz q  steps  when  started  with  inputs p  and q,  where q  >  1. 

DEFINITION 

A  variant  of  Post's  correspondence  problem  is  the  partial  corre- 
spondence  problem  over  alphabet  E.  An  instance  of  the  partial  corre- 
spondence problem is to determine, given a finite set of pairs in  E /  *  E+, 
whether  there  exists  for  each  m  >  0  a  sequence  of  not  necessarily  dis- 
tinct  pairs  (xl, y~),  (x2, Yz), * * *,  (Xm, Ym)  such  that  the  first  m  symbols 

of  the  stnng  X xXz  ...   Xm  coincide  with  the  first  m  symbols  of 

YlY2  "'"  ym, 

* *0.4.24.  Prove  that  the  partial  correspondence  problem  is  undecidable. 

B I B L I OG R A P H I C   N O T E S  
Davis  [1965]  is  a  good  anthology  of many  early  papers  in  the  study  of proce- 
dures and algorithms.  Turing's paper [Turing,  1936-1937] in which Turing machines 
first  appear  makes  particularly  interesting  reading  if  one  bears  in  mind  that  the 
paper  was  written  before  modern  electronic  computers  were  conceived. 

The  study  of  recursive  and  partial  recursive  functions  is  part  of  a  now  well- 
developed  branch  of mathematics  called  recursive  function  theory.  Rogers  [1967], 
Kleene  [1952],  and  Davis  [1958]  are  good  references  in  this  subject. 

Post's  correspondence  problem  first  appeared  in  Post  [1947].  The  partial  cor- 
respondence  problem  preceding  Exercise  0.4.24  is  from  Knuth  [1965]. 

Computational  complexity  is  the  study  of  algorithms  from  the  point  of  view 
of measuring  the  number  of primitive  operations  (time  complexity)  or  the  amount 
of  auxiliary  storage  (space  complexity)  required  to  compute  a  given  function. 
Borodin  [1970]  and  Hartmanis  and  Hopcroft  [1971]  give  readable  surveys  of 

this  topic,  and  Irland  and  Fischer  [1970]  have  compiled  a  bibliography  on  this subject. 

Solutions  to  many  of the  *'d  exercises  in  this  section  can  be  found  in  Minsky [1967]  and  Hopcroft  and  Ullman  [1969]. 

SEC. 0.5  CONCEPTS FROM GRAPH THEORY  37 
0.5.  CONCEPTS  FROM  GRAPH  THEORY 

Graphs  and  trees  provide  convenient  descriptions  of  many  structures 
that  are  useful  in  performing  computations.  In  this  section  we  shall  examine 
a  number  of  concepts  from  graph  theory  which  we  shall  use  throughout 
the  remainder  of our  book. 

0.5.1.  Directed  Graphs 

Graphs  can  be  directed  or  undirected  and  ordered  or  unordered.  Our 
primary  interest  will  be  in  ordered  and  unordered  directed  graphs. 

DEFINITION 

An  unordered directed graph G is a pair (A,  R), where A is a set of elements 
called  nodes  (or  vertices)  and  R  is  a  relation  on  A.  Unless  stated  otherwise, 
the  term  graph  will  mean  directed graph. 

Example 0.21 

Let  G = ( A , R ) ,   where  A = { 1 , 2 , 3 , 4 }   and  R = { ( 1 , 1 ) , ( 1 , 2 ) , ( 2 , 3 ) ,  
(2, 4),  (3, 4),  (4,  1), (4,  3)}. We  can  draw  a  picture  of the  graph  G by number- 
ing  four  points  1,  2,  3,  4  and  drawing  an  arrow  from  point  a  to  point  b  if 

(a,  b)  is  in  R.  Figure  0.6  shows  a  picture  of this  directed  graph.  [Z] 

Fig. 0.6  Example of a  directed  graph. 
A  pair  (a, b) in  R  is  called  an  edge or  arc  of G.  This  edge  is  said  to  leave 
node  a  and  enter node  b  (or  be  incident  upon  node  b).  For  example,  (1, 2)  is 

an  edge  in  the  example  graph.  If (a,  b)  is  an  edge,  we  say  that  a  is  a  prede- 

cessor  of b  and  b  is  a  successor  of a. 

Loosely  speaking,  two  graphs  are  the  same  if we  can  draw  them  to  look 
the  same,  independently  of what  names  we  give  to  the  nodes.  Formally,  we 

define  equality  of  unordered  directed  graphs  as  follows. 

DEFINITION 

Let  G1 =  (At,  R~)  and  G2  =  (A2,  R2)  be  graphs.  We  say  G~ and  G2  are 
equal (or  the same)  if there  is  a  bijectionf:  A~ ---~ A2  such  that  a  R~ b  if and 

38  MATHEMATICAL PRELIMINARIES  CHAP.  0 

only  if f  (a)Rzf(b).  That  is,  there  is  an  edge  between  nodes  a  and  b  in  G a if 

and  only  if there  is  an  edge  between  their  corresponding  nodes  in  G2. 

It  is  common  to  have  certain  information  attached  to  the  nodes  and/or 
edges  of  a  graph.  We  call  such  information  a  labeling. 

DEFINITION 

Let  (A,  R)  be  a  graph.  A  labeling  of  the  graph  is  a  pair  of  functions  f 
and g,  where f,  the  node  labeling,  maps  A  to  some  set,  and  g,  the  edge  labeling, 

maps  R  to  some  (possibly  distinct)  set.  Let  G~  =  (A 1, Rx)  and  G2  =  (Az,  R2) 

be  labeled  graphs,  with  labelings  (f~, gl)  and  (f2,  g2),  respectively.  Then  G1 
and  G z are  equal  labeled graphs  if  there  is  a  bijection  h"  A i ~  A 2 such  that 

(1)  a  R1 b if and  only if h(a) R2  h(b) (i.e.,  G1 and  Gz are equal  as unlabeled 
graphs). 

(2)  fl (a)  =  f2(h(a))  (i.e.,  corresponding  nodes  have  the  same  labels). 
(3)  ga((a,  b ) ) =   g2((h(a),  h(b)))  (i.e.,  corresponding  edges  have  the  same 
label.) 

In  many  cases,  only  the  nodes  or  only  the  edges  are  labeled.  These  situa- 
tions  correspond,  respectively,  to  f  or  g  having  a  single  element  for  its 
range.  In  these  cases,  condition  (2)  or  (3),  respectively,  is  trivially  satisfied. 

Example 0.22 

Let  G,  =  ((a,  b,  c}, [(a,  b),  (b,  c),  (c,  a)])and  G 2 =  ((0,  1, 2},  ((1, 0),  (2,  1), 
(0,  2)]).  Let  the  labeling  of  G I  be  defined  by  f~(a)  =  fl(b)  =  X,  f~(c)  =  Y, 
g~ ( ( a,b )) = g~ ( ( b,c ) )= ot ,   g l ( ( c , a ) ) = f l .   Let  the  labeling  of  G2  be 
fz(0)  =  f2(2)  =  X, f2(1)  =  Y, gz((0,  2))  =  g2((2,  1))  =  a,  and  gz((1,  0))  =  ft. G 1 and  G z are  shown  in  Fig.  0.7. 

G~  and  G z are  equal.  The  correspondence  is h(a)  =  O, h(b)  =  2,  h(c)  =  1. 

X 
b 

X  X 

Y 
(a)  G1  (b)  G 2 

Fig.  0.7  Equal  labelled  graphs. 

) 

) 

X 

SEC. 0.5  CONCEPTS FROM GRAPH THEORY  39 
DEFINITION 

A  sequence  of  nodes  (a0,  a t , . . . ,   a,),  n  ~  1,  is  a  path  of  length n  from 
node  a0  to  node  a,  if there  is an  edge  which  leaves  node  a~_ 1 and  enters  node 
a~  for  1 ~  i <  n.  For  example,  (1,  2,  4,  3)  is  a  path  in  Fig.  0.6.  If  there  is 

a  path  from  node  a 0 to  node  a,,  we  say that  a,  is  accessible from  a o. A  cycle 

(or  circuit)  is  a  path  (a0, a t , . . . ,   a,)  in  which  a0  =  a,.  In  Fig. 
0.6,  (1,  1)  is  a  cycle  of  length  1. 

A  directed  graph  is  strongly  connected if  there  is  a  path  from  a  to  b  for 
every  pair  of  distinct  nodes  a  and  b. 

Finally  we  introduce  the  concept  of  the  degree  of  a  node.  The  in-degree 
of  a  node  a  is  the  number  of  edges  entering  a  and  the  out-degree of a  is  the 

number  of edges  leaving  a. 

0.5.2.  Directed  Acyclic Graphs 

DEFINITION 

A  dag  (short  for  directed  acyclic  graph)  is  a  directed  graph  that  has  no 
cycles.  Figure  0.8  shows  an  example  of a  dag. 

A  node  having  in-degree  0  will  be  called  a  base  node.  One  having  out- 
degree  0  is  called  a  leaf  In  Fig.  0.8,  nodes  1,  2,  3,  and  4  are  base  nodes  and 

nodes  2,  4,  7,  8,  and  9  are  leaves. 

Q 

Fig. 0.8  Example of a  dag. 
If (a,  b) is an  edge in  a  dag,  a  is called  a  direct ancestor of b,  and b  a  direct descendant 

of a. 
If there  is  a  path  from  node  a  to  node  b,  then  a  is  said  to  be  an  ancestor 
of b  and  b  a  descendant  of  a.  In  Fig.  0.8,  node  9  is  a  descendant  of node  1 ; 
node  1 is  an  ancestor  of  node  9. 

Note  that  if R  is  a  partial  order  on  a  set  A,  then  (A,  R)  is  a  dag.  More- 
over,  if we  have  a  dag  (A,  R)  and  let  R'  be  the  relation  "is  a  descendant  of" 

on  A,  then  R'  is  a  partial  order  on  A. 

40  MATHEMATICAL PRELIMINARIES  CHAP. 0 

0.5.3.  Trees 

A  tree  is  a  special  type  of  dag  and  has  many  important  applications  in 
compiler  theory. 

DEFINITION 

An  (oriented)  tree  T  is  a  directed  graph  G  =  (A,  R)  with  a  specified  node 
r  in  A  called  the  root  such  that 

(1)  r  has  in-degree  0, 
(2)  All  other  nodes  of  T have  in-degree  1,  and 

(3)  Every  node  in  A  is  accessible  from  r. 

Figure  0.9  provides  an  example  of  a  tree  with  six  nodes.  The  root  is 

Fig. 0.9  Example  of a  tree. 
numbered  1.  We  shall  follow  the  convention  of  drawing  trees  with  the  root 

on top and  having  all  arcs directed  downward.  Adopting  this  convention  we 
can  omit  the  arrowheads. 

THEOREM 0.3 

A  tree  T  has  the  following  properties: 
(1)  T  is  acyclic. 
(2)  For  each  node  in  a  tree  there  is  a  unique  path  from  the  root  to  that 
node. 

Proof.  Exercise.  E] 
DEFINITION 

A  subtree  of  a  tree  T  =  (A,  R)  is  any  tree  T'  =  (A',  R')  such  that 

(1)  A'  is  nonempty  and  contained  in  A, 
(2)  R ' = A '   *  A ' A R ,   and 

(3)  No  node  of A  --  A'  is  a  descendant  of a  node  in  A'. 

SEC. 0.5  CONCEPTS FROM GRAPH THEORY  41 

For  example, 

is a  subtree  of the tree in Fig. 0.9. We  say that the root  of  a  subtree  dominates 
the  subtree. 

0.5.4.  Ordered  Graphs 

DEFINITION 

An  ordered  directed graph  is  a  pair  (A,  R)  where  A  is  a  set  of vertices  as 
before  and  R  is  a  set  of linearly  ordered  lists  of edges  such that each element 
of R  is  of the  form  ((a,  b~), (a,  b2),.  * *,  (a,  b~)),  where  a  is  a  distinct  member 
of A.  This  element  would  indicate  that,  for  vertex  a,  there  are  n  ares  leaving 
a,  the  first  entering  vertex  b~,  the  second  entering  vertex  b2,  and  so forth. 

Example 0.23 

Figure  0.10  shows  a  picture  of  an  ordered  directed  graph.  The  linear 
ordering  on  the  arcs  leaving  a  vertex  is  indicated  by  numbering  the  arcs 
leaving  a  vertex  by  1, 2 , . . . ,   n,  where  n  is  the  out-degree  of that  vertex. 

4 

Fig. 0.10  Ordered directed graph. 
The  formal  specification  for  Fig.  0.10  is  (A,  R),  where  A  =  [a,  b,  c}  and 
R  =  [((a,  c), (a, b), (a,  b),  (a,  a)),  ((b,  c))}.  D 

Notice  that  Fig.  0.10  is  not  a  directed  graph  according  to  our  definition, 
since  there  are  two  arcs  leaving  node  a  and  entering  node  b.  (Recall  that 
in  a  set  there  is  only  one  instance  of  each  element.) 

As  for  unordered  graphs,  we  define  the  notions  of labeling  and  equality 
of  ordered  graphs. 

42  MATHEMATICAL PRELIMINARIES  CHAP. 0 

DEFINITION 

A  labeling  of  an  ordered  graph  G  =  (A, R)  is  a  pair  of mappings f  and 
g  such  that 

(1) f :A  ----~ S  for  some  set  S  ( f  labels  the  nodes),  and 
(2)  g  maps  R  to  sequences  of symbols  from  some  set  T  such  that  g  maps 
((a,  b l ) , . . . ,   (a, b,))  to  a  sequence  of  n  symbols  of  T.  (g  labels  the  edges.) 

Labeled  graphs  Gi  =  (A1, R1)  and  Gz  =  (As, R2)  with  labelings  (fl,  gl) 
and  (fz, g2),  respectively,  are  equal  if  there  exists  a  bijection  h 'A  1 ~  A2 

such  that 

(1)  Ri  contains  ((a,  b ~ ) , . . . ,   (a, b,))  if  and  only  if  R2  contains 
((h(a),  h ( b l ) ) , . . . ,   (h(a),  h(b;))), (2) 

f~fa)  =  fz(h(a))  for  all  a  in  A 1, and (3) 

gl(((a,  bl),  .  .  .  ,  (a,  b,)))  :  g2((h(a),  h(bl)) .. . .  ,  (h(a),  h(b,))). 

Informally,  two  labeled  ordered  graphs  are  equal  if there  is  a  one-to-one 
correspondence  between  nodes  that  preserves  the  node  and  edge  labels.  If 
the  labeling  functions  all  have  a  range  with  one  element,  then  the  graph  is 
essentially  unlabeled,  and  only  condition  (1)  needs  to  be  shown.  Similarly, 

only the node labeling  or only the  edge labeling  may map  to  a  single  element, 
and  condition  (2)  or  (3)  will  become  trivial. For  each  ordered  graph  (A,  R),  there  is  an  underlying  unordered  graph 

(A, R')  formed  by  allowing  R'  to  be  the  set  of (a, b)  such  that  there  is  a  list 
((a, bl) . . . .   , (a, bn)) in  R,  and  b  =  bi for  some  i,  1 _~ i <  n. An 

ordered  dag  is  an  ordered  graph  whose  underlying  graph  is  a  dag. 
An  ordered  tree  is  an  ordered  graph  (A, R)  whose  underlying  graph  is 
a  tree,  and  such  that  if  ((a, bl),.  * *,  (a,  bn)) is  in  R,  then  b~ ~  bj,  if  i -~ j. 

Unless  otherwise  stated,  we  shall  assume  that  the  direct  descendants  of 
a  node  of  an  ordered  dag  or  tree  are  always  linearly  ordered  from  left  to 
right  in  a  diagram. There is a  great  distinction  between ordered  graphs  and unordered  graphs 

from  the  point  of view  of when  two  graphs  are  the  same. 

For  example  the  two  trees  T~ and  T2 in Fig.  0.11  are  equivalent  if  T1 and 

T1  7'2 

Fig. 0.11  Two trees. 

SEC.  0.5  CONCEPTS  FROM  GRAPH  THEORY  43 

T2  are  unordered.  But  if  T~ and  T2  are  ordered,  then  T1 and  T2  are  not the 
same. 

0.5.5.  Inductive  Proofs  Involving  Dags 

Many theorems  about  dags,  and  especially trees,  can  be proved by induc- 
tion,  but  it is  often  not  clear  on what  to  base the induction.  Theorems which 
yield  to  this  kind  of proof are  often  of the  form  that  something  is  true  for 
all,  or  a  certain  subset  of,  the  nodes  of the  tree.  Thus  we  must  prove  some- 
thing  about  nodes  of  the  tree,  and  we  need  some  parameter  of nodes  such 
that  the  inductive  step  can  be  proved. 

Two  such  parameters  are  the  depth of a  node,  the  minimum  path  length 
(or  in  the  case  of a  tree,  the  path  length)  from  a  base  node  (root in  the  case 

of a  tree) to  the given node,  and the height (or  level) of a  node,  the maximum 
path  length  from  the  node  to  a  leaf. 

Another  approach  to  inductions  on  finite  ordered  trees  is  to  order  the 
nodes  in  some  way  and  perform  the  induction  on  the  position  of the  node 

in  that  sequence.  Two  common  orderings  are  defined below. 

DEFINITION 

Let  T  be  a  finite  ordered  tree.  A  preorder  of the  nodes  of  T  is  obtained 
by applying step  1 recursively,  beginning  at  the  root  of T. 

Step  1:  Let this  application  of step  1 be to  node a.  If a  is  a  leaf, list node 
a  and  halt.  If a  is not  a  leaf,  let its direct descendants  be a~, a 2 , . . . ,   a,.  Then 
list  a  and  subsequently  apply  step  1 to  a~, a 2 , . . . ,   a,  in  that  order. A postorder 

of T is formed  by changing the  last  sentence  of step 1 to  read 
"Apply step  1 to  al,  a2,  * * *,  a,  in that  order  and then list a." 

Example  0.24 

Consider  the  ordered  tree  of  Fig.  0.12.  The  preorder  of  the  nodes  is 
123456789.  The  postorder  is  342789651.  [-] 

Sometimes  it is possible  to perform  an induction  on  the place that a  node 
has  in  some  order,  such  as  pre-  or  postorder.  Examples  of  these  forms  of 
induction  appear  throughout  the  book. 

0.5.6.  Linear  Orders  from  Partial  Orders 

If we  have  a  partial  order  R  on  a  set  A,  often  we  wish  to  find a  linear 
order  which  is  a  superset  of  the  partial  order.  This  problem  of embedding 
a  partial  order  in  a  linear  order  is  called  topological sorting. 

Intuitively,  topological  sorting  corresponds  to  taking  a  dag,  which  is  in 
effect  a  partial  order,  and  squeezing  the  dag  into  a  single  column  of  nodes 

44  MATHEMATICAL PRELIMINARIES  CHAP. 0 

Fig. 0.12  Ordered tree. 
such that  all edges point  downward.  The linear order is given by the position 

of nodes  in  the  column. For example, under  this  type  of transformation  the  dag of Fig. 0.8 could 

look  as  shown  in  Fig.  0.13. 

! 

Fig. 0.13  Linear order from the dag of Fig. 0.8. 
Formally we  say that R' is  a  linear order  that  embeds a partial order R  on 
a  set  A  if R'  is  a  linear  order  and  R  ~  R',  i.e.,  a  R  b  implies  that  a R' b  for 
all a, b in A.  Given a partial  order R, there are many linear orders that embed 
R  (Exercise 0.5.5).  The following algorithm finds  one  such linear order. 

SEC. 0.5  CONCEPTS  FROM GRAPH THEORY  45 

ALGORITHM 0.1 

Topological  sort. 
Input.  A  partial  order  R  on  a  finite  set  A. 

Output.  A  linear  order  R'  on  A  such  that  R  ~  R'. 
Method.  Since  A  is  a  finite  set,  we  can  represent  the  linear  order  R'  on 
A  as  a  list  a 1, a 2 . . . .   , a,  such  that  as  R'  aj  if  i  < j ,   and  A  =  [ a l , . . . ,   a,}. 
The  following  steps  construct  this  sequence  of elements: 

(1)  L e t i =   I , A  t = A , a n d R ~ = R .  
(2)  If  At  is  empty,  halt,  and  al,  a 2 , . . . ,   a~_l  is  the  desired  linear  order. 
Otherwise,  let  at  be  an  element  in  A~ such  that  a  R~ a~  is  false  for  all  a  ~  A~. 

(3)  Let  A,.+t  be  A ; -   {a,.} and  Ri+ 1  be  R,  ~  (A,.+i  *  A,.+~).  Then  let  i 
be  i  +  1 and  repeat  step  (2).  I~ 

If we  represent  a  partial  order  as  a  dag,  then  Algorithm  0. l  has  a  p ar ticu -  
larly  simple  interpretation.  At  each  step  (A~,  R,)  is  a  dag  and  ai  is  a  base 

node  of  (A i,  Ri).  The  dag  (A~+~, R,.+~) is  formed  from  (A,., R;)  by  deleting 
node  ai  and  all  edges  leaving  a,. 

Example 0.25 

Let  A  =  {a,  b,  c,  d}  and  R  ---- {(a,  b),  (a,  c),  (b,  d),  (c,  d)}.  Since  a  is  the 
only node  in A  such  that  a'  R a  is false  for  all  a'  E  A,  we must  choose  a 1 =  a. 

Then  A z  =  {b,  c,  d}  and  R z  =  {(b,  d),  (c,  d)};  we  now  choose  either  b  or 
c  for  a2.  Let  us  choose  az  =  b.  Then  A 3 =  {c, d}  and  R3  =  {(c, d)}.  Continu- 
ing,  we  find  a3  =  c  and  a 4 =  d. 

The  complete  linear  order  R'  is  {(a,  b),  (b,  c),  (c,  d),  (a,  c),  (b,  d),  (a,  d)}. 

THEOREM  0.4 

Algorithm  0.1  produces  a  linear  order  R'  which  embeds  the  given  partial 
order  R. 

Proof.  A  simple  inductive  exercise. 

0.5.7.  Representations  for  Trees 

A  tree  is  a  two-dimensional  structure,  but  in  many  situations  it  is  con- 
venient  to  use  only  one-dimensional  data  structures.  Consequently  we  are 

interested  in  having  one-dimensional  representations  for  trees  which  have 
all  the  information  contained  in  the  two-dimensional  picture.  What  we  mean 

by  this  is  that  the  two-dimensional  picture  can  be  recovered  from  the  one- 
dimensional  representation. 

Obviously  one  one-dimensional  representation  of  a  tree  T =   (A,  R) 
would  be  the  sets  A  and  R  themselves. 

46  MATHEMATICAL PRELIMINARIES  CHAP. 0 

But  there  are  also  other  representations.  For  example,  we  can  use  nested 
brackets  to  indicate  the  nodes  at  each  depth  of  a  tree.  Recall  that  the  depth 
of  a  node  in  a  tree  is  the  length  of  the  path  from  the  root  to  that  node.  For 
example,  in  Fig.  0.9,  node  1 is  at  depth  0,  node  3  is  at  depth  1,  and  node  6 
is  at  depth 2.  The  depth of a  tree  is the  length  of the  longest  path.  The  tree  of 

Fig.  0.9  has  depth  2. 

Using  brackets  to  indicate  depth,  the  tree  of Fig.  0.9  could  be represented 
as  1(2,  3(4,  5,  6)).  We  shall  call  this  the  left-bracketed  representation,  since 
a  subtree  is  represented  by  the  expression  appearing  inside  a  balanced  pair 

of parentheses  and  the  node  which  is  the  root  of that  subtree  appears  imme- 
diately  to  the  left  of the  left  parenthesis. 

DEFINITION 

In  general  the  left-bracketed representation of a  tree  T can  be  obtained  by 
applying  the  following  recursive  rules  to  T.  The  string  lrep(T)  denotes  the 
left-bracketed  representation  of  tree  T. 

(l)  If  T  has  a  root  numbered  a  with  subtrees  T1,  T 2 , . . . ,   Tk  in  order, 
then  lrep(T)  =  a(lrep(T1), l r e p ( T 2 ) , . . . ,   lrep(Tk)). 

(2)  If  T  has  a  root  numbered  a  with  no  direct  descendants,  then 
lrep(T)  =  a. 

If we delete  the  parentheses  from  a  left-bracketed  representation  of a tree, 
we  are  left  with  a  preorder  of the  nodes. 

We  can  also  obtain  a  right-bracketed representation for  a  tree  T,  rrep(T), 
as  follows: 

(1)  I f T h a s   a  root  numbered  a with  subtrees  T 1, T 2 , . . . ,   Tk, then  rrep(T) 
--  (rrep(T~),  r r e p ( T 2 ) , . . . ,   rrep(Tk))a. 

(2)  If  T  has  a  root  numbered  a  with  no  direct  descendants,  then 

rrep(T)  =  a. 

Thus  rrep(T)  for  the  tree  of  Fig.  0.12  would  be  ((3,  4)2,  ((7,  8,  9)6)5)1. 
In  this  representation,  the  direct  ancestor  is  immediately  to  the  right  of 
the  first  right  parenthesis  enclosing  that  node.  Also,  note  that  if  we  delete 
the  parentheses  we  are  left  with  a  postorder  of the  nodes. 

Another  representation  of  a  tree  is  to  list  the  direct  ancestor  of  nodes 
l,  2,  . . . ,   n  of  a  tree  in  that  order.  The  root  would  be  recognized  by  letting 
its  ancestor  be  0. 

Example 0.26 

The  tree  shown  in  Fig.  0.14  would  be  represented  by  0122441777.  Here 
0  in  position  1 indicates  that  node  1 has  "node  0"  as  its  direct  ancestor  (i.e., 

node  1 is the  root).  The  1 in  position  7 indicates  that  node  7 has  direct  ances- 
tor  1. 

sac.  0.5  CONCEPTS FROM GRAPH  THEORY  47 

Fig. 0.14  A tree. 
0.5.8.  Paths Through  a  Graph 

In  this  section  we  shall  outline  a  computationally  efficient  method  of 
computing  the  transitive  closure  of  a  relation  R  on  a  set  A.  If  we  view  the 

relation  as  an  unordered  graph  (A,  R),  then  the  transitive  closure  of  R  is 

equivalent  to  the  set  of  pairs  of  nodes  (a,  b)  such  that  there  is  a  path  from 

node  a  to  node  b. Another  possible  interpretation  is  to  view  the  relation  (or  the  unordered 

graph)  as  a  (square)  Boolean  matrix  (that  is,  a  matrix  of  O's  and  l's)  called 

an  adjacency matrix, in  which  the  entry  in  row  i, column j  is  1 if and  only  if 
the  element  corresponding  to  row  i is R-related  to  the  element corresponding 
to  column j.  Figure  0.15  shows  the  Boolean  matrix  M  corresponding  to  the 

1  2  3  4 
1  1  0  0 
0  0  1  1 

0  0  0  1 

1  0  i  1  0  Fig. 0.15  Boolean  matrix  for  Fig.  0.6. 

oo graph  of  Fig.  0.6.  If  M  is  a  Boolean  matrix,  then  M + =  ,=]=~ M"  (where  M" 
represents  M  Boolean  multipliedt  by  itself n  times)  represents  the  transitive 

"t'That is, use the usual formula for  matrix multiplication with the Boolean  operations 

* and  +  for multiplication and addition, respectively. 

48  MATHEMATICAL PRELIMINARIES  CHAP. 0 
closure  of  the  relation  represented  by  M.  Thus  the  algorithm  could  also  be 

used  as  a  method  of computing  M +. 

For  Fig.  0.15,  M + would  be  a  matrix  of all  l's. 
Actually  we  shall  give a  slightly more  general  algorithm  here.  We  assume 
that  we  have  an  unordered  directed  graph  in  which  there  is  a  nonnegative 
cost  ctj  associated  with  an  edge  from  node  i  to  node j.  (If  there  is  no  edge 
from  node  i  to  node j,  ctj  is  infinite.)  The  algorithm  will  compute  the  mini- 

mu m  cost  of  a  path  between  any  pair  of  nodes.  The  case  in  which  we  wish 
to  compute  only  the  transitive  closure  of  a  relation  R  over  { a l , . . . ,   a,}  is 
expressed  by  letting  ctj  =  0  if  at R  aj  and  c~i  =  oo  otherwise. 

ALCORITHM 0.2 

Minimum  cost  of paths  through  a  graph. 
Input.  A  graph  with  n  nodes  numbered  1, 2 , . . . ,   n  and  a  cost  function 
c~j for  1 <  i, j  <  n  with  ctj >_  0  for  all  i  and j. 

Output.  An  n  *  n  matrix  M  =  [ m j ,   with  m~j  the  lowest  cost  of  any 
path  from  node  i  to  node j,  for  all  i  and j. 

Method. 
(1)  Set  mi~ =  c~j for  all  i and j  such  that  1 <  i, j  ~  n. 

(2)  S e t k =   1. 
(3)  For  all  i  and j,  if mr1 >  m~k +  me j,  set  m~j to  m~k -t-  mk i- 

(4)  If k  <  n,  increase  k  by  1 and  go  to  step  (3).  If k  =  n,  halt.  [Z] 

The  heart  of  Algorithm  0.2  is  step  (3),  in  which  we  deduce  whether  the 
current  cost  of  going  from  node  i  to  node  j  can  be  made  smaller  by  first 
going  from  node  i  to  node  k  and  then  from  node  k  to  node j. 

Since  step (3) is executed  once  for  all possible  values  of i, j,  and  k,  Algorithm 
0.2  is  n 3 in  time  complexity. 

It is not  immediately clear that Algorithm  0.2  does  produce  the  minimum 
cost  of any  path  from  node  i  to j.  Thus  we  should  prove  that  Algorithm  0.2 
does  what  it  claims. 

THEOREM 0.5 

When  Algorithm  0.2  terminates,  mii  is  the  smallest  value  expressible  as 
e,,,.  +  .-.  +  c,~_lv ~  such  that  v 1 --  i  and  Vm  =  j.  (This  sum  is  the  cost  of 
the  path  v~, v2,  * * *,  vm from  node  i  to  node j.) 

SEC. 0.5  CONCEPTS  FROM  GRAPH  THEORY  49 

Pr o o f  To  prove  the  theorem  we  shall  prove  the  following  statement  by 
induction  on  1, the  value  of k  in  step  (3)  of the  algorithm. 

Statement  (0.5.1).  After  step  (3) is executed  with  k  = / ,   mr1 has  the  small- 
est  value  expressible  as  a  sum  of the  form  c,~,,  +  . . .   +  c~..,~.,  where  v~  =  i, 
v ,  =  j,  and  none  of  v 2 , . . . ,   Vm_ ~ is  greater  than  I. 

We  shall  call  this  min im um   value  the  correct  value  of  m~j  with  k  =  1. 
This  value  is  the  cost  of  a  cheapest  path  from  node  i  to  node j  which  does 

not  pass  through  a  node  whose  number  is  higher  than  l. 

Basis.  Let  us  consider  the  initial  condition,  which  we  can  represent  by 
letting  l  =  0.  [If  you  like,  we  can  think  of  step  (1)  as  step  (3)  with  k  =  0.] 

Wh en   l  --  0,  m  =  2,  so  mtj  =  ctj,  which  is  the  correct  initial  value. 

Inductive  Step.  Assume  that  statement  (0.5.1)  is  true  for  all  l  <  10. Let  us 
consider  the  value  of mt~  after  step  (3)  has  been  executed  with  k  =  lo. 

Suppose  that  the  m in im u m   sum  C.~v, +  . "   +  c.._,.,  for  mil  with  k  =  l0 
is such  that  no vp, 2  <  p  <  m  --  1, is equal  to lo. F ro m   the inductive  hypothe- 

sis  e.,~,-+-  ....  +  e.._,.,  is  the  correct  value  of  mtj  with  k  =  10  - - I ,   so 

c.,~.  +  . . .   +  c~..,.,  is  also  the  correct  value  of  mtj  with  k  --  10. 

N o w  suppose  that  the  m inim um   sum  stj  =  e.,~,  -+-  . . .   +  c~._,v,  for  mr1 
with  k  --  1o is  such  that  vp  =  l0  for  some  2  ~  p  <  m  --  1.  That  is,  stj  is the 

cost  of  the  path  v 1, v2,  * * *,  vm.  We  can  assume  that  there  is  no  node  vq  on 
this  path,  q  #:  p,  such  that  v~ is  10. Otherwise  the  path  vl,  v 2 , . . . ,   Vm contains 

a  cycle,  and we can  delete at  least  one  term  from  the  sum  c~,~, +  * * *  +  c~._,.. 
without  increasing  the  value  of  the  sum  stj.  Thus  we  can  always  find  a  sum 

for  stj  in  which  vp  =  10 for  only  one  value  of p,  2  ~  p  <  m  --  1. 

Let  us  assume  that  2 < p < m - -   1.  The  c a s e s p = 2 a n d   p - - m - -   1 
are  left  to  the  reader.  Let  us  consider  the  sum  st.,--  c.,.,  -+-  . . .   +  c.._,~. 

and  s..j  =  e.....,  +  . . .   +  c.._,..  (the  costs  of  the  paths  from  node  i  to  node 
v p and  from  node  v p to  node j  in  the  sum  s~).  F r o m   the  inductive  hypothesis 
we  can  assume  that  st..  is  the  correct  value  for  m;..  with  k  =  l0  --  1  and 

that  s~.j  is  the  correct  value  for  mv.j  with  k  =  l0  --  1.  Thus  when  step  (3) 

is  executed  with  k  =  l o,  mij  is  correctly  given  the  value  mi.,  +  m~.j. 

We  have  thus  shown  that  statement  (0.5.1)  is  true  for  all  1.  Wh en  l  =  n, 
statement  (0.5.1)  states  that  at  the  end  of  Algorithm  0.2,  m~  has  the  lowest 
possible  value.  [-7 

A  c o m m o n   special  case  of  finding  m in im u m   cost  paths  through  a  graph 
occurs  when  we  want  to  determine  the  set  of nodes  which  are  accessible  from 
a  given  node.  Equivalently,  given  a  relation  R  on  set  A,  with  a  in A,  we  want 
to  find  the  set  of b  in A  such  that  a  R + b,  where  R + is  the  transitive  closure  of 

R.  For  this  purpose  we  can  use  the  following  algorithm  of  quadratic  time 
complexity. 

50  MATHEMATICAL PRELIMINARIES  CHAP. 0 
ALGORITHM  0.3 

Finding  the  set of nodes  accessible from  a  given node  of a  directed  graph. 
Input.  Graph  (A,  R),  with  A  a  finite  set  and  a  in  A. 

Output.  The  set  of  nodes  b  in  A  such  that  a  R*  b. 
Method.  We  form  a  list  L  and  update  it  repeatedly.  We  shall  also  mark 
members  of  A  during  the  course  of  the  algorithm.  Initially,  all  members  of 
A  are  unmarked.  The  nodes  marked  will  be those  accessible  from  a. 

(1)  Set  L  =  a  and  mark  a. 

(2)  If  L  is  empty,  halt.  Otherwise,  let  b  be  the  first  element  on  list  L. 
Delete  b  from  L. 

(3)  For  all  c in  A  such  that  b  R  c  and  c  is unmarked,  add  c  to  the  b otto m 
of list  L,  mark   c,  and  go  to  step  (2).  [~] 

We  leave  a  proof  that  Algorithm  0.3  works  correctly  to  the  Exercises. 

E X E R C I S E S  
0.5.1.  What  is  the  maximum  number  of edges  a  dag  with  n  nodes  can  have ? 

0.5.2.  Prove Theorem 0.3. 
0.5.3.  Give  the  pre-  and  postorders  for  the  tree  of  Fig.  0.14.  Give  left-  and 

right-bracketed representations for the  tree. 

*0.5.4.  (a)  Design  an  algorithm  that  will  map  a  left-bracketed  representation 

of  a  tree  into  a  right-bracketed  representation. 
(b)  Design  an  algorithm  that  will map  a  right-bracketed representation 

of  a  tree  into  a  left-bracketed  representation. 

0.5.5.  How  many linear orders embed the  partial  order of the  dag of Fig.  0.8 ? 

0.5.6.  Complete the proof of Theorem 0.5. 
0.5.7.  Give  upper  bounds  on  the  time  and  space  necessary  to  implement 

Algorithm  0.1.  Assume  that  one  memory  cell  is  needed  to  store  any 
node  name  or  integer,  and  that  one  elementary step  is  needed  for  each 
of  a  reasonable  set  of  primitive  operations,  including  the  arithmetic 
operations  and  examination  or  alteration  of  a  cell  in  an  array  indexed 
by  a  known  integer. 

0.5.8.  Let  A  =  (a, b, c, d}  and  R  =  ((a, b),  (b, c), (a, c),  (b,  d)}.  Find  a  linear 

order  R'  such  that  R  ~  R'.  How  many  such  linear  orders  are  there ? 

DEFINITION 

An  undirected graph G is  a  triple (A, E, f )   where A is a set of nodes, 
E  is  a  set  of edge  names  and f  is  a  mapping  from  E  to  the  set  of  un- 
ordered  pairs  of node~.  Iff(e)  =  ~a, b],  then  we  mean  that  edge  e con- 
nects  nodes  a  and  b.  A  path  in  an  undirected  graph  is  a  sequence  of 

EXERCISES  51 
nodes  a0, al,  a2 . . . . .   an  such  that  there  is  an  edge  connecting  at_l  and 
at  for  1 _~ i ~  n.  An  undirected  graph  is  connected  if  there  is  a  path 

between  every  pair  of distinct  nodes. 

DEFINITION 

An  undirected  tree  can  be  defined  recursively  as  follows.  An 
undirected  tree  is  a  set  of  one  or  more  nodes  with  one  distinguished 

node  r  called  the  root  of  the  tree.  The  remaining  nodes  can  be  parti- 
tioned  into  zero  or  more  sets  T~ . . . .   , Tk  each  of  which  forms  a  tree. 

The  trees  T~ .....  Tk  are  called  the  subtrees  of  the  root  and  an 

undirected  edge  connects  r  with  all of  and  only  the  subtree roots. 

A  spanning  tree  for  a  connectefi  undirected  graph  G  is  a  tree  which 
contains  all nodes  of N. 

0.5.9.  Provide  an  algorithm  to  construct  a  spanning  tree  for  a  connected 

undirected  graph. 

0.5.10.  Let  (A,R)  be  an  unordered  graph  such  that  A  =  {1,2,3,  4}  and R  =  {(1, 2), (2,  3), (4,  1), (4,  3)}.  Find  R /,  the  transitive  closure  of  R. 

Let  the  adjacency  matrix  for  R  be  M.  Compute  M  /  and  show  that 

M  + is  the  adjacency  matrix  for  (A, R+). 

0.5.11.  Show  that  Algorithm  0.2  takes  time  proportional  to  n 3  in  basic  steps 

similar  to  those  mentioned  in  Exercise  0.5.7. 

0.5.12.  Prove  that  Algorithm  0.3  marks  node  b  if and  only  if a  R / b. 

0.5.13.  Show  that  Algorithm  0.3  takes  time  proportional  to  the  maximum  of 

# A   and  # R .  

0.5.14.  The  following  are  three  unordered  directed  graphs.  Which  two  are  the 

same ? 

G1  =  ({a, b,  c}, [(a,  b),  (b,  c), (c, a))) 

G2  =  ({a, b,  c), {(b, a),  (a,  c), (b,  c))) 

G3  =  ({a, b,  c], {(c, b),  (c, a),  (b, a)]) 

0.5.15.  The following are three ordered directed graphs with  only nodes labeled. 

Which  two  are  the  same 9. 

G1  =  ({a, b,  cl, {((a, b),  (a, c)), ((b,  a), (b,  c)), ((c, b))}) 
with  labeling  ll(a)  =  X,  ll(b)  =  Z,  and  It(c)  =  Y. 

G2  =  ([a,  b,  c}, [((a,  c)), ((b,  c), (b, a)),  ((c, b), (c, a))}) 
with  labeling/2(a)  =  Y,  lz(b)  =  X,  and/2(c)  =  Z. 

G3  =  ([a,  b,  c}, [((a,  c), (a,  b)),  ((b,  c)),  ((c, a),  (c, b))}) 
with  labeling/3(a)  =  Y,/3(b)  =  X,  and/3(c)  =  Z. 

52  MATHEMATICAL PRELIMINARIES  CHAP. 0 

0.5.16.  Complete  the  proof of Theorem 0.4. 
0.5.17.  Provide  an  algorithm  to  determine  whether  an  undirected  graph  is connected. 

"0.5.18.  Provide  an  algorithm  to  determine whether  two  graphs  are  equal. 
"0.5.19.  Provide  an  efficient  algorithm  to  determine  whether  two  nodes  of  a 

tree  are  on  the  same  path.  Hint: Consider  preordering  the  nodes. 
**0.5.20.  Provide  an  efficient  algorithm  to  determine  the  first  common  ancestor of two  nodes  of a  tree. 

Programming  Exercises 

0.5.21.  Write  a  program  that  will  construct  an  adjacency  matrix  from  a  linked list  representation  of a  graph. 

0.5.22.  Write  a  program  that  will  construct  a  linked  list  representation  of  a 

graph  from  an  adjacency  matrix. 
0.5.23.  Write  programs  to  impIement  Algorithms  0.1,  0.2,  and  0.3. 

B I B L I O G R A P H I C   NO TES 
Graphs  are  an  ancient  and  honorable  part  of  mathematics.  Harary  [1969], 
Ore  [1962], and  Berge  [1958] discuss  the  theory  of graphs.  Knuth  [1968] is  a  good 
source  for  techniques  for  manipulating  graphs  and  trees  inside  computers. 

Algorithm 0.2 is Warshalrs  algorithm as given in Floyd [1962a].  One interesting 
result  on  computing  the  transitive  closure  of  a  relation  is  found  in  Munro  [1971], 
where  it  is  shown  that  the  transitive  closure  of a  relation  can  be  computed  in  the 
time  required  to  compute  the  product  of two  matrices  over  a  Boolean  ring.  Thus, 
using  Strassen  [1969], the  time  complexity  of transitive  closure  is  no  greater  than n 2-81, not  n 3,  as  Algorithm  0.2  takes. 

1 

A N   INTRODUCTION 
TO  COMPILING 

This book considers the problems involved in mapping one representation 
of a  procedure  into  another.  The  most  common  occurrence  of this  mapping 
is  during  the  compilation  of  a  source  program,  written  in  a  high-level  pro- 

gramming language,  into  object code for a  particular  digital  computer. 

We  shall  discuss  algorithm-translating  techniques  which  are  applicable 
to  the  design  of  compilers  and  other  language-processing  devices.  To  put 

these  techniques  into  perspective,  in  this  chapter  we  shall  summarize  some 

of  the  salient  aspects  of  the  compiling  process  and  mention  certain  other 

areas  in  which  parsing  or  translation  plays  a  major  role. 

Like  the  previous  chapter,  those  who  have  a  prior  familiarity  with  the 
material,  in  this  case  compilers,  will  find  the  discussion  quite  elementary. 
These  readers  can  skip  this  chapter  or  merely  skim  it for  terminology. 

1.1.  PROGRAMMING  LANGUAGES 

In  this  section  we  shall  briefly  discuss  the  notion  of a  programming  lan- 
guage.  We  shall  then  touch  on  the  problems  inherent  in  the  specification  of 

a programming language and in the design of a translator for such a language. 

1.1.1.  Specification  of  Programming  Languages 

The  basic  machine  language  operations  of a  digital  computer  are  invari- 
ably  very  primitive  compared  with  the  complex  functions  that  occur  in 
mathematics,  engineering,  and  other  disciplines.  Although  any function  that 
can be specified as  a procedure  can be implemented  as a  sequence  of exceed- 
ingly simple  machine  language  instructions,  for  most  applications  it is  much 

53 

54  AN  INTRODUCTION  TO  COMPILING  CHAP.  1 
preferable  to  use  a  higher-level  language  whose  primitive  instructions 
approximate  the  type  of operations  that  occur inthe  application.  For  exam- 
ple,  if  matrix  operations  are  being  performed,  it  is  more  convenient  to 

write  an  instruction  of the  form 

A = B , C  
to  represent  the  fact  that  A  is  a  matrix  obtained  by  multiplying  matrices  B 
and  C  together  rather  than  a  long  sequence  of machine  language  operations 

whose  intent  is  the  same. 

Programming  languages  can  alleviate  much  of the  drudgery  of program- 
ming in machine language,  but they also introduce  a number of new problems 

of their own.  Of course,  since computers  can  still "understand"  only machine 
language,  a  program  written  in  a  high-level  language  must  be  ultimately 
translated  into  machine  language.  The  device performing  this  translation  has 

become  known  as  a  compiler. 

Another  problem  concerned  with  programming  languages  is  the  specifi- 
cation  of  the  language  itself.  In  a  minimal  specification  of  a  programming 

language  we  need  to  define 

(1)  The  set  of symbols  which  can  be  used  in  valid  programs, 
(2)  The  set  of valid  programs,  and 
(3)  The "meaning"  of each  vali,4 ~ros:am. 

Defining  the permissible  set of symbols is easy.  One  should bear in mind, 
however,  that  in  some  languages  such  as  SNOBOL  or  F O R T R A N ,   the 
beginning and/or end  of a card has significance and thus should be considered 
a  "symbol." Blank is also considered  a symbol  in some cases.  Defining the  set 

of programs which are "valid" is a much more difficult task. In many cases it is 
very hard just to  decide whether a given program should  be  considered  valid. 

In  the  specification  of programming  languages  it  has  become  customary 
to  define  the  class  of  permissible  programs  by  a  set  of  grammatical  rules 
which  allow  some  programs  of  questionable  validity  to  be  constructed.  For 

example,  many  F O R T R A N   specifications  permit  a  statement  of  the  form 

L  GOTO  L 
within  a  "valid"  FO R T R A N  program.  However,  the specification  of a  super- 

set  of the  truly  valid  programs  is  often  much  simpler  than  the  specification 
of all and  only those programs  which we would  consider valid in the narrow- est  sense  of the  word. 

The  third  and  most  difficult  aspect  of  language  specification  is  defining 
the meaning  of each valid program.  Several approaches  to  this problem  have 
been  taken.  One  method  is  to  define  a  mapping  which  associates  with  each 

valid  program  a  sentence  in  a  language  whose  meaning  we  understand.  For 

SEC. 1.1  PROGRAMMING  LANGUAGES  55 

example,  we  could  use  functional  calculus  or  lambda  calculus  as  the  "well- 
understood"  language.  Then  we can  define  the  meaning  of a  program  in  any 
programming  language  in  terms  of  an  equivalent  "program"  in  functional 

calculus  or  lambda  calculus.  By  equivalent  program,  we  mean  one  which 

defines  the  same  function. 

Another  method  of  giving  meaning  to  programs  is  to  define  an  idealized 
machine.  The  meaning  of  a  program  can  then  be  specified  in  terms  of  its 
effect on  this machine  started  off in  some predetermined initial configuration. 
In  this  scheme the  abstract  machine  becomes  an  interpreter  for  the  language. 

A  third  approach  is  to  ignore  deep  questions  of  "meaning"  altogether, 
and  this  is  the  appproach  we  shall  take  here.  For  us,  the  "meaning"  of  a 

source  program  is  simply  the  output  of  the  compiler  when  applied  to  the 
source  program. 

In  this  book  we  shall  assume  that  we  have  the  specification  of a  compiler 
as  a  set of pairs  (x, y),  where x  is a  source  language  program  and y  is a  target 
language  program  into  which  x  is  to  be  translated.  We  shall  assume  that  we 

know  what  this  set  of  pairs  is  beforehand,  and  that  our  main  concern  is 
the  construction  of  an  efficient  device  which  when  given  x  as  input  will  pro- 

duce  y  as  output.  We  shall  refer  to  the  set  of  pairs  (x, y)  as  a  translation. 
If each x  is a  string over alphabet  5: and y is a  string over A, then  a translation 

is  merely  a  mapping  from  l~* to  A*. 

1.1.2.  Syntax and  Semantics 

It  is  often  more  convenient  in  specifying  and  implementing  translations 
to  treat  a  translation  as  the  composition  of  two  simpler  mappings.  The  first 

of  these  relations,  known  as  the  syntactic  mapping,  associates  with  each 
input  (program  in  the  source  language)some  structure  which  is  the  domain 
for  the  second  relation,  the semantic mapping.  It  is not  immediately apparent 
that  there  should  be  any  structure  which  will  aid  in  the  translation  process, 

but  almost  without  exception,  a  labeled  tree  turns  out  to  be  a  very  useful 

structure  to  place  on  the  input.  Without  delving  into  the  philosophy  of  why 
this  should  be  so,  much  of  this  book  will  be  devoted  to  algorithms  for  the 

efficient  construction  of  the  proper  trees  for  input  programs. 

As  a  natural  example  of  how  tree  structures  are  built  on  strings,  every 
English  sentence  can  be  broken  down  into  syntactic  categories  which  are 

related  by  grammatical  rules.  For  example,  the  sentence 

"The  pig  is  in  the  pen" 
has  a  grammatical  structure  which  is indicated  by the  tree  of Fig.  1.1,  whose 
nodes  are  labeled  by  syntactic  categories  and  whose  leaves  are  labeled  by 
the  terminal  symbols,  which,  in  this  case,  are  English  words. 

Likewise,  a  program  written  in  a  programming  language  can  be  broken 

56  AN  INTRODUCTION TO  COMPILING  CHAP.  1 

<sentence> J 
<noun  phrase> 
<adjective>  <noun> I  I 

the  pig 

<verb> I 

is 

<verb phrase>. / ~  

<phrase> 
<preposition>  <noun  phrase> I  /  \ 

in  <adjective>  <noun> I  I 

the  pen 
Fig. 1.1  Tree structure for English sentence. 

down  into  syntactic  components  which  are  related  by  syntactic  rules  govern- 
ing  the  language.  F or  example,  the  string 

a + b , c  
may have  a  syntactic structure  given by the tree of Fig.  1.2.t  The term parsing 
or  syntactic analysis is  given  to  the  process  of finding  the  syntactic  structure 

<expression> 

<term>  <term>  *  <factor> 
<factor>  <factor>  <identifier> 
<identifier>  <identifier>  c 

b 
Fig. 1.2  Tree from arithmetic expression. 

tThe use of three syntactic categories, (expression), (term), and (factor), rather than just (expression), is forced on us by our desire that the structure of an arithmetic expres- 
sion be unique. The reader should bear this in mind, lest our subsequent examples of the syntactic analysis  of arithmetic expressions  appear unnecessarily complicated. 

BIBLIOGRAPHIC NOTES  57 
associated  with  an  input  sentence.  The  syntactic  structure  of  a  sentence  is 
useful  in  helping to  understand  the  relationships  among  the  various  symbols 

of a  sentence. 

The  term  syntax  of a  language  will  refer  to  a  relation  which  associates 
with  each  sentence  of  a  language  a  syntactic  structure.  We  can  then  define 

a  valid  sentence  of  a  language  as  a  string  of  symbols  which  has  the  overall 

syntactic  structure  of (sentence).  In the  next  chapter  we  shall discuss  several 
methods  of rigorously  defining  the  syntax  of a  language. 

The  second  part  of  the  translation  is  called  the  semantic  mapping,  in 
which  the  structured  input  is  mapped  to  an  output,  normally  a  machine 
language  program.  The  term  semantics of a language will refer  to  a  mapping 
which  associates  with  the  syntactic  structure  of each  input  a  string  in  some 
language  (possibly  the  same  language)  which  we  consider  the  "meaning" 
of the  original  sentence.  The  specification  of the  semantics  of a  language  is 
a  very difficult  matter  which  has  not  yet  been  fully  resolved,  particularly  for 
natural  languages,  e.g.,  English. 

Even  the  specification  of  the  syntax  and  semantics  of  a  programming 
language  is  a  nontrivial  task.  Although  there  are  no  universally  applicable 

methods,  there  are  two  concepts  from  language  theory  which  can  be  used  to 

make  up  part  of the  description. 

The  first  of these  is  the  concept  of a  context-free  grammar.  Most  of the 
rules  for  describing  syntactic  structure  can  be  formalized  as  a  context-free 
grammar.  Moreover,  a  context-free  grammar  provides  a  description  which 
is  sufficiently  precise  to  be  used  as  part  of the  specification  of the  compiler 
itself.  In  Chapter  2 we  shall present  the  relevant  concepts  from  the  theory  of 

context-free  languages. 

The  second  concept  is  the  syntax-directed  translation  schema,  which  can 
be  used  to  specify  mappings  from  one  language  to  another.  We  shall  study 
syntax-directed  translation  schemata  in  some  detail  in  Chapters  3  and  9. 

In  this  book  an  attempt  has  been  made  to  present  those  aspects  of lan- 
guage  theory  and  other  formal  theories  which  bear  on  the  design  of  pro- 
gramming  languages  and  their  compilers.  In  some  cases  the  impact  of  the 

theory  is  only  to  provide  a  framework  in  which  to  talk  about  problems  that 

occur in  compiling.  In  other  cases the  theory  will provide  uniform  and  prac- 
ticable  solutions  to  some  of  the  design  problems  that  occur  in  compiling. 

B I B L I O G R A P H I C   NO TE S 
High-level  programming  languages  evolved  in  the  early  1950's. At  that  time computers  lacked  floating-point  arithmetic  operations,  so  the  first  programming 
languages  were  representations  for  floating-point  arithmetic.  The  first  major programming language was  FORTRAN,  which  was  developed  in the  mid-!950's. 

58  AN  INTRODUCTION  TO  COMPILING  CHAP.  1 
Several other algebraic languages were also developed at that time, but FORTRAN 
emerged as the most widely used language.  Since that  time hundreds  of high-level 
programming  languages  have  been  developed.  Sammet  [1969] gives an  account of many  of the  languages  in  existence  in  the  mid-1960's. 

Much  of  the  theory  of  programming  languages  and  compilers  has  lagged behind  the  practical  development.  A  great  stimulus  to  the  theory  of formal  lan- 
guages  was  the  use  of what  is  now  known  as  Backus  Naur  Form  (BNF)  in  the syntactic  definition  of  ALGOL  60  (Naur  [1963]). This  report,  together  with  the 
early  work  of  Chomsky  [1959a, 1963], stimulated  the  vigorous  development  of the  theory  of  formal  languages  during  the  1960's.  Much  of  this  book  presents 
results from  language  theory  which  have relevance to the design and  understand- ing of language translators. 

Most  of the  early work  on  language  theory  was  concerned  with  the  syntactic definition of languages. The semantic definition of languages, a much more difficult 
question,  received  less  attention  and  even  at  the  time  of the  writing  of this  book 
was not a  fully resolved matter.  Two good anthologies on, the formal specification of  semantics  are  Steel  [1966] and  Engeler  [1971].  The  IBM  Vienna  laboratory 

definition  of  PL/I  [Lucas  and  Walk,  1969] is  one  example  of  a  totally  formal approach  to  the  specification  of a  major programming language. 

One of the more interesting developments in programming languages has been the  creation  of extensible  languages--languages  whose  syntax  and  semantics can 
be  changed  within  a  program.  One  of the  earliest  and  most  commonly proposed 
schemes for language extension is the macro definition.  See McIlroy [1960], Leaven- worth  [1966], and  Cheatham  [1966], for  example.  Galler  and  Perlis  [1967] have 

suggested  an extension scheme whereby new data  types  and new operators can be introduced  into  ALGOL.  Later  developments  in  extensible  languages  are  con- 

tained  in  Christensen  and  Shaw  [1969] and  Wegbreit  [1970]. ALGOL  68  is  an 
example of a major programming language with language extension facilities  [Van Wijngaarden,  1969]. 

1.2.  AN  OVERVIEW  OF  COMPILING 

We  shall  discuss  techniques  and  algorithms  which  are  applicable  to  the 
design  of  compilers  and  other  language-processing  devices.  To  put  these 
algorithms  into  perspective,  in  this  section  we  shall  take  a  global  picture  of 
the  compiling  process. 

1.2.1,  The  Portions  of  a  Compiler 

Many  compilers  for  many  languages  have  certain  processes  in  common. 
We  shall  attempt  to  abstract  the  essence  of some  of these processes.  In  doing 

so we shall attempt to  remove from these  processes  as  many  machine-depen- 
dent  and  operating-system-dependent  considerations  as  possible.  Although 
implementation  considerations  are  important  (a  bad  implementation  can 

destroy  a  good  algorithm),  we  feel  that  understanding  the  fundamental 

SEC. 1.2  AN  OVERVIEW  OF  COMPILING  59 

nature  of  a  problem  is  essential  and  will  make  the  techniques  for  solution 

of that  problem  applicable  to  other  basically  similar  problems. 

A  source  program  in  a  programming  language  is  nothing  more  than 
a  string of characters.  A  compiler ultimately converts this string of characters 
into  a  string  of  bits,  the  object  code.  In  this  process,  subprocesses  with 

the  following  names  can  often  be  identified: 

(1)  Lexical  analysis. 
(2)  Bookkeeping,  or  symbol  table  operations. 

(3)  Parsing  or  syntax  analysis. 

(4)  Code  generation  or  translation  to  intermediate  code  (e.g.  assembly 

language). (5)  Code  optimization. 

(6)  Object  code  generation  (e.g.  assembly). 
In  any given compiler,  the  order  of the processes  may be slightly  different 
from  that  shown,  and  several  of the  processes  may be combined into  a  single 
phase.  Moreover,  a compiler should not be shattered by any input it receives; 

it  must  be capable  of responding  to  any input  string.  For  those  input  strings which  do  not  represent  syntactically  valid  programs,  appropriate  diagnostic 

messages  must  be  given. 

We shall describe the  first five phases  of compilation  briefly.  These phases do  not  necessarily  occur  separately  in  an  actual  compiler.  However,  it  is 

often  convenient  to  conceptually  partition  a  compiler  into  these  phases  in 

order  to  isolate  the  problems  that  are  unique  to  that  part  of the  compilation 

process. 

1.2.2.  Lexical Analysis 

The lexical analysis phase comes first. The input to the compiler and hence 
the  lexical  analyzer  is  a  string  of  symbols  from  an  alphabet  of  characters. In  the  reference  version  of  PL/I  for  example,  the  terminal  symbol  alphabet 

contains  the  60 symbols 

A B   C  . . .   Z  $ @ ~  

0  1  2  . . .   9  blank 

u 

= / - , / ( ) , . ; " & l ~ > <   7% 

In  a  program,  certain  combinations  of  symbols  are  often  treated  as  a 
single  entity.  Some  typical  examples  of  this  would  include  the  following" 

(1)  In  languages  such  as PL/I  a  string  of one  or  more  blanks  is normally treated  as  a  single  blank. 

(2)  Certain  languages  have  keywords  such  as  BEGIN,  END,  GOTO, 
DO,  INTEGER,  and  so  forth  which  are  treated  as  single  entities. 

60  AN  INTRODUCTION  TO  COMPILING  CHAP.  1 

(3)  Strings  representing  numerical  constants  are  treated  as  single  items. 
(4)  Identifiers  used  as  names  for  variables,  functions,  procedures,  labels, 
and  the  like  are  another  example  of  a  single  lexical  unit  in  a  programming 

language. 

It  is  the  job  of  the  lexical  analyzer  to  group  together  certain  terminal 
characters  into  single  syntactic  entities,  called  tokens.  What  constitutes 
a  token  is  implied  by  the  specification  of  the  programming  language.  A 
token  is a  string of terminal  symbols, with which we associate  a  lexical  struc- 
ture  consisting  of a  pair  of the  form  (token  type,  data).  The  first  component 

is  a  syntactic  category  such  as  "constant"  or  "identifier,"  and  the  second 
component  is  a  pointer  to  data  that  have  been  accumulated  about  this  par- 
ticular  token.  For  a  given  language  the  number  of  token  types  will  be  pre- sumed  finite.  We  shall  call  the  pair  (token  type,  data)  a  "token"  also,  when 

there  is no  source  of confusion. 

Thus  the  lexical  analyzer  is  a  translator  whose  input  is  the  string  of 
symbols  representing  the  source  program  and  whose  output  is  a  stream  of 
tokens.  This  output  forms  the  input  to  the  syntactic  analyzer. 

Example 1.1 

Consider  the  following  assignment  statement  from  a  FORTRAN-like 
language" 

COST  --  (PRICE  /  TAX)  *  0.98 

The  lexical  analysis  phase  would  find  COST,  PRICE,  and  TAX  to  be  tokens 
of  type  identifier  and  0.98  to  be  a  token  of  type  constant.  The  characters 

--,  (,  -+-, ),  and  *  are  tokens  by  themselves.  Let  us  assume  that  all  constants 
and  identifiers  are  to  be  mapped  into  tokens  of  the  type  (id).  We  assume 
that  the  data  component  of a  token  is  a  pointer,  an  entry  to  a  table  contain- 
ing  the  actual  name  of  the  identifier  together  with  other  data  we  have  col- 
lected  about  that  particular  identifier.  The  first component  of a  token  is used 
by  the  syntactic  analyzer  for  parsing.  The  second  component  is  used  by  the 

code  generation  phase  to  produce  appropriate  machine  code. 

Thus  the  output  of  the  lexical  analyzer  operating  directly  on  our  input 
string  would  be  the  following  sequence  of tokens" 

(ida1  =  ((ida2  +  (id)3)  *  (ida, 
Here  we  have  indicated  the  data  pointer  of a  token  by  means  of a  subscript. 
The  symbols  --,  (,  + ,   ), and  * are to be construed  as tokens whose token type 

is  represented  by  themselves.  They  have  no  associated  data,  and  hence  we 

indicate  no  data  pointer  for  them.  [---I 

SEC.  1.2  AN  OVERVIEW  OF  COMPILING  61 

Lexical  analysis is  easy if tokens  of more  than  one  character  are  isolated 
by  characters  which  are  tokens  themselves.  In  the  example  above,  --,  (,  -+-, 
and  *  cannot  appear  as  part  of  an  identifier,  so  COST,  PRICE,  and  TAX 
can  be  readily  distinguished  as  tokens. 

However,  lexical  analysis  may  not  be  so  easy  in  general.  For  example, 
consider  the  following valid  F O R T R A N   statements: 

(1)  DO  10  I  =  1.15 
(2)  DO  10  I - -   1,15 

In statement (1) DO  10 l i s a  variablet  and  1.15 a constant.  In statement (2) 
DO  is a  keyword,  10 a  constant,  and  I  a variable;  1 and  15 are  constants. 

If a  lexical  analyzer  were  implemented  as  a  coroutine  [Gentleman,  1971; 
Mcllroy,  1968] and were  to  start  at  the  beginning  of one  of these  statements, 
with a command such as  "find the next token,"  it could  not  determine  if that 

token  was  DO  or  DO  10  I  until it  had reached the comma or decimal point. 

Thus  a  lexical  analyzer  may need to  look  ahead  of the  token  it is actually 
interested  in.  A  worse  example  occurs  in  PL/I,  where  keywords  may  also  be 

variables.  Upon  seeing  an  input  string  of  the  form 

DECLARE(X1,  X 2 , . . . ,   Xn) 
the  lexical  analyzer  would  have  no  way  of  telling  whether  DECLARE  was 
a  function  identifier  and  X1,  X 2 , . . . ,   Xn  were  its  arguments  or  whether 

DECL ARE   was  a  keyword  causing  the  identifiers  X1,  X2,  . . . ,   Xn  to  have 
the  attribute  (or  attributes)  immediately  following  the  right  parenthesis. 

Here the  distinction  would have  to  be made  on what  follows  the  right paren- 
thesis.  But  since  n  can  be  arbitrarily  large,** the  PL/I  lexical  analyzer  might 

have to look ahead  an arbitrary distance.  However, there is another approach 
to lexical analysis that is less convenient but it avoids the problem of arbitrary 

lookahead. 

We  shall  define  two  extreme  approaches  to  lexical  analysis.  Most  tech- 
niques  in  use  fall  into  one  or  the  other  of  these  categories  and  some  are 
a  combination  of the  two: 

(1)  A  lexical  analyzer  is said to  operate  directly if,  given a  string  of input 
text  and a  pointer into  that  text,  the  analyzer will determine  the  token  imme- 
diately to  the  right  of the  place  pointed  to  and  move  the  pointer  to  the  right 

of the  portion  of text  forming  the  token. 

(2)  A  lexical  analyzer  is  said  to  operate  indirectly  if,  given  a  string  of 
text,  a  pointer  into  that  text,  and  a  token  type,  it  will  determine  if  input 

tRecall  that in FORTRAN blanks are ignored. :]:The language specification does not  impose an  upper  limit on n.  However, a  given 
PL/I compiler will. 

62  AN  INTRODUCTION  TO  COMPILING  CHAP. 1 
characters  appearing  immediately  to  the  right  of  the  pointer  form  a  token 

of  that  type.  If  so,  the  pointer  is  moved  to  the  right  of the  portion  of  text 
forming  that  token. 

Example 1.2 

Consider  the  F OR T R A N  text 

DO  10  I =   1,15 
with  the  pointer  currently  at  the  left  end.  An  indirect  lexical  analyzer  would 

respond  "yes" if asked for a token  of type DO  or a token of type (identifier). 
In  the  former  case,  the  pointer  would  be  moved  two  symbols  to  the  right, 

and  in  the  latter  case,  five  symbols  to  the right. 

A  direct  lexical  analyzer  would  examine  the  text  up  to  the  comma  and 
conclude  that  the  next  token  was  of type  DO.  The  pointer  would  then  move 
two  symbols to  the  right,  although  many  more  symbols would  be scanned  in 
the  process.  El 

Generally,  we  shall  describe  parsing  algorithms  under  the  assumption 
that  lexical  analysis  is  direct.  The  backtrack  or  "nondeterministic"  parsing 
algorithms  can  be  used  with  indirect  lexical  analysis.  We  shall  include  a  dis- 
cussion  of this  type  of parsing  in  Chapters  4  and  6. 

1.2.3.  Bookkeeping 

As  tokens  are  uncovered  in  lexical  analysis,  information  about  certain 
tokens  is  collected  and  stored  in  one  or  more  tables.  What  this  information 

is  depends  on  the  language.  In  the  F O R TR A N   example  we  would  want  to 
know  that  COST,  PRICE,  and  TAX  were  floating-point  variables  and  0.98 

a  floating-point  constant. 

Assuming  that  COST,  PRICE,  and  TAX  have  not  been  declared  in 
a type  statement,  this information  about  these  variables  can be  gleaned from 
the  fact  that  COST,  PRICE,  and  TAX  begin  with  letters  other  than  I,  J,  K, 
L,  M,  or  N. 

As  another  example  about  collecting  information  about  variables,  con- 
sider  a  F O R T R A N   dimension  statement  of  the  form 

D IMENSION  A(10,20) 
On  encountering  this  statement,  we  would  have  to  store  information  that  A 
is  an  identifier  which  is  the  name  of  a  two-dimensional  array  whose  size  is 

10 by 20. 

In  complex  languages  such  as  PL/I,  the  number  of facts  which  might  be 
stored  about  a  given  variable  is  quite  large--on  the  order  of a  dozen  or  so. 

Let  us  consider  a  somewhat  simplified  example  of a  table  in which  infor- 

SEC. 1.2  AN  OVERVIEW OF  COMPILING  63 

mation  about  identifiers  is  stored.  Such  a  table  is often  called  a symbol table. 
The  table  will  list  all  identifiers together  with  the  relevant  information  con- 
cerning  each  identifier. 

Suppose  that  we  encounter  the  statement 

COST  =  (PRICE  +  TAX)  *  0.98 
After  this  statement,  the  table  might  appear  as  follows" 

Entry  Identifier  Information 

1  COST  Floating-point  variable 2  PRICE  Floating-point  variable 
3  TAX  Floating-point  variable 4  0 . 9 8   Floating-point  constant 

On  encountering  a  future  identifier  in  the  input  stream,  this  table  would 
be  consulted  to  see  whether  that  identifier  has  already  appeared.  If  it  has, 
then  the  data  portion  of  the  token  for  that  identifier  is  made  equal  to  the 

entry  number  of  the  original  occurrence  of  the  variable  with  that  name. 

For  example,  if  a  succeeding  statement  in  the  F O R T R A N   program  con- 
tained  the  variable  COST,  then the  token  for  the  second  occurrence  of COST 
would  be  (id)l,  the  same  as  the  token  for  the  first  occurrence  of  COST. 

Thus  such  a  table  must  simultaneously  allow  for 

(1)  The  rapid  addition  of  new  identifiers  and  new  items  of  information, 
and (2)  The  rapid  retrieval  of  information  for  a  given  identifier. 

The  method  of  data  storage  usually  used  is  the  hash  (or  scatter)  table, 
which  will  be  discussed  in  Chapter  10  (Volume  II). 

1.2.4.  Parsing 

As  we  mentioned  earlier,  the  output  of the  lexical  analyzer  is  a  string  of 
tokens.  This  string  of tokens  forms  the  input  to  the  syntactic  analyzer,  which 

examines  only  the  first  components  of  the  token  (the  token  types).  The 
information  about  each  token  (second  component)  is  used  later  in  the  com- 
piling  process  to  generate  the  machine  code.  * 

Parsing,  or  syntax analysis,  as it is sometimes  known,  is a process in which 
the string  of tokens  is examined  to  determine  whether  the  string  obeys  certain 
structural  conventions  explicit  in  the  syntactic  definition  of  the  language. 

It  is  also  essential  in  the  code  generation  process  to  know  what  the  syn- 
tactic  structure  of  a  given  string  is.  For  example,  the  syntactic  structure  of 
the  expression  A  q-  B  *  C  must  reflect  the  fact  that  B  and  C  are  first  multi- 

64  AN INTRODUCTION  TO  COMPILING  CHAP. 1 
plied and  that  then  the  result  is  added to  A.  No  other  ordering  of the  opera- 
tions  will  produce  the  desired  calculation. 

Parsing  is  one  of the  best-understood  phases  of compilation.  From  a  set 
of syntactic  rules  it  is  possible  to  automatically  construct  parsers  which  will 
make  sure  that  a  source  program  obeys  the  syntactic  structure  defined  by 
these  syntactic rules.  In  Chapters  4-7  we shall  study several different parsing 

techniques  and  algorithms  for  generating  a  parser  from  a  given  grammar. 

The  output  from  the parser  is  a  tree  which  represents  the  syntactic  struc- 
ture inherent in the source program. In many ways this tree structure is closely 
related  to  the  parsing  diagrams  we  used  to  make  for  English  sentences  in 
elementary  school. 

Example 1.3 

Suppose  that  the  output  of the  lexical  analyzer is  the  string  of tokens 

(1.2.1)  (id),  :  ( (id)2  +  (id)3)  *  (id)4 
This  string  conveys  the information  that  the  following  three  operations  are 
to  be performed  in  exactly the  following  way: 

(1)  (id)3  is  to  be  added  to  (id)2, 

(2)  The  result  of (1)is  to  be  multiplied  by  (id)4,  and 
(3)  The  result  of  (2)  is  to  be  stored  in  the  location  reserved  for  (id)~. 

This  sequence  of steps  can be pictorially represented in terms  of a  labeled 
tree,  as  shown  in  Fig.  1.3. That  is,  the  interior  nodes  of  the  tree  represent 

<id>l  <id>4 
<id> 7  +  <id> 3 

Fig.  1.3  Tree  structure. 

sEc. 1.2  AN  OVERVIEW  OF  COMPILING  65 

actions  which  must  be  taken.  The  direct  descendants  of  each  node  either 
represent  values to  which the  action  is to  be applied  (if the  node  is labeled  by 
an  identifier  or  is  an  interior  node)  or  help  to  determine  what  the  action 
should  be. (In particular,  the  = ,   q-,  and  *  signs do this.) Note  that the paren- 
theses  in  (1.2.1)  do  not  explicitly appear  in  the  tree,  although  we  might want 
to  show  them  as  direct  descendants  of n 1. The  role  of the  parentheses  is  only 

to  influence  the  order  of computation.  If they  did  not  appear  in  (1.2.1),  then 
the  usual  convention  that  multiplication  "takes  precedence"  over  addition 
would  apply,  and  the  first  step  would  be  to  multiply  (ida3  and  (id)4.  [Z] 

1.2.5.  Code  Generation 

The  tree  built  by  the  parser  is  used  to  generate  a  translation  of the  input 
program.  This translation  could be a program in machine language,  but more 

often  it  is  in  an  intermediate  language  such  as  assembly  language  or  "three 
address  code."  (The  latter  is  a  sequence  of  simple  statements;  each  involves 
no  more  than  three  identifiers,  e.g.  A  =  B,  A  =  B  -t-  C,  or  GOTO  A.) 

If  the  compiler  is  to  do  extensive  code  optimization,  then  code  of  the 
three  address  type  is  preferred.  Since  three  address  code  does  not  pin  com- 
putations  to  specific computer  registers,  it is  easier  to  use  registers to  advan- 

tage when optimizing. If little or no  optimization is to be done,  then assembly 

or  even  machine  code  is preferred  as  an  intermediate  language.  We  shall give 

a  running  example  of  a  translation  into  an  assembly  type  language  to  illus- 
trate  the  salient  points  of the  translation  process. 

For  this discussion  let us assume that  we have  a  computer with one work- 
ing register (the accumulator)  and  assembly language instructions  of the form 

Instruction  Effect 
LOAD  m ADDt  m 

MPY  m STORE  m 
LOAD  =m ADD  =m 

MPY  =m 

c(m) ~  accumulator c(accumulator) +  c(m) ~  accumulator 
c(accumulator) * c(m) ~  accumulator c(accumulator) ~  m 

m ---, accumulator c(accumutator) +  m ~  accumulator 
c(accumulator) * m ~  accumulator 
tLet us assume that ADD and MPY refer to floating-point operations. 

Here  the  notation  c(m) ---~ accumulator,  for  example,  means  the  contents  of 
memory  location  m  are  to  be  placed  in  the  accumulator.  The  expression 

= m   denotes  the  numerical  value  m.  With  these  comments,  the  effects  of 
the  seven  instructions  should  be  obvious. 

The  output  of the  parser  is  a  tree  (or  some  representation  of  one)  which 
represents  the  syntactic structure  inherent  in  the  string  of tokens  coming  out 

66  AN  INTRODUCTION  TO  COMPILING  CHAP.  1 
of  the  lexicai  analyzer.  From  this  tree,  and  the  information  stored  in  the 
symbol  table,  it is possible  to  construct  the  object  code.  In  practice,  tree  con- 
struction  and  code  generation  are  often  carried  out  simultaneously,  but 
conceptually  it  is  easier  to  think  of these  two  processes  as  occurring  serially. 

There  are several methods for specifying how the intermediate  code is to be 
constructed  from  the  syntax  tree.  One  method  which  is  particularly  elegant 
and  effective is  the  syntax-directed translation. Here  we  associate  with  each 
node  n  a  string  C(n) of intermediate  code.  The  code  for  node  n  is constructed 
by  concatenating  the  code  strings  associated  with  the  descendants  of n  and 

other  fixed  strings  in a fixed  order.  Thus  translation  proceeds  from  the  bot- 
tom  up  (i.e.,  from  the  leaves  toward  the  root).  The  fixed  strings  and  fixed 
order  are  determined  by  the  algorithm  used.  More  will  be  said  about  this 
in  Chapters  3  and  9. 

An  important  problem  which  arises  is  how  to  select  the  code  C(n) for 
each  node  n  such  that  C(n) at  the  root  is  the  desired  code  for  the  entire 
statement.  In  general,  some  interpretation  must  be  placed  on  C(n) such  that 
the  interpretation  can  be  uniformly  applied  to  all  situations  in  which  node 
n  can  appear. 

For  arithmetic  assignment  statements,  the  desired  interpretation  is  fairly 
natural  and.  will  be  explained  in  the  following  paragraphs.  In  general,  the 
interpretation  must  be  specified  by  the  compiler  designer  if  the  method  of 

syntax-directed  translation  is  to  be  used.  This  task  may  be  easy  or  hard,  and 
in  difficult  cases,  the  detailed  structure  of  the  tree  may  have  to  be  adjusted 
to  aid  in  the  translation  process. 

For  a  specific  example,  we  shall  describe  a  syntax-directed  translation  of 
simple  arithmetic  expressions.  We  notice  that  in  Fig.  1.3,  there  are  three 
types  of  interior  nodes,  depending  on  whether  their  middle  descendant  is 
labeled  = ,   + ,   or  ,.  These  three  types  of nodes  are  shown  in  Fig.  1.4,  where 

(a)  (b) 

Fig. 1.4  Types of interior nodes. 

(c) 
P 

/ 

represents  an  arbitrary  subtree  (possibly  a  single  node).  We  observe  that 
for  any  arithmetic  assignment  statement  involving  only  arithmetic  operators 

+  and  ,,  we  can  construct  a  tree  with  one  node  of  type  (a)  (the  root)  and 
other  interior  nodes  of  types  (b)  and  (c)  only. 

The  code  associated  with  a  node  n  will  be  subject  to  the  following  inter- 
pretation" 

SEC. 1.2  AN  OVERVIEW  OF  COMPILING  6'7 

(1)  If n  is  a  node  of type  (a),  then  C(n) will  be  code  which  computes  the 
value  of the  expression  on  the  right  and  stores  it in  the  location  reserved for 
the  identifier  labeling  the  left  descendant. 

(2)  If n is a node of type (b) or (c), then  C(n) is code which, when preceded 
by  the  operation  code  LOAD,  brings  to  the  accumulator  the  value  of  the 
subtree  dominated  by n. 

Thus,  in  Fig.  1.3,  when  preceded  by LOAD,  C(nl) brings  to  the  accumu- 
lator  the  value  of  (id)2  /  (id)3,  and  C(n2) brings  to  the  accumulator  the 
value  of ((id)2  -t- (id)3)  * (id)4.  C(n3)  is  code,  which  brings  the  latter  value 
to  the  accumulator  and  stores  it  in  the  location  of (id)l. 

We  must  consider  how  to  build  C(n) from  the  code  for  n's  descendants. 
In  what  fellows,  we  assume  that  assembly  language  statements  are  to  be 
generated  in  one  string,  with  a  semicolon  or  a  new  line  separating  the state- 
ments.  Also,  we  assume  that  assigned  to  each  node  n  of  the  tree  is  a  level number l(n), 

which  denotes  the  maximum  length  of a  path  from  that  node  to 
a  leaf.  Thus,  l(n)=  0  if  n  is  a  leaf,  and  if  n  has  descendants  n t , . . .   , nk, l(n) 

=  maxx_<~_<k l(n~) +  1.  We can compute  l(n)  bottom up,  at the  same time 
that  C(n) is  computed.  The  purpose  of recording  levels  is  to  control  the  use 

of temporary  stores.  We  must  never  store  two  needed  quantities  in  the  same 
temporary  location  simultaneously.  Figure  1.5  shows  the  level  numbers  of 
each  node  in  the  tree  of Fig.  1.3. 

We  shall  now  define  a  syntax-directed  code  generation  algorithm  to 

3  n3 

<i  ~>4 
<id>  2  +  <id~  3 0  0  0 

Fig. 1.5  Level numbers. 

68  AN  INTRODUCTION  TO  COMPILING  CHAP.  1 
compute  C(n) for  all  nodes  n  of  a  tree  consisting  of  leaves,  a  root  of  type 

(a),  and  interior  nodes  of either  type  (b)  or  type  (c). 

ALGORITHM 1.1 

Syntax-directed  translation  of  simple  assignment  statements. 
Input.  A  labeled  ordered  tree  representing  an  assignment  statement 
involving  arithmetic  operations  -+-  and  *  only.  We  assume  that  the  level  of 

each  node  has  been  computed. Output. 

Assembly language  code  to  perform  the  assignment. Method. 

Do  steps  (i)  and  (2)  for  all  nodes  of level  0.  Then  do  steps  (3), 
(4), (5)  on all nodes  of level  1, then  level 2,  and  so forth,  until  all nodes  have 

been  acted  upon. 

(1)  Suppose  that  n  is  a  leaf with  label  (id~>j. 

(i)  Suppose  that  entry  j  in  the  identifier  table  is  a  variable.  Then C(n) 

is  the  name  of that  variable. 
(ii)  Suppose  that  entry j  in  the  identifier  table  is  a  constant  k.  Then C(n) 

is  ' = k . ' t  
(2)  If  n  is  a  leaf,  with  label  = ,   ,,  or  -t-,  then  C(n) is  the  empty  string. 
(In  this  algorithm,  we  do  not  need  or  wish  to  produce  an  output  for  leaves 

labeled  = ,   ,,  or  + . )  

(3)  If n  is  a  node  of type  (a)  and  its  descendants  are  hi,  n2,  and  n 3, then C(n) 

is  'LOAD'  C(ns)';  STORE'  C(nl). 
(4)  If n  is  a  node  of type  (b)  and  its  descendants  are  nx,  n2,  and  n a,  then C(n) 

is  C(n3)';  STORE  $'  l(n)';  LOAD'  C(nl)';  ADD  $'  l(n). 

This  sequence  of instructions  uses  a  temporary  location  whose  name  is 
the  character  $ followed  by the  level number  of node  n.  It  is  straightforward 
to  see  that  when  this  sequence  is preceded  by  LOAD,  the  value  finally resid- 
ing in the  accumulator  will be  the  sum  of the  values  of the  expressions  domi- 
nated  by nl  and  n3. 

We  make  two  comments  on  the  choice  of temporary  names.  First,  these 
names  are  chosen  to  start  with  $  so  that  they  cannot  be  confused  with  the 
identifier  names  in  FORT RAN.  Second,  because  of  the  way  l(n) is  chosen, 
we  can  claim  that  C(n) contains  no  reference  to  a  temporary  $i if i is  greater 
than  l(n). Thus,  in  particular,  C(nx) contains  no  reference  to  '$'  l(n). We can 
thus  guarantee  that  the  value  stored  into  '$'  l(n) will  still  be  there  when  it  is 

added  to  the  accumulator. 

(5)  If all is as in (4) but  node n is of type (c),  then  C(n) is 

C(n3) ';  STORE  $'  l(n)  ';  LOAD'  C(nl)';  MPY  $'  l(n). 
This  code  has  the  desired  effect,  with  the  desired  result  appearing  in  the 

accumulator.  5 

1"For emphasis, we  surround  with quotes those strings which represent themselves, rather than naming a string. 

SEC. 1.2  AN OVERVIEW OF COMPILING  69 

We  leave  a  proof  of  the  correctness  for  Algorithm  1.1  for  the  Exercises. 
It  proceeds  recursively  on  the  height  (i.e.,  level)  of  a  node. 

Example 1.4 

Let  us  apply  Algorithm  1.1  to  the  tree  of  Fig.  1.3.  The  tree  given  in  Fig. 1.6  has  the  code  associated  with  each  node  explicitly  shown  on  the  tree. 

The  nodes  labeled  (id)l  through  (id)4  are  given  the  associated  code  COST, 

LOAD  = 0.98 STORE  $2 

LOAD  TAX n 3  STORE  $1 

LOAD  PRICE ADD  $1 
MPY  $2 STORE  COST 

/  LOAD / 

Y 
TAX  ) STORE  ~  . 

n l j  sl LOAD  PRICE  < 1 ~ 

ADD  $1 

<  id >2  +  <  id >3 PRICE  TAX 
Fig.  1.6  Tree  with  generated  code. 

= 0.98 $2 

TAX $1 

PRICE $1 

$2 

PRICE,  TAX,  and  =0.98,  respectively.  We  are  now in a  position to  compute C(nl). 

Since  l(nl)  --  1,  the  formula  of  rule  (4)  gives 

C(nl)  =  'TAX;  STORE  $1;  LOAD  PRICE;  ADD  $1' 
Thus,  when  preceded  by  LOAD,  C(nl)  produces  the  sum  of PRICE  and 
TAX  in  the .accumulator,  although  it  does  it  in  an  awkward  way.  The  code 

70  AN  INTRODUCTION  TO  COMPILING  CHAP.  1 
optimization  process  can  "iron  out"  some  of this  awkwardness,  or  the  rules 
by  which  the  object  code  is  constructed  can  be  elaborated  to  take  care  of 

some  special  cases. 

Next  we  can  evaluate  C(n2) using  rule  (5),  and  get 

C(n2) =  '=0.98;  STORE  $2;  LOAD'  C(nl)';  MPY  $2' 
Here,  C(nl)  is  the  string  mentioned  in  the  previous  paragraph,  and  $2 
is  used  as  temporary,  since  l(n2) =  2. 

We  evaluate  C(n3)  using  rule  (3)  and  get 

C(n3) =  'LOAD'  C(n2)';  STORE  COST' 
The  list  of  assembly  language  instructions  (with  semicolons  replaced  by 
new  lines)  which  form  the  translation  of  our  original  "COST  . . . .   "  state- 
ment  is 

(1.2.2)  LOAD  =0.98 

STORE  $2 
LOAD  TAX 

STORE  $1 
LOAD  PRICE 
A D D   $1 

MPY  $2 

STORE  COST  5 

1.2.6.  Code  Optimization 

In  many  situations  it  is  desirable  to  have  compilers  produce  object  pro- 
grams  that  run  efficiently.  Code  optimization is  the  term  generally  applied  to 

attempts  to  make  object  programs  more  "efficient,"  e.g.,  faster  running  or 

more  compact. 

There  is  a  great  spectrum  of  possibilities  for  code  optimization.  At  one 
extreme  is  true  algorithm  optimization.  Here  a  compiler  might  attempt  to 

obtain  some  idea  of the  functions  that  are  defined  by  the  procedure  specified 
by  the  source  language  program.  If  a  function  is  recognized,  then  the  com- 
piler  might  substitute  a  more  efficient procedure  to  compute  a  given  function 

and  generate  machine  code  for  that  procedure. 

Unfortunately  optimization  of  this  nature  is  exceedingly  difficult.  It  is 
a  sad  fact  that  there  is  no  algorithmic  way  to  find  the  shortest  or  fastest- 
running  program  equivalent  to  a  given  program.  In  fact  it  can  be  shown  in 

an  abstract  way  that  there  exist  algorithms  which  can  be  speeded  up  indefi- 
nitely,  That  is to  say,  there  are  some  recursive  functions  for  which  any  given 

algorithm  defining  that  function  can  be  made  to  run  arbitrarily  faster  for 
large  enough  inputs. 

Thus  the  term  optimization  is  a  complete  misnomer---in  practice  we 
must  be  content  with  code  improvement.  Various  code  improvement  tech- 

SEC.  1.2  AN  OVERVIEW  OF  COMPILING  71 

niques  can  be  employed  at  various  phases  of  the  compilation  process. 

In  general,  what  we  can  do  is  perform  a  sequence  of  transformations  on 
a  given  program  in  hopes  of  transforming  the  program  to  a  more  efficient 

one.  Such  transformations  must,  of course,  preserve  the  effect of the program 

on  the  outside  world.  These  transformations  can  be  applied  at  various  times 

during  the  compilation  process.  For  example,  we  can  manipulate  the  input 
program  itself,  the  structures  produced  in  the  syntax  analysis  phase,  or  the 
code  produced  as  output  of  the  code  generation  phase.  In  Chapter  11,  we 

shall  discuss  code  optimization  in  more  detail. 

In  the  remainder  of  this  section  we  shall  discuss  some  transformations 
which  can  be  applied  to  shorten  the  code  (1.2.2): 

(1)  If we  assume  that  q-  is  a  commutative  operator,  then  we  can  replace 
a  sequence  of  instructions  of  the  form  LOAD  ~;  A DD   fl  by  the  sequence 

LOAD  fl;  AD D  ~,  for  any  0c and  ft.  We  require,  however,  that  there  be  no 
transfer  to  the  statement  A DD   fl  from  anywhere  in  the  program. 

(2)  Likewise,  if  we  assume  that  *  is  a  commutative  operator,  we  can 
replace  LOAD  ~;  MPY  fl  by  LOAD  fl;  MPY  ct. (3)  For  any  0c, a  sequence  of  statements  of  the  form  STORE  ~;  LOAD  ct 

can  be  deleted,  provided  either  that  ~  is  not  subsequently  used  or  that 
is  stored  into  before  being  used  again.  (We  can  more  often  delete  the  first 

statement  LOAD  ~  alone;  to  do  so,  it  is  required  only  that  no  transfers  to 
the  statement  LOAD  ~  occur  elsewhere  in  the  program.) 

(4)  The  sequence  LOAD  ~;  STORE  fl  can  be  deleted  if it  is  followed  by 
another  LOAD,  provided  that  there  is  no  transfer  to  STORE  fl  and  that 

subsequent  mention  of fl  is replaced  by  ~  until,  but  not  including,  such  time 

as  another  STORE  fl  instruction  appears. 

Example  1.5 

These  four  transformations  have  been  selected  for  their  applicability  to 
(1.2.2).  In  general  there  would  be  a  large  set  of  transformations,  and  they 
would  be  tried  in  various  combinations.  In  (1.2.2),  we  notice  that  rule  (1) 

applies  to  LOAD  PRICE;  AD D  $1,  and  we  can,  on  speculation,  tempo- 
rarily  replace  these  instructions  by  LOAD  $1;  A DD   PRICE,  obtaining 

the  code 

(1.2.3)  LOAD  =0.98 

STORE  $2 

LOAD  TAX 

STORE  $1 

LOAD  $1 
A D D  PRICE 

MPY  $2 
STORE  COST 

72  AN  INTRODUCTION  TO  COMPILING  CHAP.  1 

We  now  observe  that  in  (1.2.3),  the  sequence  STORE  $1;  LOAD  $1 
can  be  deleted  by  rule  (3).  Thus  we  obtain  the  codet 

(1.2.4)  LOAD  =0.98 

STORE  $2 
LOAD  TAX 
ADD  PRICE 

MPY  $2 

STORE  COST 

We  can  now  apply  rule  (4)  to  the  sequence  LOAD  =0.98;  STORE  $2. 
These  two  instructions  are  deleted  and  $2  in  the  instruction  MPY  $2  is 

replaced  by  MPY  =0.98.  The  final  code  is 

(1.2.5)  LOAD  TAX 

ADD  PRICE 

MPY  =0.98 

STORE  COST 

The  code  of  (1.2.5)  is  the  shortest  that  can  be  obtained  using  our  four 
transformations  and  is  the  shortest  under  any  set  of  reasonable  transfor- 
mations.  5 

1.2.7.  Error Analysis and  Recovery 

We  have  so  far  assumed  that  the  input  to  the  compiler  is  a  well-formed 
program  and  that  each  phase  of compiling  can  be  carried  out  in  a  way  that 

makes  sense.  In  practice,  this  will  not  be  the  case  in  many  compilations. 
Programming is still much  an  art,  and  there  is ample  opportunity  for  various 
kinds  of  bugs  to  creep  into  most  programs.  Even  if  we  feel  that  we  have 
understood  the  problem  for  which  we  are  writing  a  program,  and  even  if 
we  have  chosen  the  proper  algorithm  to  solve  the  problem,  we  often  cannot 
be  sure  that  the  program  we  have  written  faithfully  executes  the  algorithm 

it  should  perform. 

A  compiler  has  an  opportunity  to  detect  errors  in  a  program  in  at  least 
three  of  the  phases  of  compilation--lexical  analysis,  syntactic  analysis,  and 

code generation.  When  an  error  is encountered,  it is a  difficult job,  bordering 

on  an  application  of  "artificial  intelligence,"  for  the  compiler  to  be  able  to 
look  at an  arbitrary  faulty program  and tell what  was probably  meant.  How- 
ever,  in  certain  cases,  it  is  easy  to  make  a  good  guess.  For  example,  if  the 
source  statement  A  =  B ,   2C  is  seen,  there  is  a  high  likelihood  that 
A  =  B  *  2  *  C  was  meant. 

tA  similar simplification could  be  obtained  using rule  (4)  directly. However, we  are trying to give some examples of how different types of transformations  can be used. 

SEC. 1.2  AN  OVERVIEW  OF  COMPILING  73 

In general,  when the compiler comes to  a point in the input  stream where 
it cannot  continue producing  a valid parse,  some  compilers  attempt  to  make 
a  "minimal"  change  in  the  input in  order  for  the  parse  to  proceed.  Some possible  changes  are 

(1)  Alteration  of  a  single  character.  For  example,  if  the  parser  is  given 
"identifier"  INTEJER  by  the  lexical  analyzer  and  it  is  not  proper  for  an 
identifier  to  appear  at  this  point  in  the  program,  the  parser  may  guess  that the  keyword INTEGER  was  meant. 

(2)  Insertion  of  a  single  token.  For  example,  the  parser  can  replace  2C 
by  2  *  C.  (2  -q- C  would  do  as  well,  but  in  this  case,  we  "know"  that  2 *  C is  more  likely.) 

(3)  Deletion of a  single token.  For example,  a comma is often incorrectly 
inserted  after  the  10  in  a  FORTRAN  statement  such  as  DO  10  I  =  1, 20. 

(4)  Simple  permutation  of  tokens.  For  example,  INTEGER  I  might  be written  incorrectly  as  I  INTEGER. 

In  many  programming  languages,  statements  are  easily  identified.  If  it 
becomes  hopeless  to  parse  a  particular  (ill-formed)  statement,  even  after applying changes  such  as those above,  it is  often possible  to ignore the  state- 

ment  completely  and  continue  parsing  as  though  this  ill-formed,  statement did  not  appear. 

In  general,  however,  there  is  very little  of a  mathematical  nature  known 
about  error  recovery  algorithms  and  algorithms  to  generate  "good"  diag- 
nostics.  In  Chapters  4 and  5, we shall discuss certain  parsing algorithms,  LL, 
LR,  and  Earley's  algorithm,  which  have  the  property  that  as  soon  as  the 
input  stream  is  such  that  there  is  no  possible  following  sequence  which 

could  make  a  well-formed  input,  the  algorithms  announce  this  fact.  This 
property is useful in error recovery and analysis, but  some parsing algorithms discussed  do  not  possess  it. 

1.2.8.  Summary 

Our  conceptual  model  of a  compiler is  summarized in  Fig.  1.7. The  code 
optimization  phase  is  shown  occurring  after  the  code  generation  phase,  but 
as  we  remarked  earlier,  various  attempts  at  code  optimization  can  be  per- formed  throughout  the  compiler. 

An  error  analysis  and  recovery  procedure  can  be  called  from  the  lexical 
analysis  phase,  syntactic  analysis  phase,  or  code  generation  phase,  and  if 
the  recovery  is  successful,  control  is  returned  to  the  phase  from  which  the 
error  recovery  procedure  was  called.  Errors  in  which  no  token  appears  at 

some  point  in  the  input  stream  are  detected  during  lexicat  analysis.  Errors 
in  which  the  input  can  be  broken  into  tokens  but  no  tree  structure  can  be 

placed  on  these  tokens  are  detected  during  syntactic  analysis.  Finally,  errors 
in  which  the  input  has  a  syntactic  structure,  but no meaningful  code  can  be 

74  AN  INTRODUCTION  TO  COMPILING  CHAP.  1 
Source program 

Book keeping 

Object program 
Error analysis 
Fig.  1.7  Model  of a  compiler. 

generated from this  structure,  are  detected  during code generation.  An  exam- 
ple  of this situation would be a variable used without declaration.  The  parser 
ignores  the  data  component  of  tokens  and  so  could  not  detect  this  error. 

The  symbol  tables  (bookkeeping)  are  produced  in  the  lexical  analysis 
process  and  in  some  situations  also  during  syntactic  analysis  when,  say, 

attributes  and  the  identifiers  to  which  they  refer  are  connected  in  the  tree 

structure  being  formed.  These  tables  are  used  in  the  code  generation  phase 

and  possibly  in  the  assembly  phase  of  compilation. A  final  phase,  which we refer  to as  assembly, is shown in Fig.  1.7. In this 

phase  the  intermediate  code  is  processed  to  produce  the  final  machine  lan- 
guage  representation  of  the  object  program.  Some  compilers  may  produce 

machine  language  code  directly as the  result of  code  generation,  so that  the 
assembly phase  may not  be explicitly present. The  model  of  a  compiler  we  have  portrayed  in  Fig.  1.7  is  a  first-order 

approximation to  a real compiler.  For  example,  some compilers are  designed 
to  operate  using  a  very  small  amount  of storage  and  as  a  consequence  may 

EXERCISES  75 
consist  of  a  large  number  of  phases  which  are  called  upon  successively  to 

gradually  change  a  source  p rogr am  into  an  object  program. 

Our  goal  is  not  to  tabulate  all  possible  ways  in  which  compilers  have 
been  built.  Rather  we  are  interested  in  studying  the  fundamental  problems 
that  arise  in  the  design  of  compilers  and  other  language-processing  devices. 

E X E RCI S E S  
"1.2.1. 

"1.2.2. 

1.2.3. 
*'1.2.4. 

*'1.2.5. 

1.2.6. 

1.2.7. 

1.2.8. 
"1.2.9. 

1.2.10. 
"1.2.11. 

Describe  the  syntax  and  semantics  of  a  FORTRAN  assignment  state- 

ment. 

Can  your  favorite  programming  language  be  used  to  define  any  recur- 
sively  enumerable  set ?  Will  a  given  compiler  necessarily  compile  the 
resulting program ? 

Give  an  example  of  a  FORTRAN  program  which  is  syntactically  well 
formed but  which  does  not  define an  algorithm. 

What  is  the  maximum  lookahead  needed  for  the  direct  lexical  analysis 

of  FORTRAN ?  By  lookahead  is  meant  the  number  of  symbols  which 

are  scanned  by  the  analyzer  but  do  not  form  part  of  the  token  found. 

What  is  the  maximum  lookahead  needed  for  the  direct  lexical  analysis 

of  ALGOL  60?  You  may  assume  that  superfluous  blanks  and  end  of 
card  markers have  been  deleted. 

Parse the  statement  X  =  A *  B  /  C *  D  using  a  tree with  interior nodes 

of the  forms  shown  in  Fig.  1.4. Hint:  Recall  that,  conventionally,  mul- 
tiplications are performed before additions  in the absence of parentheses. 

Parse  the  statement  X  =  A *  (B  +  C ) ,   D,  as  in  Exercise  1.2.6.  Hint: 

When  several  operands  are  multiplied  together,  we  assume  that  order 
of multiplication  is  unimportant.t  Choose  any  order  you  like. 

Use  the  rules  of code  generation  developed  in  Section  1.2.5  to  translate 
the  parse  trees  of  Exercises  1.2.6  and  1.2.7  in  a  syntax-directed  way. 

Does  the  transformation  of  LOAD  0c;  STORE  fl;  LOAD  ~,;  STORE 

into  LOAD  ~,;  STORE  t~;  LOAD  ~;  STORE  fl  preserve  the  input- 

output  relation  of  programs?  If  not,  what  restrictions  must  be  placed 
on  identifiers  ~, fl, ~,, ~ ?  We  assume  that  no  transfers  into  the  interior 
of the sequence occur. 

Give some  transformations  on  assembly code which  preserve the  input- 

output  relation  of programs. 

Construct  a  syntax-directed  translation  for  arithmetic  assignment  state- 

ments  involving  +  and  *  which  will,  in  particular,  map  the  parse  of 

Fig.  1.3  directly into  the  assembly code (1.2.5). 

tStrictly  speaking,  order may  be important due  to  overflow and/or  rounding. 

76  AN  INTRODUCTION  TO  COMPILING  CHAP.  1 

"1.2.12.  Design  a  syntax-directed  translation  scheme  which  will  generate  object 

code  for expressions  involving  both  real  and  integer  arithmetic.  Assume 
that the type of each identifier is known,  and that the result of operating on  a  real  and  an integer is a  real. 

"1.2.13.  Prove  that  Algorithm  1.1  operates  correctly.  You  must first define when 

an input  assignment statement  and output  assembly code are equivalent. 

Research  Problem 

There  are many research  areas  and  open  problems  concerned  with  compiling 
and  translation  of  algorithms.  These will be mentioned  in more  appropriate  chap- 
ters.  However,  we  mention  one here,  because  this  area  will  not  be treated  in  any 

detail in the book. 

1.2.14.  Develop  techniques  for  proving  compilers  correct.  Some work  has  been 

done  in  this  area  and  in  the  more  general  area  of  proving  programs 

and/or  algorithms  correct.  (See  the  following  Bibliographic  Notes.) 

However,  it  is  clear  that  more  work  in  the  area  is  needed. 

An  entirely  different  approach  to  the  problem  of producing  reliable 
compilers  is  to  develop  theory  applicable  to  their  empirical  testing. 
That  is,  we  assume we  "know"  our  compiling  algorithms  to  be  correct. 
We want to test whether a particular program implements them correctly. 

In the first approach,  above,  one would  attempt to prove the equivalence 
of  the  written  program  and  abstract  compiling  algorithm.  The  second 
approach  suggested  is  to  devise  a  finite  set  of  inputs  to  the  compiler 
such  that  if these  are  compiled  correctly,  one  can  say  with  reasonably 

certainty  (say  a  99 Yo confidence  level)  that  the  compiler  program  has 
no  bugs.  Apparently,  one  would  have  to  make  some  assumption  about 
the  frequency  and  nature  of  programming  errors  in  the  compiler  pro- 

gram itself. 

B I B L I OG R A P H I C   N O T E S 
The  development  of  compilers  and  compiling  techniques  paralleled  that  of 
programming  languages.  The  first  FORTRAN  compiler  was  designed  to  produce 
efficient  object  code  [Backus  et  al.,  1957]. Numerous  compilers  have  been  written 
since,  and  several  new  compiling  techniques  have  emerged.  The  greatest  strides 
have occurred in lexical and  syntactic analysis and  in some understanding  of  code 

generation techniques. 

There are a large number  of papers in the literature relating to compiler design. 
We  shall  not  attempt  to  mention  all  these  sources  here.  Comprehensive  surveys 

of  the  history  of  compilers  and  compiler  development  can  be  found  in  Rosen 
[1967],  Feldman  and  Gries  [1968], and  Cocke  and  Schwartz  [1970]. Several books that  describe  compiler  construction  techniques  are  Randell  and  Russell  [1964], 

McKeeman  et  al.,  [1970], Cocke  and  Schwartz  [1970], and  Gries  [1971]. Hopgood [1969]  gives  a  brief  but  readable  survey  of  compiling  techniques.  An  elementary 
discussion  of compilers  is  given in  Lee [1967]. 

SEC.  1.3  OTHER  APPLICATIONS  OF  PARSING  AND  TRANSLATING  ALGORITHMS  77 

Several  compilers  have  been  written  which  emphasize  comprehensive  error 
diagnostics,  such  as  DITRAN  [Moulton  and  Muller,  1967] and  IITRAN  [Dewar 
et al.,  1969]. Also,  a few compilers have been written which attempt to correct each 
error encountered  and  to  execute the  object  program  no  matter  how  many  errors 
have  been  encountered.  The  philosophy  here  is  to  continue  compilation  and 
execution  in  spite  of  errors,  in  an  effort  to  uncover  as  many  errors  as  possible. 

Examples of such compilers are CORC [Conway and Maxwell,  1963, and Freeman, 

1964],  CUPL  [Conway  and  Maxwell,  1968], and  PL/C  [Conway  et  al.,  1970]. 

Spelling mistakes  are  a  frequent  source  of errors  in  programs.  Freeman  [1964] 
and  Morgan  [1970] describe  some  techniques  they  have  found  effective  in  cor- 
recting spelling errors  in programs. 

A  general  survey  of  error  recovery  in  compiling  can  be  found  in  Elspas  et  al. 
[1971]. 

Some  work  on  providing  the  theoretical  foundations  for  proving  that  com- 
pilers work correctly is reported in McCarthy  [1963], McCarthy  and Painter [1967], 

Painter  [1970], and  Floyd  [1967a]. 

The implementation of a compiler is a task that involves a considerable amount 
of effort.  A  large  number  of programming systems called  compiler-compilers have 
been  developed  in  an  attempt  to  make  the  implementation  of  compilers  a  less onerous  task.  Brooker  and  Morris  [1963],  Cheatham  [1965],  Cheatham  and 

Standish  [1970], Ingerman  [1966], Irons  [1963b],  Feldman  [1966], McClure  [1965], 

McKeeman  et  al.  [1970],  Reynolds  [1965],  Schorre  [1964],  and  Warshall  and 

Shapiro  [1964] are just  a  few  of the  many  references  on  this  subject.  A  compiler- 
compiler  can  be  simply  viewed  as  a  programming  language  in  which  a  source 

program is the description of a compiler for some language and the object program 

is the compiler  for  that  language. 

As such,  the source program  for a  compiler-compiler is merely a  formalism for 
describing  a  compiler.  Consequently,  the  source  program  must  contain  explicitly 
or  implicitly,  a  description  of the  lexical  analyzer,  the  syntactic  analyzer,  the  code 
generator,  and  the  various  other  phases  of  the  compiler  to  be  constructed.  The 
compiler-compiler  is  an  attempt  at  providing  an  environment  in  which  these  de- 

scriptions  can  be  easily written  down. 

Several  compiler-compilers  provide  some  variant  of  a  syntax-directed  transla- 
tion  scheme  for  the  specification  of  a  compiler,  and  some  also  provide  an  auto- 

matic  parsing  mechanism.  TMG  [McClure,  1965] is  a  prime  example  of this  type 

of system.  Other  compiler-compilers,  such  as  TGS  [Cheatham,  1965] for  example, 

instead  provide  an  elaborate  high-level  language  in  which  to  describe  the  various 

algorithms  that  go  into  the  making  of a  compiler.  Feldman  and  Gries  [1968] have 
provided  a  comprehensive survey of compiler-compilers. 

1.3.  OTHER  APPLICATIONS  OF  PARSING  AND 

TRANSLATING  ALGORITHMS 

In this  section we shall  mention  two  areas,  other  than  compiling,  in which 
hierarchical  structures  such  as  those  found  in  parsing  and  translating 

algorithms  can  play  a  major  role.  These  are  the  areas  of  natural  language 
translation  and  pattern  recognition. 

78  AN  INTRODUCTION  TO  COMPILING  CHAP.  1 
1.3.t.  Natural  Languages 

It  would  seem  that  text  in  a  natural  language  could  be  translated,  either 
to  another  natural  language  or  to  machine  language  (if  the  sentences 

described  a  procedure),  exactly  as  programming  languages  are  translated. 
Problems  first  appear  in  the  parsing  phase,  however.  Computer  languages 

are  precisely  defined  (with  occasional  exceptions,  of  course),  and  the  struc- 
ture  of  statements  can  be  easily  discerned.  The  usual  model  of the  structure 

of  statements  is  a  tree,  as  described  in  Section  1.2.4. 

Natural  languages,  first of all,  are afflicted with both  syntactic and  seman- 
tic ambiguities.  To take  English as the obvious  example of a natural language, 
the  sentence  "I  have  drawn  butter"  has  at  least  two  meanings,  depending  on 

whether  "drawn"  is  an  adjective  or  part  of  the  verb  of  the  sentence.  Thus 
it is impossible  always to  produce  a unique  parse  tree for  an  English  sentence, 

especially if the  sentence  is treated  outside  of the  context  in which  it appears. 

A  more  difficult  problem  concerning  natural  languages  is that  the  words, 
i.e.,  terminal  symbols  of the  language,  relate  to  other  words  in  the  sentence, 

outside  the  sentence  and  possibly  the  general  environment  itself.  Thus  the 
simple  tree  structure  is  not  always  sufficient  to  describe  all  the  information 
about  English  sentences  that  one  would  wish  to  have  around  when  transla- 
tion  (the  analog  of  code  generation  for  programming  languages)  occurred. 

For  a  commonly  used  example,  the  noun  "pen"  is  really  at  least  two 
different  nouns  which  we  might  refer  to  as  "fountain  pen"  and  "pig  pen." 
We  might  wish  to  translate  English  into  some  language  in  which  "fountain 
pen"  and  "pig  pen"  are  distinct  words.  If we  were  given  the  sentence  "This 
pen  leaks"  to  translate,  it  seems  clear  that  "fountain  pen"  is  correct.  How- 

ever,  if  the  sentence  were  taken  from  the  report  "Preventing  Nose  Colds in Hogs," 

we  might  want  to  reconsider  our  decision. 
The  point  to  be  made  is  that  the  meaning  and  structure  of  an  English 
sentence  can  be  determined  only  by  examining  its  total  environment"  the 
surrounding  sentences,  physical  information  (i.e.,  "Put  the  pen  in  the  glass" 
refers  to  "fountain  pen"  because  a  pig pen  won't  fit  in  a  glass),  and  even  the 
nature  of  the  speaker  or  writer  (i.e.,  what  does  "This  pen  leaks"  mean  if 
the  speaker  is  a  convict ?). 

To  describe  in  more  detail  the  information  that  can  be  gleaned  from 
natural  language  sentences,  linguists  use  structure  systems  that  are  more 
complicated  than  the  tree  structures  sufficient  for  programming  languages. 

Many  of  these  efforts  fall  under  the  heading  of  context-sensitive  grammars 
and  transformational  grammars.  We  shall  not  cover  either  theory  in  detail, 
although  context-sensitive  grammars  are  defined  in  the  next  chapter,  and 
a  rudimentary  form  of transformational  grammar  can  be  discussed  as  a  gen- 
eralized  form  of  syntax-directed  translation  on  trees.  This  notion  will  be 

mentioned  in  Chapter  9.  The  bibliographic  notes  for  this  section  include 
some  places  to  look  for  more  information  on  natural  language  parsing. 

SEc. 1.3  OTHER APPLICATIONS OF PARSING AND TRANSLATING ALGORITHMS  79 
1.3.2.  Structural  Description  of  Patterns 

Certain  important  sets  of  patterns  have  natural  descriptions  that  lend 
themselves to a form of syntactic analysis. For example, Shaw [1970] analyzed 
cloud  chamber  photographs  by putting  a  tree  structure  on  relevant  lines  and 
curves  appearing  therein.  We  shall  here  describe  a  particularly  appealing 
way of defining sets of graphs, called "web grammars"  [Pfaltz and Rosenfeld, 

1969]. While  a  complete  description  of web  grammars  would  require  knowl- 
edge of Section 2.1, we can give a  simple example here to illustrate  the essen- 
tial  ideas. 

Example 1.6 

Our  example  concerns  graphs  called  "d-charts,"t  which  can  be  thought 
of  as  the  flow  charts  for  a  programming  language  whose  programs  are 
defined  by the  following  rules: 

(1)  A  simple  assignment  statement  is  a  program. 

(2)  If  $1  and  Sz  are  programs,  then  so  is  S 1 ; S  ~. (3)  If S 1 and  Sz  are  programs  and  A is  a  predicate,  then 

if A  then $i  else $2  end 
is  a  program. 

(4)  If S  is  a  program  and  A  a  predicate,  then 

while A  do  S  end 
is  a  program. 

We can write flow charts  for  all such programs,  where the  nodes  (blocks) 
of the flow chart represent  code  either to  test a predicate  or perform  a  simple 

assignment  statement.  All the  d-charts  can be constructed  by beginning  with 
a  single  node,  representing  a  program,  and  repeatedly replacing nodes  repre- 

senting  programs  by  one  of  the  three  structures  shown  in  Fig.  1.8. These 
replacement  rules  correspond  to  rules  (2),  (3),  and  (4)  above,  respectively. 

The  rules  for  connecting  these  structures  to  the  rest  of  the  graph  are 
the  following.  Suppose  that  node  no  is  replaced  by  the  structure  of  Fig. 

1.8(a),  (b),  or  (c). 

(1)  Edges  entering  no  now  enter  nx,  n3,  or  n6,  respectively. 
(2)  An  edge  from  no  to  node  n  is  replaced  by  an  edge  from  n2  to  n  in 
Fig.  1.8(a),  by edges  from  both n4  and n5 to  n in  Fig.  1.8(b),  and  by an  edge 
from  n6  to  n in  Fig.  1.8(c). 

Nodes n3 and n6 represent predicate tests and may not be further replaced. 
The  other  nodes  represent  programs  and  may  be  further  replaced. 

tThe d honors E. Dijkstra. 

( 

CHAP.  1 
) 

( 

(a) 

nl 

n4 

Fig.  1.8 
n2 

80  AN INTRODUCTION TO COMPILING 

n6  ~ n 7  
(b)  (c) 
Structures representing subprograms in  a  d-chart. 

Let  us  build  the  d-chart  which  corresponds,  in  a  sense,  to  a  program  of 
the  form 

if B1  then while  B2  do 

if B 3 then  $1  else  S 2 end  end; 
$3 else  ifB4  then 

$4; 
$5; while  B s  do  $6  end 

else  $7  end  end 
The  entire  program  is  of  the  form  if  B1  then  Ss  else  S~  end,  where 
Ss  represents  everything  from  the  first  while  to  $3  and  $9  represents  if 
B4  " "   $7  end.  We  can  also  show  this  analysis  by  replacing  a  single  node 
by  the  structure  of Fig.  !.8(b). 

Continuing  the  analysis,  Ss  is  of  the  form  Sx0;  $3,  where  S~0  is while 

B2  --"  $2  end  end.  Thus  we  can  reflect  this  analysis  by  replacing node 
n4  of  Fig.  1.8(b)  by  Fig.  1.8(a).  The  result  is  shown  in  Fig.  1.9(a).  Then, 
we  see  S~0 is  of  the form  while  B2  do $11  end,  where  S~  is if  B 3 . . .   $2  end. 
We  can  thus  replace  the  left  direct  descendant  of  the  root  in  Fig.  1.9(a)by 
the  structure  of  Fig.  1.8(c).  The  result  is  shown  in  Fig.  1.9(b). 

The  result  of  analyzing  the  program  in  this  way  is  shown  in  Fig.  1.10. 
Here,  we  have  taken  the  liberty  of drawing  contours  around  each  node  that 
is  replaced  and  of  making  sure  that  all  subsequent  replacements  remain 
inside  the  contour.  Thus  we can place  a  natural  tree  structure  on  the  d-chart 

by  representing  nodes  of  the  d-chart  as  leaves,  and  contours  by  interior 
nodes.  Nodes  have  as  direct  ancestor  the  contour  most  closely  including 
what  they  represent.  The  tree  structure  is  shown  in  Fig.  1.11.  Nodes  are 

labeled  by  the  node  or  contour  they  represent. 

SEC. 1.3  OTHER APPLICATIONS OF PARSING AND TRANSLATING ALGORITHMS 

(a)  (b) 

Fig.  1.9  Constructing  a  d-chart. 

81 

/  . /   i~- ---- --- >.,,f"- "~.  .~  / r " ~ O   ~  ~ .   O'S  X 
/  /  /  \  i 

I  i  \ . ,   " \   , 
'  I  ~  i  I I  ,  I  I !  'l  \1  \il"s,~ 

( s ~ ) , , i !   i!  L(s~))  \'~6  I  I t  t \   .K'-"  i ~ . , ; / /   t\  " T '   c~  ,~  /  i 

\ 

\,~,  1 1 11  

Fig.  1.10  Complete d-chart. 

In a  sense, the example  above is a fraud.  The d-chart structure,  as reflected 
in  Fig.  1.11, is essentially the  same  structure  that  the parsing phase  of a  com- 
piler  would  place  on  the  original  program.  Thus  it  appears  that  we  are  dis- 

cussing  the  same  kind  of  syntax  analysis  as  in  Section  1.2.3.  However,  it 

should  be  borne  in  mind.  that  this  kind  of  structural  analysis  can  be  done 
without  reference  to  the  program,  looking  only  at  the  d-chart.  Moreover, 
while  we  used  this  example  of a  web  grammar  because  of its  relation  to  pro- 
gramming  languages,  there  are  many  purely  graph-theoretic  notions  that 

82  AN INTRODUCTION  TO COMPILING  CHAP.  1 

C1 

Bl 

B2 

C5 
c3 J  %  s3  B4 d  ,i, C6 %  S7 

B3  S1  $2  $4  $5  B5 

Fig. 1.11  Tree describing d-chart structure. 

$6 

can  be  defined  using web  grammars  (suitably generalized  from  Example  1.8), 
for  example,  the  class  of planar  graphs  or  the  class  of binary  trees. 

B I B L I O G R A P H I C   N O T E S  
Chomsky  [1965] gives  a  good  treatment  of  the  difficulties  in  trying  to  find  a satisfactory grammatical model for English.  Bobrow [1963] surveys efforts  at using 
English  or,  more  accurately,  some  subset  of English  as  a  programming language. Bar-Hillel  [1964] surveys  theoretical  aspects  of linguistics. 

The  notion  of  a  web  grammar  is  from  Pfaltz  and  Rosenfeld  [1969]  and  the theory was extended in Montanari  [1970] and Pavlidis [1972]. Some of the  original 
work  in  syntactic  analysis  of patterns  is  due  to  Shaw  [1970]. A  survey  of results in  this  area  can  be  found  in  Miller and  Shaw [1968]. 

2 

ELEMENTS  OF 
LANGUAGE  THEORY 

In  this  chapter  we  shall  present  those  aspects  of formal  language  theory which  are  relevant to parsing and translation.  Initially,  we shall  concentrate 
on  the  syntactic  aspects  of language.  As  most  of the  syntax  of modern  pro- gramming languages  can  be  described  by  means  of a  context-free  grammar, 
we  shall  focus  our  attention  on  the  theory  of  context-free  languages. We  shall  first  study  an  important  subclass  of the  context-free  languages, 

namely the regular sets.  Concepts  from  the theory of regular sets have wide- 

spread  application  and  pervade  much  of the  material  of this  book. Another  important  class  of  languages  is  the  deterministic  context-free 

languages.  These  are  context-free  languages  that  have  grammars  which  are easily parsed,  and fortunately,  or by intent,  modern programming languages 
can  be  viewed  as  deterministic  context-free  languages  with  good,  although 

not  complete,  precision. These  three  classes  of  languages,  the  context-free,  regular,  and  deter- 

ministic  context-free,  will be  defined  and  some  of their  principal  properties given.  Since  the  theory  of  languages  encompasses  an  enormous  body  of 

material,  and  since  not  all  of it  is  relevant  to  parsing  and  translation,  some 
important  theorems  of  language  theory  are  here  proved  in  a  very  sketchy 

way or relegated to  the  Exercises. We try to  emphasize  only those  aspects  of 

language theory which  are  useful in  the  development  of this  book. As in  Chapters  0  and  1, we invite  the reader who  has been introduced  to 

the  theory  of  languages  to  skip  or  skim  this  chapter. 

2.1.  REPRESENTATIONS  FOR  LANGUAGES 

In  this  section,  we  shall  discuss  from  a  general  point  of  view  the  two 
principal  methods  of defining  languages--the  generator  and  the  recognizer. 

83 

84  ELEMENTS OF  LANGUAGE THEORY  CHAP.  2 

We  shall  discuss  only  the  most  common  kind  of  generator,  the  Chomsky 
grammar.  We  treat  recognizers  in  somewhat  greater  generality,  and  in  sub- 

sequent  sections  we  shall  introduce  some  of the  great  variety  of recognizers 
that  have  been  studied. 

2.1.1.  Motivation 

Our  definition  of a  language  L  is  a  set  of finite-length  strings  over  some 
finite  alphabet  Z.  The  first  important  question  is  how  to  represent  L  when 
L  is  infinite.  Certainly,  if L  consisted  of a  finite  number  of strings,  then  one 

obvious  way  would  be  to  list  all  the  strings  in  L. 

However,  for many languages  it is not possible (or perhaps  not desirable) 
to  put  an  upper  bound  on  the  length  of the  longest  string  in  that  language. 

Consequently,  in  many  cases  it  is  reasonable  to  consider  languages  which 
contain  arbitrarily  many  strings.  Obviously,  languages  of this  nature  cannot 
be  specified  by an  exhaustive  enumeration  of the  sentences  of the  language, 

and  some  other  representation  must  be  sought.  Invariably,  we  want  our 

specification  of a  language  to  be  of finite  size,  although  the  language  being 

specified  may  not  be  finite. 

There  are  several  methods  of specification  which  fulfill  this  requirement. 
One  method  is  to  use  a  generative  system,  called  a  grammar.  Each  sentence 
in  the  language  can  be  constructed  by well-defined  methods,  using  the  rules 

(usually  called  productions)  of  the  grammar.  One  advantage  of  defining 

a  language  by  means  of  a  grammar  is  that  the  operations  of  parsing  and 
translation are  often made simpler by the  structure  imparted  to the  sentences 

of  the  language  by  the  grammar.  We  shall  treat  grammars,  particularly 
the  "context-free"  grammars,  in  detail. 

A  second  method  for  language  specification  is  to  use  a  procedure  which 
when  presented  with  an  arbitrary  input  string  will  halt  and  answer  "yes" 

after  a  finite  amount  of computation  if that  string  is in the  language.  In the 
most  general  case,  we  could  allow  the  procedure  to  either  halt  and  answer 

"no"  or  to  continue  operating  forever  if  the  string  under  consideration 
were not in the language.  In practical  situations,  however, we must insist that 

the  procedure  be  an  algorithm,  so  that  it  will  halt  for  all  inputs. 

We  shall  use  a  somewhat  stylized  device  to  represent  procedures  for 
defining  languages.  This  device,  called  a  recognizer,  will  be  introduced  in 

Section  2.1.4. 

2.1.2.  Grammars 

Grammars  are  probably  the  most  important  class  of  generators  of  lan- 
guages.  A  grammar  is  a  mathematical  system  for  defining  a  language,  as 

well  as  a  device  for  giving  the  sentences  in  the  language  a  useful  structure. 
In  this  section  we  shall  look  at  a  class  of grammars  called  Chomsky gram- 

mars,  or  sometimes  phrase  structure  grammars. 

SEC. 2.1  REPRESENTATIONS FOR LANGUAGES  85 

A  grammar  for  a  language  L  uses  two  finite  disjoint  sets  of  symbols. 
These  are  the  set  of  nonterminal  symbols,  which  we  shall  often  denote  by 
N,t  and  the  set  of terminal  symbols,  which  we  shall  denote  by  Z.  The  set of 
terminal  symbols  is  the  alphabet  over  which  the  language  is  defined.  Non- 
terminal  symbols  are  used  in  the  generation  of  words  in  the  language  in 

a  way  which  will  become  clear  later. 

The heart  of a  grammar is a  finite  set P  of formation rules,  or productions 
as  we  shall  call  them,  which  describe  how  the  sentences  of the  language  are 
to  be  generated.  A  production  is merely  a  pair  of strings,  or; more  precisely, 

an  element  of  (N  U  Z)*N(N  U  Z)*  x  (N  U  Z)*.  That  is,  the  first  compo- 
nent  is  any  string  containing  at  least  one  nonterminal,  and  the  second  com- 
ponent  is  any  string. 

For  example,  a pair  (AB, CDE) might be a  production.  If it is determined 
that  some  string  a  can  be  generated  (or  "derived")  by  the  grammar,  and 

has  AB,  the  left  side  of  the  production,  as  a  substring,  then  we  can  form 

a  new  string  fl  by  replacing  one  instance  of the  substring  AB  in  a  by  CDE. 
We  then  say  that  fl  is  derived  by  the  grammar.  For  example,  if  FGABH 

can  be  derived,  then  FGCDEH  can  also  be  derived.  The  language  defined 
by  the  grammar  is  the  set  of  strings  which  consist  only  of  terminals  and 
which  can  be  derived  starting  with  one  particular  string  consisting  of  one 

designated  symbol,  usually  denoted  S. 

CONVENTIO~q 

If  (a,  fl)  is  a  production,  we  use  the  descriptive  shorthand  t~ --~ fl  and 
refer  to  the  production  as  ~  --~  fl  rather  than  (~,  fl). 

We  now  give  a  formal  definition  of grammar. 

DEFINITION 

A  grammar is  a  4-tuple  G  =  (N,  Z, P,  S)  where 

(i)  N  is  a  finite  set of nonterminal symbols (sometimes  called  variables  or 
syntactic  categories). 

(2)  Z  is  a  finite  set  of  terminal  symbols,  disjoint  from  N. 
(3)  P  is  a  finite  subset  of 

(N  U  ~)*N(N  U  $)*  x  (N  U  Z)* 
An  element  (a,  fl) in  P  will  be  written  a  ~  fl  and  called  a production. 

(4)  S  is a  distinguished  symbol in N  called the sentence (or start) symbol. 

Example 2.1 

An  example  of a  grammar  is  G1 
of 

=  ({A, S}, (0,  1 }, P,  S),  where  P  consists 

1"According  to  our  convention about  alphabet  names, this  symbol  is  a  capital Greek nu,  although  the  reader  will probably want to call it  "en," as is more customary 
anyway. 

86  ELEMENTS  OF  LANGUAGE  THEORY  CHAP.  2 

S:  ~ 0A1 
0,4  ~ 00A 1 

A  ~e 

The  nonterminal  symbols  are  A  and  S  and  the  terminal  symbols  are  0  and 

1.  D 

A  grammar  defines  a  language  in  a  recursive  manner.  We  define  a  special 
kind  of string  called  a  sententialform  of  a  grammar  G  =  (N,  E, P,  S)  recur- 
sively  as  follows" 

(1)  S  is  a  sentential  form. 
(2)  If afly is  a  sentential  form  and  fl  ~  ~  is  in P,  then  ~d~7 is  also  a  sen- 
tential  form. 

A  sentential  form  of  G  containing  no  nonterminal  symbols  is  called a sentence 

generated  by  G. 
The  language generated by  a grammar  G,  denoted  L(G),  is  the  set  of sen- 
tences  generated  by  G. 

We  shall  now  introduce  some  terminology  which  we  shall  find  useful. 
Let  G  =  (N,  ~, P,  S) be  a  grammar.  We  can  define  a  relation  ==~ (to  be  read 

G as  directly  derives)  on  (N  u  E)*  as  follows"  If  efl~, is  a  string  in  (N  W E)* 

and  fl  ~  t~ is  a  production  in  P,  then  eft7  =-~ ed~,. 

G + 

We  shall  use  ~  (to  be  read  derives  in  a  nontrivial  way)  to  denote  the G 

transitive  closure  of  ~ ,   and  ~  (to  be  read  derives)  to  denote  the  reflexive G  G 
and  transitive  closure  of =~.  When  it  is  clear  which  grammar  we  are  talking G 

+  * about,  we  shall  drop  the  subscript  G  from  =-~, =-~, and  =-~. 

k We  shall  also  use  the  notation  ~  to  denote  the  k-fold  product  of  the 
k relation  =-~. That  is  to  say,  e  ~-~ fl  if  there  is  a  sequence  ix0, e l , . . . ,   ek  of 

k--t-1  strings  (not  necessarily  distinct)  such  that  e  =  e0, e~-~ ~  et  for 

1 ~  i ~  k  and  ek  =  fl-  This  sequence  of  strings  is  Called  a  derivation  of 

length  k  of  fl  from  e  in  G.  Thus,  L(G)  =  {wlw  is  in  2~*  and  S  ~  w}.  Also 

t  + notice  that  e  ~  fl  if and  only  if  e  =-~ fl  for  some  i  ~  0,  and  e  =-~ fl  if  and 

i only  if e  ~  fl  for  some  i  ~  1. 

Example  2.2 

Let us consider  grammar  G 1 of Example 2.1  and the  following  derivation' 
S  ==~ 0A1  ~  00A1 t  =-~ 0011.  That  is,  in  the  first  step,  S  is  replaced  by  0A1 
according  to  the  production  S  ~  0A1.  At  the  second  step,  0A  is  replaced 

SEC. 2.1  REPRESENTATIONS FOR  LANGUAGES  87 

3 by  00A 1,  and  at  the  third,  A  is  replaced  by  e.  We  may  say  that  S =-~ 0011, 
+  , S  ~  0011,  S ~  00i 1,  and  that  0011  is  in L(G~).  It  canbe  shown  that 

L (G, )  --  {0"1" I n  ~  1} 
and  we  leave  this  result  for  the  Exercises. 
CONVENTION 

A  notational  shorthand  which  is  quite  useful  for  representing  a  set  of 
productions  is  to  use 

to  denote  the  n  productions 

a  ~fll 
~Z -----> ff2 

o 
tg  >  ~n 

We  shall  also  use  the  following  conventions  to  represent  various  symbols 
and  strings  concerned  with  a  grammar" 

(1)  a,  b,  c,  and  d  represent  terminals,  as  do  the  digits  0,  1 , . . . ,   9. 
(2)  A,  B,  C,  D,  and  S  represent  nonterminals;  S  represents  the  start 
symbol. (3)  U,  V , . . . ,   Z  represent  either  nonterminals  or  terminals. 

(4)  a,  f l , . . ,   represent  strings  of  nonterminals  and  terminals. 

(5)  u,  v , . . . ,   z  represent  strings  of  terminals  only. 

Subscripts  and  superscripts  do  not  change  these  conventions.  When 
a  symbol  obeys  these  conventions,  we  shall  often  omit ~ mention  of  the  con- 
vention.  We  can  thus  specify  a  grammar  by  merely  listing  its  productions  if 

all  terminals  and  nonterminals  obey  conventions  (1)  and  (2).  Thus  grammar 

G i  can  be  specified  simply  as 

S  ~ 0A1 
OA  >~ OOA 1 

A  ~e 

No  mention  of the  nonterminal  or  terminal  sets  or  the  start  symbol  is neces- 
sary. 

We  now  give further  examples  of grammars. 

88  ELEMENTS  OF  LANGUAGE THEORY  CHAP.  2 

Example 2,3 

Let  G  =  ([<digit>},  {0,  1 , . . . ,   9}, {<digit>  ---~ 0111...  19}, <digit>).  Here 
(digit>  is  treated  as  a  single  nonterminal  symbol.  L(G)  is  clearly  the  set  of 
the  ten  decimal  digits.  Notice  that  L(G)  is  a  finite  set. 

Example 2.4 

Let  Go =  ([E, T, F},  [a,  + ,  ,,  (,)}, P,  E),  where  P  consists  of the  produc- 
tions 

E  > E + T I T  

T  > T , F ] F  
F  > (E) la 

An  example  of a  derivation  in  this  grammar  would  be 

E - - - >   E-+-  T 

~ T +   T 

>'F-q-  T 
> a + T  
>a-+-  T , F  
- - - > a + F , F  

.-=-> a  +  a  ,  F 
--->  a  .+-  a  ,  a 

L(Go)  is the  set  of arithmetic  expressions  that  can  be  built  up  using  the  sym- 

bols  a,  / ,   ,,  (,  and).  D 

The  grammar  in  Example  2.4  will  be  used  repeatedly  in  the  book  and  is 
always  referred  to  as  Go. 

Example 2.5 

Let  G be  defined  by 

S  .> aSBC[abC 
CB  > BC 

bB  ~  bb 
bC  > bc 

cC  > cc 

SEC. 2.1 
We  have  the  following  derivation  in  G" 

S  ~  a S B C  

aab CB C 
>. aabBCC 

aabbCC 
aabbcC 

aabbcc 

The  language  generated  by  G is [a"b"c"[n  ~  1}. 

Example 2.6 

Let  G be  the  grammar  with  productions 

S  ~ CD  Ab  ~  bA 
C  > aCA  Ba  ~  aB 

C  ~  bCB  Bb  ~ ~ bB 
A D   >aD  C  > e 

B D -   >bD  D  >e 

Aa  ~ aA 

An  example  of a  derivation  in  G is 

S - - - ~ C D  

-  ~.aCAD 
-  ~  a b C B A D  
===~. abBA D 

abBaD 

abaBD 

abab D 
abab 

REPRESENTATIONS  FOR  LANGUAGES 

D 

89 

We  shall  show  that  L ( G ) =   [ww] w  ~  {a,  b}*}.  That  is,  L(G)  consists  of 

strings  of a's  and  b's  of even  length  such  that  the  first  half  of each  string  is 
the  same  as the  second  half. Since 

L(G)  is a  set, the  easiest way to  show thaf L(G)  =  {ww[w  ~  {a, b}*} 
is to  show that  {ww]w  ~  {a, b}*} ..q L(G)  and that L(G)  ~  [ww[w  ~  {a, b}*}. 

90  ELEMENTS OF  LANGUAGE THEORY  CHAP*  2 

To  show  that  {wwlw  ~  {a,  b}*} ~  L(G)  we  must  show  that  every  string 
of the  form  ww  can  be  derived  from  S.  By  a  simple  inductive  proof  we  can 

show  that  the  following  derivations  are  possible  in  G" 

(I)  s  ~  CD . 
(2)  For  n  .>  O, 

n C  ~  c~c2  . ."   c .C X.X._ ~  .. .  X~ 

c~c~  . ..  c~X.X~_~  . ..  X~ 
where,  for  1 ~  i ~  n,  ct  =  a  if  and  only  if  Xt  =  A,  and  ct =  b  if and  only 

i f X t =   B. 

(3)  xo  * ..  X2X1D  ~ X n * ' "   X2c,D n--I 

c t X  ~ . . .   X2D 
.~ c~X~  . . .   X3c2D n-2 
~" c lc2X~  . . .   X3D 

clc2  " "   c,,-tXnD 
CtC 2  *..  Cn_lCnD 
>"  ClC  2  *  *  .  Cn_iC  n 

The  details  of  such  an  inductive  proof  are  straightforward  and  will  be 
omitted  here. 

In  derivation  (2),  C  derives  a  string  of  a's  and  b's  followed  by  a  mirror 
image  string  of  A's  and  B's.  In  derivation  (3),  the  A's  and  B's  migrate  to 
the  right  end  of  the  string,  where  an  A  becomes  a  and  B  becomes  b  on  con- 
tact  with  D,  which  acts  as  a  right  endmarker.  The  only  way  an  A  or  B  can 
be  replaced  by  a  terminal  is  for  it  to  move  to  the  right  end  of  the  string. 
In  this  fashion  the  string  of  A's  and  B's  is  reversed  and  thus  matches  the 
string  of  a's  and  b's  derived  from  C  in  derivation  (2). 

Combining  derivations  (1),  (2),  and  (3)  we  have  for  n  ~  0 

+ 
S~cic2  ..*  cnciC2  ***  C n 

where  c,  E  {a, b} for  1 <  i <  n.  Thus  [wwlw  ~  {a, b}*}  ~  L(G). 

We  would  now  like  to  show  that  L(G)  ~  [wwl w  ~  [a, b}*}.  To  do  this 
we must  show  that  S  derives  terminal  strings  only of the form  ww. In general, 

to  show  that  a  grammar  generates  only  strings  of a  certain  form  is  a  much 

more  difficult  matter  than  showing  that  it  generates  certain  strings. 

At  this  point  it  is  convenient  to  define  two  homomorphisms  g  and  h 
such  that 

SEC. 2.1  REPRESENTATIONS FOR LANGUAGES  91 

g(a)  =  a,  g(b)  =  b,  g(A)  =  g ( B ) =   e 
and 

h(a)  =  h(b)  =  e,  h(A)  =  A,  h(B)  =  B 

m For this  grammar  G we can  prove  by induction  on  m  ~  1 that  if  S  ~  ~, 

then  ac can  be  written  as  a  string  clcz  . . .   c, U fl V  such  that 

(1)  Each  ct is  either  a  or  b; 
(2)  U is  either  C  or  e; 

(3)  fl  is  a  string  in  [a,  b,  A, B}"  such  that  g ( f l ) =   c ~ c 2. .. c ~ , 

h(,O  =  X.X._,  .  .  .  X,+,, 
and  Xj  is A  or  B  as  cj is  a  or  b,  i  <  j  <  n;  and 

(4)  V is  either  D  or  e. 

The  details  of this  induction  will  be  omitted. 

We  now  observe  that  the  sentential  forms  of  G  which  consist  entirely  of 
terminal  symbols  are  all  of  the  form  e l e z . . ,   e , e ~ c z . . . e , ,   where  each 

c;  ~  [a,  b]. 
Thus,  L(G)  ~  {ww]w  ~  [a,  b}*}. 

We  can  now  conclude  that  L(G)  =  (wwlw  ~  [a,  b}*}.  D 

2.1.3.  Restricted  Grammars 

Grammars  can  be classified  according  to  the  format  of their  productions. 
Let  G  =  (N,  2:, P,  S)  be  a  grammar. 

DEFINITION 

G  is  said  to  be 
(1)  Right-linear  if  each  production  in  P  is  of  the  form  A  ~  xB   or 
A  ~  x,  where  A  and  B  are  in  N  and  x  is  in  Z*. 

(2)  Context-free  if  each  production  in  P  is  of  the  form  A  ~  ~,  where 
A  is in  N  and  tx is  in  (N  u  Z)*. 

(3)  Context-sensitive  if each  production  in P  is of the form  ~  ---~ fl,  where I~I_<lPl. 

A  grammar  with  no  restrictions  as  above  is  called  unrestricted. 

The grammar  of Example  2.3  is a  right-linear  grammar,  Another  example 
of  a  right-linear  grammar  is  the  grammar  wi th the  productions 

S  ---->. OS[1St  e 
This  grammar  generates  the  language  [0,  1}*. 

92  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

The  grammar  of Example  2.4  is  an  important  example  of a  context-flee 
grammar. Notice that according to our definition, every right-linear grammar 
is  also  a  context-free  grammar. 

The  grammar  of  Example  2.5  is  clearly  a  context-sensitive  grammar. 
We  should  emphasize  that  the  definition  of  context-sensitive  grammar 
does  not  permit  a  production  of  the  form A  ~  e,  commonly known  as  an e-production. 

Thus  a  context-free  grammar  having  e-productions  would  not 
be  a  context-sensitive grammar. 

The  reason  for  not  permitting  e-productions  in  context-sensitive  gram- 
mars is to ensure that the language generated by a context-sensitive grammar 
i~ recursive.  That  is  to  say,  we  want  to  be  able  to  give an  algorithm  which, 
presented  with  an  arbitrary  context-sensitive  grammar  G  and  input  string 

w,  will  determine  whether  or  not  w is  in  L(G). (See  Exercise  2.1.18.) 

Even if we permitted just one e-production in a context-sensitive grammar 
(without imposing additional conditions on the grammar), then the expanded 

class  of grammars  would  be  capable  of defining any  recursively  enumerable 
set  (see  Exercise  2.1.20).  The  grammar  in  Example  2.6  is  unrestricted.  Note 
that  it  is  not  right-linear,  context-free,  or  context-sensitive. 

CONVENTION 

If a  language L  can  be generated  by  a  type  x  grammar,  then L  is said to 
be  a  type  x  language,  for  all  the  "type x"  's  that  we  have  defined  or  shall 
define. 

Thus  L(G) of  Example  2.3  is  a  right-linear  language,  L(Go) in  Example 
2.4  is  a  context-free  language,  and L(G) of Example  2.5  is  a  paradigm  con- 
text-sensitive language.  The  language  generated  by the  grammar in  Example 
2.6  is  an  unrestricted  language,  although  {wwlw ~  (a, b}*  and  w ~  e]  also 
happens  to  be  a  context-sensitive  language. 

The  four  types  of  grammars  and  languages  we  have  defined  are  often 
referred  to  as  the  Chomsky hierarchy. 

CONVENTION 

We  shall  hereafter  abbreviate  context-free  grammar  and  language  by 
CFG  and  CFL,  respectively,  Likewise,  CSG  and  CSL  stand  for  context- 
sensitive  grammar  and  context-sensitive  language. 

Every  right-linear  language  is  a  CFL,  and  there  are  CFL's,  such  as 
[0"l"]n  ~>  1},  that  are  not  right-linear.  The  CFL's  which  do  not  contain 
the  empty  string  likewise form  a  proper  subset  of the  context-sensitive  lan- 
guages.  These  in  turn  are  a  proper  subset  of  the  recursive  sets,  which  are 
in  turn  a  proper  subset  of  the  recursively  enumerable  sets.  The  (unrestric- 
ted) grammars  define  exactly the  recursively  enumerable  sets. These  matters 

are  left for the  Exercises. 

SEC. 2.1  REPRESENTATIONS FOR LANGUAGES  93 

Often,  the  context-sensitive  languages  are  defined  to  be  the  languages 
that  we  have  defined  plus  all those  languages L  U  [e}, where L  is  a  context- 

sensitive  language  as  defined  here.  In  that  case,  we  may  call  the  CFL's 
a  proper  subset  of the  CSL's. 

We  should  emphasize  the  fact  that  although  we  may  be  given  a  certain 
type of grammar, the language generated by that grammar might be generated 

by a less powerful  grammar.  As  a  simple example,  the context-free  grammar 

S  >  ASIe 
A  >011 

generates  the language  {0,  1 }*, which,  as we have  seen,  can  also  be generated 
by  a  right-linear  grammar. 

We  should  also  mention  that  there  are  a  number  of grammatical  models 
that  have  been  recently introduced  outside  the  Chomsky hierarchy.  Some  of 
the motivation in introducing new grammatical models is to find a generative 

device  that  can  better  represent  all  the  syntax  and/or  semantics  of  pro- 
gramming languages.  Some  of these  models  are  introduced  in  the  Exercises. 

2.1.4.  Recognizers 

A  second  common  method  of  providing  a  finite  specification  for  a  lan- 
guage  is  to  define  a  recognizer  for  the  language.  In  essence  a  recognizer  is 

merely  a  highly  stylized  procedure  for  defining  a  set.  A  recognizer  can  be 
pictured  as  shown in  Fig.  2.1. 

There  are three parts to a recognizer  an input tape,  a finite state control, 
and  an  auxiliary memory. 

! ao  at  [  a2  an  Input tape 
t 
~  Input head 

Finite state control 

Fig. 2.1  A recognizer. 

94  ELEMENTS  OF LANGUAGE THEORY  CHAP.  2 

The  input  tape can  be  considered  to  be  divided  into  a  linear  sequence  of 
tape  squares,  each  tape  square  containing  exactly  one  input  symbol  from 

a  finite input  alphabet.  Both  the  leftmost  and  rightmost  tape  squares  may be 
occupied  by  unique  endmarkers,  or  there  may  be  a  right  endmarker  and  no 
left  endmarker,  or  there  may  be  no  endmarkers  on  either  end  of  the  input 
tape. 

There  is an  input head, which  can  read  one  input  square at a  given instant 
of time.  In  a  move  by  a  recognizer,  the  input  head  can  move  one  square  to 
the  left,  remain  stationary,  or  move  one  square  to  the  right.  A  recognizer 
which  can  never move  its input  head  left is called  a  one-way recognizer. 

Normally,  the  input  tape  is  assumed  to  be  a  read-only  tape,  meaning 
that  once  the  input  tape  is  set  no  symbols  can  be  changed.  However,  it  is 
possible  to  define  recognizers  which  utilize  a  read-write  input  tape. 

The  memory  of  a  recognizer  can  be  any  type  of  data  store.  We  assume 
that  there  is  a  finite  memory  alphabet  and  that  the  memory  contains  only 

symbols  from  this  finite  memory  alphabet  in  some  data  organization.  We 

also  assume  that  at  any  instant  of time  we  can  finitely  describe  the  contents 
and  structure  of  the  memory,  although  as  time  goes  on,  the  memory  may 
become  arbitrarily  large.  An  important  example  of  an  auxiliary  memory  is 
the  pushdown  list, which  can  be  abstractly  represented  as  a  string of memory 

symbols,  e.g.,  Z1 Z2  . . .   Zn,  where  each  Zt  is  assumed  to  be  from  some finite 
memory  alphabet  IF', and  Z 1 is  assumed  to  be  on  top. 

The  behavior  of the  auxiliary  memory  for  a  class  of recognizers  is  char- 
acterized  by  two  functions--a  store  function  and  a  fetch  function.  It  is 
assumed  that  the fetch function  is a  mapping  from the  set of possible  memory 
configurations  to  a  finite set  of information symbols, which  could  be  the  same 

as  the  memory  alphabet. 

For example,  the  only information  that  can be  accessed from  a pushdown 
list  is the  topmost  symbol.  Thus  a fetch  function f  for a pushdown  list would 

be  a  mapping  from  r  + to  IF' such  that f(Za  . . .   Z , ) =   Za. 

The  store function  is  a  mapping  which  describes  how  memory  may  be 
altered.  It  maps  memory  and  a  control string  to  memory.  If we  assume  that 

a  store  operation  for  a  pushdown  list  replaces  the  topmost  symbol  on  the 
pushdown  list  by  a  finite  length  string  of  memory  symbols,  then  the  store 
function  g  could  be  represented  as  g:  F +  *  IF'* ~  F*,  such  that 

g ( Z , Z ~ . . .   Zn,  r,  . . .   r , ) =   Y,  . . .   Y k Z ~ . . .   Zn. 
If  we  replace  the  topmost  symbol  Z 1  on  a  pushdown  list  by  the  empty 

string,  then  the  symbolZ2  becomes  the  topmost  symbol  and  can  then  be 
accessed  by  a  fetch  operation. 

Generally  speaking,  it  is the  type  of memory  which  determines  the  name 
of a  recognizer.  For  example  a  recognizer  having  a  pushdown  list for  a  mem- 

src.  2.1  REPRESENTATIONS FOR LANGUAGES  95 

ory  would  be  called  a  pushdown  recognizer  (or  more  usually,  pushdown 
automaton). The  heart  of a  recognizer  is the finite  state  control,  which can  be thought 

of as  a  program  which  dictates  the  behavior  of the  recognizer.  The  control 
can  be  represented  as  a  finite  set  of  states  together  with  a  mapping  which 
describes  how the  states  change in accordance  with the  current input  symbol 
(i.e.,  the  one  under  the  input  head)  and  the  current  information  fetched 
from  the  memory.  The  control  also  determines  in which  direction  the  input 

head  is  to  be  shifted  and  what  information  is  to  be  stored  in  the  memory. 

A  recognizer  operates  by  making  a  sequence  of  moves.  At  the  start  of a move, 

the current input symbol is read, and the memory is probed by means 
of the  fetch function,  The  current  input  symbol and  the  information  fetched 
from  the  memory,  together  with  the  current  state  of the  control,  determine 

what  the  move  is  to  be.  The  move  itself consists  of 

(1)  Shifting  the  input  head  one  square  left,  one  square  right,  or  keeping 
the  input  head  stationary; (2)  Storing information  into  the  memory;  and 

(3)  Changing the  state  of the  control. 
The  behavior  of a  recognizer  can  be  conveniently  described  in  terms  of 
configurations  of the recognizer. A  configuration is a picture of the recognizer 

describing 

(1)  Thestate  of the  finite  control; (2)  The contents of the input tape, together with the location  of the input 

head;  and (3)  The  contents  of the  memory. 

We  should  mention  here  that  the  finite  control  of  a  recognizer  can  be 
deterministic  or  nondeterministic.  If the  control  is  nondeterministic,  then  in 
each  configuration  there  is  a  finite  set  of possible  moves  that  the  recognizer 

can  make. The  control  is  said  to  be  deterministic  if  in  each  configuration  there  is 

at  most  one  possible  move.  Nondeterministic  recognizers  are  a  convenient 
mathematical  abstraction,  but,  unfortunately,  they  are  often  difficult  to 

simulate  in  practice.  We  shall  give  several  examples  and  applications  of 
nondeterministic  recognizers  in  the  sections  that  follow. 

The  initial configuration  of a  recognizer is  one in  which  the  finite controI 
is in  a  specified initial  state,  the  input  head  is  scanning  the  leftmost  symbol 

on the input  tape,  and the  memory has a  specified initial  content. A final configuration 

is one in which the finite control  is in one of a  speci- 
fied set of final states  and the input  head is  scanning the right endmarker  or, 
if there is no right endmarker,  has moved  off the right end  of the input  tape. 

Often,  the  memory  must  also  satisfy certain  conditions  if the  configuration 
is to  be  considered  final. 

96  ELEMENTS OF LANGUAGE  THEORY  CHAP.  2 

We  say  that  a  recognizer  accepts  an  input  string  w  if,  starting  from  the 
initial  configuration  with  w  on  the  input  tape,  the  recognizer  can  make 

a  sequence  of moves  and  end  in  a  final  configuration. 

We  should  point  out  that  a  nondeterministic  recognizer  may  be  able  to 
make  many different  sequences  of moves from an initial configuration.  How- 

ever,  if  at  least  one  of  these  sequences  ends  in  a  final  configuration,  then 
the  initial  input  string  will  be  accepted. 

The  language  defined  by  a  recognizer  is  the  set  of input  strings  it  accepts. 
For  each  class  of grammars  in  the  Chomsky hierarchy  there  is  a  natural 
class of recognizers that defines the same class of languages.  These recognizers 

are  finite  automata,  pushdown  automata,  linear  bounded  automata,  and 
Turing machines.  Specifically, the following characterizations  of the Chomsky 
languages  exist" 

(I)  A  language L  is right-linear  if and  only if L  is  defined  by  a  (one-way 
deterministic)  finite  automaton. 

(2)  A  language L  is context-free  if and  only if L  is  defined  by  a  (one-way 
nondeterministic)  pushdown  automaton, 

(3)  A  language L  is context-sensitive  if and  only if L  is defined by a  (two- 
way  nondeterministic)  linear  bounded automaton. 

(4)  A  language L  is recursively  enumerable  if and  only if L  is defined  by a  Turing  machine. 

The  precise  definition  of these  recognizers  will  be  found  in  the  Exercises 
and  later  sections.  Finite  automata  and  pushdown  automata  are  important 
in  the  theory  of compiling  and  will be  studied  in  some  detail  in this  chapter. 

E X E R C I S E S  
2.1.1.  Construct right-linear grammars for 

(a)  Identifiers  which can  be  of arbitrary  length  but  must  start  with a letter (as in ALGOL). 

(b)  Identifiers  which can  be  one  to  six  symbols in length  and  must start  with L  J,  K, L,  M,  or  N  (as  for  FORTRAN  integer variables). 
(c)  Real  constants  as in  PL/I  or  FORTRAN,  e.g.,  -- 10.8, 3.14159, 2.,  6.625E-27. 
L (d)  All  strings  Of  o's  and  l's  having  both  an  Odd  number  of  O's and an  odd number  of l's. 

2.1.2.  Construct  context-free  grammars  that  generate 

(a)  All  strings  of O's and  l's  having  equal  numbers  of 0's  and  l's. (b) 

[aiaz  ...  anan..,  azal  lag  ~  {0,  1},  1 ~  i ~  n}. (c)  Well-formedstatements in propositional calculus. 

(d)  (0'lJl i ~  j  and  i, j  >  0). (e)  All possible sequences  of balanced parenthesesl 

EXERCISES  97 
"2.1.3.  Describe  the  language  generated  by  the  productions  S----~  bSSla. 

Observe that  it is not  always easy to  describe what language  a  grammar 

generates. 

"2.1.4.  Construct  context-sensitive grammars  that  generate 

(a)  [an'In  >  1). 
(b)  {ww I w  ~  [a, b} +}. (c)  [wlw  ~  [a, b, c} + and  the  number  of a's  in  w equals  the  number 

of b's  which  equals  the  number  of c's]. 

(d)  [ambnambnl m,  n  >  1 }. 

Hint:  Think  of  the  set  of  productions  in  a  context-sensitive  grammar 

as  a  program.  You  can  use  special  nonterminal  symbols  as  a  combina- 
tion  "input  head"  and  terminal  symbol. 

"2.1.5.  A  "true"  context-sensitive  grammar  G  is  a  grammar  (N,  ~, P,  S)  in 

which  each  production  in P  is  of  the  form 

where  *  and  fl  are  in  (N  U  E)*,  ~'  ~  (N  u  ZZ) +,  and  A  ~  N.  Such 

a  production  can  be  interpreted  to  mean  that  A  can  be  replaced  by 

only  in  the  context  0~_fl. Show  that  every  context-sensitive  language 

can  be  generated  by  a  "true"  context-sensitive grammar. 

* "2.1.6.  What  class  of  languages  can  be  generated  by  grammars  with  only 

left  context,  that  is,  grammars  in which  each  production  is  of the  form 
~zA ~  ~zfl, ~z in  (N  U  X~)*, fl  in  (N  U  E)+ ? 

2.1.7.  Show  that  every context-free language  can  be  generated  by  a  grammar 

G  =  (N,  ~ , P,   S)  in  which  each  production  is  of  either  the  form 
A  ---~ cz, cz in  N*,  or  A  ~  w,  w in  E*. 

2.1.8.  Show that  every context-sensitive language can  be generated by a  gram- 

mar  G  =  (N, X~, P,  S)  in  which  each  production  is  either  of  the  form 

----~ fl,  where  cz  and  fl  are  in  N +,  or  A ~  w,  where  A  ~  N  and 
w~X  +. 

2.1.9.  Prove  that L(G)  =  [a"bnc" In ~  1}, where  G is the  grammar  in Example 

2.5. 

"2.1.10.  Can  you  describe  the  set  of  context-free  grammars  by  means  of  a 

context-free grammar ? 

* 2.1.11.  Show  that  every  recursivelY  enumerable  set  can  be  generated  by  a 

grammar  with  at  most  two  nonterminal  symbols.  Can  you  generate 
every  recursively  enumerable  set  with  a  grammar  having  only  one 

nonterminal  symbol ? 

2.1.12.  Show  that  if  G  =  (N,  ~, P,  S)  is  a  grammar  such  that  ~ N   =  n,  and 

X~ does  not  contain  any  of  the  symbols  A1, A2 . . . .   ,  then  there  is  an 
equivalent  grammar  G'  =  (N',  E, P',  A1)  such  that 

N'  =  [A1,  A 2 , . . . ,   An}. 

98  ELEMENTS OF LANGUAGE THEORY  CHAP. 2 

2.1.13.  Prove  that  the  grammar  G~  of  Example  2.1  generates  {0"l"[n  >  1}. 

Hint:  Observe  that  each  sentential  form  has  at  most  one  nonterminal. 
Thus  productions  can  be  applied  in  only  one  place  in  a  string. 

DEFINITION 

In  an  unrestricted  grammar  G  there  are  many  ways  of  deriving  a 
given  sentence  that  are  essentially  the  same,  differing  only  in  the  order 
in  which  productions  are  applied.  If  G  is  context-free,  then  we  can 
represent  these  essentially  similar  derivations  by  means  of  a  derivation 

tree.  However,  if  G  is  context-sensitive  or  unrestricted,  we  can  define 
equivalence  classes  of  derivations  in  the  following  manner. 

Let  G  =  (N,  E, P,  S)  be  an  unrestricted  grammar.  Let  D  be  the 

set  of  all  derivations  of  the  form  S  ~  w.  That  is,  elements  of  D 
are  sequences  of the form  (0~0, 0~1 . . . .   , t~n) such  that  t~0  =  S,  ~n  ~  ~*, 
and  oct_ ~ ~  oct  1 <  i <  n. 

Define  a  relation  R0  on  D  by  (t~0, t X l , . . . ,   o~n) R0  (fl0,  r  i,.  * . ,   fin) if  and  only  if there  is  some  i  between  1  and  n  -  1  such  that 

(1)  txj  =  fl~  for  all  1 ~  j  ~  n  such  that j  ~  i. 
(2)  We  can  write  oct_l =  71727a~'4~'s  and  t~+l  =  ~'lO?aeys  such  that 
7 2 ~ t ~   and  Y4~6  are  in  P,  and  either  t x t = 7 ' ~ 3 y 4 ~ s   and 
fit  =  71Y2736~'5, or  conversely. 

Let  R  be  the  least  equivalence  relation  containing  R0.  Each  equiva- 
lence  class  of R  represents  the  essentially  similar  derivations  of a  given 

sentence. 

*'2.1.14.  What  is  the  maximum  size  of  an  equivalence  class  of  R  (as  a  function 

of n  and  i 0c~ I) if  G  is 

(a)  Right-linear. 
(b)  Context-free. 
(c)  Such that  every production is of the form  tx ~  fl  and  [ tx I <  [fl [. 

* 2.1.15.  Let  G be  defined  by 

S  ~  AOBIB1A 
A -   > B_BI0 

B----~  AAI  1 

What  is  the  size  of  the  equivalence  class  under  R  which  contains  the 

derivation 

S  ~  AOB ==~ BBOB ==~ 1BOB ==~  1AAOB  ~  IOAOB 

1000B ~  10001 

DEFINITION 

A  grammar  G  is  said  to  be  unambiguous  if each  w  in  L(G)  appears 
as  the  last  component  of  a  derivation  in  one  and  only  one  equivalence 
class  under  R,  as  defined  above.  For  example, 

EXERCISES  99 
S -   > abClaB 
B-  > be 
bC---->  bc 

is an  ambiguous  grammar  since the  sequences 

(S, abC, abc)  and  (S, aB, abc) 
are  in  two  distinct  equivalence classes. 
"2.1.16.  Show  that  every right-linear  language  has  an  unambiguous right-linear 

grammar. 

"2.1.17.  Let  G  =  (N,  ~, P,  S)  be  a  context-sensitive  grammar,  and  let  N  u  X~ 

n have  m  members.  Let  w be  a  word  in  L(G).  Show  that  S ==~ w,  where 

G n <  (m  +  1)IwI. 

2.1.18.  Show  that  every  context-sensitive  language  is  recursive.  Hint:  Use  the 

result  of Exercise  2.1.17  to  construct  an  algorithm  to  determine  if  w is 
in L(G)  for  arbitrary  word  w  and  context-sensitive grammar  G. 

2.1.19.  Show  that  every  CFL  is  recursive.  Hint:  Use  Exercise  2.1.18,  but  be 

careful  about  the  empty word. 

"2.1.20.  Show  that  if  G  =  (N,  ~,  P,  S)  is  an  unrestricted  grammar,  then  there 

is  a  context-sensitive  grammar  G ' =   (N',  X~ U  {c}, P',  S')  such  that 

w is in L(G) if and  only if wc ~ is in L(G') for some  i >  0 
Hint:  Fill  out  every  noncontext-sensitive  production  of  G  with  c's. 
Then  add  productions  to  allow  the  c's  to  be  shifted  to  the  right  end 

of any sentential form. 

2.1.21.  Show  that  if  L  =  L(G)  for  any  arbitrary  grammar  G,  then  there  is 

a  context-sensitive  language  Li  and  a  homomorphism  h  such  that 
L  =  h(L1). 

2.1.22.  Let  {A1, A2 . . . .   }  be  a  countable  set  of  nonterminal  symbols,  not 

including  the  symbols  0  and  1.  Show  that  every  context-sensitive 
language  L  ~ { 0 , 1 } *   has  a  CSG  G  =  (N,  {0,1},  P,  A1),  where 
N  =  {A1, A 2 , . . .   ,A~},  for  some  i.  We  call  such  a  context-sensitive 

grammar  normalized. 

"2.1.23.  Show  that  the  set  of normalized  context-sensitive  grammars  as  defined 

above  is countable. 

"2.1.24.  Show  that  there  is  a  recursive  set  contained  in  {0,  i}*  which  is  not  a 

context-sensitive  language.  Hint:  Order  the  normalized,  context- 
sensitive  grammars  so  that  one  may  talk  about  the  ith  grammar.  Like- 
wise,  lexicographicaUy  order  {0,  1 }*  so  that  we  may  talk  about  the  ith 

string  in  {0,  1}*. Then  define  L  =  {w~lw~ is  not  in  L(G3}  and  show 
that L  is recursive but  not  context-sensitive. 

'1 O0  ELEMENTS  OF  LANGUAGE THEORY  CHAP.  2 
*'2.1.25.  Show  that  a  language  is  defined  by  a  grammar  if  and  only  if it  is  re- 

cognized  by  a  Turing  machine.  (A  Turing  machine  is  defined  in  the 
Exercises of Section 0.4  on  page  34.) 

2.1.26.  Define a nondeterministic recognizer whose memory is an initially blank 

Turing  machine  tape,  which  is  not  permitted  to  grow  longer  than  the 
input.  Show  that  a  language is defined by  such  a  recognizer if and  only 
if  it  is  a  CSL.  This  recognizer  is  called  a  linear  bounded  automaton 

(LBA,  for short). 

DEFINITION 

An  indexed grammar  is a  5-tuple  G  =  (N,  E, A, P,  S),  where  N,  E, 
and  A  are  finite  sets  of  nonterminals,  terminals,  and  intermediates, 
respectively.  S  in  N  is  the  start  symbol  and  P  is  a  finite  set  of produc- 

tions  of the forms 

and 

A  +  XiViX2 V2  "'"  X. V.,  

M---->  X~ ~u I X2 V 2 " "   X+V., 

n > 0  

n ~ 0  
where  A  is  in  N,  the  X's  in  N  kJ  E, f  in  A,  and  the  V's  in  A*,  such 

that if Xi is in Z,  then  V~  =  e.  Let ~  and  ,8  be  strings  in  (NA*  u  E)*, 
A  e  N,  0  e  A*,  and  let A --+  X~ V~  "'"  X,V,  be  in P.  Then  we  write 

ocXOp ~  ocXt e  ~ O~ x2e20[  ...  X~e~O'J~ G 
where  0~  =  0  if  ~  ~  N  and  0~  =  e  if  X~ ~  E.  That  is,  the  string  of 
intermediates  following  a  nonterminal  distributes  over  the  nonter- 
minals,  but  not  over  the  terminals,  which  can  never  be  followed 
by  an  intermediate.  If  A f  ~  X1Vx  ,'"  XnV,  is in P, then 

o~AfOfl ~  O~Xllp'lO~ . "   X.ly,O~fl, G 
as  above.  Such  a  step  "consumes"  the  intermediate  following  A,  but , 
otherwise  is the  same  as  the first  type  of step.  Let ~  be  the  reflexive, 

G transitive  closure  of ===~, and  define 

G 
L(G)  =  {wlw  in  ~*  and  S  ~  w}. 

G Example  2.7 

Let  G  =  ({S, T, A, B,  C},  {a, b, c}, {f, g }, P,  S),  where  P  consists  of 

S---->Tg 

T ~  Tf 
T ~  A~ C 
A f   > aA 

B f - - - >   bB 

EXERCISES  101 
C f -   >cC 
Ag----+a 

B g - - - ~ b  

Cg  =-+. c 

Then L(G)  =  {a"b"c" In _> 1 }. For  example,  aabbcc  has the derivation 

S - - ~ T g  

- - ~   Tfg 

AfgBfgCfg 
aAgBfgCfg 
aaBfgCfg 
- - ~   aabBgCfg 

aabbCfg 

aabbeCg 
> aabbcc  [-] 

"2.1.27.  Give  indexed  grammars  for  the  following  languages: 

(a)  {wwl  w  ~  {a, b]*]. 
(b)  {a"b"21n ~  1). 

*'2.1.28.  Show  that  every  indexed  language  iscontext-sensitive. 

2.1.29.  Show  that  every  CFL  is  an  indexed  language. 
2.1.30.  Let  us  postulate  a  recognizer  whose memory  is  a  single  integer  (written 

in  binary  if  you  will).  Suppose  that  the  memory  control  strings  as 

described  in  Section  2.1.4  are  only  X  and  Y.  Which  of  the  following 
could  be  memory  fetch  functions  for  the  above  recognizer ? 

0,  if i is even (a)  f(i)  =  1,  if i is  odd. 

a,  if i is even (b)  f(i)  b,  if i is  odd. 

*  [0,  if  i  is  even  and  the  input  symbol  under  the  input 
(c)  f ( i ) =   1  head  is a 1,  otherwise. 

2.1.31.  Which  of  the  following  could  be  memory  store  functions  for  the 

recognizer  in Exercise  2.1.30 ? 

(a)  g(i,  X)  --  0 

g(i,  Y)  --  i  /  t. 
(b)  g(i,  X)  --  0 

g(i,  Y ) - { i   /  1,  if the  previous  store  instruction  was  X 

/  2,  if the  previous  store  instruction  was  Y. 

102  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

DEFINITION 

A  tag system consists  of two  finite  alphabets  N  and  ~  and  a  finite 
set  of  rules  of  the  form  (t~, fl),  where tx  and  fl  are  in  (N  U  E)*.  If  ~, 
is  an  arbitrary  string  in  (N  u  E)*  and  (t~, fl)  is  a  rule,  then  we  write 

* ~' I-  ?ft.  That  is,  the  prefix  ~  may  be  removed  from  the  front  of  any . 

string  provided  fl  is  then  placed  at  the  end  of  the  string.  Let  ~  be 
the  reflexive,  transitive  closure  of  ~.   For  any  string  ?  in  (N  W E)*, 

L r  is  {w I w is  in  E*  and  ?  t--  w}. 

* "2.1.32.  Show  that  L r  is  always  defined  by  some  grammar.  Hint:  Use  Exercise 

2.1.25  or  see  Minsky  [1967]. 

* "2.1.33.  Show  that  for  any  grammar  G, L(G)  is  defined  by  a  tag  system  in  the 

manner  described  above.  The hint  of Exercise  2.1.32  again  applies. 

Open  Problems 

2.1.34.  Is  the  complement  of  a  context-sensitive  language  always  context- 

sensitive ? 

The  recognizer  of  Exercise  2.1.26  is  called  a  linear  bounded  auto- 
maton (LBA).  If we  make  it  deterministic,  we have  a  deterministic LBA 

(DLBA). 

2.1.35.  Is every context-sensitive  language  recognized  by  a  DLBA ? 

2.1.36.  Is  every  indexed  language  recognized  by  a  DLBA ? 

By  Exercise  2.1.28,  a  positive  answer  to  Exercise  2.1.35  implies  a 
positive  answer  to  Exercise  2.1.36. 

B I B L I O G R A P H I C   N O T E S  
Formal  language  theory  was  greatly  stimulated  by  the  work  of  Chomsky  in 
the  late  1950's  [Chomsky,  1956,  1957,  1959a,  1959b].  Good  references  to  early 
work  on  generative  systems  are  Chomsky  [1963]  and  Bar-Hillel  [1964]. 

The  Chomsky  hierarchy  of  grammars  and  languages  has  been  extensively 
studied.  Many  of  the  major  results  concerning  the  Chomsky  hierarchy  are  given 
in the Exercises.  Most  of these results  are proved in detail  in Hopcroft  and  UUman 

[1969]  or  Ginsburg  [1966]. 

Since  Chomsky  introduced  phrase  structure  grammars,  many  other  models  of 
grammars  have  also  appeared  in  the  literature.  Some  of these  models  use  special- 

ized  forms  of  productions.  Indexed  grammars  [Aho,  1968],  macro  grammars 

[Fischer,  1968],  and  scattered  context  grammars  [Greibach  and  Hopcroft,  1969] 
are  examples  of  such  grammars.  Other  grammatical  models  impose  restrictions 

on  the  order in which  productions  can be applied.  Programmed  grammars  [Rosen- 
krantz,  1968]  are  a  prime  example. 

Recognizers  for languages  have  also  been  extensively studied.  Turing  machines 
were  deffined by  A.  Turing  in  1936.  Somewhat  later,  the  concept  of  a  finite  state 

SEC. 2.2  REGULAR SETS, THEIR  GENERATORS, AND  THEIR  RECOGNIZERS  103 
machine  appeared  in  McCulloch  and  Pitts  [1943]. The  study  of  recognizers  was stimulated by the work  of Moore  [1956] and  Rabin  and  Scott  [1959]. 

A  significant  amount  of effort  in  language  theory  has  been  expended  in  deter- 
mining the  algebraic properties  of classes of languages and  in  determining decida- 
bility results for classes of grammars and recognizers.  For  each  of  the four classes of grammars in the Chomsky hierarchy there is a  class  of recognizers  which defines 

precisely those languages generated by that class of grammars.  These observations have  led  to  a  study  of  abstract  families  of  languages  and  recognizers  in  which 
classes of languages  are  defined  in  terms of algebraic properties.  Certain algebraic properties  in  a  class  of  languages  are  necessary  and  sufficient  to  guarantee  the 
existence  of  a  class  of  recognizers  for  those  languages.  Work  in  this  area  was 
pioneered  by  Ginsburg  and  Greibach  [1969] and  Hopcroft  and  Ullman  [1967]. Book  [1970] gives  a  good  survey  of language  theory  circa  1970. 

Haines  [1970] claims  that  the  left context  grammars in  Exercise  2.1.6  generate exactly the  context-sensitive languages.  Exercise 2.1.28  is  from  Aho  [1968]. 

2.2.  REGULAR  SETS,  THEIR  GENERATORS, 

AND  THEIR  RECOGNIZERS 

The  regular  sets  are  a  class  of  languages  central  to  much  of  language 
theory. In this section we shall study several methods  of specifying languages, 

all  of  which  define  exactly  the  regular  sets.  These  methods  include  regular 
expressions,  right-linear  grammars,  deterministic  finite  automata,  and  non- 

deterministic  finite  automata. 

2.2.1.  Regular  Sets and  Regular  Expressions 

DEFINITION 

Let  Z  be  a  finite  alphabet.  We  define  a  regular  set  over  Z  recursively  in 
the  following  manner: 

(1)  ~  (the  empty  set)  is  a  regular  set  over  Z. 
(2)  {e} is  a  regular  set  over  E. 

(3)  {a} is  a  regular  set  over  Z  for  all a  in  Z. 
(4)  If P  and  Q  are  regular  sets  over  Z,  then  so  are 

(a)  P  U  a.  
(b)  PQ. (c)  P*. 

(5)  Nothing  else  is  a  regular  set. 
Thus  a  subset  of Z*  is  regular  if and  only  if it  is  ~ ,   {e}, or  {a}, for  some 
a  in  Z,  or  can  be  obtained  from  these  by  a  finite  number  of  applications  of 
the  operations  union,  concatenation,  and  closure. 

We  shall  define  a  convenient  method  for  denoting  regular  sets  over 
a  finite  alphabet  E. 

104  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 
DEFINITION 

Regular  expressions over X  and  the  regular  sets  they  denote are  defined recursively,  as follows" 

(1)  ~  is a  regular  expression  denoting  the  regular  set  ~.  (2)  e is a  regular  expression denoting  the  regular  set  {e}. 

(3)  a  in  X is  a  regular  expression  denoting  the  regular  set  {a}. (4)  I fp  and q  are  regular  expressions  denoting  the  regular  sets P  and  Q, 
respectively,  then (a)  (p  +  q)  is a  regular  expression  denoting P  u  Q. 

(b)  (pq) is  a  regular  expression  denoting PQ. (e)  (p)*  is  a  regular  expression  denoting P*. 
(5)  Nothing  else is  a  regular  expression. 
We  shall  use  the  shorthand  notation p+  to  denote  the  regular  expression pp*.  Also,  we  shall  remove  redundant  parentheses  from  regular  expressions 

whenever  no  ambiguity  can  arise.  In  this  regard,  we  assume  that  *  has  the highest  precedence,  then  concatenation,  and  then  -+-. Thus,  0  -+- 10"  means 
(0  +  (1 (0"))). 

Example 2.8 

Some examples  of regular  expressions  are 

(1)  01,  denoting  {01}. (2)  0",  denoting  {0}*. 

(3)  O  +  1)*,  denoting  {0,  1}*. (4)  (0  +  1)* 011, denoting the set of all strings of 0's and  l's ending in 011. 

(5)  (a +  b)(a +  b +  0  +  1)*, denoting the set of all strings in {0,  1, a, b}* beginning  with  a  or  b. 
(6)  (00  +  11)*((01  +  10)(00  +  11)*(01  +  10)(00  +  11)*)*,  denoting  the set  of all  strings  of  O's and  l's  containing  both  an  even  number  of  O's  and 
an  even number  of  l's.  E] 

It  should  be  quite  clear that  for  each  regular  set  we  can  find  at  least  one regular expression denoting that regular set. Also, for each regular expression 
we  can  construct  the  regular  set  denoted  by  that  regular  expression.  Unfor- tunately, for  each regular set there is an infinity of regular expressions  denot- 
ing that  set. We shall say two regular expressions  are equal ( =)  if they denote the same 

set. Some  basic  algebraic  properties  of  regular  expressions  are  stated  in  the 
following  lemma. 

LEMMA 2.1 

Let  g,  p,  and  ~, be  regular  expressions.  Then 

(1)  ~  +  fl  =  fl  +  ~  (2)  ~*  --  e 

SEC. 2.2  REGULAR  SETS,  THEIR GENERATORS,  AND  THEIR RECOGNIZERS  105 

(3)  0~ /  (fl  --k y)  =  (0~ +  fl)  +  y  (4)  oc(fl?) =  (~fl)? 
(5)  ~(fl  +  y ) =   ~fl  -q- ~?  (6)  (~  +  fl)}, =  ~,  +  ,8? 
(7)  ~e  =  e0c =  ~  (8)  Z0c  =  ~ ~  =  O 
(9)  ~*  =  *  +  ~*  (1 O)  (~*)*  --  ~* (11)  ~- + - ~ = ~   (12)  ~ +   ~  = 

Proof  (1)  Let  ~  and  fl  denote  the  sets  L 1  and  L 2,  respectively.  Then 

* -Jr fl denotes L1  U  L2 and  fl  +  ~ denotes L2  U  L1. But L t  U  L2  =  L2  u  L 1 from  the  definition  of union.  Hence,  0c +  ,8  =  fl  -q- 0~. 

The  remaining  parts  are  left  for  the  Exercises. 
In  what  follows,  we  shall  not  distinguish  between  a  regular  expression 
and  the  set  it  denotes  unless  confusion  will  arise.  For  example,  under  this 
convention  the  symbol  a  will  represent  the  set  ~a}. 

W h e n   dealing  with  languages  it  is  often  convenient  to  use  equations 
whose  indeterminates  and  coefficients  represent  sets.  Here,  we  shall  consider 

sets  of  equations  whose  coefficients  are  regular  expressions  and  shall  call 
such  equations  regular  expression  equations. 

For  example,  consider  the  regular  expression  equation 

(2.2.1)  X  =  a X   +  b 
where  a  and  b are  regular  expressions.  We  can  easily verify by direct  substitu- 
tion  that  X  =  a*b  is  a  solution  to  Eq.  (2.2.1).  That  is  to  say,  when  we  sub- 

stitute  the  set  represented  by  a*b  in  both  sides  of  Eq.  (2.2.1),  then  each  side 
of the  equation  represents  the  same  set. 

We  can  also  have  sets  of  equations  that  define  languages.  For  example, 
consider  the  pair  of equations 

(2.2.2)  X  =  a~ X  --Jr- a 2 Y -~- a3 Y =   blX-+-  b2 Y-+-  b3 
where  each  at  and  b~  is  a  regular  expression.  We  shall  show  how  we  cart 

solve  this  pair  of  simultaneous  equations  to  obtain  the  solutions 

X  :  (a 1 -Jr- azb2*ba)*(a 3 q-  a2bz*b3) 

Y  :  (b 2 +  b~aa*a2)*(b3  -if- b~aa*a3) 

However,  we  should  first  mention  that  not  all  regular  expression  equa- 
tions  have  unique  solutions.  For  example,  if 

(2.2.3)  X  =  ~X  +  fl 
is a regular  expression  equation  and  a  denotes  a  set which  contains  the empty 

string,  then  X  =  0c*(fl +  y) is  also  a  solution  to  (2.2.3)  for  all  ?.  (?  does  not 
even  have  to  be  regular.  See  Exercise  2.2.7.)  Thus  Eq.  (2.2.3)  has  an  infinity 

'! 06  ELEMENTS OF LANGUAGE  THEORY  CHAP.  2 
of  solutions.  In  situations  of this  nature  we  shall  use  the  smallest  solution, 
which  we  call  the  minimal  fixed  point.  The  minimal  fixed  point  for  Eq. 

(2.2.3) is  X  =  0~*fl. 

DEFINITION 

A  set  of regular  expression  equations  is  said  to  be  in standard form  over 
a  set  of  indeterminates  A  =  [X1, X 2 , . . . ,   X,}  if  for  each  Xt  in  A  there  is 
an  equation  of the  form 

Yi  :  ~io  +  ~ilYl  +  oci2Y2 ~-  "'"  +  O~inYn 
with  ~u  a  regular  expression  over  some  alphabet  disjoint  from  A. 

The  ~'s  are  the  coefficients.  Note  that  if  ~u  :  @,  a  possible  regular 
expression,  then  effectively  there  is  no  term  for  Xj  in  the  equation  for  Xt. Also,  if  0~ u  =  e,  then  effectively  the  term  for  Xj  in  the  equation  for  X~  is 

just  Xj. That  is,  Z~ plays  the  role  of coefficient 0,  and  e the  role  of coefficient 

1 in  ordinary  linear  equations. 

ALGORITHM 2. I 

Solving  a  set  of regular  expression  equations  in  standard  form. 
Input.  A  set  Q  of regular  expression  equations  in  standard  form  over  A, 
whose  coefficients  are  regular  expressions  over  alphabet  X.  Let  A be  the  set {x,,  xo}. 

Output.  A  set  of  solutions  of  the  form  Xt  =  0ci, 1 ~  i ~  n,  where  ~,  is 
a  regular  expression  over  E. 

Method.  The  method  is  reminiscent  of  solving  linear  equations  using 
Gaussfan  elimination. 

Step  1"  Let  i  =  1. 
Step 2"  If  i  =  n,  go  to  step  4.  Otherwise,  using  the  identities  of  Lemma 
2.1,  write the  equation  for  Xt as  X~ =  ~Xt  --k fl,  where  ~ is  a  regular  expres- 

sion over 1~ and fl is a  regular expression  of the form flo+fl~X~+~ + . . .   +fl,X,, with  each  fit a  regular  expression  over  I:.  We  shall  see that  this  will  always 

be  possible.  Then  in  the  equations  for  X~+~,...,  X,,  we  replace  Xt  on  the 
right  by the  regular  expression  0~*fl. 

Step  3"  Increase  i by  1 and  return  to  step  2. 
Step  4"  After  executing step  2 the equation  for Xt will have  only  symbols 
in  E  and  X t , . . . ,   X,  on  the  right.  In  particular,  the  equation  for  X n will 

have  only X,  and  symbols  in  E  on  the right.  At  this  point  i =  n  and  we now 
go  to  step  5. 

Step  5"  The  equation  for  X~ is  of the  form  Xt  =  ~zX~ --k fl,  where  ~  and 
/~ are regular expressions  over Z. Emit the statement  X~ =  e*fl  and  substitute 
e*,t/for  X~ in  the  remaining  equations. 

Step  6"  If i  =  1, end.  Otherwise,  decrease  i by  1 and return to  step  5.  [-7 

SEC. 2.2  REGULAR SETS, THEIR GENERATORS~ AND  THEIR RECOGNIZERS  107 

Example  2.9 

Let  A  =  {X~, X2, X3}, and  let  the  set  of equations  be 

(2.2.4)  X1----OX" 2 -t-- 1X1  +  e 
(2.2.5)  X2  =  OX3 -+-  1X2 
(2.2.6)  X 3 --OXt  +  1X a 

From  (2.2.4)  we  obtain  X1  =  1X1  +  (0X2  +  e).  We  then  replace  Xt  by 

l*(0X2  +  e)  in  the  remaining  equations.  Equation  (2.2.6)  becomes 

X 3 --01*(OX 2 +  e)  q-  1X 3, 
which  can  be  written,  using  Lemma  2.1,  as 
(2.2.7)  X 3 = 0 1 * 0 X 2 + l X   3 + 0 1 .  

If we now  work  on  (2.2.5),  which  was  not  changed  by  the  previous  step, 
we  replace  X 2 by  I*0X 3, in  (2.2.7),  and  obtain 

(2.2.8)  X 3 =  (01"01"0  _jr_ 1)X3 q-- 01" 

We  now  reach  step  5  of Algorithm  2.1.  From  Eq.  (2.2.8)  we  obtain  the 
solution  for  X3" 

(2.2.9)  X 3 =  (01"01"0  --c- 1)*01" 
We  substitute  (2.2.9)  in  (2.2.5),  to  yield 
(2.2.10)  X2  =  0(01"01"0  +  1)*01"  +  1X2 

Since  X 3 does  not  appear  in  (2.2.4),  that  equation  is  not  modified.  We  then solve  (2.2.10),  obtaining 

(2.2.11)  X2  --  1 *0(01 *01 *0  +  1)*01 * 

Substituting  (2.2.11)  into  (2.2.4),  we  obtain 
(2.2.12)  X t  =  01"0(01 *01"0  +  1)*01 *  q-  1X 1 q-- e 
The  solution  to  (2.2.12)  is 

(2.2.13)  X,  =  1"(01"0(01"01"0  .+  1)*01"  -]- e) 
The  output  of Algorithm  2.1  is  the  set  of  Eq.  (2.2.9),  (2.2.11),  and  (2.2.13). 

D 

108  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

We  must  show  that  the  output  of  Algorithm  2.1  is  truly  a  solution  to 
the  equations,  in  the  sense  that  when  the  solutions  are  substituted  for  the 
indeterminates,  the  sets  denoted  by both  sides  of each equation  are the  same. 
As  we  have  pointed  out,  the  solution  to  a  set  of  standard  form  equations  is 
not  always unique.  However,  when  a  set of equations  does  not  have  a  unique 
solution  we  shall  see that  Algorithm  2.1  yields the  minimal  fixed point. 

DEFINITION 

Let  Q  be  a  set  of standard  form  equations  over  A  with  coefficients  over 
~.  We  say that  a  mapping f  from  A  to  languages  in  E*  is  a  solution  to  Q  if 
upon  substituting f ( X )   for  X  in  each  equation,  for  all  X  in  A, the  equations 
become  set  equalities.  We  say  that f :   A  --,  6'(E*)  is  a  minimalfixed  point  of 

Q  i f f i s   a  solution,  and  if g is any other  solution, f( X)   ~  g(X)  for  all  X in  A. 

The  following  two  lemmas  provide  useful  information  about  minimal 
fixed  points. 

LEMMA 2.2 

Every  set  Q  of  standard  form  equations  over  A  has  a  unique  minimal 
fixed  point. 

Proof  Let f ( X ) =   {w l for  all  solutions  g  to  Q,  w  is  in  g(X)},  for  all  X 
in A.  It is  straightforward  to  show  that f  is  a  solution  and  that f ( X )   ~  g(X) 

for  all solutions  g.  Thus, f  is the  unique  minimal  fixed  point  of  Q.  E] 

We  shall  now  characterize  the  minimal  fixed  point  of  a  set  of equations. 
LEPTA 2.3 

Let  Q  be a set of  standard form equations  over A, where  A =  [ X x , . . .   ,X,} 
and  the  equation  for  each  X~ is 

Then  the  minimal  fixed  point  of  Q  is f,  where 

f(X~)   =  [wa  ...  w~  l for  some  sequence  of integers j~ . . . .   ,Jm, m  ~  1, 

W,  is  in  0cj,0, and  w k  is  in  txj~j~/,, 1 <  k  <  m,  where j~  =  i}. 

Proof.  It  is  straightforward  to  show  that  the  following  set  equations  are 
valid: 

f ( X , ) - -   O~,o U  oc, l f ( X  1)  U  .. .   W  o~o,f(X,,) 

for  all  i. Thus, f  is  a  solution. 

To  show  that f  is  the  minimal  fixed  point,  suppose  that  g  is  a  solution 
and  that  for  some  i  there  exists  a  string  w  in f(X~)  --  g(X~).  Since  w  is  in f(X~), 

we  can  write  w - -   wl  - "   win, where  for  some  sequence  of  integers 
j l , . . . , j m ,   we  have  Wm  in  OC:~o, Wk  in  0~j~:~/~, for  1 ~  k  <  m,  and  Jt  =  i. 

SEC. 2.2  REGULAR SETS, THEIR GENERATORS, AND THEIR RECOGNIZERS  109 
Since g  is  a  solution,  we  have  g(Xj)  =  ~jo  U  ajlg(X1)  (_)  "'"  U  ajng(Xn)  for 
all j.  In  particular,  a j0  ~  g(Xj)  and  aj~g(Xk)  ~  g(Xj)  for  all j  and  k.  Thus, 
Wm is in g(Xj.),  Win-~Wm is in g(Xj._,),  and  so  forth.  Finally,  w =  w~w2 -..  Wm 
is  in  g(Xi, )  =  g(X,).  But  then  we  have  a  contradiction,  since  we  supposed 

that  w was  not  in  g(X3.  Thus  we  can  conclude  that  f ( X 3  ~  g( X 3  for  all  i. 

It  immediately  follows  that f  is  the  minimal  fixed  point  of  Q.  [~] 

LEMMA  2.4 

Let  Q1  and  Q2  be  the  set  of  equations  before  and  after  a  single  applica- 
tion  of  step  2  of  Algorithm  2.1.  Then  Q1  and  Q2  have  the  same  minimal 

fixed  point. 

Proof.  Suppose that in  step  2  the  equation 

At  ---  O~iO -~-  O~tiAi  .q_ ~i,i+lAi+l  _ql_ ...  _~_ ~i,,A, ' 
is  under  consideration.  (Note:  The  coefficient  for  A  h  is  ~  for  1 ~  h  <  i.) 

In  Q1  and  Q2  the  equations  for  Ah,  h  ~  i,  are  the  same. 

Suppose  that 

(2.2.14)  Aj  =  ~jo  -+"  ~]  ~jkAk k=l 

is the  equation  for  A~, j  >  i, in  Q1.  In  Q2 the  equation  for  Aj  becomes 
(2.2.15)  Aj  --  flo  +  ~  ,OkA~ k=i+i 
where 

~ytO~u ~i0 
Pk  ~jk  +  *  for  i  <  k  <  n 

We  can  use  Lemma  2.3  to  express  the  minimal  fixed  points  of  Q1  and 
Q2,  which  we  shall  denote  by fl  and f2,  respectively.  From  the  form  of  Eq. 
(2.2.15),  every  string  in  f2(A~)  is  in  fl(A j).  This  follows  from  the  fact  that 

any  string  w  which  is  in  the  set  denoted  by  * * j~,0ci~  can  be  expressed  as 

WlW2  .."  Wm,  where  wl  is  in  ~ji,  wm is  in  ~ik,  and  w2 . . . .   ,  Wm-1  are  in  ~,. 
Thus,  w is  the  concatenation  of  a  sequence  of  strings  in  the  sets  denoted  by 
coefficients  of  Q1,  for  which  the  subscripts  satisfy  the  condition  of  Lemma 

2.3.  A  similar  observation  holds  for  strings  in  o~j~.*o%. Thus  it can be  shown 
that f2(A~)  ~  f~(Aj). 

Conversely,  suppose  w  is  in  fl( A j).  Then  by  Lemma  2.3  we  may  write 
w  =  W l . . ' W m ,   for  some  sequence  of  nonzero  subscripts  ll,  . . . .   lm  such 
that  Wm  is  in  0ct.0, wp  is  in  ~I~1 ....  1 ~  p  <  m,  and  11  =  j.  We  can  group  the 
w/s  uniquely,  such  that  we  can  write  w  :  Yl  "'"  Y,,  where  yp  :  w,  . . .   w,, 
and 

1 10  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

(1)  If 1, ~  i, then  s  =  t  +  1. 

(2)  If l,  >  i, then  s  is  chosen  such  that  l ,+ ~, ... ,  l,  are  all  i and  l,+~ =/= i. 

It follows that in either case, yp is in the coefficient of A j,/, in the equation 
of  Q2  for  A h,  and  hence  w  is  in  fz(Aj).  We  conclude  that  f~(A~)=  f2(Aj) 
for  all j.  [S] 

LE~VnV~A 2.5 

Let  Q~  and  Q2  be  the  sets  of  equations  before  and  after  a  single  appli- 
cation  of  step  5 in  Algorithm  2.1.  Then  Q~  and  Q2  have  the  same  minimal 
fixed points. 

Proof.  Exercise,  similar  to  Lemma  2.4. 
THEOREM 2.1 

Algorithm  2.1  correctly  determines  the  minimal  fixed  point  of  a  set  of 
standard  form  equations. 

Proof.  After  step  5  has  been  applied  for  all  j,  the  equations  are  all  of 
the  form  A,. =  a~,  where  at  is  a  regular  expression  over  Z.  The  minimal 

fixed point  of such  a  set is clearly f(A,)  =  ~. 

2.2.2.  Regular  Sets and  Right-Linear  Grammars 

We  shall  show  that  a  language  is  defined  by  a  right-linear  grammar  if 
and  only  if it  is  a  regular  set.  A  few  observations  are  needed  to  show  that 
every regular  set has  a right-linear  grammar.  Let ~  be a  finite alphabet. 

LEMMA 2.6 

(i)  ~ ,   (ii)  [e}, and  (iii)  {a} for  all  a  in  I~ are  right-linear  languages. 
Proof. 

(i)  G  =  ({S}, E,  ~ ,   S)  is  a  right-linear  grammar  such  that  L(G)  =  rE. 
(ii)  G =   ({S},Z, {S--~  e},S)  is  a  right-linear  grammar  for  which L(G) 

=  [e}. (iii)  G,  =  ([S}, l~, {S --~ a}, S)  is  a  right-linear  grammar  for  which 

L(G,)  =  [a}.  [Z 
LEMMA 2.7 

If L t  and Lz  are  right-linear  languages,  then  so  are  (i) L~  U Lz,  (ii) L~L2, 
and  (iii) L~*. 

Proof.  Since  L~  and  L 2 are  right-linear,  we  can  assume  that  there  exist 
right-linear  grammars  Gt  =  (N 1, ~,  P~,  S~)  and  G2 =  (N2,  X, P2,  $2)  such 

that L(Gt)  =  Lt  and L(G2)  =  L2.  We  shall  also  assume  that  N 1 and  N 2 are 

disjoint.  That  is,  we  know  we  can  rename  nonterminals  arbitrarily,  so  this 
assumption  is without  loss  of generality. 

(i)  Let  G3  be  the  right-linear  grammar 

(N,  u  u  {s,},  x,  P,  v  v  {s,  s,  l&},  s,), 

SEC. 2.2  REGULAR  SETS,  THEIR  GENERATORS,  AND  THEIR  RECOGNIZERS  1 1 1 
where S a is a new nonterminal symbol not in N~  or N 2.  It should be clear that 

+ L(G3)  =  L(G1)  U  L(G2)  because  for  each  derivation  $3 =-~ w  there  is  either 

+  +  Ga a  derivation  S~ =-~ w  or  $2 =-~ w  and  conversely.  Since  G 3 is  a  right-linear 
Gt  G2 grammar,  L(G3)  is  a  right-linear  language. 

(ii)  Let  G4  be  the  right-linear  grammar  (N1  U  N~,  X, P4, St)  in  which 
P4  is  defined  as  follows: 

(1)  If A  ~  x B   is  in  P~, then  A  ~  x B   is  in  P4. 
(2)  If A  --~ x  is  in  P1,  then  A  ~  xS2  is  in  P4. 
(3)  All  productions  in  P2  are  in  P4. 

+  +  +  + Note  that  if  S~  =~  w,  then  $1  ~  wSz,  and  that  if  $2  ~  x,  then  S z - ~   x. 

G1  G4  G~  G4 -4- 
Thus,  L(G1)L(G2)  ~  L(G4).  Now  suppose  that  $1  ~  w.  Since  there  are  no 

G4 productions  of  the  form  A  ---~ x  in  P4  that  "came  out  of"  P~  we  can  write 

+  + the  derivation in the  form  S 1 ~  x S  z =-> xy,  where  w  =  x y  and  all  produc- 
G4  G4 + 
tions  used  in  the  derivation  S1  =-~ x S  z  arose  from  rules  (1)  and  (2)  of  the 

construction  of P4.  Thus  we  must  have  the  derivations  S~ =~  x  and  S  ~  y. 

Gt  G2 Hence,  L(G4)  ~  L(Gi)L(G2).  It  thus  follows  that  L(G4)  =  L(Gt)L(G2). 

(iii)  Let  G5  =  (Nt  u  ($5),  X, Ps,  $5)  such  that  $5  is  not  in  N~  and  P5 
is  constructed  as  follows" 

(1)  If A  ~  x B   is  in  P~,  then  A  ---~ x B   is in  Ps. 
(2)  If A  ~  x  is  in  P1,  then  A  ~  xS~  and  A  ~  x  are  in  P5- 

(3)  $5  ~  S a l e   are  in  Ps- 

+  +  +  +  + A  proof  that  $5  ==~ x~S5  ==> x~xzS5  =~  . . .   ==~ x t x   z  . . .   x,,_~S5  =* 

Gn  G5  Gn  Gs  175 

+  +  + XlX z  . . .   X,,xX .  if and  only  if S 1 ~  x~, S~  ==~ xz . . . .   , S1 = *   x.  is  left  for  the 

G1  G1  Gl Exercises. 

From  the  above,  we  have L(Gs)  =  (L(G1))*.  [~] 
We  can  now  equate  the  class  of  right-linear  languages  with  the  class  of 
regular  sets. 

THEOREM 2.2 

A  language  is  a  regular  set  if  and  only  if  it  is  a  right-linear  language. 
Proof. 

Only  if." This  portion  follows  from  Lemmas  2.6  and  2.7  and  induction 
on  the  number  of  applications  of  the  definition  of  regular  set  necessary  to 
show  a  particular  regular  set  to  be  one. 

If:  Let G =  (N,  E, P,  S) be a right-linear grammar  with  N  =  (A l , . . . ,   A,]. 
We  can  construct  a  set  of  regular  expression  equations  in  standard  form 
with  the  nonterminals  in  N  as  indeterminates.  The  equation  for  A t  is 

112  ELEMENTS OF LANGUAGE THEORY  CHAP. 2 
A t =  0%  +  ~tlA1  + . . .   +  0c,.,A,, where 

(1)  0%  =  wl  + ' "   +  wk,  where  At--*  w i t . . .   I wk  are  all  productions 
with Ai on the  left and  only terminals  on the right.  If k  --  0,  take ~o  to  be  ~ .  

(2)  0q.j, j  >  0,  is  xl  + . . .   +  Xm,  where  At---,  xlA~l.."  J x,,,Aj  are  all 
productions  with At on the  left  and a right  side  ending  in Aj.  Again, if m =  0, 
then  ~tj  =  ~ .  

Using  Lemma  2.3,  it  is  straightforward  to  show  that  L(G) is f(S),  where 
f  is  the  minimal  fixed point  of the  constructed  set  of equations.  This  portion 

of  the  proof  is  left  for  the  Exercises.  But f(S)  is  a  language  with  a  regular 
expression,  as  constructed  by Algorithm  2.1.  Thus,  L(G) is a  regular  set. 

Example  2.10 

Let  G  be  defined  by  the  productions 

S  > OAI1SIe 
A  > 0BI1A 

B  > 0 S] IB  

Then  the  set  of  equations  generated  is  that  of  Example  2.9,  with  S,  A, 
and  B,  respectively,  identified  with  X1,  X2,  and  X 3.  In  fact,  L(G) is the  set  of 

strings  whose  number  of O's is  divisible  by  3.  It  is not  hard  to  show  that  this 

set  is  denoted  by  the  regular  expression  of (2.2.13).  [~] 

2.2.3.  Finite  Automata 

We  have  seen  three  ways  of defining  the  class  of regular  sets: 
(1)  The  class  of  regular  sets  is  the  least  class  of  languages  containing 
~ ,   [e},  and  {a}  for  all  symbols  a  and  closed  under  union,  concatenation, 
and  *. 

(2)  The  regular  sets  are  those  sets  defined  by  regular  expressions. 

(3)  The regular  sets are the languages generated by right-linear grammars. 

We  shall  now  consider  a  fourth  way,  as  the  sets  defined  by  finite  automata. 

A  finite  automaton  is  one  of the  simplest  recognizers.  Its  "infinite"  memory 
is  null.  Ordinarily,  the  finite  automaton  consists  only  of  an  input  tape  and 

a  finite  control.  Here,  we shall allow the  finite control  to  be nondeterministic, 
but  restrict  the  input  head  to  be  one  way.  In  fact,  we  require  that  the  input 
head  shift  right  on  every move.'t  The  two-way finite  automaton  is considered 
in  the  Exercises. 

tRecall that,  by definition, a one-way recognizer does not shift its input head left but may keep it stationary during a move. Allowing a finite automaton to keep its input head 
stationary does not permit the finite automaton to recognize any language not recognizable by a conventional finite automaton. 

SEC. 2.2  REGULAR SETS, THEIR GENERATORS, AND THEIR RECOGNIZERS  113 

We  specify  a  finite automaton  by  defining  its  finite  set  of control  states, 
the  allowable  input  symbols,  the  initial  state,  and  the  set  of  final  states, 

i.e., the  states  which  indicate  acceptance  of the  input.  There  is a  state  transi- 

tion  function  which,  given  the  "current"  state  and  "current"  input  symbol, 

gives  all  possible  next  states.  It  should  be  emphasized  that  the  device  is 

nondeterministic  in  an  automaton-theoretic  sense.  That  is,  the  device  goes 

to  all  its  next  states,  if  you  will,  replicating  itself  in  such  a  way  that  one 
instance  of itself exists  for  each  of its possible  next  states.  The  device  accepts 

if  any  of  its  parallel  existences  reaches  an  accepting  state. The  nondeterminism  of  the  finite  automaton  should  not  be  confused 

with  "randomness,"  in  which  the  automaton  could  randomly  choose  a  next 

state  according  to  fixed  probabilities  but  had  a  single  existence.  Such  an 

automaton  is  called  "probabilistic"  and  will  not  be  studied  here. We  now  give  a  formal  definition  of  nondeterministic  finite  automa- 

ton. 

DEFINITION 

A  nondeterministic  finite  automaton  is  a  5-tuple  M  =  (Q,  %, d~, q0, F), 
where 

(1)  Q  is  a  finite  set  of states; (2)  % is  a  finite  set  of permissible  input symbols; 

(3)  6  is  a  mapping  from  Q  x  % to  6~(Q) which  dictates  the  behavior  of 
the  finite  state  control;  6  is  sometimes  called  the  state  transition  function; 

(4)  qo in  Q  is the  initial state  of the  finite  state  control;  and 

(5)  F  ~  Q  is  the  set  of final states. 

A  finite  automaton  operates  by  making  a  sequence  of moves.  A  move  is 
determined  by  the  current  state  of  the  finite  control  and  the  input  symbol 

currently  scanned  by  the  input  head.  A  move  itself  consists  of  the  control 
changing  state  and  the  input  head  shifting  one  square  to  the  right. 

To  determine  the  future  behavior  of  a  finite  automaton,  all  we  need  to 

know  are 

(1)  The  current  state  of the  finite  control  and (2)  The  string  of  symbols  on  the  input  tape  consisting  of  the  symbol 

under  the  input  head  followed  by  all  symbols  to  the  right  of  this  symbol. 

These  two  items  of  information  provide  an  instantaneous  description  of 

the  finite  automaton,  which  we  shall  call  a  configuration. 

DEFINITION 

If  M  =  (Q,%,~,  qo, F)  is  a  finite  automaton,  then  a  pair  (q, w) i n  
Q  x  %*  is  a  configuration  of  M.  A  configuration  of  the  form  (q0,  w)  is 
called  an  initial configuration,  and  one  of the  form  (q, e),  where  q  is in  F,  is 
called  a final (or  accepting) configuration. A  move by  M  is  represented  by  a  binary  relation  q--~- (or  }--, where  M  is 

1 14  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 
understood)  on  configurations.  If  6(q, a) contains  q',  then  (q, aw) ~  (q',  w) 
for  all  w in  E*. 

This  says that  if M  is in  state  q  and  the  input  head  is  scanning  the  input 
symbol  a,  then  M  may make  a  move in  which  it  goes  into  state q'  and  shifts 
the  input  head  one  square  to  the  right.  Since  M  is  in  general  nondetermin- 

istic,  there  may be  states  other than q' which it could  also  enter  on  one move. 

We  say  that  C [.~- C' if and  only if  C  =  C'.  We say that  C O [-~- Ck, k  ~  1, 
if and  only  if there  exist  configurations  C 1 , . . . ,   Ck_ 1  such  that  Ct [-~- Ct+~, 

for  all  i,  0  <  i  <  k.  CI-~- C'  means  that  C I- ~- C'  for  some  k  ~  1,  and 

C[-~- C' means that  C [.~- C' for some k ~  0. Thus,  1--~. and  !~- are, respectively, 
the  transitive  and  reflexive-transitive  closure  of  l~-.  We  shall  drop  the  sub- 

script  M  if no  ambiguity  arises. 

We  shall  say  that  an  input  string  w  is  accepted by  M  if  (qo,  w)1.-~-- (q, e) 
for  some q in F.  The  language defined by M, denoted L(M), is the  set of input 

strings  accepted  by  M,  that  is, 

[wlw  ~  ~*  and  (q0,  w)1.-~-(q, e)  for  some  q in  F}. 

We  shall  now  give  two  examples  of  finite  automata.  The  first  is  a  simple 
"deterministic"  automaton;  the  second  shows  the  use  of  nondeterminism. 

Example 2.11 

Let  M  =  (~p,q,  r}, [0,  1}, $,p,  {r~}) be  a  finite  automaton,  where  $  is 
specified  as  follows" 

Input 
$  0  1 

State  p 

q 

r  Jr]  ~r} 

M  accepts  all  strings  of O's and  l's  which  have  two  consecutive  O's. That 
is,  state p  is  the  initial  state  and  can  be  interpreted  as  "Two  consecutive  O's 

have  not  yet appeared,  and  the  previous  symbol  was  not  a  0."  State q means 

"Two  consecutive  O's have  not  appeared,  but  the  previous  symbol  was  a  0." 
State  r  means  "Two  consecutive  O's  have  appeared."  Note  that  once  state  r 
is entered,  M  remains  in that  state. 

On  input  01001,  the  only  possible  sequence  of configurations,  beginning 
with  the  initial  configuration  (p, 01001),  is 

(p, 01001)  [-- (q,  1'001) 

(p, ool) 

(q, 01) 

SEC. 2.2  REGULAR  SETS, THEIR GENERATORS, AND  THEIR RECOGNIZERS  1 15 
Thus,  01001  is  in L(M).  E] 

~-  (r,  1) 
[-  (r, e) 

Example  2.12 

Let  us  design  a  nondeterministic  finite  automaton  to  accept  the  set  of 
strings  in  {1, 2,  3} such  that  the  last  symbol  in  the  input  string  also  appears 
previously  in  the  string.  That  is,  121  is  accepted,  but  31312  is  not.  We  shall 
have  a  state  q0,  which  represents  the  idea  that  no  attempt  has  been  made  to 
recognize  anything.  In  this  state  the  automaton  (or  that  existence  of  it, 

anyway)  is  "coasting  in  neutral."  We  shall  have  states  q t,  q2,  and  q3, which 
represent  the  idea  that  a  "guess"  has  been  made  that  the  last  symbol  of  the 
string  is  the  subscript  of the  state.  We  have  one  final  state,  qs"  In  addition 
to  remaining  in  qo,  the  automaton  can  go  to  state  q= if  a  is  the  next  input. 

If (an existence  of) the  automaton  is in  qa, it can  go to  qs if it sees another  a. 
The automaton  goes  no  place from  qs,  since the  question  of acceptance  must be  decided  anew  as  each  symbol  on  its  input  tape  becomes  the  "last."  We 

specify  M  formally  as 

M  =  ([qo, qa, q2, q3, qs}, [1, 2,  3}, ~, qo, {qt'}) 
where  ~ is  given  by the  following  table" 

State  q0 ql 

q2 q3 
qf 

Input 
1  2  3 

(qo,ql}  {qo,q2}  [qo,q3} {ql,q:}  (qa}  {ql] 
[q2}  {q2, qf}  {q2} {q3}  {q3}  {q3,qf} 

On  input  12321,  the  proliferation  of  configurations  is  shown  in  the  tree 
in  Fig.  2.2. 

~.q3, 21)--(q3, 1)---(q3, e) tq2,321)--(q2,  21)x-(q2, 1)--(q2, e) 

"\qf , 1) k q x ,   2321)~(q1,321)~(qx,  21)~(q'a, 1)X--(ql, e) 

\"t.qf, e) 
Fig.  2.2  Configurations  of  M. 

1 16  ELEMENTS OF LANGUAGE THEORY  CHAP. 2 

Since (q0, 12321)l-~-- (qs, e), the  string  12321 is in L(M). Note  that  certain configurations  are repeated  in  Fig.  2.2,  and  for this  reason  a  directed  acyclic 
graph might be a  more  suitable representation  for the  configurations  entered b y M .   E] 

It is often convenient  to  use  a  pictorial representation  of finite automata. 
DEFINITION 

Let  M  =  (Q,  E, ~, q0, F)  be  a  nondeterministic  finite  automaton.  The transition graph for  M  is  an  unordered  labeled  graph  where 'the  nodes  of 

the  graph  are  labeled  by  the  names  of the  states  and  there  is  an  edge  (p, q) 
if  there  exists  an  a  ~  E  such  that  ~(p,  a)  contains  q.  Further,  we  label the 

edge  (p, q)  by  the  list  of a  such  that  q  ~  ~(p, a).  Thus  the  transition  graphs for  the  automata  of Examples  2.11  and  2.12  are  shown  in  Fig.  2.3.  We  have 

1  0 
Start  0  _ ~ 0 , 1  

(a) Example 2.11 

1, 2, 3 

1,2, l  1  1 
Start  -  ) 

(b) Example 2.12 
Fig. 2.3  Transition graphs. 

indicated  the  start  state  by pointing  to  it  with  an  arrow  labeled  "start,"  and final  states  have  been  circled. 

We  shall  define  a  deterministic  finite  automaton  as  a  special  case  of the nondeterministic  variety. 
DEFINITION 

Let  M  =  (Q,  E,  ~, q0, F)  be a  nondeterministic  finite  automaton.  We  say that  M  is  deterministic if ~(q, a)  has  no  more  than  one  member  for  any q  in 

SEC.  2.2  REGULAR SETS, THEIR  GENERATORS,  AND  THEIR  RECOGNIZERS  1 17 

Q  and  a  in  E.  If  O(q, a)  always  has  exactly  one  member,  we  say  that  M  is 
completely  specified. 

Thus  the  automaton  of  Example  2.11  is  a  completely  specified  determin- 
istic  finite  automaton.  We  shall  hereafter  reserve  the  term  finite  automaton 

for  a  completely  specified  deterministic  finite  automaton. 

One  of  the  most  important  results  from  the  theory  of  finite  automata  is 
that  the  classes  of  languages  defined  by  nondeterministic  finite  automata 

and  completely  specified  deterministic  finite  automata  are  identical.  We  shall 
prove  the  result  now. 

CONVENTION 

Since  we  shall  be  dealing  primarily  with  deterministic  finite  automata, 
we  shall  write  "6(q,  a ) =   p"  instead  of  "6(q,  a ) =   {p}"  when  the  automaton 
with  transition  function  6  is  deterministic.  If  O(q, a ) =   ~ ,   we  shall  often 

say  that  6(q,  a)  is  "undefined." 

THEOREM 2.3 

If  L - - L ( M )   for  some  nondeterministic  finite  automaton  M,  then L  --  L(M') 

for  some  finite  automaton  M'. 

Proof  Let  M - -   (Q,  E,  6, qo,  F).  We  construct  M'  =  (Q',  E,  6', q0,  F'), 
as  follows" 

(1)  Q'  --  (p(Q).  Thus  the  states  of  M'  are  sets  of  states  of  M. 

(2)  qo  =  {q0}. (3)  F '   consists  of  all  subsets  S  of  Q  such  that  S  ~  F  =/=  ~ .  

(4)  For  all  S  ~  Q,  6'(S,  a ) =   s',  where  S ' =   {pl0(q,  a)  contains  p  for 
some  q  in  S}. 

It  is  left  for  the  Exercises  to  prove  the  following  statement  by  induction 
o n / '  

(2.2.16)  (S,  w)l-~  (S',  e)  if and  only  if 

S'  =  {Pl(q,  w)  l~- (P,  e)  for  some  q  in  S} 

As  a  special  case  of  (2.2.16),  ({qo}, w)I-~  (S',  e)  for  some  S'  in  F'  if  and 
only  if (q0  w)I .A-. (p  e)  for  some  p  in  F.  Thus,  L(M')  =  L(M) '  M  ' 

Example  2.13 

Let  us  construct  a  finite  automaton  M'  =  (Q,  { 1, 2,  3}, ,6', (q0}, F)  accept- 
ing  the  language  of  M  in  Example  2.12.  Since  M  has  5  states,  it  seems  that 

M'  has  32  states.  However,  not  all  of  these  are  accessible  from  the  initial 
state.  That  is,  we  call  a  state  p  accessible  if  there  is  a  w  such  that 
(q0,  w)I*  (P, e),  where  q0  is the  initial  state.  Here,  we  shall  construct  only the 

accessible  states. 

We  begin  by  observing  that  {q0} is  accessible.  6'((q0},  a ) =   (q0, q~}  for 
a  =  1,  2,  and  3.  Let  us  consider  the  state  {q0, ql}.  We  have  6'((q0,  qx}, 1) = 

1 18  ELEMENTS OF LANGUAGE  THEORY  CHAP. 2 

Input 
1  2  3 

State  A  =  {q0} 

B ={q o,  qx} 

C={ qo,q2]  
D  =  [qo, q3} 

E  =  [qo, q l,qf} 

F  =  {qo, qI,q2} 
G  ={qo,  qx,q3} 
H  =  [q0, q2, qf} 

1  =  [qo,qz,  q3} 
J  =  {qo, q 3, qf} 
K  =  {qo, ql,qz,  qy} 

L  =  [qo, ql,q2,q3} 
M  =  {qo, ql,q3,  qy} 

N  ={qo ,q2,q 3,  qf} 

P  =  {qo,ql,q2,  q3,qf} 

B  C  D 

E  F  G 
F  H  I 

G  I  J 
E  F  G 

K  K  L 
M  L  M 
F  H  I 
L  N  N 

G  I  J 
K  K  L 
P  P  P 

M  L  M 
L  N  N 
P  P  P 

Fig.  2.4  Transition  function  of M'. 

{qo,  ql,  qr}" Proceeding  in  this  way,  we  find  that  a  set  of  states  of  M  is  acces- sible  if  and  only  if- 

(1)  It  contains  qo,  and (2)  If  it  contains  @,  then  it  also  contains  ql,  q~,  or  q3. 

The  complete  set  of  accessible  states,  together  with  the  8'  function,  is given  in  Fig.  2.4. 
The  initial  state  of  M'  is  A,  and  the  set  of final  states  consists  of E,  H,  .1, K,  M,  N,  and  P. 

2.2.4.  Finite  Automata  and  Regular  Sets 

We  shall  show  that  a  language  is  t  regular  set  if  and  only  if  it  is  defined by  a  finite  automaton.  The  method  i;  first  to  show  that  a  finite  automaton 

language  is  defined  by  a  right-linear  ~ :ammar.  Then  we  show  that  the  finite automaton  languages  include  ~ ,   (e}  {a}  for  all  symbols  a,  and  are  closed 

under  union,  concatenation,  and  *. Ti  us  every  regular  set  is  a  finite  automa- ton  language.  The  following  sequence  of  lemmas  proves  these  assertions. 

LEMMA  2.8 

If  L  =  L(M)  for  finite  automaton  M,  then  L  =  L(G)  for  some  right- linear  grammar  G. 

Proof.  Let  M  =  (Q,  E,  8,  qo,  F)  (M  is  deterministic,  of  course).  We  let G'  =  (Q,  ~, P,  qo),  where  P  is  defined  as  follows: 

(1)  If  d~(q, a)  =  r,  then  P  contains  the  production  q  ~  at. (2)  If p  is  in  F,  then p  ---~ e  is  a  production  in P. 

SEC. 2.2  REGULAR  SETS, THEIR  GENERATORS, AND  THEIR  RECOGNIZERS  'l 19 

We  can  show  that  each  step  of  a  derivation  in  G  mimics  a  move  by  M. 
We  shall  prove  by  induction  on  i  that 

(2.2.17) 

i+1 For  q  in  Q,  q ~  w if and  only  if (q,  w)  [_L_ (r,  e)  for  some  r  in  F 

Basis.  For  i  =  0,  clearly  q =~  e  if  and  only  if  (q,  e)  [.-.~- (q,  e)  for  q  in  F. 
Inductive  Step.  Assume  that  (2.2.17)  is  true  for  i  and  let  w  =  ax,  where 

i+1  i [xl  =  i. Then q ~  w if and  only ifq  ==~ as ~  ax for some s  6  Q. But q =~ as 

i if and  only  if O(q, a)  =  s.  From  the  inductive  hypothesis,  s  ==~ x  if and  only 
i+1 if (s, x) I'- 1 (r,  e) for  some r  ~ F. Therefore,  q ~  w if and only if (q,  w)  l-L- (r, e) 

for  some  r  ~  F.  Thus  Eq.  (2.2.17)  is  true  for  all  i  >  0. 

+ We  now  have  q0  =~  w  if  and  only  if  (q0,  w)I .-~- (r,  e)  for  some  r  ~  F. 

Thus,  L(G)  =  L(M).  E] 

LEMMA  2.9 

Let  X  be  a  finite  alphabet.  (i)  ~ ,   (ii)  {e}, and  (iii)  {a} for  a  ~  X  are  finite 
automaton  languages. 

Proof. 

(i)  Any  finite  automaton  with  an  empty  set  of  final  states  accepts  ~ .  
(ii)  Let  M  =  ({q0}, X, ~5, qo, [q0}),  where  6(qo,  a)  is  undefined  for  all  a 
in  X.  Then  L ( M ) =   {e}. 

(iii)  Let  M  =  ({q0,  ql}, X, 6, qo, [ql}),  where  ~(qo, a)  =  ql  and  6  is  un- 
defined  otherwise.  Then  L ( M ) =   [a].  E] 

LEMMA 2.10 

Let  L1  =  L(Mi)  and  L2  =  L(M2)  for  finite  automata  Mt  and  M2.  Then 
(i) L~  U  L2,  (ii) L~L2,  and  (iii) Lt*  are  finite  automaton  languages. 

Proof  Let  M~  =  (Qi,  E, 61,  qt,  El)  and  M2  =  (Q2, E, c52, q2, F2).  We assume  without  loss  of  generality  that  Q1  (q  Q2  =  ~ ,   since  states  can  be 

renamed  at  will. 

(i)  Let  M  =  (Q1  u  Q2  u  [q0}, E, c5, q0, F)  be  a  nondeterministic  finite 
automaton,  where 

(1)  qo  is  a  new  state, (2)  F = F 1   U F 2 i f e i s n o t i n L l o r L z a n d F = F   1 U F 2 U { q o } i f e  

is in L1  or L2,  and (3)  (a)  8(q0,  a)  =  ~5(ql, a)  u  ~5(q2, a)  for  all  a  in  X, 

(b)  8(q,  a)  =  6i(q,  a)  for  all  q  in  Q1,  a  in  X,  and 
(c)  cS(q, a)  =  ~2(q,  a)  for  all  q  in  Q2,  a  in  X. 
Thus,  M  guesses whether  to  simulate  M1  or M2.  Since  M  is nondeterministic, 

it  actually  does  both.  It  is  straightforward  to  show  by  induction  on  i >  1 
that  (q0,  w) l.-~- (q,  e) if and  only if qi s  in  Qi  and  (ql,  w) [~-~ (q,  e)  or qis  in  Q2 

120  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 
and  (q2,  w)1~7 (q,  e).  This  result,  together  with  the  definition  of  F,  yields L(M)  =  L(M~)  U  L(M2). 

(ii)  To  construct  a  finite  automaton  M  to  recognize  L1L 2  let 
M  =  (Q1  t.j  Q z, E,  t~, q~, F),  where  ~ is  defined  by 

(1)  ~(q, a)  =  ~l(q,  a)  for  all q  in  Q~  -  F~, 
(2)  5(q,  a)  =  ill(q,  a)  U  t~2(q2, a)  for  all q  in  F1,  and 
(3)  $(q,  a)  =  ~2(q,  a)  for  all q  in  Q2. 
Let 

F = J  Fz',  if q2  ~  Fz 

IF1  U  F2,  if q2  ~  F2 

That  is,  M  begins  by  simulating  M1.  When  M  reaches  a  final  state  of  M1, 

it  may,  nondeterministically,  imagine  that  it  is  in  the  initial  state  of  M2 
by  rule  (2).  M  will  then  simulate  M2.  Let  x  be  in  L 1 and  y  in  L 2.  Then (q~, xy)[.~- (q, y) 

for some  q  in  F~. If  x  =  e, then q  =  q l.  If y  ~  e, then,  using 
one  rule  from  (2) and  zero  or more  from  (3),  (q, y)1.~- (r,  e)  for  some  r  ~ F2. 
If y  =  e,  then  q  is  in  F,  since  q2  ~  F2.  Thus,  xy  is  in L(M).  Suppose  that  w 
is  in  L(M).  Then  (q l,  w)I-~ (q,  e)  for  some  q  ~  F.  There  are  two  cases  to 
consider  depending  on  whether  q  ~  F2  or  q  ~  F1.  Suppose  that  q  ~  /72. 

Then  we  can  write  w  =  xay  for  some  a  in  E  such  that 

O (ql,  xay)  l~- (r, ay)  I~- (s, y)  I M*~ (q, e), 

where  r  ~  Fi,  s  ~  Q2,  and  $2(q,  a)  contains  s.  Then  x  ~ L 1 and  ay  ~  L 2. 

Suppose  that  q  ~  F 1.  Then  q2  ~  F2  and  e is in  L 2.  Thus,  w  ~  L 1.  We  con- clude  that  L(M)  =  L~L z. 

(iii)  We  construct  M  =  (Q1  u  [q'~}, ~,  $, q',  F 1 u  {q'}),  where  q'  is  a  new 
state  not  in  Q1,  to  accept  L~* as  follows.  ~  is  defined  by 

(1)  O(q, a)  =  O l(q,  a)  if q  is  in  Q 1 -  F1  and  a  ~  E, 
(2)  t~(q, a)  =  ~1 (q, a)  u  ~ ~ (ql,  a)  if q  is  in  F 1 and  a  ~  E,  and 
(3)  $(q',  a)  =  ~1 (q l,  a)  for  all  a  in  E. 
Thus,  whenever  M  enters  a  final  state  of M1,  it  has  the  option  of  continuing 
to  simulate  M1  or to  begin simulating M1  anew  from the initial  state.  A  proof 
that  L ( M ) =   L~*  is  similar  to  the  proof  of  part  (ii).  Note  that  since  q'  is 

a  final  state,  e  ~  L(M).  D 

THEOREM  2.4 

A  language  is  accepted  by  a  finite  automaton  if and  only if  it  is  a  regular 
set. 

Proof  Immediate  from  Theorem  2.2  and  Lemmas  2.8,  2.9,  and  2.10.  D 

2.2.5.  Summary 

The  results  of  Section  2.2  can  be  summarized  in  the  following  theorem. 

THEOREM 2.5 

The  following  statements  are  equivalent: 

(1)  L  is  a  regular  set. 
(2)  L  is  a  right-linear  language. 

(3)  L  is  a  finite  au to m ato n   language. 
(4)  L  is  a  nondeterministic  finite  aut om ato n   language. 

(5)  L  is  denoted  by  a  regular  expression.  [ ] 

EXERCISES  1 21 

E X E R C I S E S  
2.2.1.  Which  of  the  following  are  regular  sets?  Give  regular  expressions  for 

those which  are. 
(a)  The  set  of words  with  an  equal  number  of 0's  and  l's. 

(b)  The  set  of words  in {0,  1 }* with  an  even  number  of O's  and  an  odd 

number  of  l's. 
(c)  The  set  of words  in ~*  whose length  is  divisible by  3. 
(d)  The  set  of words  in {0,  1 ]* with  no  substring  101. 

2.2.2.  Show  that  the  set  of regular  expressions  over 1~ is  a  CFL. 

2.2.3.  Show  that  if  L  is  any  regular  set,  then  there  is  an  infinity  of  regular 

expressions  denoting L. 

2.2.4.  Let  L  be  a  regular  set.  Prove  directly  from  the  definition  of  a  regular 

set  that  L R is  a  regular  set.  Hint:  Induction  on  the  number  of  applica- 
tions  of the  definition  of regular  set  used  to  show  L  to  be  regular. 

2.2.5.  Show  the  folilowing  identities  for  regular  expressions  0~, fl,  and  ~: 

(a)  ~(fl  +  7)  =  ~P  +  ~7. (b)  ~  +  (p  +  ~)  =  (~  +  p)  +  7. 

(d)  0ce --  e0~ =  0~. 
(e)  ~ = ~   = ~ .  

(f)  ~ + 0 ~ = ~ .  

(g)  (~  +  P)r  =  ~r  +  Pr. 

(h)  ~ * = e .  
(i)  0C* -t-  0C --  ~;*. (j)  (~*)*  =  ~*. 

(k)  (~  +  p)*  =  (~*p*)*. (1)  ~ + ~ = ~ .  

2.2.6.  Solve the  following  set  of regular  expression  equations: 

A1  =  (01"  +  1)A1  +  A2 

A2  =  11  +  1A1  +  00A3 

A3  = e + A 1   / A 2  

2.2.7.  Consider  the  single equation 

(2.2.18)  X  =  0CX +  fl 
where  0~ and  fl  are  regular  expressions  over ~  and  X  q~ E.  Show  that 

(a)  If e is  not  in 0c, then  X  =  0~*fl is the unique  solution  to  (2.2.18). 

(b)  If e  is  in  ~,  then  0c*fl is  the  minimal  fixed  point  of (2.2.18),  but there  are  an  infinity  of solutions. 

122  ELEMENTS OF LANGUAGE THEORY  CHAP. 2 

2.2.8. 

(c)  In  either  case,  every  solution  to  (2.2.18)  is  a  set  of  the  form 

* * ( f l u   L)  for  some  (not  necessarily regular)  language  L. 

Solve the  following  general  pair  of  standard  form  equations" 

X  =  ~1 X  -t- 0~2 Y  -b  ~3 

Y =  PI X  ~  1~2 Y  ~  ~3 

2.2.9.  Complete  the  proof  of Lemma  2.4. 
2.2.10.  Prove  Lemma  2.5. 
2.2.11.  Find  right-linear  grammars  for  those  sets  in  Exercise  2.2.1  which  are 

regular sets. 

DEFINITION 

A  grammar  G  =  (N,  E, P,  S)  is  left-linear  if every  production  in  P 
is  of the  form  A  ---~ Bw  or  A  --,  w. 

2.2.12.  Show  that  a  language  is  a  regular  set  if  and  only  if  it  has  a  left-linear 

grammar.  Hint:  Use  Exercise  2.2.4. 

DEFINITION 

A  right-linear  grammar  G  =  (N,  Z, P,  S)  is  called  a  regular  gram- 
mar  when 

(1)  All productions  with  the  possible  exception  of S  ~  e  are  of the 
form  A  ~  aB  or  A  ----~ a,  where  A  and  B  are  in  N  and  a  is  in  Z. 

(2)  If  S---~  e  is  in  P,  then  S  does  not  appear  on  the  right  of  any 
production. 

2.2.13.  Show  that  every  regular  set  has  a  regular  grammar.  Hint:  There  are 

several ways  to  do  this.  One  way  is  to  apply  a  sequence  of transforma- 
tions  to  a  right-linear  grammar  G  which  will  map  G  into  an  equivalent 
regular  grammar.  Another  way  is  to  construct  a  regular  grammar 

directly  from  a  finite  automaton. 

2.2.14.  Construct  a  regular  grammar  for  the  regular  set  generated  by  the 

right-linear  grammar 

A----~  B I C  

B - - - ~   OBI1BIOll 
C----~  ODI1 Cle 
D  ~  0Cl  1D 

2.2.15.  Provide  an  algorithm  which,  given  a  regular grammar  G  and  a  string  w, 

determines  whether  w is  in L(G). 

2.2.16.  Prove  line  (2.2.16)  in  Theorem  2.3. 

2.2.17.  Complete  the  proof  of L emma  2.7(iii). 

EXERCISES  123 
DEFINITION 

A  production  A  ~  0c of right-linear  grammar  G  =  (N,  E, P,  S)  is 
useless  if there  do  not  exist  strings  w  and  x  in  ~*  such  that 

S  =:~ wA  ==~ woc ~  wx. 
2.2.18.  Give  an  algorithm  to  convert  a  right-linear  grammar  to  an  equivalent 

one with  no  useless productions. 

"2.2.19.  Let G  =  (N,  Z, P,  S) be a right-linear grammar.  Let N  =  [A1 . . . . .   A,}, 

and define ~xtj =  xl +  x2 + . - -   +  xm,  where At  ~  xt At ... ..  At  ~  xmA# 

are  all  the  productions  of  the  form  At- -,y A# .  Also  define 
OClo =  xt  +  .. .  +  Xm,  where  At ~  xt . . . . .   At ---~ xm  are  all  produc- 
tions  of the form  At ---~ y.  Let  Q  be the  set of standard  form  equations 
At  =  ~zt0 +  tgtlA1 +  tZtzA2 +  .. .  +  tZt~A,.  Show  that  the  minimal 

fixed point  of  Q  is L(G).  Hint:  Use  Lemma  2.3. 

2.2.20.  Show  that  L(G )o f  Example  2.10  is  the  set  of  strings  in  [0,  1 }*,  whose 

length is  divisible by  3. 

2.2.21.  Find  deterministic  and  nondeterministic  finite  automata  for  those  sets 

of Exercise 2.2.1  which  are  regular. 

2.2.22.  Show  that  the  finite  automaton  of  Example  2.11  accepts  the  language 

(0  +  i)*00(0  +  1)*. 

2.2.23.  Prove  that  the  finite  automaton  of Example  2.12  accepts  the  language 

[wa[a  in  [1,  2, 3} and  w has  an  instance  of a}. 

2.2.24.  Complete the proof of Lemma  2.10(iii). 
*2.2.25.  A  two-way  finite  automaton  is  a  (nondeterministic)  finite  control  with 

an  input  head  that  can  move  either  left  or  right  or  remain  stationary. 
Show  that  a  language is accepted by  a  two-way finite  automaton if and 
only  if  it  is  a  regular  set.  Hint:  Construct  a  deterministic  one-way 
finite  automaton  which,  after  reading  input  w-~  e,  has  in  its  finite 

control  a  finite  table  which  tells,  for  each  state q  of the  two-way  auto- 
maton  in what  state,  if any,  it would  move off the right end  of w, when 

started  in  state  q  at  the  rightmost  symbol  of  w. 

*2.2.26.  Show  that  allowing  a  one-way finite  automaton  to  keep  its  input  head 

stationary does not increase the class of languages defined by the device. 

**2.2.27.  For arbitrary n, show that there is a regular set which can be recognized 

by  an  n-state  nondeterministic  finite  automaton  but  requires  2"  states 

in  any  deterministic finite  automaton  recognizing it. 

2.2.28.  Show  that  every  language  accepted  by  an  n-state  two-way  finite  auto- 

maton  is  accepted  by  a  2 "~"+ 1)-state  finite  automaton. 

**2.2.29.  How  many  different languages  over  [0,  1} are  defined by  two-state 

(a)  Nondeterministic finite automata ? 

(b)  Deterministic finite automata ? 

(c)  Finite  automata? 

124  ELEMENTS OF LANGUAGE THEORY  CHAP. 2 

DEFINITION 

A  set  S  of integers  forms  an  arithmetic  progression if we  can  write S = [ c , c + p , c / 2 p   . . . . .   c + i p  . . . .   ].  For  any  language  L,  let 

S(L)  =  {/[for some  w in L,  I wl  =  i3. 
**2.2.30.  Show  that  for  every  regular  language  L,  S(L)  is  the  union  of  a  finite number  of arithmetic progressions. 

Open  Problem 

2.2.31.  How  close  to  the  bound  of Exercise  2.2.28  for  converting  n-state  two- way  nondeterministic  finite  automata  to  k-state  finite  automata  is  it 

actually possible to come ? 

B IB L I O GR A P H IC   N OT E S  
Regular expressions were defined by Kleene [1956]. McNaughton  and  Yamada [1960]  and  Brzozowski  [1962] cover  regular  expressions  in  more  detail.  Salomaa 
[1966]  describes  two  axiom  systems  for  regular  expressions.  The  equivalence  of regular  languages  and  regular  sets  is  given  by  Chomsky  and  Miller  [1958]. The 
equivalence  of  deterministic  and  nondeterministic  finite  automata  is  given  by Rabin and  Scott [1959]. Exercise 2.2.25 is from there and from Shepherdson [1959]. 

2.3.  PROPERTIES  OF  REGULAR  SETS 

In  this  section  we  shall  derive  a  number  of  useful  facts  about  finite 
automata  and  regular  sets.  A  particularly  important  result  is  that  for  every 
regular  set  there  is  an  essentially  unique  minimum  state  finite  automaton 
that  defines  that  set. 

2.3.1.  Minimization  of  Finite  Automata 

Given  a  finite  automaton  M,  we  can  find  the  smallest  finite  automaton 
equivalent  to  M  by eliminating  all inaccessible  states  in  M  and  then  merging 

all redundant  states  in  M.  The  redundant  states  are  determined  by partition- 
ing  the  set  of  all  accessible  states  into  equivalence  classes  such  that  each 

equivalence  class  contains  indistinguishable  states  and  is  as  large  as  possible. 
We  then  choose  one  representative  from  each  equivalence  class  as  a  state  for 
the  reduced  automaton.  Thus  we  can  reduce  the  size  of  M  if  M  contains 
inaccessible  states  or  two  or  more  indistinguishable  states.  We  shall  show 
that  this  reduced  machine  is  the  smallest  finite  automaton  that  recognizes 
the  regular  set  defined  by  the  original  machine  M. 

DEFINITION 

Let  M  =  (Q, E, ~, q0, F)  be a  finite  automaton,  and  let  qt  and  qz  be  dis- 
tinct states.  We say that  x  in  E*  distinguishes  qx  from  q2  if  (qx, x)[-~-- (q3, e), 

SEC. 2.3  PROPERTIES  OF  REGULAR  SETS  125 
(q2,x)l-~-(q4,  e),  and  exactly  one  of  q3  and  q4  is  in  F.  We  say  that  ql 

k and  qz  are  k-indistinguishable,  written  qa  ~  q~,  if  and  only  if  there  is  no  x, 

with  ]xl  ~  k,  which  distinguishes  ql  from  q2.  We  say  that  two  states  ql  and 

q2 are indistinguishable,  written ql  ~  qz, if and only if they are k-indistinguish- 
able  for  all  k  _~ 0. 

A  state  q  ~  Q  is  said  to  be  inaccessible  if there  is  no  input  string  x  such 
that  (qo, x ) l +-   (q, e). 

M  is  said  to  be  reduced if no  state  in  Q is inaccessible  and  no  two  distinct 
states  of  Q  are  indistinguishable. 

Example 2.14 

Consider  the  finite  automaton  M  whose  transition  graph  is  shown  in 
Fig.  2.5. 

0 

0 S t a r t - ~  

Fig. 2.5  Transition graph of M. 
To  reduce  M  we  first  notice  that  states  F  and  G  are  inaccessible  from 
the  start  state A  and  thus  can  be removed.  We  shall  see in the  next  algorithm 

that  the  equivalence  classes  under  ~  are  {A}, {B, D},  and  {C, E}.  Thus  we 

can  represent  these  sets  by  the  states p,  q,  and  r,  respectively,  to  obtain  the 

finite  automaton  of  Fig.  2.6,  which  is  the  reduced  automaton  for  M.  Q 

LEMMA 2.11 

Let  M  =  (Q, ~,8,  q0,F)  be  a  finite  automaton  with  n  states.  States  ql 
and  qz  are indistinguishable  if and  only if they  are (n  --  2)-indistinguishable. 

Start  * ( ~ . ) - - - - - - ~ - - - - - ~ x ~ , ~ . . . . . . . . . . , ~ ~  0 ' ' - ' ' N  0.1  / ~ _ ~ _ ~ j ~ l  

1 

0 
Fig. 2.6  Reduced machine. 

126  ELEMENTS  OF  LANGUAGE  THEORY  CHAP.  2 

Proof  The  "only  if"  portion  is  trivial.  The  "if"  portion  is  trivial  if  F 
has  0  or  n  states.  Therefore,  assume  the  contrary. 

We  shall  show  that  the  following  condition  must  hold  on  the  k-indistin- 
guishability  relations" 

n-2  n-3  2  1  0 

To  see  this,  we  observe  that  for  ql  and  q2  in  Q, 

0 (1)  ql  =-- qz  if and  only  if both  q~  and  qz  are  either  in  F  or  not  in  F. 

k  k--1  k--1 
(2)  ql ~  q2 if and  only  if ql ~  q2 and  for  all  a  in  2,  6(ql,  a) ~  6(q2, a). 

0 The  equivalence  relation  =  is the  coarsest  and  partitions  Q  into  two  equiva- 

k+l  k  k+l  k lence  classes,  F  and  Q  -  F.  Then  if  --  ~  ~ ,   --   is  a  strict  refinement  o f - - ,  
k+l  k that  is,  - -  contains  at  least  one  more  equivalence  class  than  --.  Since  there 

are at  most  n  --  1  elements in  either  F  or  Q  -  F  we can  have  at  most  n  --  2 

0  k+l  k  k+l  k+2 successive  refinements  of  --.  If for  s6me  k,  _ ,   then  --  . . - ,  

k  k+l  k by  (2).  Thus,  --  is  the  first  relation  --  such  that  --  --.  [Z] 

Lemma  2.11  has  the  interesting  interpretation  that  if  two  states  can  be 
distinguished,  they  can  be  distinguished  by  an  input  sequence  of  length  less 
than  the  number  of  states  in  the  finite  automaton.  The  following  algorithm 
gives  the  details  of how  to  minimize  the  number  of states  in  a  finite  automa- 
ton. 

ALGORITHM  2.2 

Construction  of  the  canonical  finite  automaton. 
Input.  A  finite  automaton   M  --  (Q,  Z,  5,  q0,  F). 

Output.  A  reduced  equivalent  finite  au tomato n  M'. 
Method. 

Step  1."  Use  Algorithm  0.3  on  the  transition  graph  of  M  to  find  those 
states  which  are  inaccessible  from  q0.  Delete  all  inaccessible  states. 

o  1 Step  2:  Construct  the  equivalence  relations  - - , - - , . . . ,   as  outlined  in 

k+l  k  k Lemma  2.11.  Continue  until  --  _ .   Choose  --  to  be  --. 

Step  3:  Construct  the  finite  auto maton  M ' =   (Q',  E,  5',  q;,  F'),  where 
(a)  Q' is the  set of equivalence classes under  _=.  Let [p]  be the  equivalence 
class  of  state p  under  ~ .  

(b)  5'([p],  a)  =  [q]  if 6(p,  a)  =  q. 
(c)  q;  is  [q0]- 
(d)  F'  =  [[q]lq  ~  F]. 

SEC. 2.3  PROPERTIES  OF  REGULAR  SETS  127 

It  is  straightforward  to  show  that  step  3(b)  is  consistent;  i.e.,  whatever 
member  of  [p]  we  choose  we  get  the  same  equivalence  class  for  t~([p], a). 
A  proof that L(M')  =  L(M) is also  straightforward  and left for the  Exercises. 

We  prove  that  no  automaton  with  fewer  states  than  M'  accepts  L(M). 

THEOREM 2.6 

M'  of  Algorithm  2.2  has  the  smallest  number  of  states  of  any  finite 
automaton  accepting  L(M). 

Proof.  Suppose  that M "   had fewer states than  M'  and that L(M") -- L(M). 
Each  equivalence  class  under  --~  is  nonempty,  so  each  state  of  M'  is 
accessible.  Thus  there  exist  strings  w and  x  such  that  (q0',  w)1~,, (q,  e)  and 
(q0', x)[~,, (q, e),  where  qo'  is  initial  state  of  M",  but  w  and  x  take  M'  to 

different  states.  Hence,  w and  x  take  M  to  different  states,  say p  and  r,  which 

are  distinguishable.  That  is,  there  is  some  y  such  that  exactly  one  of  wy 
and  xy is in  L(M).  But  wy and  xy  must  take  M "  to  the  same  state,  namely, 
that  state  s  such  that  (q, y)[~,, (s,  e).  Thus  is  not  possible  that  exactly  one 

of  wy and  xy  is  in L(M"),  as  supposed. 

Example 2.15 

Let  us  find  a  reduced  finite  automaton  for  the  finite  automaton  M  whose 

k transition  graph  is  shown  in  Fig.  2.7.  The  equivalence  classes  for  = ,   k  >  0, 

are  as  follows" 

0 For  ~ "  

1 For  ~ "  
2 For  --" 

{A, F},  {B,  C,  D,  E} 

{A, F},  {B, E},  {C,  D} 
{A, F},  {B, E},  {C,  D}. 

Start  a 

b  a  _~  /"- 

Fig. 2.7  Transition  graph of M. 

128  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

2  1  1 Since  ~  =  ~  we  have  ~ -   ~ .   The  reduced  machine  M'  is (~[A], [B], [C]}, 

[a,  b}, ~',  A,  [[A]}),  where  ~'  is  defined  as 

[A] [B] 
[C] 

a  b 
[,4]  In] IS]  [c] 

[c]  [A] 

Here  we  have  chosen  [A]  to  represent  the  equivalence  class  [A,  F},  [B]  to 
represent  [B,  E},  and  [C]  to  represent  [C,  D}. 

2.3.2.  The  Pumping  Lemma  for  Regular  Sets 

We  shall  now  derive  a  characterization  of  regular  sets  that  will  be  useful 
in  proving  certain  languages  not  to  be  regular.  The  next  theorem  is  referred 
to  as  a  "pumping"  lemma,  because  it  says,  in  effect,  that  given  any  regular 

set  and  any  sufficiently  long  sentence  in  that  set,  we  can  find  a  nonempty 
substring  in  that  sentence  which  can  be  repeated  as  often  as  we  like  (i.e., 
"pumped")  and  the  new  strings  so  formed  will  all  be  in  the  same  regular  set. 
It  is  often  possible  to  thus  derive  a  contradiction  of  the  hypothesis  that  the 

set  is  regular. 

THEOREM 2.7 

The  pumping  lemma  for  regular  sets"  Let L  be  a  regular  set.  There  exists 
a constant p  such that if a  string  w is in L  and  l wt  ~  p,  then  w can be written 
as  xyz,  where  0  <  t yl_< p  and  xy~z  ~  L  for  all  i  _~ 0. 

Proof.  Let  M  =  (Q,  X, ~,  q0, F)  be  a  finite  automaton  with  n  states  such 
that L(M)  =  L.  Let p  =  n. If w  ~  L  and  I w l ~  n,  then  consider  the  sequence 

of configurations  entered  by  M  in  accepting  w.  Since  there  are  at  least n  +  1 
configurations  in  the  sequence,  there  must  be  two  with  the  same  state  among 
the  first  n  +  1 configurations.  Thus  we  have  a  sequence  of  moves  such  that 

(qo, xyz)1.-~-- (ql,  yz)  [-&. (ql,  z)[--~. (q2,  e) 
for  some  q~  and  0  <  k  ~  n.  Thus,  0  < l y l _ <   n.  But  then 

(qo,  xfiz)1-~-. (q~, y'z) 

[2-(ql,y~-~z  ) 

o 

l "~-- (ql, yz) 
l -'~-" (q l,  z) 
I -~- (q2,  e) 

SEC. 2.3  PROPERTIES OF REGULAR SETS  129 

must  be  a  valid  sequence  of  moves  for  all  i >  0.  Since  w =  xyz  is  in  L, xytz 
is  in  L  for  all  i  >  1.  The  case  i  =  0  is  handled  similarly,  l---1 

Example  2.16 

We  shall  use  the  pumping  iemma  to  show  that  L  --  [0"l"[n  >  1} is  not 
a  regular  set.  Suppose  that  L  is  regular.  Then  for  a  sufficiently  large  n,  0"1" 
can  be  written  as  xyz  such  that  y  =/= e  and  xyiz  ~  L  for  all  i ~  0.  If y  ~  0 + 

or y  ~  1 +, then  xz  =  xyoz  ~  L.  If y  E  0+1 +, then xyyz  ~  L.  We have  a  con- 
tradiction,  so L  cannot  be  regular.  [--] 

2.3.3.  Closure  Properties of  Regular Sets 

We say that a set A  is closed under the  n-ary  operation  0  if O(a 1, a2,  ...,  a,) 
is in A  whenever  at is in  A  for  1 <  i <  n.  For  example,  the  set  of  integers  is 
closed  under  the  binary  operation  addition. 

In  this  section  we  shall  examine  certain  operations  under  which  the  class 
of  regular  sets  is  closed.  We  can  then  use  these  closure  properties  to  help 
determine  whether  certain  languages  are  regular.  We  already  know  that  if 
Lt  and  L2  are  regular  sets,  then  L~  U  L2, LiL2,  and  L*  are  regular. 

DEFINITION 

A  class  of  sets  is  a  Boolean  algebra  of sets  if  it  is  closed  under  union, 
intersection,  and  complementation. 

THEOREM  2.8 

The  class  of  regular  sets  included  in  Z*  is  a  Boolean  algebra  of  sets  for 
any  alphabet  l~. 

Proof  We  shall  show  closure  under  complementation.  We  already  have 
closure  under  union,  and  closure  under  intersection  follows  from  the  set- 

theoretic  law  A  n  B  =  7 U - B -   (Exercise  0.1.4).  Let  M  --  (Q,  A, d~, q0, F) 

be  any  finite  automaton  with  A  ~  E.  It  is  easy  to  show  that  every  regular 

set  L  ~  E*  has  such  a  finite  automaton.  Then  the  finite  automaton 
M'  =  (Q,  A, 6,  qo,  Q  -  F)  accepts  A* -- L(M).  Note  that the  fact that  M  is 
completely  specified is needed  here.  Now L(M),  the  complement  with  respect 
to  ~*,  can  be  expressed  as  L ( M ) = L ( M ' )   U  E * ( E - -   A)E*.  Since 

]g*(E  --  A)E*  is  regular,  the  regularity  of L(M)  follows  from  the  closure  of 

regular  sets  under  union.  E] 

THEOREM  2.9 

The  class  of regular  sets  is  closed  under  reversal. 
Proof.  Let  M  --  (Q,  IE, ~, q0, F)  be  a  finite  automaton  defining  the  regu- 
lar set L.  To  define L R we  "run  M  backward."  That  is, let M'   be the  nondeter- 

ministic  finite  automaton  (Q  u  {q~}, ~:, 6', q~, F'),  where  F'  is  [q0]  if  e  ~  L 
and  F ' =   {q0, q~} if e  e  L. 

130  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

(1)  ~3'(q~,, a)  contains  q if $(q, a)  ~  F, 
(2)  For  all q' in  Q  and  a  in  E,  d~'(q', a)  contains q if O(q, a)  =  q'. 

It  is  easy  to  show  that  (q0,  w)[.~-(q,  e),  where  q  ~  F,  if  and  only  if 
(q~,, wR)1~ (qo, e).  ThUs, L(M' )  =  (L(M)) R =  L n.  E] 

The  class  of regular  sets is closed under  most common language  theoretic 
operations,  More  of  these  closure  properties  are  explored  in  the  Exer- 
cises. 

2.3.4.  Decidable  Questions  About  Regular  Sets 

We  have  seen  certain  specifications  for  regular  sets,  such  as  regular 
expressions  and finite automata.  There  are certain natural  questions  concern- 
ing these  representations  that  come  up.  Three  questions  with  which  we  shall 

be  concerned  here  are  the  following: 

The  membership  problem"  "Given  a  specification  of  known  type  and 
a  string  w,  is  w  in  the  language  so  specified ?" 

The  emptiness  problem:  "Given  a  specification  of  known  type,  does  it 
specify  the  empty  set ?" 

The  equivalence  problem:  "Given  two  specifications  of  the  same  known 
type,  do  they  specify the  same  language ?" 

The  specifications  for  regular  sets that  we shall consider  are 

(1)  Regular  expressions, 
(2)  Right-linear  grammars,  and 

(3)  Finite  automata. 

We  shall  first  give  algorithms  to  decide  the  three  problems  when  the  specifi- 

cation  is  a  finite  automaton. 

ALGORITHM 2.3 

Decision  of the  membership  question  problem  for  finite  automata. 

Input.  A  finite  automaton  M  =  (Q,  E,  A, q0, F)  and  a  word  w  in  2;*. 

Output.  "YES"  if  w  ~  L(M),   "NO"  otherwise. 
Method.  Let  w  =  ata z  . . .   an.  Successively  find  the  states  q~ =  di(qo, a~), 
q2  =  t~(q~, a 2 ) , . . . ,   q,  =  ~(q,_~, a,).  If  q,  is  in  F,  say  "YES";  if  not,  say 
"NO."  ~ 

The  correctness  of Algorithm  2.3 is too  obvious to  discuss.  However,  it is 
worth  discussing  the  time  and  space  complexity  of the  algorithm.  A  natural 
measure  of  these  complexities  is  the  number  of  steps  and  memory  cells 

needed  to  execute  the  algorithm  on  a  random  access  computer  in which each 

memory cell can  store  an integer  of arbitrary  size.  (Actually  there  is a  bound 

on  the  size  of integers  for  real  machines,  but  this  bound  is  so  large  that  we 
would undoubtedly never come against it for finite automata which we might 

SEC. 2.3  PROPERTIES OF REGULAR SETS  1 31 
reasonably  consider.  Thus  the  assumption  of unbounded  integers  is a reason- 

able  mathematical  simplification  here.) 

It is easy to  see that  the  time  taken  is a  linear  function  of the  length  of w. 
However,  it  is  not  so  clear  whether  or  not  the  "size"  of  M  affects  the  time 
taken.  We  must  assume  that  the  actual  specification  for  M  is  a  string  of 

symbols  chosen  from  some  finite  alphabet.  Thus  we  might  suppose  that 
states  are  named  q0,  q~,  . . . ,   qi,  -.  *,  where  the  integer  subscripts  are  binary 

numbers.  Likewise,  the  input  symbols  might  be  called  a t, a2,  . . . .   Assuming 
a  normal  kind  of  computer,  one  could  take  the  pairs  in  the  relation  d; and 
construct  a  two-dimensional  array  that  in  cell  (i, j)  gave  5(qi,  a~). Thus  the 

total  time  of  the  algorithm  would  be  an  amount  proportional  to  the  length 
of the  specification  of M  to  construct  the  table,  plus  an  amount  proportional 
to  lw]  to  execute  the  algorithm. 

The  space  required  is  primarily  the  space  required  by  the  table,  which  is 
seen  to  be  proportional  to  the  length  of  M's  specification.  (Recall  that  tf  is 

really  a  set  of pairs,  one  for  each  pair  of a  state  and  input  symbol.) 

We  shall  now  give  algorithms  to  decide  the  emptiness  and  equivalence 
problems  when  the  method  of specification  is  a  finite  automaton. 

ALGORITHM  2.4 

Decision  of emptiness  problem  for  finite  automata. 
Input.  A  finite  automaton  M  =  (Q,  E,  tS, q0, F). 

Output.  "YES"  if L(M)  ~  ~,  "NO"  otherwise. 
Method.  Compute  the  set  of states  accessible  from  qo.  If this  set contains 
a  final  state,  say  "YES";  otherwise,  say  "NO."  D 

ALGORITHM 2.5 

Decision  of  equivalence  problem  for  finite  automata. 
'Input.  Two  finite  automata  Mi  =  (Ql,  Xl,  t~l, ql,F1)  and  M2  = 
(Q2,  X2, ~2, q2, F2)  s0ch  that  Q1  A  Q2  =  ~ .  

Output.  "YES"  if L(M~)  =  L(M2),  "NO"  otherwise. 
Method.  Construct  the  finite  automaton 

M  =  (Q,  u  Qz, E,  u  E2, ~,  u  ~2, q,,  F,  u  Fz). 
Using  Lemma 2.11  determine  whether  ql  ~  q2.  If so,  say  "YES";  otherwise, 
say  "NO".  [Z] 

We  point  out  that  we  could  also  use  Algorithm  2.4  to  solve  the  equiva- 
lence  problem  since  L(M1)=  L(M2)  if  and  only  if 

(L(M~)  ~  L(M2) )  t,,.) (L(Mi)  ~  L(M2) )  =  ~ .  

132  ELEMENTS  OF  LANGUAGE THEORY  CHAP.  2 

We  now  turn  to  the  decidability  of  the  membership,  emptiness,  and 
equivalence  problems  for  two  other  representations  of  regular  sets--the 
regular  expressions  and  right-linear  grammars.  It  is  simple  to  show  that  for 
these representations  the three problems  are also decidable.  A regular expres- 

sion can  be converted,  by an  algorithm  which is implicit  in  Lemmas  2.9  and 
2.10,  into  a  finite  automaton.  The  appropriate  one  of  Algorithms  2.3-2.5 
can then  be  applied.  A  right-linear  grammar can be converted into  a  regular 
expression  by  Algorithm  2.1  and  the  algorithm  implicit  in  Theorem  2.2. 

Obviously,  these  algorithms  are  too  indirect  to  be  practical.  Direct,  fast- 
working  algorithms  will  be  the  subject  of several  of the  Exercises. 

We  can  summarize  these  results  by  the  following  theorem. 

THEOREM 2.10 

If the  method  of specification  is  finite  automata,  regular  expressions,  or 
right-linear  grammars,  then  the  membership,  emptiness,  and  equivalence 
problems  are  decidable  for  regular  sets.  [Z 

It  should  be  emphasized  that  these  three  problems  are  not  decidable  for 
every  representation  of  regular  sets.  In  particular,  consider  the  following 
example. 

Example 2.17 

We can enumerate the Turing machines. (See the Exercises in Section 0.4.) 
Let  M1, Mz ,..  * be  such  an  enumeration.  We  can  define  the  integers  to  be 
a  representation  of the  regular  sets  as follows: 

(1)  If Mt accepts a  regular set, then let integer i represent that regular set. 
(2)  If  Mt  does  not  accept  a  regular  set,  then  let  integer  i  represent  {e}. 

Each  integer  thus  represents  a  regular  set,  and  each  regular  set  is  repre- 
sented  by  at  least  one  integer.  It  is  known  that  for  the  representation  of 
Turing  machines  used  here  the  emptiness  problem  is  undecidable  (Exercise 
0.4.16).  Suppose  that  it  were  decidable  whether  integer  i  represented  @. 

Then  it is  easy to  see  that  M~ accepts  ~  if and  only if i represents  ~ .   Thus 
the  emptiness problem  is  undecidable  for  regular  sets  when  regular  sets  are 

specified  in this  manner.  D 

E X E R C I S E S  
2.3.1.  Given  a  finite automaton  with n accessible states,  what  is the  smallest number of states  the reduced machine can have ? 

EXERCISES  1 33 
2.3.2.  Find  the  minimum  state  finite  automaton  for  the  language  specified  by 

the  finite  automaton  M  =  ((A, B,  C,  D,  E,  F],  {0,  1 }, ~, A, [E, F}),  where 

is  given by 

State  A B 

C D 
E F 

Input 
0  1 

B  C E  F 

A  A F  E 

D  F D  E 

2.3.3.  Show  that  for  all  n  there  is  an  n-state  finite  automaton  such  that 

n-2  n-3 ~  ~. 

2.3.4.  Prove  that  L ( M' )   =  L( M )   in  Algorithm  2.2. 

DEFINITION 

We  say  that  a  relation  R  on  1~*  is  right-invariant  if  x  R y  implies 
xz  R  yz  for  all  x, y, z  in  ~*. 

2.3.5.  Show  that L  is  a  regular  set  if and  only  if L  is the  union  of some  of the 

equivalence  classes  of  a  right-invariant  equivalence  relation  R  of  finite 

index.  Hint:  Only  if."  Let  R  be  the  relation  x  R  y  if  and  only  if 
(q0, x)1~  (p,  e), (q0, Y) [ ~  (q, e),  and p  =  q.  (That is,  x and y  take a finite 

automaton  defining L  to the same state.)  Show that  R is a right-invariant 
equivalence  relation  of  finite  index.  If:  Construct  a  finite  automaton 
for  L  using  the  equivalence  classes  of  R  for  states. 

DEFINITION 

We  say that  E  is  the  coarsest  right-invariant  equivalence  relation  for 
a  language  L  ~  E*  if  x  E y   if  and  only  if  for  all  z  ~  E*  we  find 
xz   ~  L  exactly  when  yz  ~  L. 

The  following  exercise  states  that  every  right-invariant  equivalence 
relation  defining  a  language  is  always  contained  in  E. 

2.3.6.  Let L  be the union of some of the equivalence classes  of a right-invariant 

equivalence  relation  R  on  ~*.  Let  E  be  the  coarsest  right-invariant 
equivalence  relation  for  L.  Show  that  E  ~  R. 

*2.3.7.  Show  that  the  coarsest  right  invariant  equivalence  relation  for  a  lan- 

guage  is  of  finite  index  if  and  only  if  that  language  is  a  regular  set. 

2.3.8.  Let  M  =  (Q,  E,  ~, q0, F)  be  a  reduced  finite  automaton.  Define  the 

relation  E  on  ~*  as  follows:  x  E  y  if  and  only  if  (q0, x)I ~  (p,  e), 

(q0, Y)I ~  (q, e),  and  p  =  q.  Show  that  E  is  the  coarsest  right-invariant 
equivalence  relation  for  L(M) . 

134  ELEMENTS  OF  LANGUAGE  THEORY  CHAP. 2 

DEFINITION 

An  equivalence  relation  R  on  E*  is  a  congruence  relation  if  R  is 
both  left-  and  right-invariant  (i.e.,  if  x  R y,  then  wxz  R  wyz  for  all 

w, x, y, z  in  E*). 

2.3.9.  Show that L  is a  regular set if and  only if L  is the union  of some of the 

equivalence classes  of a  congruence relation  of finite index. 

2.3.10.  Show  that  if  M1  and  Mz  are  two  reduced  finite  automata  such  that 

L(M i)  =  L(M2),  then the transition graphs  of M1  and M2  are the same. 

"2.3.11.  Show  that  Algorithm  2.2  is  of time  complexity n 2.  (That  is,  show  that 

there exists a  finite automaton  M  with n  states  such that  Algorithm  2.2 
requires  n 2 operations  to  find  the  reduced  automaton  for  M.)  What  is 
the  expected  time  complexity  of  Algorithm  2.2 ? 

It  is  possible  to  find  an  algorithm  for  minimizing  the  states  in  a 
finite  automaton  which  always  runs  in  time  no  greater  than  n  log n, 
where  n  is  the  number  of states  in  the  finite  automaton  to  be  reduced. 
The  most  time-consuming  part  of  Algorithm  2.2  is  the  determination 

of the equivalence classes under  =_ in step 2 using the method suggested 
in  Lemma 2.11.  However,  we can  use  the following algorithm  in  step  2 
to  reduce  the  time  complexity  of Algorithm  2.2  to  n  log n. 

This  new  algorithm  refines  partitions  on  the  set  of  states  in  a 
manner  somewhat  different  from  that  suggested  by  Lemma  2.11. 
Initially,  the  states  are  partitioned into  final  and  nonfinal  states. 

Then,  suppose  that  we  have  the  partition  consisting  of  the  set  of 
blocks  {Itt, lt2 . . . .   , uk-~}.  A  block  lrt  in  this  partition  and  an  input 

symbol  a  are  selected  and  used  to  refine  this  partition.  Each  block  no 
such  that  ~(q, a)  ~  us  for  some q  in  ~zj is  split  into  two  blocks/t~  and 
7t~'  such  that  7t~. = [ q t q   ~  7r~  and  5(q,a)  ~  7it}  and  7t~' =Tt s --  7t: j, 

Thus,  in  contrast  with  the  method  in  Lemma  2.11,  here  blocks  are 
refined  when  the  successor  states  on  a  given  input  have  previously 
been shown inequivalent. 

ALGORITHM 2.6 

Determining  the  equivalence  classes  of  a  finite  automaton. 
Input.  A  finite  automaton  M  =  (Q,  Z,  5, q0, F). 

Output.  The  indistinguishability classes  under  _=. 
Method. 

(1)  Define  tS-l(q,  a ) =   [ p l t ~ ( p , a ) = q }   for  all  q  ~  Q  and  a  ~  ~. 
For  a  ~  Z,  let  ;g;.o =  [qlq  ~  ~,  and  5-1(q, a)  ~  ~}. 

(2)  L e t g l   = F a n d g z   = Q - - F .  
(3)  For  all a  ~  Z,  define the  index set 

[[ 1},  if  z~n,,. ~  #n2,. I(a) 
.-.,._ 

([2},  otherwise 

(4)  Set k  =  3. 

EXERCISES  135 
(5)  Select  a  ~  ~  and  i  ~  I(a).  [If  I ( a ) =   Z~  for  all  a  ~  1~,  halt; 
the  output  is  the  set {~za, zr2 . . . . .   ~zk-a].] 

(6)  Delete  i from  l(a). 

(7)  For  all j  <  k  such  that  there  is  a  state  q  ~  n~  and  O(q, a)  ~  nt 
do  steps  7(a)-7(d): 

t  It (a)  Let  ztj  =  {q[O(q,  a)  6  xi  and  q  6  ztj} and  let/ti  =  try -  x~. 

"  Construct  new  ztl, a  and (b)  Replace  zt~  by  zr~ and  let  zrk  =  zri. 
~zk,= for  all a  6  Z. 
(c)  For  all a  ~  ]~, modify  l(a)  as  follows: 

[I(a)  t..) {j},  if j  q~  I(a)  and  0  <  ~.~.a  ~  #n;k,a 
I(a)  =  i I(a)  U  {k},  otherwise 

(d)  S e t k = k + l .  
(8)  Go  to  step  5. 

2.3.12.  Apply  Algorithm  2.6  to  the  finite  automata  in  Example  2.15  and 

Exercise  2.3.2. 

2.3.13.  Prove  that  Algorithm  2.6  correctly  determines  the  indistinguishability 

classes  of  a  finite  automaton. 

*'2.3.14.  Show  that  Algorithm  2.6  can  be  implemented  in  time  n  log  n. 

2.3.15.  Show  that  the  following  are  not  regular  sets: 

(a)  {0"10"ln  >  1}. 
(b)  {wwl w  is  in  {0,  1}*]. (c) 

L(G),  where  G  is  defined  by  productions  S  ----. aSbS[ e. 
(d)  {a"~ln >_ 1}. 

(e)  {ap IP is  a  prime}. (f) 

{wiw  is  in  {0,  1]*  and  w has  an  equal  number  of O's  and  l's]. 

2.3.16.  Let  f ( m )   be  a  monotonically  increasing  function  such  that  for  all  n 

there exists m  such that f ( m   +  1) > f ( m )   +  n.  Show  that  {aS(m)[m >  1} 

is  not  regular. 

DEFINITION 

Let L1  and  Lz  be  languages.  We  define  the  following  operations" 
(1)  L1/L2  =  {wlfor some x  ~  L2,  wx  is  in L1]. 
(2)  INIT(Lx)  =  {wlfor some  x,  wx  is  in L1]. 
(3)  FIN(La)  =  {wlfor some  x,  xw  is  in L1]. 

(4)  SUB(L1)  =  {wl for some x  and y,  xwy  is  in La ]. 
(5)  M I N ( L i ) =   {wlw  ~  L1  and  for  no  proper  prefix  x  of  w  is 

x  eLx}. 
(6)  MAX(L1)  =  {wlw  ~  L1  and  for  no  x  v~ e  is  wx  ~  L,}. 

Example 2.18 

Let  L1  ={0"1"0 m l n , m ~ l }   and  L2  = l * 0 * .  

ThenL~/Lz  =  L1  U  {0il~li>_  1,]  <  i]. 

Lz/L1  =  ~3. 

136  ELEMENTS OF LANGUAGE  THEORY  CHAP.  2 

INIT(L,) =  L~ u  {Oel:li ~  I,] ~  i} u  0". 

HN(Li)  =  [O~l:Oklk >  I,]> I, i<j}  U  I+0 + U  0". 
SUB(L1)  =  {0qJ0k] i < ] }   U  1'0"  U  0"1". 
MIN(L1)  =  {0"l"01n  >  1]. 

MAX(L a)  --  ~ .   D 

"2,3.17,  Let  L1  and L2  be  regular.  Show  that  the  following  are  regular: 

(a)  L1/L2. 
(b)  INIT(L~). 

(c)  FIN(L1). 
(d) SUB(L1). 
(e) MIN(L~). 
(f)  MAX(L1). 

"2.3,18,  Let L1  be  a  regular  set  and  L2  an  arbitrary  language.  Show  that  La]L2 

is  regular.  Does  there  exist  an  algorithm  to  find  a  finite  automaton  for 
L 1 [L2  given  one  for  L a ? 

DEFINITION 

The  derivative  Dx~z of a  regular expression  0c with  respect  to  x  ~  E* 
can  be  defined  recursively  as follows: 

(1)  OeOC =  0~. 

(2)  For  a  ~  X 

(a)  Oaf2  =  ~ .  (b)  Dae  =  ~ .  

(c)  D a b =   { ~ '   i f a - C b  e,  i f a   = b .  
(d)  Da(O~ +  fl)  =  ha~Z +  Daft. {(Da00fl,  if e  c  0c 
(e)  Da(OCfl)=  (DaOOfl  +  Daft,  if  e  ~  0C. 
(f)  Boa*  =  (Boa)a*. (3)  F o ra ~   X a n d x   ~  X*, 

Da.a  =  Ox(Oaa) 
2.3.19.  Show  that  if o~ =  10"1,  then 

(a)  DeOC =  10"1. 

(b)  D0a  =  ~ .  (c)  Dla;  =  0"1. 

*2.3.20.  Show  that  if  ~  is  a  regular  expression  that  denotes  the  regular  set  R, 

then  Dx0c denotes  x\R  =  (w[ xw  ~  R}. 

*'2.3.21.  Let  L  be  a  regular  set.  Show  that  {xlxy  ~  L  for  some  y  such  that 

I xl  =  [y I} is  regular. 

A  generalization  of Exercise  2.3.21  is the  following. 

**2.3.22.  Let  L  be  a  regular  set  and  f ( x )  a  polynomial  in  x  with  nonnegative 

integer  coefficients.  Show  that 

EXERCISES  137 
*2.3.23. 

2.3.24. 

2.3.25. 

2.3.26. 

*2.3.27. 

2.3.28. 

**2.3.29. 

*2.3.30. 

"2.3.31. 

[w[ wy  ~  L  for  some y  such  that  [y[  =  f(I w 1)] 
is regular. 

Let  L  be  a  regular  set  and  h  a  homomorphism.  Show  that  h(L)  and 
h-I(L)  are regular sets. 

Prove  the  correctness  of Algorithms  2.4-2.5. 
Discuss  the  time  and  space  complexity  of Algorithms  2.4  and  2.5. 

Give  a  formal  proof  of  Theorem  2.9.  Note  that  it  is  not  sufficient  to 
show  simply  that,  say,  for  every  regular  expression  there  is  a  finite 

automaton  accepting  the  set  denoted  thereby.  One  must  show  that 
there  is  an  algorithm  to  construct  the  automaton  from  the  regular 
expression.  See  Example  2.17  in  this  connection. 

Give  an  efficient  algorithm  to  minimize  the  number  of  states  in  an 
incompletely  specified  deterministic  finite  automaton. 

Give  efficient  algorithms  to  solve  the  membership,  emptiness,  and 
equivalence problems for 

(a)  Regular  expressions. 
(b)  Right-linear  grammars. 

(c)  Nondeterministic  finite automata. 

Show  that  the  membership  and  equivalence  problems  are  undecidable 
for  the  representation  of regular  sets  given  in  Example  2.17. 

Show  that  the  question  "Is L(M)  infinite ?"  is  decidable for finite  auto- 
mata.  Hint:  Show  that L(M)  is  infinite,  for  n-state  finite  automaton M, 

if  and  only  if L(M)  contains  a  word  w  such  that  n  _<lwl  <  2n. 

Show  that  it  is  decidable,  for  finite  automata  M1  and  M2,  whether L(M~)  g_ L(Mz). 

Open Problem 

2.3.32.  Find  a  fast  algorithm  (say,  one  which  takes  time  n k for  some  constant 

k  on  automata  of  n  states)  which  gives  a  minimum  state  nondeter- 
ministic  finite  automaton  equivalent  to  a  given  one. 

Programming  Exercises 

2.3.33. 

2.3.34. 
2.3.35. 

2.3.36. 

Write  a  program  that  takes  as  input  a  finite  automaton,  right-linear 

grammar,  or  regular  expression  and  produces  as  output  an  equivalent 
finite  automaton,  right-linear  grammar,  or  regular  expression.  For 

example, this program can be used to construct a finite automaton from 

a  regular  expression. 

Construct  a  program  that  takes  as  input  a  specification  of  a  finite 
automaton  M  and  produces  as  output  a,reduced  finite  automaton  that 
is equivalent to  M. 

Write  a  program that will simulate a nondeterministic finite automaton. 

Construct  a  program  that  determines  whether  two  specifications  of  a 
regular set are  equivalent. 

138  ELEMENTS OF  LANGUAGE THEORY  CHAP.  2 

B I B L I O G R A P H I C   NO T E S  
The  minimization  of finite  automata  was  first  studied  by  Huffman  [1954] and Moore  [1956]. The  closure  properties  of  regular  sets  and  decidability  results  for 
finite  automata  are  from  Rabin  and  Scott  [1959]. The  Exercises  contain  some  of  the  many  results  concerning  finite  automata 

and regular  sets.  Algorithm  2.6  is from  Hopcroft  [1971]. Exercise 2.3.22  has  been proved by Kosaraju [1970]. The derivative of  a regular  expression  was  defined  by 
Brzozowski [1964]. There  are  many techniques  to  minimize incompletely specified  finite  automata 

(Exercise  2.3.27).  Ginsburg  [1962]  and  Prather  [1969]  consider  this  problem. 
Kameda  and  Weiner  [1968] give  a  partial  solution  to  Exercise 2.3.32. The  books  by  Gill  [1962], Ginsburg  [1962], Harrison  [1965], Minsky  [1967], 

Booth  [1967], Ginzburg  [1968],  Arbib  [1969],  and  Salomaa  [1969a] cover  finite 

automata  in detail. Thompson  [1968] outlines  a  useful  programming  technique  for  constructing  a 

recognizer from  a regular expression. 

2.4.  CONTEXT-FREE  LANGUAGES 

Of the  four  classes  of grammars  in  the  Chomsky  hierarchy,  the  context- 
free  grammars  are  the  most  important  in  terms  of  application  to  program- 
ming languages and compiling. A  context-free grammar can be used to specify 
most  of  the  syntactic  structure  of  a  programming  language.  In  addition, 

a  context-free  grammar  can  be  used  as  the  basis  of  various  schemes  for 
specifying  translations. 

During  the  compiling  process  itself,  we  can  use  the  syntactic  structure 
imparted  to  an  input  program  by  a  context-free  grammar  to  help  produce 
the  translation  for  the  input.  The  syntactic  structure  of  an  input  sentence 
can  be  determined  from  the  sequence  of  productions  used  to  derive  that 
input  string.  Thus  in  a  compiler  the  syntactic  analyzer  can  be  viewed  as 

a  device  which  attempts  to  determine  if  there  is  a  derivation  of  the  input 
string  according  to  some  context-free  grammar.  However,  given  a  CFG  G 
and  an  input  string  w,  it  is  a  nontrivial  task  to  determine  whether  w  is  in L(G) 

and,  if so,  what  is  a  derivation  for  w in  G.  We  shall  treat  this  question 
in  detail in  Chapters  4-7. 

In  this  section  we  shall  build  the  foundation  on  which  we  shall  base  our 
study  of  parsing.  In  particular,  we  shall  define  derivation  trees  and  study 
some  transformations  which  can  be  applied  to  context-free  grammars  to 
make  their  representation  more  convenient. 

SEC. 2.4  CONTEXT-FREE LANGUAGES  139 

2.4.1.  Derivation  Trees 

In  a  grammar it is possible to have several derivations that  are equivalent, 
in the  sense  that  all  derivations  use  the  same  productions  at  the  same  places, 

but  in  a  different  order.  The  definition  of when  two  derivations  are  equiva- 
lent  is  a  complex  matter  for  unrestricted  grammars  (see  the  Exercises  for 

Section  2.2),  but  for  context-free  grammars  we  can  define  a  convenient 
graphical representative  of an equivalence  class of derivations called a  deriva- 
tion  tree. 

A  derivation tree for a context-free grammar  G  =  (N,  X, P,  S) is a labeled 
ordered  tree  in  which  each  node  is  labeled  by  a  symbol  from  N  U  X  W {e}. 
If  an  interior  node  is  labeled  A  and  its  direct  descendants  are  labeled 
Xi,  X 2 , . . . ,   X,,  then  A  --,  X~X 2 . . .   X,  is  a  production  in  P. 

DEFINITION 

A  labeled  ordered  tree  D  is  a  derivation  tree  (or parse  tree) for  a  context- 
free  grammar  G(A)  =  (N,  X, P,  A)  if 

(1)  The  root  of  D  is  labeled  A. 
(2)  If  D ~ , . . . ,   D k are  the  subtrees  of the  direct  descendants  of the  root 
and  the  root  of  D t is  labeled  X~, then  A  ~  Xt  . . .   X k is  a  production  in  P. 

D~ must  be a  derivation tree for  G(Xg)  =  (N,  X, P,  X~)  if  X~ is a  nonterminal, 
and  D t is  a  single  node  labeled  X t if  X~ is  a  terminal. 

(3)  Alternatively,  if  D a is  the  only  subtree  of the  root  of  D  and  the  root 
of  D a is  labeled  e,  then  A  ~  e  is  a  production  in P. 

Example  2.19 

The  trees  in  Fig.  2.8  are  derivation  trees  for  the  grammar  G  =  G(S) 
defined  by  S  ---~ aSbSIbSaS]  e.  D 

We  note  that  there  is  a  natural  ordering  on  the  nodes  of an  ordered  tree. 
That is, the direct descendants  of a node are ordered "from the left" as defined 

J  a  S 
(a)  (b) 

e 

(c) 

Fig. 2.11  Derivation  trees. 

(d) 

e  e 

CHAP.  2 
DEFINITION 
in  Section  0.5.4.  We  extend  the  from-the-left  ordering  as  follows.  Suppose 
that  n  is  a  node  and  n ~ , . . . ,   n k are  its  direct  descendants.  Then  if  i  <  j,  n, 
and  all  its  descendants  are  to  the  left  of  nj  and  all  its  descendants.  It  is  left 
for  the  Exercises  to  show  that  this  ordering  is  consistent.  All  that  needs  to 
be  shown  is  that  given  any  two  nodes  of  an  ordered  tree,  they  are  either  on 

a  path  or  one  is  to  the  left  of the  other. 

DEFINITION 

The frontier  of  a  derivation  tree  is  the  string  obtained  by  concatenating 
the  labels  of  the  leaves  (in  order  from  the  left).  For  example,  the  frontiers 

of the  derivation  trees  in  Fig.  2.8  are  (a)  S,  (b)  e,  (c)  abab,  and  (d)  abab. 

We  shall  now  show  that  a  derivation  tree  is  an  adequate  representation 
for  derivations  by  showing  that  for  every  derivation  of  a  sentential  form 

in  a  CF G  G  there  is  a  derivation  tree  of  G  with  frontier  ~,  and  conversely. 

To  do  so  we  introduce  a  few  more  terms.  Let  D  be  a  derivation  tree  for  a 
CF G   G  =  (N,  E, P,  S). 

S 
A  cut  of  D  is  a  set  C  of nodes  of  D  such  that 
(1)  No  two  nodes  in  C  are  on  the  same  path  in  D,  and 

(2)  No  other  node  of D  can  be  added  to  C  without  violating (1). 

Example  2.20 

The  set  of  nodes  consisting  of  only  the  root  is  a  cut.  Another  cut  is  the 
set  of leaves.  The  set  of  circled  nodes  in  Fig.  2.9  is  a  cut.  E] 

140  ELEMENTS OF LANGUAGE THEORY 

Fig.  2.9  Example  of a  cut. 
DEFINITION 

Let us define an interior frontier of D  as the  string obtained by concatenat- 
ing (in order from the left) the labels  of the nodes  of a  cut  of D.  For  example, abaSbS 

is  an  interior  frontier  of the  derivation  tree  shown  in  Fig.  2.9. 

SEC. 2.4  CONTEXT-FREE  LANGUAGES  '141 

LEMMA 2.12 

Let  S  =  a0,  a l , . . . ,   an  be  a  derivation  of  a,  from  S  in  CFG  G  = 
(N,  Z,P,  S).  Then  there  is a  derivation  tree  D  for  G  such that  D  has frontier 
a n and  interior  frontiers  a0,  a l , . . . ,   a,-1  (among  others). 

Proof.  We  shall  construct  a  sequence  of  derivation  trees  D t,  0  <  i <  n, 
such  that  the  frontier  of  D~ is  at. 

Let  D O be  the  derivation  tree  consisting  of  the  single  node  labeled  S. 
Suppose  that  at  =  fltAT'~  and  this  instance  of  A  is  rewritten  to  obtain 
at+~  =  fl~X~X2  . ..  Xky~.  Then  the  derivation  tree  Di+ 1 is  obtained  from  D~ 
by  adding  k  direct  descendants  to  the  leaf  labeled  with  this  instance  of  A 
(i.e.,  the  node  which  contributes  the  Ifl, I /  1st  symbol  to  the  frontier  of  Dr) 
and  labeling  these  direct  descendants  X,,  X 2 , . . . ,   Xk  respectively.  It  should 
be  evident  that  the  frontier  of  D~+ 1 is  a~+~. The  construction  of  D~+~ from 

D~ is  shown  in  Fig.  2.10. 

s 

fli  A  Yi 

S 
fit  A  rt 
X1  X2. .. Xk  
(a)  Di  (b)  Di+ 1 

Fig. 2.10 Alteration of Trees 

D,  will  then  be  the  desired  derivation  tree  D.  [Z 

We  will  now  obtain  the  converse  of Lemma  2.I2.  That  is,  for  every  deri- 
vation  tree for  G there  is at least  one  derivation  in  G. 

LEMMA 2.13 

Let  D  be  a  derivation  tree  for  a  C FG  G  =  (N,  Z, P,  S)  with  frontier  a. 
Then  S  =>  a. 

Proof.  Let  C 0, C 1, C 2 , . . .   , C,  be  any  sequence  of  cuts  of  D  such  that 

(1)  C O contains  only  the  root  of  D. 
(2)  C~+~ is  obtained  from  C~ by  replacing  one  interior  node  in  C~  by  its 
direct  descendants,  for  0  ~  i  <  n. 

(3)  C,  is  the  set  of leaves  of  D. 
Clearly  at  least  one  such  sequence  exists. 

If  a~  is  the  interior  frontier  associated  with  C~,  then  a0,  a ~ , . . . ,   a,  is 
a  derivation  of  a,  from  a0  in  G.  D 

142  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

There  are  two  derivations  that  can  be  constructed  from  a  derivation  tree 
which  will  be  of particular  interest  to  us. 

DEFINITION 

In  the  proof of Lemma 2.13,  if C;+t  is  obtained  from  Ci by  replacing  the 
leftmost nonleaf  in Ct by its direct descendants,  then the associated derivation 

* 0, ~1,.  - . ,   ~,  is  called  a  le~most  derivation  of  ~,  from  ~o  in  G.  We  define 
a  rightmost  derivation  analogously  by  replacing  "leftmost"  by  "rightmost" 
above.  Notice  that  the  leftmost  (or  rightmost)  derivation  associated  with 
a  derivation  tree  is  unique. 

If  S  --  ~o,  ~1,  * * *,  ~,  --  w is  a  leftmost  derivation  of the  terminal  string 
w,  then  each  0~t, 0  <  i  <  n,  is  of  the  form  xtAd3t  with  xt  ~  E*,  A t  ~  N, 
and  Pt  6  (N  U  Z)*. The  leftmost  nonterminal  A t is rewritten to  obtain  each 
succeeding  sentential  form.  The  reverse  situation  holds  for  rightmost  deriva- 
tions. 

Example 2.21 

Let  Go  be  the  CFG 

E=  E + T I T  

T=  > T , F I F  
F  > (E) la 

The  derivation  tree  shown  in  Fig.  2.11  represents  ten  equivalent  derivations 

!  I T  F I  I 
F  a i 

a  Fig. 2.11  Example of a tree. 

of the  sentence  a  +  a.  The  leftmost  derivation  is 

E------~ E  +  T - - - ~   T  +  T - - - >   F  +  T - -- >   a  +  T - - - ~   a  +  F - - - ~   a  +  a 
and  the  rightmost  derivation  is 

E---->  E  +  T----> E-+-  F----~  E  +  a~----~ T  +  a -- ->   F  +  a----~,  a  +  a D 

SEC. 2.4  CONTEXT-TREE  LANGUAGES  143 
DEFINITION 

If  S----0~0,  0~1 . . . .   ,0on is  a  leftmost  derivation  in  grammar  G,  then  we 
shall  write  S  =~  0~, or  S  ~=~ ~,,  if  G  is  clear,  to  indicate  the  leftmost  deriva- G lm  lm 
tion.  We  call  c~  a  left  sentential form.  Likewise,  if  S  =  a0,  a l , . . . ,   c~  is  a 

rightmost  derivation,  we  shall  write  S  ~  a~,  and  call  a~  a  right  sentential 

rm form.  We  use  ~  and  ~  to  indicate  single-step  leftmost  and  rightmost 

lm  rm derivations. 

We  can  combine  Lemmas  2.12 and  2.13  into  the following  theorem. 
THEOREM 2.1 1 

Let  G-~   (N,  E ,P,  S)  be  a  CFG.  Then  S ~ 0 c   if  and  only  if  there  is 
a  derivation  tree  for  G  with  frontier  0~. 

Proof.  Immediate  from  Lemmas  2.12  and  2.13.  [Z] 

Notice  that  we  have  been  careful  not  to  say  that  given  a  derivation 
S ~-~ ~  in a  C F G   G we can find a unique  derivation tree  for  G  with frontier  ~. 
The  reason  for this is that  there  are context-free  grammars which have several 
distinct  derivation  trees  with  the  same  frontier.  The  grammar  in  Example 

2.19  is  an  example  of such  a  grammar.  Derivation  trees  (c)  and  (d)  (Fig.  2.8) 
in  that  example  have  equal  frontiers  but  are  not  the  same  trees. 

DEFINITION 

We  say  that  a  C F G   G  is  ambiguous if  there  is  at  least  one  sentence  w 
in L(G) for which  there  is more  than  one  distinct  derivation  tree  with frontier 

w.  This  is  equivalent  to  saying  that  G  is  ambiguous  if  there  is  a  sentence  w 
in L(G) with two  or more distinct leftmost (or rightmost) derivations  (Exercise 

2.4.4). 

We  shall  consider  ambiguity  in  more  detail  in  Section  2.6.5. 

2.4.2.  Transformations  on  Context-Free  Grammars 

Given  a  grammar  it  is  often  desirable  to  modify  the  grammar  so  that 
a  certain  structure  is imposed  on  the  language  generated.  For  example,  let us 
consider L(Go). This  language  can  be  generated  by  the  grammar  G  with  pro- 

ductions 

E-  ~ E  Jr  EtE  .  EI(E)Ia 

But  there  are  two  features  of  G  which  are  not  desirable.  First  of  all,  G  is 
ambiguous  because  of  the  productions  E  ~  E  -k  El E .   E.  This  ambiguity 
can  be  removed  by  using  the  grammar  G I  with  productions 

E  ~ E  +  T I E *   T[ T 

Z  ~  (E) ta 

144  ELEMENTS  OF LANGUAGE  THEORY  CHAP.  2 

The  other  drawback  to  G,  which  is  shared  by  G1,  is  that  the  operators 
+  and  *  have  the  same  precedence.  That  is  to  say,  in  the  expressions 
a  +  a  *  a  and  a  *  a  +  a,  the  operators  would  associate  from  the  left  as  in 

(a  +  a)  *  a  and  (a  *  a)  +  a,  respectively. 

In  going  to  the  grammar  Go  we  can  obtain  the  conventional  precedence 
of  +  and  .. 

In  general,  there  is  no  algorithmic  method  to  impose  an  arbitrary  struc- 
ture  on  a  given  language.  However,  there  are  a  number  of  useful  transfor- 

mations  which  can  be  used  to  modify  a  grammar  without  disturbing  the 
language  generated.  In  this  section  and  in  Sections  2.4.3-2.4.5  we  shall  con- 

sider  a  number  of transformations  of this  nature. 

We  shall begin by considering  some very obvious but  important  transfor- 
mations.  In  certain  situations,  a  CFG  may  contain  useless  symbols  and 
productions.  For  example,  consider  the  grammar  G =  ({S, A}, [a,  b~, P,  S), 
where  P  =  IS ~  a, A  ~  b].  In  G,  the  nonterminal  A  and  the  terminal  b 

cannot  appear  in  any  sentential  form.  Thus  these  two  symbols are  irrelevant 
insofar  as  L(G)  is  concerned  and  can  be  removed  from  the  specification  of 

G  without  affecting L(G). 

DEFINITION 

We  say that  a  symbol  X  ~  N  U  ~  is  useless in  a  CF G  G =  (N,  ~, P,  S) 
if there  does  not  exist a  derivation  of the  form  S  ~  wXy ~  wxy.  Note  that 

w, x,  and y  are  in  ~*. 

To  determine  whether  a  nonterminal  A  is  useless,  we  first  provide  an 
algorithm  to  determine  whether  a  nonterminal  can  generate  any  terminal 

strings; i.e., is {w[ A ~  w, w in E*}  :  ~  ? The existence of such an algorithm 
implies  that  the  emptiness  problem  is  solvable  for  context-free  grammars. 

ALGORITHM 2.7 

IS L(G) nonempty ? 
Input.  CFG  G  =  (N,  E, P,  S). 

Output.  "YES"  if L(G) ~  ~ ,  "NO"  otherwise. 
Method.  We  construct  sets  No,  N t , . . .   recursively  as  follows: 

(1)  Let  N 0 =   ~  and  s e t i =   1. 
(2)  Let  N t = { A [ A ~ a   is  in  P  and  a  ~  (N t_l  U E ) * } w N t _   ~. 

(3)  If  N~ :/:  N,._i,  then  set  i - - i   +  1  and  go  to  step  2.  Otherwise,  let 
N,  --  N,. 

(4)  If S  is  in  N~,  output  "YES";  otherwise,  output  "NO."  V-] 

Since N,  _~ N, Algorithm 2.7 must terminate  after at most n +  1 iterations 
of step  (2) if N  has  n  members.  We  shall  prove  the  correctness  of  Algorithm 
2.7.  The  proof is  simple  and  will  serve  as  a  model  for  several  similar  proofs. 

SEC. 2.4  CONTEXT-FREE  LANGUAGES  145 

THEOREM 2. t 2 

Algorithm  2.7  says  "Y ES"  if and  only  if  S  =~  w  for  some  w  in  X*. 

Proof.  We  first  prove  the  following  statement  by  induction  on  i" 

(2.4.1)  If  A  is  in  N~,  then  A  ~  w  for  some  w  in  X* 

The  basis,  i  =  0,  holds  vacuously,  since  N o =  ~ .   Assume  that  (2.4.1) 
is  true  for  i,  and  let  A  be  in  Nt+l.  If  A  is  also  in  Nt,  the  inductive  step  is 
trivial.  If  A  is  in  N t + l -   N~,  then  there  is  a  production  A----~ X1  ..  Xk, 

where  each  X~ is either  in  X or  a  nonterminal  in N r   Thus  we can  find a  string 

wj  such  that  Xj  ~  wj  for  each j.  If  X t  is  in  X,  wj  =  Xj,  and  otherwise  the 
existence  of  w j  follows  from  (2.4.1).  It  is  simple  to  see  that 

A  ~  X  1  ...  X  k  ::::-~, w1X  2  ...  Xk  ~  "'"  ~  W1 "'"  Wk" 
The  case  k  =  0  (i.e.,  production  A  ---~ e)  is  not  ruled  out.  The  inductive 

step  is  complete. 

The  definition  of Nt  assures  us that  if N~  =  Nt_ 1,  then  N~  =  Nt+ 1 . . . .   * 

We  must  show  that  if A  ~  w for  some  w  ~  X*, then  A  is in Ne.  By  the  above 
comment,  all  we  need  to  show  is  that  A  is  in  Nt  for  some  i.  We  show  the 

following  by  induction  on  n" 

(2.4.2) 

/I If  A  ~  w,  then  A  is  in  N t for  some  i 

The  basis,  n  =  1,  is  trivial;  i  =  1 in  this  case.  Assume  that  (2.4.2)  is  true 

n+l  n for  n,  and  let  A  =:~  w.  Then  we  can  write  A  :=~ X1  " "   Xk  =:~ W,  where 

nj w  =  wt  " "   we  such  that  Xj  ~  w I  for  each j,  where  nj  <  n.t 

By  (2.4.2),  if  X~  is  in  N,  then  Xj  is  in  N,,  for  some  i~.  If  Xj  is  in  X,  let 
i j  =  0.  Let  i  =  1  +  max  ( i t , . . . ,   ik).  Then  by  definition,  A  is  in  N t.  The 
induction  is  complete.  Letting  A  =  S  in  (2.4.1)  and  (2.4.2),  we  have  the 

theorem.  D 

COROLLARY 

It  is  decidable,  for  C F G   G,  if L(G)  =  ~3.  [Z] 

DEFINITION 

We  say  that  a  symbol  X  in  N  U  X  is  inaccessible  in  a  C F G   G  = 
(N,  X, P,  S)  if  X  does  not  appear  in  any  sentential  form. 

"l'This is an  "obvious"  comment that requires a little thought. Think about the deriva- 

n+l tion  tree for the  derivation  A ~  w.  wy is the  frontier  of the  subtree with root  Xj. 

148  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

The  following  algorithm,  which  is  an  adaptation  of  Algorithm  0.3,  can be  used  to  remove  inaccessible  symbols from  a  CFG. 
ALGORITHM  2.8 

Removal  of inaccessible  symbols. 
lnput.  CFG  G  =  (N,  E, P,  S). 

Output.  CFG  G'  =  (N',  E', P',  S)  such  that 

(i)  L(G')  =  L(G). 
(ii)  For  all  X  in  N'  w  E'  there  exist  ~  and  fl  in  (N'  U  E')*  such  that 

S=~  ~Xp. G' 

Method. 
(1)  Let  V0  =  {S} and  set  i  =  1. 
(2)  Let  V~ =  {X[ some A  --,  ~Xp  is in P  and  A  is in  Vt_~ } U  V~_~. 
(3)  If  Vt ~  Vt_ 1,  set  i  =  i +  1  and  go  to  step  (2).  Otherwise,  let 

N ' =   V,  A N  z ' =   z,  n z  

P'  be  those  productions  in P  which  involve only  symbols  in  Vt 

C'  =  (N', Z',P',  S)  5 
There  is  a  great  deal  of similarity  between  Algorithms  2.7  and  2.8.  Note 
that  in  Algorithm  2.8,  since  V~ _.q N  u  E,  step  (2)  of  the  algorithm  can  be 
repeated  at most a finite number  of times.  Moreover,  a straightforward proof 

by  induction  on  i  shows  that  S  =-~ o~Xfl if  and  only if  X  is in  Vt  for  some  i. G 

We  are  now  in  a  position  to  remove  all  useless  symbols  from  a  CFG. 
ALGORITHM  2.9 

Useless  symbol removal. 
Input.  CFG  G  =  (N,  E, P,  S),  such  that  L(G)  ~  ~3. 

Output.  CFG  G'  =  (N',  E',  P',  S) such that  L(G')  =  L(G) and no symbol in  N'  U  E'  is  useless. 

Method. 
(1)  Apply Algorithm 2.7 to  G to obtain N,.  Let G 1 =  (N  n  N,,  E,  Pi,  S), 
where  P1  contains  those productions  of P  involving only symbols in N e U  E. 

(2)  Apply  Algorithm  2.8  to  G~  to  obtain  G'  =  (N',  E', P',  S).  [Z 

Step  (1)  of Algorithm  2.9 removes from  G all nonterminals  which  cannot 
generate  a  terminal  string.  Step  (2)  then  proceeds  to  remove  all  symbols 
which  are  not  accessible.  Each  symbol  X  in  the  resulting  grammar  must 

SEC. 2.4  CONTEXT-FREE LANGUAGES  147 

appear  in  at  least  one  derivation  of  the  form  S  =:~ wXy  :=~ wxy.  Note  that 

applying Algorithm  2.8  first and  then  applying  Algorithm  2.7 will not  always 
result  in  a  grammar  with  no  useless  symbols. 

T~EOREM 2.13 

a'  of Algorithm  2.9  has  no  useless  symbols,  and  L(G')  =  L(G). 
Proof.  We  leave  it for  the  Exercises  to  show  that  L(G')  =  L(G).  Suppose 
that  A  E  N'  is  useless.  From  the  definition  of  useless,  there  are  two  cases 

to  consider. 

Case  1"  S  =-~ eArl  is  false  for  all  e  and  ft.  In  this  case,  A  would  have G' 
been  removed  in  step  (2)  of  Algorithm  2.9. 

Case  2"  S  =-~ eArl  for  some  a  and  p,  but  A  =~  w is false  for  all  w in  ~'*. 

G"  G' 

Then  A  is not  removed  in  step  (2),  and,  moreover,  if A =~- ~,B~, then  B  is  not G 

removed  in  step  (2).  Thus,  if A  ~  w,  it  would  follow  that  A ~  w.  We  con- 

G  G' 

clude  that  A  ==~ w is  also  false  for  all  w, and  A  is eliminated  in  step  (1). 

G The  proof  that  no  terminal  of  G'  is  useless  is  handled  similarly  and  is 

left  for  the  Exercises.  [~] 
Example  2.22 

Consider  the  grammar  G  =  ((S,  A, B~}, (a,  b}, P,  S),  where  P  consists  of 

S -   > a l A  
A  > A B  

B  >b 

Let  us  apply  Algorithm  2.9  to  G.  In  step  (1),  N e -   {S,B}  so  that 

G1  --  ({S, B}, [a,  b], (S  --,  a,  B  ~  b],  S).  Applying  Algorithm  2.8,  we  have 

Vz  --  V,  --  {S,  a).  Thus,  G'  --  ({S],  (a],  (S--~  a],  S). 

If  we  apply  Algorithm  2.8  first  to  G,  we  find  that  all  symbols  are  acces- 
sible,  so  the  grammar  does  not  change.  Then  applying  Algorithm  2.7  gives 
N e --  (S,  B],  so  the  resulting  grammar  is  G1  above,  not  G'.  V-I 

It  is  often  convenient  to  eliminate  e-productions,  that  is,  productions  of 
the  form  A  ~  e,  from  a  CF G  G.  However,  if  e  is  in  L(G),  then  clearly  it  is 
impossible  to  have  no  productions  of the  form  A  ~  e. 

DEFINITION 

We  say  that  a  CFG  G  =  (N,  E, P,  S)  is  e-free  if either 
(1)  P  has  no  e-productions,  or 

148  ELEMENTS  OF  LANGUAGE  THEORY  CHAP.  2 

(2)  There  is  exactly  one  e-production  S - - ,   e  and  S  does  not  appear  on the  right  side  of any  production  in P. 
ALGORITHM 2.10 

Conversion  to  an  e-free  grammar. 
Input.  CFG  G  =  (N,  Z, P,  S). 

Output.  Equivalent  e-free  CFG  G'  --  (N',  Z, P',  S'). 
Method. 

+ (1)  Construct  N,  :  [AIA  ~  N  and  A :=~ e}.  The  algorithm  is  similar  to 

G that  used  in  Algorithms  2.7  and  2.8  and  is  left  for  the  Exercises. 

(2)  Let P'  be  the  set  of productions  constructed  as  follows" 

(a)  If  A  ~  aoBla,Bza z  . . .   BkO~ k  is  in  P,  k  _~ O,  and  for  1 _G i ~  k 

each  B~ is  in  N e but  nosymbols  in  any  ~xj are  in  N,,  0  G  j  G  k, 
then  add  to  P'  all  productions  of  the  form 

where  Xi  is  either  Bi  or  e,  without  adding  A  ---~ e  to  P'.  (This could  occur  if all  ~  =  e.) 
(b)  If S  is  in  Ne, add  to  P'  the  productions 

S'  > e lS  
where  S'  is  a  new  symbol,  and  let  N ' =   N  t,.)[S'}.  Otherwise, let  N'  =  N  and  S ' =   S. 
(3)  Let  G'  =  (N',  Z, P',  S').  [~ 
Example  2.23 

Consider  the  grammar  of Example  2.19  with  productions 

S  > aSbS[ bSaSi  e 
Applying  Algorithm  2.10  to  this  grammar,  we  would  obtain  the  grammar with  the  following productions" 

S 

! 

S 

>Sle 

aSbSl bSaSl  aSb l abSl ab l bSa l baS l ba 

THEOREM 2.14 

Algorithm  2.10  produces  an  e-free  grammar  equivalent  to  its  input grammar. 

Proof  By  inspection,  G'  of  Algorithm  2.10  is  e-free.  To  prove  that 

SEC. 2.4  CONTEXT-FREE  LANGUAGES  149 
L(G)  --  L(G'),  we  can  prove  the  following  statement  by  induction  on  the 

length  of w" 

(2.4.3)  A  ~  w  if and  only  if w  =/= e  and  A ~  w G'  G 

The proof of (2.4.3) is left for the Exercises.  Substituting  S for A in  (2.4.3), 
we  see  that  for  w  =/= e,  w  ~  L(G)  if  and  only  if  w  ~  L(G').  The  fact  that 

e  ~  L(G)  if and  only  if e  ~  L(G')  is  evident.  Thus,  L(G)  --  L(G').  [--7 

Another transformation  on grammars  which we find useful  is the  removal 
of  productions  of  the  form  A  --.  B,  which  we  shall  call  single  productions. 

ALGORITHM 2.11 

Removal  of single  productions. 
Input.  An  e-free  CF G  G. 

Output.  An  equivalent  e-free  CFG  G'  with  no  single  productions. 
Method. 

(1)  Construct  for  each  A  in  N  the  set  N~  --  (BI A  ==~ B} as  follows" 

(a)  Let  N o =  {A]  and  s e t / =   1. 

(b)  Let  N,.  =  {C] B  ----, C  is  in  P  and  B  ~  N,._ 1 } U  N,._i. 
(c)  If  N~ =/= N,_ ~,  set  i - -   i  -c-  1  and  repeat  step  (b).  Otherwise,  let 

N A =  N i. 
(2)  Construct P'  as  follows"  If B  --,  0~ is in P  and  not  a  single  production, 
place  A  ---, a  in  P'  for  all  A  such  that  B  ~  N~. 

(3)  Let  G'  --  (N,  E, P',  S).  [-] 

Example  2.24 

Let  us  apply  Algorithm  2.11  to  the  grammar  G O with  productions 

E - - + E +   T I T  

T  ~ T , F I F  
F -   ~. ( E ) l a  

In step (1), N~  =  (E,  T,  F}, Nr  =  {T,  F}, N~ =  {F]. After step (2), P'  becomes 

E  ,  E-t-  T I T ,   FI(E)Ia  

T  , T , F I ( E ) l a  
F  ,  ( E ) la   [Z] 

THEOREM 2.15 

In  Algorithm  2.11,  G'  has  no  single  productions,  and L(G)  =  L(G'). 

-  ?  :  - 

150  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

Proof  By  inspection,  G'  has  no  single  productions.  We  shall  first  show 
that  L(G')  c__ L(G).  Let  w  be  in  L(G').  Then  there  exists  in  G'  a  derivation 
S -   0% =~  0c 1 =~  . . .   =>  ~ , - -   w.  If  the  production  applied  going  from  ~;  to 
~,.+~  is  A--~  fl,  then  there  is  some  B  in  N  (possibly,  A -   B)  such  that 

A  =~  B  and B =~  ft. Thus,  A =~  fl and ~z,. =~  ~,.+ 1.  It  follows  that  S  =~  w,  and 

O  G  O  G  O w  is  in  L(G).  Thus,  L(G')  ~  L(G). 

To  show  that  L(G')  --L(G),  we  must  show  that  L(G)  c__ L(G').  Thus  let 
w  be  in  L(G)  and  S  --  ~0  =~  ~  =~  " "   =~  ~,  --  w  be  a  leftmost  derivation 

Im  lm  lm of  w  in  G.  We  can  find  a  sequence  of  subscripts  il,  iz,.  * *,  ik  consisting  of 

exactly  those j  such  that  ~j_ 1 =~  ~j  by  an  application  of  a  production  other 

lm than  a  single  production.  In  particular,  since  the  derivation  of  a  terminal 

string  cannot  end  with  a  single  production,  i k  -~  n. 

Since  the  derivation  is  leftmost,  consecutive  uses  of  single  productions 
replace  the  symbol  at  the  same  position  in  the  left  sentential  forms  involved. 
Thus  we  see  that  S  ==~ ~;,  z>   0c;~ =~  . . .   =~  0~;~ --  w.  Thus,  w  is  in  L(G'). 

G"  O'  G'  G' We  conclude  that  L(G')  --  L(G). 

DEFINITION 

A  C F G   G  --  (N,  Z, P,  S)  is  said  to  be  cycle-free  if there  is  no  derivation 

+ of  the  form  A  =~  A  for  any  A  in  N.  G  is  said  to  be proper  if  it  is  cycle-free, 

is  e-free,  and  has  no  useless  symbols. 

Gramm ars  w h i ch   have  cycles  or  e-productions  are  sometimes  more  dif- 
ficult  to  parse  than  grammars  which  are  cycle-free  and  e-free.  In  addition, 
in  any  practical  situation  useless  symbols  increase  the  size  of  a  parser  un- 
necessarily.  T hroug hou t  this  boo k  we  shall assume  a  grammar  has  no  useless 

symbols.  For  some  of the  parsing  algorithms  to  be  discussed  in  this  book   we 

shall  insist  that  the  grammar  at  hand  be proper.  The  following theorem  shows 
that  this  requirement  still  allows  us  to  consider  all  context-free  languages. 

THEOREM 2.16 

If L  is  a  CFL,  then  L  =  L(G)  for  some  proper  C F G   G. 
Proof  Use  Algorithms  2.8-2.11.  [~] 
DEFINITION 

An  A-production  in  a  C F G   is  a  production  of the  form  A  ~  0~ for  some 
0c. (Do  not  confuse  an  "A-production"  with  an  "e-production,"  which  is  one 

of the  form  B  ~  e.) 

Next  we  introduce  a  transformation  which  can  be  used  to  eliminate  from 
a  g ramm ar  a  production  of the  form  A  ~  o~Bfl. To  eliminate  this  production 

we  must  add  to  the  g ramm ar  a  set  of  new  productions  formed  by  replacing 

the  nonterminal  B  by  all  right  sides  of B-productions. 

SEC. 2.4  CONTEXT-FREE LANGUAGES  1 51 

LEMMA 2.14 

Let  G  =  (N,  Z, P,  S)  be  a  CFG  and  A  ---~ ~Bfl  be  in P  for  some  B  ~  N 
and  0~ and  fl  in  (N  U  Z)*.  Let  B  ---~ ~'1 I?z[  "'"  [~'k be  all  the  B-productions 
in  P.  Let  G ' =   (N,  Z ,P ',  S ) w h e r e  

Then L(G)  =  L(G'). 

Proof.  Exercise. 

Example  2.25 

Let  us  replace  the  production  A  ---~ aAA  in  the  grammar  G  having  the 
two  productions  A  ~  aAA[b.  Applying  Lemma  2.14,  assuming  that  ~  =  a, 
B  =  A,  and  fl  =  A,  we  would  obtain  G'  having  productions 

A  ---~ aaA A Al abA l b. 
Derivation  trees  corresponding  to  the  derivations  of  aabbb  in  G  and  G' 
are  shown  in  Fig.  2.12(a)  and  (b).  Note  that  the  effect  of  the  transformation 
is to  "merge"  the  root  of the tree in  Fig. 2.12(a) with its second direct descen- 
dant.  [~ 

a / ! ~ A ~ b  L I  

b  b  I  !  I b  b  b 

(a)  (b) 
In G  In G' 

Fig. 2.12  Derivation trees in  G and  G'. 

2.4.3.  Chomsky  Normal  Form 

DEFINITION 

A  CFG  G  =  (N,  Z, P,  S)  is  said  to  be  in  Chomsky  normal form  (CNF) 
if each  production  in P  is  of one  of the  forms 

(1)  A  ~  BC  with  A,  B,  and  C  in  N,  or 
(2)  A ~ a w i t h a   E  Z,  or 

152  ELEMENTS OF  LANGUAGE THEORY  CHAP.  2 

(3)  If e  ~  L(G),  then  S--~  e  is  a  production,  and  S  does  not  appear  on 
the  right  side  of any  production. 

We  shall  show  that  every  context-free  language  has  a  Chomsky  normal 
form  grammar.  This  result  is  useful  in  simplifying  the  notation  needed  to 
represent  a  context-free  language. 

ALGORITHM 2.12 

Conversion  to  Chomsky  normal  form. 
Input.  A  proper  C FG   G  =  (N,  2;, P,  S)  with  no  single  productions. 

Output.  A  C FG   G'  in  CNF,  with  L(G)  --  L(G'). 
Method.  From  G  we  shall  construct  an  equivalent  CN F  grammar  G' 
as  follows.  Let  P'  be  the  following  set  of productions: 

(1)  Add  each  production  of the  form  A  --~  a  in  P  to  P'. 
(2)  Add  each  production  of the  form  A  --~  B C   in  P  to  P'. 
(3)  If S  --,  e  is  in  P,  add  S  --,  e  to  P'. 
(4)  For  each  production  of the  form  A  ---,  Xi  ..-  X k in  P,  where  k  >  2, 
add  to  P'  the  following  set  of productions.  We  let  X',  stand  for  X~.  if  X,. is  in 
N,  and  let  X',. be  a  new  nonterminal  if X;  is  in  E. 

A  > X ' i ( X z . . .   Xk> 
< X , . . .   X,>  >x~<x,  ...  x,> 

<x,_~  ... x,>  > x;_~<x,_,x~> 

<x,_,x,>  > X~_lX; 

where  each  ( X  i  .--  Xk)  is  a  new  nonterminal  symbol. 

(5)  For  each  production  of  the  form  A  -->  X 1 X  2,  where  either  X1  or  X 2 
or  both  are  in  E,  add  to  P'  the  production  A  -->  X'iX'z. 

(6)  For  each  nonterminal  of the  form  a'  introduced  in  steps  (4)  and  (5), 
add  to  P'  the  production  a'  ---~ a.  Finally,  let  N'  be  N  together  with  all  new 

nonterminals  introduced  in  the  construction  of  P'.  Then  our  desired  gram- 
mar  is  G'  --  (N',  E, P',  S).  [~ 

THEOREM 2.17 

Let  L  be  a  CFL.  Then  L  --  L(G)  for  some  C F G   G  in  Chomsky  normal 
form. 

Proof.  By  Theorem  2.16,  L  has  a  proper  grammar.  The  grammar  G'  of 
Algorithm  2.12  is  clearly  in  CNF.  It  suffices  to  show  that  in  Algorithm  2.12, 
L ( G ) -   L(G').  This  statement  follows  by  an  application  of  Lemma  2.14  to 

SEC. 2.4  CONTEXT-FREE LANGUAGES  15:3 
each  production  of  G'  with  a  nonterminal  a',  and  then  to  each  production 
with  a  nonterminal  of  the  form  (Xt  -..  Xj).  The  resulting  grammar  will 
beG.  [Z] 

Example  2.26 

Let  G  be  the  proper  C FG   defined  by 

S  > a A B I B A  
A  .  > B BB  [ a 

B  > ASIb 

We  construct  P'  in  Algorithm  2.12  by  retaining  the  productions  S  ---~ BA, 
A  ~  a,  B  ~  AS,  and  B  ~  b.  We  replace  S  ~  aA B  by  S  ~  a ' ( A B )   and 

( A B )   ~  AB.  A  ~  B BB   is  replaced  by  A  ---~ B ( B B )   and  ( B B ) ~   BB. 

Finally,  we  add  a'--~  a.  The  resulting  grammar  is  G ' =   (N',  {a, b}, P',  S), 
where  N'  =  [S,  A,  B,  ( A B ) ,   ( B B ) ,   a'}  and  P'  consists  of 

S -   > a ' ( A B )   I BA 
A  > B<BB)  Ia 

B  > A S I b  
(AB>  ,  AB 

<BB>  ,  nB 

a'  > a  [Z] 

2.4.4.  Greibach  Normal  Form 

We next show that  it is possible  to  find for each  CFL  a grammar  in  which. 
every  production  has  a  right  side  beginning  with  a  terminal.  Central  to  the 
construction  is  the  idea  of left recursion  and  its  elimination. 

DEFINITION 

A  nonterminal  A  in  C FG   G  =  (N,  ~, P,  S)  is  said  to  be  recursive  if 
+ A  ~  ~Afl  for some  ~  and  ft.  If ~ =  e, then  A  is said to be  left-recursive.  Simi- 

larly,  if  fl  =  e,  then  A  is  right-recursive.  A  grammar  with  at  least  one  left- 

(right-)  recursive  nonterminal  is said  to  be  left-  (right-)  recursive.  A  grammar 
in  which  all  nonterminals,  except  possibly  the  start  symbol,  are  recursive  is 

said  to  be  recursive. 

Certain  of  the  parsing  algorithms  which  we  shall  discuss  do  not  work 
with  left-recursive  grammars.  We  shall  show  that  every context-free  language 

has  at  least  one  non-left-recursive  grammar.  We  begin  by  showing  how  to 
eliminate  immediate  left  recursion  from  a  CFG. 

154  ELEMENTS OF LANGUAGE THEORY  CHAP. 2 
LEMMA 2.15 

Let  G =  (N,  Z, P,  S) be  a  C FG   in which 

a  ~  A ~   IA~2i  . . .   I A ~   IP~ I/~  I'''  I/L 
are  all  the  A-productions  in  P  and  no  fl~  begins  with  A.  Let 

G ' =   (N  U  [A'},  Z, P',  S), 
where P '   is P  with  these  productions  replaced  by 

A  > P~ I/ ~ I' "  IP, lP~A'I/~A'I...  I/LA' 
A'  > ~ 1 ~ ! " "   I ~ l ~ a ' l ~ a ' l , . ,   IO~mA' 

A'  is  a  new nonterminal  not  in  N.t  Then  L(G')  =  L(G). 

Proof  In  G, the  strings  which  can  be  derived  leftmost  from  A  using  only 
A-productions  are  seen  to  be  exactly  those  strings  in  the  regular  set 

(Pl  -q-  P2  +  "'"  "2U P,)(O~I  +  ~2  +  "' '  -3U ~m)*"  These  are  exactly  the  strings 
which  can  be  derived  rightmost  from  A  using  one  A-production  and  some 

number  of A'-productions  of  G'.  (The  resulting  derivation  is  no  longer  left- 
most.)  All  steps  of the  derivation  in  G that  do  not  use  an  A-production  can 

be done  directly in G', since the non-A-productions  of  G and  G' are the  same. 
We  conclude  that  w is  in  L(G)  and  that  L(G')  ~  L(G). 

For  the  converse,  essentially  the  same  argument  is  used.  The  derivation 
in  G'  is  taken  to  be  rightmost,  and  sequences  of  one  A-production  and  any 

number  of  A'-productions  are  considered.  Thus,  L(G) =  L(G').  D 

The  effect  of  the  transformation  in  Lemma  2.15  on  derivation  trees  is 
shown  in  Fig.  2.13. 

Example 2.27 

Let  Go  be  our  usual  grammar  with  productions 

E  > E + T I T  

T  > T , F I F  
F  > (E) la 

The  grammar  G'  with  productions 

E  > TITE' 
E'  > +  T I + T E '  

T  > FIFT' 

tNote  that the A ~  p{s  are in the initial and final sets of A-productions. 

SEC. 2.4  CONTEXT-FREE LANGUAGES  155 

A / \  
A  Oql / \  
.,4  oq2 

A" / \  
A  aik / 

A / \  

A' / \  
~ik  A ' / \  

Oqk-  I  A" / \ .  

*o 

A' / \  

oq2  A ' \ 

oq 1 
(a) Portion of tree in G  (b) Corresponding portion  in  G' 

Fig. 2.13  Portions  of trees. 

T '-   > , F [ , F T '  

F  > ( E) l a 

is  equivalent  to  Go  and  is  the  one  obtained  by  applying  the  construction  in 

Lemma  2.15  with  A  --  E  and  then  A  --  T.  [-] 

We  are  now  ready  to  give  an  algorithm  to  eliminate  left  recursion  from 
a  proper  CFG.  This  algorithm  is  similar  in  spirit  to  the  algorithm  we  used 
to  solve  regular  expression  equations. 

i ALGORITHM 2.13 

Elimination  of left  recursion. 
Input.  A  proper  CF G  G  --  (N,  X, P,  S). 

Output.  A  C F G  G'  with  no  left  recursion. 
Method. 

(1)  Let  N  =  [ A I , . . . ,   A,}.  We  shall  first  transform  G  so  that  if  At  ---~ 
is  a  production,  then  a  begins  either  with  a  terminal  or  some  Aj  such  that 
j  >  i.  For  this  purpose,  set  i  =  1. 

(2)  Let  the  Arproductions  be  A~--~  Atal  1 " "   I AtO~m fl~ I "   "lflp,  where 
no  ,81 begins  with  A k if k  <  i.  (It will  always  be  possible  to  do  this.)  Replace 
these  Arproductions  by 

A,  > l~,i...  IB~IP, A',I""  llbA~ 

156  ELEMENTS  OF  LANGUAGE  THEORY  CHAP. 2 
where  A'~ is a  new variable.  All the  At-productions  now  begin with  a  terminal 

or  Ak for  some  k  >  i. 

(3)  If  i  =  n,  let  G'  be  the  resulting  grammar,  and  halt.  Otherwise,  set 
i = i +   1 and j =   1. 

(4)  Replace  each  production  of the  form  A t ---~ A~0~, by  the  productions 
A,---> fll0~! " "   [ flm0~,  where  Aj  ---~ fl~ ] . . .   [tim  are  all the  Approductions.  It 
will  now  be  the  case  that  all  Afproductions  begin  with  a  terminal  or  A k, 
for  k  >  j,  so  all  At-productions  will  then  also  have  that  property. 

(5)  If j =   i -   1, go to step (2).  Otherwise,  setj =j-Jr-  1 and go to step (4). 

D 
THEOREM 2.18 

Every  CFL  has  a  hon-left-recursive  grammar. 
Proof.  Let  G  be  a  proper  grammar  for  CFL  L.  If  we  apply  Algorithm 
2.13,  the  only  transformations  used  are  those  of  Lemmas  2.14  and  2.15. 

Thus  the  resulting  G'  generates  L. 

We  must  show  that  G'  is  free  of left  recursion.  The  following  two  state- 
ments  are  proved  by  induction  on  a  quantity  which  we  shall  subsequently 

define: 

(2.4.4)  After  step  (2)  is  executed  for  i,  all  At-productions  begin  with 

a  terminal  or  A k, for  k  >  i 

(2.4.5)  After  step  (4) is executed  for  i and j,  all At-productions  begin with 

a  terminal  or  A u,  for  k  >  j 

We  define, the  score  of  an  instance  of  (2.4.4)  to  be  ni.  The  score  of  an 
instance  of  (2.4.5)  is  ni  +  j.  We  prove  (2.4.4)  and  (2.4.5)  by  induction  on 
the  score  of an  instance  of these  statements. 

Basis  (score  of  n)"  Here  i  =  1  and  j  =  0.  The  only  instance  is  (2.4.4) 
with  i  =  1.  None  of  ill,-  * . ,   ft,  in  step  (2)  can  begin  with  A1,  so  (2.4.4)  is 
immediate  if i  =  1. 

Induction.  Assume  (2.4.4)  and  (2.4.5)  for  scores  less  than  s,  and  let  i 
and j  be such that 0  <  j  <  i ~  n and ni  +  j  =  s.  We shall prove this instance 
of  (2.4.5).  By  inductive  hypothesis  (2.4.4),  all  A j-productions  begin  with 
a  terminal  or  A k,  for  k  >  j.  [This  follows  because,  if j  >  1,  the  instance  of 
(2.4.5)  with  parameters  i  and j  --  1 has  a  score  lower  than  s.  The  case j  =  1 
follows from  (2.4.4).]  Statement  (2.4.5)  with  parameters  i and j  is thus  imme- 

diate  from  the  form  of the  new  productions. 

An  inductive  proof  of  (2.4.4)  with  score  s  (i.e.,  ni  =  s,  j  =  0)  is  left  for 
the  Exercises. 

It  follows  from  (2.4.4)  that  none  of  A 1 . . . .   , A,  could  be  left-recursive. + 

Indeed,  if A t ==> At0c  for  some  0~,  there  would  have  to  be  Aj  and  A k  with lm 

k  <  j  such that  A~ =-~ Ajfl  ==~ Ak?  =-~ Aloe.  We  must  now  show  that  no  A't lm  lm  lm 

SEC. 2,4  CONTEXT-FREE LANGUAGES  157 

introduced  in  step  (2)  can  be  left-recursive.  This  follows  immediately  from 

the  fact  that  if A~  ~  A)~, is  a  production  created  in  step  (2),  then j  <  i since 

A~ is  introduced  after  A). 

Example 2.28 

Let  G  be 

A  ~ BCIa 

B---->  CA IAb 

C  > AB[  CC[ a 

We  take  A t  =  A,  A z =  B,  and  A 3 =  C.  The  grammar  after  each  appli- 
cation  of step  (2)  or  step  (4)  of Algorithm  2.13  is  shown  below.  At  each  step 
we  show  only  the  new  productions  for  nonterminals  whose  productions 

change. 

Step  (2)  with  i  --  1:  no  change. 
Step  (4)  with  i - -   2, j - -   1:  B  ----, CA]BCb]ab 
Step  (2)  with  i  =  2 : B   --~  CA[ab[CAB'IabB' 

B'  --~  CbB' l Cb 

Step  (4) with  i =   3 , j   =  1:  C  ~  BCB]aB]CC]a 

Step  (4)  with  i  =  3, j  =  2: 

C  ~  CA CBI abCB[  CAB' CBI abB'CB[aB[  CCI a 

Step  (2)  with  i =  3: 

C  ---~ abCBI abB' CBI aBI a l abCBC'  [ abB'CBC'  I aBC'I aC' c'  ~  A CBC'  I AB' CBC'  I CC'  I A CB  I AB' CB  I C 

An  interesting  special  case  of  non-left  recursiveness  is  Greibach  no[mal 
form. 

DEFINITION 

A  CF G  G  =  (N,  E, P,  S)  is  said.  to  be  in  Greibach  normal form  (GNF) 
if  G  is  e-free  and  each  non-e-production  in  P  is  of  the  form  A  --~  aa  with 
a  ~  E a n d a   ~  N*. 

If a  grammar  is not  left-recursive,  then  we can  find  a  natural  partial  order 
on  the  nonterminals.  This  partial  order  can  be  embedded  in  a  linear  order 
which  is  useful  in  putting  a  grammar  into  Greibach  normal  form. 

LEMMA 2.16 

Let  G  =  (N,  ~, P,  S)  be  a  non-left-recursive  grammar.  Then  there  is 
a  linear  order  <  on  N  such  that  if A  --~ Ba  is  in P,  then  A  <  B. 

+ Proof.  Let  R  be  the  relation  A  R  B  if  and  only  if  A =~  Ba  for  some  a. 

158  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 
By  definition  of  left  recursion,  R  is  a  partial  order.  (Transitivity  is  easy  to 
show.)  By  Algorithm  0.1,  R  can  be  extended  to  a  linear  order  <  with  the desired  property.  D 

ALGORITHM 2.14 

Conversion  to  Greibach  normal  form. 
Input.  A  non-left-recursive  proper  C FG   G  =  (N,  E, P,  S). 

Output.  A  grammar  G' in  G N F   such  that  L(G)  =  L(G'). 
Method. 
(1)  Construct,  by  Lemma  2.16,  a  linear  order  <  on  N  such  that  every 
A-production  begins  either  with  a  terminal  or  some  nonterminal  B  such 
that  A  <  B.  Let  N  --  [ A ~ , . . . ,   A,},  so  that  A~  <  A2  . . .   <  A,. (2)  Set  i =   n -   1. 

(3)  If i  --  0, go to  step (5). Otherwise,  replace each production  of the form 
A,  ~  Ajoc,  where  j  >  i,  by  A~ ---, fl~0~[ . . .   [ fl,,~z,  where  Aj  ---,  ,8~ ] . . .   l fl,, are  all  the  Alproductions.  It  will  be  true  that  each  of  , 8 ~ , . . . ,   tim  begins 

with  a  terminal. 

(4)  Set  i  =  i -   1 and  return  to  step  (3). 
(5)  At  this  point  all  productions  (except  possibly  S----.  e)  begin  with 
a  terminal.  For  each  production,  say  A - - .   a X e . . .   Xk,  replace  those  Xj which  are  terminals  by  X' 

j,  a  new  nonterminal. 
(6)  For  all  X~. introduced  in  step  (5),  add  the  production  X~ --~  Xj.  [-] 

THnOREM 2.19 

If L  is  a  CFL,  then  L  --  L(G)  for  some  G in  GNF. 
Proof  A  straightforward  induction  on n  --  i (that  is,  backwards,  starting 
at  i - - n -   1  and  finishing  at  i  =  1)  shows  that  after  applying  step  (3)  of 
Algorithm  2.13  for  i all  At-productions  begin  with  a  terminal.  The  property 

of the  linear  order  <  is  crucial  here.  Step  (5)  puts  the  grammar  into  GNF, 
and  by  Lemma  2.14  does  not  change  the  language  generated.  [--] 

Example 2.29 

Consider  the  grammar  G with  productions 

E  > TI TE'  
E'  > +T]  +T E'  

T  > FIFT' 
T'  > , F]  ,FT' 

F  > (E)  ia 

Take  E'  <  E  <  T'  <  T  <  F  as  the  linear  order  on  nonterminals. 

SEC. 2.4  CONTEXT-FREE LANGUAGES  159 

All  F-productions  begin  with  a  terminal,  as  they  must,  since  F  is  highest 
in  the  order.  The  next  highest  symbol,  T,  has  productions  T---,  F[ FT',  so 
we  substitute  for  F  in  both  to  obtain  T ~  (E )la [ (E)T'  [aT'.  Proceeding  to 

T',  we  find  no  change  necessary.  We  then  replace  the  E-productions  by 
E---,  ( E ) I a I ( E ) T ' I a T ' I ( E ) E ' I a E ' I ( E ) T ' E ' I a T ' E ' .   No  change for  E'  is  nec- 
essary. Steps  ( 5 ) an d  (6)  introduce  a  new  nonterminal  )'  and  a  production 

)'--~  ).  All  instances  of  )  are  replaced  by  )',  in  the  previous  productions. 

Thus  the  resulting  G N F   grammar  has  the  productions 

g  > ( E ) ' I a I ( E ) ' T ' I a T ' I ( E ) ' E ' I a E ' I ( E ) '   T'E']aT'E' 
E'  ~ + T I   + T E '  

T  ~ ( E ) ' I a I ( E ) ' T ' I a T '  
T'  ~ , F   I , FT '  

r  ~ (E)' I a )'  ---~) 

One  undesirable  aspect  of  using  this  technique  to  put  a  grammar  into 
G N F  is  the  large  number  of  new  productions  created.  The  following  tech- 
nique  can  be  used  to  find  a  G N F   grammar  without  introducing  too  many 

new  productions.  However,  this  new  method  may  introduce  more  nonter- 
minals. 

2.4.5.  An  Alternative  Method  of  Achieving 

Greibach  Normal  Form 

There  is  another  way  to  obtain  a  grammar  in  which  each  production  is 
of  the  form  A  ~  a~.  This  technique  requires  the  grammar  to  be  rewritten 
only  once.  Let  G  =  (N,  Z, P,  A)  be  a  C FG  which  contains  no  e-productions 
(not  even  A  - ~  e)  and  no  single  productions. 

Instead  of  describing  the  method  in  terms  of  the  set  of  productions  we 
shall  use  a  set  of defining  equations,  of the  type  introduced  in  Section  2.2.2, 
to  represent  the  productions.  For  example,  the  set  of  productions 

A  > Aa BI BB [b  

B  ~ aA [BAatBdlc 

can  be  represented  by  the  equations 

A  =  A aB  q-- BB  -+- b 
(2.4.6)  B  =  a A  -Jr- B A a  -a t- B d  -+- c 

where  A  and  B  are  now  indeterminates  representing  sets. 

160  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 
DEFINITION 

Let  A  and  X  be  two  disjoint  alphabets.  A  set  of  defining  equations  over 
and  A  is  a  set  of equations  of  the  form  A . =   o~  +  ~2  +  "'"  +  OCk, where 
A  ~  A  and  each  0~ is  a  string  in  (A  u  X)*.  If  k  =  0,  the  equation  is  taken 

to  be  A  =  ~ .   There  is  one  equation  for  each  A  in  A.  A  solution to  the  set  of 
defining  equations  is  a  function f  from  A  to  6'(Z*)  such  that  ill(A)  is  sub- 

stituted  everywhere  for  A,  for  each  A  ~  A,  then  the  equations  become  set 
equalities.  We  say  that  solution f  is  a  minimalfixed point  ill(A)  ~  g(A)  for 

all  .4  ~  A  and  solutions  g. 

We  define  a  C FG   corresponding to  a  set of defining  equations  by creating 
the  productions  A  --~ 0~ [0c2[ . . .   10Ok for  each  equation  A  =  ~i  +  " "   +  ~zk. 

The  nonterminals  are  the  symbols  in  A.  Obviously,  the  correspondence  is 

one-to-one.  We  shall  state  some  results  about  defining  equations  that  are 
generalizations  of  the  results  proved  for  standard  form  regular  expression 
equations  (which  are  a  special  case  of  defining  equations).  The  proofs  are 
left  for  the  Exercises. 

LEMMA  2.17 

The  minimal  fixed  point  of  a  set  of  defining  equations  over  A  and  X  is 
unique  and  is  given  by f ( A ) =   [wlA  ==> w  with  w  ~  X*}, where  G  is  the G 
corresponding  CFG. 

Proof.  Exercise.  [Z] 

We shall employ a matrix  notation  to  represent  defining equations.  Let us 
assume  that  A  =  [A 1, A 2 , . . . ,   A,}.  The  matrix  equation 

d  =  d R   +  _B 
represents  n  equations.  Here  _A is  the  row  vector  [A 1, A2,  ..  *,  A,],  R  is  an 
n  *  n  matrix  whose  entries  are  regular  expressions,  and  _B is  a  row  vector 
consisting  of n  regular  expressions. 

We  take  "scalar"  multiplication  to  be  concatenation,  and  scalar  addition 
to  be  +  (i.e.,  union).  Matrix  and  vector  addition  and  multiplication  are 
defined  as  in the  usual  (integer,  real,  etc.)  case.  We  let  the  regular  expression 
in  row j,  column  i  of  R  be  ~  +  . . .   +  ~k,  if  A j ~ i , . . . ,   Aim k  are  all  terms 
with  leading  symbol  A~ in  the  equation  for  A~.  We  let  the jth  component  of 
B  be  those  terms  in  the  equation  for  A j  which  begin  with  a  symbol  of  X. 

Thus,  Bj  and  Rio  are  those  expressions  such  that  the  productions  for  A~ 
can  be  written  as 

A j - -   A tR l j  -+- A2R2s  +  . . .  +  AtRtj  +  . . .   -+- A,,R,j  +  B i 
where  Bj  is  a  sum  of expressions  beginning  with  terminals. 

Thus  the  defining  equations  (2.4.6)  would  be  written  as 

SEC. 2.4  CONTEXT-FREE  LANGUAGES  161 

IaB B  f~  l + [ b ,   a A + c ]  (2.4.7)  [A,  B]  =  [A, B]  Aa  +  d 
We  shall  now  find  an  equivalent  set  of  defining  equations  for  d  = d R +   _B 

such  that  the  new  set  of  defining  equations  corresponds  to  a  set 
of productions  all  of whose  right  sides  begin  with  a  terminal  symbol. 

The  transformation  turns  on  the  following  observation. 

LEMMA 2.18 

Let  A  =  dR   /  B  be  a  set  of defining  equations.  Then  the  minimal  fixed 
point  is  d  =  BR*,  where  R*  =  I +   R  /  R 2 +  R 3 +  . . . .   I  is  an  identity 
matrix  (e  along  the  diagonal  and  ~  elsewhere),  R 2 =  RR,  R a =  RRR,  and 

so  forth. 

Proof.  Exercise. 

If  we  let  R / =  RR*,  then  we  can  write  the  minimal  fixed  point  of  the 
equations  _A =  dR  +  _B as  _A =  _B(R + +  I ) =   _BR + -+- BI  =  _BR + +  _B.  Un- 
fortunately,  we  cannot  find  a  corresponding  grammar  for  these  equations; 

they  are  not  defining  equations,  as  the  elements  of R / may  be  infinite  sets  of 
terms.  However,  we  can  replace  R / by  a  new  matrix  of  "unknowns."  That 
is,  we  can  replace  R /  by  a  matrix  Q  with  qt~  as  a  new  symbol  in  row  i, 
column j. 

We  can  then  obtain  equations  for  the  q~j's by  observing  that  R + =  RR + +  R. 

Thus,  Q  =  RQ  /  R  is  a  set  of  defining  equations  for  the  q~j's. 
Note  that  there  are  n 2 equations  if  Q  and  R  are  n  *  n  matrices.  The  fol- 
lowing  lemma  relates  the  two  sets  of equations. 

LEMMA 2.19 

Let  d  =  dR  +  _B be  a  set  of defining  equations  over  A  and  X.  Let  Q  be 
a  matrix  of  the  size  of  R  such  that  each  component  of  Q  is  a  unique  new 
symbol.  Then  the  system  of defining  equations  represented  by  d  =  _BQ +  _B 
and  Q  =  R Q  +  R  has  a  minimal  fixed  point  which  agrees  on  A  with  that  of 

d  =  dR   +  _B. 

Proof.  Exercise.  E] 

We  now  give  another  algorithm  to  convert  a  proper  grammar  to  GNF. 

ALCORITHM 2.15 

Conversion  to  Greibach  normal  form. 
Input.  A  proper  grammar  G  =  (N,  X, P,  S)  such  that  S  ~  e  is  not  in  P. 

Output.  A  grammar  G'  ---- (N',  X, P',  S) in  GNF. 
Method. 
(1)  From G, write the corresponding set of  defining equations A =  _AR +  _B 
over  N  and  X. 

162  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

(2)  Let  Q be an n  x  n matrix  of new symbols, where  @N  =  n.  Construct 
the  new  set  of  defining  equations  d  =  _BQ-Jr _B,  Q  =  RQ  q-R,  and  let 

G 1 be  the  corresponding  grammar.  Since  every  term  in  B  begins  with  a  ter- 
minal,  all  A-productions  of  G 1, for  A  ~  N,  will  begin  with  terminals. 

(3)  Since  G is proper,  e  is  not  a  coefficient  in  R.  Thus  each  q-production 
of G1, where q is a component  of  Q, begins with a  symbol in N  u  Z.  Replace 
each  leading  nonterminal  A  in  these  productions  by  all  the  right  sides  of 
the  A-productions.  The  resulting  grammar  has  only productions  whose  right 
sides  begin  with terminals. 

(4)  For  each  terminal  a  appearing  in  a  production  as  other  than  the  first 
symbol  on  the  right  side,  replace  it  by  new  nonterminals  of the  form  a'  and 
add  production  a'  --~ a.  Call  the  resulting  grammar  G'.  [Z 

THEOREM 2.20 

Algorithm  2.15  yields  a  grammar  G'  in  GNF,  and  L(G)  =  L(G'). 
Proof.  That  G'  is in  G NF  follows  from  the  properness  of G.  That  is,  no 
component  of  _B or  R  is  e.  That  L(G')  =  L(G)  follows  from  Lemmas  2.14, 
2.17,  and  2.19.  D 

Example 2.30 

Let  us  consider  the  grammar  whose  corresponding  defining  equations 
are  (2.4.7),  that  is, 

A  ~ AaBIBBIb 

B  ~ aA I BAaIBd[  c 

We  rewrite  these  equations  according  to  step  (2)  of Algorithm  2.15  as 

We  then  add  the  equations 
(2.4.9)  I~  ZXI=fa:  Z~  EaBB *  A a q - - d l I ~   zX]  -q-  A a q - d l  
The  grammar  corresponding  to (2.4.8)  and  (2.4.9)  is 

A  > b WI an YI c YI b 

B  > b X l a A Z l c Z l a A l e  
W  ~ aBWla11 

X  > aBX 

Y  > B W [ A a Y [ d Y [ B  
z  ~nXlAaZldZlAa[d 

EXERCISES  163 
Note  that  X  is  a  useless  symbol.  In  step  (3),  the  productions 

Y - - ,   BWI  Aa YI B  and  Z  --~ B X  I AaZt  Aa  are  replaced  by  substituting  for 
the  leading  A's  and  B's.  We  omit  this  transformation,  as  well  as  that  of 

step  (4),  which  should  now  be  familiar  to  the  reader.  [~ 

EXERCI SES 
2.4.1.  Let  G be defined by 

S---->AB 
A  --->  AalbB 

B  ~ al Sb 

Give derivation  trees for the  following sentential forms: 
(a)  baabaab. 

(b)  bBABb. 
(c)  baSb. 

2.4.2.  Give  a  leftmost  and  rightmost  derivation  of the  string  baabaab  in  the 

grammar  of Exercise 2.4.1. 

2.4.3.  Give all cuts  of the  tree of Fig.  2.14. 

nl 

n4 
n5  n7  Fig. 2.14  Unlabelled  derivation tree. 

2.4.4.  Show that  the  following are equivalent statements  about  a  CFG  G and 

sentence  w: 

(a)  w is the frontier  of two  distinct  derivation trees of G. 
(b)  w has two  distinct leftmost derivations in  G. 
(c)  w has two  distinct rightmost derivations  in  G. 

**2.4.5.  What  is  the  largest  number  of  different  derivations  that  are  represen- 

table  by the same  derivation tree of n nodes ? 

2.4.6.  Convert the grammar 

S  > AI B 
A  > aBlbSlb 

B  ----~. ABIBa 

C---->. AS[b 

to  an equivalent  CFG  with  no  useless symbols. 

164  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

2.4.7.  Prove  that  Algorithm  2.8  correctly  removes  inaccessible  symbols. 
2.4.8.  Complete the proof of Theorem 2.13. 
2.4.9.  Discuss the time and space complexity of Algorithm 2.8.  Use a  random 

access computer model. 

2.4.10.  Give  an  algorithm  to  compute  for  a  CFG  G  =  (N,  E, P, S)  the  set  of . 

A  ~  N  such  that  A :=~ e.  How  fast  is  your  algorithm ? 
2.4.11.  Find  an  e-free grammar equivalent to  the following" 

S  > ABC 
A  -----~  BBIe 

B  > CCIa 
C  >AAIb 

2.4.12.  Complete the proof of Theorem  2.14. 
2.4.13.  Find  a  proper  grammar equivalent to  the following" 

S-----> A I B  
A----->  CID 

B----~  D I E  
C---->.  S la l e  
1 9   > Slb 

E----> S l c l e  

2.4.14.  Prove Theorem 2.16. 
2.4.15.  Prove Lemma 2.14. 
2.4.16.  Put  the following grammars in  Chomsky normal form" 

(a)  S  ~  0S1 t01. 
(b)  S  ---~ aBIbA 

A  ---~ aS IbAAla 
B  ---,  bSlaBBIb. 

k 2.4.17.  If  G  =  (N,  ~, P,  S)  is  in  CNF,  S :=~ w,  I w[  =  n,  and  w is  in  ~*,  what 

G is k? 

2.4.18.  Give  a  detailed proof of Theorem  2.17. 

2.4.19.  Put  the grammar 

into  GNF 

(a)  Using Algorithm 2.14. 

(b)  Using Algorithm  2.15. 

S  ~ BalAb 
A  ~  Sa[AAb[a 

B  ~  SblBBalb 

EXERCISES  165 
*2.4.20.  Give  a  fast  algorithm  to  test  if a  C F G   G  is  left-recursive. 

2.4.21.  Give  an  algorithm  to  eliminate right  recursion  from  a  CFG. 

2.4.22.  Complete  the  proof  of Lemma  2.15. 
*2.4.23.  Prove  Lemmas  2.17-2.19. 

2.4.24.  Complete  Example  2.30  to  yield  a  proper  grammar  in  G N F. 

2.4.25.  Discuss  the  relative  merits  of Algorithms  2.14  and  2.15,  especially  with 

regard  to  the  size  of the  resulting  grammar. 

*2.4.26.  Show  that  every  CFL  without  e  has  a  grammar  where  all  productions 

are  of  the  forms  A  ~  aBC,  A  ---~ aB,  and  A  ----~ a. 

DEFINITION 

A  C F G   is  an  operator  grammar  if  no  production  has  a  right  side 
with  two  adjacent  nonterminals. 

*2.4.27.  Show  that  every  CFL  has  an  operator  grammar.  Hint:  Begin  with  a 

G N F   grammar. 

*2.4.28.  Show  that  every  CFL  is  generated  by  a  grammar  in  which  each  pro- 

duction  is  of  one  of  the  forms 

A  ~  aBbC,  A  ~  aBb,  A  ~  aB,  or  A ~  a 
If e  ~  L(G),  then  S  ---~ e  is  also  in P. 
**2.4.29.  Consider  the  grammar  with  the  two  productions  S---~  SS[a.  Show 

that  the  number  of  distinct  leftmost  derivations  of  a n  is  given  by 

X~  =  ~t+j=,  ~X~,  where  X1  =  1.  Show  that i~0 

j~0  n) 

Xn+l  =  n  +  1 

(These  are  the  Catalan  numbers.) 
*2.4.30.  Show  that  if L  is  a  CFL  containing  no  sentence  of  length  less  than  2, 

then L  has  a  grammar  with  all productions  of the  form  A  ---~ aocb. 

2.4.31.  Show  that  every  CF L  has  a  grammar  in  which  if  X1 X2  . ..   Xk  is  the 

right  side  of  a  production,  then  XI . . . . .  Xk  are  all  distinct. 

DEFINITION 

A  C F G   G  =  (N,  E, P,  S)  is  linear  if every  production  is  of the  form 
A  ~  wBx  or  A  ---~ w  for  w  and  x  in  Z*  and  B  in  N. 

2.4.32.  Show  that  every  linear  language  without  e  has  a  grammar  in  which 

each  production  is  of  one  of  the  forms  A  ~  aB,  A  ~  Ba,  or  A  ~  a. 

*2.4.33.  Show  that  every  C FL  has  a  grammar  G  ---- (N,  E, P,  S)  such  that  if  A , 

is  in  N  --  [S],  then  [w lA   :=~  w  and  w  is  in  Z*}  is  infinite. 
2.4.34.  Show  that  every  CFL  has  a  recursive  grammar.  Hint:  Use  Lemma  2.14 

and  Exercise  2.4.33. 

166  ELEMENTS OF LANGUAGE THEORY  CHAP. 2 

*2.4.35.  Let  us  call a  CFG  G  =  (N,  ~, P, S)quasi-linear  if for  every production 

A  ---,  Xa  ...  Xk  there  is  at  most  one  X~ which  generates  an  infinite  set 

of  terminal  strings.  Show  that  every  quasi-linear  grammar  generates 
a  linear language. 

DEFINITION 

2.4.36. 

The graph of a  CFG  G  =  (N,  Z, P,  S) is a  directed unordered graph 
(N  u  ~  u  {e}, R)  such  that  A  R  X  if  and  only  if A  ----~ ocXfl  is  a  pro- 
duction  in P  for  some 0c and  ft. 

Show  that  if a  grammar  has  no  useless  symbols,  then  all nodes  are  ac- 
cessible from  S.  Is the converse  of this  statement  true ? 

2.4.37.  Let  T  be  the  transformation  on  context-free  grammars  defined  in 

Lemma  2.14.  That  is,  if G  and  G'  are the  grammars  in  the statement  of 
Lemma  2.14,  then  T  maps  G  into  G'.  Show  that  Algorithms  2.10  and 

2.11  can  be  implemented  by  means  of  repeated  applications  of  this 
transformation  T. 

Programming  Exercises 

2.4.38.  Construct  a  program  that  eliminates  all  useless  symbols  from  a  CFG. 
2.4.39.  Write  a  program  that  maps  a  CFG  into  an  equivalent  proper  CFG. 
2.4.40.  Construct  a  program  that  removes  all left recursion from  a  CFG. 
2.4.41.  Write  a  program  that  decides whether  a  given derivation tree  is a  valid 

derivation tree for  a  CFG. 

B I B L I O GR AP H I C   N O T E S  
A  derivation  tree  is  also  called  a  variety  of  other  names  including  generation 
tree,  parsing  diagram,  parse  tree,  syntax  tree,  phrase  marker,  and p-marker.  The 
representation  of  a  derivation  in  terms  of  a  derivation  tree  has  been  a  familiar 
concept  in  linguistics.  The  concept  of  leftmost  derivation  appeared  in  Evey 

[1963]. 

Many  of the  algorithms  in this chapter have been known  since the early  1960's, 
although  many  did  not  appear  in  the  literature  until  considerably  later.  Theorem 
2.17  (Chomsky  normal  form)  was  first  presented  by  Chomsky  [1959a].  Theorem 
2.18  (Greibach  normal  form)  was  presented  by  Greibach  [1965].  The  alternative 
method  of  achieving  GNF  (Algorithm  2.15)  and  the  result  stated  in  Exercise 

2.4.30  were  presented by Rosenkrantz  [1967]. Algorithm 2.14 for  Greibach  normal 
form has  been  attributed  to  M.  Paull. 

Chomsky  [1963],  Chomsky  and  Schutzenberger  [1963],  and  Ginsburg  and  Rice 
[1962]  have  used  equations  to  represent  the  productions  of  a  context-free  gram- 
mar. 

Operator  grammars  were  first  considered  by  Floyd  [1963]. The  normal  forms 
given  in  Exercises  2.4.26-2.4.28  were  derived  by  Greibach  [1965]. 

SEC. 2.5  PUSHDOWN  AUTOMATA  167 
2.5.  PUSHDOWN  AUTOMATA 

We now  introduce  the  pushdown  automaton--a  recognizer  that  is a natu- 
ral  model  for  syntactic  analyzers  of  context-free  languages.  The  pushdown 
automaton  is  a  one-way  nondeterministic  recognizer  whose  infinite  storage 
consists  of  one  pushdown  list,  as  shown  in  Fig.  2.15. 

a2 

L  Read  only 

*  .-   art  input  tape 

! [ Filit  
state 
control  Zi 

Z2 

Zm 

Pushdown 

list 

Fig.  2.15  Pushdown  automaton. 
We  shall  prove  a  fundamental  result  regarding  pushdown  automata-- 
that  a  language  is  context-free  if and  only  if it  is  accepted  by a  nondetermin- 
istic  pushdown  automaton.  We  shall  also  consider  a  subclass  of context-free 
languages  which  are  of  prime  importance  when  parsability  is  considered. 
These,  called  the  deterministic  CFL's,  are  those  CFL's  which  can be  recog- 
nized  by  a  deterministic  pushdown  automaton. 

2.5.1.  The  Basic  Definition 

We  shall  represent  a  pushdown  list  as  a  string  of  symbols  with  the  top- 
most  symbol  written  either  on  the  left  or  on  the  right  depending  on  which 
convention  is  most  convenient  for  the  situation  at  hand.  For  the  time  being 
we  shall  assume  that  the  top  symbol  on  the  pushdown  list  is  the  leftmost 
symbol  of the  string  representing  the  pushdown  list. 

DEFINITION 

A  pushdown  automaton  (PDA  for  short)  is  a  7-tuple 

p  =  (Q,  z,  r ,   3,  q0, z0,  F), 

168  ELEMENTS OF LANGUAGE  THEORY  CHAP.  2. 
where 

(1)  Q  is  a  finite  set  of  :tate  symbols  representing  the  possible  states  of 
the  finite  state  control, 

(2)  2~ is  a  finite  input  alphabet, 

(3)  F  is  a  finite  alphabet  of pushdown  list  symbols, 
(4)  6  is  a  mapping  from  Q  x  (Z  w  {e}) x  F  to  the  finite  subsets  of 
Q x F * ,  

(5)  qo  ~  Q  is  the  initial  state  of  the  finite  control, 
(6)  Z0  E  F  is  the  symbol  that  appears  initially  on  the  pushdown  list 
(the  start  symbol),  and 

(7)  F  ~  Q  is  the  set  of final  states. 

A  configuration  of P  is  a  triple  (q, w, 0c) in  Q  x  ~*  x  F*,  where 

(1)  q  represents  the  current  state  of the  finite  control. 
(2)  w represents  the  unused  portion  of  the  input.  The  first  symbol  of  w 
is  under  the  input  head.  If  w  =  e,  then  it  is  assumed  that  all  of  the  input 
tape  has  been  read. 

(3)  a  represents  the  contents  of the  pushdown  list.  The  leftmost  symbol 
of  ~  is  the  topmost  pushdown  symbol.  If 0c =  e,  then  the  pushdown  list  is 

assumed  to  be  empty. 

A  move by P  will be represented  by the binary relation  I-v- (or  ~  whenever 
P  is understood)  on  configurations.  We  write 

(2.5.1)  (q, aw, ZoO  ~  (q',  w, 7oc) 
if  dr(q, a, Z)  contains  (q',  7')  for  any  q  ~  Q,  a  ~  :E w  [e},  w  ~  Z*,  Z  ~  F, 

and  0~ ~  F*. 

If  a  c-  e,  Eq.  (2.5.1)  states  that  if P  is  in  a  configuration  such  that  the 
finite  control  is  in  state  q,  the  current  input  symbol  is  a,  and  the  symbol  on 
top  of the  pushdown  list  is  Z,  then  P  may  go  into  a  configuration  in  which 
the  finite control  is now in state q', the  input  head has  been  shifted one  square 
to  the  right,  and  the  topmost  symbol  on  the  pushdown  list has  been  replaced 
by the  string 7' of pushdown  list  symbols.  If 7' =  e, we  say that  the  pushdown 
list  has  been popped. 

If  a  =  e,  then  the  move  is  called  an  e-move.  In  an  e-move  the  current 
input  symbol  is  not  taken  into  consideration,  and  the  input  head  is  not 

moved.  However,  the  state  of  the  finite  control  can  be  changed,  and  the 
contents  of  the  memory  can  be  adjusted.  Note  that  an  e-move  can  occur 
even  if all  of the  input  has  been  read. 

No  move  is  possible  if the  pushdown  list  is  empty. 
We  can  define  the  relations  1.4-, for  i  ~  0,  I.-~--, and  [--~- in  the  customary 
fashion.  Thus,  1-~-- and  1-~-- are,  respectively,  the  reflexive-transitive  and  tran- 

sitive  closures  of  l---. 

SEC. 2.5  PUSHDOWN AUTOMATA  169 

An  initial configuration of P  is  one  of  the  form  (qo, w, Zo) for  some  w in lg*.  That  is,  the  finite  state  control  is  in  the  initial  state,  the  input  contains 
the  string  to  be  recognized,  and  the  pushdown  list  contains  only  the  symbol Z0.  A  final  configuration  is  one  of  the  form  (q,  e, ~x), where  q  is  in  FF and 
is  in  F*. 

We  say  that  a  string  w is  accepted by P  if (q0,  w, Z0)I-~-- (q,  e, a0 for  some q  in  F  and  tx  in  r'*.  The  language  defined  by  P,  denoted  L(P),  is  the 

set  of  strings  accepted  by  P.  L(P)  will  be  called  a  pushdown  automaton language. 

Example 2.31 

Let  us  give  a  pushdown  automaton  for  the  language  L  =  [0"l']n  >  0}. 
Let  P  =  ({qo,  ql,  q2}, {0,  1}, {Z, 0}, d~, qo, Z,  {q0}), where 

6(qo,  O, Z)  =  [(q~, OZ)} 

6(q~,  O, O) =  [(q~,  00)} 

,fi(q~, 1, O) =  {(qz,  e)} 
6(q2,  1, O)  =  [(qz,  e)] 
6(q2,  e, Z)  =  {(qo, e)} 

P  operates  by  copying  the  initial  string  of  O's  from  its  input  tape  onto  its pushdown  list  and  then  popping  one  0  from  the  pushdown  list  for  each  1 

that  is  seen  on  the  input.  Moreover,  the  state  transitions  ensure  that  all  O's must precede  the  l's.  For  example,  with  the input  string  001 I, P  would  make 
the  following  sequence  of  moves: 

(qo, 0011, Z)  F-  (q~, 011,  0Z) 

l-  (q~,  l l,  00Z) 
~-  (qz,  1, 0Z) 
F-  (q2,  e, Z) 

(q0,  e,  e) 

In  general  we  can  show  that 

(qo, O, Z)  J---- (q~, e, OZ) 
(q~, 0', OZ) [d__ (q~, e, O'/~Z) 
(ql,  1, 0'+'Z)  [-  (q2, e, 0'Z) 

(q~,  1', 0'Z)! ~  (q~, e, Z) 

(q2,  e, Z)  t--  (q0,  e,  e) 

170  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 
Stringing  all  of this  together,  we  have  the  following  sequence  of moves by P: 

and 
Thus,  L  ~  L(P). 

(qo, 0"1", Z)[2,+1  (qo,  e, e)  f o r n ~   1 

(qo, e, Z)l-q-.  (q0, e, Z) 
Now  we  need  to  show  that  L  ~  L(P).  That  is,  P  will  accept  only  strings 
of the form 0"1". This is the hard part.  It is generally easy to show that a recog- 
nizer  accepts  certain  strings.  As  with  grammars,  it  is  invariably  much  more 

difficult  to  show  that  a  recognizer  accepts  only  strings  of  a  certain  form. 

Here  we  notice  that  if  P  accepts  an  input  string  other  than  e,  it  must 
cycle  through  the  sequence  of  states  q0,  q l,  q2,  q0. 

We  notice  that  if (q0,  w, Z) [A_ (q l,  e,  ~),  i ~  1, then  w  =  0 t and  0c =  0tZ. 
Likewise  if  (q2,  w, 00 [._t_. (q2,  e,  fl),  then  w  =  1 t  and  0~ =  0~fl.  Also, 
(q~,  w, tO  ~  (q2,  e,  fl)  only  if w  =  1 and  ~  =  Off;  (q2,  w, Z ) I ~   (q0, e, e)  only if w  =  e. Thus  if  (q0,  w, Z)I-z-(q0,  e, ~),  for  some  i  _~ 0,  either  w  =  e  and 

i =   0  or  w  =  0"1",  i  =  2n  +  1,  and  0~ =  e.  Hence  L  _p_ L(P).  [Z] 

We  emphasize  that  a  pushdown  automaton,  as  we  have  defined  it,  can 
make moves even though it has  scanned  all of its input.  However,  a pushdown 

automaton  cannot  make  a  move  if its  pushdown  list  is  empty. 

Example 2.32 

Let  us  design  a  pushdown  automaton  for  the  language 

L  --  {wwRI w  ~  {a, b}+]}. 
Let P  =  ([qo,  ql,  q2}, [a,  b}, {Z,  a,  b}, ~,  qo, Z,  {q2]), where 

(1)  $(qo,  a, Z)  =  [(qo,  aZ)} 

(2)  $(qo,  b, Z)  :  [(qo,  bZ)} 

(3)  $(qo,  a,  a)  =  [(qo,  aa),  (q l,  e)} (4)  O(qo, a,  b)  :  ((qo,  ab)} 

(5)  6(qo,  b,  a)  :  {(qo, ba)} 
(6)  ,6(qo, b,  b)  =  ((qo,  bb),  (q l,  e)} (7)  ,6(ql,  a,  a)  :  [(ql,  e)} 

(8)  6(ql,  b, b)  --  [(ql,  e)] (9)  d~(ql, e, Z)  :  [(q2,  e)} 

P  initially  copies  some  of  its  input  onto  its  pushdown  list,  by  rules  (1), 
(2),  (4),  and  (5)  and  the  first  alternatives  of  rules  (3)  and  (6).  However,  P  is 

nondeterministic.  Anytime  it  wishes,  as  long  as  its  current  input  matches 
the  top  of  the  pushdown  list,  it  may  enter  state  q~  and  begin  matching  its 

SEC. 2.5  PUSHDOWN AUTOMATA  171 

pushdown  list  against  the  input.  The  second  alternatives  of rules  (3)  and  (6) 
represent  this  choice,  and  the  matching  continues  by  rules  (7)  and  (8).  Note 
that  if P  ever  fails  to  find  a  match,  then  this  instance  of P  "dies."  However, 

since P  is  nondeterministic,  it makes  all possible  moves.  If any  choice  causes 
P  to  expose  the Z  on  its  pushdown  list,  then  by  rule  (9) that  Z  is erased  and 

state  q2 entered.  Thus P  accepts  if and  only if all matches  are  made. 

For  example,  with  the  input  string  abba,  P  can  make  the  following 
sequences  of moves,  among  others" 

(1)  (q0, abba, Z)  [--- (qo, bba,  aZ) 

(qo,  ba, baZ) 
(qo,  a, bbaZ) (qo, e, abbaZ) 

(2)  (q0, abba, Z)  ~  (qo, bba,  aZ) 

[-- (qo,  ba, baZ) 

(q l, a, aZ) k-  (q~,e,Z) 

k-  (q2, e,e). 
Since  the  sequence  (2)  ends  in  final  state  q2, P  accepts  the  input  string  abba. 

Again  it  is  relatively  easy  to  show  that  if  w =  CxC 2 . ..   c,,c,,c,,_~ . . .   c~, each  ct in  {a, b},  1 <  i <  n,  then 

(q0,  w, Z)  [.2_ (q0, c,,c,,-1  "'"  ci,  CnCn-t "'"  clZ) 

I--(ql,  cn_~  ...  c,,  c,,_~  ...  c~Z) 
I "-'  (q~, e, Z) 
I ~   (q2, e, e). 
Thus,  L  ~  L(P). 

It  is  not  quite  as  easy  to  show  that  if  (q0,  w, Z)I-~--(q2, e, ~)  for  some 
~  I'*,  then  w is  of  the  form  xx R for  some  x  in  (a  +  b) / and  ~  =  e.  This 
proof  is  left  for  the  Exercises.  We  can  then  conclude  that  L ( P ) =   L. 

The  pushdown  automaton  of  Example  2.32  quite  clearly  brings  out  the 
nondeterministic  nature  of  a  PDA.  From  any  configuration  of  the  form 
(qo, aw, a~)  it  is  possible  for  P  to  make  one  of  two  moves---either  push 

another  a  on  the  pushdown  list  or  pop  the  a  from  the  top  of the  pushdown 
list. 

We  should  emphasize  that  although  a  nondeterministic  pushdown 
automaton  may  provide  a  convenient  abstract  definition  for  a  language, 
the  device  must  be  deterministically  simulated  to  be  realized  in  practice.  In 

Chapter  4  we  shall  discuss  systematic  methods  for  simulating  nondetermin- 
istic  pushdown  automata. 

172  ELEMENTS  OF  LANGUAGE  THEORY  CHAP. 2 
2.5.2.  Variants  of  Pushdown  Automata 

In  this  section  we  shall  define  some  variants  of PDA's  and  relate  the  lan- 
guages  defined  to  the  original  PDA  languages.  First  we  would  like  to  bring 

out  a  fundamental  aspect  of  the  behavior  of  a  PDA  which  should  be  quite 
intuitive.  This  can  be  stated  as  "What  transpires  on  top  of  the  pushdown 
list  is  independent  of what  is  under  the  top  of  the  pushdown  list." 

LEtvr~L~ 2.20 

Let  P  =  (Q,  Z, I", d~, q0,  Z0,  F)  be  a  PDA.  If  (q,  w, A)I -z- (q',  e, e),  then 
(q, w, A~)I -n- (q',  e, t~) for  all  A  ~  17' and  ~  ~  r*. 

Proof  A  proof  by  induction  on  n  is  quite  elementary.  For  n  =  1,  the 
lemma  is  certainly  true.  Assuming  that  it  is  true  for  all  1 ~  n  <  n',  let 
(q,  w, A)!-~-- (q',  e, e).  Such  a  sequence  of moves  must  be  of the form 

(q,  w, A)  1-- (q,,  w~, x~  . . .   x~) 

Io~ (q~, w~, x~  .. .  x~) 

. 
I'~-~ (q,,  w~, x~) 

I"~(q',  e,  e) 

where  k  >  1 and  n~ <  n'  for  1 <  i  <  k.t 

Then  the  following  sequence  of  moves  must  also  be  possible  for  any 

~ r * .  

(q,  w, AtE) ~  (q l,  w,  X1  . . .   XkO0 

1"1  (q2,  W2, X2  "'"  Xut~) 

1 ~-'~ (qk,  Wk, Xutx) 

1"~ (q',  e, t~) 

Except  for  the  first  move,  we  invoke  the  inductive  hypothesis.  El 

Next,  we  would  like  to  extend  the  definition  of a  PDA  slightly  to  permit 
the  PD A  to  replace  a  finite-length  string  of symbols  on  top  of the  pushdown 

*]'This  is  another  of  those  "obvious"  statements  which  may  require  some  thought* 
Imagine the  PDA  running  through  the  indicated  sequence  of configurations.  Eventually, 
the length of the pushdown list becomes k  -  1 for the first time.  Since none of  X2  ...  Xk 

has  ever been the top  symbol,  they must  still be  there,  so let n l  be the  number  of elapsed 

moves* Then wait until the length of the list first becomes k  --  2 and  let n2  be the number 

of additional moves  made.  Proceed  in  this  way  until  the  list  becomes  empty* 

SEC. 2.5  PUSHDOWN  AUTOMATA  173 

list by some other  finite-length  string in a single move.  Recall that  our original 
version  of PDA  could  replace  only  the  topmost  symbol  on  top  of the  push- 
down  list  on  a  given  move. 

DEFINITION 

Let  an  extended  PDA  be  a  7-tupte  P  =  (Q,  X, F,  ~, qo, Z0, F),  where 
is  a  mapping  from  a  finite  subset  o f Q   x  (Z  u  [e})  x  F*  to  the  finite  subsets 

of  Q  x  r'*  and  all  other  symbols  have  the  same  meaning  as  before. 

A  configuration  is  as  before,  and  we  write  (q, aw, a},)~  (q',  w, fl?)  if 
6(q,  a,  a)  contains  (q',  13) for  q  in  Q,  a  in  X  u  [e}, and  a  in  F*.  In  this  move 
the  string a  is replaced  by the  string  fl on  top  of the pushdown  list.  As before, 

the  language  defined by P,  denoted L(P),  is 

{w I (q0,  w, Z)I-~-- (q,  e, a) for  some  q  in  F  and  a  in  F*}. 
Notice  that  unlike  a  conventional  PDA,  an  extended  pushdown  automa- 
ton  is  capable  of making  moves  when  its  pushdown  list  is  empty. 

Example 2.33 

Let  us  define  an  extended  PDA  P  to  recognize  L  =  {wwRlw  ~  [a, b}*}. 
Let P  =  ([q, p},  [a,  b}, {a, b,  S, Z}, 6, q, Z,  [p}),  where 

(1)  6(q, a,  e)  =  {(q, a)} 
(2)  6(q, b,  e)  =  [(q,  b)} 
(3)  O(q, e, e)  =  {(q, S)} 
(4)  O(q, e, aSa)  =  {(q, S)} 

(5)  O(q, e, bSb)  =  {(q,  S)} 
(6)  6(q, e, SZ)  =  {(p,  e)} 

With  input  aabbaa,  P  can  make  the  following  sequence  of moves: 

(q,  aabbaa, Z)  ~  (q, abbaa,  aZ) 

(q,  bbaa,  aaZ) 
(q,  baa,  baaZ) 
k-  (q, baa,  SbaaZ) 
}-  (q, aa, bSbaaZ) 
}-- (q, aa,  SaaZ) 
k-  (q, a, aSaaZ) 
]-- (q, a, SaZ) 

(q,  e, aSaZ) 
[-  (q, e, SZ) 
[-- (p,  e, e) 

174  ELEMENTS OF  LANGUAGE THEORY  CHAP.  2 
P  operates  by  first  storing  a  prefix  of the  input  on  the  pushdown  list.  Then 

a  centermarker  S  is  placed  on  top  of  the  pushdown  list.  P  then  places  the 
next  input  symbol  on  the  pushdown  list  and  replaces  aSa  or  bSb  by  S  on 
the  list. P  continues  in  this  fashion  until  all  of the  input  is  used.  If  SZ  then 
remains  on  the  pushdown  list,  P  erases  SZ  and  enters  the  final  state.  D 

We  would  now  like  to  show  that  L  is  a  PDA  language  if  and  only  if L 
is  an  extended  PDA  language.  The  "only if"  part  of this  statement  is  clearly 
true.  The  "if"  part  is  the  following lemma. 

LEMMA 2.21 

Let  (Q,  E, F,  6,  qo, Zo,  F)  be  an  extended  PDA.  Then  there  is  a  PDA 
P,  such  that  L(P1)  =  L(P). 

Proof  Let 

m  =  max{l all 6(q,  a, 00 is  nonempty  for  some  q  E  Q,  a  ~  X u  [e}]. 
We  shall  construct  a  PDA  Pa  to  simulate  P  by  storing  the  top  m  sym- 
bols  that  appear  on  P's  pushdown  list  in  a  "buffer"  of  length  m  located 
in the finite state control of P1. In this way P~ can tell at the start of each move 
what  the  top  m  symbols  of P's  pushdown  list  are.  If,  in  a  move,  P  replaces 

the  top  k  symbols  on  the  pushdown  list  by  a  string  of  l  symbols,  then  P1 
will replace  the first k  symbols in the  buffer by the  string  of length  l. If i  <  k, 
then  P1  will  make  k  --  I  bookkeeping  e-moves  in  which  k  --  l  symbols  are 

transferred  from  the  top  of the  pushdown  list  to  the  buffer  in  the  finite  con- 
trol.  The  buffer will then  be full and  P~  ready to  simulate  another  move  of P. 
If I >  k,  symbols  are  transferred  from  the  buffer  to  the  pushdown  list. 

Formally,  let  P~  =  (Qi,  E, F~, ~l,  q~, z1,  Fi),  where 

(1)  Qt  =  {[q, a]]q  ~  Q,  a  ~  F~,  and  0  <  [a[  ~  m}. (2)  =  r  u  ( z, }. 

(3)  ~1  is  defined  as  follows" 

(a)  Suppose  that  ~(q,  a,  X~  . ..   Xk)  contains  (r,  Y~  . . .   Yi). 

(i)  If  l > k ,   then  for  all  Z ~   F~  and  a  ~  F~'  such  that 

l a [ = m - k ,  

6,([q,  X,  . . .   Xka ], a, Z)  contains  ([r, fl], yZ) 
where  fly  =  Y,  . . .   Y#  and  ]fl[  =  m. 
(ii)  If  l < k ,   then  for  all  Z ~   F1  and  a  ~  F~*  such  that 

l a ] = m - k ,  

~5,([q, X1  .-.  Xka ], a, Z)  contains  ([r,  Y,  . . .   YlaZ],  e) 
(b)  For  all q  ~  Q,  Z  ~  F1,  and  a  ~  F~* such  th at [a   I <   m, 

fil([q, a],  e, Z)  =  {([q, aZ],  e)} 

SEC. 2.5  PUSHDOWN AUTOMATA  175 

These  rules  cause  the  buffer  in  the  finite  control  to  fill  up  (i.e., 
contain  m  symbols). 
(4)  qt  =  [q0, Zo ZT- 1].  The  buffer  initially  contains  Z0  on top  and m  --  1 
Zl's  below.  Z l's  are  used  as  a  special marker for  the bottom  of the pushdown 
list. 

(5)  F1  =  [[q,  ~]lq  ~  F,  0c  ~  F~*}. 
It  is not  difficult  to  show  that 

(q,  aT,  X l  . . .   XkXk+ ~ . "   Xn)  [-~-(r,  w,  Y1  "'"  YIXk+i  "'"  Xn) 
if and  only  if ([q,  a],  aT,  fl)1-~-  ([r,  a'],  w, fl'),  where 

(1)  ~fl  =  x~  . . .   x , z ~ ,  
(2)  ~ ' f l ' =   r~  . . .   Y,X~+x  . . .   x o z ~ ,  

(3)  I~t  =  t~:1  =  m,  and (4)  Between  the  two  configurations  of P1  shown  is  none  whose  state  has 

a  second  component  (buffer)  of  length  m.  Direct  examination  of  the  rules 

of  P1  is  sufficient. 

Thus,  (qo,  w,  Zo)]-~- (q,  e,  oO for  some  q  in  F  and  ~z in  F*  if  and  only  if 

([q0  Z0Z~-l],  w,  Zl)[--~--- ([q,  ~],  e,  r) Pt 
where  lfll  =  m  and  fl~, =  0cZ~.  Thus,  L(P1)  =  L(P).  [[] 

Let  us  now  examine  those  inputs  to  a  P DA  which  cause  the  pushdown 
list  to  become  empty. 

DEFINITION 

Let  P  =  (Q,  Z,  F,  ~,  qo,  Z0,  F)  be  a  PDA,  or  an  extended  PDA.  We say 
that  a  string  w  ~  E*  is  accepted  by  P  by  empty  pushdown  list  whenever 

(q0, w, Z0) [--~- (q, e, e) for  some  q  ~  Q.  Let L,(P)  be the  set of  strings  accepted 
by P  by  empty  pushdown  list. 

LEMMA 2.22 

Let L  be L(P) for some PDA P  =  (Q,  Z,  F,  ~, q0, Zo, F).  We can construct 
a  PDA  P'  such  that  L,(P')  =  L. 

Proof.  We  shall  let P'  simulate  P.  Anytime  P  enters  a  final  state,  P'  will 
have  a  choice  to  continue  simulating  P  or  to  enter  a  special  state  q,  which 

causes  the  pushdown  list to  be  emptied.  However,  there  is  one  complication. 
P  may make  a  sequence  of moves  on  an  input  string  w which  causes  its push- 

down  list  to  become  empty  without  the  finite  control  being  in  a  final  state. 
Thus,  to  prevent P'  from  accepting  w when  it  should  not,  we  add  to P'  a  spe- 

cial  bottom  marker  for  the  pushdown  list  which  can  be  removed  only  by 
P'  in  state  q,.  Formally,  let  P'  be  (Q  u  {q,, q'}, Z,  F  u  {Z'}, 6',q',Z',  O),t 

tWe  shall  usually  make  the  set  of final  states  ~  if the  PDA  is  to  accept  by empty pushdown list. Obviously, the set of final states  could be anything we wished. 

176  ELEMENTS  OF  LANGUAGE THEORY  CHAP.  2 
where  d~' is  defined  as  follows: 

(1)  If  8(q,  a, Z)  contains  (r,  y),  then  8'(q,  a, Z)  contains  (r,  y)  for  all 
q ~   Q , a   ~  X u { e ] , a n d Z   ~  F. 

(2)  d~'(q', e,Z')  =  {(q0, ZoZ')} * P "s  first move is to write ZoZ'  on  the push- 
down  list  and  enter  the  initial  state  of P.  Z'  will act  as  the  special  marker  for 
the  bottom  of the  pushdown  list. 

(3)  For  all q  E  F  and  Z  ~  F  u  {Z'},  ~'(q,  e, Z)  contains  (q,,  e). 

(4)  For  all Z  ~  F  u  {Z'},  ~'(q,,  e, Z)  =  {(qe, e)}. 
We  can  clearly  see  that 

(q',  w, z')  ~  (qo, w, z0z') 

(q,  e,  Y1  "'"  Y,) 
(q,,  e,  Y2  "'"  Yr) 
F~.(qe,  e,  e) 

where  Yr  =  Z',  if and  only  if 

(qo,  w, Z 0 ) ~   (q,  e,  Y1  "'"  Yr-1) 
for  q  E  F  and  Y1  "'"  Yr-1  ~  F*.  Hence,  L,(P')  =  L(P).  E] 

The  converse  of  Lemma  2.22  is  also  true. 

LEMMA 2.23 

Let  P  =  (Q,  E,  r',  3,  q0, zo,  ~ )   be  a  PDA.  We  can  construct  a  PDA  P ' 
such  that  L(P')  =  L,(P). 

Proof.  P'  will  simulate  P  but  have  a  special  symbol  Z'  on  the  bottom  of 
its pushdown  list.  As  soon  as P'  can  read Z', P'  will enter  a  new final  state  q s" 

A  formal  construction  is  left  for  the  Exercises. 

2.5.3.  Equivalence  of  PDA  Languages  and  CFL's 

We  can  now  use  these  results  to  show  that  the  PDA  languages  are  exactly 
the  context-free  languages.  In  the  following  lemma  we construct  the  natural 

(nondeterministic)  "top-down"  parser  for  a  context-free  grammar. 

LEMMA 2.24 

Let  G  =  (N,  Z, P,  S)  be  a  CFG.  From  G  we  can  construct  a  PDA  R 
such  that  L,(R)  =  L(G). 

Proof.  We  shall  construct  R  to  simulate  all  leftmost  derivations  in  G. 
Let  R  =  ({q}, Z,  N  U  X, 5, q,  S,  ~) ,  where  5  is  defined  as  follows: 

(1)  If A  ~  ~  is  in  P,  then  ~(q,  e, A)  contains  (q,  00. 
(2)  J(q,  a,  a)  =  [(q,  e)} for  all  a  in  X. 

We  now  want  to  show  that 

SEC. 2.5  PUSHDOWN  AUTOMATA  177 

wl (2.5.2)  A  ===~ w  if  and  only  if  (q,  w, A)1.2-- (q,  e,  e)  for  some  m,  n  ~  1 

Only  if."  We  shall  prove  this  part  by  induction  on  m.  Suppose  that 
m A==~w.  I f m =   1 a n d w = a l   " " a k ,   k > 0 ,   then 

(q,  al  . . .   ak,  A) l---(q,  a~  . . .   ak,  al  . . .   ak) 

]--(q,  e,  e) 
m Now  suppose  that  A  :=~ w  for  some  m  >  1.  The  first  step  of  this  deriva- 

mt tion  must  be  of  the  form  A  ~  X12"2  " "   Xk,  where  X~ ==~ xi  for  some 

m~ <  m,  1 <  i  <  k,  and  where  x t x   2  . ..   x k  =  w.  Then 

(q,  w,  A)  ~  (q,  w,  X~X,  . . .   X,,) 
If  X~ is  in  N,  then 

(q,  x,,  X,) ~--  (q,  e,  e) 

by  the  inductive  hypothesis.  If  Xt  --  xt  is  in  Z,  then 

(q,  x,,  X,)  ]-  (q,  e,  e) 

Putting  this  sequence  of  moves  together  we  have  (q,  w,  A ) ~ -   (q,  e,  e). 

If:  We  shall  now  show  by induction  on  n  that  if (q,  w,  A) ~z_ (q,  e,  e),  then 
+ A : = ~ w.  

For  n  =  1,  w  =  e  and  A  ---,  e  is  in  P.  Let  us  assume  that  this  statement  is 
true  for  all  n'  <  n.  Then  the  first  move  made  by  R  must  be  of the  form 

(q,  w,  A)  1--- (q,  w,  X  1  "o"  Yk) 
and  (q,  x,,  Xi)  ~ - ( q ,   e,  e)  for  1 <  i  ~  k,  where  w  =  x l x 2   " "   Xk  (Lemma 

+ 2.20).  Then  A  ~  21  '-"  Xk  is  a  production  in  P,  and  Xt ==~ x~  from  the 

0 inductive  hypothesis  if Xi  ~  N.  If Xt  is  in  Z,  then  X~ ==~ x~.  Thus 

A - - - ~ . X 1   . . .  X k :,g 

,  x~X~  ..  " Xk 

~  2122  *..  Xk_lXk 

-  ~.  2122  ..o  2k_lXk  =  W 

is  a  derivation  of  w  from  A  in  G. 

+ As  a  special  case  of  (2.5.2),  we  have  the  derivation  S  ~  w  if  and  only 

if  (q,  w,  S)  ~  (q,  e,  e).  Thus,  L~(R)  =  L(G).  E] 

178  ELEMENTS OF  LANGUAGE  THEORY  CHAP.  2 
Example 2.34 

Let  us construct  a  PDA  P  such  that L,(P)  =  L(Go) , where  Go is our  usual 
grammar  for  arithmetic  expressions.  Let  P  =  ({~q), Z,  F,  ~, q,  E,  ~),  where 

is  defined  as  follows: 

(1)  6(q,  e, E)  =  [(q,  E  +  T),  (q,  T)}. 
(2)  6(q,  e,  T)  =  [(q,  T, F),  (q, F)}. 
(3)  5(q,  e, F)  =  [(q,  (E)),  (q, a)}. 
(4)  d~(q, b, b)  =  [(q,  e)} for  all  b  ~  [a,  + ,   ,,  (,)}. 

With  input  a  +  a  *  a,  P  can  make  the  following  moves  among  others: 

(q,  a  -k  a  ,  a, E)  ~  (q, a  -l-  a  ,  a, E  +  T) 

~-(q,a  +  a , a , T +   T) 
F-  (q,a  +  a , a , F +   T) 
~ ( q , a +   a , a , a +   T) 
F- (q,  +  a , a ,   +  T) 
~-  (q, a  *  a,  T) 
~ ( q , a , a , T , F )  
]-  (q, a  ,  a,  F ,   F) 
~ ( q , a , a , a , F )  
~ ( q , , a , , F )  
[-- (q, a, F ) 

(q, a, a) 
~  (q,e,e) 

Notice  that  in  this  sequence  of moves P  has  used  the  rules  in  a  sequence  that 
corresponds  to  a  leftmost  derivation  of  a  +  a  *  a  from  E  in  G 0.  D 

This  type  of  analysis  is  called  "top-down  parsing,"  or  "predictive  analy- 
sis,"  because  we  are  in  effect  constructing  a  derivation  tree  starting  off from 
the  top  (at  the  root)  and  working  down.  We  shall  discuss  top-down  parsing in  greater  detail  in  Chapters  3,  4,  and  5. 

We  can  construct  an  extended  PDA  that  acts  as  a  "bottom-up  parser" 
by  simulating  rightmost  derivations  in  reverse  in  a  C FG  G.  Let  us  consider the  sentence  a  +  a  *  a  in L(Go).  The  sequence 

E===~ E  +  T--->   E  +  T  ,  F---~.  E  +  T  ,  a----> E  +  F ,  a 

E +   a ,   a-----~. T +   a , a - - - > F +   a ,   a---->a  +  a ,   a 

of right-sentential  forms  represents  a  rightmost  derivation  of a  +  a  *  a  from E  in  Go. 

SEC. 2.5  PUSHDOWN AUTOMATA  179 

Now  suppose  that  we  write  this  derivation  reversed.  If we  consider  that 
in  going  from  the  string  a  4-  a  *  a  to  the  string  F  -Jr- a  *  a  we  have  applied 
the  production  F--~  a  in  reverse,  then  we  can  say  that  the  string  a  +  a  *  a 

has  been  "left-reduced"  to  the  string  F  4 - - a ,   a.  Moreover,  this  represents 

the  only  leftmost  reduction  that  is  possible.  Similarly  the  right  sentential 

form  F  +  a  *  a  can  be left-reduced  to  T  4-  a  *  a  by means  of the  production 

T---~ F,  and  so  forth.  We  can  formally  define  the  process  of  left  reduction 
as  follows. 

DEFINITION 

Let  G  =  (N,  ~, P,  S)  be  a  CFG,  and  suppose  that 

S  ==> ocAw ==> o~flw ==~ xw 

i,m  i,m  i,m 

is  a  rightmost  derivation.  Then  we  say  that  the  right-sentential  form  ~flw 
can  be  left-reduced  under  the  production  A  ~  fl  to  the  right-sentential 

form  ocAw.  Furthermore,  we  call  the  substring  fl  at  the  explicitly  shown 
position  a  handle  of  0~flw. Thus  a  handle  of  a  right-sentential  form  is  any 

substring  which  is  the  right  side  of  some  production  and  which  can  be 

replaced  by the  left  side  of that production  so that  the  resulting  string is also 

a  right-sentential  form. 

Example 2.35 

Consider  the  grammar  with  the  following productions" 

S  > A c t B d  
A -   ~ aAb[ab 

B  ~  aBbblabb 

This  grammar  generates  the  language  {a"b"cln  ~>  1]  u  {a"b2"d[ n  ~>  1]. 

Consider the right-sentential  form aabbbbd.  The  only handle  of this string 
is  abb,  since  aBbbd  is  a  right-sentential  form.  Note  that  although  ab  is  the 
right  side  of the  production  A  ---,  ab,  ab  is  not  a  handle  of  aabbbbd  since 

aAbbbd is  not  a  right-sentential  form.  [-] 

Another  way  or  defining  the  handle  of  a  right-sentential  form  is  to  say 
that  the  handle  is  the  frontier  of  the  leftmost  complete  subtree  of  depth  1 
(i.e.,  a  node  all  of whose  direct  descendants  are  leaves,  together  with  these 

leaves)  of  some  derivation  tree  for  that  right-sentential  form. 

In  the  grammar  Go,  the  derivation  tree  for  a  +  a ,   a  is  shown  in  Fig. 
2.16(a).  The  leftmost  complete  subtree  has  the  leftmost  node  labeled  F  as 
root  and  frontier  a. 

If we  delete  the  leaves  of the  leftmost  complete  subtree,  we  are  left  with 
the  derivation  tree  of Fig.  2.16(b).  The  frontier  of this  tree  is  F  +  a  *  a,  and 

180  ELEMENTS  OF  LANGUAGE  THEORY  cI-Ial>. 2 

+  T 
i  I  ! F  F  a 
!  i a  a 

(a) 

E  +  T  E  T 

T  T  *  F  T  T  F I  I  i  I 

F  F  a  F  a I  I 

a  a 
(b)  (c) 

Fig. 2.16  Handle pruning. 

this  string  is  precisely  the  result  of  left-reducing  a  -k  a  *  a.  The  handle  of 

this  tree  is  the  frontier  F  of  the  subtree  with  root  labeled  T.  Again  remov- 
ing  the  handle  we  are  left  with  Fig.  2.16(c). 

The  process  of  reducing  trees  in  this  manner  is  called  handle  pruning. 
From  a  C F G   G  we  can  construct  an  equivalent  extended  PDA~P  which 
operates  by  handle  pruning.  At  this  point  it  is  convenient  to  represent  a 
pushdown  list  as  string  such  that  the  rightmost  symbol of  the  pushdown 
list,  rather  than  the  leftmost,  is  at  the  top.  Using  this  convention,  if P  --" 
(Q,  ~,  F,  tS, q0, Z0,  F)  is  a  PDA,  its  configurations  are  exactly  as  before. 

However,  the  [-  relation  is  defined  slightly  differently.  If  tS(q, a,  00  con- 
tains (p,  fl),  then  we write (q, aw, ?oc) ~-  (p,  w, ?fl)  for  all  w ~  ~*  and  ? ~  F*. 

Thus  a  notation  such  as  "~(q,  a,  YZ)  contains  (p,  VWX)"  means  different 
things  depending  on whether  the  (extended)  PDA  has the top  of its pushdown 
list  at  the  left  or  right.  If at  the  left,  Y and  V are  the  top  symbols  before  and 

after  the  move.  If  at  the  right,  Z  and  X  are  the  top  symbols.  Given  a  PDA 

sEc. 2.5  PUSHDOWN  AUTOMATA  1 81 

with  the  top  at the  left,  one  can  create  a  PDA  doing  exactly  the  same  things, 
but  with  the  pushdown  top  at  the  right,  by  reversing  all  strings  in  F*.  For 

example,  (p,  VWX)  ~  ~(q, a,  YZ)  becomes  (p,  XW V)  ~  ~(q, a, Z  Y).  Of 

course,  one  must  specify the  fact that  the  top  is now  at the  right.  Conversely, 

a  PDA  with  top  at  the  right  can  easily  be  converted  to  one  with  the  top  a 

the  left. We  see  that  the  7-tuple  notation  for  PDA's  can  be  interpreted  as  two 

different  PDA's,  depending  on  whether  the  top  is  taken  at  the  right  or  left. 

We  feel  that  the  notational  convenience  which  results  from  having  these  two 
conventions  outweighs  any  initial  confusion.  As  the  "default  condition," 

unless  it  is  specified  otherwise,  ordinary  PDA's  have  their  pushdown  tops 

on  the  left  and  extended  PDA's  have  their  pushdown  tops  on  the  right. 

LEMMA 2.25 

Let  (N,  X, P,  S)  be  a  CFG.  From  G  we  can  construct  an  extended  PDA 
R  such  that  L(R)  =  L(G).t  R  can  "reasonably"  be  said  to  operate  by  handle 

pruning. 

Proof.  Let R  =  ({q, r }, X, N  U  ~  U  { $}, ~, q,  $, {r }) be  an extended  PDA~ 
in  which  $  is  defined  as  follows: 

(1)  d~(q, a,  e)  =  {(q, a)}  for  all  a  ~  Z.  These  moves  cause  input  symbols 
to  be  shifted  on  top  of the  pushdown  list. 

(2)  If A  --~  ~  is  in P,  then  O(q, e,  00 contains  (q, A). 

(3)  $(q,  e,  $ S ) =   {(r, e)}. 

We  shall  show  that  R  operates  by computing  right-sentential  forms  of G, 
starting  with  a  string  of  all  terminals  (on  R's  input)  and  ending  with  the 

string  S.  The  inductive  hypothesis  which  will  be  proved  by  induction  on  n  is 

(2.5.3) 

*  n S  ~  aAy  ~  xy  implies  (q, xy,  $) ~  (q, y,  SaA) 

fm  rm 

The basis,  n  =  0, is trivial;  no  moves  of R  are involved.  Let us  assume  (2.5.3) 
for  values  of  n  smaller  than  the  value  we  now  choose  for  n.  We  can  write 

n--1 eAy  :=~ efly=-~  xy.  Suppose  that  eft  consists  solely  of  terminals.  Then 

Fm  I"m eft  =  x  and  (q, xy,  $ ) ~   (q, y,  $efl)  !-  (q, Y,  SEA). 

If eft  is not  in  Z*,  then  we  can  write  eft  =  ~,Bz, where  B  is  the  rightmost 
tObviously,  Lemma 2.25 is implied by Lemmas 2.23 and  2.24. It  is the  construction 
that is of interest here. 

:l:Our convention puts pushdown tops  on the right. 

182  ELEMENTS  OF  LANGUAGE  THEORY  CHAP.  2 

*  n--I nonterminal.  By  (2.5.3),  S  ==> ?Bzy ==~ xy  implies  (q, xy,  $ ) ~   (q, zy,  $?B). 
rm  rm Also,  (q, zy,  $?B)  ~  (q, y,  $~,Bz) ~-- (q, y,  $o~A) is  a  valid  sequence  of moves. 

We  conclude  that  (2.5.3)  is  true.  Since  (q,  e,  $ S ) ~   (r,  e, e),  we  have L(C)  _~ L(R). 
We must now show the following,  in  order to conclude  that L(R)  ~  L(G), and  hence,  L(G)  =  L(R)" 

(2.5.4)  If (q, xy,  $)  ~  (q, y,  $aA),  then  aA y  ~  xy 

The  basis,  n  =  0,  holds  vacuously.  For  the  inductive  step,  assume  that 
(2.5.4) is true  for all values  of n  <  m. When  the  top  symbol of the pushdown 
list  of R  is  a  nonterminal,  we  know  that  the  last  move  of  R  was  caused  by rule  (2)  of the  definition  of ,3. Thus  we  can  write 

(q, xy,  $) ~  (q, y,  $afl)  t-- (q, y,  $aA), 
where  A  --,  fl is in P.  If 0~fl has  a  nonterminal,  then  by  inductive  hypothesis 
(2.5.4),  o~fly =-~ xy.  Thus,  ocAy ==~ ocfly ~  xy,  as  contended. 

As  a  special  case  of  (2.5.4),  (q,  w,  $ ) ~   (q, e,  $S)  implies  that  S ~  w. 
Since  R  only  accepts  w  if  (q, w,  $) t---(q, e,  $S)  ~  (r,  e, e),  it  follows  that L(R)  _~ L(G).  Thus,  L(R)  --  L(G).  [~ 

Notice  that  R  stores  a  right-sentential  form  of the  type  aAx  with  aA  on 
the  pushdown  list  and  x  remaining  on  the  input  tape  immediately  after 

a  reduction.  Then  R  can  proceed  to  shift  symbols  of x  on  the  pushdown  list 
until  the  handle  is  on  top  of  the  pushdown  list.  Then  R  can  make  another 
reduction.  This  type  of  syntactic  analysis  is  called  "bottom-up  parsing"  or "reductions  analysis." 

Example  2.36 

Let  us  construct  a  bottom-up  analyzer  R  for  Go.  Let  R  be  the  extended PDA  ({q, r}, Z, I',  5,  q,  $, [r]),  where  5  is  as  follows: 

(1)  O(q, b, e)  =  [(q, b)} for  all  b  in  {a,  + , . ,   (,)}. (2)  5(q, e, E  +  T)  =  [(q,  E)} 

5(q, e,  T)  =  [(q, E)} 5(q, e,  T .   F)  =  [(q,  T)} 
6(q, e, F)  =  {(q, T)] 
~(q, e, (e))  =  {(q, F)} 

5(q,  e, a)  =  [(q,  F)}. (3)  5(q, e,  $ E ) =   {(r, e)~. 

With  input  a  +  a  *  a,  R  can make  the following  sequence  of moves" 

SEC. 2.5  PUSHDOWN AUTOMATA  183 

(q, a  +  a ,   a,  $)  1-- (q,  +  a ,   a,  $a) 

1-  (q,  +  a  *  a,  $F) 

[-(q,  + a , a , $ T )  

(q,  +  a ,   a,  $E) 

(q,  a  *  a,  $E  +) 

(q,  ,  a,  $E  +  a) 

(q,  *  a,  $E  +  F) 
~--- (q,  *  a,  $E  +  T) 

~-  (q,  a,  $E  +  T  ,) 

1-- (q,  e,  SE  +  T  *  a) 

[-  (q,  e,  SE  +  T ,   F) 

1-  (q,  e,  $E  +  T) 

1-- (q,  e,  $E) 

y-  (r,  e,  e) 

Notice  that  R  can  make  a  great  number  of  different  sequences  of moves 
with  input  a  +  a  *  a.  This  sequence,  however,  is the  only  one  that  goes from 

an  initial  configuration  to  a  final  configuration.  5 

We  shall  now demonstrate  that  a  language  defined  by a  PDA  is a  context- 
free  language. 

LEMMA 2.26 

Let  R  =  (Q,  X, F,  8,  q0, Z0,  F)  be a  PDA.  We  can construct  a  C F G  such 
that  L(G)  =  L,(R). 

Proof.  We  shall  construct  G  so  that  a  leftmost  derivation  of  w  in  G 
directly  corresponds  to  a  sequence  of  moves  made  by  R  in  processing  w. 
We  shall  use  nonterminal  symbols  of  the  form  [qZr]  with  q  and  r  in  Q  and 

+ Z  ~  F.  We  shall  then  show  that  [qZr] =~ w  if and  only if (q, w, Z)  t---- (r, e, e). 

Formally,  let  G  =  (N,  Z, P,  S),  where 

(1)  N  =  ([qZr][q,  r  ~  O , Z   ~  F}  u  (S}. (2)  The  productions  in P  are  constructed  as  follows" 

(a)  If  8(q,  a, Z)  contains  (r,  X1  " "   Xk), t  k  _>  1,  then  add  to  P  all 

productions  of the  form 

[qZsk]  ~  a[rXlsl][siX2s2]  "'"  [Sk-lXkS~] 
tR  has its pushdown list top  on the left, since we did  not state otherwise. 

184  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

for  every  sequence  s 1, s 2 , . . . ,   se  of  states  in  Q. 
(b)  If  6(q, a,Z)  contains  (r, e),  then  add  the  production  [qZr]---,  a 

to P. 
(c)  Add  to P,  S  --~  [qoZoq] for  each  q  E  Q. 

It is straightforward to show by induction  on m and n that for all q, r  ~  Q, 

m and  Z  ~  F,  [qZr] =~ w if and  only if (q, w, Z)  Fz- (r, e, e).  We  leave  the  proof 

+ for the  Exercises.  Then,  S =~ [qoZoq] =~ w if and  only if (q0, w, Z0) ~-- (q, e, e) 

for q  in  Q.  Thus,  Le(R)  --  L(G).  D 

We  can  summarize  these  results  in  the  following  theorem. 

THEOREM 2.21 

The  following  statements  are  equivalent: 

(1)  L  is L(G)  for  a  CFG  G. 
(2)  L  is L(P)  for  a  PDA  P. 

(3)  L  is Le(P) for  a  PDA  P. 

(4)  L  is L(P)  for  an  extended  PDA  P. 

Proof  ( 3 ) ~   (1)  by  Lemma  2.26.  (1)----~ (3)  by  Lemma  2.24.  (4)--~  (2) 
by  Lemma  2.21,  and  (2) ~  (4)  is  trivial.  (2)  ~  (3)  by  Lemma  2.22  and 

(3)----,  (2)  by  Lemma  2.23.  r-] 

2.5.4.  Deterministic  Pushdown  Automata 

We  have  seen  that  for  every  context-flee  grammar  G  we  can  construct 
a  PDA  to  recognize L(G).  The  PDA  constructed  was  nondeterministic,  how- 
ever.  For practical  applications  we are more  interested in  deterministic push- 

down automata--PDA's  which can make at most one move in any configura- 
tion.  In  this  section  we  shall  study deterministic  PDA's  and  later  on  we shall 

see  that,  unfortunately,  deterministic  PDA's  are  not  as  powerful  in  their 
recognitive  capability  as  nondeterministic  PDA's.  There  are  context-free 

languages  which  cannot  be  defined  by  any  deterministic  PDA. 

A  language  which is defined  by a  deterministic  pushdown  automaton  will 
be  called  a  deterministic  CFL.  In  Chapter  5 we  shall  define  a  subclass  of the 

context-free  grammars  called  LR(k)  grammars.  In  Chapter  8  we  shall  show 
that  every  LR(k)  grammar  generates  a  deterministic  CFL  and  that  every 

deterministic  CFL  has  an  LR(1)  grammar. 

DEFINITION 

A  PDA  P  -=  (Q,  Z, F, ~, q0, Zo, F)  is  said  to  be  deterministic  (a  DPDA 
for  short)  if for  each  q  ~  Q  and Z  ~  I" either 

(1)  ~(q,a,Z)  contains  at  most  one  element  for  each  a  in  E  and 
~(q, e, Z)-=  ~  or 

SEC. 2.5  PUSHDOWN  AUTOMATA  185 

(2)  O(q, a, Z ) - -   ~  for  all  a  ~  Z  and  6(q,  e, Z)  contains  at  most  one 
element. 

These  two  restrictions  imply  that  a  D PD A  has  at  most  one  choice  of 
move  in  any  configuration.  Thus  in  practice  it  is  much  easier  to  simulate 

a  deterministic  PDA  than  a  nondeterministic  PDA.  For this  reason  the  deter- 
ministic  CFL's  are  an  important  class  of languages  for practical  applications. 

CONVENTION 

Since  6(q,  a, Z)  contains  at  most  one  element  for  a  DPDA,  we  shall  write 6(q,  a, Z)  =  (r, ~) 

instead  of  6(q, a, Z)  =  {(r, ?)}. 

Example 2.37 

Let  us  construct  a  D P DA   for  the  language  L  =  {wcw~lw  ~  [a,  b}+]. 
Let  P  =  ([qo,  q l,  q2}, {a, b,  c}, {Z, a,  b}, ,6, qo, Z,  {q2]),  where  the  rules  of  t~ 
are 

6(qo,  X,  Y)  =  (qo,  X  Y) 

6(qo,  c,  Y ) =   (q~,  Y) 
O(q~, X,  X)  =  (q~, e) 

di(q t , e, Z)  =  (q2, e) 

for  all  X  ~  (a,  b} 

Y  ~  [Z,a,b} 
for  all  Y  E  [a,  b} 
for  all  X  ~  [a,  b} 

Until  P  sees  the  centermarker  c,  it  stores  its  input  on  the  pushdown  list. 
When  the  e is reached,  P  goes to  state  q l and proceeds  to match its subsequent 

input  against  the  pushdown  list.  A  proof that  L(P)  --  L  is  left  for  the  Exer- 

cises.  D 

The definition  of a  DP DA  can be naturally  widened  to include  the  extend- 
ed  PDA's  which  we  would  naturally  consider  deterministic. 

DEFINITION 

An  extended  PDA  P  --  (Q,  E,  r', ~, q0, Zo, F)  is  an  (extended)  determin- istic PDA 

if the  following  conditions  hold: 
(1)  For  n o q   ~  Q , a   ~  E u { e } a n d ~ ,   ~  F*  is  z~O(q, a,  7 ) >   1. 
(2)  If O(q, a,  0c) ~  ~ ,   6(q, a, fl) ~  ~ ,   and  ~ ~  fl,  then  neither  of ~  and  fl is  a  suffix  of the  other.? 

(3)  If  6(q,  a, oc) ~  ~ ,   and  O(q, e,/3)  :~  ~ ,   then  neither  of  0c  and  fl  is 
a  suffix  of the  other. 

We  see  that  in  the  special  case in  which  the  extended  PDA  is an  ordinary 
PDA,  the  two  definitions  agree.  Also,  if the  construction  of  Lemma  2.21  is 

tlf the extended PDA has its pushdown list top at the left, replace "suffix" by "prefix." 

186  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 
applied  to  an  extended  PDA  P,  the  result  will  be  a  DPDA  if  and  only  if P 
is  an  extended  DPDA. 

When  modeling  a  syntactic  analyzer,  it  is  desirable  to  use  a  DPDA  P 
that  reads  all  of its  input,  even  when  the  input  is  not  in L(P).  We  shall  show 
that  it  is  possible  to  always  find  such  a  DPDA. 

We  first  modify  a  D PD A  so  that  in  any configuration  with  input  remain- 
ing  there  is  a  next  move.  The  next  lemma  shows  how. 

LEMMA 2.27 

Let P  =  (Q, E,  r',  ~,  q0, z0,  F)  be  a  DPDA.  We  can  construct  an  equiva- 
lent  DPDA  P'  =  (Q',  Z,  F',  ~', q~, Z~,  F')  such  that 

(1)  For  all  a  ~  E,  q  ~  Q'  and  Z  ~  F',  either 

(a)  8'(q,  a, Z )contains  exactly  one  element  and  t~'(q, e , Z ) =   ~  or 
(b)  tS'(q, a, Z)  =  ~  and  O'(q, e, Z)  contains  exactly  one  element. 
(2)  If  $(q,  a, Z~,) =  (r, ~,) for  some  a  in  ~  u  {e}, then  ~, =  aZ~  for  some 
a ~ F * .  

Proof  Z~  will  act  as  an  endmarker  on  the  pushdown  list  to  prevent  the 
pushdown  list  from  becoming  completely  empty.  Let  F ' =   F  u  {Zg},  and. 
let  Q'  =  {q~, q,}  u  Q.  6'  is  defined  thus" 

(1)  dV(q~, e, Zg)  =  (q0, ZoZ~) * (2)  For  all  q  E  Q,  a  ~  Z  u  {e}, and  Z  ~  F  such  that  t~(q, a , Z ) - ~   ~ ,  

~'(q,  a, Z)  =  ~(q,  a, Z). 

(3)  If  fi(q,  e , Z ) =   ~  and  $(q,  a , Z ) =   ~  for  some  a  ~  Z  and  Z  ~  F, 
let  $'(q,  a, Z)  =  (q,, Z). 

(4)  For  all  Z  ~  r '   and  a  ~  ~,  dV(q,, a, Z ) =   (q,, Z). 

The  first  rule  allows  P'  to  simulate  P  by  having P'  write  Z0  on top  of Z~ 
on  the  pushdown  list and  enter  state  qo.  The rules  in (2) permit P'  to  simulate 
P  until  no  next  move is possible.  In such  a  situation P'  will go  into  a  nonfinal 

state  q,,  by  rule  (3),  and  remain  there  without  altering  the  pushdown  list, 
while  consuming  any  remaining  input.  A  proof  that  L(P')  =  L(P)  is  left  for 
the  Exercises.  Q 

It  is  possible  for  a  D PD A  to  make  an  infinite  number  of  e-moves  from 
some  configurations  without  ever  using  an  input  symbol.  We  call  these  con- 
figurations  looping. 

DEFINITION 

Configuration  (q,  w, a)  of  D PDA   P  is  looping  if  for  all  integers  i  there 
exists  a  configuration  (p,,  w, l?,) such  that  ]fl, I>--I~t  and 

(q,  w, 00  ~-  (P~,  w, fl~)  ~  (P2,  w, 1/2) F-'"  ". 
Thus  a  configuration  is  looping  if  P  can  make  an  infinite  number  of 

SEC. 2.5  PUSHDOWN AUTOMATA  187 
e-moves  without  creating  a  shorter  pushdown  list;  that  list  might  grow 
indefinitely  or  cycle between  several  different  strings. 

Note  that  there  are  nonlooping  configurations  which  after  popping  part 
of their  list  using  e-moves  enter  a  looping  configuration.  We  shall  show  that 
it  is  impossible  to  make  an  infinite  number  of e-moves  from  a  configuration 
unless  a  looping  configuration  is  entered  after  a  finite,  calculable  number  of 
moves. 

If P  enters  a  looping  configuration  in  the  middle  of the  input  string,  then 
P  will  not  use  any  more  input,  even  though  P  might  satisfy  Lemma  2.27. 

Given  a  D P D A   P,  we  want  to  modify  P  to  form  an  equivalent  D P D A   P' 
such  that  P'  can  never  enter  a  looping  configuration. 

ALGORITHM 2.16 

Detection  of looping  configurations. 
Input.  DP D A  P  =  (Q,  E,  I',  ~,  q0, Z0,  F). 

Output. 
(1)  C a =  {(q, A)](q,  e, A)  is  a  looping  configuration  and  there  is  no  r  in 
F  such  that  (q,  e, A)  1----(r, e,  00  for  any  0~  ~  F*},  and 

(2)  C2  =  {(q,A)l(q,  e,A)  is  a  looping  configuration  and  (q, e,  A)~---- 
(r,  e,  a)  for  some  r  ~  F  and  a  ~  F*}. 

Method.  Let  ~  Q  =  n t,  # F   =  n 2,  and  let  l  be  the  length  of the  longest 
string  written  on  the  pushdown  list  by  P  in  a  single  move.  Let  n 3 = n~(n~ '"'~-  n2)/(n z -- 

1),  where  n3  =  na  if n2  =  1.  n3  is  the  maximum  number 
of  e-moves P  can  make  without  looping. 

(1)  For  each  q  ~  Q  and  A  ~  F  determine  whether  (q,  e, A)  ~  (r,  e,  a) 
for  some r  e  Q  and  ~  ~  F +. Direct  simulation  of P  is used.  If  so,  (q,  e, A)  is 

a  looping  configuration,  for  then  we  shall  see  that  there  must  be  a  pair (q',  A'), 

with  q'  ~  Q  and  A'  E  F,  such  that 

(q,  e,  A)  t--- (q',  e, A'fl) 

(q',  e, 
Ira(j- 1,)  (q,,  e, A' rJ fl) 

where  m  >  0  and j  >  0.  Note  that  ~, can  be  e. 

(2)  If  (q,  e,  A)  is  a  looping  configuration,  determine  whether  there  is 
an  r  in  F  such  that  (q,  e, A ) ~   (r,  e, 00  for  some  0  _< j  <  n 3.  Again,  direct 
simulation  is  used.  If  so,  add  (q, A)  to  C z.  Otherwise,  add  (q, A)  to  C~.  We 
claim  that  if P  can  reach  a  final  configuration  from  (q,  e, A),  it  must  do  so 
in  n 3 or  fewer  moves. 

THEOREM 2.22 

Algorithm  2.16  correctly  determines  C~  and  C2. 

188  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

Proof  We first prove that  step (1) correctly determines  C1  U  C2. If (q, A) 
is  in  C1  U  C2,  then,  obviously,  (q, e, A ) ~   (r,  e,  ~).  Conversely,  suppose 
that  (q,  e, A)  ~  (r,  e, ~). 

Case  1"  There  exists  fl  ~  r *,  with  I//I  >  nan21 and  (q, e , A ) ~   (p, e, fl) ~--(r,  e,  ~ ) f o r   some  p  ~  Q.  If  we  consider,  for  j  =  1, 2 , . . . ,   nan2l  +  1, 

the  configurations  which  P  entered  in  the  sequence  of  moves  (q,  e, A) 
(p,  e, fl)  the  last  time  the  pushdown  list  had  length j,  then  we  see  that there 

must  exist q'  and  A'  such  that  at  two  of those  times  the  state  of P  was q'  and 
A'  was  on top  of the list. In  other words,  we can  write  (q, e, A)  l-- (q',  e, A'6) (q',  e, A'76 ) ~  (p,  e, fl). 

Thus,  (q, e, A)  ~  (q',  e, A'$)  ~  (q',  e, A'rJ~)  for allj  ~  0 by Lemma 2.20.  Here,  m  >  0, so an infinity of e-moves can be made 

from  configuration  (q, e, A),  and  (q, A) is  in  C~  U  C 2. 

Case  2:  Suppose  that  the  opposite  of case  1 is true,  namely that  for  all  fl 
such  that  (q, e, A) ~  (p,  e,  fl) ]--- (r, e,  ~)  we  have  I/~1 _< nln2l.  Since  there 
are  n 3 -+-  1 different  fl's,  na  possible  states,  and  n2  -k- n~  -+- n~  -+-  . . .   -q- n~ '"a =  (n~ ~"~-  n2)/(n 2 -- 1)  

possible  pushdown  lists  of  length  at  most  nlnJ, 
there  must  be  some  repeated  configuration.  It  is  immediate  that  (q, A)  is 
in  Cx  U  C 2. 

The  proof that  step  (2)  correctly  apportions  C a U  C2 between  C a and  C~ 
is  left  for  the  Exercises.  E] 

DEFINITION 

A  DPDA  P  --  (Q,  Z, F,  ~, qo, Zo, F)  is  continuing  if for  all  w  ~  E*  there 
exists p  ~  Q  and  ~  ~  F*  such that  (qo, w, Zo) ~-  (p, e, ~).  Intuitively,  a  con- 
tinuing  DPDA   is  one which  is capable  of reading  all of its input  string. 

LEMMA 2.28 

Let  P  --  (Q, ~, F, 6, qo, Zo, F)  be  a  DPDA.  Then  there  is  an  equivalent 
continuing  DPDA  P'. 

Proof  Let  us  assume  by  Lemma  2.27  that  P  always  has  a  next  move. 
Let  P ' =   (Q  u  {p,  r},  ~,  F,  t~', q0, Z0,  F  U  [p]),  where  p  and  r  are  new 
states,  t~' is  defined  as  follows" 

(1)  For  all  q  ~  Q,  a  ~  Z,  and  Z  E  F,  let  6'(q,  a, Z)  =  O(q, a, Z). 
(2)  For  all q  E  Q  and Z  ~  r'  such  that  (q, e, Z)  is not  a  looping configu- 
ration,  let  O'(q,  e, Z)  =  $(q,  e, Z). 

(3)  For  all  (q, Z)  in  the  set  Cl  of Algorithm  2.16,  let  6'(q,  e, Z)  =  (r, Z). 
(4)  For  all  (q, Z)  in  the  set  C 2 of Algorithm  2.16,  let r~'(q, e, Z)  =  (p, Z). 
(5)  For all a  E  E and Z  ~  I", t~'(p,  a, Z)  =  (r, Z) and ~'(r,  a, Z)  =  (r, Z). 

Thus, P'   simulates P.  If P  enters a looping configuration,  then P'  will enter  on 

the  next  move  either  state p  or  r  depending  on  whether  the  loop  of configu- 

rations  contains  or  does  not  contain  a  final  state.  Then,  under  all  inputs,  P' 

s~c.  2.5  PUSHDOWN  AUTOMATA  189 
enters  state  r  from  p  and  stays  in  state  r  without  altering  the  pushdown  list. 
Thus,  L(P')  =  L(P). 

It  is necessary  to  show  that  P'  is continuing.  Rules  (3),  (4),  and  (5) assure 
us that  no  violation  of the  "continuing"  condition  occurs  if P  enters  a  looping 
configuration.  It  is  necessary  to  observe  only  that  if P  is  in  a  configuration 
which  is  not  looping,  then  within  a  finite  number  of moves  it  must  either 

(1)  Make  a  non-e-move  or 

(2)  Enter  a  configuration  which  has  a  shorter  pushdown  list. 

Moreover,  (2)  cannot  occur  indefinitely,  because  the  pushdown  list  is 
initially  of  finite  length.  Thus  either  (1)  must  eventually  occur  or  P  enters 

a  looping  configuration  after  some  instance  of  (2).  We  may  conclude  that 
P '  is  continuing.  [~] 

We  can  now  prove  an  important  property  of  DPDA's,  namely  that  their 
languages  are  closed  under  complementation.  We  shall  see in the  next  section 
that  this  is  not  true  for  the  class  of all  CFL's. 

THEOREM  2.23 

If L  =  L(P) for  D P D A   P,  then  L  =  L(P')  for  some  D P D A   P'. 
Proof.  We  may,  by  Lemma  2.28,  assume  that  P  is  continuing.  We  shall 
construct  P'  to  simulate  P  and  see,  between  two  shifts  of  its  input  head, 
whether  or  not  P  has  entered  an  accepting  state.  Since  P'  must  accept  the 
complement  of L(P), P' accepts  an  input  if P  has  not  accepted  it  and  is about 
to  shift  its  input  head  (so  P  could  not  subsequently  accept  that  input). 

Formally,  let P  =  (Q,  ~, r', ~, q0, z0,  F)  and  P'  =  (Q',  E, I", ~', q~, z0,  F'), where 

(1)  Q'  =  {[q, i] Iq  ~  Q,  i  ~  {0,  1, 2}}, 
(2)  q~  =  [q0,  o]  if q0  ~  F a n d   q~  =  [q0,  1] if q0  e  F,  and (3)  F ' =   {[q, 2][q  in  a}. 

The  states  [q,  0]  are  intended  to  mean  that  P  has  not  been  in  a  final  state 
since it last  made  a  non-e-move.  [q,  1] states  indicate  that P  has  entered  a  final 
state  in  that  time.  [q,  2]  states  are  used  only  for  final  states.  If P '  is  in  a  [q,  0] 
state  and  P  (in  simulation)  is  about  to  make  a  non-e-move,  then  P'  first 
enters  state  [q,  2]  and  then  simulates  P.  Thus,  P'  accepts  if and  only if P  does 
not  accept.  The  fact  that  P  is  continuing  assures  us  that  P'   will  always  get  a 
chance  to  accept  an  input  if P  does  not.  The  formal  definition  of  6'  follows: 

(i)  I f q   ~  Q , a   ~  ~E, a n d Z   ~  F,  then 

d~'([q, 1], a,  Z)  =  J'([q,  2],  a, Z)  =  ([p,  i],  ~,), 
where  ~ ( q , a , Z ) = ( p , ~ , ) ,   i = 0   i f p ~ F ,   and  i = l   i f p E F .  

(ii)  If q  c  Q,  Z  e  F,  and  d(q,  e, Z)  =  (p,  ~,), then 

1 90  ELEMENTS OF  LANGUAGE THEORY  CHAP.  2 

6'([q,  1],  e, Z)  =  ([p,  1],  ~,) 
and  ~'([q,  0],  e, Z)  --  ([p,  i],  7),  where  i  - - 0   if p  ~  F  and  i -   1  if p  ~  F. 

(iii)  If  6(q,  e,  Z)  --  O,  then  6'([q,  0],  e, Z)  --  ([q,  2],  Z). 

Rule  (i)  handles  non-e-moves.  The  second  com p one nt  of  the  state  is  set 
to  0  or  ]  properly.  Rule  (ii)  handles  e-moves;  again  the  second  c om p one nt   of 

the  state  is  handled  as  intended.  Rule  (iii)  allows  P '  to  accept  an  input  exactly 
when  P  does  not.  A  formal  p roof   that  L ( P ' ) -   L(P)  will  be  omitted.  ~-] 

There  are  a  number  of other  importa nt  properties  of deterministic  CFL's. 
We  shall  defer  the  discussion  of  these  to  the  Exercises  and  the  next  section. 

EXERCI SES 
2.5.1.  Construct  PDA's  accepting  the  complements  (with  respect  to  [a,  b}*) 

of the  following  languages: 
(a)  {a"b"a" [ n  ~>  1}. (b)  [wwRIw  ~  [a, b}*}. 

(c)  [ambnamb" [ m,  n  ~  1}. 
(d)  (ww]  w  ~  (a, b}*). Hint: 

Have  the  nondeterministic  'PDA  "guess"  why  its  input  is  not  in 
the  language  and  check  that  its  guess  is  correct. 

2.5.2.  Prove  that  the  PDA  of  Example  2.31  accepts  [wwR[ W ~  (a,  b}+}. 

2.5.3.  Show  that  every  CFL  is  accepted  by  a  PDA  which  never  increases  the 

length  of  its  pushdown  list  by  more  than  one  on  a  single  move. 

2.5.4.  Show  that  every  CFL  is  accepted  by  a  PDA  P  =  (Q,  E,  1-', 6, q0, Z0,  F) 

such  that  if  (p,  7)  is  in  O(q, a, Z),  then  either  ~, =  e,  ~, -  Z,  or  7  --  YZ 

for  some  Y  ~  F.  Hint:  Consider  the  construction  of  Lemma  2.21. 

2.5.5.  Show  that  every  CFL  is  accepted  by  a  PDA  which  makes  no  e-moves. 

Hint:  Recall  that  every  CFL  has  a  grammar  in  Greibach  normal  form. 

2.5.6.  Show  that  every  CFL  is L(P)  for  some  two-state  PDA  P. 

2.5.7.  Complete  the  proof  of Lemma  2.23. 
2.5.8.  Find  bottom-up  and  top-down  recognizers  (PDA's)  for  the  following 

grammars: 

(a)  S  ---,  aSb [ e. 

(b)  S--~  AS[b 

A - ~   SAla. (c) 

s ~ ,   SSlA A--~OAliSIO1. 

2.5.9.  Find  a  grammar  generating L(P),  where 

P  =  ([q0, ql,  q2],  [a,  b}, {Z0,  A],  6, q0, Z0,  [q2}) 

EXERCISES  1 91 
and  ~  is  given by 

6(qo,  a,  Zo)  --  (q~,  AZo) 

O(qo,  a,  A)  --  (q l,  AA) 
O(ql,  a,  A)  --  (qo,  AA)  

(~(ql,  e,  A)  --  (qz,  A) 
6(q2,  b,  A)  =  (q2,  e) 

Hint:  It  is  not  necessary  to  construct  the  productions  for  useless  non- 
terminals. 

"2.5.10.  Show  that  if P  --  (Q,  Z, F,  6, q0, Z0, F)  is  a PDA,  then  the  set  of  strings 

which  can  appear  on  the  pushdown  list  is  a  regular  set.  That  is,  show 

that  {0~ I (q0,  w, 2"o) ~  (q, x,  ~)  for  some q,  w,  and  x}  is  regular. 

2.5.11.  Complete  the  proof  of Lemma  2.26. 
2.5.12.  Let  P  be  a  PDA  for  which  there  is  a  constant  k  such  that  P  can  never 

have  more  than  k  symbols  on  its  pushdown  list  at  any  time.  Show  that 
L(P)  is  a  regular  set. 

2.5.13.  Give  DPDA's  accepting  the  following  languages: 

(a)  [0~1J IJ ~  i}. 
(b)  [wl w consists  of an  equal  number  of a's  and  b's}. 
(c)  L(Go),  where  Go  is the  usual  grammar  for rudimentary  arithmetic 
expressions. 

2.5.14.  Show  that  the  DPDA  of  Example  2.36  accepts  {wewRI  w  E  {a,  b}+}. 
2.5.15.  Show  that  if  the  construction  of  Lemma  2.21  is  applied  to  an  extended 

DPDA,  then  the  result  is  a  DPDA. 

2.5.16.  Prove  that  P  and  P'  in  Eemma  2.27  accept  the  same  language. 
2.5.17.  Prove  that  step  (2)  of Algorithm  2.16 correctly  distinguishes  C1  from  Cz. 
2.5.18.  Complete  the  proof  of Theorem  2.23. 
2.5.19.  The  PDA's  we  have  defined  make  a  move  independent  of  their  input 

unless  they  move  their  input  head.  We  could  relax  this  restriction  and 

allow  the  input  symbol  scanned  to  influence  the  move  even  when  the 
input  head  remains  stationary.  Show that  this  extension  still  accepts  only 

the  CFL's. 

*2.5.20.  We  could  further  augment  the  PDA  by  allowing  it  to  move  two  ways 

on  the  input.  Also,  let  the  device  have  endmarkers  on  the  input.  We  call 
such  an  automaton  a  2PDA,  and  if it  is  deterministic,  a  2DPDA.  Show 

that  the  following  languages  can  be  recognized  by  2DPDA's: 

(a)  {a"b"c" [ n  >  1}. (b)  (wwlw 

~  {a,  b}*}. (c)  {a2"l n  Z>  1}. 

2.5.21.  Show  that  a  2PDA  canrecognize  {wxw  I w  and  x  are  in  {0,  1}+}. 

192  ELEMENTS  OF  LANGUAGE THEORY  CHAP.  2 

Open  Questions 

2.5.22.  Does there exist a  language  accepted by  a  2PDA  that  is  not  accepted by 

a  2DPDA ? 

2.5.23.  Does  there exist a  CFL which  is  not  accepted  by any  2DPDA ? 

Programming  Exercises 

2.5.24.  Write a  program  that  simulates  a  deterministic  PDA. 
*2.5.25.  Devise  a  programming  language  that  can  be  used  to  specify  pushdown 

automata.  Construct  a  compiler  for  your  programming  language.  A 
source program in the language is to define a PDA P.  The object program 

is to be a recognizer which given an input string w simulates the behavior 
of P  on  w in  some reasonable sense. 

2.5.26.  Write  a  program  that  takes  as  input  CFG  G and  constructs  a  nondeter- 

ministic  top-down  (or  bottom-up)  recognizer  for  G. 

B I B L I OG R A P H I C   N O T E S  
The  importance  of pushdown  lists,  or  stacks,  as  they  are  also  known,  in  lan- 
guage  processing  was  recognized  by  the  early  1950's.  Oettinger  [1961]  and 
Schutzenberger  [1963] were the first to  formalize the concept  of a  pushdown  auto- 
maton.  The  equivalence  of  pushdown  automaton  languages  and  context-free 
languages  was  demonstrated  by  Chomsky  [1962] and  Evey  [1963]. 

Two-way  pushdown  automata  have  been  studied  by  Hartmanis  et  al.  [1965], 
Gray  et  al.  [1967], Aho  et  al.  [1968],  and  Cook  [1971]. 

2.6.  PROPERTIES  OF  CONTEXT-FREE 

LANGUAGES 

In  this  section  we  shall  examine  some  of the  basic  properties  of context- 
free  languages.  The  results  mentioned  here  are  actually  a  small  sampling  of 
the  great  wealth  of  knowledge  about  context-free  languages.  In  particular, 
we  shall  discuss  some  operations  under  which  CFL's  are  closed,  some  decid- 
ability  results,  and  matters  of  ambiguous  context-free  grammars  and  lan- 
guages. 

2.6.1.  Ogden's  Lemma 

We  begin  by  proving  a  theorem  (Ogden's  lemma)  about  context-free 
grammars  from  which  we  can  derive  a  number  of results  about  context-free 
languages.  From  this theorem we can  derive a  "pumping  lemma"  for context- 
free  languages. 

SEC. 2.6  PROPERTIES OF  CONTEXT-FREE LANGUAGES  193 

DEFINITION 

A  position  in  a  string  of length  k  is  an  integer  i  such  that  1 <  i <  k.  We 
say  that  symbol  a  occurs  at  position  i  of  string  w  if  w  =  w law 2  and i w~l= 

i  --  1.  For  example,  the  symbol  a  occurs  at  the  third  position  of  the 
string  baacc. 

THEOREM 2.24 

For  each  CF G   G  =  (N,  ~,  P,  S),  there  is  an  integer  k  ~  1  such  that  if 
z is in L(G), ]z[  ~  k, and if any k  or more distinct positions  in z are designated 

as being  "distinguished,"  then  z  can  be written  as  uvwxy  such  that 

(1)  w  contains  at  least  one  of  the  distinguished  positions. 
(2)  Either  u  and  v  both  contain  distinguished  positions,  or  x  and  y  both 
contain  distinguished  positions. 

(3)  vwx  has  at  most  k  distinguished  positions. 
(4)  There  is  a  nonterminal  A  such  that 

+  +  +  +  + S  ~  uAy  ~  uvAxy  :-  . . .   ~  uv~Axty ~  uv~wxiy 

G  G  G  G  G 
for  all  integers  i  (including  i  =  0,  in  which  case  the  derivation  is 

+  + S  ==~ uAy  ==~ uwy). 

o  G 

Proof.  Let  m  =  ~ N   and  l  be  the  length  of  the  longest  right  side  of 
a  production  in  P.  Choose  k  =  l 2"+3,  and  consider  a  derivation  tree  T  for 

some  sentence  z  in  L(G),  where  J z [ ~   k  and  at  least  k  positions  of  z  are 
designated  distinguished.  Note  that  T  must  contain  at  least  one  path  of 
length  at  least  2m  -k- 3.  We  can  distinguish  those  leaves  of  T  which,  in  the 
frontier  z  of  T,  fill  the  distinguished  positions. 

Let  us  call  node  n  of  T  a  branch  node  if n  has  at  least  two  direct  descen- 
dants,  say  n 1 and  n 2,  such  that  nl  and  n 2 both  have  distinguished  leaves  as descendants. 

We  construct  a  path  nl,  n 2 , . . ,   in  T  as  follows" 

(1)  nl  is  the  root  of  T. 
(2)  If we have  found  n i and  only  one  of nt's  direct  descendants  has  distin- 
guished  leaves  among  its  descendants  (i.e.,  nt  is  not  a  branch  node),  then 
let  n~+~ be  that  direct  descendant  of n~. 

(3)  If n~ is  a  branch  node,  choose  n~+~ to  be  that  direct  descendant  of  nt 
with  the  largest  number  of  distinguished  leaves  for  descendants.  If  there  is 
a  tie,  choose  the  rightmost  (this  choice  is  arbitrary). 

(4)  If ni  is  a  leaf,  terminate  the  path. 

Let  nl,  n 2 , . . .   ,np  be  the  path  so  constructed.  A  simple  induction  on  i 
shows  that  if  n i , . . . ,   n~  have  r  branch  nodes  among  them,  then  ni+l  has  at 
least  12m+3-r distinguished  descendants.  The  basis,  i  =  0,  is  trivial;  r  =  0, 

194  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 
and  nl  has  at  least  k  =  12m+3 distinguished  descendants.  For  the  induction, 

observe  that  if n~ is not  a  branch  node,  then n~ and  n~+ 1 have the  same  number 
of  distinguished  descendants,  and  that  if  n~  is  a  branch  node,  n~+ 1 has  at 
least  1/Ith  as  many. 

Since  n~  has  l 2m+3 distinguished  descendants,  the  path  n l , . . . ,   np  has  at 
least  2m  -k  3  branch  nodes.  Moreover,  np  is  a  leaf  and  so  is  not  a  branch 

node.  Thus,  p  >  2m  -q- 3. 

Let ba,b2,...,  b2m+3 be the last 2m /  3 branch  nodes  in the path  n l , . . .   ,n~. 
We  call  bt  a  left  branch  node  if  a  direct  descendant  of  b~  not  on  the  path 

has  a  distinguished  descendant  to  the  left  of  np  and  a  right  branch  node 

otherwise. 

We  assume  that  at  least  m  -t-  2  of  b l , . . . ,   b2m+3 are  left  branch  nodes. 
The  case  in  which  at  least  m  -k  2  are  right  branch  nodes  is  handled  analo- 
gously. 

Let  1 1 , . . . ,   lm+z  be  the  last  m- q - 2   left  branch  nodes  in  the  sequence b~,...,  b2m+3. 

Since  ~ N   =  m,  we  can  find  two  nodes  among  lz,  . . . ,   lm+2, 
say  I s  and  18, such  that f  <  g,  and  the  labels  of I r  and  lg are  the  same,  say A. 
This  situation  is  depicted  in  Fig.  2.17.  The  double  line  represents  the  path 
n 1,  . . . ,   np;  .'s  represent  distinguished  leaves,  but  there  may  be  others. 

If we  delete all  of ls's  descendants,  we  have  a  derivation  tree with frontier 
uAy,  where  u  represents  those  leaves  to  the  left  of  If  and  y  represents  those 

+ to  the  right.  Thus,  S  ==~ uAy.  If  we  consider  the  subtree  dominated  by  1 r 

+ with  the  descendants  of lg  deleted,  we  see  that  A  ==~ vAx,  where  v  and  x  are 

the  frontiers  from  the  descendant  leaves  of  lr  to  the  left  and  right,  respec- 
tively,  of  Ig.  Finally,  let  w  be  the  frontier  of  the  subtree  dominated  by  Ig. 

+ Then  A  ~  w.  We  observe  that  z  =  uvwxy. 

o  w  x  y 
Fig.  2.17  Derivation  tree  T. 

SEC. 2.6  PROPERTIES  OF  CONTEXT-FREE  LANGUAGES  195 

4-  + Putting  all  these  derivations  together,  we  have  S  =-~ uAy  ==~ uwy,  and 
+  +  +  +  +  + for  all  i  >  1,  S  =-~ uAy  ==~ u v A x y   =-~ uv2AxZy  ==~. .  .  ==~ uvtAxty  ==~ uvtwxty. 

Thus  condition  (4)  is  satisfied.  Moreover,  u  has  at  least  one  distinguished 
position,  the  descendant  of  some  direct  descendant  of  1~.  v  likewise 
has  at  least  one  distinguished  position,  descending  from  1 r.  Thus  condition 

(2) is  satisfied.  Condition  (1) is  satisfied,  since  w has  a  distinguished  position, 
namely  np. 

To  see  that  condition  (3),  that  vwx  has  no  more  than  k  distinguished 
positions,  is  satisfied,  we  observe  that  b 1,  being  the  2m  -t-- 3rd  branch  node 
from the  end  of path  n 1,  . . . ,   np, has  no  more than  k  distinguished  positions. 

Since  1 r  is  a  descendant  of b 1,  our  desired  result  is immediate. 

We  should  also  consider  the  alternative  case  in  which  at  least  m  -+  2  of 
b l,  . . . ,   b2,,+ 3 are right branch  nodes.  However,  this  case  is handled  symmet- 
rically,  and  we  shall  find  condition  (2)  satisfied  because  x  and  y  each  have 
distinguished  positions.  [~ 

An  important  corollary  of Ogden's  lemma  is  what  is  usually  referred  to 
as  the  pumping  lemma  for  context-free  languages. 

COROLLARY 

Let L  be  a  CFL.  Then  there  exists  a  constant  k  such  that  if lzl  >_ k  and 
z  ~  L,  then  we  can  write  z  =  uvwxy  such  that  vx  ~  e,  l v w x [ <   k,  and  for 

all  i,  uvtwxty  is  in L. 

Pr oof   In  Theorem  2.24,  choose  any  CF G  for  L  and  let  all  positions  of 
each  sentence  be  distinguished.  [~] 

It  is  the  corollary  to  Theorem  2.24  that  we  most  often  use  when  proving 
certain  languages  not  to  be  context-free.  Theorem  2.24  itself  will  be  used 

when  we  talk  about  inherent  ambiguity  of  CFL's  in  Section  2.6.5. 

Example  2.38 

Let  us  use  the  pumping  lemma  to  show  that  L  =  [a"'[n  >  1}  is  not  a 
CFL.  If L  were  a  CFL,  then  we  would have  an  integer k  such  that  if n 2 >  k, 
then a"'  =  uvwxy,  where v and x  are not both  e and i vwxl  <  k.  In particular, 

let  n  be  k  itself.  Certainly  k 2 >  k.  Then  uv2wxZy  is  supposedly  in  L.  But 

since  I vwxl_<  k,  we  have  1 _<lvxl_< k,  so  k 2 <  t uv~wx~y l <  k 2 +  k. 
But  the  next  perfect  square  after  k 2  is  (k  +  1 ) 2 =   k 2 +   2k  +  1.  Since 
k 2 +  k  <  k 2 +  2k  +  1,  we  see  that  luvZwxZy[  is  not  a  perfect  square.  But 

by  the  pumping  lemma,  uv2wx2y  is  in  L,  which  is  a  contradiction.  D 

Example 2.39 

Let  us  show  that  L  --  {a"b"c"[n  ~  1}  is  not  a  CFL.  If  it  were,  then  we 
would  have  a  constant  k  as  defined  in  the  pumping  lemma.  Let  z  =  akb~'e k. 

196  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 
Then  z  =  uvwxy.  Since  [vwxl_~  k,  it  is  not  possible  that  v  and  x  together 
have  occurrences  of a's,  b's,  and  c's;  vwx  will  not  "stretch"  across  the  k  b's. 
Thus,  uwy,  which  is  in  L  by  the  pumping  lemma,  has  either  k  a's  or  k  c's. 
It  does  not,  however,  have k  instances  of each  of the  three  symbols,  because 
l uwyl  <  3k. Thus,  uwy  has more  of one symbol than  another  and is not in L. 
We  thus  have  a  contradiction  and  can  conclude  only  that  L  is  not  context- 

free.  E] 

2.6.2.  Closure  Properties  of  CFL's 

Closure properties  can  often  be used to  help prove that  certain  languages 
are  not  context-free,  as  well  as  being  interesting  from  a  theoretical  point  of 
view.  In this section we shall summarize some of the major closure properties 

of the  context-free  languages. 

DEFINITION 

Let  ,,C be  a  class  of  languages  and  let  L  ~  ~*  be  in  ,C.  Suppose  that 
for  each  a  in  ~, La is a language in  ~.  ,,C is closed under  substitution if  for  all 

choices  of L, 

L ' = [ x l x 2 . . .   x, l a l a z . . . a ,   ~  L 

x l  ~  L,t 

x2  ~  Za, 

. 

o 

x , ~ L o . }  
is  in  #. 

Example 2.40 

Let L  =  [0"l"in  ~  1}, L0  =  [a},  and  L 1 =  [bmcmlm  _~  1}.  Then  the  sub- 
stitution  of L0  and  Li  into Z  is 

L'  --  [anbmlcmlbm"-c m . . . .   bm.cm"I n  >  1,  mt  ~>  1}  [--] 

THEOREM 2.25 

The  class  of context-free  languages  is  closed  under  substitution. 

Proof  Let  L  ~  Z*  be  a  CFL  where  ~  --  {a 1, a 2 , . . . ,   a,].  Let  L a _  ~* 
be a  CFL for each a  in ~.  Call the language that  results from the  substitution 

of the  La's  for  a  in L  by  the  name  L'.  Let  G  --  (N,  E, P,  S)  be  a  CFG  for L 
and  G~ --  (N,,  E,,  Pa, a')  be a  CFG  for La.  We  assume  that  N  and  all Na are 
mutually  disjoint.  Let  G ' =   (N',  E',  P',  S),  where 

SEC. 2.6  PROPERTIES OF  CONTEXT-FREE LANGUAGES  197 

(1)  N ' =   U  N~  w  N. aEE 
(2)  ~ff=  U  ~a" aEZ 

(3)  Let h  be the  homomorphism  on  N  U  Z  such  that  h(A)  =  A  for  all A  in 
N  and  h(a)  =  a'  for  a  in  Z.  Let  P'  =  {A  ----~ h(a) lA  ---~ a  is  in  P}  U  U  P,. aEZ 

Thus,  P'  consists  of the  productions  of the  Ga'S  together  with  the  produc- 
tions  of  G  with  all  terminals  made  (primed)  nonterminals.  Let  al  ...  a,  be 

!  f  ,  f i n L a n d x ~ i n L ~ , f o r l   ~ i ~ n .   T h e n S ~ a ' l   ...  a,  ~  xlaz  ..  a,  ~  .  .  . 
G'  G"  O' 

x~  ...  x,. Thus,  L'  _~ L(G'). 
G'  Suppose  that  w is in L(G')  and  consider  a  derivation  tree  T  of w.  Because 

of the  disjointness  of N  and  the  N,'s,  each  leaf with  non-e-label  has  at  least 
one  ancestor  labeled a'  for  some  a in Z.  If we delete all nodes  of Twhich  have 
an  ancestor  other  than  themselves  with  label  a'  for  a  ~  Z,  then  we  have 

a  derivation  tree  T'  with  frontier  a'l  . . .   a',,  where  a I  . . . a ,   is  in L.  If  we 
let  x i be  the  frontier  of  the  subtree  of  T  dominated  by  the  ith  leaf  of  T', 
then  w  =  x~  . . .   x,  and  xt  is  in  L,,.  Thus,  L(G')  =  L'.  [Z] 

COROLLARY 

The  context-free  languages  are  closed  under  (1)  union,  (2)  product, 
(3)  ,,  (4)  + ,   and  (5)  homomorphism. 

Proof.  Let L  a  and  L b be  context-free  languages. 

(1)  Substitute  L a for  a  and  Lb for  b  in  the  CFL  {a, b}. 
(2)  Substitute  Z  a  for  a  and  L b for  b  in  {ab}. 

(3)  Substitute  La  for  a  in  a*. 
(4)  Substitute  L,  for  a  in  a /. 
(5)  Let L~  =  {h(a)}  for  homomorphism  h,  and  substitute  into  L  to  obtain h(L). 

THEOREM  2.26 

The  class  of  context-free  languages  is  closed  under  intersection  with 
regular  sets. 

Proof  We  can  show  that  a  P DA  P  and  a  finite  automaton  A  running  in 
parallel  can  be  simulated  by  a  PDA  P'.  The  composite  PDA  P '  simulates  P 

directly  and  changes  the  state  of  A  each  time  P  makes  a  non-e-move.  P' 
accepts  if and  only  if both  P  accepts  and  A  is  in  a  final  state.  The  details  of 

such  a  proof  are  left  for  the  Exercises.  [Z] 

Unlike  the  regular  sets,  the  context-free  languages  are  not  a  Boolean 
algebra  of  sets. 

THEOREM  2.27 

The  class  of  context-free  languages  is  not  closed  under  intersection  or 
complement. 

1 98  ELEMENTS  OF LANGUAGE THEORY  CHAP.  2 

Proof  L 1 =  {a"b"dln  >  1,  i _>  1 } and L2 =  {db"c"[i  _> 1, n >  1 } are  both 
context-free languages.  However,  by Example 2.39, L1  ~  L z =  {a"b"c"[n  >  1 } 
is  not  a  context-free  language.  Thus  the  context-free  languages  are  not 

closed  under  intersection. 

We can also conclude  that  the  context-free  languages  are not  closed under 
complement.  This  follows  from  the  fact  that  any  class  of  languages  closed 
under  union  and  complement  must  also  be  closed  under  intersection,  using 
De  Morgan's  law. 

The  CFL's  are  closed  under  union  by the  corollary  to  Theorem  2.25.  [Z] 

There  are  many  other  operations  under  which  the  context-free  languages 
are  closed.  Some  of  these  operations  will  be  discussed  in  the  Exercises.  We 

shall  conclude  this  section  by  providing  a  few  applications  of  closure  prop- 
erties  in  showing  that  certain  sets  are  not  context-free  languages. 

Example 2.41 

L  =  {ww[w  ~  {a,  b} +}  is  not  a  context-free  language.  Suppose  that  L 
were  context-free.  Then  L'  =  L  A  a+b + a+b + =  {amb"amb" l m,  n  >  1 } would 

also  be  context-free  by  Theorem  2.26.  But  from  Exercise  2.6.3(e),  we  know 
that  L'  is  not  a  context-free  language.  [Z] 

Example 2.42 

L  =  {wwlw  ~  {c,f} +} is  not  a  context-free  language.  Let  h  be  the  homo- 
morphism  h ( c ) =   a  and  h ( f ) =   b.  Then  h ( L ) =   {wwlw  ~  {a,  b}+}, which  by 
the  previous  example  is  not  a  context-free  language.  Since  the  CFL's  are 
closed  under  homomorphism  (corollary  to  Theorem  2.25),  we  conclude 
that  L  is  not  a  CFL.  [2] 

Example 2.43 

AL GOL   is  not  a  context-free  language.  Consider  the  following  class  of 
A LG OL   programs: 

L  =  {begin  integer  w;  w : =   1;  endtw is  any  string  in  {c,f}+}. 
Let LA be  the  set  of all  valid  AL G OL   programs.  Let  R  be  the  regular  set 
denoted  by  the  regular  expression 

begin  integer  (c  +  f )+ ;   (c  / f ) +   :  =  1;  end 
Then  L  =  LA  ~  R.  Finally  let  h  be  the  homomorphism  such  that  h(c)  =  c, 
h ( f )   =  f,  and  h( X)   =  e  otherwise.  Then  h(L)  =  {ww [w  e  {c, f}+}. 

Consequently,  if LA is  context-free,  then  h(LA  ~  R)  must  also  be  context- 
free.  However,  we  know  that  h(LA  ~  R)  is  not  context-free  so  we  must  con- 

clude  that  L A,  the  set  of  all  valid  AL GOL   programs,  is  not  a  context-free 
language. 

SEC. 2.6  PROPERTIES  OF CONTEXT-FREE LANGUAGES  199 

Example  2.43  shows  that  a  programming  language  requiring  declaration 
of identifiers  which  can  be  arbitrarily  Iong is  not  context-free.  In  a  compiler, 
however,  identifiers  are  usually  handled  by  the  lexical  analyzer  and  reduced 
to  single  tokens  before  reaching  the  syntactic  analyzer.  Thus  the  language 

that  is  to  be  recognized  by  the  syntactic  analyzer  usually  can  be  considered 
to  be  a  context-free  language. 

There  are  other  non-context-free  aspects  of  ALGOL   and  many  other 
languages.  For example,  each procedure takes the same number of arguments 

each  time  it is mentioned.  It  is thus possible to  show that  the language which 
the  syntactic  analyzer  sees  is  not  context-free  by  mapping  programs  with 
three  calls  of the  same  procedure  to  {0"10"10" In  >  0},  which  is  not  a  CFL. 
Normally,  however,  some  process  outside  of  syntactic  analysis  is  used  to 
check  that  the  number  of  arguments  to  a  procedure  is  consistent  with  the 
definition  of the  procedure. 

2.6.3.  Decidability  Results 

We have already seen that the emptiness problem is decidable for context- 
free  grammars.  Algorithm  2.7  will  accept  any  context-free  grammar  G  as 

input  and  determine  whether  or  not  L(G)  is  empty. 

Let  us  consider  the  membership  problem  for  CFG's.  We  must  find 
an  algorithm  which  given  a  context-free  grammar  G  =  (N,  E, P,  S)  and 
a  word  w  in  E*,  will  determine  whether  or  not  w  is  in  L(G).  Obtaining  an 
efficient  algorithm  for  this  problem  will  provide  much  of the  subject  matter 

of  Chapters  4-7.  However,  from  a  purely  theoretical  point  of  view  we 
can  immediately  conclude  that  the  membership  problem  is  solvable  for 

CFG's,  since  we  can  always  transform  G  into  an  equivalent  proper  context- 
free  grammar  G'  using  the  transformations  of  Section  2.4.2.  Neglecting 
the  empty  word,  a  proper  context-free  grammar  is  a  context-sensitive  gram- 
mar,  so we can  apply the  brute  force  algorithm  for  deciding  the  membership 
problem  for  context-sensitive  grammars  to  G'.  (See  Exercise  2.1.19.) 

Let  us  consider  the  equivalence  problem  for  context-free  grammars. 
Unfortunately,  here  we  encounter  a  problem  which  is  not  decidable.  We 
shall prove that  there is no algorithm which, given any two CFG's  G1 and  G2, 
can  determine  whether L(G1)  =  L(G2).  In fact,  we shall show that  even given 

a  CFG  G1  and  a  right-linear  grammar  G2 there  is no  algorithm  to  determine 
whether L(Gi)  =  L(G2).  As  with  most  undecidable  problems,  we  shall  show 

that  if we  can  solve  the  equivalence  problem  for  CFG's,  then  we  can  solve 
Post's correspondence  problem.  We can  construct  from  an  instance  of Post's 
correspondence  problem  two  naturally  related  context-free  languages. 

DEFINITION 

Let  C  =  (x 1, y l ) , . . . ,   (x,, y,)  be  an  instance  of  Post's  problem  over 
alphabet  E.  Let  I =   {1, 2 , . . . ,   n},  assume  that  I ~   E  =  .*,  and  let  Lc 

200  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 
be  { x ~ , x , . . .   X~,.i,,,ir,,-t''"  i 1 1 i ~ , ' ' ' ,   im  are  in  L  m  ~>  1}. 
{YtlY,  "  " Yi,.i,,,ir,,-t  "'"  it  lit,  . . .   ,im  are  i n / ,   m  >  1}. 

LEMMA 2.29 

Let  Mc  be 

Let  C - - ( x t ,   Y t ) , . . . ,   (x,, y,)  be  an  instance  of  Post's  correspondence 
problem  over  X,  where  X  rq  {1,  2 . . . .   , n}  =  ~ .   Then 

(1)  We  can  find  extended  DPDA's  accepting  Lc  and  M o  
(2)  Lc  n  Mc  --  ~  if and  only  if  C  has  no  solution. 

Proof 

(1)  It is straightforward  to  construct  an  extended  D P D A  (with pushdown 
top  on  the  right)  which  stores  all  symbols  in  ~:  on  its  pushdown  list.  When 

symbols  from  { 1 , . . . ,   n}  appear  on  the  input,  it  pops  x~ from  the  top  of  its 
list if integer  i appears  on the input.  If x~ is not at the top  of the list, the D P D A 
halts.  The  D P D A   also  checks  with  its  finite  control  that  its  input  is  in 

X + { 1 , . . . ,   n} +  and  accepts  when  all  symbols  from  X  are  removed  from  the 
pushdown  list.  Thus,  L c  is  accepted.  We  may  find  an  extended  D P DA  
for  Mc  similarly. 

(2)  If L c  ~  Mc  contains  the  sentence  wire "'"  it,  where  w  is  in  X +,  then 
w  is  clearly  a  viable  sequence.  If xil  "-"  x~  =  yi~ - ..  y~, =  w,  then  wim  . . .   i t 
will  be  in Lc  ~  M o   D 

Let  us  return  to  the  equivalence  problem  for  CFG's.  We  need  two  addi- 
tional  languages  related  t o   an  instance  of  Post's  correspondence  problem. 

DEFINITION 

Let  C - - ( x ~ ,   Y t ) , . . . ,   (x,, y,)  be  an  instance  of  Post's  correspondence 
problem  over  X  and  let  I  =  ~ 1 , . . . ,   n}.  Assume  that  X  (3  I  =  ~ .   Define 

Q____c =  {w4pwR!] w  is  in  x+I+},  where  #  is not  in ]g or L  Define  Pc =  Lc:#:Mg. 

LEMMA 2.30 

Let  C  be  as  above.  Then 
(1)  We  can  find  extended  DPDA's  accepting  Qc  and  Pc,  and 
(2)  Qc  ~  Pc  =  ~  if and  only  if  C  has  no  solution. 

Proof. 

(1)  A  D P DA   accepting  Qc  can  be  constructed  easily.  For  Pc,  we  know 
by  Lemma  2.29  that  there  exists  a  DPDA,  say  M~,  accepting  L o   To  find 
a  D P D A   M2  that  accepts  M g   is  not  much  harder;  one  stores  integers  and 
checks  them  against the portion  of input  that  is in  X+.  Thus  we  can  construct 

D P D A   M3  to  simulate  Mr,  check  for  :#,  and  then  simulate  M2. 

(2)  If  u v ~ w x   is  in  Qc  ~  Pc,  where  u  and  x  are  in  X +  and  v  and  w  in 
1 +,  then  u  =  x ~,  and  v  =  w R,  because  uv@ w x  is  in  Qo  Because  u v ~ w x   is 

in  Pc,  u  is  a  viable  sequence.  Thus,  C  has  a  solution.  Conversely,  if we  have 
x i , ' "   xi,.  =  Yii  "'"  Yt,.,  then  x~,  . . .   Xt,,i m  '''  it  ~  il  "'"  i,,,x~.,..,  xi,  is  in 

Qc  A  Po 

SEC. 2.6  PROPERTIES OF CONTEXT-FREE LANGUAGES  201 

LEMMA  2.31 

Let  C  be  as  above.  Then 

(1)  We  can  find  a  CF G  for  Qc  u  Pc, and 
(2)  Qc  u  Pc =  (E  u  I)*  if and  only if C  has  no  solution. 

Proof. 

(1)  From the closure of deterministic  CFL's under complement (Theorem 
2.23),  we  can  find  DPDA's  for  Qc and  Pc.  From  the  equivalence  of  CFL's 

and  PDA  languages  (Lemma  2.26),  we  can  find  CFG's  for  these  languages. 

From  closure  of CFL's  under  union,  we  can  find  a  CFG  for  Qc  u  Pc. 

(2)  Immediate  from  Lemma  2.30(2)  and  De  Morgan's  law.  D 

We  can  now  show  that  it  is  undecidable  whether  two  CFG's  generate 
the  same  language.  In fact,  we can prove  somethingstronger"  It is still unde- 
cidable  even if one  of the  grammars  is  right-linear. 

THEOREM 2.28 

It  is undecidable  for  a  CF G  G~  and  a  right-linear  grammar  G 2 whether L(G~) =  L(G~). 

Proof.  If not,  then  we  could  decide  Post's  problem  as  follows" 

(1)  Given an instance  C, construct,  by Lemma 2.31, a  CFG  G~ generating Qc  L9 Pc, 

and  construct  a  right-linear  grammar  G z generating  the  regular  set 
(~  U  I)*,  where  C  is  over  ~,  C  has  lists  of  length  n,  and  I  =  {1 . . . .   , n}. 
Again,  some  renaming  of  symbols  may  first  be  necessary,  but  the  existence 

or  nonexistence  of a  solution  is  left  intact. 

(2)  Apply  the  hypothetical  algorithm  to  determine  if  L(G~)=  L(G2). 
By Lemma  2.31(2),  this  equality  holds  if and  only if C  has  no  solution.  [Z] 

Since  there  are  algorithms  to  convert  a  CF G  to  a  PDA  and  vice  versa, 
Theorem  2.28  also  implies  that  it  is  undecidable  whether  two  PDA's,  or 
a  PDA  and  a finite  automaton,  recognize the  same language,  whether  a  PDA 
recognizes  the  set  denoted  by  a  regular  expression,  and  so  forth. 

2.6.4.  Properties  of  Deterministic  CFL's 

The  deterministic  context-flee  languages  are  closed  under  remarkably 
few  of the  operations  under  which  the  entire  class  of context-free  languages 

is  closed.  We  already  know  that  the  deterministic  CFL's  are  closed  under 

complement.  Since La  =  {a~b~cJ[i, j  ~  1} and  L2  ---- {a~bJcJ[i, j  ~  1 } are  both 
deterministic  CFL's  and  L~  ~  Lz  =  {a"b"c"[n ~  1}  is  a  language  which  is 

not  context-free  (Example  2.39),  we  have  the  following  two  nonclosure 
properties. 

THEOREM 2.29 

The class of deterministic  CFL's is not closed under intersection  or union. 

202  ELEMENTS  OF  LANGUAGE  THEORY  CHAP.  2 

Proof.  Nonclosure  under  intersection  is  immediate  from  the  above. 
Nonclosure  under  union  follows  from  De  Morgan's  law  and  closure  under 

complement. 

The  deterministic  CFL's  form  a  proper  subset  of  the  CFL's,  as  we  see 
from  the  following  example. 

Example 2.44 

We can  easily show  that  the  complement  of L  :  {a"bnc"ln ~  1 } is a  CFL. 
The  sentence  w  ~  L  if and  only  if  one  or  more  of the  following  hold" 

(1)  w is  not  in  a+b /c /. 
(2)  w  :  aib@ k, and  i  ~  j. 
(3)  w  :  atbJc k,  and j  ~  k. 

The  set satisfying (1) is regular,  and  the  sets  satisfying  (2) and  (3) are  each 
context-free,  as  the  reader  can  easily  show  by  constructing  nondeterministic 

PDA's  recognizing them.  Since the  CFL's  are closed under  union,  L  is a CFL. 

But  if L  were  a  deterministic  CFL,  then L  would  be likewise,  by Theorem 
2.23.  But  L  is  not  even  a  CFL.  [Z] 

The  deterministic  CFL's  have  the  same  positive  decidability  results  as 
the  CFL.  That  is,  given  a  DP D A  P,  we  can  determine  whether  L (P ) =   ~, 
and  given  an  input  string  w,  we  can  easily  determine  whether  w  is  in L(P). 

Moreover,  given  a  deterministic  DP D A  P  and  a  regular  set  R,  we  can 
determine whether L(P)=  R, since L(P)=  R if and only if we have (L(P)n  R) U  (L(P)  n  R)  =  ~.  (L(P) 

n  /~)  U  (L(P)  N  R)  is  easily  seen  to  be  a  CFL. 
Other  decidability  results  appear  in  the  Exercises. 

2.6.5.  Ambiguity 

Recall  that  a  context-free  grammar  G  =  (N,  E, P,  S)  is  ambiguous  if 
there  is  a  sentence  w  in  L(G)  with  two  or  more  distinct  derivation  trees. 

Equivalently,  G  is  ambiguous  if  there  exists  a  sentence  w  with  two  distinct 
leftmost  (or  rightmost)  derivations. 

When  we  are  using  a  grammar  to  help  define  a  programming  language 
we  would  like  that  grammar  to  be  unambiguous.  Otherwise,  a  programmer 
and  a  compiler  may  have  differing  opinions  as  to  the  meaning  of  some 

sentences. 

Example 2.45 

Perhaps  the  most  famous  example  of  ambiguity  in  a  programming  lan- 
guage  is  the  dangling  else.  Consider  the  grammar  G  with  productions 

S  > if b  then  S  else  S[if  b  then  S la 

SEC. 2.6  PROPERTIES  OF  CONTEXT-FREE  LANGUAGES  203 

G  is  ambiguous  since  the  sentence 

if  b  then  if  b  then  a  else  a 
has  two  derivation  trees  as  shown  in  Fig.  2.18.  The  derivation  tree  in  Fig. 
2.18(a)  imposes  the  interpretation 

if  b  then  (if  b  then  a)  else  a 

while  the  tree  in  Fig.  2.18(b)  gives 

if  b  then  (if  b  then  a  else  a) 

S 
if 

(a) 
S 

if  b  then  S 

if  b  then  S  else i 

a 
(b) 
Fig. 2.18  Two derivation  trees. 

We  might  like  to  have  an  algorithm  to  determine  whether  an  arbitrary 
C F G  is unambiguous.  Unfortunately,  such  an  algorithm  does  not  exist. 

THEOREM 2.30 

It  is  undecidable  whether  a  CF G   G  is  ambiguous. 
Proof.  Let  C  =  (x 1, Yl),  * * *,  (x,, y,,)  be  an  instance  of Post's  correspon- 
dence  problem  over  X.  Let  G  be  the  C F G   (IS,  A,  B}, X  U / ,   P,  S),  where 
I  =  {1, 2 , . . . ,   hi,  and  P  contains  the  productions 

--  7 

204  ELEMENTS OF LANGUAGE THEORY  CHAP.  2 

S  >AI B 
A  > x~Ailxti, 

B  ~  yiBilyti, 

for  1 < i < n  

for  1 <  i < n  

The  nonterminals  A  and  B  generate  the  languages  Lc  and  Mc,  respec- 
tively,  defined  on  p.  199.  It  is easy to  see that  no  sentence  has  more  than  one 

distinct leftmost derivation  from A, or from B. Thus,  if there exists a  sentence 
with  two  leftmost  derivations  from  S,  one  must  begin  with  S  ~  A,  and Im 

the  other  with  S  ~  B.  But  by  Lemma  2.29,  there  is  a  sentence  derived  from 

Im both  A  and  B  if  and  only  if  instance  C  of  Post's  problem  has  a  solution. 

Thus,  G is ambiguous  if and  only if C  has  a  solution.  It is then  a  straight- 
forward  matter  to  show  that  if there  were  an  algorithm  to  decide  the  ambi- 

guity  of  an  arbitrary  CFG,  then  we  could  decide  Post's  correspondence 
problem.  [~] 

Ambiguity is a  function  of the grammar  rather than  the language.  Certain 
ambiguous  grammars  may  have  equivalent  unambiguous  ones. 

Example 2.46 

Let  us  consider  the  grammar  and  language  of  the  previous  example. 
The  reason  that  grammar  G  is  ambiguous  is  that  an  else  can  be  associated 
with  two  different  then's.  For  this  reason,  programming  languages  which 

allow  both  if-then--else  and  if-then  statements  can  be  ambiguous.  This 
ambiguity  can  be  removed  if  we  arbitrarily  decide  that  an  else  should  be 
attached  to  the  last  preceding  then,  as  in  Fig.  2.18(b). 

We  can  revise  the  grammar  of  Example  2.45  to  have  two  nonterminals, 
S~  and  $2.  We insist that  $2  generate  if-then-else,  while S~  is free  to  generate 
either  kind  of statement.  The  rules  of the  new  grammar  are 

S 1 
$2 

> if b  then  S~lif  b  then  $2  else  Sxla 
> if b  then  $2  else  S2la 

The  fact  that  only  $2  precedes  else  ensures  that  between  the  then-else 
pair  generated  by any one production  must  appear  either  the single  symbol  a 

or another  else.  Thus the  structure  of  Fig.  2.18(a)  cannot  occur.  In Chapter  5 
we  shall  develop  deterministic  parsing  methods  for  various  grammars, 
including  the  current  one,  and  shall  be  able  at  that  time  to  prove  our  new 
grammar  to  be  unambiguous.  [Z] 

Although  there  is no  general  algorithm  which  can  be  used  to  determine  if 
a grammar  is ambiguous,  it is possible  to isolate certain constructs  in produc- 
tions  which  lead  to  ambiguous  grammars.  Since  ambiguous  grammars  are 

SEC. 2.6  PROPERTIES  OF  CONTEXT-FREE  LANGUAGES  205 

often  harder  to  parse  than  unambiguous  ones,  we  shall  mention some  of the 
more  common  constructs  of  this  nature  here  so  that  they  can  be  recognized 
in  practice. 

A  proper  grammar  containing  the  productions  A--~  AAI;o~ will  be  am- 
biguous  because  the  substring  A A A   has  two  parses- 

A  A 
A / \   / \  
A  A  A  A 

This  ambiguity  disappears  if instead  we  use  the  productions 

or  the  productions 

A  >AB1B 
B  > ~  

A  > B A IB  

B  > c 

Another  example  of an  ambiguous  production  is  A  ~  AocA.  The  pair  of 
productions  A  --~  ocA ] A fl  introduces  ambiguity  since  A  ~=~ ocA  ~  ~A fl  and 
A  ~>  Aft  ~=~ ~Afl  imply  two  distinct  leftmost  derivations  of  ocAfl.  A  slightly 

more  elaborate  pair  of productions  which  gives  rise  to  an  ambiguous  gram- 
mar  is  A - -~   ocA ]o cArlA.  Other  exampIes  of  ambiguous  grammars  can  be 
found  in  the  Exercises. 

We  shall  call  a  CFL  inherently  ambiguous  if  it  has  no  unambiguous 
CFG.  It  is  not  at  first  obvious  that  there  is  such  a  thing  as  an  inherently 
ambiguous  CFL,  but  we  shall  present  one  in  the  next  example.  In  fact,  it  is 
undecidable  whether  a  given  CFG  generates  an  inherently  ambiguous  lan- 
guage  (i.e.,  whether  there  exists  an  equivalent  unambiguous  CFG).  However, 

there are large subclasses  of the  CFL's  known not  to be inherently ambiguous 
and  no  inherently  ambiguous  programming  languages  have  been  devised  yet. 

Most  important,  every  deterministic  CFL  has  an  unambiguous  grammar,  as 
we  shall  see  in  Chapter  8. 

Example 2.47 

Let  L - - [ a i b : c 1 ] i - - j   or  j  ~  l~.  L  is  an  inherently  ambiguous  CFL. 
Intuitively,  the  reason  is  that  the  words  with  i - - j   must  be  generated  by 
a  set  of  productions  different  from  those  generating  the  words  with j - -   I. 
At  least  some  of the  words  with  i  -- j  --  l must  be  generated  by  both  mecha- 

nisms. 

206  ELEMENTS OF LANGUAGE  THEORY  CHAP.  2 

One  CFG  for L  is 

S:  ~ > A B   I D C  
A  > a A l e  

B  > b B c l e  
C  > cCl  e 
D  > a D b [ e 

Clearly  the  above  grammar  is  ambiguous. 

We  can  use  Ogden's  lemma  to  prove  that L  is  inherently  ambiguous.  Let 
G  be  an  arbitrary  grammar  for  L,  and  let  k  be  the  constant  associated  with 
G  in  Theorem  2.24.  If  that  constant  is  less  than  3,  let  k  =  3.  Consider  the 
word  z  =  akbkc k+k',  where  the  a's  are  all  distinguished.  We  can  write 
z  =  uvwxy.  Since  w  has  distinguished  positions,  u  and  v  consist  only  of  a's. 

If x  consists  of two  different  symbols,  then  uvZwx~y  is  surely  not  in L,  so  x 
is  either  in  a*,  b*,  or  c*.  kt 

If x  is in a*,  uv2wx2y  would be  ak+Pbkc k+  for  some p,  1 ~  p  ~  k,  which 
is not in L.  If x  is in  c*,  uv2wx2y  would be  ak+P'bkc k/kt/~'', where  1 ~  pt  ~  k. 
This  word  likewise  is  not  in L. 

In  the  second  case,  where  x  is  in  b*,  we  have  uv2wx2y  =  ak+P'bk+P'c k/k:, 
where  1 ~  p~  ~  k.  If this  word  is  in  L,  then  either p~  =  P2  or p~  :c: P2  and 
p~  =  k!  In  the  latter  case,  uv3wx3y  =  ak+2Plbk+Zr'C k+kt  is  surely  not  in  L. 

So  we  conclude  that Pl  =  P2-  Observe  that pl  =  Iv[  and  pz  ----[x]. 

By  Theorem  2.24,  there  is  a  derivation 

+  +  + (2.6.1)  S  ~  uAy  ~  u v m A x ' y  

~  btvmwxmy  for  all  m  :>  0 

In  particular,  let  m  =  k!/p 1.  Since  1 ~  pl  ~  k,  we  know  that  m  is  an 
integer.  Then  uv'nwxmy  :  ak+k:bk+ktC k+k:. 

A  symmetric  argument  starting  with  the  word  ak+k~bkc k  shows  that 

!  !  W  ! there  exist  u ,  v,   , x', y',  where  only  u'  has  an  a,  v'  is  in  b*,  and  there  is 
a  nonterminal  B  such  that 

(2.6.2) 

+  +  + S  ~  u'By'  ~  u'(v')m'B(x')m'y '  ~  U'(v')m'w(x')m'y 

=  ak+k!bk+ktck+kt 
If  we  can  show  that  the  two  derivations  of  ak+k~be+ktC k/kt  have  different 
derivation  trees,  then  we  shall  have  shown  that  L  is  inherently  ambiguous, 
since  G was chosen  without restriction  and  has  been  shown  ambiguous. 

Suppose  that  the  two  derivations  (2.6.1)  and  (2.6.2) have  the  same  deriva- 
tion  tree.  Since  A  generates  a's  and  b's  and  B  generates  b's  and  c's,  neither 
A  nor  B  could  appear  as  a  label  of  a  descendant  of  a  node  labeled  by  the 

EXERCISES  207 
other.  Thus  there  exists  a  sentential  form  tiAt2Bt3,  where  the  t's  are terminal 
strings.  Fo r  all  i  and j,  tlv~wxq2(v')iw'(x')it3  would  presumably  be  in  L.  But 
I vl  =  ]x[  and  [v'l  =  Ix' [.  Also,  x  and  v'  consist  exclusively  of b's,  v  consists 

of  a's,  and  x'  consists  of  c's.  Thus  choosing  i  and  j  equal  and  sufficiently 
large  will  ensure  that  the  above  word  has  more  b's  than  a's  or  o's.  We  may 
thus  conclude  that  G  is  ambiguous  and  that  L  is  inherently  ambiguous.  []  

EXERCISES 
2.6.1.  Let  L  be  a  context-free  language  and  R  a  regular  set.  Show  that  the 

following  languages  are  context-free: 

(a)  INIT(L). 
(b)  FIN(L). 
(c)  SUB(L). 

(d)  L/R. (e)  L  ~  R. 

The  definitions  of  these  operations  are  found  in  the  Exercises  of  Sec- tion  2.3  on  p.  135. 

2.6.2.  Show  that  if L  is  a  CFL  and  h  a  homomorphism,  then  h-a(L)  is a  CFL. 

Hint:  Let P  be  a  PDA  accepting  L.  Construct  P'  to  apply  h  to  each  of 
its  input  symbols  in  turn,  store  the  result  in  a  buffer  (in  the  finite  con- 
trol),  and  simulate  P  on  the  symbols  in  the  buffer.  Be  sure  that  your buffer  is  of finite  length. 

2.6,3.  Show  that  the  following  are  not  CFL's: 

(a)  [aibic j [j <  i]. 
(b)  {a~bJckl  i  <  j  <  k}. 
(c)  The  set  of strings  with  an  equal  number  of a's,  b's,  and  c's. 
(d)  [a'bJaJb' IJ ~  i]. (e)  [amb"a'~b ~ [m, n  ~  1]. 

(f)  {a~biek[none  of i, j,  and  k  are  equal}. 
(g)  {nHa ~ [n  is  a  decimal  integer >  1}.  (This  construct  is  representative of F OR T R AN  Hollerith  fields.) 

**2.6.4.  Show  that  every  CFL  over  a  one-symbol  alphabet  is  regular.  Hint: 

Use  the  pumping  lemma. 

**2.6.$.  Show  that  the  following  are  not  always  CFL's  when  L  is  a  CFL: 

(a)  MAX(L). 
(b)  MIN(L). 
(c)  L 1/2  =  [x[ for  some y,  xy  is in L  and  [xl  =  [y[}. 

*2.6.6.  Show the following pumping  lemma for linear languages.  If L  is a  linear 

language,  there  is  a  constant  k  such  that  if  z  ~  L  and  Iz]  >  k,  then 
z  =  uvwxy,  where[uvxy]  <  k,  vx  ~  e  and  for  all  i,  uvtwx~y  is  in  L. 

2.6.7.  Show  that  [a"b"amb m In, m  >  1} is  not  a  linear  language. 
'2.6.8.  A  one-turn  PDA  is  one  which  in  any  sequence  of  moves  first  writes 

208  ELEMENTS OF  LANGUAGE  THEORY  CHAP.  2 

symbols  on  the  pushdown  list  and  then  pops  symbols  from  the  push- 
down  list.  Once  it  starts  popping  symbols  from  the  pushdown  list,  it 

can  then  never  write  on  its  pushdown  list.  Show  that  a  CFL  is  linear 

if and  only  if it  can  be  recognized  by  a  one-turn  PDA. 

*2.6.9.  Let  G  =  (N,  2~, P,  S)  be  a  CFG,  Show  that  the  following  are  CFL's" 

(a)  {tg[S ==~ tg}. 

Im 

(b)  { ~ l s ~   ~}. 

rm 

(c)  (~ls=~  ~]. 
2.6.10.  Give  details  of the  proof  of the  corollary  to  Theorem  2.25. 

2.6.11.  Complete  the  proof  of Theorem  2.26. 
2.6.12.  Give formal  constructions  of the DPDA's  used in the proofs  of Lemmas 

2.29( 1 )  and  2.30(1 ). 

"2.6.13.  Show  that  the  language  Qc  n  Pc  of Section  2.6.3  is  a  CFL  if  and  only 

if it  is  empty. 

2.6.14.  Show  that  it  is  undecidable  for  CFG  G  whether 

(a)  L(G) is  a  CFL. 

(b)  L(G) is regular. 
(c)  L(G) is  a  deterministic  CFL. 
Hint:  Use  Exercise  2.6.13  and  consider  a  CFG  for  Qc  u  Pc. 

2.6.15.  Show  that  it  is  undecidable  whether  context-sensitive  grammar  G 

generates  a  CFL. 

2.6.16.  Let  G1  and  G2  be  CFG's.  Show  that  it  is  undecidable  whether 

L(G1)  n  L(Gz)  =  ~ . 

"2.6.17.  Let  G1  be  a  CFG  and  Gz  a  right-linear  grammar.  Show  that 

(a)  It  is  undecidable  whether L(Gz)  ~  L(G1). 
(b)  It  is  decidable  whether L(Gi)  ~  L(G2). 

"2.6.18.  Let P1  and  Pz  be  DPDA's.  Show  that  it  is  undecidable  whether 

(a)  L(Pi)  U  L(Pz)  is  a  deterministic  CFL. 
(b)  L(Pa)L(P2) is  a  deterministic  CFL. 

(c)  L(P1)  ~  L(Pz). 

(d)  L(P1)*  is  a  deterministic  CFL. 

*'2.6.19,  Show  that  it  is  decidable,  for  D PDA   P,  whether  L(P)  is  regular.  Con- 

trast  Exercise  2.6.14(b). 

**2.6.20.  Let  L  be  a  deterministic  CFL  and  R  a  regular  set.  Show  that  the  fol- 

lowing  are  deterministic  CFL's" 

(a)  LR.  (b)  L/R. 
(c)  L  u  R.  (d)  MAX(L). 
(e)  MIN(L).  (f)  L  N  R. Hint: 

For  (a,  b,  e, f),  let P  be  a  D PD A   for L  and M  a  finite  automaton 
for  some  regular  set R.  We  must  show  that  there  is  a  D PD A   P'  which 

EXERCISES  209 
simulates P  but  keeps  on  each  cell of its  pushdown  list the  information, 

"For  what  states p  of  M  and  q  of P  does  there  exist  w  that  will  take 
M  from  state p  to  a  final  state  and  cause P  to  accept  if started  in  state 
q  with  this  cell  the  top  of  the  pushdown  list ?"  We  must  show  that 

there  is  but  a  finite  amount  of  information  for  each  cell  and  that  P' 
can  keep  track  of  it  as  the  pushdown  list  grows  and  shrinks.  Once  we 
know  how  to  construct P',  the  four  desired  DPDA's  are  relatively easy 
to  construct. 

2.6.21.  Show  that  for  deterministic  CFL  L  and  regular  set  R,  the  following 

may  not  be  deterministic  CFL's: 

(a)  RL. 

(b)  [ x lx R  ~  L]. 
(c)  {xi for  some  y  ~  R,  we  have y x  ~  L}. 
(d)  h(L),  for  homomorphism  h. 

2.6.22.  Show  that  h-I(L)  is  a  deterministic  CFL  if L  is. 
**2.6.23.  Show  that  Qc  u  Pc  is  an  inherently  ambiguous  CFL  whenever  it  is 

not  empty. 

**2.6.24.  Show  that  it  is  undecidable  whether  a  C F G   G  generates  an  inherently 

ambiguous  language. 

*2.6.25.  Show  that  the  grammar  of Example  2.46  is  unambiguous. 
**2.6.26.  Show  that  the  language  LI  U  Lz,  where  L1  =  {a'b'ambmlm,  n  >  1} 

and  L2  =  {a'bma~b" i m, n  >  1 },  is  inherently  ambiguous. 

**2.6.27.  Shove  that  the  C F G   with  productions  S----~  a S b S cl aS b lb S c ld   is 

ambiguous.  Is  the  language  inherently ambiguous ? 

*2.6.28.  Show  that  it  is  decidable  for  a  D P D A   P  whether  L(P)  has  the  prefix 

property.  Is  the  prefix  property  decidable  for  an  arbitrary  CFL ? 

DEFINITION 

A  Dyck   language  is  a  CFL  generated  by  a  grammar  G  = 
({S}, Z, P,  S),  where  E  =  {al . . . . .   a~, bl . . . . .   bk} for  some  k  _~  1 and 
P  consists  of  the  productions  S  --~  S S la iS b l  la 2 S b zl " "  lakSbkle. 

**2.6.29.  Show  that  given  alphabet  Z,  we  can  find  an  alphabet  Z',  a  Dyck  lan- 

guage LD  ~  ~',  and  a  homorphism  h  from  Z'*  to  Z*  such  that  for  any 
CFL   L  ~  Z*  there  is  a  regular  set  R  such  that  h(LD  ~  R)  =  L. 

*2.6.30.  Let  L  be  a  CFL  and  S( L)=   [ilfor  some  w  ~  L,  we  have  Iwl  =  i}. 

Show  that  S(L)  is  a  finite union  of arithmetic  progressions. 

DEFINITION 

An n-vector  is an n-tuple of nonnegative integers. If v i  =  (a l , . . . ,   a,) 
and  v~  =  (bl . . . . .   b,)  are  n-vectors  and  c  a  nonnegative  integer,  then 
v l  -~- vz  =  (a l  /  b l .... .  a,  +  b,)  and  cv l  =  (ca l ,  .  .  .,  ca,).  A  set  S 

of  n-vectors  is  linear  if  there  are  n-vectors  v0 . . . . .   Vk  such  that 
S = [v lv   = v 0   +  ClVl  +  ...   +  CkVk,  for  some  nonnegative  integers 

21 0  ELEMENTS OF  LANGUAGE THEORY  CHAP.  2 
*'2.6.31. 

**2.6.32. 

Cl . . . . .   ck].  A  set  of  n-vectors  is  semilinear  if it  is  the  union  of  a  finite 
number  of linear  sets. 

Let  E - - a ~ ,   a2, . . . .   a,.  Let  ~b(x)  be  the  number  of  instances  of  b 
in  the  string  x.  Show  that  [(#al(w),  @as(w) . . . . .   :~a.(w))l w  ~  L)  is  a 

semilinear  set  for  each  C FL  L  _  E*. 

DEFINITION 

The  index  of  a  derivation  in  a  CF G   G  is  the  maximum  number  of 
nonterminals  in  any  sentential  form  of  that  derivation,  l(w),  the  index 
of a  sentence  w,  is  the  smallest  index  of  any  derivation  of  w  in  G.  I(G), 

the  index  of  G,  is  max  I(w)  taken  over  all  w  in  L(G).  The  index  of  a 

CFL  L  is  min  I(G)  taken  over  all  G  such  that  L(G)  =  L. 

Show  that  the  index  of  the  grammar  G  with  productions 

S  ~  SSIOSI  le 

*2.6.33. 

2.6.34. 
*2.6.35. 

**2.6.36. 

is  infinite.  Show  that  the  index  of  L(G)  is  infinite. + 
A  C F G   G  --  (N,  Z, P,  S)  is  self-embedding  if A  =~  uAv  for  some  u  and 
v  in  Z +.  (Neither  u  nor  v  can  be  e.)  Show  that  a  CF L  L  is  not  regular 

if  and  only  if  all  grammars  that  generate  L  are  self-embedding. 

DEFINITION 

Let  ~  be  a  class  of  languages  with  L1  ~  E~  and  L2  ~  Ez*  in  #. 
Let  a  and  b  be  new  symbols  not  in  Z1  U  Z2.  ~  is  closed  under 

(1)  Marked  union  if aL 1 U  bL2  is  in  #, 
(2)  Marked  concatenation  if LlaL2  is  in L,  and 

(3)  Marked  *  if (aLl)*  is  in  #. 

Show  that  the  deterministic  CFL's  are  closed  under  marked  union, 
marked  concatenation,  and  marked  .. 

Let  G  be  a  (not  necessarily  context-free)  grammar  (N,  Z, P,  S),  where 
each  production  in  P  is  of  the  form  xAy  ~  x~y,  x  and  y  are  in  Z*, 
A  E  N,  and  ~'  E  (N  u  E)*.  Show  that  L(G)  is  a  CFL. 

Let  G1  --  (N1,  El,  P1,  $1)  and  G2  --  (N2,  Z2, Pz,  $2)  be  two  CFG's. 
Show  it  is  undecidable  whether  [~1S1  - > ~ }   = [ f l l S z : = ~ f l }   and Gl  G2 

whether  [~1Si  ~  0~} =  [fllS2  ~  fl]. Gi  Im  G~ Im 

Open  Problem 

2.6.37.  Is  it  decidable,  for  DP DA 's  PI  and  P2,  whether  L(P1)  --  L(P2)? 

Research  Problems 

2.6.38.  Develop  methods  for  proving  certain  grammars  to  be  unambiguous. 

By  Theorem  2.30  it  is  impossible  to  find  a  method  that  will  work  for 

BIBLIOGRAPHIC  NOTES  211 
an arbitrary  unambiguous  grammar.  However,  it would be nice to have 
techniques  that  could  be  applied  to  large  classes  of context-free  gram- 
mars. 

2.6.39.  A  related  research  area  is  to  find  large  classes  of  CFL's  which  are 

known  to  have  at  least  one  unambiguous  CFG.  The  reader  should  be 

aware  that  in  Chapter  8 we  shall  prove  the  deterministic  CFL's  to  be 
such  a  class. 

2.6.40.  Find  transformations  which  can  be used  to  make  classes  of ambiguous 

grammars  unambiguous. 

B IB L I OGR A P HIC   NOT E S  
We shall  not  attempt  to  reference here  all the  numerous  papers  that  have been 
written  on  context-free  languages.  The  works  by  Hopcroft  and  Ullman  [1969], 

Ginsburg  [1966], Gross  and  Lentin  [1970], and  Book  [1970] contain  many  of the 
references on the theoretical  developments of context-free languages. 

Theorem  2.24,  Ogden's  lemma,  is  from  Ogden  [1968]. Bar-Hillel  et  al.  [1961] 
give several of the basic theorems  about  closure properties  and  decidability results of  CFL's.  Ginsburg  and  Greibach  [1966] give  many  of  the  basic  properties  of 

deterministic  CFL's. 

Cantor  [1962], Floyd  [1962a],  and  Chomsky  and  Schutzenberger  [19631 inde- 
pendently  discovered  that  it  is  undecidable  whether  a  CFG  is  ambiguous.  The 
existence  of  inherently  ambiguous  CFL's  was  noted  by  Parikh  [1966]. Inherently 

ambiguous  CFL's  are  treated  in  detail  by  Ginsburg  [1966]  and  Hopcroft  and 
Ullman  [19691. 

The Exercises contain many results that  appear in the literature.  Exercise 2.6.19 
is  from  Stearns  [1967]. The  constructions  hinted  at  in  Exercise  2.6.20  are  given  in 

detail  by  Hopcroft  and  Ullman  [1969].  Exercise  2.6.29  is  proved  by  Ginsburg 
[1966]. Exercise 2.6.31  is known  as Parikh's  theorem and  was first given by  Parikh 
[1966].  Exercise  2.6.32  is  from  Salomaa  [1969b].  Exercise  2.6.33  is  from  Chomsky 
[1959a].  Exercise 2.6.36  is from  Blattner  [19721. 

THEORY  OF 
TRANSL ATION 

A  translation  is  a  set  of pairs  of strings.  A  compiler  defines  a  translation 
in  which  the  pairs  are  (source  program,  object  program).  If  we  consider 

a  compiler  consisting  of the  three  phases  lexical  analysis,  syntactic  analysis, 
and  code  generation,  then  each  of  these  phases  itself  defines  a  translation. 
As  we mentioned  in  Chapter  1, lexical analysis  can  be  considered  as a  trans- 
lation in which  strings representing  source programs  are  mapped into sti'ings 

of tokens.  The  syntactic  analyzer  maps  strings  of tokens  into  strings  repre- 
senting  trees.  The  code  generator  then  takes  these  strings  into  machine  or assembly  language. 

In  this  chapter  we  shall  present  some  elementary  methods  for  defining 
translations.  We  shall  also  present  devices  which  can  be  used  to  implement 
these  translations  and  algorithms  which  can  be  used  to  automatically  con- 
struct  these  devices from  the  specification  of a  translation. 

We shall first explore translations from an abstract point of view and then 
consider  the  applicability  of  the  translation  models  to  lexical  analysis  and 
syntactic analysis.  For the  most part,  we defer  treatment  of code  generation, 
which  is  the  principal  application  of translation  theory,  to  Chapter  9. 

In general,  when designing a  large  system, such as a compiler,  one should 
partition  the  overall system into  components  whose behavior  and  properties 

can  be  understood  and  precisely  defined.  Then  it  is  possible  to  compare 
algorithms which  can  be  used to implement the function  to  be performed  by 
that  component  and  to  select  the  most  appropriate  algorithm  for  that  com- 
ponent.  Once the components have been isolated and specified, it should then 

also  be possible to  establish performance  standards  for each  component  and 

tests  by  which  a  given  component  can  be  evaluated.  We  must  therefore 

212 

SEC. 3.1  FORMALISMS FOR TRANSLATIONS  213 

understand  the  specification  and  implementation  of  translations  before  we 
can  apply  engineering  design  criteria  to  compilers. 

3.1.  FORMALISMS  FOR  TRANSLATIONS 

In  this  section  two  fundamental  methods  of  defining  translations  are 
presented.  One  of these is the  "translation  scheme,"  which is a  grammar with 
a  mechanism for producing an  output for each  sentence generated.  The  other 
method is the  "transducer,"  a  recognizer which  can emit a  finite-length  string 

of output  symbols on each  move.  First we shall consider translation  schemes 
based  on  context-free  grammars.  We  shall  then  consider  finite  transducers 
and  pushdown  transducers. 

3.1.1.  Translation  and  Semantics 

In Chapter 2 we considered  only the  syntactic aspects  of languages.  There 
we saw several methods  for defining the well-formed sentences  of a  language. 
We  now  wish  to  investigate  techniques  for  associating  with  each  sentence  of 

a  language  another  string  which  is  to  be  the  output  for  that  sentence.  The 
term  "semantics"  is  sometimes  used  to  denote  this  association  of  outputs 

with  sentences  when  the  output  string  defines  the  "meaning"  of  the  input sentence. 

DEFINITION 

Suppose that  Z  is an input  alphabet  and A an output  alphabet.  We define 
a  translation  f r o m   a  language  L  1 ~  E*  to  a  language  L 2  ~  A*  as  a  relation 
T  from  Z*  to  A*  such  that  the  domain  of  T  is L 1 and  the  range  of T  is L 2. 

A  sentence  y  such  that  (x, y)  is  in  T  is  called  an  output  for  x.  Note  that, 
in  general,  in  a  translation  a  given  input  can  have  more  than  one  output. 

However,  any  translation  describing  a  programming  language  should  be 
a  function  (i.e.,  there  exists at  most  one  output  for each input). 

There  are  many  examples  of translations.  Perhaps  the  most  rudimentary 
type  of translation  is that  which  can  be  specified  by a  homomorphism. 

Example  3.1 

Suppose  that  we  wish  to  change  every  Greek  letter  in  a  sentence  in  E* 
into  its  corresponding  English  name.  We  can  use  the  homomorphism  h, where 

(1)  h(a)  --  a  if a  is  a  member  of Z  minus  the  Greek  letters  and 
(2)  h(a)  is  defined  in  the  following table  if a  is  a  Greek  letter: 

214  THEORY  OF TRANSLATION  CHAP.  3 

Greek Letter  h  Greek Letter  h 

A  a  alpha  N  v  nu B  fl  beta  E,  ~  xi 

F  y  gamma  O  o  omicron A  5  delta  II  r~  pi 
E  e  epsilon  P  p  rho Z  (  zeta  Z  a  sigma 
H  1/  eta  T  z  tau O  0  theta  T  o  upsilon 
I  z  iota  *  ~b  phi K  x  kappa  X  Z  chi 
A  2  lambda  ~F  V  psi M  g  mu  ~  co  omega 

For  example,  the  sentence  a  =  nr 2 would have  the translation  a  =  pi r 2.  [Z] 

Another  example  of  a  translation,  one  which  is  useful  in  describing 
a  process  that  often  occurs in compilation,  is mapping  arithmetic  expressions 
in  infix  notation  into  equivalent  expressions  in  Polish  notation. 

DEFINITION 

There  is a  useful  way of representing  ordinary  (or infix)  arithmetic  expres- 
sions  without  using  parentheses.  This  notation  is  referred  to  as  Polish  nota- 

tion.t  Let  19 be  a  set  of binary  operators  (e.g.,  [ + ,   ,]),  and  let  Z  be  a  set  of 
operands.  The  two forms  of Polish  notation, prefix  Polish  and postfix  Polish, 
are  defined  recursively  as  follows: 

(1)  If an infix expression E  is a  single  operand a,  in Z, then  both the prefix 
Polish  and  postfix  Polish  representation  of E  is  a. 

(2)  If E1 0  E2  is  an  infix  expression,  where  0  is  an  operator,  and  Et  and 
E2  are  infix  expressions,  the  operands  of  0,  then 

(a)  0 E'tE~  is  the  prefix  Polish  representation  of  El  0  E2,  where  E'i 

and  E~  are  the  prefix  Polish  representations  of E~ and  Ez,  respec- 
tively,  and 
(b)  ,_.~r" r"' 0 ~ z   is  the  postfix  Polish  representation  of  Ei  0 Ez,  where E~' 

and E~  are the postfix Polish  representations  of E~ and Ez,  respec- 
tively. 
(3)  If  (E)  is  an  infix  expression,  then 

(a)  The  prefix  Polish  representation  of  (E) is  the  prefix  Polish repre- 

sentation  of E,  and 
(b)  The  postfix  Polish  representation  of  (E)  is  the  postfix  Polish 

representation  of E. 

]'The term "Polish" is used, as this notation was first described by the Polish mathe- matician  Lukasiewicz, whose name is significantly harder to pronounce than  is "Polish." 

SEC. 3.1  FORMALISMS FOR  TRANSLATIONS  215 

Example 3.2 

Consider  the  infix  expression  (a  +  b)  *  c.  This  expression  is  of the  form 
E1  * E2,  where  E1  =  (a  q-  b)  and  E2  =  c.  Thus  the  prefix  and  postfix  Polish 
expressions  for  E2 are  both  c.  The prefix  expression  for  E~ is the  same  as that 
for  a-q-b,  which  is  q-ab.  Thus  the  prefix  expression  for  (a  /  b ) ,   c  is ,+abe. 

Similarly,  the  postfix  expression  for  a  /  b  is  ab  /,  so  the  postfix  expres- 
sion  for  (a  +  b)  *  c is  a b / c , .   D 

It  is  not  at  all  obvious  that  a  prefix  or  postfix  expression  can  be  uniquely 
returned  to  an  infix  expression.  The  observations  leading  to  a  proof  of  this 

fact  are  found  in  Exercises  3.1.16  and  3.1.17. 

We  can  use  trees  to  conveniently  represent  arithmetic  expressions.  For 
example,  (a  /  b ) ,   c  has  the  tree  representation  shown  in  Fig.  3.1.  In  the 

+ 

Fig.  3.1  Tree  representation  for (a + b)*c. 

tree  representation  each  interior  node  is  labeled  by  an  operator  from  O  and 
each  leaf  by  an  operand  from  ~.  The  prefix  Polish  representation  is  merely 

the  left-bracketed  representation  of  the  tree  with  all  parentheses  deleted. 

Similarly,  the  postfix  Polish  representation  is  the  right-bracketed  represen- 
tation  of the  tree,  with  parentheses  again  deleted. 

Two  important  examples  of  translations  are  the  sets  of pairs 

((x, y)lx  is  an  infix  expression  and  y  is  the  prefix  (or, 

alternatively,  postfix)  Polish  representation  of x]. 

These translations  cannot  be specified  by a homomorphism.  We need transla- 

tion  specifiers  with  more  power  and  shall  now  turn  our  attention  to formal- 
isms  which  allow  these  and  other  translations  to  be  conveniently  specified. 

3.1.2.  Syntax-Directed  Translation  Schemata 

The  problem  of  finitely  specifying  an  infinite  translation  is  similar  to 
the  problem  of  specifying  an  infinite  language.  There  are  several  possible 

approaches  toward  the  specification  of translations.  Analogous  to  a language 

generator,  such as a grammar,  we can have a  system which generates  the pairs 
in  the  translation.  We  can  also  use  a  recognizer  with  two  tapes  to  recognize 

216  THEORY  OF  TRANSLATION  CHAP.  3 
those  pairs  in  the  translation.  Or  we  could  define  an  automaton  which  takes 
a  string  x  as  input  and  emits  (nondeterministically  if  necessary)  all  y  such 
that  y  is  a  translation  of x.  While  this  list  does  not  exhaust  all  possibilities, 
it  does  cover  the  models  in  common  use. 

Let  us  call  a  device  which  given  an  input  string  x,  calculates  an  output 
string y  such  that  (x, y)  is  in  a  given  translation  T,  a  translator  for  T.  There 
are  several  features  which  are  desirable  in  the  definition  of  a  translation. 
Two  of these  features  are 

(1)  The  definition  of  the  translation  should  be  readable.  That  is  to  say, 
it  should  be  easy  to  determine  what  pairs  are  in  the  translation. 

(2)  It  should  be possible  to  mechanically  construct  an  efficient  translator 
for  that  translation  directly  from  the  definition. 

Features  which  are  desirable  in  translators  are 

(1)  Efficient  operation.  For  an  input  string  w of length  n,  the  amount  of 
time  required  to  process  w  should  be  linearly  proportional  to  n. 

(2)  Small  size. 

(3)  Correctness.  It would  be  desirable  to  have  a  small  finite test  such that 
if the  translator  passed  this  test,  this  would  be  a  guarantee  that  the translator 
works  correctly  on  all  inputs. 

One  formalism  for  defining  translations  is  the  syntax-directed  translation 
schema.  Intuitively, a  syntax-directed translation  schema is simply a grammar 
in  which  translation  elements  are  attached  to  each  production.  Whenever 

a  production  is  used  in  the  derivation  of  an  input  sentence,  the  translation 
element  is  used  to  help  compute  a  portion  of the  output  sentence  associated 
with  the  portion  of the input  sentence  generated  by that  production. 

Example 3.3 

Consider  the  following  translation  schema  which  defines  the  translation 
{(x, XR)[X  ~  {0,  1}*}. That  is,  for  each  input  x,  the  output  is  x  reversed.  The 

rules  defining  this  translation  are 

Production  Translation  Element 
(1)  S---*  OS  S =  SO (2)  S ~ I S   S = S 1  

(3)  S---.  e  S - -   e 

An  input-output  pair  in  the  translation  defined  by  this  schema  can  be 
obtained  by generating  a  sequence  of pairs  of strings  (0c, fl)  called  translation 
forms,  where  ~  is  an  input  sentential  form  and  fl  an  output  sentential  form. 

We  begin  with  the  translation  form  (S,  S).  We  can  then  apply  the  first  rule 

sac.  3.1  FORMALISMS FOR TRANSLATIONS  217 
to  this  form.  To  do  so,  we  expand  the  first  S  using  the  production  S  ~  0S. 
Then  we  replace  the  output  sentential  form  S  by  SO in  accordance  with  the 
translation  element  S  ---- SO.  For  the  time  being,  we  can  think  of the  trans- 
lation  element  simply  as  a  production  S  --~  SO.  Thus  we  obtain  the  transla- 
tion  form  (0S,  SO).  We  can  expand  each  S  in  this  new  translation  form  by 
using  rule  (1)  again  to  obtain  (00S,  SO0).  If we then  apply rule  (2),  we  obtain 
(001S,  S100).  If we then  apply rule  (3), we  obtain  (001,100).  No  further  rules 
can  be  applied  to  this  translation  form  and  thus  (001,100)  is  in  the  transla- tion  defined  by  this  translation  schema.  V-] 

A  translation  schema  T  defines  some  translation  z(T).  We  can  build 
a  translator  for  T(T)  from  the  translation  schema  that  works  as  follows. 

Given  an  input  string  x,  the  translator  finds  (if possible)  some  derivation  of 
x  from  S  using  the  productions  in  the  translation  schema.  Suppose  that 
S  =  ~0 =*  ~  =*  ~z =~  "'"  =*  ~,  =  x  is  such  a  derivation.  Then  the  trans- lator  creates  a  derivation  of translation  forms 

(~0,/~0) --~-  ( ~ ,   P,)  . . . . .   > (~.,  p.) 
such  that  (~0,  fl0)  =  (S,  S),  (~,,  ft.)  =  (x, y),  and  each  fl,  is  obtained  by 
applying  to  fl~-i  the  translation  element  corresponding  to  the  production 
used  in  going  from  t~_ 1 to  ~t  at  the  "corresponding"  place.  The  string  y  is an  output  for  x. 

Often  the  output  sentential  forms  can  be  created  at  the  time  the  input  is being  parsed. 

Example 3.4 

Consider  the following  translation  scheme  which  maps  arithmetic  expres- sions  of 

L(Go)  to  postfix  Polish" 

Production  Translation Element 
E----~ E  +  T  E =  ET-+- 
E- - ~ T   E - 'T  

T --*  T *   F  T =   TF* 
T---~F  T = F 
F  ~  (E)  F  =  E 
F---~a  F =a 

The  production  E  ~  E  q-  T  is  associated  with  the  translation  element 
E  =  E T  +.   This translation  element  says that  the translation  associated  with 
E  on  the  left  of  the  production  is  the  translation  associated  with  E  on  the 
right  of  the  production  followed  by  the  translation  of  T  followed  by  + .  

21 8  THEORY OF TRANSLATION  CHAP. 3 

Let us  determine  the  output  for the input  a  +  a  *  a.  To  do  so,  let  us first 
find  a  leftmost  derivation  of a  -+  a  *  a  from  E  using  the  productions  of the 
translation  scheme.  Then  we  compute  the  corresponding  sequence  of trans- 
lation  forms  as  shown" 

(E,  E)  ~  (E  -q- T,  E T  +) 

-- -->(T+  T,  T T + )  

------~ (F q-  T,  FT  + ) 

~- (a  q-  T,  aT q-) 
---->  (a  -q- T ,   F,  aTF ,  +) 

u ( a  -q- F ,   F,  aFF ,  +) 

> (a  +  a  ,  F,  aaF ,  + ) 
~  > ( a - + a , a ,   a a a , + )  

Each  output  sentential  form  is  computed  by  replacing  the  appropriate  non- 

terminal  in  the  previous  output  sentential  form  by  the  right  side  of transla- 
tion  rule  associated  with  the  production  used  in  deriving  the  corresponding 

input  sentential  form. 

The  translation  schemata  in  Examples  3.3  and  3.4  are  special  cases  of 
an  important  class  of translation  schemata  called  syntax-directed translation 

schemata. 

DEFINITION 

A  syntax-directed  translation  schema  (SDTS  for  short)  is  a  5-tuple 
T - -   (N,  Z, A,  R,  S),  where 

(1)  N  is  a  finite  set  of nonterminal  symbols. 
(2)  Z  is  a  finite  input  alphabet. 

(3)  A  is  a  finite  output alphabet. 
(4)  R  is  a  finite  set  of rules  of the  form  A  ----~ ~,  fl,  where  ~  ~  (N  U  Z)*, 
fl  ~  (N  u  A)*,  and  the  nonterminals  in  ,6'  are  a  permutation  of  the  non- 
terminals  in  ~. 

(5)  S  is  a  distinguished  nonterminal  in  N,  the  start  symbol. 

Let  A  ~  ~, fl  be  a  rule.  To  each  nonterminal  of  ~  there  is  associated 
an  identical  nonterminal  of  ft.  If  a  nonterminal  B  appears  only  once  in 
and  ,8,  then  the  association  is  obvious.  If B  appears  more  than  once,  we  use 
integer superscripts to indicate the association.  This association is an intimate 
part  of the  rule.  For  example,  in the  rule  A  ~  B(I~CB .~2~, B(2~B(I~C, the  three 

positions  in  BcI~CB Cz~ are  associated  with  positions  2,  3,  and  1,  respectively, 
in  B~2~B~I~C. 

We  define  a  translation form  of T  as  follows: 

SEC. 3.1  FORMALISMS FOR  TRANSLATIONS  219 

(1)  (S,  S)  is  a  translation  form,  and  the  two  S's  are  said  to  be associated. 
(2)  If  (eArl,  e'Afl')  is  a  translation  form,  in  which  the  two  explicit 
instances  of A are associated,  and  if A  ---~ 7, ~" is a rule in R,  then  (e?fl,  e'?'fl') 
is a  translation  form.  The  nonterminals  of ? and  ~,' are  associated  in the trans- 
lation  form  exactly  as  they  are  associated  in  the  rule.  The  nonterminals  of 

*  and  fl  are  associated  with  those  of  e'  and  fl'  in  the  new  translation  form 

exactly  as  in  the  old.  The  association  will  again  be  indicated  by  superscripts, 
when  needed,  and  this  association  is  an  essential  feature  of  the  form. 

If  the  forms  (eArl,  e'Afl')  and  (eI, fl,  e'~,'fl'),  together  with  their  associa- 
tions,  are  related  as  above,  then  we  write  (eArl,  e'Afl')  =-~ (e?fl,  e'?'fl'). T 

+  *  k We  use  =~,  ==~, and  ==~ to  stand  for  the  transitive  closure,  reflexive-transitive 

T  T  T closure,  and  k-fold  product  of  ==~. As  is  customary,  we  shall  drop  the  sub- 

T script  T  whenever  possible. 

The  translation  defined by  T,  denoted  z(T),  is the  set  of pairs 

{(x, y) l (S, S)  ~  (x, y),  x  ~  12"  and  y  ~  A*}. 
Example 3.5 

Consider  the  SDTS  T  =  (IS}, {a,  -q-}, {a,  +  }, R,  S),  where  R  has  the  rules 

S  > -k  Sc~)S ~2~,  S ~  +  S <z) 
S  > a,  a 

Consider  the  following  derivation  in  T: 

(S,  S)  ~  (+  S ~ ) S ~2~, S C~' +  S TM) 

(-q-  -~-  S(3)S(4)S(2)  '  S(3)  .q_  S(4)  .~-  S  TM) 

(+  +  aS~4~S ~2~, a  +  S")   +  S ~2~) 
> (-q- +  aaS,  a  -q- a  +  S) 
- - - ~ (  +  +  aaa,  a  q-  a  +  a) 

z(T)  =  ((x,  a(q-a)~) [ i ~  0  and  x  is  a  prefix  polish  representation  of  a(-t-a) ~ 

with  some  order  of  association  of the  -q-'s}.  D 

DEFINITION 

If  T  =  (N,  E,  A,  R,  S)  is  an  SDTS,  then  z(T)  is  called  a  syntax-directed 
translation  (SDT).  The  grammar  G~ =  (N,  E, P,  S),  where 

P  =  [A  ~  e l A   ---~ e,  fl  is  in  R}, 

220  THEORY  OF  TRANSLATION  CHAP.  3 
is  called  the  underlying  (or  input)grammar  of  the  SDTS  T.  The  grammar 

Go =  (N,  A, P',  S),  where  P ' =   {A----~ ill A  ---~ t~,  fl  is  in  R}  is  called  the output  grammar 

of  T. 

We  can  alternatively  view  a  syntax-directed  translation  as  a  method  of 
transforming  derivation  trees  in  the  input  grammar  G~ into  derivation  trees 

in  the  output  grammar  Go.  Given  an  input  sentence  x,  a  translation  for  x 
can  be  obtained  by  constructing  a  derivation  tree  for  x,  then  transforming 
the  derivation  tree  into  a  tree  in  the  output  grammar,  and  then  taking  the 

frontier  of the  output  tree  as  a  translation  for  x. 

ALGORITHM 3.1 

Tree  transformation  via  an  SDTS. 
lnput.  An  SDTS  T =   (N,  # , A ,   R,  S),  with  input  grammar  Gt  = 
(N,  #,  P~, S),  output  grammar  Go =  (N,  A,  Po, S),  and  a  derivation  tree 

D  in  G i,  with  frontier  in  X*. 

Output.  Some  derivation  tree  D'  in  G o such  that  if x  and  y  are  the  fron- 
tiers  of  D  and  D',  respectively,  then  (x, y)  ~  ~:(T). 

Method. 

(1)  Apt~ly  step  (2),  recursively,  starting  with  the  root  of  D. 
(2)  Let this  step  be  applied  to  node  n.  It  will be  the  case that  n  is  an  inte- 
rior  node  of  D.  Let  n  have  direct  descendants  n x , . . . ,   nk. 

(a)  Delete  those  of  n l , . . . ,   nk  which  are  leaves  (i.e.,  have  terminal 

or  e-labels). 
(b)  Let  the  production  of  G~ represented  by  n  and  its  direct  descen- 

dants  be  A  ~  ~.  That  is,  A  is  the  label  of  n  and  t~ is  formed  by 
concatenating  the  labels  of  n 1,  . . . ,   nk.  Choose  some  rule  of  the 

form  A  ~  ~,  fl  in  R.t  Permute  the  remaining  direct  descendants 
of n,  if any,  in  accordance  with  the  association  between  the  non- 
terminals  of  ~  and  ft.  (The  subtrees  dominated  by  these  nodes 
remain  in  fixed  relationship  to  the  direct  descendants  of n.) 
(c)  Insert  direct  descendant  leaves  of n  so  that  the  labels  of its  direct 

descendants  form  ft. 
(d)  Apply  step  (2)  to  the  direct  descendants  of n which  are not  leaves, 

in  order  from  the  left. 
(3)  The  resulting  tree  is  D'.  E] 

Example  3.6 

Let  us  consider  the 
consists  of 

SDTS  T  =  ({S,  A}, {0,  1}, {a, b}, R,  S),  where  R 

"['Note that fl may not  be uniquely  determined  from A  and  ~.  If  more  than  one rule is applicable,  the choice can be arbitrary. 

SEC. 3.1  FORMALISMS FOR TRANSLATIONS  221 

S  > OAS,  SAa 
A  > OSA,  ASa 

S  > 1, b 
A  > l , b  

A  derivation  tree in the input  grammar  is shown  in Fig. 3.2(a).  If we apply 
step  (2) of Algorithm  3.1  to  the  root  of Fig.  3.2(a),  we delete  the  leftmost  leaf 
labeled  0.  Then,  since  S  ---~ OAS  was  the  production  used  at  the  root  and 
the  only  translation  element  for  that  production  is  SAa,  we  must  reverse  the 

order  of  the  remaining  direct  descendants  of  the  root.  Then  we  add  a  third 
direct  descendant,  labeled  a,  at  the  rightmost  position.  The  resulting  tree  is 

shown  in  Fig.  3.2(b). 

s / 1 \  
0  A  S / 1 \  
o  s  A I  I 

1  1 

\ 

1 

s  s / 1 \   / 1 \  
S  A  a  S  A  a / / 1 \   / / 1 \  
1  0  S  A  b  A  S  a I  I  I  I 

1  1  b  b 
(a)  (b)  (c) 

Fig. 3.2  Application  of Algorithm 3.1. 

We  next  apply  step  (2)  to  the  first  two  direct  descendants  of  the  root. 
Application  of step  (2) to  the  second  of these  descendants  results  in two  more 
calls  of  step  (2).  The  resulting  tree  is  shown  in  Fig.  3.2(c).  Notice  that 

(00111,  bbbaa)  is  in  z(T). 

To  show  the  relation  between  the  translation  process  of  Algorithm  3.1 
and  the  SDTS  which  is  input  to  that  algorithm,  we  prove  the  following 
theorem. 

THEOREM 3.1 

(1)  If  x  and  y  are  the  frontiers  of  D  and  D',  respectively,  in  Algorithm 
3.1,  then  (x, y)  is  in  z(T). 

(2)  If (x, y)  is  in  I:(T),  then  there  exists  a  derivation  tree  D  with  frontier 
x  and  a  sequence  of  choices  for  each  execution  of  step  (2b)  such  that  the 
frontier  of the  resulting  tree  D'  is y. 

Proof. 

(1)  We  show  the  following  by induction  on  the  number  of interior  nodes 
of a  tree  E: 

222  THEORY OF TRANSLATION  CHAP. 3 

(3.1.1)  Let E  be  a  derivation  tree in  G~ with frontier  x  and  root  labeled  A, 

and  suppose  that  step  (2) applied  to E  yields a  tree  E'  with frontier 

y.  Then  (A,  A)  ~  (x, y) 

The  basis,  one  interior  node,  is  trivial.  All  direct  descendants  are  leaves, 
and  there  must  be  a  rule  A  --,  x, y  in  R. 

For  the  inductive  step,  assume  that  statement  (3.1.1)  holds  for  smaller 
trees,  and  let  the  root  of  E  have  direct  descendants  with  labels  X 1 , . . . ,   Xk. 

Then  x  =  xl  .."  x~,  where  Xj =-> xj,  1 _<j _< k.  Let  the  direct  descendants  of Gt 

the  root  of  E'  have  labels  Y~  * * *  Y~.  Then  y  =  y~  . * * Yl,  where  Yj ~  yi, Go 

1 ~ j   ~  1.  Also,  there  is  a  rule  A  ----~ X1  . . .   Xk,  Y~  .."  Yi in  R. 

If  Xj  is  a  nonterminal,  then  it  is  associated  with  some  Y~,  where 

X~  =  Yp,.  By  the  inductive  hypothesis  (3.1.1),  (Xj,  X:) =~  (xj,  yp).  Because 

of  the  permutation  of  nodes  in  step  (2b),  we  know  that 

(A,  A)  ~  (X,  . . .   X,,  Y,  . . .   Y,) 

(x~X~  .. .  x~,  < ' ~ . . .   ~'~) 

(X 1  "'"  Xk,  I~i k)''"  t~}k))~, 
where  a) m)  is 

(a)  y j  if  Yj  is  in  N  and  is  associated  with  one  of  X1,  * . . ,   Xm,  and 
(b)  Yj  otherwise. 
Thus  Eq.  (3.1.1)  follows. 

Part  (2)  of the  theorem  is  a  special  case  of the  following  statement" 

(3.1.2) 

i If  (A,  A) ~  (x, y),  then  there  is  a  derivation  tree  D  in  Gi,  with 

root  labeled  A,  frontier  x,  and  a  sequence  of  choices  in  step  (2b) 

so  that  the  application  of step  (2)  to  D  gives  a  tree  with frontier  y. 

A  proof  of  (3.1.2)  by  induction  on  i  is  left  for  the  Exercises. 

We  comment  that  the  order  in  which  step  (2)  of Algorithm  3.1  is  applied 
to  nodes  is  unimportant.  We  could  choose  any  order  that  considered  each 
interior  node  exactly  once.  This  statement  is  also  left  for  the  Exercises. 

DEFINITION 

An  SDTS  T  =  (N,  E,  A,  R,  S)  such  that  in  each  rule  A  --~ 0c, ,8  in  R, 
associated  nonterminals  occur  in the  same  order  in  0c and  fl is called  a  simple 

SEC. 3.1  FORMALISMS  FOR TRANSLATIONS  223 

SDTS.  The  translation  defined  by  a  simple  SDTS  is  called  a  simple  syntax- 
directed  translation  (simple  SDT). 

The  syntax-directed  translation  schemata  of  Examples  3.3-3.5  are  all 
simple.  That  of Example  3.6  is  not. 

The  association  of nonterminals  in  a  form  of a  simple  SDTS  is  straight- 
forward.  They must  be  associated  in the  order  in which  they  appear. 

The  simple  syntax-directed  translations  are  important  because  for  each 
simple  SDT  we  can  easily  construct  a  translator  consisting  of  a  pushdown 
transducer.  This  construction  will  be  given  in  Section  3.1.4.  Many,  but  not 

all,  useful  translations  can  be  described  as  simple  SDT's.  In  Chapter  9  we 
shall  present  several  generalizations  of  syntax-directed  translation  schemata 
which  can  be  used  to  define  larger  classes  of  translations  on  context-free 

languages.  We  close  this  section  with  another  example  of  a  simple  SDT. 

Example  3.7 

The  following  simple  SDTS  maps  the  arithmetic  expressions  in  L(Go) 
to  arithmetic  expressions  with  no  redundant  parentheses: 

(I)  E  > (E),  E 
(2)  E  > E-+-  E,  E-+-  E 

(3)  E ~  T,  T 
(4)  T  > (T),  T 
(5)  T  > A , A ,   A * A  

(6)  T  > a,  a 
(7)  A  > (E  -k  E),  (E  q-  E) 

(8)"  A  > T,  T 

For  example,  the  translation  of  ((a  q - ( a ,   a ) ) ,   a)  according  to  this 
SDTS  is  (a  +  a  ,  a)  ,  a.t  Q 

3.1.3.  Finite  Transducers 

We  shall  now  introduce  our  simplest  translator,  the  finite  transducer. 
A  transducer  is  simply  a  recognizer  which  emits  an  output  string  during 

each  move  made.  (The  output  may  be  e,  however.)  The  finite  transducer  is 

obtained  by  taking  a  finite  automaton  and  permitting  the  machine  to  emit 
a  string  of  output  symbols  on  each  move  (Fig.  3.3).  In  Section  3.3  we  shall 

use  a  finite  transducer  as  a  model  for  a  lexical  analyzer. 

tNote that the underlying grammar is ambiguous, but that each input word has exactly one output. 

224  THEORY OF TRANSLATION  CHAP.  3 

I '  al  a2 1 

Read only an  input tape 

Finite control 

Write only output  tape 
Fig. 3.3  Finite transducer. 
For  generality  we  shall  consider  a  nondeterministic  finite  automaton 
which  is  capable  of making  e-moves,  as  the  basis  of a  finite  transducer. 

DEFINITION 

A finite  transducer  M  is  a  6-tuple  (Q,  E,  A,  ~,  q0,  F),  where 

(1)  Q  is  a  finite  set  of states. 
(2)  E  is  a  finite  input alphabet. 
(3)  A  is  a  finite  output  alphabet. 

(4)  ~ is a  mapping  from  Q  x  (E  u  [e}) to  finite  subsets  of  Q  x  A*. 
(5)  q0  ~  Q  is the  initial state. 

(6)  F  ~  Q  is  the  set  of final states. 

We  define  a  configuration  of M  as  a  triple  (q,  x, y),  where 

(1)  q  ~  Q  is  the  current  state  of the  finite  control. 
(2)  x  ~  E*  is  the  input  string  remaining  on  the  input  tape,  with  the  left- 
most  symbol  of x  under  the  input  head. 

(3)  y  E  A*  is  the  output  string  emitted  up  to  this  point. 

We define ~  (or  ~ ,   when  M  is clear),  a  binary relation  on configurations, 
to  reflect  a  move  by  M.  Specifically,  for  all  q  ~  Q,  a  ~  ~  u  {e},  x  ~  E*, 
and  y  ~  A*  such  that  d~(q, a)  contains  (r,  z),  we  write 

(q, ax, y)  ~-  (r, x, yz) 
We  can  then  define  [A_, [____, and  [--  in  the  usual  fashion. 

We  say  that  y  is  an  output  for  x  if  (q0, x,  e)[--  (q,  e, y)  for  some  q  in  F. 
The  translation  defined  by  M,  denoted  z ( g ) ,   is  {(x, Y) i (qo, x,  e) ~-- (q, e, y) 
for  some  q  in  F}.  A  translation  defined  by  a  finite  transducer  will  be  called a  regular  translation 

or finite  transducer mapping. 

Notice  that  before  an  output  string  y  can  be  considered  a  translation  of 

SEC. 3.1  FORMALISMS FOR TRANSLATIONS  225 
an  input  x,  the  input  string  x  must  take  M  from  an  initial  state  to  a  final 

state. 

Example 3.8 

Let  us  design  a  finite  transducer  which  recognizes  arithmetic  expressions 
generated  by  the  productions 

S  ~ a + S I a - - S I + S I - - S I a  
and  removes  redundant  unary  operators  from  these  expressions.  For  exam- 
ple,  we  would  translate  - - a - - 4 - -   a  ~  a  into  - - a -   a-q-a.  In  this 

language,  a  represents  an  identifier,  and  an  arbitrary  sequence  of unary  -q-'s 

and  --'s  is permitted  in  front  of an  identifier.  Notice  that  the  input  language 

is  a  regular  set.  Let  M  =  (Q,  ~,  A,  d~, q0, F),  where 

(1)  a  =  {q0, ql,  q2,  q3,  q,}. (2)  E  =  [a,  + , - - ] .  

(3)  A  =  E. (4)  6 is defined  by the  transition  graph  of Fig.  3.4.  A  label x/y on an  edge 
directed from  the  node  labeled  qt to  the  node  labeled  qj indicates  that 6(q~, x) 
contains  (qj, y). 

(5)  F =   {ql]. 

M  starts  in  state  q0  and  determines  whether  there  are  an  odd  or  even 

Start 

+/e 

--/e 

Fig. 3.4  Transition graph. 

226  THEORY  OF  TRANSLATION  CHAP.  3 
n um b e r   of minus  signs  preceding  the  first  a  by  alternating  between  q0  and  q4 

on  input  --.  W he n  an  a  appears,  M  goes  to  state  q~,  to  accept  the  input,  and 
emits  either  a  or  --  a,  depending  on  whether  an  even  or  odd  n um be r   o f - - ' s  
have  appeared.  F or   subsequent  a's,  M  counts  whether  the  nu m b e r   o f - - ' s  
is  even  or  odd  using  states  q2  and  q3.  The  only  difference  between  the 
q2  --  q3  pair  and  q0  --  q4  is  that  the  former  emits  +  a,  rather  than  a  alone, 
if an  even  num be r   of  --  signs  precede  a. 

With  input  --  a  4-  --  a  --  -b  --  a,  M  would  m ake  the  following  sequence 
of  moves: 

(q0,  --  a  4-  --  a  --  -1-  --  a,  e)  [--  (qa,  a  -q-  --  a  --  -b  --  a,  e) 

(q l,  4-  --  a -   -b  --  a , -   a) 
( q z , -   a -   -t-  --  a,  --a) 

(q3,  a - -   + - - a , - a )  
~ - ( q l , - - + - - a , - - a - - a )  ~(q3,  q-  - - a , - - a - - a )  

(q3,--a,  - -a -   a) 
(q2,  a , -   a -   a) 
1-  (ql,  e,  - - a   --  a  +  a) 

Thus,  M  maps  --  a  4-  --  a  [  a  into  --  a  --  a  +  a,  since  q l  is  a  final 

state.  D 

We  say  that  a  finite  transducer  M  is  deterministic  if the  following  condi- 
tion  holds  for  all  q  ~  Q" 

(1)  Either  ~(q,  a)  contains  at  most  one  element  for  each  a  ~  E,  and 
~(q,  e)  is  empty,  or 

(2)  ~(q,  e)  contains  one  element,  and  for  all  a  ~  E,  ~(q,  a)  is  empty. 

The  finite  transducer  in  Example  3.8  is  deterministic. 
Note   that  a  deterministic  finite  transducer  can  define  several  translations 
for  a  single  input. 

Example  3.9 

Let  M =   ({q0,  q~}, {a},  {b}, ~,  qo,  {q~})  and  let  J(q0,  a)  =  {(ql,  b)}  and 
d~(ql,  e)  =  {(q~,  b)}.  Then 

(q0,  a,  e ) 1 ~   (q~,  e,  b) 

~-~-(ql,  e, b '+1) 

is  a  valid  sequence  of moves  for  all  i  ~  0.  Thus,  'r(M)  =  {(a,  bt)[i ~  1}.  [Z] 

sac.  3.1  FORMALISMS FOR  TRANSLATIONS  227 

There  are  several  simple  modifications  of  the  definition  of  determinism 
for  finite  transducers  that  will  ensure  the  uniqueness  of  output.  The  one  we 

suggest  is  to  require  that  no  e-moves  be  made  in  a  final  state. 

A  number  of closure  properties  for  classes  of languages  can  be  obtained 
by  using  transducers  as  operators  on  languages.  For  example,  if M  is  a  finite 
transducer  and  L  is  included  in  the  domain  of  z(M),  then  we  can  define 

M(L)  :  {Y lx  ~  L  and  (x, y)  ~  z(M)}. We  can  also  define  an  inverse finite  transducer  mapping  as  follows.  Let  M 

be  a  finite  transducer.  Then  M-I(L)  =  {xly ~  L  and  (x, y)  ~  z(M)}. 

It  is  not  difficult  to  show  that  finite  transducer  mappings  and  inverse 
finite  transducer  mappings  preserve  both  the  regular  sets  and  the  context- 

free languages.  That is, if L  is a  regular  set (C FL )and  M  is a finite transducer, 

then  M(L)  and  M-I(L)  are  both  regular  sets  (CFL's).  Proofs  are  left  for  the 

Exercises.  We  can  use  these  observations  to  show  that  certain  languages  are 
not  regular  or  not  context-free. 

Example 3,10 

The  language  generated  by the  following  grammar  G is not  regular" 

Let 

Li 

S  ---+  if S  then  S[a 
=  L(G)  ~  (if)*  a  (then  a)* 

=  {(if)"  a  (then  a)"ln  ~  0} 

Consider  the  finite  transducer  M  --  (Q, Z,  A,  J,  q0,  F),  where 

(1)  Q  =  {q,10  ~  i ~   6}. 
(2)  Z  =  {a,  i, fi  t,  h,  'e',  n}. 
(3)  A  =  {0,  1}. 
(4)  ~  is  defined  by  the  transition  graph  of  Fig.  3.5. 
(5)  F  =  {qz]. 

Here  'e'  denotes  the  letter  e,  as  distinguished  from  the  empty  string. 
Thus  M(L~)  =  {0kl k [k  ~  0},  which  we  know  is  not  a  regular  set.  Since the 
regular  sets  are  closed  under  intersection  and  finite  transducer  mappings, 
we  must  conclude  that  L(G)  is  not  a  regular  set.  D 

3.1.4.  Pushdown  Transducers 

We  shall  now  introduce  another  important  class  of  translators  called 
pushdown  transducers.  A  pushdown  transducer  is  obtained  by  providing 

a  pushdown  automaton  with  an  output.  On  each  move  the  automaton  is 
permitted  to  emit  a  finite-length  output  string. 

228  THEORY OF TRANSLATION  CHAP. 3 

a/e Start 

i/e  '  ' e 

Fig. 3.5  Transition graph of M. 
DEFINITION 

A  pushdown  transducer  (PDT)  P  is  an  8-tuple  (Q,  #,  F,  A, ~,  qo,  Z0, F), 
where  all  symbols  have  the  same  meaning  as  for  a  PDA  except  that  A  is 

a  finite  output  alphabet  and  ~  is  now  a  mapping  from  Q  x  (#  u  {e}) x  r 
to  finite  subsets  of  Q  x  IF*  x  A*. 

We  define  a  configuration  of P  as  a  4-tuple  (q,  x,  ~, y),  where  q,  x,  and 
are  the  same  as  for  a  PDA  and  y  is  the  output  string  emitted  to  this  point. 
If  ~(q,  a,  Z)  contains  (r,  ~, z),  then  we  write  (q, ax,  ZT, y)  k-  (r, x,  ~?, yz) 
for  all  x  ~  #*,  ?  ~  r'*,  and  y  ~  A*. 

We  say  that  y  is  an  output  for  x  if  (qo, x,  Z0,  e)[---  (q, e,  ~, y)  for  some 
q  ~  F  and  ~  ~  F*.  The  translation  defined by P,  denoted  z(P),  is 

[(x, Y) [ (qo, x,  Z o, e) [-----. (q,  e,  ~, y)  for  some  q  ~  F  and  ~  ~  I'*}. 
As  with  PDA's,  we  can  say  that  y  is  an  output  for  x  by  empty pushdown 
list  if  (q0, x,  Z0,  e)t---- (q,  e,  e, y)  for  some  q  in  Q.  The  translation  defined by P  by  empty pushdown  list, 

denoted  z,(P),  is 

{(x, Y) [ (qo, x,  Zo,  e) ~  (q,  e,  e, y)  for  some  q  ~  Q}. 
We  can  also  define  extended  PDT's  with  their  pushdown  list  top  on  the 
right  in  a  way  analogous  to  extended  PDA's. 

Example 3.11 

Let  P  be  the  pushdown  transducer 

([q},  {a,  + ,   ,},  {-k,  *, E},  {a,  -q-, ,}, ~, q, E,  {q}), 
where  6  is  defined  as  follows: 

sec.  3.1  FORMALISMS FOR TRANSLATIONS  229 

6(q,  a,  E)  =  {(q,  e,  a)} 
6(q,  + ,   E)  =  ((q,  EE  +,  e)] 

6( q ,.,   E)  =  [(q,  E E . ,   e)} 
6(q,  e,  + )   =  {(q,  e,  +)} 

6(q,  e,  . ) =   [(q,  e,  .)] 

With  input  +  *  aaa,  P  makes  the  following  sequence  of moves" 

(q,  +  *  aaa,  E,  e)  t-  (q,  * aaa,  EE  +,  e) 

~-  (q,  aaa,  EE   *  E  +,  e) 

F-  (q,  aa,  E  *  E  +,  a) 

(q,  a,  *  E  +,  aa) 

F-  ( q , a , E + , a a . )  

t-  (q,  e,  +,  aa  *  a) 
~ - ( q , e , e ,   a a . a + )  

Thus  a  translation  by  empty pushdown  list  of  +  *  aaa  is aa  *  a  +.  It  can  be 
verified  that  z,(P)  is  the  set 

{(x, y) lx  is  a  prefix  Polish  arithmetic  expression  over  [ + , . ,   a} 

and  y  is  the  corresponding  postfix  Polish  expression}.  [1 

DEFINITION 

If P  =  (Q,  E,  F,  A, c5, qo,  Zo, F)  is a  pushdown  transducer,  then  the  push- 
down  automaton  (Q,  E,  F,  ~',  q0,  Z0,  F),  where  ~'(q,  a,  Z)  contains  (r, ~,) if 
and  only if  ~(q,  a,  Z)  contains  (r, 7, y)  for  some  y,  is called  the  PDA  under- 

lying P. We  say  that  the  PDT  P  =  (Q,  E,  F,  A,  6,  q0,  Z0,  F)  is  determip,stie 

(a  DPDT)  when 

(1)  For  all  q  e  Q,  a  ~  E  w  {e}, and  Z  e  F,  6(q,  a, Z)  contains  at  most 
one  element,  and 

(2)  If 0(q,  e, Z)  ~  ~ ,   then  6(q,  a, Z)  =  ~  for  all  a  E  ~:.t 

Clearly,  if  L  is  the  domain  of  z(P)  for  some  pushdown  transducer  P,  then 
L  =  L(P'),  where  P '   is  the  pushdown  automaton  underlying P. 

?Note that this  definition is slightly stronger than  saying that  the underlying PDA is deterministic.  The latter  could  be  deterministic,  but  (1) may not  hold  because the PDT 
can give two different outputs on two moves which are otherwise identical. Also note that condition (2) implies that if 6(q, a, Z)  ~  ~  for some a e  ~,  then 6(q, e, Z)  =  ~. 

230  THEORY  OF TRANSLATION  CHAP.  3 

Many  of the  results  proved  in  Section  2.5  for  pushdown  automata  carry 
over  naturally  to  pushdown  transducers.  In  particular,  the  following lemma 
can  be  shown in  a  way analogous  to  Lemmas  2.22 and  2.23. 

LEMMA 3.1 

A  translation  T  is  z(P1)  for  a  pushdown  transducer  P1  if  and  only  if  T 
is  ze(P2)  for  a  pushdown  transducer  P2. 

Proof  Exercise. 

A  pushdown  transducer,  particularly  a  deterministic  pushdown  trans- 
ducer,  is a  useful  model  of the  syntactic  analysis phase  of compiling.  In  Sec- 
tion  3.4  we  shall  use  the  pushdown  transducer  in  this  phase  of  compiling. 

Now  we  shall  prove  that  a  translation  is  a  simple  SDT  if and  only  if it 
can  be  defined  by  a  pushdown  transducer.  Thus  the  pushdown  transducers 
characterize  the  class  of  simple  SDT's  in  the  same  manner  that  pushdown 
automata  characterize  the  context-free  languages. 

LEMMA 3.2 

Let  T  =  (N,  Z,  A, R,  S)  be  a  simple  SDTS.  Then  there  is  a  pushdown 
transducer  P  such  that  z,(P)  =  z(T). 

Proof.  Let  G~ be  the  input  grammar  of  T.  We  construct  P  to  recognize 
L(G~)  top-down  as  in  Lemma  2.24. 

To  simulate a rule A  ---~ ~, fl of T, P  will replace ,4 on top  of its pushdown 
list by 0~ with output symbols offl intermeshed.  That is, if ~ =  Xo A ~xi  * * * A, x ,  
and  fl---yoA~y~...  A,y,,  then  P  will  place  XoYoA~x~y~...  A,x,,y,  on  its 
pushdown  list.  We  need,  however,  to  distinguish  between  the  symbols  of Z 

and  those  of  A,  so  that  the  word  x~y~ can  be  broken  up  correctly.  If Z  and 
A  are  disjoint,  there  is  no  problem,  but  to  take  care  of  the  general  case,  we 
define  a  new  alphabet  A'  corresponding  to  A  but  known  to  be  disjoint 
from  Z.  That  is,  let  A'  consist  of  new  symbols  a'  for  each  a  ~  A.  Then 

Z  ~  A ' =   ~ .   Let  h  be  the  homomorphism  defined  by  h( a ) =   a'  for  each 
a i n A .  

Let  P  =  ({q}, Z,  N  U  Z  U  A',  A, 5, q,  S,  ~),  where  5  is  defined  as  fol- lows- 

(1)  If  A  ~  XoBlX~...  BkXk,  YoB1Yl  " "   BkYk  is  a  rule  in  R  with  k  _~ 0, 
then  5(q,  e,  A)  contains  (q,  Xoy~B~x~y'~  . . .   BkxkY~,,  e),  where  y~ =  h(yt), 
O ~ l i ~ k .  

(2)  6(q,  a,  a)  =  {(q,  e,  e)} for  all  a  in  Z. 
(3)  O(q,  e,  a')  =  {(q, e, a)} for  all  a  in  A. 

By  induction  on  m  and  n,  we  can  show  that,  for  A  in  N  and  m,  n  ~  1, 

m (3.1.3)  (A, A) :::> (x, y) for  some m if and  only if (q, x,  A, e) ~ -   (q, e, e, y) 

for  some  n 

SEC. 3.1  FORMALISMS  FOR  TRANSLATIONS  231 

O n l y / f :   The  basis,  m  =  1,  holds  trivially,  as  A  ~  x, y  must  be  in  R. 
Then  (q,  x,  A,  e)  ~  (q,  x,  xh(y),  e) ~  (q,  e, h(y),  e) ~  (q,  e,  e, y). 

For  the  inductive  step  assume  (3.1.3)  for  values  smaller  than  m,  and 

m--1 let  (A,  A)  ~  (xoBlXl  .. .   BkXk,  YoB1Yl  "'"  BkYk)  ~  (X, y).  Because  simple 

SDTS's  involve  no  permutation  of  the  order  of  nonterminals,  we  can  write 

mlt x  =  x o u l x l ' "   UkXk  and  y  =  yoVlyl  . . .  VkYk,  SO that  (B~, B~):=~ (u~, %)  for 

1 ~  i  _~ k,  where  m t <   m  for  each  i.  Thus,  by  the  inductive  hypothesis 
(3.1.3),  (q,  ui,  Bi,  e) ~  (q,  e,  e,  v~). Putting  these  sequences  of moves together, 
we  have 

(q,  x,  A,  e) ~--   (q,  x,  xoh(Yo)B~  .  . .  BkXkh(yk),  e) 

(q,  uax~  . . .  UkX k,  h(Yo)Ba  "'"  B k x e h ( y k) ,   e) 
]--- (q,  u tx l  "'"  UkX~,, Bi  . ..   BkXkh(Yk),  Yo) 

~---- (q,  Xl  "'"  UkXk, xah(Y~)  " " " BkXkh(Yk),  YoVa) 
~---  .  . . ~--- (q,  e,  e, y) 

If:  Again  the  basis,  n  =  1,  is  trivial.  It  must  be  that  A  ~  e,  e  is  in  R. 
For  the  inductive  step,  let  the  first  move  of P  be 

(q,  x,  A,  e)  ~  (q,  x,  xo h( yo )B lxlh (y a)   .  . .  B~xkh(Yk),  e), 
where  the  xt's  are  in  Z*  and  the  h(y~)'s  denote  strings  in  (A')*,  with  the  y~'s 

in  A*.  Then  Xo  must  be  a  prefix  of  x,  and  the  next  moves  of P  remove  x 0 

from  the  input  and  pushdown  list  and  then  emit  Yo.  Let  x'  be  the 
remaining  input.  There  must  be  some  prefix  u 1 of  x'  that  causes  the  level 
holding  Ba  to  be  popped  from  the  pushdown  list.  Let  Vl  be  emitted  up  to 
the  time  the  pushdown  list  first  becomes  shorter  than  [Bi"-BkXkh(Yk)l. 

Then  (q,  u~, B~,  e)1--- (q,  e,  e,  vl)  by  a  sequence  of  fewer  than  n  moves.  By 

inductive  hypothesis  (3.1.3),  (B~,  Bi)  ==~ (ua,  v~). 

Reasoning  in this  way,  we find that  we can  write  x  as XoUlXl  . . .   UkX ~ and 

y  as  Y o V l Y~ ' ' ' V k y k   SO  that  ( B ,B ~) = : ,   (u,  %)  for  1 _~  i_~  k.  Since  rule 
A - - ~  xoB~x~  . ..   BkXk,  YoBiYl  "'"  BkYe  is  clearly  in  R,  we  have 

(A,  A)  = -   (x,  y). 

As  a  special  case  of  (3.1.3),  we  have  (S,  S ) = ~   (x, y)  if  and  only  if 
(q,  x,  S,  e)[---(q,  e,  e, y),  so  z,(P)  =  z(T).  [~ 

Example  3.'12 

The  simple  SDTS  T  having  rules 

E -   > q-  EE,  E E   --k 

E  ~  *  EE,  E E   * 

E----~  a,  a 

232  THEORY  OF  TRANSLATION  CHAP.  3 
would  give  rise  to  the  pushdown  transducer 

P  =  ({q}, [a,  + ,   ,},  {E, a,  + ,   ,,  a',  -q-', ,'},  [a,  + ,   ,},  6,  q, E,  ~), 
where  d~ is  defined  by 

(1)  6(q,  e, E)  =  {(q,  +  EE  +',  e),  (q,  *  EE  ,',  e),  (q,  aa',  e)} 
(2)  d~(q, b,  b)  =  [(q,  e,  e)} for  all  b  in  [a,  + ,   ,] 
(3)  6(q,  e, b')  =  [(q,  e, b)} for  all  b  in  {a,  + ,   ,}. 

This  is  a  nondeterministic  pushdown  transducer.  Example  3.11  gives an  equivalent  deterministic  pushdown  transducer.  [~[] 

LEMMA 3.3 

Let P  =  (Q,  E, F,  A, $,  qo,  Zo, F)  be  a  pushdown  transducer.  Then  there is  a  simple  SDTS  T  such  that  z(T)  =  1re(P). 

Proof.  The  construction  is  similar  to  that  of  obtaining  a  CFG  from a  PDA.  Let  T  =  (N,  E,  A,  R,  S),  where 

(1)  N  =  [[pAq]  IP, q  ~  Q,  A  ~  F}  t,J {S}. (2)  R  is  defined  as  follows: 

(a)  If  5(p,  a,  A)  contains  (r, X ~ X z . . .   X k, y),  then  if  k  >  0,  R  con- tains  the  rules 

[pAqk ]  > a[rXlql][qiX2q2]...  [qk_~Xkqk], 

y[rX~ql][q~X2q2]""  [qk-~Xkqk] 

for  all  sequences  ql,  q 2 , . . . ,   qe  of  states  in  Q.  If  k  =  0,  then the  rule  is  [pAr]  ~  a, y. 

(b)  For  each q in Q, R  contains  the rule S  ~  [q0 Zo q],  [q0 Zo q]. 
Clearly,  Tis  a  simple  SDTS.  Again,  by induction  on m and n it is straight- forward  to  show  that 

(3.1.4) 

m ([pAq],  [pAq])  =0. (x, y)  if  and  only  if  (p,  x,  A,  e ) ~   (q,  e,  e, y) 

for  all p  and  q in  Q  and  A  ~  F 
We  leave  the  proof  of (3.1.4)  for  the  Exercises. 

+ Thus  we  have  (S,  S)  ~  ([q0 Z0 q],  [q0 Z0 q]) ==~ (x, y)  if  and  only  if 

(q0, x,  Zo, e ) ~   (q,  e, e, y).  Hence  z(T)  =  z,(P).  [[] 

Example 3.13 

Using the  construction  in  the  previous  lemma  let  us  build  a  simple  SDTS from  the  pushdown  transducer  in  Example  3.11.  We  obtain  the  SDTS 

T  =  (N, {a,  + ,   ,},  [a,  + ,   ,},  R,  S),  where  N  =  {[qXq]]  X  ~  [ + ,   ,,  E}}  U  S and  where  R  has  the  rules 

EXERCISES  233 
S  > [qEq],  [qEq] 
[qEq]  > a, a 

[qEq]  > +  [qEq][qEq][q  +  q],  [qEq][qEq][q  -F  q] 
[qEq]  > *  [qEq][qEq][q  *  q],  [qEq][qEq][q  *  q] 
[q  +  q ] -   ~ e , +  

[ q , q ]   > e , ,  

Notice  that  using  transformations  similar  to  those  for  removing  single 
and  e-productions  from  a  CFG,  we  can  simplify  the  rules  to 

S  >a,a 
S  > +  SS,  S S  + 
S  > , S S ,   S S ,   Q 

THEOREM 3.2 

Tis  a  simple SDT if and  only if Tis z(P) for some pushdown  transducer P. 
Proof  Immediate  from  Lemmas  3.1,  3.2,  and  3.3.  E] 

In  Chapter  9 we  shall introduce  a  machine  called the pushdown  processor 
which  is  capable  of defining  all  syntax-directed  translations. 

E X E R C I SE S  
3.1.1.  An  operator  with  one  argument is called a unary operator, one with two 

arguments  a  binary  operator,  and  in  general  an  operator  with  n  argu- 
ments  is  called  an  n-ary  operator.  For  example,  --  can  be  either  a 

unary  operator  (as  in  - - a )   or  a  binary  operator  (as  in  a -   b).  The degree 

of  an  operator  is  the  number  of  arguments  it  takes.  Let  19  be 
a  set  of  operators  each  of  whose  degree  is  known  and  let  ~  be  a  set 
of  operands.  Construct  context-free  grammars  G1  and  G2  to  generate 
the  prefix  Polish  and  postfix  Polish  expressions  over O  and  ~. 

"3.1.2.  The  "precedence"  of  infix  operators  determines  the  order  in  which 

the  operators are to  be applied.  If binary  operator 0t  "takes precedence 

over"  02,  then  a02  b01  c  is  to  be  evaluated  as  a02  (b01 c).  For 
example,  *  takes  precedence  over  +,  so  a  +  b *  c  means  a  +  (b *  c) 

rather  than  (a  +  b) *  c.  Consider  the  Boolean  operators  ~  (equiva- 
lence), ~  (implication),  V (or),  A (and),  and  --7  (not).  These operators 

are  Iisted  in  order  of increasing precedence. -7  is  a unary operator and 
the  others  are  binary.  As  an  example,  ---7 (a V b)  _= -7  a A --7 b  has  the 

implied  parenthesization  (--a (a V b))  :-  ((--~ a) A (--1 b)).  Construct  a 
CFG  which  generates  all  valid  Boolean  expressions  over  these  opera- 
tors  and  operands  a, b,  c  with  no  superfluous  parentheses. 

234  THEORY OF TRANSLATION  CHAP.  3 

"3.1.3.  Construct  a  simple  SDTS  which  maps  Boolean  expressions  in  infix 

notation  into  prefix  notation. 

'3.1.4.  In  AL GO L,   expressions  can  be  constructed  using  the  following  binary 

operators  listed  in  order  of  their  precedence  levels.  If  more  than  one 
operator  appears  on  a  level,  these  operators  are  applied  in  left-to-right 
order.  For  example  a  -  b  -Jr  c  means  (a  -  b)  ~  c. 

(1)  ~  (2)  ~  (3)  V 
(4)  A  (5)  --1  (6)  ~  <  =  ~  > 

(7)  + -  (8)  *  /  "--  (9)  1" 

Construct  a  simple  SDTS  which  maps  infix expressions  containing  these 
operators  into  postfix  Polish  notation. 

3.1.5.  Consider  the  following  SDTS.  A  string  of  the  form  ( x )  is  a  single 

nonterminal" 

(exp)  > sum  (exp) C1~ with  (v ar)  ~--  (exp) C2) to  (exp)~3~,f 

begin  local  t; 

t  ~--- 0; 
for  (var)  ~  (exp) ~z~ to  (exp) ~3~ do 

t  ~  t  4-  (exp){1};  result  t 
end 
(var)  - -->   (id),  (id) 
(exp )  > (id),  (id) 

(id)  ~  a (id),  a (id) 
(id)  -  ~ b(id),  b(id) 
(i d )-  > a,  a 
(id)  --->   b,  b 
Give  the  translation  for  the  sentences 
(a)  sum  aa  with  a  ~  b  to  bb. 
(b)  sum  sum  a  with  aa  ~  aaa  to  aaaa  with  b  ~--  bb  to  bbb. 

"3.1.6.  Consider  the  following  translation  scheme" 

(statement)  > for  (var)  ~  (exp){ 1) to  (exp) (z) do  (statement), 

begin  (var)  ~---- (exp){1) ; 

L"  if  (var)  .~  (exp) {2} then 

begin  (statement); 

(var)  ~  (var)  +  1; 
go  to  L 
end 
end 

tNote  that  this  comma  separates  the  two  parts  of the rule. 

EXERCISES  235 
(var)  ~  (id),  (id) 
( e x p ) -   ~ (id),  (id) 
(statement)  ~  (var)  ~--  (exp),  (var)  ~--  (exp) 

(id)  ~  a(id),  a ( id ) 
( i d ) -   > b(id),  b(id) 
(id)  ---->  a,  a 
(id)  ~  b, b 

Why  is this  not  an  SDTS ? What  should  the  output  be  for the following 
input  sentence: 

for  a  ~--  b  to  aa do  baa  ~  bba 
Hint:  Apply  Algorithm  3.1  duplicating  the  nodes  labeled  (var)  in  the 

output  tree. 

Exercises  3.1.5  and  3..1.6 provide  examples  of  how  a  language  can 
be  extended  using  syntax  macros.  Appendix  A.1  contains  the  details 

of  how  such  an  extension  mechanism  can  be  incorporated  into  a 
language. 

3.1.7.  Prove  that  the  domain  and  range  of  any  syntax-directed  translation  are 

context-free  languages. 

3.1.8.  Let  L  _q  ~*  be  a  CFL  and  R  ~  ~*  a  regular  set.  Construct  an  SDTS 

T  such  that 

~(T)  =  {(x, y) lif x  ~  L  --  R,  then y  =  0 

if x  ~  L  A  R,  then y  =  1} 

"3.1.9.  Construct  an  SDTS  T  such  that 

'r(T)  =  {(x, y )l x  e  {a, b}*  and  y  =  d,  where 

i  =  [ ~, (x)   -  ~b(x) l, where  ~a(x)  is 
the  number  of d's  in  x} 

"3.1.10.  Show  that  if  Z  is  a  regular  set  and  M  is  a  finite  transducer,  then 

M (L )   and  M - I ( L )   are  regular. 

"3.1.11.  Show  that  if L  is  a  CFL  and  M  is  a  finite  transducer,  then  M ( L)   and 

M-~(L)  are  CFL's. 

"3.1.12.  Let  R  be  a  regular  set.  Construct  a  finite  transducer  M  such  that 

M (L )   =  L/R  for  any  language L.  With  Exercise  3.1.11,  this  implies  that 
the  regular  sets  and  CFL's  are  closed  under/R. 

"3.1.13.  Let  R  be  a  regular  set.  Construct  a  finite  transducer  M  such  that 

M(L)   =  R/L  for  any  language  L. 

236  THEORY OF  TRANSLATION  CHAP.  3 

3.1.14.  An  SDTS  T  =  (N,  ~,  A,  R,  S)  is  right-linear  if each  rule  in  R  is  of the 

form 

A  ~  xB,  yB  
or 

A - -- - - ~x, y 

where A, B  are in  N,  x  ~  ~*,  and y  ~  A*.  Show that  if T  is right-linear, 
then  'r(T)  is  a  regular  translation. 

*'3.1.15.  Show  that  if  T  g  a*  *  b*  is  an  SDT,  then  T can  be  defined  by  a  finite 

transducer. 

3.1.16.  l e t   us  consider  the  class  of  prefix  expressions  over  operators  O  and 

operands  ~.  If  a l. .. a,  is  a  sequence  in  ((R)  u  E)*,  compute  st,  the 
score  at  position  i,  0  <  i <  n,  as  follows" 

(1)  So  =  1. 
(2)  If at  is  an  m-ary  operator,  let  & =  &_ 1 +  m  --  1. 

(3)  If at  ~  ~,  let st  =  st-1  --  1. 

Prove  that  a a. .. a,,   is  a  prefix  expression  if  and  only  if  s,  =  0  and 
st >  0  for  all  i  <  n. 

"3.1.17.  Let  a l .. .a ,  be  a  prefix  expression  in  which  a l  is  an  m-ary  operator. 

Prove  that  the  unique  way  to  write  a l .. .a,   as  a aw ~' . 'W m,   where 

wl . . . .   ,Wm  are  prefix  expressions,  is  to  choose  w j,  1 <  j  <  m,  so  that 
it  ends  with  the  first  ak  such  that  sk  =  m  --j. 

"3.1.18.  Show  that  every  prefix  expression  with  binary  operators  comes  from 

a  unique  infix  expression  with  no  redundant  parentheses. 

3.1.19.  Restate  and  prove  Exercises  3.1.16-3.1.18  for  postfix  expressions. 
3.1.20.  Complete  the  proof  of Theorem  3.1. 
"3.1.21.  Prove  that  the  order  in  which  step  (2)  of  Algorithm  3.1  is  applied  to 

nodes  does  not  affect  the  resulting  tree. 

3.1.22.  Prove  Lemrna  3.1. 
3.1.23.  Give  pushdown  transducers  for  the  simple  SDT's  defined  by  the  trans- 

lation  schemata  of Examples  3.5  and  3.7. 

3.1.2,4.  Construct  a  grammar  for  SNOBOL4  statements  that  reflects  the 

associativity  and  precedence  of  operators  given  in  Appendix  A.2. 

3.1.25.  Give an  SDTS that  defines the (empty store) translation  of the following 

PDT" 

({q, p}, {a, b}, [Zo,  A, B}, [a,  b}, ~, q, Z0,  ~ )  
where  ~  is  given by 

BIBLIOGRAPHIC  NOTES  237 
6(q,  a,  X)  =  (q, A X,  e),  for  all  X  =  Z0, A, B 
6(q,  b,  X)  =  (q,  BX,  e),  for  all  X  =  Z0, A, B 

O(q, e, A)  --  (p,  A, a) 
0(p,  a, A)  =  (p,  e, b) 
6(p,  b, B)  =  (p,  e,  b) 
6(p,  e, Zo)  =  (p,  e,  a) 

*'3.1.26.  Consider  two  pushdown  transducers  connected  in  series,  so  the  output 

of the first forms the input of the second.  Show that with such a tandem 
connection,  the set  of possible  output  strings  of the second PDT  can  be 

any recursively enumerable  set. 

3.1.27.  Show  that  T  is  a  regular  translation  if  and  only  if  there  is  a  linear 

context-free  language L  such  that  T  =  [(x, y)lxcy R ~  L},  where  c  is  a 
new  symbol. 

'3.1.28.  Show  that  it  is  undecidable  for  two  regular  translations  T,  and  /'2 

whether  Ti  = / 2 .  

Open  Problems 

3.1.29.  Is  it  decidable  whether  two  deterministic  finite  transducers  are  equiva- 

lent ? 

3.1.30.  Is  it  decidable  whether  two  deterministic  pushdown  transducers  are 

equivalent ? 

Research  Problem 

3.1.31.  It  is  known  to  be  undecidable  whether  two  nondeterministic  finite 

transducers  are  equivalent  (Exercise  3.1.28).  Thus  we  cannot  "mini- 
mize"  them  in  the  same  sense  that  we  minimized  finite  automata  in 

Section  3.3.1.  However,  there  are  some  techniques  that  can  serve  to 
make  the  number  of states  smaller.  Can  you  find  a  useful  collection  of 
these ? The  same  can  be  attempted  for  PDT's. 

B I B L I OG R A P H I C   N OT E S  
The  concept  of  syntax-directed  translation  has  occurred  to  many people.  Irons 
[1961]  and  Barnett  and  Futrelle  [1962] were  among  the  first  to  advocate  its  use. 
Finite  transducers  are  similar  to  the  generalized  sequential  machines  introduced 
by  Ginsburg  [1962].  Our  definitions  of  syntax-directed  translation  schema  and 
pushdown  transducer  along  with  Theorem  3.2  are  similar  to  those  of  Lewis  and 

Stearns  [1968]. Griffiths  [1968] shows  that  the  equivalence  problem  for  nondeter- 
ministic finite  transducers  with  no  e-outputs  is also  unsolvable. 

238  THEORY OF TRANSLATION  CHAP.  3 
3.2.  PROPERTIES  OF  SYNTAX-DIRECTED 

TRANSLATIONS 

In  this  section  we  shall  examine  some  of  the  theoretical  properties  of 
syntax-directed  translations.  We  shall  also  characterize  those  translations 
which  can  be  defined  as  simple  syntax-directed  translations. 

3.2.1.  Characterizing  Languages 
DEFINITION 

We  say  that  language  L  characterizes  a  translation  T  if  there  exist  two 
homomorphisms  h~  and  h2  such  that  T  =  [(h~ (w),  h2(w))  l w  E  L}. 

Example 3.14 

The  translation  T =   {(a~,aO[n  ~  1}  is  characterized  by  0 /,  since 
T  =  {(ha(w),  h2(w))lw  ~  0+}, where  ha(0)  =  h2(0)  =  a.  E] 

We  say  that  a  language  Z  ~  (X  U  A')*  strongly  characterizes  a  transla- 
tion  T  ~  X*  *  A*  if 

(I)  X A A ' = ~ 3 .  

(2)  Z  =  {(ha(w),  h2(w))lw  ~  L},  where 

(a)  ha (a)  =  a  for  all a  in  Z  and  h, (b)  =  e for  all b in  A'. 
(b)  hz(a )  =  e  for  all  a  in  X  and  h z  is  a  one-to-one  correspondence 

between  A' and  A [i.e.,  h2(b)  e  A for all b in A' and  h2(b)  =  h2(b') 
implies  that  b  =  b']. 

Example  3.15 

The  translation  T =   [ (a" ,a") [ n>   1}  i s   strongly  characterized  by 
L 1 =  {a"b"!  n  >  1}.  It  is  also  strongly  characterized  by  L 2 =  {wlw  consists 

of  an  equal  number  of  a's  and  b's].  The  homomorphisms  in  each  case  are 
ha(a)  =  a,  h i ( b ) =   e  and  h2(a)=  e,  hz(b)=  a.  T  is  not  strongly  characterized 

by  the  language  0 +.  [Z 

We  can  use  the  concept  of  a  characterizing  language  to  investigate  the 
classes  of  translations  defined  by  finite  transducers  and  pushdown  trans- 

ducers. 

LEMMA  3.4 

Let  T  =  (N,  E,  A, R,  S)  be  an  SDTS  in  which  each  rule  is  of  the  form 
A ~ a B ,   bB  or  A  ---,  a,  b f o r a ~ X u { e } ,   b e A u [ e ] ,   a n d B ~ N .   Then 

v(T)  is  a  regular  translation. 

Proof  Let  M  be  the  finite  transducer  (N  U  If},  X, A, ~,  S,  {f}),  w h e r e f  
is  a  new  symbol.  Define  O(A, a)  to  contain  (B,  b)  if A  ---~ aB,  bB  is in  R,  and 

SEC. 3.2  PROPERTIES OF SYNTAX-DIRECTED TRANSLATIONS  239 
to  contain  (f,  b)  if A  ~  a, b  is  in R.  Then  a  straightforward  induction  on  n 

shows  that 

?1 (S, x,  e ) ~   (A,  e, y)  if and  only  if (S,  S)  ---9.  (xA,  yA) 

It  follows  that  (S,  x,  e) ~  (f,  e, y)  if  and  only  if  (S,  S) ==~ (x, y).  The 

details  are  left  for  the  Exercises.  Thus,  z(T)  =  z(M).  E] 

THEOREM  3.3 

T  is  a  regular  translation  if  and  only  if  T  is  strongly  characterized  by 
a  regular  set. 

Proof 

If:  Suppose  that  L  G  (X  U  A')*  is  a  regular  set  and  that  ht  and  h2  are 
homomorphisms  such  that  h~(a)=  a  for  a  ~  X,  h i ( a ) =   e  for  a  ~  A', 

hz(a) =  e for  a  ~  X, and h2 is a  one-to-one  correspondence  from A' to A.  Let 
T =   {(ha(w),  hz(w)lw  ~  L],  and  let  G =  (N,  Z  U  A', P,  S)  be  a  regular gram- 
mar  such  that  L(G)  =  L.  Then  consider  the  SDTS  U  =  (N,  X, A, R,  S), 
where  R  is  defined  as  follows: 

(1)  If A  ~  aB  is  in P,  then  A  --,  hl(a)B,  h2(a)B  is  in  R. 

(2)  If A  ~  a  is  in P,  then  A  --~  ha(a),  h2(a)  is  in  R. 

/t An  elementary  induction  shows  that  (A, A ) = ~   (x, y)  if  and  only  if 

U n 
A  ~  w,  h,(w)  =  x,  and  h2(w)  =  y. 

G  + 

Thus  we  can  conclude  that  (S,  S)  =~  (x, y)  if  and  only  if  (x, y)  is  in  T. 

U Hence  z ( U ) =   T.  By  Lemma  3.4,  there  is  a  finite  transducer  M  such  that 

, ( M)   =  T. 

Only  if:  Suppose  that  T  ~  Z*  x  A*  is  a  regular  translation,  and  that 
M  =  (Q,  X,  A,  tS, q0, F)  is  a  finite  transducer  such  that  , ( M )   =  T. 

Let  A ' =   {a'[a  ~  A]  be  an  alphabet  of  new  symbols.  Let  G  = 
(Q, X  u  A', P,  q0)  be  the  right-linear  grammar  in  which  P  has  the  following 
productions" 

(1)  If 6(q,  a)  contains  (r, y),  then  q  ~  ah(y)r  is in P,  where  h  is a  homo- 
morphism  such  that  h(a)  =  a'  for  all  a  in  A. 

(2)  If q  is  in  F,  then  q  ~  e  is  in P. 

Let  ha  and  h2  be  the  following  homomorphisms" 

ha(a )  =  a  for  all  a  in  X 
ha(b)  =  e  for  all  b  in  A' 

h2(a)  =  e  for  all  a  in  X 
h2(b' )  =  b  for  all  b'  in  A' 

240  THEORY OF TRANSLATION  CHAP.  3 

We  can  now  show  by induction  on  m  and  n  that  (q,  x,  e ) ~   (r,  e, y)  for 

n some  m  if  and  only  if q  :=~  wr  for  some  n,  where  h~(w)  =  x  and  h2(w)  =  y. 

+ Thus,  (qo,  x,  e) ~  (q,  e, y),  with  q  in  F,  if  and  only  if 
qo  =>  wq  ==~ w,  where 

hl (w )  =  x  and  h2(w )  =  y.  Hence,  T  =  {(hi(w),  h2(w)) I w  ~  L(G)}.  Thus, L(G) 

strongly  characterizes  T.  D 

COROLLARY 

T is a  regular  translation  if and  only  if T is characterized  by  a  regular  set. 
Proof.  Strong  characterization  is a  special  case  of characterization.  Thus 
the  "only  if"  portion  is  immediate.  The  "if"  portion  is  a  simple  generaliza- 

tion  of the  "if"  portion  of the  theorem. 

In  much  the  same  fashion  we  can  show  an  analogous  result  for  simple 
syntax-directed  translations. 

THEOREM  3.4 

T is  a  simple  syntax-directed  translation  if and  only  if it  is  strongly  char- 
acterized  by  a  context-free  language. 

?roof. 

If:  Let  T  be  strongly  characterized  by  the  language  generated  by 
Ga =  (N, Z U  A',P, S), where hi  and h2 are the two homomorphisms  involved. 
Construct  a  simple  SDTS  T1  =  (N,  X, A,  R,  S),  where  R  is  defined  by: 

For  each  production  A  --~  woB awa  .  .  . B k W k  in  P,  let 

A  ~  h~(wo)B aha(w~ ). . .   Bkh~(wk),  h2(wo)B ah2(wa )" "  Bkhz(we) 
be  a  rule  in  R. 

A  straightforward  induction  on  n  shows  that 

Itl  n (1)  If A  :=~ w,  then  (A, A) ==~ (hi(w),  hz(w)). 

Gx  T1 

B  B (2)  If (A,  A) =~  (x, y),  then  there  is some  w such  that  A  =:~ w, hi (w)  =  x, 

T1  Gt and  h2(w)  =  y. 

Thus,  z ( T 1 ) =   T. 

Only  if:  Let  T =   z(T2),  where  T2  =  (N,  E , A , R ,   S),  and  let  A ' =  
{a'[a  ~  A}  be  an  alphabet  of  new  symbols.  Construct  C FG  G2  = 

(N,  X U  A', P,  S),  where P  contains  production  A  ~  Xoy~Blxmy'l  . . .  B kxky~ 
for  each rule A  ~  xoBl x  t  . . .   BkXk,  Y oBl Yl   . ..   BkY k  in  R;  Yl is Yt with  each symbol  a  ~  A replaced  by a'.  Let  h 1 and  h 2 be  the  obvious  homomorphisms, 

ha(a)  =  a  for  a  ~  E,  hi(a)  =  e for a  ~  A',  hz(a)  =  e  for  a  E  E,  and  h2(a')  =  a 

for  a  ~  A.  Again  it  is  elementary  to  prove  by  induction  that 

SEC. 3.2  PROPERTIES  OF SYNTAX-DIRECTED  TRANSLATIONS  241 

n  n (1)  If A ==~ w,  then  (A,  A) ~  (ha(w), h2(w)). 
G~  T2 n  n 
(2)  If  (A,  A)  ~  (x, y),  then  for  some  w,  we  have  A  ~  w,  ha(w)  =  x, 

T2  G~ 

and  h2(w)=  y.  Eli 

COROLLARY 

A  translation is a  simple SDT if and only if it is characterized by a context- 
free  language.  Eli 

We  can  use  Theorem  3.3 and  3.4 to  show that  certain  translations  are  not 
regular  translations  or  not  simple  SDT's.  It  is  easy  to  show  that  the  domain 

and range  of every simple  SDT is a  CFL.  But there are  simple syntax-directed 
translations  whose  domain  and  range  are  regular  sets  but  which  cannot  be 

specified  by  any  finite  transducer  or  even  pushdown  transducer. 

Example 3.16 

Consider  the  simple  SDTS  T  with  rules 

S----~  OS, SO 

S  > 1S,  S1 

S  > e , e  

z(T)  =  {(w, wR.)[ W E  {0,  1}*}. We  shall  show  that  z(T)  is  not  a  regular  trans- 
lation. 

Suppose  that  x(T)  is  a  regular  translation.  Then  there  is  some  regular 
language L  which  strongly characterizes  z(T).  We  can  assume  without loss of 
generality  that  L  ~  {0,  1, a,  b}*,  and  that  the  two  homomorphisms  involved 
are hi(0)  =  0, hi(l)  =  1, ha(a)  =  hi(b)  =  e and h2(0)  =  h2(1)  =  e, h2(a)  =  O, hz(b)  =  1. 

If L  is  regular,  it  is  accepted  by  a  finite  automaton 

M  =  (Q, [0,  1, a,  b}, d~, qo, F) 
with  s  states  for  some  s.  There  must  be  some  z  ~  L  such  that  hi(z)  =  0~1 ' 

and  h2(z)=  lS0 ".  This  is  because  (0Sl ",  1"0 ")  ~  z(T).  All  O's  precede  all 

l's  in  z,  and  all  b's  precede  all  a's.  Thus  the  first  s  symbols  of  z  are  only 
O's  and  b's.  If  we  consider  the  states  entered  by  M  when  reading  the  first  s 

symbols  of z,  we  see that  these  cannot  all be  different;  we can  write  z  =  uvw 
such  that  (qo, z) ~  (q, vw) t--- (q,  w) ~  (p,  e),  where  l uvl  <_ s,  l vl  >_  1,  and p  ~  F.  Then  uvvw  is  in  L.  But  hl(uvvw )  =  0"+ml"  and  h2(uvvw)  =  l'+n0 s, 

where  not  both  m  and  n  are  zero.  Thus,  (0'/ml s,  l'+n0s)  ~  z(T),  a  contradic- 

tion.  We  conclude  that  z(T)  is not  a  regular  translation.  D 

242  THEORY  OF  TRANSLATION  CHAP.  3 

Example 3.17 

Consider  the  SDTS  T  with  the  rules 

S -   ~ A~I~cA ~2~, A~2~cA ~1~ 
A  > 0A,  0A 
A  ~  1A,  1A 
A  > e , e 

Here  'r(T)  =  {(ucv,  vcu)lu,  v  ~  [0,  1}*}.  We  shall  show  that  'r(T)  is  not 
a  simple  SDT. 

Suppose  that  L  is  a  CFL  which  strongly  characterizes  z(T).  We  can  sup- 
pose  that  A'  =  [c',  0',  1'}, L  ~  ({0, 1,  c} u  A3*,  and  that  h 1 and  h2  are  the 

obvious  homomorphisms.  For  every  u  and  v  in  {0,  1}*, there  is  a  word  zuv 
in  L  such  that  hl(zu,)=  ucv  and  h2(z,,)=  vcu.  We  consider  two  cases, 

depending  on  whether  c  precedes  or  follows  c'  in  certain  of  the  zu,'s. 

Case  1:  For  all  u  there  is  some  v  such  that  c precedes  c' in  z~v. Let  R  be 
the  regular  set  {0,  1, 0',  l'}*c[0,  1, 0',  l'}*c'{0,  1, 0',  1'}*.  Then  L  n  R  is 
a  CFL,  since  the  CFL's  are  closed  under  intersection  with  regular  sets. 
Note  that L  n  R  is the  set  of sentences  in Z  in  which  c precedes  c'.  Let M  be 
the  finite  transducer  which,  until  it  reads  c,  transmits  O's  and  l's,  while 

skipping  over  primed  symbols.  After  reading  c,  M  does  nothing  until  it 
reaches  c'.  Subsequently,  M  prints  0  for  0'  and  1  for  1',  skipping  over  O's 

and  l's.  Then  M(L  n  R)  is  a  CFL,  since  the  CFL's  are  closed  under  finite 
transductions,  and  in this  case  M(L  N  R) =  [uulu  ~  [0,  1 }*}. The latter is not 
a  CFL  by  Example  2.41. 

Case  2:  For  some  u  there  is  no  v  such  that  c precedes  c'  in  zu,. Then  for 
every  v  there  is  a  u  such  that  c'  precedes  c  in  z,,.  An  argument  similar  to 
case  1  shows  that  if  L  were  a  CFL,  then  [vv[v  ~  [0,  1}*} would  also  be 
a  CFL.  We  leave  this  argument  for  the  Exercises. 

We  conclude  that  ~(T)  is  not  strongly  characterized  by  any  context-free 
language  and  hence  is  not  a  simple  SDT.  [[] 

Let  3,  denote  the  class  of  regular  translations,  3,  the  simple  SDT's,  and 
3  the  SDT's.  From  these  examples  we  have  the  following  result. 

THEOREM 3.5 

Z,  ~  Z,  ~  Z. 
Proof  3,  ~  3  is  by  definition.  3,  ~  3s  is  immediate  when  one  realizes 
that  a  finite  transducer  is  a  special  case  of a  PDT.  Proper  inclusion  follows 
from  Examples  3.16  and  3.17.  [[] 

SEC. 3.2  PROPERTIES OF SYNTAX-DIRECTED TRANSLATIONS  243 

3.2.2.  Properties  of  Simple  SDT's 

Using  the  idea  of  a  characterizing  language,  we  can  prove  analogs  for 
many  of the  normal  form  theorems  of Section  2.6.  We  shall  mention  two  of 

them  here  and  leave  some  others  for  the  Exercises.  The  first  is  an  analog  of 

Chomsky normal  form. 

THEOREM 3.6 

Let  T  be  a  simple  SDT.  Then  T  =  I:(T~),  where  Ti  =  (N,  ~,  A, R,  S)  is 
a  simple  SDTS  such  that  each  rule  in  R  is  of one  of the  forms 

(1)  A  ~  BC,  BC,  where  A,  B,  and  C  are  (not  necessarily  distinct)  mem- 
bers  of N,  or 

(2)  A  --~ a,  b,  where  exactly  one  of a  and  b  is  e  and  the  other  is  in  ~  or 

A,  as  appropriate. 

(3)  S  ---~ e,  e  if (e, e)  is  in  T  and  S  does  not  appear  on  the  right  of any 

rule. 

Proof.  Apply the construction of Theorem 3.4 to a grammar in CNF.  D 

The  second  is an  analog  of Greibach  normal  form. 
THEOREM 3.7 

Let  T  be  a  simple  SDT.  Then  T  =  ~(T1),  where  T1  =  (N,  E,  A, R,  S)  is 
a  simple  SDTS  such  that  each  rule  in  R  is  of the  form  A  - 4   a~,  b~,  where 
is in N*,  exactly one of a  and  b is e, and  the  other is in ~  or A [with the  same 

exception  as  case  (3)  of the  previous  theorem]. 

Proof.  Apply the construction of Theorem 3.4 to a grammar in GNF.  [Z] 

We  comment  that  in  the  previous  two  theorems  we  cannot  make  both  a 
be  in  E  and  b  in  A  at  the  same  time.  Then  the  translation  would  be  length 
preserving,  which  is  not  always  the  case  for  an  arbitrary  SDT. 

3.2,3.  A  Hierarchy  of  SDT's 

The  main  result  of  this  section  is  that  there  is  no  analog  of  Chomsky 
normal  form  for  arbitrary  syntax-directed  translations.  With  one  exception, 

each time we increase the number of nonterminals which we allow on the right 

side of rules  of an  SDTS,  we can  define a  strictly larger class  of SDT's.  Some 
other  interesting  properties  of  SDT's  are  proved  along  the  way. 

DEFINITION 

Let  T  =  (N,  ~,  A, R,  S)  be  an  SDTS.  We  say  that  T  is  of  order  k  if for 
no  rule  A  ~  ~,  fl in R  does 0c (equivalently fl) have  more than  k  instances  of 
nonterminals.  We  also  say  that  z(T)  is  of order  k.  Let  3 k be  the  class  of all 

SDT's  of order  k. 

244  THEORY  OF  TRANSLATION  CHAP. 3 

Obviously,  ~1 ~  ~32 ~  "'"  -~  ~3~ ~ . . . .   We  shall  show  that  each  of  these 
inclusions  is  proper,  except  that  ~33 =  32.  A  sequence  of preliminary  results 
is  needed. 

LEMMA 3.5 

Proof.  It  is  elementary  to  show  that  the  domain  of  an  SDTS  of  order  1 
is  a  linear  CFL.  However,  by  Theorem  3.6,  every  simple  SDT  is  of  order  2, 
and  every  CFL  is  the  domain  of  some  simple  SDT  (say,  the  identity  trans- 
lation  with  that  language  as  domain).  Since the  linear  languages  are  a  proper 

subset  of the  CFL's  (Exercise  2.6.7),  the  inclusion  of  3~  in  ~2  is proper.  D 

There  are  various  normal  forms  for  SDTS's.  We  claim  that  it  is  possible 
to  eliminate  useless  nonterminals  from  an  SDTS  as  from  a  CFG.  Also,  there 
is a  normal  form  for  SDTS's  somewhat  similar  to  CNF.  All rules  can  be  put 
in  a  form  where  the  right  side  consists  wholly  of  nonterminals  or  has  no 
nonterminals. 

DEFINITION 

A  nonterminal  A in an  SDTS  T  =  (N,  E,  A, R,  S)  is useless if either 

(1)  There  exist  no  x  ~  E*  and  y  ~  A*  such  that  (A, A) =>  (x, y),  or 
(2)  For  no  t~  and  t~2 i n   (N  U  E)*  and  fl~  and  flz  in  (N  U  A)*  does :g 

(s, s) 
LEMMA 3.6 

Every  SDT  of  order  k  is  defined  by  an  SDTS  of  order  k  with  no  useless 
nonterminals. 

Proof.  Exercise  analogous  to  Theorem  2.13.  [~ 

LEMMA 3.7 

Every SDT  T of order k  ~  2 is defined  by an  SDTS  T1 =  (N,  1~, A, R,  S), 
where  if A  ---~ ~,  fl  is  in  R,  then  either 

(1)  ~  and  fl  are  in  N*,  or 
(2)  ~  is  in  E*  and  fl  in  A*. 

Moreover,  T1  has  no  useless  nonterminals. 

Proof.  Let  T2  =  (N',  E,  A,  R', S)  be  an  SDTS  with  no  useless  nonter- 
minals  such  that  z ( T z ) =   T.  We  construct  R  from  R'  as  follows.  Let 
A  -----~ x o B l x l ' "   BkXk, YoCiYl"'"  CkYk be  a  rule  in  R',  with  k  >  0.  Let 

be  the  permutation  on  the  set  of  integers  1 to  k  such  that  the  nonterminal 
Bi  is  associated  with  the  nonterminal  C~(i).  Introduce  new  nonterminals 

A',  D ~ , . . . ,   D k and  E 0 , . . . ,   Ek,  and  replace  the  rule  by 

SEC. 3.2  PROPERTIES OF SYNTAX-DIRECTED TRANSLATIONS  245 

A  > Eo A',  EoA' 
Eo  > Xo, Yo 

A '~  > D r ' "   Dk,  D't  * * * D~  where  D~ =  D',~.) for  1 ~  i <  k 

D t  >B~E i , B  iE i  f o r l ~ i < k  

Et  > x;, y..)  for  1 ~  i <  k 

For  example,  if  the  rule  is  A  ---,  xoB~XlB2X2B3x3,  yoB3y~B~y2B2y3,  then 
zr --  (2,  3,  1).  We  would  replace  this  rule  by 

A  > EoA',  EoA'  
Eo  -  > Xo, Yo 

A' - - - - ~   D 1D 2D 3,  D3D1D2 

D~  ~ B~ E~, B t E~,  for  i  =  1, 2,  3 

E1  > xl, Y2 
E2  =  > x2, Y3 

E3  > x3, Y l 

Since  each  Dt  and  E~ has  only  one  rule,  it  is  easy  to  see  that  the  effect  of 
all  these  new  rules  is  exactly  the  same  as  the  rule  they  replace.  Rules  in  R' 
with  no  nonterminals  on  the  right  are  placed  directly  in  R.  Let  N  be  N' 

together  with  the  new  nonterminals.  Then  z ( T 2 ) :   z(T~)  and  T2  satisfies 
the  conditions  of the  lemma.  D 

LEMMA 3.8 

~2  :  ~3" 
Proof.  It  suffices,  by  Lemma  3.7,  to  show  how  a  rule  of  the  form 
A  ~  B1B2B 3, C1C2C 3 can  be  replaced by two rules with two nonterminals  in 
each  component  of  the  right  side.  Let  ~z be  the  permutation  such  that  Bt  is 

associated with  C~,).  There  are  six possible  values for ~z. In  each case,  we can 
introduce  a  new  nonterminal  D  and  replace  the  rule  in  question  by two  rules 

as  shown  in  Fig.  3.6. 

n(1)  n(2)  n(3)  Rules 

1  2  3  A  ~  BtD,  B1D 
1  3  2  A  ~  BiD, BtD 
2  1  3  A  ~  DB3,  DB3 
2  3  1  A  ~  DB3, B3D 3  1  2  /1 ~  B1 D,  DB1 

3  2  1  A  ~  B1D,  DBi 

D ~  BzB3, B2B3 D ~  BzB3, 

B3Bz 
D  ~  B1B2, BzBx D ~  B1Bz, 

BiBz 
D  ~  B2B3, BzB3 
D  ~  B2B3, B3Bz 

Fig. 3.6  New rules. 

246  THEORY OF TRANSLATION  CHAP.  3 

It  is  straightforward  to  check  that  the  effect  of the  new  rules  is  the  same as  the  old  in  each  case. 

LEMMA  3.9 

Every  SDT  of  order  k  ~  2  has  an  SDTS  T  =  (N,  E, A, R,  S)  satisfying 
Lemma  3.7,  and  the  following" 

(1)  There  is  no  rule  of the  form  A  ---~ B,  B  in  R. 
(2)  There  is  no  rule  of the  form  A  --~ e,  e  in  R  (unless  A  =  S,  and  then 
S  does  not  appear  on the  right  side  of any rule). 

Proof.  Exercise  analogous  to  Theorems  2.14  and  2.15.  [~] 

We  shall  now  define  a  family  of  translations  Tk for  k  ~  4  such  that  Tk 
is  of  order  k  but  not  of  order  k -   1.  Subsequent  lemmas  will  prove  this. 

DEFINITION 

Let  k  ~  4.  Define  2~k, for  the  remainder  of  this  section  only,  to  be 
{ a l , . . . ,   ak}. Define  the  permutation  nk,  for  k  even,  by 

k + i _ I  i  +  1 2 ~ ( i )   if i is  odd 

if i is  even 

Thus,  re4 is  [3,  1, 4, 2] and  zt6 is  [4,  1, 5, 2, 6,  3].  Define  ztk for  k  odd  by 

~ ( i )   = 

r k + l  2  ' 

i k - -   - ~ - +  1, 
i - - 1  2  ' 

i f / =   1 

if i is  even 
if i is  odd  and  i c  1 
Thus,  zt5  =  [3, 5,  1, 4, 2]  and  zt7  =  [4,  7,  1, 6,  2,  5,  3]. 

Let  T k be  the  one-to-one  correspondence  which  takes 

a~'a~ ~. . .   a~*  to  .~'~"'.~"~'~'  "~'~'~' ~n(1)t~zt(2)"..  ten(k) 
For  example,  if as,  a2,  a3,  and  a4  are  called  a,  b,  c,  and  d,  then 

T4  =  {(a'bJckd z,  cea'd~bJ)  l i, j,  k,  l  ~  0} 
In  what  follows,  we  shall  assume  that  k  is  a  fixed  integer,  k  ~  4,  and 
that  there  is  some  SDTS  T  =  (N, ~k, Ek, R,  S)  of order  k  --  1 which  defines 

SEC. 3.2  PROPERTIES OF SYNTAX-DIREC~D TRANSLATIONS  247 

Tk.  We  assume  without  loss  of  generality  that  T  satisfies  Lemma  3.9,  and 
hence  Lemmas  3.6  and  3.7.  We  shall  prove,  by  contradiction,  that  T  cannot 

exist. 

DEFINITION 

Let  E  be  a  subset  of Ek  and  A  ~  N.  (Recall  that  we  are  referring  to  the 
hypothetical  SDTS  T.)  We  say  that  X  is  (A,  d)-bounded  in  the  domain  (alt. 

range) if for every (x, y)  such  that  (A, A) =-> (x, y),  there  is  some  a  E  X  such 

T that x  (alt. y) has no more than  d occurrences  of a. If X is not  (A, d)-bounded 

in  the  domain  (alt.  range)  for  any  d,  we  say  that  A  covers  X  in  the  domain 

(alt.  range). 

LEMMA 3.10 

If A covers X in the domain,  then it covers X in the range,  and conversely. 
Proof  Suppose  that  A  covers  Z  in  the  domain,  but  that  X  is  (A, d)- 
bounded  in  the  range.  By  Lemma  3.6,  there  exist  wl,  w2, w3,  and  w4  in  X~ 

such  that  (S,  S)=0.  (w~Aw2, w3Aw4).  Let  m  =  [WaW, l.  Since  A  covers  X in 
the  domain,  there  exist  w5 and  w6 in  Xk* such that  (A, A) ==~ (w~, w6), and  for 
all  a  ~  Z,  ws  has  more  than  m  +  d  occurrences  of  a.  However,  since  X  is 

(A,  d)-bounded  in  the  range,  there  is  some  b  ~  X  such  that  Wn has  no  more 
than  d  occurrences  of  b.  But  (wlwsw2,  w3w6w4) would  be  a  member  of  T k 

under  these  circumstances,  although  WlWsW 2 has  more  than  m  +  d  occur- 
rences  of b  and  w3w6w4 has  no  more  than  m  +  d  occurrences  of b. 

By  contradiction,  we  see  that  if  X  is  covered  by  A  in  the  domain,  then 
it  is  also  covered  by  A  in  the  range.  The  converse  is proved  by  a  symmetric 
argument.  D 

As  a  consequence  of Lemma  3.10,  we  are  entitled  to  say that  A  covers  E 
without  mentioning  domain  or  range. 

LEMMA 3.11 

Let  A  cover  Xk"  Then  there  is  a  rule  A  ~  B 1 . . .   Bin,  C1 .. .  Cm  in  R, 
and  sets  19 ~ ,... ,  (R)m,  whose  union  is  Xk,  such  that  Bi  covers  (R)t,  for  1 ~  i 

~m . 

Proof  Let do be the  largest finite  integer  such that  for  some  X ~  Xk and 
Bi,  1 <  i _< m,  X is  (B~, d0)-bounded  but  not  (B~, do  --  1)-bounded.  Clearly, 

do exists.  Define  d~ =  do(k  --  1)  +  1.  There  must  exist  strings x  and y  in  X~ 

such  that  (A, A) ==~ (x, y),  and  for  aU a  E  Xk,  x  and  y  each  have  at  least  d~ 
occurrences  of  a,  for  otherwise  Xk  would  be  (A, dl)-bounded. 

Let  the  first  step  of  the  derivation  (A, A) ==~ (x, y)  be  (A, A) ==~ 
(B~ . . .   Bin, C 1 " "   C~).  Since  T  is  assumed  to  be  of  order  k -   1,  we  have 

m ~  k  --  1. We can write x  =  x ~ . . .   xm so  that  (B~, Bi) =-~ (xt, yi) for some y~. 

248  THEORY OF TRANSLATION  CHAP. 3 
If a  is  an  arbitrary  element  of E k, it  is  not  possible  that  none  of x i has  more 
than  do  occurrences  of a,  because  then  x  would  have  no  more  than  do(k  --  1) 

=  dl  --  1 occurrences  of a.  Let  (R)t be  the  subset  of  #k  such  that  xt  has  more 
than  do  occurrences  of all  and  only  those  members  of Oi.  By  the  foregoing, 

O1  U  02  t,A . . .   U  O m =   #k"  We  claim  that  Bt  covers  Ot  for  each  i.  For  if 
not,  then  O~ is  (Be, d)-bounded  for  some  d  >  do.  By  our  choice  of do,  this  is impossible.  D 

DEFINITION 

Let  at,  aj,  and  at  be  distinct  members  of  #k.  We  say  that  a~  is  between 
a t and  al  if either 

(1)  i < j < l ,   or 
(2)  zte(i)  <  ~k(J)  <  z~k(l). 

Thus  a  symbol is formally between  two  others if it appears  physically between 
them  either  in  the  domain  or  range  of  Tk. 

LEMMA 3.12 

Let  A  cover  #k,  and  let  A  ---~ B ~ . . .   Bin,  C 1 " "   Cm  be  a  rule  satisfying 
Lemma  3.11.  If Bt  covers  {at}  and  also  covers  {a,},  and  at  is  between  a t  and 
a,,  then  Bt  covers  {a,},  and  for  no j  ~  i  does  Bj  cover  {a,}. 

P ro of  Let  us  suppose  that  r  <  t  <  s.  Suppose  that  Bj  covers  {a,}, j  ~  i. 
There  are  two  cases  to  consider,  depending  on  whether j  <  i  or j  >  i. 

Case  1:  j  <  i.  Since  in  the  underlying  grammar  of T,  Bt  derives  a  string g 
with  a,  in  it  and  Bj  derives  a  string  with  a,  in  it,  we  have  (A, A):=~  (x, y), 
where  x  has  an  instance  of  at  preceding  an  instance  of  a t.  Then  by  Lemma 

3.6,  there  exists  such  a  sentence  in  the  domain  of  Tk, which  we  know  not  to be  the  case. 

Case  2:  j  >  i.  Allow  Be  to  derive  a  sentence  with  a,  in  it,  and  we  can 
similarly find  a  sentence  in the  domain  of  T k with  a,  preceding  a,. 

By  contradiction,  we  rule  out  the  possibility  that  r  <  t  <  s.  The  only 
other  possibility,  that  nk(r)  <  nk(t)  <  nk(S),  is  handled  similarly,  reasoning 
about  the  range  of  T k.  Thus  no  Bj, j  ~  i,  covers  {at}. If Bj  covers  #,  where 
a,  e  #,  then  B~ certainly  covers  {a,}. Thus  by  Lemma  3.11,  Bt  covers  some 

set  containing  a,,  and  hence  covers  {a,}.  D 

LEMMA 3.13 

If A  covers  Ek,  k  >  4,  then  there  is  some  rule  A  --~  B ~ . . .   Bin,  C ~ . . .   C~ 
and  some  i,  1 ~  i ~  m,  such  that  Bg covers  #k' 

Proof.  We  shall  do  the  case in which  k  is even.  The  case  of odd  k  is  simi- 
lar  and  will be left for the  Exercises.  Let  A  ~  B ~ . . .   Bin,  C x "'"  Cm  be  a  rule 

satisfying  Lemma  3.11.  Since  m  ~  k  --  1 by  hypothesis  about  T,  there  must 

SEC. 3.2  PROPERTIES  OF  SYNTAX-DIRECTED TRANSLATIONS  249 

be  some  Bt  which  covers  two  members  of  Ek,  say  Bt  covers  [a,,  as},  r  ~  s. 

Hence,  Bt covers {at} and  [as},  and  by Lemma  3.12, if at is between ar and  as, 
then  Bt  covers  (at}  and  no  C i, j  ~  i,  covers  {at}. 

If  we  consider  the  range  of  Tk,  we  see  that,  should  Bt  cover  (ak/z]  and 
{akin+ ~},  then  it  covers  (a}  for  all  a  ~  E~,  and  no  other  Bj  covers  any  {a}. 
It  will  follow  by  Lemma  3.11  that  Bt  covers  Ek.  Reasoning  further,  if  Bt 
covers  [am} and  {a,},  where  m  <  k/2  and  n  >  k/2,  then  consideration  of 
the  domain  assures  us  that  B~ covers  {ak/2}  and  (ak/2+ ~}. 

Thus,  if one of r and s is equal to or less than k/2, while the other is greater 
than  k/2,  the  desired  result  is  immediate. 

The  other  cases  are  that  r  <  k/2  and  s  <  k/2  or  r  >  k/2,  s  >  k/2.  But  in 
the  range,  any  distinct  r  and  s,  both  equal  to  or  less  than  k/2,  have  some  at, 

t  >  k/2,  between  them.  Likewise,  if  r  >  k/2  and  s  >  k/2,  we  find  some  at, 

t  <  k/2,  between  them.  The  lemma  thus  follows  in  any  case.  [~] 

LEMMA 3.14 

Tk is  in  3k  --  3e-~,  for  k  >  4. 
Proof.  Clearly,  T k  is  in  ~3 k.  It  suffices  to  show  that  T,  the  hypothetical 
SDTS  of order  k  --  1, does  not  exist.  Since  S  certainly covers  Zk,  by  Lemma 

3.13  we  can  find  a  sequence  of  nonterminals  A0, A 1 , . . . ,   A#N  in  N,  where 
A 0 =  S  and  for 0  <  i  <  ~ N ,   there  is a  rule A t ~  stAr+lilt,  ?tAt+Id;r  More- 

over,  for  all  i,  At  covers  Ek.  Not  all  the  A's  can  be  distinct,  so  we  can  find 

i and j,  with  i  <  j  and  A t =  Aj.  By  Lemma  3.6,  we  can  find  w t , . . . ,   w~o  so 
that  for  all p  ~  0, 

(S, S)  --->  (w~A~w~, w3A,w~) 

(w~w~A,w6w2, W3WTA:sW4) 

(w ~ w~A,w~w2, w3w~A~wfw,) 

"  "  wcw~ w~w,). (WlWsW9W6W2~ W3  0 

By  Lemma  3.9(1)  we  can  assume  that  not  all  of  ~t,  fit,  ~'t, and  Jt  are  e,  and 
by  Lemma  3.9(2)  that  not  all  of  ws,  w6,  WT, and  ws  are  e. 

For each a  ~  Ek, it must be that  wsw6  and  w7w8 have the  same number  of 
occurrences  of  a,  or  else  there  would  be  a  pair  in  z(T)  not  in  T k.  Since  A t 
covers  Ek,  should  ws, w6, w7, and  w8 have  any  symbol  but  ax or a~, we could 
easily choose  w9 to  obtain  a  pair  not  in  Tk.  Hence  there  is  an  occurrence  of 
a t or  a k  in  w 7 or  w8.  Since  A t covers  E~  again,  we  could  choose  wl0  to  yield 
a  pair  not  in  T k.  We  conclude  that  T  does  not  exist,  and  that  T k  is  not  in 

250  THEORY OF  TRANSLATION  CHAP.  3 
THEOREM 3.8 

With  the  exception  of k  =  2,  3k  is  properly  contained  in  ~3k/ 1 for  k  .~  1. 
Proof.  The  case  k  =  1  is  L emm a  3.5.  The  other  cases  are  Lemma  3.14. 

D 

An  interesting  practical  consequence  of T heo rem   3.8  is  that  while  it  may 
be  attractive  to  build  a  compiler  writing  system  that  assumes  the  underlying 
g ram ma r  to  be  in  Chomsky  normal  form,  such  a  system  is  not  capable  of 
performing  any  syntax-directed  translation  of  which  a  more  general  system 

is  capable.  However,  it  is  likely  that  a  practically  motivated  SDT  would  at 
worst  be  in  ~33 (and  hence  in  ~3z). 

E X E R C I S E S  
"3.2.1.  Let  T  be  a  SDT.  Show  that  there  is  a  constant  c  such  that  for  each  x  in 

the  domain  of  T,  there  exists y  such  that  (x, y)  6  T  and  [y I<_  c([ x[  /  1). 

*3.2.2  (a)  Show  that  if  T1  is  a  regular  translation  and  Tz  is  an  SDT,  then 

Ti  o T2  =  {(x,  z)[ for some y,  (x, y)  ~  T1 and (y,  z)  ~  T2} is  an  SDT.I" 
(b)  Show  that  T1  o T2  is  simple  if T2  is. 

3.2.3  (a)  Show  that  if T is  an  SDT,  then  T-~  is  an  SDT. 

(b)  Show  that  T-1  is  simple  if T is. 

*3.2.4  (a)  Let  T~  be  a  regular  translation  and  T2  an  SDT.  Show  that  T2  o T~ 

is  an  SDT. 
(b)  Show  that  T2  o T~  is simple  if  T2  is. 

3.2.5.  Give  strong  characterizing  languages  for 

(a)  The  SDT  Example  3.5. 
(b)  The  SDT  of Example  3.7. 
(c)  The  SDT  of Example  3.12 

3.2.6.  Give  characterizing  languages  for  the  SDT's  of  Exercise  3.2.5  which  do 

not  strongly  characterize  them. 

3.2.7.  Complete  the proof of Lemma  3.4. 
3.2.8.  Complete  case  2  of Example  3.17. 
3.2.9.  Show  that  every simple  SDT  is  defined by  a  simple  SDTS  with  no  useless 

nonterminals. 

3.2.10.  Let  T~  be  a  simple  SDT  and  Tz  a  regular  translation.  Is  T1  ~  T2  always 

a  simple  SDT ? 

3.2.11.  Prove  Lemma  3.6. 

tOften,  this operation on translations,  called composition, is written with the operands in  the  opposite  order.  That  is,  our  definition  above  would  be  for  Tz  o T1,  not  T1 o T2. 
We  shall  not  change to  the  definition  given here, for  the sake  of natural  appearance. 

SEC. 3.3  LEX~CAL ANALYSIS  251 
3.2.12.  Prove  Lemma 3.9. 

3.2.13.  Give  an  SDTS  of order  k  for  Tk. 
3.2.14.  Let  T  =  (N, ~,  ~,  R, S),  where N  =  {S, A, B, C, D}, :E  =  {a, b, c, d},  and 

R  has  the rules 

A  ~  aA,  aA 
A--->  e,  e 

B-  ~  bB,  bB 

B----~  e,  e 

C  -----~  cC,  cC 

C-. -->  e,  e 
D  -  > dD,  dD  

D----->  e,  e 

and  one  other  rule.  Give  the  minimum  order  of  "r(T)  if  that  additional 
rule is (a) 

S  ---~ AB CD ,   A BC D.  (b) 
S  --.   AB CD ,   BC DA.  (c) 
S  ~  AB CD ,   D BC A.  (d) 
S  ~  A BC D,   BD AC.  

3.2.15.  Show  that  if T is defined by a  DPDT,  then  T is strongly  characterized by 

a  deterministic context-free language. 

3.2.16.  Is the  converse  of Exercise  3.2.15  true? 
3.2.17.  Prove  the  corollaries  to  Theorems  3.3  and  3.4. 

B I B L I O G R A P H I C   N O T E S  
The  concept  of a  characterizing  language  and  the  results  of  Sections  3.2.1 and 3.2.2  are  from  Aho  and  Ullman  [1969b].  The  results  of  Section  3.2.3  are  from 
Aho  and  Ultman  [1969a]. 

3.3.  LEXlCAL  ANALYSIS 

Lexical  analysis  is  the  first  phase  of the  compiling  process.  In  this  phase, 
characters  from  the  source  program  are  read  and  collected  into  single logical 
items  called  tokens.  Lexical  analysis  is  important  in  compilation  for  several 

reasons.  Perhaps  most  significant,  replacing  identifiers  and  constants  in 

a  program  by  single  tokens  makes  the  representation  of  a  program  much 
more  convenient  for  later  processing.  Lexical  analysis  further  reduces  the 
length  of  the  representation  of  the  program  by  removing  irrelevant  blanks 

252  THEORY  OF  TRANSLATION  CHAP.  3 
and  comments  from  the  representation  of the  source  program.  During  sub- 

sequent  stages  of  compilation,  the  compiler  may  make  several  passes  over 
the internal representation  of the program.  Consequently,  reducing the length 

of this  representation  by  lexical  analysis  can  reduce  the  overall  compilation time. 

In many situations the constructs we choose to isolate as tokens are  some- 
what arbitrary.  For example,  if a  language  allows complex  number  constants of the  form 

<complex  constant>  (<real>,  <real>) 
then  two  strategies  are  possible.  We  can  treat  <real>  as  a  lexical  item  and 
defer  recognition  of the  construct  (<real>,  <real>)  as  complex  constant  until 
syntactic analysis. Alternatively, utilizing a more complicated lexical analyzer, 
we  might  recognize  the  construct  (<real>,  <real>)  as  a  complex  constant  at 
the  lexical  level and  pass  the  token  identifier  to  the  syntax analyzer.  It is also 
important  to  note  that  the  variations  in  the  terminal  character  set  local  to 

one  computer  center  can  be  confined  to  the  lexical  level. 

Much  of the  activity  that  occurs  during  lexical  analysis  can  be  modeled 
by  finite  transducers  acting  in  series  or  parallel.  As  an  example,  we  might 
have  a  series  of finite  transducers  constituting  the  lexical  analyzer.  The  first 
transducer  in  this  chain  might  remove  all  irrelevant  blanks  from  the  source 
program,  the  second might suppress  all comments,  the third might search for 
constants,  and  so forth.  Another  possibility  might  be  to  have  a  collection  of 
finite  transducers,  one  of  which  would  be  activated  to  look  for  a  certain lexical  construct. 

In  this  section  we  shall  discuss  techniques  which  can  be  used  in  the  con- 
struction  of  efficient  lexical  analyzers.  As  mentioned  in  Section  1.2.1,  there 
are  essentially  two  kinds  of lexical  analyzers--direct  and  indirect.  We  shall 
discuss  how  to  design  both  from  the  regular  expressions  that  describe  the tokens  involved. 

3.3.1.  An  Extended  Language for  Regular Expressions 

The  sets  of allowable  character  strings  that  form  the  identifiers  and  other 
tokens  of  programming  languages  are  almost  invariably  regular  sets.  For 
example,  F OR TR AN   identifiers  are  described  by  "from  one  to  six letters  or 

digits,  beginning with  a  letter."  This  set is clearly regular  and  has  the  regular expression 

(A  + . . .   +  Z)(e  +  (/t  + . . . - t -   Z-t-  0  + . . .   /  9) 

(e  +  (A  +  . . .   +  Z  +  0  +  .. .   +  9)(e  +  (A  /  . . .   +  Z  +  0  +  . . .   +  9) 
(e  +  (A  -q- * * * +  Z  +  0  + . . .   -t-  9)(e  --Jr- A  +  . . .   +  Z  +  0  +  . . .   +  9))))) 

SEC. 3.3  LEXICAL ANALYSIS  253 
Since  the  above  expression  is  cumbersome,  it  would  be  wise  to  introduce 
extended  regular  expressions  that  would  describe  this  and  other  regular 
expressions  of practical  interest  conveniently. 

DEFINITION 

An  extended  regular  expression  and  the  regular  set  it  denotes  are  defined 
recursively  as  follows" 

(1)  If R  is  a  regular  expression,  then  it  is  an  extended  regular  expression 
and  denotes  itself.'l 

(2)  If R  is  an  extended  regular  expression,  then 

(a)  R / is  an  extended  regular  expression  and  denotes  RR*. 
(b)  R *~ is  an  extended  regular  expression,  and  denotes 

[e}  U  R  U  R R   t,.) . . .   U  R". 
(c)  R /~ is  an  extended  regular  expression  and  denotes 

R  t,A R R   U  . . .   U  R". 
(3)  If  R I  and  R 2  are  extended  regular  expressions,  then  R1  ~  R 2  and 
R~ -- R 2 are  extended  regular expressions  and  denote  {x] x  e  R~  and  x  ~  R2} 
and  { x l x   ~  R~  and  x  q~ R2],  respectively. 

(4)  Nothing  else  is  an  extended  regular  expression. 

CONVENTION 

We  shall  use  liD  extended  regular  expressions  in  place  of  the  binary  + 
operator  (union)  to  make  the  distinction  between  the  latter  operator  and 
the  unary  / and  /" operators  more  apparent. 

Another  useful  facility  when  defining  regular  sets  is  the  ability  to  give 
names  to  regular  sets.  We  must  be  careful  not  to  make  such  definitions  cir- 
cular,  or  we  have  essentially  a  system  of  defining  equations,  similar  to  that 
in  Section  2.6,  capable  of  defining  any  context-free  language.  There  is,  in 
principle,  nothing  wrong  with  using the  power  of defining  equations  to  make 

our  definitions  of  tokens  (or  using  a  pushdown  transducer  to  recognize 
tokens).  However,  as  a  general  rule,  the  lexical analyzer  has  simple  structure, 
normally that  of a finite automaton.  Thus  we prefer to  use a definition  mecha- 
nism  that  can  define  only  regular  sets  and  from  which  finite  transducers  can 
be  readily  constructed.  The  inherently  context-free  portions  of  a  language 
are  analyzed  during  the  parsing  phase,  which  is  considerably  more  complex 
than  the  lexical  phase. 

DEFINITION 

A  sequence  o f   regular  definitions  over  alphabet  X  is  a  list  of  definitions 
A 1 =  Ri,  A 2 =  R 2 , . . . ,   A,  =  R,,  where  A ~ , . . . ,   A,  are  distinct  symbols 

* l'Recall that we do not distinguish between a regular expression and the set it denotes if the distinction is clear. 

254  THEORY  OF  TRANSLATION  CHAP.  3 
not  in  Z  and  for  1 <  i <  n,  Rt  is  an  extended  regular  expression  over 

u  {A 1 ,.. .,  A~_ 1}. We define R'~, for  1 <  i _~ n,  an  extended  regular  expres- 
sion  over  E,  recursively  as  follows" 

(1)  R'~  =  R~. 
(2)  R~ is  R,  with  R'  substituted  for  each  instance  of A j,  1 < j   <  i. 

The  set  denoted  by  A~ is  the  set denoted  by  R't. 

It  should  be  clear  that  the  sets  denoted  by  extended  regular  expressions 
and  sequences  of  regular  definitions  are  regular.  A  proof  is  requested  in 
the  Exercises. 

Example 3.18 

We  can  specify  the  F O R TR A N   identifiers  by  the  following  sequence  of 
regular  definitions" 

(letter)  =  A I B I . ..   I Z 

(digit)  =  0 1 1 [ . - . t 9  
(identifier)  =  (letter)((letter)  I (digit)) *s 

If we  did  not  wish  to  allow  the  keywords  of  F O RT R A N   to  be  used  as 
identifiers,  then  we could revise the  definition  of (identifier)  to  exclude  those 

strings.  Then  the  last  definition  should  read 

(identifier)  =  ((letter)((letter)l(digit)) .5)  --  (DO I I F ] . . . )   V--] 
Example 3.19 

Wecan define the usual real constants such as 3.14159, --682, or 6.6E -- 29, 
by  the  following  sequence  of regular  definitionst" 

(digit)  =  0l  1 [. ..  19 

(sign)  =  -% I -  [ e 
(integer)  =  (sign) (digit) + 
(decimal)  =  (sign)((digit)*.  (digit)+  I (digit) + * (digit)*) 
(constant)  =  (integer) l (decimal)  [(decimal )E(integer) 

3.3.2.  Indirect  Lexical  Analysis 

In indirect lexicat analysis, we are expected to determine,  scanning a string 
of  characters,  whether  a  substring  forming  a  particular  token  appears.  If 
the  set  of possible  strings  of characters  which  can  form  this  token  is denoted 

by a  regular  set,  as it usually can be,  then the problem  of building an indirect 

lexical  analyzer  for  this  token  can  be  thought  of as  a  problem  in  the  imple- 

~A specific implementation of a language would usually impose a  restriction on the length of a constant. 

SEC. 3.3  LEXICAL ANALYSIS  255 
mentation  of  a  finite  transducer.  The  finite  transducer  is  almost  a  finite 

automaton  in  that  it  looks  at  the  input  without  producing  any  output  until 
it  has  determined  that  a  token  of  the  given  type  is  present  (i.e.,  reaches 

a  final  state).  It  then  signals  that  this  token  has  appeared,  and  the  output  is 

the  string  of symbols  constituting  the  token. 

Obviously,  the  final  state  is  itself  an  indication.  However,  a  lexical  ana- 
lyzer  may  have  to  examine  one  or  more  symbols  beyond  the  right  end  of 

the  token.  A  simple  example  is  that  we  cannot  determine  the  right  end  of 

an  AL GO L  identifier  until  we  encounter  a  symbol  that  is neither  a  letter  nor 

a  digit--symbols  normally  not  considered  part  of  the  identifier. 

In indirect lexical analysis it is possible to accept an output from the lexical 
analyzer which says that a certain token might appear,  and if we later discover 

that  this  token  does  not  appear,  then  backtracking  of the  parsing  algorithm 

will  ensure  that  the  analyzer  for  the  correct  token  is  eventually  set  to  work 

on  the  same  string.  Using  indirect  lexical analysis we must  be  careful  that  we 
do  not perform  any erroneous  bookkeeping  operations.  Normally,  we should 

not  enter  an  identifier  in  the  symbol  table  until  we  are  sure  that  it  is  a  valid 
identifier.  (Alternatively,  we  can  provide  a  mechanism  for  deleting  entries 

from  tables.) 

The  problem  of indirect  lexical  analysis  is thus  essentially  the  problem  of 
constructing  a  deterministic  finite  automaton  from  a  regular  expression  and 
its  implementation  in  software.  The  results  of  Chapter  2  convince  us  that 

the  construction  is  possible,  although  much  work  is  involved.  It  turns  out 

that  it is not  hard  to  go  directly from  a  regular  expression  to  a  nondetermin- 
istic  finite  automaton.  We  can  then  use  Theorem  2.3  to  convert  to  a  deter- 
ministic  one  or  we  can  simulate  the  nondeterministic  finite  automaton  by 

keeping  track  of  all  possible  move  sequences  in  parallel.  In  direct  lexical 
analysis as well, it is convenient  to begin the  design  of a  direct lexical analyzer 
with  concise  nondeterministic  finite  automata  for  each  of  the  tokens. 

The nondeterministic  finite automaton  can be constructed  by an algorithm 
similar  to  the  one  by  which  right-linear  grammars  were  constructed  from 
regular  expressions  in  Section  2.2.  It  is  rather  tricky  to  extend  the  construc- 

tion  of  nondeterministic  automata  to  all  the  extended  regular  expressions 

directly,  especially  since  the  n  and  --  operations  imply  constructions  on 

deterministic  automata.  (It  is  very  difficult  to  prove  that  R~  n  R2  or 
R~  --  R~  are  regular  if R~ and  R z are  defined  by  nondeterministic  automata 
without  somehow  making  reference  to  deterministic  automata.  On  the  other 

hand,  proofs  of closure  under  u , . ,   and  *  need  no  reference  to  deterministic 

automata.)  However,  the  operators  /,  /~, and  *" are  handled  naturally. 

ALGORITHM 3.2 

Construction  of  a  nondeterministic  finite  automaton  from  an  extended 
regular  expression. 

256  THEORY  OF TRANSLATION  CHAP.  3 

Input.  An  extended  regular  expression  R  over  alphabet  X,  with  no 
instance  of  symbol  Z~  or  operator  n  o r - - .  

Output.  A  nondeterministic  finite  automaton  M  such  that  T ( M ) =   R. 

Method. 

(I)  Execute  step  (2)  recursively,  beginning  with  expression  R.  Let  M  be 
the  automaton  constructed  by  the  first  call  of that  step. (2)  Let R o be the extended  regular  expression  to which  this  step is applied. 

A  nondeterministic  finite  automaton  Mo  is  constructed.  Several  cases  occur" 

(a)  Ro is  the  symbol  e.  Let  Mo  =  ([q},  E,  ~ ,   q, {q}); where  q  is  a  new 

symbol. (b)  R o  is  symbol  a  in  X.  Let  Mo  =  ({ql,  q2],  Z,  ~o,  ql,  {q2]),  where 

~o(ql,  a)  =  {q2} and  ~o is  undefined  otherwise;  q i and  q2  are  new 

symbols. (c)  R o  is  R i[R z.  Then  we  can  apply  step  (2)  to  R1  and  R z 

to  yield  M1  :  (Q1,  X, ~ I, ql,  F1)  and  M2  =  (Q2,  X,  ~2,  q2, Fz), respectively,  where  Q1  and  Q2  are  disjoint.  Construct  Mo  = 

(01  u  Q2  t..j {q0}, X,  ~o,  qo, Fo),  where (i)  qo  is  a  new  symbol. 

(ii)  c5 o  includes  ~ 1 and  ~2,  and  ~o(qo,  a)  =  ~l(q 1, a)  U  ~2(q2, a). (iii)  Fo  is  F1  U  F 2  if  neither  q l  ~  F1  nor  q2  ~  F2,  and  Fo  : 

F1  U  F2  U  {qo} otherwise. (d)  Ro  is  R1R 2.  Apply  step  (2)  to  R 1 and  R2  to  yield  M1  and  M2  as 
in  case  (c).  Construct  Mo  =  (Q1  u  Q2,  X,  ~o,  q 1, Fo),  where 

(i)  ~o  includes  ~2;  for  all  q  ~  Q,  and  a  ~  X,  ~o(q,  a)  =  ~l(q,  a) if  q  ~  F,  and  ~5o( q,  a)  =  c5 l(q,  a)  u  ~2(q2,  a)  otherwise. 

(ii)  Fo  =  F2  if q2  is  not  in  F2  and  Fo  =  F1  U  F2  otherwise. 
(e)  Ro  is  Rt*. Apply  step  (2)  to  R1  to  yield  M1  =  (Q1,  X,  ~51, ql,  El). 

Construct  Mo  =  (Q1 u  {qo],  E,  ~o,  qo, F1  U  {qo}),  where  qo  is 
a  new  symbol,  and  ~o  is  defined  by 

(i)  ~o(qo,  a)  =  ~l(q 1, a). (ii)  If q  c= El,  then  ~o(q,  a)  :  ~ l(q,  a). 

(iii)  If q  ~  F,  then  C5o( q,  a)  =  ~( q,   a)  U  c51(ql, a). (f)  Ro  is  Rt.  Apply  step  (2)  to  R 1 to  yield  M1  as  in  (e).  Construct 
Mo  =  (Q1, X, ~o,  ql,  Ft),  where  ~o(q,  a)  =  ~l(q,  a)  if q  6  F1  and 
~o(q,  a)  =  ~ l(q,  a)  u  ~ l(q 1, a)  if q  ~  El. (g)  Ro  is  R~*". Apply  step  (2)  to  R1  to  yield  M1  as  in  (e).  Construct 

Mo  :  (Qi  *  { 1 , . . . ,   n],  X, ~o,  [q l,  1], Fo),  where (i)  Ifq  ~  F 1 or i = n ,   then~o([q,i],a):{[p,i]l~5~(q,a)containsp}. 

(ii)  If  q  ~  F1  and  i <  n,  then  ~o([q,  i],  a)  = 

([P, i]1~ ~(q, a) contains p} U  {[p,  i +  1][~ ~(q~, a) contains p}. 

(iii)  Fo  =  {[q,  i][q  ~  F,,  1 <  i ~  n]  L) [[q,,  1]}. 

SEC. 3.3  LEXICAL ANALYSIS  257 

(h)  R0 is R~ -~. Do  the  same  as  in  step  (g),  but  in part  (iii) F0  is defined 

as  [[q,  i] l q  ~  El,  1 ~  i  <  n]  instead. 

THEOREM 3.9 

Algorithm  3.2  yields  a  nondeterministic  finite  automaton  M  such  that T ( M )   =  R. 

Proof.  Inductive  exercise. 

We  comment  that  in  parts  (g)  and  (h)  of Algorithm  3.2  the  second  com- 
ponent  of  the  state  of  M0  can  be  implemented  efficiently  in  software  as  a 
counter,  in  many  cases,  even  when  the  automaton  is  converted  to  a  deter- 
ministic  version.  This  is  so because  in  many  cases  R 1 has  the  prefix  property, 
and  a  word  in  Rt n can  be  broken  into  words  in  R x trivially.  For  example, 
R~  might  be  (digit)  as  in  Example  3.18,  and  all  members  of  (digit)  are  of 
length  1. 

Example 3.20 

Let  us  develop  a  nondeterministic  automaton  for  the  identifiers  defined 
in  Example  3.18.  To  apply  step  (2) of Algorithm  3.2 to  the  expression  named 
(identifier),  we  must  apply  it  to  (letter)  and  ((letter)]  (digit)) *s.  The  con- 

struction  for  the  former  actually  involves  26  applications  of step  (2b)  and  25 
of  step  (2c).  However,  the  result  is  seen  to  be  ({q l,  q2],  X,  ~1,  q x, {q2}),  if 
obvious  state  identifications  are  applied, t  X  =  [ A , . . .   ,Z,  0 , . . . ,   9],  and 
6~(q,,  A)  =  6,(q,,  B)  . . . . .   6,(q,,  Z )   =  [q2}. 

To  obtain  an  automaton  for  ((letter)l(digit)) .5,  we  need  another 
automaton  for  (letter),  say  ([q3, q4}, X, ~2,  q3, [q4}),  and  the  obvious  one for 
(digit),  say  ({qs, q6}, X, 63,  qs,  {q6}).  To  take  the  union  of  these,  we  add 
a  new  initial  state,  qT, and  find  that  q3 and  q5 cannot  be  reached  therefrom. 

Moreover,  q4  and  q6  can  clearly  be  identified.  The  resulting  machine  is 
([q4,  qT}, X, 64,  q7, {q4}), where 

64(q7, A)  . . . . .   ~4(q7, Z)  =  t~,(q7,  0)  . . . . .   ~,(qT,  9)  =  [q,}. 
To  apply  case  (g),  we  construct  states  [q,,  i]  and  [qT, i],  for  1 ~  i ~  5. 
The  final  states  are  [q4, i],  1 _~  i ~  5,  and  [qT, 1].  The  last  is  also  the  initial 

state.  We  have  a  machine  (Qs,  X, 85,  [qT, 1], Fs),  where  F5  is  as  above,  and 
t~([q7 ,  1], a)  =  {[q4,  1]};  O([q4,  i],  a)  :  {[q4,  i  +-  1]}, for  all  a  in  X  and  i :  

tThat is, two states of a nondeterministic  finite automaton can be identified if both are final or both are nonfinal and on each input they transfer to the same set of states.  There 
are other conditions under which two states of a nondeterministic  finite automaton can be identified, but  this condition is all that is needed here. 

258  THEORY  OF TRANSLATION  CHAP.  3 

1,  2,  3,  4.  Thus  states  [qT, 2],  . . . ,   [qT,  5]  are  not  accessible  and  do  not  have 
to  appear  in  Qs.  Hence,  Q5  =  Fs. 

To  obtain  the  final  automaton  for  (identifier>  we  use  case  (d).  The  result- 

ing  automaton  is 

M  =  ([ql,  q2,  [q,,  1 ] , . . . ,   [q4,  5]}, Z,  5,  ql,  [q2,  [q,,  1 ] , . . . ,   [q4,  5]}), 
where  ~5 is  defined  by 

(1)  c~(ql,  ~)  =  [q2}  for  all  letters  ix. 
(2)  c~(q2, 00  =  [[q,,  1]} for  all  0c in  Z. 
(3)  c~([q,,  i],  ~)  =  [[q,,  i  -1-  1]} for  all  ~  in  Z  and  1 _~  i  <  5. 

Note  that  [q7,  1]  is  inaccessible  and  has  been  removed  from  M.  Also,  M  is 
deterministic  here,  although  it  need  not  be  in  general. 

The  transition  graph  for  this  machine  is  shown  in  Fig.  3.7.  [ZI 

Start 

A~  .  .  .  ~ Z~ ~ 0  . . . . .   9 

A , . . . , Z ,  0 . . . . .   9 
Fig. 3.7  Nondeterministic finite automaton for identifiers. 
3.3.3.  Direct  Lexical  Analysis 

When  the  lexical  analysis  is  direct,  one  must  search  for  one  of  a  large 
number  of  tokens.  The  most  efficient  way  is  generally  to  search  for  these  in 
parallel,  since  the  search  often  narrows  quite  quickly.  Thus  the  model  of 

a  direct  lexical  analyzer  is  many  finite  automata  operating  in  parallel,  or  to 
be  exact,  one  finite  transducer  simulating  many  automata  and  emitting  a  sig- 
nal  as  to  which  of  the  automata  has  successfully  recognized  a  string. 

If we  have  a  set  of nondeterministic  finite  automata  to  simulate  in  parallel 
and  their  state  sets  are  disjoint,  we  can  merge  the  state  sets  and  next  state 
functions  to  create  one  nondeterministic  finite  automaton,  which  may  be 

converted  to  a  deterministic  one  by  Theorem  2.3.  (The  only  nuance  is  that 

SEC. 3.3  LEXICAL ANALYSIS  259 
the  initial  state  of the  deterministic  automaton  is  the  set  of all  initial  states 
of the  components.)  Thus  it  is  more  convenient  to  merge  before  converting 
to  a  deterministic  device  than  the  other  way  round. 

The  combined  deterministic  automaton  can  be  considered  to  be  a  simple 
kind  of finite transducer.  It  emits the  token  name  and,  perhaps,  information 
that will locate the instance of the token.  Each  state of the combined automa- 
ton  represents  states  from  various  of the  component  automata.  Apparently, 
when  the  combined  automaton  enters  a  state  which  contains  a  final  state 

of one  of the  component  automata,  and  no  other  states,  it  should  stop  and 
emit the name of the token for that component automaton.  However, matters are  often  not  that  simple. 

For  example,  if  an  identifier  can  be  any  string  of  characters  except  for 
a  keyword,  it  does  not  make  for  good practice  to  define  an  identifier  by  the 
exact regular set, because it is complicated and requires many states.  Instead, 
one  uses  a  simple  definition  for  identifier  (Example  3.18  is  one  such)  and 
leaves  it  to  the  combined  automaton  to  make  the  right  decision. 

In this case,  should the combined automaton enter a  state which included 
a  final  state  for  one  of the  keyword  automata  and  a  state  of the  automaton 
for  identifiers  and  the  next  input  symbol  (perhaps  a  blank  or  special  sign) 
indicated  the  end  of the  token,  the  keyword  would  take  priority,  and  indi- 
cation  that  the  keyword was  found  would  be  emitted. 

Example 3.21 

Let  us  consider  a  somewhat  abstract  example.  Suppose  that  identifiers 
are  composed  of any  string of the  four  symbols  D,  F,  I,  and  O,  followed by 
a  blank (b), except for the  keywords DO  and IF,  which need not be followed 
by  a  blank,  but  may  not  be  followed  immediately  by  any  of the  letters  D, F,  I,  or  O. 

The  identifiers  are  recognized  by  the  finite  automaton  of  Fig.  3.8(a), 
DO  by that of Fig.  3.8(b), and IF by Fig.  3.8(c).  (All automata here are deter- 
ministic,  although  that  need  not  be  true  in  general,  of course.) 

The  merged  automaton  is  shown  in  Fig.  3.9.  State  q2  indicates  that  an 
identifier has been found. However, states [q 1, q8} and [q 1, qs} are ambiguous. 

They  might indicate  IF  or  DO,  respectively,  or  they  might just  indicate  the 
initial  portion  of  some  identifier,  such  as  DOOF.  To  resolve  the  conflict, 

the  lexical  analyzer  must  look  at  an  additional  character.  If a  D , O ,   I,  or  F 
follows, we had the prefix of an identifier.  If anything else, including a  blank, 

follows  (assume  that  there  are  more  characters  than  the  five mentioned),  we 
enter  new  states,  q9  or  q l0,  and  emit  a  signal  to  the  effect  that  DO  or  IF, 
respectively,  was  detected,  and  that  it  ends  one  symbol  previously.  If  we 
enter  q2, we emit a signal saying that an identifier has been found, ending one 

symbol previously. 

Since it is the  output  of the  device,  not the  state,  that  is important,  states 

260  THEORY OF TRANSLATION  CHAP.  3 

F,I,O 
Start 

(a)  Identifiers Start--   

(b)  DO  Q Start    r 

(c) IF 
Fig. 3.8  Automata for lexical analysis. 

Q 

q2, qg, and  ql0  can  be  identified  and,  in  fact,  will  have  no  representation  at 
all in  the implementation.  U 

3.3.4.  Software  Simulation  of  Finite  Transducers 

There  are  several  approaches  to  the  simulation  of  finite  automata  or 
transducers.  A  slow  but  compact  technique  is  to  encode  the  next move 
function  of the  device  and  execute  the  encoding  interpretively.  Since  lexical 
analysis is a  major portion  of the activity of a  translator,  this mode  of opera- 
tion  is frequently  too  slow to  be  acceptable.  However,  some  computers  have 

single instructions  that can recognize the kinds  of tokens with which we have 
been  dealing.  While  these  instructions  cannot  simulate  an  arbitrary  finite 

automaton,  they  work  very well when  tokens  are  either  keywords  or  identi- 
fiers. 

An  alternative  approach  is  to  make  a  piece  of  program  for  each  state. 
The function  of the program is to  determine  the next character  (a subroutine 
may be used to locate that character),  emit any output  required,  and transfer 

to  the  entry  of the  program  corresponding  to  the  next  state. 

An  important  design  question  is  the  proper  method  of  determining  the 
next  character.  If  the  next  state  function  for  the  current  state  were  such 

that most different next characters lead to different next states, there is proba- 
bly nothing  better  to  do  than  to  transfer  indirectly through  a  table  based  on 

the  next  character.  This  method  is  as  fast  as  any,  but  requires  a  table  whose 
size is  proportional  to  the  number  of different  characters. 

In  the  typical lexical analyzer,  there  will be  many  states  such  that  all but 

EXERCISES  261 
Start 

D 

not D, F, I, 0 
D,F,I,O 

D,F,I 

F,O 

D,I,O 

D,F,I,O 

notD, F,I,O 

D, 

Fig. 3.9  Combined lexical  analyzer.t 
very  few  next  characters  lead  to  the  same  state,  It  may  be  too  expensive  of 

space  to  allocate  a  full  table  for  each  such  state.  A  reasonable  compromise 
between  time  and  space  considerations,  for  many  states,  would  be  to  use 
binary  decisions  to  weed  out  those  few  characters  that  cause  a  transition  to an  unusual  state. 

E X E R C IS E S 
3.3.1.  Give  regular  expressions  for  the  following  extended  regular  expressions: (a)  (a+3b+3) .2. 

(b)  (alb)*  --  (ab)*. 
(c)  (aal bb) .4  ~  a(ab[ ba)+b. 

tUnlike Fig.  3.8(a),  Fig.  3.9 does  not permit the empty  string to  be  an identifier. 

262  THEORY OF TRANSLATION  CHAP.  3 

3.3.2.  Give  a  sequence  of regular  definitions  that  culminate  in  the  definition  of 

(a)  ALGOL  identifiers. 
(b)  PL/I  identifiers. 
(c)  Complex  constants  of  the  form  (~, fl),  where  0c  and  fl  are  real 

F OR T R AN  constants. 
(d)  Comments  in  PL/I. 

3.3.3.  Prove  Theorem  3.3. 
3.3.4.  Give indirect  lexica! analyzers  for  the  three regular  sets  of Exercise  3.3.2. 
3.3.5.  Give  a  direct  lexical  analyzer  that  distinguishes  among  the  following 

tokens" 

(1)  Identifiers  consisting  of  any  sequence  of  letters  and  digits,  with 
at  least  one letter  somewhere.  [An  exception  occurs  in  rule  (3).] 

(2)  Constants  as  in  Example  3.19. 
(3)  The  keywords  IF,  IN,  and  INTEGER,  which  are  not  to  be 
considered  identifiers. 

3.3.6.  Extend  the  notion  of  indistinguishable  states  (Section  2.3)  to  apply  to 

nondeterministic  finite  automata.  If  all  indistinguishable  states  are 
merged,  do  we  necessarily  get  a  minimum  state  nondeterministic  auto- 

maton ? 

**3.3.7.  Is  direct  lexical  analysis  for  F OR T RA N  easier  if the  source  program  is 

scanned  backward ? 

Research Problem 

3.3.8.  Give  an  algorithm  to  choose  an  implementation  for  direct  lexical  ana- 

lyzers.  Your  algorithm  should  be  able  to  accept  some  indication  of  the 

desired time-space trade off. You may not wish to implement the symbol- 
by-symbol  action of a finite automaton, but rather allow for the possibility 

of  other  actions.  For  example,  if  many  of  the  tokens  were  arithmetic 
signs  of  length  1,  and  these  had  to  be  separated  by  blanks,  as  in 

SNOBOL,  it  might  be  wise to  separate  out  these  tokens  from  others  as 
the  first  move  of  the  lexical  analyzer  by  checking  whether  the  second 
character  was  blank. 

Programming  Exercises 

3.3.9.  Construct  a  lexical  analyzer  for  one  of the programming  languages  given 

in  the  Appendix.  Give  consideration  to  how  the  lexical  analyzer  will 
recover  from  lexicai  errors,  particularly  misspellings. 

3.3.10.  Devise  a  programming  language  based  on  extended  regular  expressions. 

Construct  a  compiler  for  this  language.  The  object  language  program 
should  be  an  implementation  of  the  lexical  analyzer  described  by  the source program. 

SEC. 3.4  PARSING  263 

B I B L I O G R A P H I C   NOTES 
The AED  RWORD  (Read a  WORD)  system was the first major system to  use finite  state  machine  techniques  in  the  construction  of  lexical  analyzers.  Johnson 
et  al.  [1968] provide  an  overview of this system. 

An  algorithm  that  constructs  from  a  regular  expression  a  machine  language 
program that simulates a corresponding  nondeterministic finite automaton is given by Thompson  [1968]. This algorithm has been  used  as  a  pattern-matching  mecha- 

nism  in  a  powerful text-editing language called  QED. 

A  lexical analyzer  should  be  designed  to  cope  with  lexical errors  in  its  input. Some  examples  of lexical  errors  are 

(1)  Substitution  of an incorrect symbol for  a  correct  symbol in a  token. (2)  Insertion  of an extra symbol in a  token. 
(3)  Deletion of a  symbol from  a token. 
(4)  Transposition  of a  pair  of adjacent symbols in a  token. 

Freeman  [1964] and  Morgan  [1970] describe  techniques  which  can  be  used  to detect  and  recover from errors  of this nature.  The  Bibliographic  Notes  at  the  end 

of  Section  1.2  provide  additional  references  to  error  detection  and  recovery  in compiling. 

3.4.  PARSING 

The  second  phase  of  compiling  is  normally  that  of  parsing  or  syntax 
analysis.  In  this  section,  formal  definitions  of  two  common  types  of  parsing 
are  given,  and  their  capabilities  are  briefly  compared.  We  shall  also  discuss 
what  it  means  for  one  grammar  to  "cover"  another  grammar. 

3.4.1.  Definition  of  Parsing 

We  say that  a  sentence  w in L(G) for  some  CFG  G has  been  parsed  when 
we  know  one  (or  perhaps  all)  of its  derivation  trees.  In  a  translator,  this  tree 
may  be  "physically"  constructed  in  the  computer  memory,  but  it  is  more 
likely  that  its  representation  is  more  subtle.  One  can  deduce  the  parse  tree 
by watching  the  steps  taken  by  the  syntax  analyzer,  although  the  connection would  hardly  be  obvious  at  first. 

Fortunately,  most  compilers  parse  by  simulating  a  PDA  which  is  recog- 
nizing  the  input  either  top-down  or  bottom-up  (see  Section  2.5).  We  shall 
see that  the  ability of a  PDA  to  parse  top-down  is associated  with  the  ability 
of a  PDT  to  map  input  strings  to  their leftmost derivations.  Bottom-up  pars- 
ing  is  similarly associated  with  mapping  input  strings  to  the  reverse  of their 
rightmost  derivations.  We  shall  thus  treat  the  parsing  problem  as  that  of 
mapping  strings  to  either  leftmost  or  rightmost  derivations.  While  there  are 

many  other  parsing  strategies,  these  two  definitions  serve  as  the  significant benchmarks. 

264  THEORY  OF TRANSLATION  CHAP.  3 

Some other parsing  strategies  are  mentioned in various parts  of the book. 
In  the  Exercises  at  the  end  of Sections  3.4,  4.1,  and  5.1  we  shall  discuss  left- 
corner  parsing,  a  parsing  method  that  is  both  top-down  and  bottom-up  in 

nature.  In  Section  6.2.1  of  Chapter  6  we  shall  discuss  generalized  top-down 
and  bottom-up  parsing. 

DEFINITION 

Let  G  =  (N,  E, P,  S)  be  a  CFG,  and  suppose  that  the  productions  of P 
are  numbered  1, 2 , . . . ,   p.  Let  a  be  in  (N  U  X)*.  Then 

(1)  A  left parse  of a  is  a  sequence  of productions  used  in  a  leftmost  deri- 
vation  of a  from  S. 

(2)  A  right parse  of a  is  the  reverse  of a  sequence  of productions  used  in 
a  rightmost  derivation  of a  from  S  in  G. 

We  can  represent  these  parses  by  a  sequence  of  numbers  from  1  to  p. 

Example 3.22 

Consider the grammar  Go, where the productions  are numbered as shown: 

(1)  E  ~  E  +  T 
(2)  E---~  T 
(3)  T---~  T .   F (4) 

T ~   F 
(5)  F  ~  (E) 

(6)  F  ~  a 

The  left  parse  of  the  sentence  a ,   (a  +  a)  is  23465124646.  The  right  parse 

of a  *  (a  +  a)  is  64642641532. 

We  shall  use  an  extension  of  the  =~  notation  to describe  left  and  right 
parses. 

CONVENTION 

Let  G  =  (N,  X, P,  S)  be  a  CFG,  and  assume  that  the  productions  are 
numbered  from  I  to  p.  We  write  ~z ~=-~ ,8  if  a  ==~ fl  and  the  production lm 

applied  is  numbered  i.  Similarly,  we  write  0~ =>~ fl  if 0~ =~  fl  and  production rm 

i is  used.  We  extend  these  notations  by 

(1)  If a  ~'==~ fl  and  fl  "'==~ 7,  then  a  .... ==~ 7. 
(2)  If a  ==~"' fl  and  fl ==~"' t',  then  a  =~ ....  2,'. 

3.4.2,  Top-Down  Parsing 

In  this  section  we wish  to  examine  the  nature  of the  left-parsing  problem 
for  CFG's.  Let  rr  =  i 1 .--  i,  be  a  left  parse  of  a  sentence  w  in  L(G),  where 

G is  a  CFG.  Knowing n,  we can  construct  a  parse  tree  for  w in  the following 

"top-down"  manner.  We  begin  with  the  root  labeled  S.  Then  i l  gives  the 

sEc. 3.4  PARSING  265 
production  to  be  used  to  expand  S.  Suppose  that  i l  is  the  number  of  the 
production  S  ---~ X i .   * * Xk.  We then create k  descendants  of the node labeled 

S  and  label  these  descendants  X1, X2,..  *,  Xk.  If  X1,  X2,..  *,  X~_ 1 are  ter- 
minals,  then  the  first  i -   I  symbols  of  w  must  be  X 1 . "   Xi_l.  Production 

i2  must  then  be  of the  form  Xt  ---~ Y ~ ' "   Y~,  and  we  can  continue  building 
the  parse  tree  for  w  by  expanding  the  node  labeled  X~.  We  can  proceed  in 
this  fashion  and  construct  the  entire  parse  tree  for  w  corresponding  to 

the  left  parse  ~r. 

Now  suppose  that  we  are  given  a  C FG   G  =  (N,  E, P,  S)  in  which  the 
productions  are  numbered  from  1 through  p  and  a  string  w ~  E*  for  which 

we  wish  to  construct  a  left  parse.  One  way  of looking  at  this  problem  is that 
we  know  the  root  and  frontier  of  a  parse  tree  and  "all"  we  need  to  do  is 

fill  in  the  intermediate  nodes.  Left  parsing  suggests  that  we  attempt  to  fill  in 

the  parse  starting  from  the  root  and  then  working  left  to  right  toward  the 
frontier. 

It  is  quite  easy  to  show  that  there  is  a  simple  SDTS  which  maps  strings 
in  L(G)  to  all  their  left  (or  right,  if you  prefer)  parses.  We  shall  define  such 

an  SDTS  here,  although  we  prefer  to  examine  the  PDT  which  implements 
the  translation,  because  the  latter  gives  an  introduction  to  the  physical 

execution  of its  translation. 

DEFINITION 

Let  G  =  (N,  Z, P,  S)  be  a  CFG   in  which  the  productions  have  been 
numbered  from  1  to  p.  Define  T~,  or  Tt,  where  G  is  understood,  to  be  the 

SDTS  (N,  ~,  { 1 , . . .   ,p},  R,  S),  where  R  consists  of  rules  A----~ ~,  fl  such 
that  A  ~  ~  is production  i in P  and  fl is  i~',  where  ~'  is ~ with  the  terminals 
deleted. 

Example 3.23 

Let  G O be  the  usual  grammar  with  productions  numbered  as  in  Example 
3.22.  Then  T~ =  ([E,  T, F},  { + , . ,   (,),  a}, { 1 , . . . ,   6}, R,  E),  where  R  consists 
of 

E  ~ E +   T,  l E T  

E  ~  T,  2T 

T~  ~ T *   F,  3TF 

T  ,~ F,  4F 
r  ~  (E),  5E 

F  ~ a,  6 

The  pair  of derivation  trees  in  Fig.  3.10  shows  the  translation  defined  for a , ( a + a ) .   D 

The  following  theorem  is  left  for  the  Exercises. 

266  THEORY  OF  TRANSLATION  CHAP.  3 

E i 

L 

1  ! T  F !  ! 
F  a I 

a 

E 2/! 
4 / !   !NNNN~, !  1/E~T 

2/!  4/! / !   I 
4  F  6 ! 

6 
(a) input  (b) output 

Fig.  3.10  Translation  T~. 

THEOREM 3.10 

Let  G  =  (N,  X, P,  S)  be  a  CFG.  Then  Ti  =  [(w, zt) lS  ~==~ w}. 

Proof.  We  can  prove  by  induction  that  (A, A) =-~ (w, n)  if  and  only  if T~ 

G 
Using  a  construction  similar  to  that  in  Lemma  3.2,  we  can  construct  for 
any  grammar  G  a  nondeterministic  pushdown  transducer  that  acts  as  a  left 
parser  for  G. 

DEFINITION 

Let  G  =  (N,  X, P,  S)  be a CFG  in which the productions  have been num- 
bered  from  1 to p.  Let  M f   (or  Mz  when  G  is  understood)  be  the  nondeter- 

ministic pushdown transducer  ([q}, X, N  U  X, [ 1, 2 , . . . ,   p}, t~, q, S,  ~),  where 

is  defined  as  follows" 

(1)  t~(q, e, A)  contains  (q,  a,  i)  if the  ith  production  in P  is  A  --+  a. 
(2)  6(q,  a,  a)  =  {(q, e,  e)} for  all a  in  X. 

We  call  M~  the  left parser  for  G. 

With input  w, Mz simulates a  leftmost  derivation  of w from  S in  G.  Using 
rules  in  (1),  each  time  Mi  expands  a  nonterminal  on  top  of  the  pushdown 

list  according  to  a  production  in  P,  Mz  will  also  emit  the  number  of  that 

SEC. 3.4  PARSING  267 
production.  If  there  is  a  terminal  symbol  on  top  of  the  pushdown  list,  M~ 

will  use  a  rule  in  (2)  to  ensure  that  this  terminal  matches  the  current  input 
symbol.  Thus,  M~ can  produce  only  a  leftmost  derivation  for  w. 

THEOREM 3.11 

Let  G  =  (N,  ~, P,  S)  be  a  CFG.  Then  lr~(M~) =  [(w,  n)[S  ~=-~ w}. 
Proof  Another  elementary  inductive  exercise.  The  inductive  hypothesis this  time  is  that  (q,  w, A,  e ) ~   (q,  e, e, rt)  if and  only  if  A "==~ w.  E] 

Note  that  M~  is  almost,  but  not  quite,  the  PDT  that  one  obtains  by Lemma  3.2 from  the  SDTS  Tj o. 
Example  3.24 

Let  us  construct  a  left  parser  for  G 0.  Here 

where 

Mi  =  ({q}, E,  N  w  E, {1, 2 , . . . ,   6},  $,q,  E,  ~), 

6(q,  e,  E)  =  {(q,  E  .+  T,  1),  (q,  T, 2)~} 

0(q,  e,  T)  =  ((q,  T  *  F,  3), (q, F,  4)} 
$(q,  e, F)  =  {(q,  (E),  5),  (q,  a,  6)} 

O(q, b, b)  =  [(q,  e, e)}  for  all  b  in  Z 

With  the  input  string  a  +  a ,   a,  M~0  can  make  the  following  sequence  of moves,  among  others: 

(q,a  +  a , a , E , e )   (q, 

(q, 
(q, 

F-  (q, 
~-  (q, 

(q, 
(q, 
(q, 
(q, 
(q, 
(q, 
(q, 
~-  (q, 

a-k-  a , a , E +   T,  1) 
a  +  a , a , T +   T,  12) 
a  +  a ,   a , F +   T,  124) 
a  +  a , a , a   -4- T,  1246) 

+  a  ,  a,  +  T,  1246) 
a  ,  a,  T,  1246) 

a  *  a,  T *  F,  12463) 
a  *  a, F  *  F,  124634) 
a  *  a,  a  *  F,  1246346) 

*  a,  *  F,  1246346) 
a, F,  1246346) 
a,  a,  12463466) 

e, e,  12463466)  [~ 

268  THEORY OF TRANSLATION  CHAP.  3 

The  left parser  is in  general  a  nondeterministic  device.  To  use  it  in  prac- 
tice,  we  must  simulate  it  deterministically.  There  are  some  grammars,  such 

as  those  which  are  not  cycle-free,  for  which  a  complete  simulation  is impos- 

sible,  in  this  case  because  there  are  an  infinity  of left parses  for  some  words. 
Moreover,  the  natural  simulation,  which  we  shall  discuss  in  Chapter  4,  fails 
on  a  larger  class  of  grammars,  those  which  are  left-recursive.  An  essential 
requirement  for  doing  top-down  parsing  is that  left  recursion  be  eliminated. 

There is a natural class of grammars,  which we shall call LL (for scanning 
the  input  from the  !eft producing  a  !eft parse)  and  discuss in  Section  5.1,  for 

which  the  left  parser  can  be  made  deterministic  by  the  simple  expedient  of 
allowing  it  to  look  some  finite  number  of  symbols  ahead  on  the  input  and 

to  base  its  move  on  what  it  sees.  The  LL  grammars  are  those  which  can  be 
parsed  "in  a  natural  way"  by  a  deterministic  left  parser. 

There  is a  wider class  of grammars  for  which  there  is  some  DPDT  which 
can  implement  the  SDTS  Tz.  These  include  all  the  LL  grammars  and  some 

others  which  can  be  parsed  only in  an  "unnatural"  way,  i.e.,  those  in  which 
the  contents  of the  pushdown  list do  not reflect  successive  steps  of a  leftmost 

derivation, as does Mz. Such grammars are of only theoretical  interest, insofar 
as  top-down  parsing  is  concerned,  but  we  shall  treat  them  briefly in  Section 

3.4.4. 

3.4,3.  Bottom-Up  Parsing 

Let us now turn  our attention  to  the  right-parsing problem.  Consider the 
rightmost  derivation  of a  +  a  *  a  from  E  in  G O * 

E - - - ~ I E +   T 

, - 3 E + T , F  
- - - ~ 6 E +   T , a  
- - - - ~ 4 E + F , a  

~ 6 E + a , a  
~.z  T +   a , a  

. , - 4 F + a , a  
~ 6 a q - a . a  

Writing  in  reverse  the  sequence  of productions  used  in  this  derivation  gives 
us  the  right  parse  64264631  for  a  --t- a  *  a. 

In  general,  a  right  parse  for  a  string  w in  a  grammar  G  =  (N, E, P,  S)  is 
a  sequence  of  productions  which  can  be  used  to  reduce  w  to  the  sentence 

symbol  S.  Viewed  in  terms  of a  derivation  tree,  a  right  parse  for  a  sentence 
w represents  the  sequence  of handle  prunings  in which  a  derivation tree with 

sEc. 3.4  PARSING  269 
frontier  w is pruned  to  a  single  node  labeled  S.  In  effect,  this is equivalent  to 

starting  with  only the  frontier  of a  derivation  tree  for  w and  then  "filling in" 
the  derivation  tree  from  the  leaves  to  the  root.  Thus  the  term  "bottom-up" 

parsing  is  often  associated  with  the  generation  of a  right  parse. 

In  analogy  with  the  SDTS  Tz  which  maps  words  in  L(G)  to  their  left 
parses,  we  can  define  T,,  an  SDTS  which  maps  words  to  right  parses.  The 

translation  elements  have  terminals  deleted  and  the  production  numbers  at 

the  right  end.  We  leave  it for  the  Exercises  to  show  that  this  SDTS  correctly defines  the  desired  translation. 

As  for  top-down  parsing,  we  are  really  interested  in a  PDT  which  imple- 
ments  Tr.  We  shall  define  an  extended  PDT  in  analogy  with  the  extended PDA. 

DEFINITION 

An  extended  P DT   is  an  8-tuple  P  =  (Q,  1~, F,  A,  6, q0,  Z0, F),  where  all 
symbols  are  as  before  except  6,  which  is  a  map  from  a  finite  subset  of 

Q  x  (Z  u  {e}) x  F*  to  the  finite  subsets  of  Q  x  F*  x  A*.  Configurations 
are  defined  as  before,  but  with  the  pushdown  top  normally  on  the  right, 
and  we  say  that  (q,  aw,  fl~,  x)  ~-  (p,  w, fly,  xy)  if  and  only  if  J(q,  a, ~) contains  (p,  7', Y). 

The  extended  PDT  P  is  deterministic 

(1)  If  for  all  q  ~  Q,  a  ~  I:  u  {e},  and  ~  ~  F*,  @6(q,  a,  ~) <  1  and, 
(2)  If  6(q,  a,  ~)  ::/:: ;2  and  J(q,  b, fl)  :/:  ;2,  with  b  =  a  or  b  =  e,  then 

neither  of ~  and  fl  is  a  suffix  of the  other. 

DEFINITION 

Let  G =  (N, 1~, P, S)  be  a  CFG.  Let M~ be the extended nondeterministic 
pushdown transducer  ([q}, ~E, N  U  E  U  { $}, [ 1 , . . . ,   p}, ~, q,  $,  ;2).  The  push- 

down  top  is  on the  right,  and  6  is defined  as  follows: 

(1)  O(q, e, ~)  contains  (q, A,  i)  if production  i in P  is  A  ---~ 0~. 
(2)  J(q,  a, e)  =  {(q, a,  e)} for  all  a  in  E. 

(3)  O(q, e,  $S)  =  {(q, e,  e)}. 

This  pushdown  transducer  embodies  the  elements  of  what  is  known  as 
a  shift-reduce  parsing  algorithm.  Under  rule  (2),  M,  shifts  input  symbols 
onto  the  top  of the pushdown  list.  Whenever  a handle  appears  on top  of the 
pushdown  list, Mr  can  reduce  the handle  under  rule  (1) and  emit the number 
of the  production  used  to  reduce  the  handle.  Mr  may  then  shift  more  input 
symbols  onto  the  pushdown  list,  until  the  next  handle  appears  on  top  of 
the  pushdown  list.  The  handle  can  then  be  reduced  and  the  production 
number  emitted.  Mr  continues  to  operate  in  this  fashion  until  the  pushdown 

list  contains  only  the  sentence  symbol  on  top  of  the  end  of  pushdown  list 
marker.  Under  rule  (3) Mr  can  then  enter  a  configuration  in which  the push- 
down  list  is  empty. 

270  THEORY OF TRANSLATION  CHAP.  3 

THEOREM  3.12 

Let  G  =  (N,  Z, P,  S)  be  a  CFG.  Then  lr,(Mr)  =  {(w, nR)[ S  :::~'~ w}. 
Proof.  The  proof  is  similar  to  that  of  Lemma  2.25  and  is  left  for  the 
Exercises.  5 

Example 3.25 

The  right  parser for  Go would  be 

Mr o0 =  ({q}, E,  N  U  *  U  {$}, {1, 2 , . . . ,   6}, ~, q,  $,  ~), 
where 

~(q,  e, E  -q- T)  =  [(q,  E,  1)} 

d~(q, e, T)  =  {(q, E,  2)} 
~(q,  e,  T  *  F)  =  [(q,  T,  3)} 

dr(q, e, F)  =  {(q,  T, 4)} 
~(q,  e,  (E))  =  {(q, F,  5)} 

,~(q,  e,  a)  =  {(q,  F,  6)} 

~(q,  b,  e)  =  {(q, b,  e)}  for  all  b  in 
~(q,  e,  SE)  =  {(q, e, e)} 

With  input  a  4-  a .   a,  M~ o  could  make  the  following  sequence  of  moves, 

among  others: 

(q,  a  +  a  .  a,  $,  e)  F-- (q,  q-a  .  a,  $a,  e) 

~-  (q,  --ka *  a,  $F,  6) 

(q,  q-  a  *  a,  ST,  64) 
~--. (q,  /  a .   a,  $E,  642) 

(q,  a  *  a,  $E  q-,  642) 
t--  (q,  * a,  $E  +  a,  642) 

(q,  *  a,  $E  +  F,  6426) 
F-  (q,  * a,  $E  +  T, 64264) 

(q, a,  $E  +  T . ,   64264) 
(q,  e,  $E  +  T .   a,  64264) 

(q,  e,  SE  +  T  *  F,  642646) 

(q,  e,  $E  q-  T,  6426463) 
~-  (q,  e,  $E,  64264631) 

(q,  e,  e,  64264631) 

SEC. 3.4  PARSING  271 

Thus,  Mr  would  produce  the  right  parse  64264631  for  the  input  string a  +  a .a .  [~] 

We  shall  discuss  deterministic  simulation  of  a  nondeterministic  right 
parser  in  Chapter  4.  In  Section  5.2  we  shall  discuss  an  important  subclass  of 

CFG's,  the  LR  (for  scanning  the  input  from  [eft  to  right  and  producing 
a  right  parse),  for  which  the  PDT  can  be  made  to  operate  deterministically 

by  allowing  it  to  look  some  finite  number  of  symbols  ahead  on  the  input. 
The  LR  grammars  are  thus  those  which  can  be  parsed  naturally  bottom- 

up  and  deterministically.  As  in  left  parsing,  there  are  grammars  which  may 
be  right-parsed  deterministically,  but  not  in  the  natural  way.  We  shall  treat 
these  in  the  next  section. 

3.4.4.  Comparison  of  Top-Down  and  Bottom-Up 

Parsing 

If we  consider  only  nondeterministic  parsers,  then  there  is  little  compari- 
son  to  be  made.  By Theorems  3.11 and  3.12,  every  CFG  has  both  a  left and 
right  parser.  However,  if  we  consider  the  important  question  of  whether 

deterministic  parsers  exist  for  a  given  grammar,  things  are  not  so  simple. 

DEFINITION 

A  CFG  G  is  left-parsable  if there  exists  a  DPDT  P  such  that 

v(P)  =  {(x$, :n:)[(x, g)  ~  T~}. 
G  is  right-parsable  if  there  exists  a  DPDT  P  with 

"r(P)  =  {(x$, ~)](x,  ~z) c  Tro}. 
In  both  cases  we  shall  permit  the  DPDT  to  use an endmarker  to delimit  the 
right  end  of  the  input  string. 

Note  that  all  grammars  are  left-  and  right-parsable  in  an  informal  sense, 
but  it  is  determinism  that  is  reflected  in  the  formal  definition. 

We  find  that  the  classes  of left-  and  right-parsable  grammars  are  incom- 
mensurate;  that  is,  neither  is  a  subset  of the  other.  This  is  surprising  in  view 
of Section  8.1,  where  we  shall  show  that  the  LL  grammars,  those  which  can 

be  left-parsed  deterministically  in  a  natural  way,  are  a  subset  of  the  LR 
grammars,  those which can be right-parsed deterministically in a natural  way. 
The  following  examples  give  grammars  which  are  left-  (right-)  parsable  but 

not  right-  (left-)  parsable. 

Example 3.26 

Let  G1  be  defined  by 

272  THEORY OF TRANSLATION  CHAP.  3 

(1)  S  ---, BAb  (2)  S  --~  CAc 

(3)  A  ~  BA  (4)  A--~  a 
(5)  B ~ a   (6)  C ~ a  

L ( G t ) - - a a + b  /  aa+e.  We  can  show  that  G1  is  neither  LL  nor  LR, 
because  we  do  not  know  whether  the  first  a  in  any  sentence  comes  from  B 

or  C  until  we  have  seen  the  last  symbol  of the  sentence. 

However  we  can  "unnaturally"  produce  a  left  parse  for  any  input  string 
with  a  D P D T   as  follows.  Suppose  that  the  input  is  a"+2b,  n  ~  O.  Then  the 

D PD T   can  produce  the  left parse  15(35)"4 by storing  all a's  on the pushdown 
list  until  the  b  is  seen.  No  output  is  generated  until  the  b  is  encountered. 

Then  the  D PD T   can  emit  15(35)"4  by  using  the  a's  stored  on  the  pushdown 

list to count  to  n.  Likewise,  if the  input  is  a"+Ze, we  can  produce  26(35)"4  as 

output.  In  either  case,  the  trick  is to  delay  producing  any  output  until  b  or  e 
is  seen. We  shall  now  attempt  to  convince  the  reader  that  there  is  no  D PD T  

which  can  produce  a  valid  right  parse  for  all  inputs.  Suppose  that  M  were 

a  D PD T   which  produced  the  right  parses  55"43"1 for  an+2b and  65"43"2  for 
a"+2e.  We  shall  give  an  informal  proof  that  M  does  not  exist.  The  proof 
draws  heavily  on  ideas  in  Ginsburg  and  Greibach  [1966]  in which  it  is shown 
that  {a"b"[n  ~  1}  u  {a"b2"[n  :>  1}  is  not  a  deterministic  CFL.  The  reader  is 

referred  there  for  assistance  in  constructing  a  formal  proof.  We  can  show 

each  of the  following" 

(1)  Let a * be input  to  M.  Then  the output  of M  is empty,  or  else M  would 
emit  a  5  or  6,  and  we  could  "fool"  it  by  placing  c  or  b,  respectively,  on  the 
input,  causing  M  to  produce  an  erroneofis  output. 

(2)  As  a's  enter  the  input  of M,  they  must  be  stored  in  some  way  on  the 
pushdown  list.  Specifically,  we  can  show  that  there  exist  integers  j  and  k, 
pushdown  strings  a  and  fl,  and  state  q  such  that  for  all  integers  p  ~  0, 

(qo, ak+JP, Zo,  e) t---  (q,  e, flPoc, e),  where  q0  and  Z0  are  the  initial  state  and 
pushdown  symbol  of M. (3)  If after k  +  jp  a's, one  b appears  on M's  input,  M  cannot  emit symbol 

4  before  erasing  its  pushdown  tape  to  e.  For  if it  did,  we  could  "fool"  it  by 
previously  placingj  more  a's  on  the  input  and  finding  that  M  emits  the  same 
number  of  5's  as  it  did  previously. 

(4)  After  reducing  its  pushdown  list  to  e,  M  cannot  "remember"  how 
many  a's  were  on  the  input,  because  the  only  thing  different  about  M's 

configurations  for  different  values  of p  (where  k  /  jp  is  the  number  of  a's) 
is  now  the  state.  Thus,  M  does  not  know  how  many  3's  to  emit. 

Example 3.27 

Let  G z be  defined  by 

(1)  S--~  Ab 
(3)  A  ~  A B 
(5)  B - - . a  

(2)  S  --~  Ac 

(4)  A  --~  a 

SEC. 3.4  PARSING  273 
L(G2)  =  a+b  +  a /c.  It  is  easy  to  show  that  G2 is  right-parsable.  Using  an 

argument similar to that  in Example  3.26  it can  be  shown  that  G2 is  not  left- 
parsable.  E] 

THEOREM 3.13 

The  classes  of  left-  and  right-parsable  grammars  are  incommensurate. 
Proof.  By Examples  3.26  and  3.27.  [Z] 

Despite  the  above  theorem,  as  a  general  rule,  bottom-up  parsing  is more 
appealing  than  top-down  parsing.  For  a  given  programming  language  is 

often  easier to  write  down  a  grammar  that  is right-parsable  than  one  that  is 
left-parsable.  Also,  as  was  mentioned,  the  LL  grammars  are  included  in 
the  LR  grammars.  In  the  next  chapter,  we  shall  also  see  that  the  natural 

simulation  of  a  nondeterministic  PDT  works  for  a  class  of  grammars  that 
is,  in  a  sense  to  be  discussed  there,  more  general  when  the  PDT  is  a  right 
parser  than  a  left parser. When we look at  translation,  however,  the  left parse  appears  more  desir- 

able.  We  shall  show that  every  simple  SDT  can  be  performed  by 

(1)  A  PDT  which  produces  left parses  of words,  followed  by 

(2)  A  DPDT  which  maps  the  left  parses  into  output  strings  of the  SDT. 

Interestingly,  there  are  simple  SDT's  such  that  "left  parse"  cannot  be 
replaced  by  "right  parse"  in  the  above. If  a  compiler  translated  by  first  constructing  the  entire  parse  and  then 

converting  the  parse  to  object  code,  the  above  claim  would  be  sufficient  to 
prove  that  there  are  certain  translations  which  require  a  left  parse  at  the 

intermediate  stage. However,  many  compilers  construct  the  parse  tree  node  by  node  and 

compute  the  translation  at  each  node  when  that  node  is  constructed.  We 

claim that  if a  translation  cannot  be  computed  directly from  the  right parse, 

then  it  cannot  be  computed  node  by  node,  if the  nodes  themselves  are  con- 
structed  in  a  bottom-up  way.  These  ideas  will be  discussed  in  more  detail  in 

Chapter  9,  and  we  ask  the  reader  to  wait  until  then  for  the  matter  of node- 
by-node  translation  to  be  formalized. 

DEFINITION 

Let  G  =  (N,  X, P,  S)  be  a  CFG.  We  define L~  and L~, the  left  and  right 
parse  languages  of G,  respectively,  by 

L~  =  {n:lS "---~  w for  some  w in L(G)} 
and 

L~  =  {nRI S  ----'~  W for  some  w in L(G)} 

We  can  extend  the  "==~ and  ==~  notations  to  SDT's  by  saying  that 

274  THEORY OF TRANSLATION  CHAP.  3 

(0~, fl)  ~==~ (~,, ~)  if  and  only  if  (tz, fl)  ~  (~,, c~)  by  a  sequence  of  rules  such 
that  the  leftmost  nonterminal  of  e  is  replaced  at  each  step  and  these  rules, 
with  translation  elements  deleted,  form  the  sequence  of productions  ~.  We 
define  ~-~  for  SDT's  analogously. 

DEFINITION 

An  SDTS  is  semantically  unambiguous if  there  are  no  two  distinct  rules 
of  the  form  A  ---~ tx, fl  and  A  ~  0c, ~,. 

A  semantically  unambiguous  SDTS  has  exactly  one  translation  element 
for  each  production  of the  underlying  grammar. 

THEOREM 3.14 

Let  T  =  (N,  E,  A,  R,  S)  be  a  semantically  unambiguous  simple  SDTS. 
Then  there  exists  a  D P D T   P  such  that  I:,(P)  =  {(zt, y) i (S,  S)  ~  (x, y)  for 
some  x  ~  X*}. 

Proof.  Assume  N  and  zX are  disjoint.  Let  P  =  ({q},  { 1 , . . . ,   p},  N  L) A, A,  ,5, q,  S,  ~), 

where  1 , . . . ,   p  are  the  numbers  of  the  productions  of  the 
underlying  grammar,  and  ~ is  defined  by 

(I)  Let  A  ---,  e  be  production  i,  and  A  ~  e,  fl  the  lone  rule  beginning 
with  A  ~  0c. Then  ~(q,  i,  A)  (q,  fl,  e). 

(2)  For  all  b  in  A,  ~(q,  e,  b)  =  (q,  e,  b). 

P  is  deterministic  because  rule  (1) applies  only  with  a  nonterminal  on  top 
of the  pushdown  list,  and  rule  (2) applies  only  with  an  output  symbol  on  top. 
The  proof  that  P  works  correctly  follows  from  an  easy  inductive  hypothesis" 

(q,  zt, A,  e)[--  (q,  e,  e, y)  if  and  only  if  there  exists  some  x  in  E*  such  that 

(A,  A)  "=-~ (x, y).  We  leave  the  proof  for  the  Exercises. 

To  show  a  simple  SDT  not  to  be  executable  by  any  D P D T   which  maps 
Lf,  where  G  is  the  underlying  grammar,  to  the  output  of  the  SDT,  we  need 

the  following  lemma. 

LEMMA 3.15 

There  is  no  D P D T   P  such  that  v(P)  =  {(wc, wRcw) lw  ~  (a, b}*}. 
Proof.  Here  the  symbol  c  plays  the  role  of  a  right  endmarker.  Suppose 
that  with  input  w,  P  emitted  some  non-e-string,  say  dx,  where  d  =  a  or  b. 
Let  d  be  the  other  of a  and  b,  and  consider  the  action  of P  with  wdc as input. 
It  must  emit  some  string,  but  that  string  begins  with  d.  Hence,  P  does  not 
map  wdc  to  dwRcwd, as  demanded.  Thus,  P  may  not  emit  any  output  until 
the  right  endmarker  c  is  reached.  At  that  time,  it  has  some  string  aw  on  its 

pushdown  list  and  is  in  state  qw. 

Informally,  a~  must  be  essentially  w,  in  which  case,  by  erasing  a~,  P  can 
emit  w R. But  once P  has  erased  aw, P  cannot  then  "remember"  all of w in order 

SEe,  3.4  PARSING  275 

to  print  it.  A  formal  proof  of  the  lemma  draws  upon  the  ideas  outlined  in 

Example  3.26.  We  shall  sketch  such  a  proof  here. 

Consider  inputs  of  the  form  w - - a ; .   Then  there  are  integers  j  and  k, 
a  state  q,  and  strings  *  and  fl  such  that  when  the  input  is  aJ+"kc, P  will  place 
eft"  on  its  pushdown  list  and  enter  state  q.  Then  P  must  erase  the  pushdown 

list  down  to  e  at  or  before  the  time  it  emits  w%.  But  since  e  is independent 

of  w,  it  is  no  longer  possible  to  emit  w.  [--] 

THEOREM 3.15 

There  exists  a  simple  SDTS  T - - ( N ,   E,  A,  R,  S)  such  that  there  is  no 
D P D T   P  for  which  r(P)  =  {Or R, x)  ! (S,  S) ~'~  (w, x)  for  some  w}. 

Proof.  Let  T  be  defined  by  the  rules 

(1)  S  --.  Sa,  aSa 
(2)  S  --,  Sb,  bSb 

(3)  S  ~  c,  c 

Then  L7  =  3(1  /  2)*,  where  G  is  the  underlying  grammar.  If  we  let 
17(1) --  a  and  h(2)  --  b,  then  the  desired  z(P)  is  [(3e,  h(e)Rch(e))la  ~  [ 1, 2}*}. 

If P  existed,  with  or  without  a  right  endmarker,  then  we  could  easily  con- 

struct  a  DP D T  to  define  the  translation  {(wc,  wRcw)lw  e  {a, b}*}, in  contra- 
diction  of  Lemma  3.15.  [--I 

We  conclude  that  both  left  parsing  and  right  parsing  are  of  interest,  and 
we  shall  study  both  in  succeeding  chapters.  Another  type  of  parsing  which 

embodies  features  of  both  top-down  and  bottom-up  parsing  is  left-corner 

parsing.  Left-corner  parsing  will be  treated  in  the  Exercises. 

3.4.5.  Grammatical  Covering 

Let  G 1 be  a  CFG.  We  can  consider  a  grammar  G 2  to  be  similar  from 
the  point  of view  of the  parsing  process  if L(Gz)  --  L(G 1) and  we can  express 
the  left  and/or  right  parse  of a  sentence  generated  by  G1  in  terms  of its  parse 

in  Gz.  If such  is the  case,  we  say that  G2  covers  G I.  There  are  several  uses  for 

covering  grammars.  For  example,  if  a  programming  language  is  expressed 
in  terms  of  a  grammar  which  is  "hard"  to  parse,  then  it  would  be  desirable 
to  find  a  covering  grammar  which  is  "easier"  to  parse.  Also,  certain  parsing 

algorithms  which  we  shall  study  work  only  if  a  grammar  is  some  normal 
form,  e.g.,  C N F   or  non-left-recursive.  If  G1 is  an  arbitrary  grammar  and  G 2 
a  particular  normal  form  of  G 1,  then  it  would  be  desirable  if  the  parses  in 

G i  can  be  simply  recovered  from  those  in  G z.  If  this  is  the  case,  it  is  not 
necessary  that  we  be  able  to  recover  parses  in  G2  from  those  in  G 1. 

For  a  formal  definition  of what  it  means  to  "recover"  parses  in  one  gram- 
mar  from  those  in  another,  we  use  the  notion  of  a  string  homomorphism 

between  the  parses.  Other,  stronger  mappings  could  be  used,  and  some  of 
these  are  discussed  in  the  Exercises. 

276  THEORY  OF  TRANSLATION  CHAP.  3 
DEFINITION 

Let  G 1 =  (N1,  ~,  P1,  $1)  and  G2  =  (N2,  ~,  P2, $2)  be  CFG's  such  that 
L(G1)  =  L(G2).  We  say  that  G 2 left-covers  G1 if there  is  a  homomorphism  h from  P2  to  P1  such  that 

(1)  If $2  ~==~ w,  then  $1 ht~)==~ W, and 
(2)  For  all  rt  such  that  Sx ~==~ w,  there  exists  n'  such  that  $2  *'==~ w  and 
h(n')  =  n. 

We  say  G 2 that  right-covers  G~ if there  is  a  homomorphism  h  from  P2  to PI  such  that 

(I)  If S 2 ==~  w,  then  S1 ==~hC~) W, and 
(2)  For  all  rt  such  that  S~ ==~ w,  there  exists  zt'  such  that  Sz  ==~' w and 
h(n')  =  n. 

Example 3.28 

Let  G 1 be  the  grammar 

(1)  S  --~ 0S1 
(2)  S  --~ 01 

and  G2  be  the  following  C NF  grammar  equivalent  to  G l: 

(1)  S  ~  AB 
(2)  s - ~   A c  
(3)  B  ~  SC (4)  A ~ 0  

(5)  C ~   1 
We  see  G 2 left-covers  G 1 with  the  homomorphism  h(1)  =  1,  h(2)  =  2,  and 

h(3)  =  h(4)  =  h(5)  =  e.  For  example, 

S  1432455  > 0011,  h(1432455)  =  12,  and  S  12_____~ 0011 

G2  G1 

G2  also  right-covers  G1,  and  in  this  case,  the  same  h  can  be  used.  For example, 

S,  > 1352544 0011,  h(1352544)  =  12,  and  S  ~  12 0011 

G~  G1 

G 1 does  not  left-  or  right-cover  G2.  Since  both  grammars  are  unambigu- 
ous,  the mapping  between  parses  is fixed.  Thus  a  homomorphism  g  showing 
that  G1  was  a  left  cover  would  have  to  map  1"2  into  (143)"24(5) "+I,  which 
can  easily  be  shown  to  be  impossible.  [~ 

Many  of  the  constructions  in  Section  2.4,  which  put  grammars  into 
normal  forms,  can  be  shown  to  yield  grammars  which  left-  or  right-cover the  original. 

EXERCISES  277 
Example 3.29 

The  key  step  in  the  Chomsky  normal  form  construction  (Algorithm  2.12) 
is  the  replacement  of  a  production  A  --~  X t ' "   X,,  n  >  2,  by  A  ~  XtBa, 
B t ~  X 2 B z , . . . ,   B,_z  ~  X,_ t X..  The  resulting  grammar  can  be  shown  to 

left-cover  the  original  if  we  map  production  A  ~  XaB~  to  A  --,  X~  . . .   X. 

and  each  of the  productions  B 1 ~  Xz B 2 ,. .. ,  Bn-z  - ~   Xn-IXn  to  the  empty 
string.  If  we  wish  a  right  cover  instead,  we  may  replace  A  ~  X 1 . . '   X,  by 

A  ~  BIXn,  O 1 ~  B2Xn_i,...  ,  B,,_ 2 ~  X tX   2.  [--] 

Other  covering  results  are  left  for  the  Exercises. 

E X E R C I S E S  
3.4.1.  Give  an  algorithm  to  construct  a  derivation  tree  from  a  left  or  right 

parse. 

3.4.2.  Let  G be a  CFG.  Show that L~ is a  deterministic  CFL. 

3.4.3.  Is Lr G always  a  deterministic  CFL? 
*3.4.4.  Construct  a  determinstic pushdown transducer P  such that 

* ( P) =  [(g, g')l~:  is in L~  and g'  is the right  parse 

for the  same derivation tree}. 

*3.4.5.  Can  you  construct  a  deterministic  pushdown  transducer  P  such  that 

'~(P)  =  {(g, g')[~:  is  in L~  and  g'  is  the  corresponding  left  parse} ? 

3.4.6.  Give left and right parses  in  Go for the following words" (a)  ((a)) 

(b)  a  +  (a  +  a) (c) 

a,a,a 

3.4.7.  Let  G be the CFG  defined by the following numbered productions 

(1)  S  ~  if B then  S  else S (2) 

S--~  s 
(3)  B-----~ B AB  
(4)  B  --,  B  V  B 
(5/  B - - ~ b  
Give SDTS's which  define T~ and  T, o. 

3.4.8.  Give PDT's  which  define  T~ and  T~,  where  G is  as  in Exercise  3.4.7. 

3.4.9.  Prove  Theorem  3.10. 
3.4.10.  Prove Theorem  3.11. 
3.4.11.  Give  an  appropriate  definition  for  T, o,  and  prove  that  for  your  SDTS, 

words in L(G)  are  mapped to their right parses. 

3.4.12.  Give  an  algorithm  to  convert  an  extended PDT  to  an equivalent  PDT. 

Your algorithm should be such that if applied to a deterministic extended 

PDT,  the result is a  DPDT.  Prove that  your algorithm  does this. 

278  THEORY OF TRANSLATION  CHAP. 3 

3.4.13.  Prove Theorem  3.12. 

* 3.4.14.  Give  deterministic  right  parsers  for  the  grammars 

(a)  (1)  S ~ S 0  

(2)  S---i S1 
(3)  S  ~  e 

(b)  (1)  S-~   AB  

(2)  A  ~  0A1 

(3)  A --, e 
(4)  B -,  B1 
(5)  B --,e 

"3.4.15.  Give deterministic left parsers  for the  grammars 

(a)  (1)  S--~  0S 

(2)  S---,  1S 
(3)  S  ~  e 

(b)  (1)  S ~   0S1 

(2)  S  ~  A 
(3)  A  --,.  A1 
(4)  A--~  e 

"3.4.16.  Which  of  the  grammars  in  Exercise  3.4.14  have  deterministic  left  par- 

sers ? Which  in Exercise  3.4.15  have  deterministic right  parsers ? 

"3.4.17.  Give  a  detailed  proof  that  the  grammars  in  Examples  3.26  and  3.27 

are  right-  (left-) parsable  but  not  left-  (right-)  parsable. 

3.4.18.  Complete the proof of Theorem  3.14. 
3.4.19.  Complete the proof of Lemma  3.15. 
3.4.20.  Complete the proof of Theorem  3.15. 

DEFINITION 

The  left  corner  of  a  non-e-production  is  the  leftmost  symbol  (ter- 
minal or nonterminal) on the right side. A  le~-corner parse  of a sentence 
is  the  sequence  of  productions  used  at  the  interior  nodes  of  a  parse 
tree  in  which  all  nodes  have  been  ordered  as  follows.  If  a  node  n  has 
p  direct  descendants  nl,  n2,  . . . ,   np,  then  all  nodes  in  the  subtree  with 

root  n l  precede  n.  Node  n  precedes  all  its  other  descendants.  The  des- 
cendants  of  n~  precede  those  of  n3,  which  precede  those  of  r/a,  and 

so  forth. 

Roughly  speaking,  in  left-corner  parsing the left corner of a produc- 
tion  is  recognized  bottom-up  and  the  remainder  of  the  production  is 
recognized top-down. 

Example 3.30 

Figure  3.11  shows  a  parse  tree  for  the  sentence  bbaaab  generated 
by the following grammar: 

(1)  S  ~  AS   (2)  S  ~  B B 
(3)  A  ~  bA A  (4)  A  --~  a 
(5)  B ~ b   (6)  B - - r e  

EXERCISES  279 
nl  (R)n3 
/ N   / \  (~n4  !  ~)n6  (~)n7 (~)n8 

/  I  I  / ~),n5  Qnl 2 Qnl 3 (~)nl 4 

(~)n9  @nl0  ~)nll !  ! 

Fig. 3.11  Parse  tree. 
The  ordering of the nodes imposed by the left-corner-parse  definition 
states  that  node  nz  and  its  descendants  precede  n l,  which  is  then  fol- 
lowed  by  n3  and  its  descendants.  Node  n4  precedes  nz,  which  precedes 
///5,  //6,  and  their  descendants.  Then  //9  precedes  ns,  which  precedes  nl0, 

n l~,  and  their  descendants.  Continuing  in  this  fashion  we  obtain  the 

following  ordering  of nodes: 

H4  /'//2 /119 /7/5 ///15 ///10  ///16 Hll  /I/12 1/6  H1  ///13 /q/7 t//3 ///14 /18 
The  left-corner  parse  is  the  sequence  of  productions  applied  at  the 

interior  nodes  in  this  order.  Thus  the  left-corner  parse  for  bbaaab  is 

334441625. 

Another  method  of  defining  the  left-corner  parse  of  a  sentence  of 
a  grammar  G  is  to  use  the  following  simple  SDTS  associated  with  G. 

DEFINITION 

Let  G  =  (N,  E , P,   S)  be  a  CF G  in  which  the  productions  are 
numbered  1 to p.  Let  T~c be the  simple SDTS  (N,  E,  (1,  2 . .. ..   p),  R, S), 
where  R  contains  a  rule for  each production  in  P  determined  as  follows: 
If  the  ith  production  in  P  is  A  --,  B0~  or  A  ~  aa  or  A  ~  e,  then  R 
contains  the  rule  A  --,  B~,  Bioc"  or  A  --~  a~,  i0~' or  A  ~  e,  i,  respec- 

tively,  where  ~'  is  ~  with  all  terminal  symbols  removed.  Then,  if  (w,  rr) 
is  in  "c(T~c),  zc  is  a  left-corner  parse  for  w. 

Example  3.31 

Tic  for  the  grammar  of  the  previous  example  is 

S  --~  AS,  A 1S   S  ---,  BB,  B2B 
A  ~  bAA,  3AA  A  ---,  a,  4 

B---,  b,  5  B- -~  e,  6 

280  THEORY OF  TRANSLATION  CHAP. 3 

We  can  confirm  that  (bbaaab, 334441625)  is  in  z(T~).  D 
3.4.21.  Prove that  (w, ~) is in z(T~) if and  only if n  is a  left-corner parse for  w. 
3.4.22.  Show that for each CFG  there is a  (nondeterministic) PDT  which  maps 

the  sentences  of  the  language  to  their  left-corner  parses. 

3.4.23.  Devise  algorithms  which  will  map  a  left-corner  parse  into  (1)  the 

corresponding  left  parse  and  (2)  the  corresponding  right  parse  and 
conversely. 

3.4.24.  Show  that  if Ga  left-  (right-)  covers  G2  and  G2  left-  (right-)  covers  G~, 

then  G3  left- (right-) covers  Gi. 

3.4.25.  Let  G a be a  cycle-free grammar.  Show  that  G i  is left- and  right-covered by  grammars with  no  single productions. 

3.4.26.  Show  that  every cycle-free grammar is left-  and  right-covered by  gram- mars  in  CNF. 
*3.4.27.  Show  that  not  every  CFG  is  covered  by  an  e-free  grammar. 

3.4.28.  Show  that  Algorithm  2.9,  which  eliminates  useless  symbols,  produces 

a  grammar  which  left-  and  right-covers  the  original. 

**3.4.29.  Show  that  not  every  proper  grammar  is  left-  or  right-covered  by  a 

grammar in  GNF.  Hint:  Consider  the  grammar  S  ~  S01 S11011. 

**3.4.30.  Show  that  Exercise  3.4.29  still  holds  if  the  homomorphism  in  the 

definition  of  cover  is  replaced  by  a  finite  transduction. 

"3.4.31.  Does  Exercise  3.4.29  still  hold  if  the  homomorphism  is  replaced  by  a pushdown  transducer mapping ? 

Research Problem 

3.4.32.  It  would  be nice if whenever  G2  left-  or  right-covered  G i,  every  SDTS 

with  G1  as  underlying  grammar  were  equivalent  to  an  SDTS  with  G~ 

as underlying  grammar.  Unfortunately,  this  is not  so.  Can  you  find the 
conditions  relating  G1  and  G2  so  that  the  SDT's  with  underlying 
grammar  G t  are  a  subset  of  those  with  underlying  grammar  Gz ? 

B I B L I O G R A P H I C   N O TE S  
Additional  details  concerning  grammatical covering  can  be  found  in  Reynolds 
and  Haskell  [1970],  Gray  [1969] and  Gray  and  Harrison  [1969].  In  some  early 
articles  left-corner  parsing  was  called  bottom-up  parsing.  A  more  extensive 
treatment  of  left-corner  parsing  is  contained  in  Cheatham  [1967]. 

4  GENERAL  PARSING  METHODS 

This  chapter  is  devoted  to  parsing  algorithms  that  are  applicable  to 
the  entire  class  of  context-free  languages.  Not  all  these  algorithms  can  be 
used  on  all  context-free  grammars,  but  each  context-free  language  has  at 
least  one  grammar  for  which  all  these  methods  are  applicable. The full backtracking algorithms  will be discussed  first.  These algorithms 

deterministically  simulate  nondeterministic  parsers.  As  a  function  of  the 
length  of the  string  to  be  parsed,  these  backtracking  methods  require  linear 

space  but  may  take  exponential  time. The  algorithms  discussed in the  second  section  of this chapter are tabular 

in  nature.  These  algorithms  are  the  Cocke-Younger-Kasami  algorithm  and Earley's  algorithm.  They  each  take  space  n 2 and  time  n 3. Earley's  algorithm 
works  for  any  context-free  grammar  and  requires  time  n 2  whenever  the 
grammar  is  unambiguous. The  algorithms  in this  chapter  are included  in this  book  primarily  to  give 

more  insight  into  the  design  of  parsers.  It  should  be  clearly  stated  at  the outset that backtrack parsing algorithms should be shunned in most practical 
applications.  Even the tabular methods, which are asymptotically much faster 
than  the  backtracking  algorithms,  should be avoided if the language  at  hand 
has a grammar for which the more efficient parsing algorithms of Chapters 5 and 

6 are applicable.  It is almost certain that virtually all programming languages 
have  easily  parsable  grammars  for  which  these  algorithms  are  applicable. The  methods  of  this  chapter  would  be  used  in  applications  where  the 

grammars  encountered  do  not  possess  the  special properties  that  are  needed 

by the algorithms  of Chapters  5 and  6.  For example,  if ambiguous  grammars 
are necessary, and all parses are of interest, as in natural language processing, 
then  some  of the  methods  of this  chapter  might  be  considered. 

281 

282  GENERAL PARSING METHODS  CHAP. 4 
4.1.  BACKTRACK  PARSING 

Suppose  that  we  have  a  nondeterministic  pushdown  transducer  P  and 
an  input  string  w.  Suppose  further  that  each  sequence  of  moves  that  P  can 
make  on  input  w  is  of  bounded  length.  Then  the  total  number  of  distinct 

sequences  of moves that P  can  make  is also  finite,  although  possibly  an  expo- 
nential  function  of  the  length  of  w.  A  crude,  but  straightforward,  way  of 

deterministically  simulating  P  is  to  linearly  order  the  sequences  of moves  in 
some  manner  and  then  simulate  each  sequence  of  moves  in  the  prescribed order. 

If  we  are  interested  in  all  outputs  for  input  w,  then  we  would  have  to 
simulate  all  move  sequences.  If we  are  interested  in  only  one  output  for  w, 
then  once  we  have  found  the  first  sequence  of  moves  that  terminates  in 
a  final  configuration,  we  can  stop  simulating P.  Of course,  if no  sequence  of 
moves  terminates  in  a  final  configuration,  then  all  move  sequences  would have  to  be  tried. 

We  can  think  of  backtrack  parsing  in  the  following  terms.  Usually, 
the  sequences  of  moves  are  arranged  in  such  an  order  that  it  is  possible  to 

simulate  the  next  move  sequence  by  retracing  (backtracking)  the  last  moves 
made  unti!  a  configuration  is  reached  in  which  an  untried  alternative  move 

is  possible.  This  alternative  move  would  then  be  taken.  In  practice,  local 
criteria  by  which  it  is  possible,  without  simulating  an  entire  sequence,  to 
determine  that  the  sequence  cannot  lead  to  a  final  configuration,  are  used 
to  speed  up  the  backtracking  process. 

In  this  section  we  shall  describe  how  we  can  deterministically  simulate 
a  nondeterministic  pushdown  transducer  using  backtracking.  We  shall  then 
discuss  two  special  cases.  The  first  will  be  top-down  backtrack  parsing  in 
which  we  produce  a  left  parse  for  the  input.  The  second  case  is  bottom-up 
backtrack  parsing  in  which  we  produce  a  right  parse. 

4.1.1.  Simulation  of  a  PDT 

Let  us  consider  a  PD T  P  and  its  underlying  PDA  M.  If  we  give  M  an 
input  w,  it  is  convenient  to  know  that  while  M  may  nondeterministically  try 
many  sequences  of  moves,  each  sequence  is  of  bounded  length.  If  so,  then 
these  sequences  can  all be tried  in  some  reasonable  order.  If there  are  infinite 
sequences  of  moves  with  input  w,  it  is,  in  at  least  one  sense,  impossible  to 
directly simulate  M  completely.  Thus  we make  the following  definition. 

DEFINITION 

A  PDA  M  =  (Q,  ~,  I', $, q0, Z0, F)  is halting if for  each  w in  ~*,  there  is 
a  constant  k,  such  that  if  (q0,  w, Z 0 ) ~   (q, x, 7),  then  m  <  kw.  A  PDT  is halting if its  underlying  PDA  is  halting. 

sEc.  4.1  BACKTRACK PARSING  283 

It  is  interesting  to  observe  the  conditions  on  a  grammar  G  under  which  1 the  left  or  right  parser  for  G  is  halting.  It  is  left  for  the  Exercises  to  s h o w  
that  the  left  parser  is  halting  if and  only  if G  is  not  left-recursive;  the  right 
parser  is  halting  if and  only  if G  is  cycle-free  and  has  no  e-productions.  We 

shall  show  subsequently  that  these  conditions  are  the  ones  under  which  our 
general  top-down  and  bottom-up  backtrack  parsing  algorithms  work, 

although  more  general algorithms  work  on  a  larger class of grammars. 

We  should  observe that  the condition  of cycle freedom plus  no  e-produc- 
tions is not  really very restrictive.  Every CFL without  e has  such a  grammar, 

and,  moreover,  any  context-free  grammar  can  be  made  cycle-flee and  e-fr~,e 
by  simple  transformations  (Algorithms  2.10  and  2.11).  What  is  more,  if  the original  grammar  is unambiguous,  then  the  modified grammar  left and  right 

covers it. Non-left  recursion is a  more stringent condition in this sense.  While 

every  CFL  has  a  non-left-recursive  grammar  (Theorem  2.18),  there  may  be 

no  non-left-recursive  covering  grammar.  (See  Exercise  3.4.29.) As  an  example  of what  is  involved  in  backtrack  parsing  and,  in  general, 

simulating  a  nondeterministic  pushdown  transducer,  let  us  consider  the 
grammar  G with  productions 

(1)  S-   ~ aSbS ~ 

(2)  S  > aS 
(3)  S  > c 

The  following  pushdown  transducer  T  is  a  left  parser  for  G.  The  moves  of 

T  are  given by  $(q, a, S)  =  {(q, SbS, 

1), (q,  S, 2)} 

iS(q, c,  S)  =  [(q,  e,  3)] 

~(q, b, b)  =  {(q, e,  e)} 

Suppose  that  we  wish  to  parse  the  input  string  aacbc. Figure  4.I  shows 
a  tree  which  represents  the  possible  sequences  of  moves  that  T  can  make 
with  this  input. 

c J C o ~  
c3 ~  ~ c 4   c , ~ C 2 ~ c , s  ! 1   [ 

C5  C8  C12 I  I  I 
C6  C9  Cl 3 I  I  ! 

C7  Clo  C14 

C16 

Fig. 4.1  Moves of parser. 

=  ?  = 

284  GENERAL PARSING METHODS  CHAP. 4 

Co represents  the initial configuration (q, aacbc,  S,  e). The rules of T show 
that two next configurations are possible from Co, namely Cx =  (q, ache, SbS,  1) 
and  C2  =  (q, acbc,  S,  2).  (The  ordering  here  is  arbitrary.)  From  C~,  T  can 
enter  configurations  C3  =  (q,  cbc,  SbSbS,  11)  and  C4  =  (q,  cbc,  SbS,  12). 
From  C2,  T  can  enter  configurations  C~  =  (q,  cbc,  SbS,  21)  and  C~ 5 = (q, cbc,  S, 

22).  The  remaining  configurations  are  determined  uniquely. 
One way to determine all parses for the given input  string  is  to  determine 
all  accepting  configurations  which  are  accessible  from  Co  in  the  tree  of con- 
figurations.  This  can  be  done  by  tracing  out  all  possible  paths  which  begin 
at  C O and terminate  in a  configuration from  which no  next  move is possible. 
We  can  assign  an  order  in  which  the  paths  are  tried  by  ordering  the  choices 

of  next  moves  available  to  T  for  each  combination  of  state,  input  symbol, 
and symbol on top of the pushdown list. For example, let us choose (q, SbS,  1) 
as  the  first  choice  and  (q, S, 2)  as  the  second  choice  of  move  whenever the  rule  t~(q, a,  S)  is  applicable. 

Let  us  now  consider  how  all  the  accepting  configurations  of  T  can  be 
determined: by  systematically  tracing  out  all  possible  sequences  of moves  of 
T.  From Co suppose that we make the first choice  of next move to obtain  C~. 
From  C~  we  again  take  the  first  choice  to  obtain  C 3.  Continuing  in  this 
fashion  we follow the  sequence  of  configurations  Co, C~,  C a,  C5,  C6,  C7.  C7 
represents  the  terminal configuration  (q,  e, bS,  1133), which is not an  accept- 
ing  configuration.  To  determine  if  there  is  another  terminal  configuration, 
we  can  "backtrack"  up  the  tree  until  we  encounter  a  configuration  from 
which another  choice  of next  move  not  yet considered  is  available.  Thus  we 
must  be  able  to  restore  configuration  C6  from  C7.  Going  back  to  C6  from 

C7  can  involve  moving  the  input  head  back  on  the  input,  recovering  what 
was  previously  on  the  pushdown  list,  and  deleting  any  output  symbols  that 
were emitted in going from  C6 to  C7.  Having restored  C6, we must also have 
available  to  us  the  next  choice  of moves  (if any).  Since  no  alternate  choices 
exist in  C6, we continue  backtracking  to  C5, and  then  C a and  C~. 

From  C~  we  can  then  use  the  second  choice  of move  for  ~(q,  a,  S)  and 
obtain  configuration  C4.  We  can  then  continue  through  configurations  C8 
and  C9  to  obtain  C~0  =  (q,  e, e,  1233),  which  happens  to  be  an  accepting configuration. 

We  can  then  emit  the  left  parse  1233 as  output.  If we  are  interested  in 
obtaining  only  one  parse  for  the  input  we  can  halt  at  this  point.  However, 
if we are interested in all parses, we can proceed to backtrack to configuration 

Co and  then try all configurations  accessible from  C2.  C~ 4 represents  another accepting  configuration,  (q, e, e,  2133). 

We  would  then  halt  after  all  possible  sequences  of  moves  that  T  could 
have made have been considered. If the input string had not been syntactically 

well formed,  then  all possible  move  sequences  would  have  to  be  considered. 

SEC. 4.1  BACKTRACK PARSING  285 

After  exhausting  all  choices  of moves  without  finding  an  accepting  configu- 

ration  we would  output  the  message  "error." 

The  above  analysis  illustrates  the  salient  features  of  what  is  sometimes 
known  as  a  nondeterministic  algorithm,  one  in  which  choices  are  allowed  at 
certain  steps  and  all  choices  must  be  followed.  In  effect,  we  systematically 
generate  all  configurations  that  the  data  underlying  the  algorithm  can  be  in 

until  we  either  encounter  a  solution  or  exhaust  all  possibilities.  The  notion 

of a  nondeterministic  algorithm  is thus  applicable  not  only to  the  simulation 
of  nondeterministic  automata,  but  to  many  other  problems  as  well.  It  is 
interesting  to  note  that  something  analogous  to  the  halting  condition  for 
PDT's  always  enters  into  the  question  of  whether  a  nondeterministic 
algorithm  can  be  simulated  deterministicaUy.  Some  specific  examples  of 
nondeterministic  algorithms  are  found  in  the  Exercises. 

In  syntax  analysis  a  grammar  rather  than  a  pushdown  transducer  will 
usually be given.  For this reason we shall now discuss top-down and bottom- 
up  parsing  directly  in  terms  of  the  given  grammar  rather  than  in  terms  of 
the  left  or  right  parser  for  the  grammar.  However,  the  manner  in  which 

the  algorithms  work  is  identical  to  the  serial  simulation  of  the  pushdown parser.  Instead  of cycling through  the possible  sequences  of moves the parser 

can  make,  we  shall  cycle through  all possible  derivations  that  are  consistent 

with  the  input. 

4.1.2.  Informal  Top-Down  Parsing 

The  name  top-down  parsing  comes  from  the  idea  that  we  attempt  to 
produce  a  parse  tree  for  the  input  string  starting  from  the  top  (root)  and 

working  down  to  the  leaves.  We  begin  by  taking  the  given  grammar  and 
numbering  in  some  order  the  alternates  for  every  nonterminal.  That  is,  if 
A---~ ~z~ 10c21"" 10c~ are  all  the  A-productions  in  the  grammar,  we  assign some  ordering  to  the  czt's (the  alternates  for  A). 

For  example,  consider  the  grammar  mentioned  in  the  previous  section. 
The  S-productions  are 

S  > aSbS[aSIc 

and let us use them in the order given.  That is, aSbS  will be the first alternate 
for  S,  aS  the  second,  and  c the  third.  Let  us  assume  that  our  input  string  is 

aacbc.  We  shall  use  an  input  pointer  which  initially  points  at  the  leftmost 

symbol  of the  input  string. 

Briefly  stated,  a  top-down  parser  attempts  to  generate  a  derivation  tree 
for the input as follows.  We begin with a  tree containing  one node  labeled  S. 

That  node  is  the  initial  active  node.  We  then  perform  the  following  steps 

recursively" 

286  GENERAL PARSING  METHODS  CHAP. 4 

(1)  If  the  active  node  is  labeled  by  a  nonterminal,  say  A,  then  choose 
the  first  alternate,  say  X~  . . .   Xk,  for  A  and  create  k  direct  descendants  for 
A  labeled  X1, X2,.  * . ,   Xk.  Make  X1  the  active  node.  If  k  =  0,  then  make the  node  immediately  to  the  right  of A  active. 

(2)  If  the  active  node  is  labeled  by  a  terminal,  say  a,  then  compare  the 
current  input  symbol  with  a.  If  they  match,  then  make  active  the  node 
immediately  to  the  right  of a  and  move  the  input  pointer  one  symbol  to  the 
right.  If  a  does  not  match  the  current  input  symbol,  go  back  to  the  node 

where  the  previous  production  was  applied,  adjust  the  input  pointer  if neces- 
sary,  and  try the next alternate.  If no  alternate  is possible,  go back to  the next previous  node,  and  so  forth. 

At  all  times  we  attempt  to  keep  the  derivation  tree  consistent  with  the 
input  string.  That  is,  if x~  is the  frontier  of the  tree  generated  thus  far,  where 

0~ is either e or begins with a nonterminal  symbol,  then x is a prefix of the input string. 

In  our  example  we  begin  with  a  derivation  tree  initially  having  one  node 
labeled  S.  We  then  apply  the  first  S-production,  extending  the  tree  in  a  man- 
ner  that  is  consistent  with  the  given  input  string.  Here,  we  would  use 
S  ~  aSbS  to  extend  the tree to  Fig.  4.2(a).  Since the active node  of the tree is 
a at this instant  and  the first input  symbol  is  a,  we  advance  the  input  pointer 
to  the  second  input  symbol  and  make  the  S  immediately  to  the  right  of  a 
the  new  active  node.  We  then  expand  this  S  in  Fig.  4.2(a),  using  the  first 
alternate,  to  obtain  Fig. 4.2(b).  Since the  new active node  is a,  which  matches 

a 
(a)  (b) 

I 
c 

(c)  i  I 

c  c 

(d) 

Fig. 4.2  Partial derivation trees. 

SEC. 4.1  BACKTRACK PARSING  287 

the  second  input  symbol,  we  advance  the  input  pointer  to  the  third  input 
symbol. 

We  then  expand  the  leftmost  S  in  Fig.  4.2(b),  but  this  time we cannot  use 
either  the  first  or  second  alternate  because  then  the  resulting  left-sentential 
form  would  not  be  consistent  with  the  input  string.  Thus  we  must  use  the 

third  alternate  to  obtain  Fig.  4.2(c).  We  can  now  advance  the  input  pointer 
from the  third  to  the  fourth  and  then  to  the fifth input  symbol,  since the next 

two  active  symbols  in  the  left-sentential  form  represented  by  Fig.  4.2(c)  are 

c and  b. 

We  can  expand  the  leftmost  S  in  Fig.  4.2(c)  using  the  third  alternate  for 
S  to  obtain  Fig.  4.2(d).  (The  first  two  alternates  are  again  inconsistent  with 
the  input.)  The  fifth terminal  symbol is c,  and  thus  we can  advance  the  input 
pointer  one  symbol  to  the  left.  (We  assume  that  there  is  a  marker  to  denote 
the  end  of the  input  string.)  However,  there  are  more  symbols  generated  by 

Fig.  4.2(d),  namely  bS,  than  there  are  in  the  input  string,  so  we  now  know 
that  we  are  on  the  wrong track  in finding  a  correct  parse  for  the  input. 

Recalling the pushdown parser  of Section 4.1.1, we have at this point gone 
through  the  sequence  of  configurations  Co,  C1,  C3,  C5,  C6,  C7.  There  is  no next  move  possible  from  C 7. 

We  must  now  find  some  other  left-sentential  form.  We  first  see if there  is 
another  alternate  for  the  production  used  to  obtain  the  tree  of  Fig.  4.2(d) 
from  the  previous  tree.  There  is  none,  since  we  used  S  ~  c  to  obtain  Fig. 
4.2(d)  from  Fig.  4.2(c).  We  then  return  to  the  tree  of  Fig.  4.2(c)  and  reset 
the input  pointer  to  position  3 on the input.  We  determine  if there  is another 
alternate  for the production  used to  obtain  Fig. 4.2(c) from the previous tree. 
Again  there  is  none,  since  we  used  S - 4   c  to  obtain  Fig.  4.2(c)  from  Fig. 
4.2(b).  We thus  return  to  Fig.  4.2(b),  resetting the input pointer to position 2. 

We used the first alternate  for S  to obtain  Fig.  4.2(b) from Fig. 4.2(a),  so now 
we  try  the  second  alternate  and  obtain  the  tree  of  Fig.  4.3(a). 

We can now advance  the input pointer to position  3, since the a generated 
matches  the  a  at  position  2  in  the  input  string.  Now,  we  may  use  only  the 
third  alternate  to  expand  the  leftmost  S  in  Fig.  4.3(a)  to  obtain  Fig.  4.3(b). 
The  input  symbols at positions  3 and  4 are  now matched,  so we can  advance 

the  input  pointer  to  position  5.  We  can  apply  only  the  third  alternate  for  S 
in  Fig.  4.3(b),  and  we  obtain  Fig.  4.3(c).  The  final  input  symbol  is  matched 

with  the  rightmost  symbol  of  Fig.  4.3(c).  We  thus  know  that  Fig.  4.3(c)  is 
a  valid parse  for  the  input.  At  this  point  we can  backtrack  to  continue  look- 

ing  for  other  parses,  or  terminate. 

Because  our grammar  is not left-recursive,  we  shall  eventually  exhaust  all 
possibilities  by  backtracking.  That  is,  we would  be  at  the  root,  and  all  alter- 

nates  for  S  would  have  been  tried.  At  this  point  we  can  halt,  and  if we  have 

not  found  a  parse,  we  can  report  that  the  input  string  is  not  syntactically 
well  formed. 

288  GENERAL  PARSING  METHODS  CHAP. 4 

a  S 

(a) 

s ! 

c 
(b) 

a  .  S  c I 

c 
(c) 
Fig. 4.3  Further attempts at parsing. 

There  is a  major pitfall in this procedure.  If the grammar is left-recursive, 
then  this  process  may  never  terminate.  For  example,  suppose  that  Ae  is 

the  first  alternate  for A.  We  would  then  apply this  production  forever  when- 

ever A  is  to  be  expanded. 

One  might  argue  that  this  problem  could  be  avoided  by trying the  alter- 
nate  A0c  for  A  last.  However,  the  left  recursion  might  be  far  more  subtle, 
involving  several  productions.  For  example,  the  first  A-production  might  be 

A  ---~ SC.  Then  if  S  ~  A B   is  the  first  production  for  S,  we  would  have 
A  =-~ S C  =~ AB C,  and  this  pattern  would  repeat.  Even if a  suitable  ordering 
for  the  productions  of  all  nonterminals  is  found,  on  inputs  'which  are  not 

syntactically  well  formed,  the  left-recursive  cycles  would  occur  eventually, 

since  all  preceding  choices  would  fail. 

A  second attempt to nullify the  effects of left recursion might be to  bound 
the  number  of nodes in the  temporary tree in terms  of the length  of the input 

string.  If we have a  CFG  G  =  (N,  X, P,  S) with  ~  N  =  k  and  an input  string 
w of length n  -  1, we can  show that  if w is in L(G),  then  there  is at least one 

derivation  tree  for  w  that  has  no  path  of  length  greater  than  kn.  Thus  we 

could  confine  our  search  to  derivation trees  of depth  (maximum  path  length) 

no  greater  than  kn. 

However,  the  number  of  derivation  trees  of  depth  <  d  can  be  an  enor- 
mous  function  of d for  some grammars.  For  example,  consider the grammar 

SEC. 4.1  BACKTRACK PARSING  289 

G  with  productions  S  ~  SSI  e.  The  number  of  derivation  trees  of  depth 
d  for  this  grammar  is  given  by  the  recurrence 

D(1)  =  1 
D(d)  =  ( D ( d- -   1))2+  1 

Values  of  D(d)  for  d  from  1 to  6  are  given  in  Fig.  4.4. 

d  D(d) 

1  1 2  2 
3  5 4  26 
5  677 6  458330 

Fig.  4.4  Values  of  D(d). 

D(d)  grows  very  rapidly,  faster  than  2 2`-` for  d >  3.  (Also,  see  Exercise 
4.1.4.)  This  growth  is  so  huge  that  any  grammar  in  which  two  productions 

of this  form  need  to  be  considered  could  not  possibly  be  reasonably  parsed 
using  this  modification  of the  top-down  parsing  algorithm. 

For  these  reasons  the  approach  generally  taken  is to  apply the top-down 
parsing  algorithm  only  to  grammars  that  are  free  of left  recursion. 

4.1.3.  The Top-Down  Parsing  Algorithm 

We are now ready to  describe our top-down backtrack parsing algorithm. 
The algorithm uses two pushdown  lists (L1  and L2) and a  counter containing 
the  current position  of the input  pointer.  To  describe the algorithm precisely, 
we shall use a stylized notation  similar to that used to describe configurations 

of a  pushdown  transducer. 

ALGORITHM 4.1 

Top-down  backtrack  parsing. 
Input.  A  non-left-recursive  CFG  G  =  (N,  X, P,  S)  and  an  input  string 
w  =  ala 2  . . .  a,,  n  ~  O.  We  assume  that  the  productions  in P  are  numbered 

1 , 2 , . . .   ,p. 

Output.  One  left parse  for  w if one  exists.  The  output  "error"  otherwise. 

Method. 

(i)  For  each  nonterminal  A  in  N,  order  the  alternates  for  A.  Let  A t be 
the  index  for  the  ith  alternate  of  A.  For  example,  if  A  --~ t~litz2[.-.  [~k 

290  GENERAL PARSING  METHODS  CHAP.  4 
are  all the  A-productions  in P  and  we  have  ordered  the alternates  as shown, 
then  A 1 is the  index  for  as,  A2 is the  index  for  0~2, and  so forth. 

(2)  A  4-tuple  (s,  i, ~, fl)  will  be  used  to  denote  a  configuration  of  the algorithm" 

(a)  s  denotes  the  state  of the  algorithm. 
(b)  i  represents  the  location  of  the  input  pointer.  We  assume  that 

the  n  +  1st  "input  symbol"  is  $,  the  right  endmarker. 
(c)  0~ represents  the  first  pushdown  list  (L1). 

(d)  fl  represents  the  second  pushdown  list  (L2). 

The  top  of 0c will be on the right and  the top  of fl will be  on the left. L2 repre- 
sents  the  "current"  left-sentential  form,  the  one  which  our  expansion  of 
nonterminals  has  produced.  Referring  to  our  informal  description  of  top- 

down parsing in Section 4.1.1,  the  symbol on top  of L2 is the symbol labeling 
the active node of the  derivation  tree  being generated.  L1  represents  the  cur- 
rent  history  of  the  choices  of  alternates  made  and  the  input  symbols  over 
which the input head has shifted.  The  algorithm  will be in  one  of three  states 
q,  b,  or  t;  q  denotes  normal  operation,  b  denotes  backtracking,  and  t is the terminating  state. 

(3)  The  initial  configuration  of  the  algorithm  is  (q,  1, e,  S$). 
(4)  There  are  six types  of steps.  These  steps  will  be described  in terms of 
their  effect  on  the  configuration  of the  algorithm.  The  heart  of the algorithm 
is  to  compute  successive  configurations  defined  by  a  "goes  to"  relation,  t--. 
The  notation  (s,  i, a,  fl)  ~  (s',  i',  a',  fl')  means  that  if  the  current  configu- 
ration is (s,  i,  ~,  fl),  then we are to go next into the configuration  (s',  i',  ~',  fl'). Unless  otherwise  stated,  i can  be  any  integer  from  1 to  n  +  1,  a  a  string  in 

(X  w  I)*,  where  I  is  the  set  of  indices  for  the  alternates,  and  fl  a  string  in (N  U  X)*.  The  six  types  of move  are  as  follows: 

(a)  Tree  expansion 

(q,  i,  oc, Aft)  F-  (q,  i,  ocA 1, ?lfl) 
where  A  ~  71 is a  production  in P  and  ?1  is the first alternate for 
A. This step corresponds to  an  expansion  of the partial  derivation 
tree  using  the  first  alternate  for  the  leftmost  nonterminal  in  the tree. 

(b)  Successful  match  of input  symbols  and  derived  symbol 

(q,  i,  oc, aft)  ~-  (q,  i  +  1, oca, fl) 
provided  a~ - - a ,   i ~  n.  If the  ith  input  symbol  matches  the  next 
terminal  symbol  derived,  we  move  that  terminal  symbol  from 
the  top  of L2  to  the  top  of L1  and  increment  the  input  pointer. 

SEC. 4.1  BACKTRACK PARSING  291 

(c)  Successful  conclusion 

( q , n +   1, a,  $)  }--- (t, n  -+- 1, a,  e) 
We  have  reached  the  end  of the  input  and  have  found a  left- 
sentential  form which matches  the  input.  We  can  recover  the  left 
parse  from  a  by  applying  the  following  homomorphism  h  to  a: h(a) 

--  e for all a in E;  h(A,)  =  p,  where p  is the production  num- 
ber  associated  with  the  production  A  ~  ?,  and  ?  is  the  ith alter- 
nate  for  A. 
(d)  Unsuccessful  match  of input  symbol  and  derived  symbol 

( q,  i,  oc, aft)  ~  (b,  i,  o~, aft)  if at  ~  a 
We  go  into  the  backtracking  mode  as  soon  as  the left-sentential 
form  being  derived  is  not  consistent  with  the  input. 
(e)  Backtracking  on  input 

(b,  i,  o~a, fl)  F-  (b,  i  --  1,  ~,  aft) 
for  all  a  in  E.  In  the  backtracking  mode  we  shift  input  symbols 

back  from L1  to L2. 
(f)  Try  next  alternate 

(b,  i,  ocAj, ?jfl)  ~- 
(i)  (q,  i, ~Aj+ 1, ?j+lfl),  if  )'j+l  is  the  j  +  1st  alternate  for  A. 

(Note  that  ),~ is  replaced  by  ~,j+t on  the  top  of L2.) 
(ii)  No  configuration,  if  i - - 1 ,   A  =  S,  and  there  are  only j 

alternates  for  S.  (This  condition  indicates  that  we  have 

exhausted  all  possible  left-sentential  forms  consistent  with 
the  input  w without  having  found  a  parse  for  w.) 
(iii)  (b, i, ~ ,A fl)  otherwise.  (Here,  the  alternates  for  A  are 

exhausted,  and  we  backtrack  by  removing  Aj  from L1  and 
replacing  ~,j by  A  on L2.) 
The  execution  of the  algorithm  is  as  follows. 

Step  1"  Starting  in  the  initial  configuration,  compute  successive  next 
configurations  Co  ~  C1  [--  " "   ~  Ci  ~  .."  until  no  further  configurations 
can  be  computed. 

Step  2:  If  the  last  computed  configuration  is  (t, n  +  1, ?,  e),  emit  h(?) 
and halt. h(?) is the first found left parse.  Otherwise, emit the error signal.  D 

Algorithm  4.1  is essentially the  algorithm  we described informally earlier, 
with  a  few  bookkeeping  features  added  to  perform  the  backtracking. 

292  GENERAL PARSING  METHODS  CHAP.  4 

Example 4.1 

Let  us  consider  the  operation  of  Algorithm  4.1  using  the  grammar  G with  productions 

(1)  E  > T-4-  E 
(2)  E  > T  
(3)  T - - - .   F .   T 
(4)  T  > F  
(5)  F -   >a 

Let  E~  be  T + E ,   Ez  be  T,  T~  be  F  *  T,  an d/' 2  be  F.  With  the input  a  -4- a, Algorithm  4.1  computes  the  following  sequence  of configurations" 

(q,  1, e, E$)  [-- (q,  1, g~, T  +  E$) 

I-  (q,  1, E 1T 1, F  *  T  +  E$) 

(q,  1, E 1T1F1, a *  T  +  E$) 
(q, 2, E~ TtFta,  *  T  +  E$) 

(b, 2, Et T~Fta,  * T  +  E$) 
[-- (b,  1, E~ T~Ft,  a *  T  +  E$) 

(b,  1, Et Tt,  F ,   T  +  E$) 
l--- (q,  1, Et T 2, F  +  E$) 
[-- (q,  1, E~ T2F~, a  +  E$) 

(q, 2, Et T2Fta,  +  E$) 
[-- (q,  3, E~ T2F~a  +,  E$) 
l-  (q,  3, E, TzF~a  +  E 1, T  +  E$) 

(q, 3, E~ T2F ~ a  +  Et Tt,  F  *  T  +  E$) 
(q,  3, E~ T2Fta  +  E~ TtF 1, a  *  T  +  E$) 
t-  (q, 4, E~ T2F~a  +  E t T~F,a,  *  T  +  E$) 

(b, 4, E t T2Fxa  +  E~ TiF~a,  *  T  +  E$) 
(b,  3, E~ T2F~a  +  E t T~F t,  a  *  T  +  E$) 
~-  (b,  3, E~ T2F~a  +  E~ T~, F ,   T  +  E$) 
~-  (q, 3, E~ T2F~ a  +  E, T2, F  q-  E$) 

(q,  3, E, TzF~a  -k  E, T2Ft,  a  -4- E$) 
[-  (q, 4, E, TzF~a  -t-- E1TzF~a,  .+- E$) 
[-  (b, 4, E~ TzFta  -4- E~ TzF, a,  +  E$) 

SEC. 4.1  BACKTRACK PARSING  293 

(b, 3, El TzF~a +  E~ TzF a, a +  E$) 
~-  (b, 3, E 1TzF~a +  E 1T2, F +  E$) 

~-  (b, 3, Et T2F~a +  Ea, T  +  E$) 
~- (q, 3, E 1TzFla  -k  E2, T$) 
]-  (q,  3,  E 1T2Fla  +  E2 T 1, F *  T$) 
]-  (q,  3,  Ea T2F~a +  E2 T~Fa, a *  T$) 

F-  (q,  4,  gaTzV~a  +  gzr~.Fta,  *  T$) 
~--- (b,  4,  E1T2Fla  +  EzT1F1 a, * T$) 
[-  (b,  3,  Ea T2Faa +  E2 TiF1, a *  T$) 

(b,  3, Ea TzFaa +  EzT1, F .   T$) 
(q,  3,  g~ T2Faa +  E2T2, F$) 
[-  (q,  3,  EI T2F~a +  E2 T2F1, aS) 

(q, 4,  g~ r~F~a +  g~r~F~a, $) 

(t,  4, E~ T2F~a +  E2T2F~a, e) 

The  left  parse  is  h(E~ T2Fxa +  E2T2F~a) =  145245.  [Z 

We  shall  now  show  that  Algorithm  4.1  does  indeed  produce  a  left  parse 
for  w  according  to  G  if  one  exists. 

DEFINITION 

A  partial  left  parse  is  the  sequence  of  productions  used  in  a  leftmost 
derivation  of  a  left-sententiai  form.  We  say  that  a  partial  left  parse  is  con- sistent 

with  the  input  string  w  if the  associated  left-sentential  form  is  consis- 

tent  with  w. Let  G  =  (N,  X, P,  S)  be  the  non-left-recursive  grammar  of  Example  4.1 

and  let  w  =  a 1 -. .   an  be  the  input  string.  The  sequence of  consistent partial 

left parses  for  w,  n0,  tel,  z~2, . . . .   zti, * * *  is  defined  as  follows: 

(1)  no  is  e  and  represents  a  derivation  of  S  from  S.  (z~0 is  not  strictly 
a  parse.) (2)  n1  is  the  production  number  for  S  ---~ ~,  where  ~z is  the  first  alternate 

for  S. (3)  ztt is defined  as follows:  Suppose  that  S  "'-~=~ xAy.  Let  fl be  the  lowest 

numbered  alternate  for  A,  if  it  exists,  such  that  we  can  write  xfl?--  xyO, 

where  ~  is  either  e  or  begins  with  a  nonterminal  and  xy  is  a  prefix  of  w. 

Then  n~--ni _iAk,  where  k  is  the  number  of  alternate  ft.  In  this  case  we 

call  rt~  a  continuation  of  n,._~.  If,  on  the  other  hand,  no  such  fl  exists,  or 

S  .... =~  x  for  some  terminal  string  x,  then  let  j  be  the  largest  integer  less 
than  i -   1  such  that  the  following  conditions  hold: 

294  GENERAL PARSING METHODS  CHAP. 4 

(a)  Let S  ~'=~ xB?,  and  let nj+ 1 be a  continuation  of nj,  with alternate 

ek  replacing  B  in  the  last  step  of  nj+ 1.  Then  there  exists  an alter- 
nate  e,,  for  B  which  follows  ek  in  the  order  of  alternates  for  B. 
(b)  We  can  write  Xem? =  xyO,  where  O is  e  or  begins  with  a  nonter- 

minal;  xy  is  a  prefix  of w.  Then  ni  --  njB,,,  where  B m is  the  num- 
ber  of  production  B  --,  e~.  In  this  case,  we  call  nt  a  modification 
of ~i- 1- 
(c)  7r~ is  undefined  if  (a)  or  (b)  does  not  apply. 

Example 4.2 

For  the  grammar  G  of  Example  4.1  and  the  input  string  a  +  a,  the 
sequence  of consistent  partial  left  parses  is 

e 

1 
13 
14 
145 
1451 
14513 
14514 
1452 
14523 
14524 
145245 
2 
23 
24 

It  should  be  observed  that  the  sequence  of  consistent  partial  left  parses 
up  to  the  first  correct  parse  is  related  to  the  sequence  of  strings  appearing 
on  L1.  Neglecting  the  terminal  symbols  on  L1,  the  two  sequences  are  the 
same,  except  that  L1  will have  certain  sequences  that  are  not  consistent  with 
the  input.  When  such  a  sequence  appears  on  L1,  backtracking  immediately 

occurs.  [~] 

It  should  be  obvious  that  the  sequence  of consistent  partial  left  parses  is 
unique  and  includes  all  the  consistent  partial  left  parses  in  a  natural  lexi- cographic  order. 

LEMMA  4.1 

Let  G  =  (N,  X, P,  S)  be  a  non-left-recursive  grammar.  Then  there  exists i 
a  constant  c  such  that  if A  ~  wBot  and  Iwl  =  n,  then  i <  c "+2.t 

lm 

tin  fact,  a  stronger  result  is possible;  i  is  linear  in  n.  However,  this  result  suffices for the  time  being  and  will help  prove  the  stronger result. 

SEC. 4.1  BACKTRACK PARSIN6  295 

Proof.  Let  ~ N   =  k,  and  consider  the  derivation  tree  D  corresponding 

i to  the  leftmost  derivation  A  =-~ wBt~.  Suppose  that  there  exists  a  path  of 

length  more  than  k(n  -I-  2) from  the  root  to  a  leaf.  Let  n o be the  node  labeled 

by  the  explicitly  shown  B  in  wBe.  If the  path  reaches  a  leaf  to  the  right  of 
no,  then  the  path  to  no  must  be  at  least  as  long.  This  follows  because  in 

a  leftmost  derivation  the  leftmost  nonterminal  is  always  rewritten.  Thus 
the  direct  ancestor  of  each  node  to  the  right  of  no  is  an  ancestor  of no.  The 

derivation  tree  D  is  shown  in  Fig.  4.5. 

A 

w  no  Fig. 4.5  Derivation tree D. 
Thus,  if there  is  a  path  of length  greater  than  k(n  +  2)  in  D,  we  can  find 
one  such  path  which  reaches  n o or  a  node  to  its  left.  Then  we  can  find  k  4--  1 
consecutive  nodes,  say  n l , . . . ,   nk+l,  on  the  path  such  that  each  node  yields 

the  same  portion  of  wB.  All  the  direct  descendants  of n,.,  1 _~  i <  k,  that  lie 

to  the  left  of ni+ ~ derive  e.  We  must  thus  be  able  to  find  two  of n a , . . . ,   nk+ ~ 

with  the  same  label,  and  this  label  is  easily  shown  to  be  a  left-recursive 

nonterminal. We  may  conclude  that  D  has  no  path  of  length  greater  than  k(n  --1- 2). 

Let  l  be  the  length  of the  longest  right  side  of  a  production.  Then  D  has  no 

i more  than  l k~"+2~  interior  nodes.  We  conclude  that  if  A  ~  wBt~,  then 

i  ~  I kC"+2). Choosing  c  =  l k proves  the  lemma.  [Z] 
COROLLARY 

Let  G  =  (N,  ~,  P,  S)  be  a  non-left-recursive  grammar.  Then  there  is 

i a  constant  c'  such  that  if S  ~  wBe  and  w  ~  e,  then  [el  <_  c'[ w 1. 

lm 
Proof.  Referring  to  Fig.  4.5,  we have  shown that  the  path  from the root to 
n o is no longer than k(I w l +  2).  Thus,  [e[_~ kl(I w[  -q- 2).  Choose  c' =  3kl.  E] 

LEMMA 4.2 

Let  G  =  (N,  Z ,P ,  S)  be  a  C F G   with  no  useless  nonterminals  and 
w  =  ala 2  . . .   a,  an  input  string  in Z*. The  sequence  of  consistent  left  parses 
for  w is  finite  if and  only  if  G  is  not  left-recursive. 

296  GENERAL PARSING METHODS  CHAP. 4 

Proof.  If  G  is  left-recursive,  then  clearly  the  sequence  of  consistent  left 
parses is infinite for some terminal  string.  Suppose  that  G is not left-recursive. 

Then each consistent left parse is of length at most c ~+ 2, for some c, by Lemma 
4.1.  There  are  thus  a  finite  number  of  consistent  left  parses.  U 

DEFINITION 

Let  G  =  (N,  2~, P,  S)  be  a  CF G  and  ?  a  sequence  of  subscripted  non- 
terminals  (indices  for  alternates)  and  terminals.  Let  n  be  a  partial  left  parse 

consistent  with  w.  We  say  that  ?? describes  rc if the  following  holds" 

(1)  Let 7t=p~  . . . p , ,   and  S  =  ~0 p,=->~l p~==~0~ 2 . . .   P~==> ~k" Let ~  =  xtfl~, 
where  fl~ is  e  or  begins  with  a  nonterminal. 

(2)  Then  7' =  Ailw~Ai~w2  "'"  At~wk,  where  At,  is  the  index  for  the  pro- 
duction  applied  going  from  0cj_~ to  ~j,  and  wj  is  the  suffix  of  xj  such  that 
xj  =  x~_ t wj. 

LEMMA 4.3 

Let  G  =  (N,  2~, P,  S)  be  a  non-left-recursive  grammar  and  no,  zc~, . . . ,  
z ~ , . . ,   be  the  sequence  of  consistent  partial  left  parses  for  w.  Suppose  that 
none  of  7t0 . . . .   , z~t are  left  parses  for  w.  Let  S  ~,==> ~  and  S  ~,+'==~ ft.  Write 

and  fl,  respectively,  as  x ~   and  yfll,  where  ~  and  ]?~  are  each  either  e  or 
begin  with  a  nonterminal.  Then  in  Algorithm  4.1, 

(q,  I,  e,  S$) }---- (q, j~, 71, ~1 $) t--- (q, J2,  7,2, fl~ $), 
where  Jl  =  lxl  +  1,  Jz  =  l y l +   1,  and  71  and  7'2  describe  z~  and  rot+l, respectively. 

Proof.  The  proof  is  by  induction  on  i.  The  basis,  i  =  0,  is  trivial.  For 
the  inductive  step,  we  need  to  consider  two  cases. 

Case  1:  zt~+l is  a  continuation  of  zc~. Let  ~1  have  first  symbol  A,  with 
alternates  f l a , . . . ,   fie.  If xAflj  is  not  consistent  with  the  input,  then  should 
Algorithm  4.1  replace  A  by  flj,  rules  (d),  (e),  and  (fi)  ensure  that  the  alter- 
nate  fli+~  will  be  the  next  expansion  tried.  Since  we  assumed  zct+l  to  be 
a  continuation  of  ztt, the  desired  alternate  for  A  will  subsequently  be  tried. 
After  input  symbols  are  shifted  by  rule  (b),  configuration  (q, Jz,  7,z, fl15)  is 
reached. 

Case  2:  Suppose  that  n~+~ is  a  modification  of rci. Then  all  untried  alter- 
nates  for  A  immediately  lead  to  backtracking,  and  by  rules  (e)  and  (fiii), 

the  contents  of L1  will eventually  describe  7~j, the  partial  left parse  mentioned 

in  the  definition  of  a  modification.  Configuration  (q, J2,  7,2, fl15)  is  then 
reached  as  in  case  1.  [Z] 

SEC.  4.1  BACKTRACK  PARSING  297 

THEOREM 4.1 

Algorithm  4.1  produces  a  left  parse  for  w  if  one  exists  and  otherwise 
emits  an  error  message. 

Proof.  From  Lemma  4.3  we  see  that  the  algorithm  cycles  through  all 
consistent  partial  left  parses  until  either  a  left  parse  is  found  for  the  input 
or  all  consistent partial  left  parses  are  exhausted.  From  Lemma  4.2 we  know 
that  the  number  of  partial  left  parses  is  finite,  so  the  algorithm  must  even- 

tually  terminate.  [[] 

4.1.4.  Time  and  Space  Complexity  of  the 

Top-Down  Parser 

Let  us  consider  a  computer  in  which  the  space  needed  to  store  a  con- 
figuration  of  Algorithm  4.1  is  proportional  to  the  sum  of  the  lengths  of 
the  two  lists,  a  very  reasonable  assumption.  It  is  also  reasonable  to  assume 

that  the  time  spent  computing  configuration  C 2  from  C I,  if  C 1 l:-C2,  is 
a  constant,  independent  of the  configurations  involved.  Under  these  assump- 
tions,  we  shall  show  that  Algorithm  4.I  takes  linear  space  and  at  most expo- 

nential  time  as  functions  of  input  length.  The  proofs  require  the  following 

lemma,  a  strengthening  of  Lemma  4.1. 

LEMMA 4.4 

Let  G  =  (N,  X, P,  S)  be  a  non-left-recursive  grammar.  Then  there  exists 

i a  constant  c  such  that  if A ==~ 0~ and  10~l ~  1, then  i ~  c] 0~ I- 

t Proof.  By  Lemma  4.1,  there  is  a  constant  c l  such  that  if  A ==~ e,  then 

i ~  c 1. Let  ~ N   =  k  and  let  l be the  length  of the  longest right side  of a  pro- 
duction.  By  Lemma  2.16  we  can  express  N  as  {A o, A ~ , . . . ,   Ak_~} such  that 

+ if A t ==~ A~0~, then j  >  i.  We  shall prove the following  statement  by induction 

on  the  parameter p  =  kn  - - j "  

t (4.1.1)  If Aj ~  g  and  1~1  =  n  ~  1,  then  i ~  klc~] oc]  --  jlc~ 

Basis.  The  basis,  p  =  0,  holds  vacuously,  since  we  assume  that  n  ~  1. 
Induction.  Assume  all  instances  of (4.1.1)  such  that  kn  - - j   <  p  are  true. 
Now  consider  a  particular  instance  with  k n -   j  =  p.  Let  the  first  step  in 
the  derivation  be  A j = = ~ X i . . .   X~,  for  r  ~  I.  Then  we  can  write  g  = 

0ct  . . .   g,  such  that  Xm =~  g~,  1 ~  m  ~  r,  and  i  =  1  +  i~  +  . . .   +  ira. Let 

g~  =  0c2 . . . . .   g,_ ~ =  e,  and  0c~ ~  e.  Since  ~  ~  e,  s  exists. 

Case  I " X ,   is  a  nonterminal,  say  Ag.  Then  Aj==~. AgX,+~  . . - X , ,   so 
g  > j .   Since  k10~, [ - - g   <  p,  we  have  by  (4.1.1),  i,  ~  klc~[oc, I -   glc t.  Since 

[0Cm] <  Igl,  for  s  +  1 ~  m  ~  r,  we  have  by  (4.1.1)  that  i ~ k l c ~ l ~ z s i w h e n -  

298  GENERAL  PARSING  METHODS  CHAP.  4 
ever  s  --k  1 <  m  <  r  and  am  ~  e.  Certainly  at  most  l  --  1 of a ~ , . . . ,   ar  are 
e,  so  the  sum  of im over  those  m  such  that  am  =  e  is  at  most  (l  --  1)c 1. Thus, 

i =   1 -q-J1 +  . . .   -q-it 

<  1 +  ( l -   1)ct  +  k/c~  I~1-gZc~ 
<  kZc~ I~1--  (g  --  1)Zci 
<  kZcxl ~I  --  jZc~. 

Case  2:  Xs  is  a  terminal.  It  is  left  for  the  Exercises  to  show  that  in  this 
case  i <  1 -q- ktci(Itx  [ --  1) ~  klcl  I~1 - j l c , .  

i We  conclude  from  (4.1.1)  that  if  S  ==~ a  and  l a]  ~  1,  then  i_< k/c 1 I~1. 

Let  c  =  k/c I  to  conclude  the  lemma.  E] 

COROLLARY  1 

Let  G  =  (N,  ~, P,  S)  be  a  non-left-recursive  grammar.  Then  there  exists 

i a  constant  c'  such  that  if S  =-~ wAa  and  w ~  e,  then  i  <  c'[w  I. 

lm 
Proof.  By  the  corollary  to  Lemma  4.1,  there  is  a  constant  c"  such  that 
la [ ~  c"] w 1.  By  Lemma  4.4,  i_~  c[ wAoc [.  Since  I wan[  ~  (2  -q- c") [w l, the 
choice  c'  =  c(2  -q-- c")  yields  the  desired  result.  ~  . 

COROLLARY 2 

Let  G  =  (N,  ~, P,  S)  be  a  non-left-recursive  grammar.  Then  there  is 
a  constant  k  such  that  if  7t is  a  partial  left  parse  consistent  with sentence  w, 
and  S " = ~   xe,  where  e  is  either  e  or  begins  with  a  nonterminal,  then I~1 _< k(Iwl  +  ~). 

Proof.  If  x  -~  e,  then  by  Corollary  1,  we  have  17tl _~ c'lx 1.  Certainly, I xt_<lwl,  so  l nl _< 

c'lwl.  As  an  exercise,  we  can  show  that  if  x  =  e,  then l r~l  _< c'. 
Thus,  17tl _< c'(lw[  +  1)  in  either  case.  [Z] 

THEOREM 4.2 

There  is  a  constant  c  such  that  Algorithm  4.1,  with  input  w  of  length 
n  ~  1, uses  no  more  than  cn  cells,  if one  cell  only is  needed  for  each  symbol 

on  the  two  lists  of the  configurations. 

Proof.  Except  possibly  for  the  last  expansion  made,  list  L2  is  part  of 
a  left-sentential  form  a  such  that  S  ~=-~ 0c,  where  7~ is  a  partial  left  parse 
consistent  with  w.  By  Corollary  2  to  Lemma  4.4,  I nl  _<  k(l w l +  1).  Since 
there  is  a  bound  on  the  length  of the  right  side  of any  production,  say/,  we 
know  that  i a  l <_ kl(i  w l -+-  1)  <  2kl  I w !.  Thus  the  length  of L2  is  no  greater 
than  2kl l w I -q-  l  --  1  <  3kl l w 1. 

List  L1  consists  of  part  of  the  left-sentential  form  ~  (most  or  all  of  the 

SEC. 4.1  BACKTRACK PARSING  299 
terminal  prefix)  and  in[  indices.  It  thus  follows  by  Corollary  2  to  Lemma 

4.4  that  the  length  of L1  is at  most  2k(!  /  1)lw [. The  sum  of the  two lengths 
is  thus  proportional  to  lw 1. 

THEOREM 4.3 

There is a constant  c such that Algorithm  4.1,  when its input  w is of length 
n  ~>  1,  makes  no  more  than  e"  elementary  operations,  provided  the  calcu- 
lation  of  one  step  of  Algorithm  4.1  takes  a  constant  number  of elementary 
operations. 

Proof.  By  Corollary  2,  every  partial  left  parse  consistent  with  w  is  of 
length  at  most  c~n  for  some  ca.  Thus  there  are  at  most  e~  different  partial 
left  parses  consistent  with  w  for  some  constant  e z.  Algorithm  4.1  computes 

at  most  n  configurations  between  configurations  whose  contents  of  L1 
describe  consecutive  partial  left  parses.  The  total  number  of  configurations 

computed  by  Algorithm  4.1  is  thus  no  more  than  nel.  From  the  binomial 
theorem the  relation  nc~  <  (c 2 -+- 1)" is immediate.  Choose  c to be (c 2 +  1)m, 
where  m  is the  maximum  number  of elementary  operations  required  to  com- 
pute  one  step  of Algorithm  4.1.  U 

Theorem  4.3  is  in  a  sense  as  strong  as  possible.  That  is,  there  are  non- 
left-recursive  grammars  which  cause  Algorithm  4.1  to  spend  an  exponential 

amount  of time,  because  there  are  c" partial  left  parses  consistent  with  some 
words  of length  n. 

Example 4.3 

Let  G  =  ({S}, {a, b}, P,  S),  where  P  consists  of  S  --~ aSS l e.  Let  X(n)  be 
the  number  of  different  leftmost  parses  of a",  and  let  Y(n)  be  the  number  of 

partial  left  parses  consistent  with  a".  The  following  recurrence  equations 
define  X(n)  and  Y(n): 

(4.1.2) 

(4.1.3) 

X(0)  =  i 

n-I X ( n ) - -   Z  X ( i ) X ( n -   1 - - i )  

i=0 
Y(0)----  2 

n-I Y(n)  =  Y ( n -   11 +  ~]  X(i) Y ( n -   1  --  i) 

i=0 

Line  (4.1.2)  comes  from  the  fact  that  every  derivation  for  a  sentence  a" 
with  n  ~  1 begins  with  production  S  ---,  aSS.  The  remaining  n  --  1  a's  can 
be  divided  any  way  between  the  two  S's.  In  line  (4.1.3),  the  Y(n  --  1)  term 

corresponds  to  the  possibility  that  after  the  first  step  S  =~ aSS,  the  second 

S  is  never  rewritten;  the  summation  corresponds  to  the  possibility  that 
the first S derives a t for  some  i. The formula  Y(0)  --  2 is from the  observation 

:300  GENERAL  PARSING  METHODS  CHAP.  4 

that  the null derivation  and  the derivation  S :::> e are consistent with  string e. 

From  Exercise  2.4.29  we  have 

X(n)--  1  (2~) n + l  

so  Xfn) :> 2"-1.  Thus 

n-1 Y(n) >  Y ( n -   1 ) +   ~]  2'-1 Y ( n -   1 - - i )  

i=0 

from  which  Y(n) >  2" certainly  follows.  [-7 

This  example  points  out  a  major  problem  with  top-down  backtrack  pars- 
ing.  The  number  of  steps  necessary  to  parse  by  Algorithm  4.1  can  be  enor- 
mous.  There  are  several  techniques  that  can  be  used  to  speed  this  algorithm 

somewhat.  We  shall  mention  a  few  of them  here. 

(1)  We  can  order  productions  so  that  the  most  likely alternates  are  tried 
first.  However,  this  will  not  help  in  those  cases  in  which  the  input  is  not 

syntactically  well formed,  and  all  possibilities  have  to  be  tried. 

DEFINITION 

For  a  CFG  G  --  (N,  Z, P,  S), 

FIRST k(a)  --  [x t~ ::~ xfl  and  [xl  =  k  or  a  =:~ x  and  i xl  <  k}. lm 
That  is,  FIRSTk(a )  consists  of  all  terminal  prefixes  of  length  k  (or  less  if  a 
derives  a  terminal  string  of  length  less  than  k)  of  the  terminal  strings  that 
can  be  derived  from  ~. 

(2)  We can  look ahead  at  the  next  k  input  symbols to  determine  whether 
a  given  alternate  should  be  used.  For  example,  we  can  tabulate,  for  each 
alternate  a,  a  lookahead  set  FIRSTk(a ).  If no  prefix  of the  remaining  input 

string  is  contained  in  FIRSTk(a),  we  can  immediately  reject  a  and  try  the 
next  alternate.  This  technique  is  very  useful  both  when  the  given  input  is 
in L(G) and  when it is not  in L(G). In  Chapter  5 we  shall  see that  for  certain 
classes  of  grammars  the  use  of  lookahead  can  entirely  eliminate  the  need 
for  backtracking. 

(3)  We  can  add  bookkeeping  features  which  will  allow  faster  backtrack- 
ing.  For  example,  if  we  know  that  the  last  m-productions  applied  have  no 

applicable  next  alternates,  when  failure  occurs  we  can  skip  back  directly  to 
the  position  where  there  is  an  applicable  alternate. 

(4)  We  can  restrict  the  amount  of  backtracking  that  can  be  done.  We 
shall  discuss  parsing  techniques  of this  nature  in  Chapter  6. 

Another  severe  problem  with  backtrack  parsing  is its poor  error-locating 
capability.  If an  input  string is not  syntactically well formed,  then  a  compiler 

SEC. 4.1  BACKTRACK PARSING  301 
should  announce  which  input  symbols  are in error.  Moreover,  once  one  error 
has  been  found,  the  compiler  should  recover  from  that  error  so  that  parsing 
can  resume  in  order  to  detect  any  additional  errors  that  might  occur. 

If the  input  string  is  not  syntactically  well  formed,  then  the  backtracking 
algorithm  as  formulated  will  merely  announce  error,  leaving  the  input 
pointer  at  the  first input  symbol.  To  obtain  more  detailed  error  information, 
we  can  incorporate  error  productions  into  the  grammar.  Error  productions 

are  used  to  generate  strings  containing  common  syntactic  errors  and  would 

make  syntactically  invalid  strings  well  formed.  The  production  numbers  in 
the  output  corresponding  to  these  error  productions  can  then  be  used  to 

signal  the  location  of  errors  in  the  input  string.  However,  from  a  practical 
point  of  view,  the  parsing  algorithms  presented  in  Chapter  5  have  better 
error-announcing  capabilities  than  backtracking  algorithms  with  error  pro- 

ductions. 

4.1.5.  Bottom-Up  Parsing 

There  is a  general  approach  to  parsing  that  is  in  a  sense  opposite  to  that 
of  top-down  parsing.  The  top-down  parsing  algorithm  can  be  thought  of 
as  building  the  parse  tree  by  trial  and  error  from  the  root  (top)  and  proceed- 

ing  downward  to  the  leaves.  Its  opposite,  bottom-up  parsing,  starts  with 
the  leaves  (i.e.,  the  input  symbols  themselves)  and  attempts  to  build  the  tree 

upwards  toward  the  root. We  shall  describe  a  formulation  of bottom-up  parsing  that  is called  shift- 

reduce  parsing.  The  parsing  proceeds  using  essentially  a  right  parser  cycling 

through  all  possible  rightmost  derivations,  in  reverse,  that  are  consistent 

with the  input.  A  move consists  of scanning the string on top  of the pushdown 
list  to  see  if  there  is  a  right  side  of  a  production  that  matches  the  symbols 

on  top  of the  list.  If so,  a  reduction  is  made,  replacing  these  symbols  by  the 
left  side  of the  production.  If more  than  one  reduction  is  possible,  we  order 
the  possible  reductions  in  some  arbitrary  manner  and  apply  the  first. 

If no  reduction  is possible,  we  shift  the  next  input  symbol  onto  the  push- 
down  list  and  proceed  as  before.  We  shall  always  attempt  to  make  a  reduc- 
tion  before  shifting.  If we  come  to  the  end  of the  string  and  no  reduction  is 
possible,  we  backtrack  to  the  last  move  at  which  we  made  a  reduction.  If 

another  reduction  was  possible  at  that  point  we  try  that. 

Let  us  consider  a  grammar  with  productions  S--~  AB,  A - - .   ab,  and 
B --~ aba. Let the input string be ababa. We would  shift  the  first a  on  the push- 

down  list.  Since  no  reduction  is  possible,  we  would  then  shift  the  b  on  the 
pushdown  list.  We  then  replace  ab  on  top  of the  pushdown  list  by A.  At  this 

point  we  have  the  partial  tree  of  Fig.  4.6(a). As  the  A  cannot  be  further  reduced,  we  shift  a  onto  the  pushdown  list. 

Again  no  reduction  is possible,  so  we  shift  b  onto  the  pushdown  list.  We  can 

then  reduce  ab  to  A.  We  now  have  the  partial  tree  of  Fig.  4.6(b). 

302  GENERAL PARSING  METHODS 

(a) 

a  b  a 

/~.b  a/A~b 

(b) 
A  B 

a  b  a  b  a 

(c) Ajs  

a/ ~b  a/!~a 

(d) 

CHAP. 4 
Fig.  4.6  Partial  parse  trees  in  bottom- 

up  parse. 

We  shift a  on the pushdown  list  and  find  that  no  reductions  are  possible. 
We then backtrack  to the last position at which we made  a reduction,  namely 
where  the  pushdown  list  contained  Aab  (b  is  on  top  here)  and  we  replaced 
ab  by  A,  i.e.,  when  the  partial  tree  was  that  of  Fig.  4.6(a).  Since  no  other 
reduction  is  possible,  we  now  shift  instead  of  reducing.  The  pushdown  list 
now  contains  Aaba.  We  can  then  reduce  aba  to  B,  to  obtain  Fig.  4.6(c). 
Next,  we replace AB by S and thus have a complete tree,  shown in Fig. 4.6(d). 

This method can be viewed as considering all possible sequences  of moves 
of  a  nondeterministic  right  parser  for  a  grammar.  However,  as  with  top- 
down  parsing,  we  must  avoid  situations  in  which  the  number  of  possible 
moves is  infinite. 

One  such  pitfall  occurs  when  a  grammar  has  cycles,  that  is,  derivations 

+ of  the  form  A  ~  A  for  some  nonterminal  A.  The  number  of  partial  trees 

can  be  infinite  in  this  case,  so  we  shall  rule  out  grammars  with  cycles.  Also, 

e-productions  cause  difficulty,  since  we  can  make  an  arbitrary  number  of 
reductions  in which the  empty string is  "reduced"  to  a  nonterminal.  Bottom- 

up  parsing  can  be  extended  to  embrace  grammars  with  e-productions,  but 
for  simplicity  we  shall  choose  to  outlaw  e-productions  here. 

sEc. 4.1  BACKTRACK PARSING  303 
ALGORITHM 4.2 

Bottom-up  backtrack  parsing. 
Input.  CFG  G  =  (N,  E, P,  S)  with  no  cycles  or  e-productions,  whose 
productions  are numbered  1 to p,  and  an  input  string  w  =  alaz  ..  * a,,  n  ~  1. 

Output.  One right  parse  for  w if one  exists.  The  output  "error"  otherwise. 
Method. 

(1)  Order  the  productions  arbitrarily. (2)  We  shall  couch  our  algorithm  in  the  4-tuple  configurations  similar 

to  those  used  in  Algorithm  4.1.  In  a  configuration  (s,  i,  a,  fl) 

(a)  s  represents  the  state  of the  algorithm. 
(b)  i represents  the  current  location  of the  input  pointer.  We  assume 

the  n  +  1st  input  symbol  is  $,  the  right  endmarker. 
(c)  a  represents  a  pushdown  list L1  (whose  top  is  on  the  right). 
(d)  fl  represents  a  pushdown  list  L2  (whose  top  is  on  the  left). 
As  before,  the  algorithm  can  be  in  one  of three  states  q,  b,  or  t. L1  will  hold 

a  string  of  terminals  and  nonterminals  that  derives  the  portion  of  input  to 

the  left of the input  pointer.  L2 will hold  a  history  of the  shifts  and  reductions 

necessary  to  obtain  the  contents  of L1  from  the  input. 

(3)  The  initial  configuration  of the  algorithm  is  (q,  1,  $,  e). 

(4)  The  algorithm  itself is  as  follows.  We  begin  by  trying  to  apply  step  1. 

Step  1"  Attempt  to  reduce 

(q,  i,  ~fl,  ?)  ~  (q,  i, ~A, j~,) 
provided  A  --~ fl  is  the jth  production  in  P  and  fl  is  the  first  right  side  in 

the  linear  ordering  in  (1)  that  is  a  suffix  of  aft.  The  production  number  is 

written  on  L2.  If  step  1 applies,  return  to  step  1.  Otherwise  go  to  step  2. 

Step  2:  Shift 

(q,  i, oc, r)  [--  (q,  i  -k  1, oca~, s?) 

provided  i :/= n  -k  1.  Go  to  step  1. 

If  i  --  n  -+-  1,  instead  go  to  step  3. If  step  2  is  successful,  we  write  the  ith  input  symbol  on  top  of L1,  incre- 

ment  the  input  pointer,  and  write  s  on  L2,  to  indicate  that  a  shift  has  been 
made. 

Step  3:  Accept 

(q,  n  --Jr- 1,  $S,  ?)  ~  (t,  n  --t- 1,  $S,  ?) 

Emit  h(?),  where  h  is  the  homomorphism 

304  GENERAL PARSING  METHODS  CHAP.  4 

h(s)  =  e 
h(j)  =  j  for  all  production  numbers 

h(7)  is  a  right  parse  of  w in  reverse.  Then  halt. 

If step  3 is  not  applicable,  go  to  step  4. 

Step  4"  Enter  backtracking  mode 

(q,n  +  1, ~x, ~)  I--  (b,n  +  1, o~, 7) 
provided  ~  -~  $S.  Go  to  step  5. 

Step  5"  Backtracking 

(a)  (b,  i, ~A, Jr)  ~  (q,  i,  rE'B, k~,) 
if the jth  production  in P  is  A  ----~ fl  and  the  next  production  in  the  ordering 

of  (1)  whose  right  side  is  a  suffix  of  ~fl  is  B  ~  fl',  numbered  k.  Note  that 

t~fl =  t~'fl'.  Go  to  step  1.  (Here  we  have  backtracked  to  the  previous  reduc- tion,  and  we  try  the  next  alternative  reduction.) 

(b)  (b,  n  +  1, ~A, jy)  ~  (b, n  -q-  1, t~fl, y) 
if  the jth  production  in  P  is  A  ---~ fl  and  no  other  alternative  reductions  of 

~fl remain.  Go to  step  5. (If no  alternative  reductions  exist,  "undo"  the reduc- 
tion  and  continue  backtracking  when  the  input  pointer  is  at  n  q-  1.) 

(c)  (b,  i,  ~A, Jr)  ~  (q,  i  --k  1,  ~fla,  sT) 
if  i ~  n  -t-  1,  the  jth  production  in  P  is  A  ~  fl,  and  no  other  alternative 
reductions  of ~fl  remain.  Here  a  =  a~ is  shifted  onto  L1,  and  an  s  is  entered on  L2.  Go  to  step  1. 

Here  we  have  backtracked  to  the  previous  reduction.  No  alternative reductions  exist,  so  we  try  a  shift  instead. 

(d)  (b,  i, ~a, sy)  ~  (b,  i  --  1, t~, r) 
if the  top  entry  on L2  is  the  shift  symbol.  (Here  all  alternatives  at  position  i 

have  been  exhausted,  and  the  shift  action  must  be  undone.  The  input  pointer 
moves  left,  the  terminal  symbol  a t  is  removed  from  L1  and  the  symbol  s  is 
removed  from  L2.)  D 

Example 4.4 

Let  us  apply  this  bottom-up  parsing  algorithm  to  the  grammar  G  with productions 

sEc.  4.1  BACKTRACK PARSING  305 

(1)  E - - - ~   E  +  T 
(2)  E  ~ T  
(3)  T  ~ T ,   F 

(4)  T~  ~ F  
(5)  F ~  ~  a 

If E  +  T appears  on  top  of L1,  we shall first try reducing,  using E  ---~ E  +  T and  then  using  E  ~  T.  If  T .   F  appears  on  top  of  L1,  we  shall  first  try 

T----, T ,   F  and then  T---. F.  With  input  a *  a the bottom-up  algorithm would 
go  through  the  following  configurations: 

(q,  1,  $, e)  ~  (q, 2,  $a, s) 

}-- (q, 2,  $F,  5s) 

(q, 2,  ST, 45s) 
~-  (q,  2,  $E,  245s) 
1-- (q,  3,  $E ,,  s245s) 

1-.- (q, 4,  $E *  a, ss245s) 

(q, 4,  $E *  F,  5ss245s) 
t--  (q, 4,  S E .   T,  45ss245s) 
I--  (q, 4,  $E *  E,  245ss245s) 

t--  (b, 4,  $ E ,   E,  245ss245s) 
l-  (b, 4,  SE *  T,  45ss245s) 

}--- (b, 4,  $E *  F,  5ss245s) 
t-  (b, 4,  $E *  a, ss245s) 
1--- (b,  3,  $ E . ,   s245s) 

(b, 2,  $E,  245s) 
t--- (q,  3,  $T,,s45s) 
t--- (q, 4,  $T *  a, ss45s) 

(q, 4,  $ T ,   F,  5ss45s) 

(q, 4,  ST,  35ss45s) 
(q, 4,  $E,  235ss45s) 
I--- (t, 4,  $E, 235ss45s)  E] 

We  can  prove  the  correctness  of  Algorithm  4.2  in  a  manner  analogous 
to  the  way  we  showed  that  top-down  parsing  worked.  We  shall  outline a  proof  here,  leaving  most  of the  details  as  Exercises. 

306  GENERAL PARSING METHODS  CHAP. 4 

DEFINITION 

Let  G  =  (N,  E, P,  S)  be  a  CFG.  We  say  that  z~ is  a  partial  right parse consistent with  w 

if there  is some  g  in  (N  W E)*  and  a  prefix  x  of w such that 

LEMMA 4.5 

Let  G be a cycle-free CFG with no  e-productions.  Then there is a constant 
c  such  that  the  number  of  partial  right  parses  consistent  with  an  input  of length  n  is  at  most  c". 

Proof.  Exercise.  [~] 
THEOREM  4.4 

Algorithm  4.2  correctly  finds  a  right  parse  of w if one  exists,  and  signals an  error  otherwise. 

Proof  By  Lemma  4.5,  the  number  of  partial  right  parses  consistent  with 
the  input  is finite.  It  is left  as  an  Exercise  to  show  that  unless  Algorithm  4.2 
finds  a  parse,  it  cycles  through  all  partial  right  parses  in  a  natural  order. 
Namely,  each  partial  right  parse  can  be  coded  by  a  sequence  of production 
indices  and  shift  symbols  (s).  Algorithm  4.2  considers  each  such  sequence 

that  is a partial  right parse in a  lexicographic  order.  That  lexicographic  order 
is  determined  by  an  order  of  the  symbols,  placing  s  last,  and  ordering  the 
production indices as in step  1 of Algorithm 4.2. Note that not every sequence 

of such  symbols  is  a  consistent  partial  right  parse.  [-7 

Paralleling  the  analysis  for  Algorithm  4.1,  we  can  also  show  that  the 
lengths  of the  lists  in  the  configurations  for  Algorithm  4.2  remain  linear  in the  input  length. 

THEOREM 4.5 

Let  one  cell  be  needed  for  each  symbol  on  a  list  in  a  configuration  of 
Algorithm  4.2,  and  let  the  number  of elementary  operations  needed  to  com- 
pute  one  step  of Algorithm  4.2  be  bounded.  Then  for  some  constants  c a and 

c 2, Algorithm  4.2  requires  can space  and  c~ time,  when given input  of length n ~ l .  

Proof  Exercise.  [~] 

There  are  a  number  of modifications we can  make to the basic bottom-up 
parsing  algorithm  in  order  to  speed  up  its  operations" 

(1)  We  can  add  "lookahead"  so  that  if we  find  that  the  next  k  symbols 
to  the  right  of the  input  pointer  could  not  possibly follow  an  A in any right- 

sentential  form,  then  we  do  not  make  a  reduction  according  to  any  A-pro- duction. 

EXERCISES  307 
(2)  We  can  attempt  to  order  the  reductions  so  that the most likely reduc- 
tions  are  made  first. 

(3)  We  can  add  information  to  determine  whether  certain  reductions 
will  lead  to  success.  For  example,  if the  first  reduction  uses  the  production 

A  ~  ax  . . .   ak,  where  a~  is  the  first  input  symbol  and  we  know that there  is 

no  y  in  Z*  such  that  S  ~  Ay,  then  this  reduction  can  be immediately ruled 

out.  In  general  we  want  to  be  sure  that  if  $~  is  on  L1,  then  ~  is  the  prefix 
of  a  right-sentential  form.  While  this  test  is  complicated  in  general,  certain 
notions,  such  as  precedence,  discussed  in  Chapter  5,  will make  it easy to rule 

out  many  ~'s  that  might  appear  on L1. 

(4)  We  can  add  features  to  make  backtracking  faster.  For  example,  we 
might  store  information  that  will  allow  us  to  directly  recover  the  previous 
configuration  at  which  a  reduction  was  made. 

Some  of these  considerations  are  explored  in  Exercises  4.1.12-4.1.14 and 
4.1.25.  The  remarks  on  error  detection  and  recovery  with  the  backtracking 
top-down  algorithm  also  apply  to  the  bottom-up  algorithm. 

EXERC ISES 
4.1.1.  Let G be  defined by 

S----~  ASIa 
A  ~  bSA[b 

What  sequence  of steps  is taken  by Algorithm 4.1  if the  order  of alter- 
nates is as shown,  and the input is 

(a)  ba? 
(b)  baba ? What are the sequences  if the  order of alternates  is reversed ? 

4.1.2.  Let  G be the  grammar 

S---->  SAIA 
A  ~  aAlb 

What  sequence  of  steps  are  taken  by  Algorithm  4.2  if  the  order  of 
reductions  is longest first,  and  the  input  is 

(a)  ab ? (b) 

abab ? What  if the  order of choice is shortest first ? 

4.1.3.  Show  that  every  cycle-free  CFG  that  does  not  generate  e  is  right- covered  by  one  for  which  Algorithm  4.2  works,  but  may  not  be  left- 

covered  by  any  for  which  Algorithm  4.1  works. 
"4.1.4.  Show that  the  solution to the recurrence 

308  GENERAL PARSING  METHODS  CHAP.  4 

D(1)  =  1 
D(d)  =  ( D ( d -   1) 2)  +  1 

is  D(d)  =  [k~],  where  k  is  a  real  number  and  [x] is  the  greatest  integer 

_< x.  Here,  k  =  1.502837  . . . .  

4.1.5.  Complete  the  proof  of Corollary  2  to  Lemma  4.4. 
4.1.6.  Modify  Algorithm  4.1  to  refrain  from  using  an  alternate  if it  is  impos- 

sible  to  derive  the  next  k  input  symbols,  for  fixed  k,  from  the  resulting 
left-sentential form. 

4.1.7.  Modify  Algorithm  4.1  to  work  on  an  arbitrary  grammar  by  putting 

bounds  on  the  length  to  which  L1  and  L2  can  grow. 

*'4.1.8.  Give  a  necessary  and  sufficient  condition  on  the  input  grammar  such 

that  Algorithm  4.1  will  never  enter  the  backtrack  mode. 

4.1.9.  Prove  Lemma  4.5. 
4.1.10.  Prove  Theorem  4.5. 

4.1.12. 

4.1.11.  Modify  Algorithm  4.2  to  work  for  an  arbitrary  CFG  by  bounding  the 

length  of the  lists L1  and  L2. 

Modify  Algorithm  4.2  to  run  faster  by  checking  that  the  partial  right 
parse  together  with  the  input  to  the  right  of  the  pointer  does  not  con- 
tain  any  sequence  of  k  symbols  that  could  not  be  part  of  a  right- 

sentential  form. 

4.1.13.  Modify  Algorithms  4.1  and  4.2  to  backtrack  to  any  specially designated 

previous  configuration  using  a  finite  number  of  reasonably  defined 
elementary operations. 

*'4.1.14.  Give  a  necessary  and  sufficient  condition  on  a  grammar  such  that 

Algorithm  4.2  will  operate  with  no  backtracking.  What  if  the  modifi- 

cation  of  Exercise  4.1.12  is  first  made? 

4.1.15.  Find  a  cycle-free  grammar  with  no  e-productions  on  which  Algorithm 

4.2  takes  an  exponential  amount  of  time. 

4.1.16.  Improve  the bound  of Lemma  4.4  if the  grammar  has  no  e-productions. 

4.1.17.  Show  that  if  a  grammar  G  with  no  useless  symbols  has  either  a  cycle 

or  an  e-production,  then  Algorithm  4.2  will  not  terminate  on  any 

sentence  not  in  L(G). 

DEFINITION 

We  shall  outline  a  programming  language  in  which  we  can  write 
nondeterministic  algorithms.  We  call  the  language  NDF  (nondeter- 
ministic FORTRAN),  because  it consists of FORTRAN-like  statements 
plus the  statement  CHOICE  (nl . . . . .   nk),  where  k  ~  2  and  nl . . . . .   nk 
are  statement  numbers. 

To  define  the  meaning  of  an  N D F   program,  we  postulate  the 
existence  of  an  interpreter  capable  of  executing  any  finite  number  of 

EXERCISES  309 
programs  in  a  round  robin  fashion  (i.e.,  working  on  the  compiled 
version  of  each  in  turn  for  a  fixed  number  of  machine  operations). 
We  assume  that  the  meaning  of  the  usual  FORTRAN  statements  is 
understood.  However,  if  the  statement  CHOICE  (nl . . . ..   nk)  is  ex- 
ecuted,  the  interpreter  makes  k  copies  of  the  program  and  its  entire 

data  region.  Control  is  transferred  to  statement  nt  in  the  ith  copy  of 
the  program  for  1 ~  i <  k.  All  output  appears  on  a  single  printer, 
and  all  input  is  received  from  a  single  card  reader  (so  that  we  had 
better  read  all  input  before  executing  any  CHOICE  statement). 

Example 4.5 

The  following  N D F   program  prints  the  legend  NOT  A  PRIME 
one  or  more  times  if  the  input  is  not  a  prime  number,  and  prints 
nothing  if it  is  a  prime" 

READ  N 
I = l  
PICK  A  VALUE  OF  I  GREATER  THAN  1 

1  I =I +l  

CHOICE  (1, 2) 
2  IF  (I  .EQ.  N)  STOP 
FIND  IF  I  IS  A  DIVISOR  OF  N  AND  NOT  EQUAL 

TO  N 

IF  ((N/I),  I  .NE.  N)  STOP 
WRITE  ("NOT  A  PRIME") 

STOP 

"4.1.38. 

'4.1.19. 

Write  an  N D F   program  which  prints  all  answers  to  the  "eight  queens 
problem."  (Select  eight  points  on  an  8  x  8  grid  so  that  no  two  lie  on 

any  row,  column,  or  diagonal  line.) 

Write  N D F   programs  to  simulate  a  left  or  right  parser. 

It  would  be  nice  if  there  were  an  algorithm  which  determined  if  a 
given  N D F   program  could  run  forever  on  some  input.  Unfortunately 
this  is  not  decidable  for  FORTRAN,  or  any  other  programming  lan- 
guage.  However,  we  can  make  such  a  determination  if we  assume  that 
branches  (from  IF  and  assigned  GOTO  statements,  not  CHOICE 
statements)  are  externally  controlled  by  a  "demon"  who  is  trying  to 

make  the  program  run  forever,  rather  than  by  the  values  of  the  pro- 

gram  variables.  We  say  an  N D F   program  is  halting  if  for  each  input 
there  is  no  sequence  of  branches  and  nondeterministic  choices  that 

cause  any  copy  of  the  program  to  run  beyond  some  constant  number 

of executed statements,  the  constant  being  a  function  of the  number  of 

310  GENERAL PARSING  METHODS  CHAP.  4 

input  cards  available  for  data.  (Assume  that  the  program  halts  if  it 
attempts  to  read  and  no  data  is  available.) 

* 4.1.20.  Give  an  algorithm  to  determine  whether  an  N D F   program  is  halting 

under  the  assumption  that  no  DO  loop  index  is  ever  decremented. 

"4.1.21.  Give  an  algorithm  which  takes  a  halting  N D F   program  and  constructs 

from  it  an  equivalent  ALGOL  program.  By  "equivalent  program," 
we  have  to  mean  that  the  ALGOL  program  does  input  and  output 

in  an  order  which  the  N D F   program  might  do  it,  since  no  order  for  the 
N D F   program  is known.  ALGOL,  rather  than  FORTRAN,  is preferred 
here,  because  recursion  is  very  convenient. 

4.1.22.  Let  G  =  (N,  ~,  P,  S)  be  a  CFG.  From  G  construct  a  CFG  G'  such 

that  L(G')  =  ~*  and  if S  '~=> w,  then  S  ~==~ w. G  G' 

A  PDT  (with  pushdown  top  on  the  left)  that  behaves  as  a  nondeter- 
ministic  left-corner  parser  for  a  grammar  can  be  constructed  from  the 
grammar.  The  parser  will  use  as  pushdown  symbols  nonterminals, 
terminals,  and  special  symbols  of  the  form  [A,  B],  where  A  and  B  are 

nonterminals. 

Nonterminals  and  terminals  appearing  on  the  pushdown  list  are 
goals  to  be  recognized  top-down.  In  a  symbol  [A, B],  A  is  the  current 
goal  to  be  recognized  and  B  is  the  nonterminal  which  has  just  been 
recognized  bottom-up.  From  a  CFG  G  =  (N,  2~, P,  S)  we can  construct 

a  PDT  M  =  ([q}, E,  N  x  N  W N  W E,  A, ~, q, S,  ~ )   which  will  be  a 
left-corner  parser  for  G.  Here  A  =  [ 1, 2 . . . .   , p} is the  set of production 

numbers,  and  5  is  defined  as  follows: 

(1)  Suppose  that  A  ~  ot is  the  ith  production  in P. 

(a)  If  0t  is  of  the  form  Bfl,  where  B  E  N,  then  O(q, e,  [C, B]) 

contains  (q, fl[C,  A],  i)  for  all  C  ~  N.  Here  we  assume  that 
we  have  recognized  the  left-corner  B  bottom-up  so  we 
establish  the  symbols  in  fl  as  goals  to  be  recognized  top- 

down.  Once  we have recognized  fl,  we  shall  have recognized 
an  A. 
(b)  If  0t  does  not  begin  with  a  nonterminal,  then  O(q, e,  C) 

contains  (q, a[C,  A], i)  for  all  nonterminals  C.  Here,  once 
is  recognized,  the  nonterminal  A  will  have  been  recognized. 
(2)  6(q,  e,  [A,  A])  contains  (q,  e, e)  for  all  A  ~  N.  Here  an  instance 
of  the  goal  A  which  we  have  been  looking  for  has  been  recognized.  If 
this  instance  of A  is  not  a  left  corner,  we  remove  [A, A]  from  the  push- 

down  list  signifying  that  this  instance  of  A  was  the  goal  we  sought. 

(3)  O(q, a,  a ) =   [(q,  e, e)}  for  all  a  ~  E.  Here  the  current  goal  is 
a  terminal  symbol  which  matches  the  current  input  symbol.  The  goal, 
being  satisfied,  is  removed. 

M  defines  the  translation 

[(w, rc)lw  ~  L(G)  and  ~  is  a  left-corner  parse  for  w}. 

EXERCISES  311 
Example 4.6 

Consider  the  C FG  G  =  (N,  ~,  P,  S)  with  the  productions 

(1)  E  ~  E  q-  T 
(2)  E  ~  T 
(3)  T -   > F  t  T 
(4)  T  ~ F  

(5)  F -  > (e) 

(6)  F - -+   a 

A  nondeterministic  left-corner  parser  for  G  is  the  PDT 

M  =  ([q},  ~,  N  x N U N U ~ , { 1 , 2   . . . .   ,6},  ~,q,  E,  Z~) 
where  ~  is  defined  as  follows  for  all  A  ~  N: 

(1)  (a)  5(q,  e,  [A, E])  contains  (q,  /  T[A,  El,  1). 

(b)  O(q, e, [A,  T])  contains  (q,  [A,  E],  2). 

(c)  5(q,  e, [A, F])  contains  (q,  t  T[A,  T],  3)  and  (q,  [A,  T], 4) 
(d)  O(q,  e, A)  =  [(q,  (E)[A,  F],  5),  (q, a[A,  F],  6)}. 
(2)  6(q,  e, [A,  A])  contains  (q,  e,  e). 
(3)  O(q, a,  a)  =  {(q,  e, e)} for  all  a  ~  E. 

Let  us  parse  the  input  string  a  t  a  +  a  using  M.  The  derivation 
tree  for  this  string  is  shown  in  Fig.  4.7.  Since  the  PDT  has  only  one 

state,  we  shall  ignore  the  state.  The  PDT  starts  off  in  configuration 

( a t   a  Jr- a, E,  e) 
The  second  rule  in  (1 d)  is  applicable  (so  is  the  first),  so  the  PDT  can  go 
into  configuration 

(a t  a  -+- a, a[E,  F],  6) 

!  I T  F 
1  1 a  F I 

a  Fig. 4.7  Derivation tree for a  t  a  -k a. 

31 2  GENERAL PARSING  METHODS  CHAP.  4 

Here,  the  left-corner  a  has  been  generated  using  production  6.  The 
symbol  a  is  then  compared  with  the  current  input  symbol  to  yield 

(1" a  +  a,  [E, F],  6) 
We  can  then  use  the  first  rule  in  (lc)  to  obtain 

(1" a  q- a, "1" T[E,  T],  63) 
Here  we  are  saying  that  the  left  corner  of  the  production  T ~  F  1" T 
will  now  be  recognized  once  we  find  t  and  T.  We  can  then  enter  the 

following  configurations: 

(a  +  a,  TIE,  T],  63)  ~--- (a  +  a, a[T, F][E,  T],  636) 

( +   a, [ T, F] [E,  T],  636) 
l--- (+  a, [T,  T][E,  T],  6364) 

At  this  point  T is  the  current  goal  and  an  instance  of  T which  is  not  a 
left  corner  has  been  found.  Thus  using  rule  (2)  to  erase  the  goal,  we 

obtain 

( +   a,  [E,  T],  6364) 

Continuing,  we can terminate with  the  following sequence of configura- 
tions: 

( +   a,  [E, El,  63642)  ~  (+  a,  +  T[E, E],  636421) 1--- (a,  TIE,  El, 

636421) (a, a[T,  F][E,  El, 

6364216) (e, [T, F][E,  E], 
6364216) (e, IT,  T][E,  El, 

63642164) 
(e, [E, E],  63642164) 
F- (e, e, 63642164)  D 

"4.1.23.  Show  that  the  construction  above  yields  a  nondeterministic  left-corner 

parser for  a  CFG. 

4.1.24.  Construct  a left-corner backtrack parsing  algorithm. 

Let  G  =  (N,  X~, P,  S)  be  a  CFG  which  contains  no  production  with 
a  right  side  of  length  0  or  1.  (Every  CFL  L  such  that  w  ~  L  implies 
that [ w I>_ 2 has such a grammar.) A nondeterministic shift-reduce right 
parser  for  G can  be  constructed  such  that  each  entry  on  the  pushdown 
list  is  a  pair  of  the  form  (X,  Q),  where  X  ~  N  u  E  u  {$}  ($  is  an 

endmarker  for  the  pushdown  list)  and  Q  is  the  set  of  productions  P 
with  an  indication  of  all  possible  prefixes  of  the  right  side  of  each 

production  which  could  have  been  recognized  to  this  point.  That  is, 

Q  will  be  P  with  dots  placed  between  some  of  the  symbols  of  the 
right  sides.  There  will  be  a  dot  in  front  of  Xt  in  the  production 

BIBLIOGRAPHIC  NOTES  313 
A----~ X~ Xz  .."  X,  if  and  only  if  X1  -..  X,-1  is  a  suffix  of  the  list  of 
grammar  symbols  on  the  pushdown  list. 

A  shift  move  can  be  made  if  the  current  input  symbol  is  the  con- 
tinuation  of  some  production.  In  particular,  a  shift  move  can  always 

be made if A  ---~ 0c is in P  and the current  input symbol  is in FIRST i(~). 

A  reduce  move  can  be  made  if  the  end  of  the  right  side  of  a  pro- 
duction  has  been  reached.  Suppose  that  A  ~  tz  is  such  a  production. 
To  reduce  we remove  It~f entries  from  the  top  of  the  pushdown  list.  If 
(X,  Q)  is  the  entry  now  on  top  of  the  pushdown  list,  we  then  write 
(A,  Q')  on  the  list,  where  Q'  is  computed  from  Q  by  assuming  that 
an  A  has  been  recognized.  That  is,  Q'  is  formed  from  Q  by  moving  all 
dots  that  are  immediately  to  the  left  of  an  A  to  the  right  of  A  and 

adding  dots  at  the left  end  of the right  sides if not  already  there. 

Example 4.7 

Consider  the  grammar  S--~  Scl ab  and  the  input  string  abc.  Ini- 
tially,  the  parser  would  have  ($,  Q0)  on  the  pushdown  list,  where  Q0 

is  S---~. Scl  .ab.  We  can  then  shift  the  first  input  symbol  and  write 
(a,  Q1)  on  the  pushdown  list,  where  Q1  is  S - - - , .  Sc[. a.b.  Here,  we 
can  be  beginning  the  productions  S  ~  Scl ab,  or  we  could  have  seen 
the  first  a  of  production  S  ~  ab.  Shifting  the  next  input  symbol  b, 

we  would  write  (b,  Qz)  on  the  pushdown  list,  where  Q1  is 
S - - ~ .   Sc]  .ab..  We  can  then  reduce  using  production  S  ~  ab.  The 
pushdown  list  would  now  contain  ($,  Qo)(S,  Q3),  where  Q3  is 

S  ~  . S . c [ . a b .   [[] 

Domolki  has  suggested  implementing  this  algorithm  using  a  binary 
matrix  M  to  represent  the  productions  and  a  binary  vector  V to  store 
the  possible  positions  in  each  production.  The  vector  V can  be  used  in 
place  of  Q  in  the  algorithm  above.  Each  new  vector  on  the  pushdown 
list  can  be  easily  computed  from  M  and  the  current  value  of  V using 

simple  bit  operations. 

4.1.25.  Use  Domolki's  algorithm  to  help  determine  possible  reductions  in 

Algorithm  4.2. 

B I B L I O GR AP H I C   N OT E S  
Many  of  the  early  compiler-compilers  and  syntax-directed  compilers  used 
nondeterministic  parsing  algorithms.  Variants  of  top-down  backtrack  parsing 

methods  were  used  in  Brooker  and  Morris'  compiler-compiler  [Rosen,  1967b]  and 

in  the  META  compiler  writing  systems  [Schorre,  1964].  The  symbolic  program- 
ming  system  COGENT  simulated  a  nondeterministic  top-down  parser  by  carrying 

along  all  viable  move  sequences  in  parallel  [Reynolds,  1965].  Top-down  back- 
tracking  methods  have  also  been  used  to  parse  natural  languages  [Kuno  and 

Oettinger,  1962]. 

314  GENERAL PARSING METHODS  CHAP.  4 

One  of  the  earliest  published  parsing  algorithms  is  the  essentially  left-corner parsing  algorithm  of  Irons  [1961].  Surveys  of  early  parsing  techniques  are  given 
by  Floyd  [1964b], Cheatham  and  Sattley  [1963], and  Griffiths  and  Petrick  [1965]. Unger  [1968] describes  a  top-down  algorithm  in which  the  initial  and  final sym- 
bols  derivable  from  a  nonterminal  are  used  to  reduce  backtracking.  Nondeter- ministic  algorithms  are  discussed  by  Floyd  [1967b]. 

One implementation  of Domolki's  algorithm is described by Hext and  Roberts [1970]. 
The  survey article by  Cohen  and  Gotlieb  [1970] describes  the  use  of list  struc- ture  representations  for  context-free  grammars  in  backtrack  and  nonbacktrack 
parsing algorithms. 

4.2,  TABULAR  PARSING  METHODS 

We  shall  study  two  parsing  methods  that  work  for  all  context-free  gram- 
mars,  the  Cocke-Younger-Kasami  algorithm  and  Earley's  algorithm.  Each 

algorithm  requires  n 3 time  and  n 2 space,  but  the  latter  requires  only  n z time 
when  the underlying grammar is unambiguous.  Moreover,  Earley's algorithm 

can be made to work in linear time and space for most of the grammars which 
can  be  parsed  in  linear  time  by  the  methods  to  be  discussed  in  subsequent 
chapters. 

4.2.1.  The  Cocke-Younger-Kasami  Algorithm 

In  the  last  section  we  observed  that  the  top-down  and  bottom-up  back- 
tracking  methods  may take an exponential  amount  of time to parse  according 
to  an  arbitrary  grammar.  In  this  section,  we  shall  give  a  method  guaranteed 
to  do  the job  in  time proportional  to  the  cube  of the  input  length.  It  is essen- 
tially  a  "dynamic  programming"  method  and  is  included  here  because  of 
its  simplicity.  It is  doubtful,  however,  that  it  will find  practical  use,  for  three 
reasons: 

(1)  n 3 time  is  too  much  to  allow  for  parsing. 

(2)  The  method  uses  an  amount  of  space  proportional  to  the  square  of 
the  input  length. 

(3)  The  method  of  the  next  section  (Earley's  algorithm)  does  at  least  as 
well  in  all  respects  as  this  one,  and  for  many  grammars  does  better. 

The method  works as follows.  Let G  =  (N,  Z, P,  S) be a  Chomsky normal 
form  C F G   with  no  e-production.  A  simple  generalization  works  for  non- 

C N F  grammars  as  well,  but  we  leave  this  generalization  to  the  reader.  Since 
a  cycle-free  CFG   can  be left-  or  right-covered  by a  CFG   in  Chomsky  normal 
form,  the  generalization  is  not  too  important. 

Let  w  =  ala  2  ...  a  n  be  the  input  string  which  is  to  be  parsed  according 
to  G.  We  assume  that  each  a~  is  in  X  for  1 <  i  <  n.  The  essence  of  the 

SEC. 4.2  TABULAR  PARSING  METHODS  315 

algorithm  is  the  construction  of  a  triangular  parse  table  T,  whose  elements 

we denote  t~j for  1 ~  i  ~  n and  1 <  j  <  n  --  i  +  1. Each  tij  will have a  value 

+ which  is  a  subset  of  N.  Nonterminal  A  will  be  in  tij  if  and  only  if  A  =~ 

a~a;+~  * ..  a~+j_~,  that  is,  if A  derives  the j  input  symbols  beginning  at  posi- 
tion  i.  As  a  special  case,  the  input  string  w is in L(G)  if and  only  if S  is in t l,. 

Thus,  to  determine  whether  string  w  is  in  L(G),  we  compute  the  parse 
table  T for  w and  look  to  see if S  is in  entry  t 2,.  Then,  if we want  one  (or  all) 
parses  of  w,  we  can  use  the  parse  table  to  construct  these  parses.  Algorithm 

4.4  can  be  used  for  this  purpose. 

We  shall  first  give  an  algorithm  to  compute  the  parse  table  and  then 
the  algorithm  to  construct  the  parses  from  the  table. 

ALGORITHM 4.3 

Cocke-Younger-Kasami  parsing  algorithm. 
Input.  A  Chomsky  normal  form  C F G   G  =  (N,  X, P,  S)  with  no  e-pro- 
duction  and  an  input  string  w  =  ala  2  ...  a,  in  X +. 

Output.  The  parse  table  T  for  w  such  that  tij  contains  A  if  and  only  if 
+ 

A  ==~ a~a~+ ~  * *  *  ai+ j_ 1. 

Method. 

(1)  Set  tit  =  [A I A  ~  a~ is in P}  for  each  i.  After  this  step,  if t~1 contains 

+ A,  then  clearly  A  ==~ a t. 

(2)  Assume  that  t~j,  has  been  computed  for  all  i,  1 <  i <  n,  and  all  j', 
1  < j '   < j .   Set 

t~y =  {A if or  some  k,  1  <  k  <j,  A  ~  BC  is  in  P, 

B  is  in  t~k, and  C  is  in  ti+k,j_k}.'l" 

Since  I  <  k  <  j,  both  k  and j  --  k  are  less  than j.  Thus  both  t~k and  ti+k,j_ k 
are  computed  before  t~j  is  computed.  After  this  step,  if  t~j contains  A,  then 

+  + 
A  ~  BC  ~  ai  * * *  ai+k_aC  ~  a,.  * * *  ai+k_iai+ k  * *  *  ai+j_ 1. 

(3)  Repeat  step  (2)  until  tij  is  known  for  all  1 _~  i  <  n,  and  1 <  j 
n--i+l. 

Example 4.8 

Consider  the  C N F   grammar  G  with  productions 

tNote that we are not discussing in detail how this is to be done.  Obviously, the com- putation  involved  can  be  done  by  computer.  When  we  discuss the  time  complexity  of 
Algorithm 4.3, we shall  give details  of this step that  enable  it to be done efficiently. 

316  GENERAL  PARSING  METHODS  c a m .   4 

S  >  A A   IASIb 
A  > SA  I A S l a  

Let  abaab  be  the  input  string.  The  parse  table  T  that  results  from  Algorithm 
4.3  is  shown  in  Fig.  4.8.  From  step  (1),  tit  =  {A}  since  A  ~  a  is  in  P  and 

at  =  a.  In  step  (2)  we  add  S  to  t32,  since  S  ---~ A A   is  in  P  and  A  is  in  both 

t3t  and  t4t.  Note  that,  in  general,  if the  tt~'s  are  displayed  as  shown,  we  can 

j t   1 

A , S   A , S  
A , S   S  A , S  

A , S   A  S 

A  S  A 

A , S  

A  S 
i ~  1  2  3  4  5  Fig. 4.8  Parse table T. 

compute  ttj,  i  >  1,  by  examining  the  nonterminals  in  the  following  pairs  of 
entries" 

(ttl,  t,+i.~-1),  (t,2, t,+z,j-2),  * * *,  (t,.j-1,  t,+j-l.,) 

Then,  if B  is  in  tie  and  C  is  in  t~+k.j-k  for  some  k  such  that  1 ~  k  <  j  and 
A  ~  B C   is  in  P,  we  add  A  to  ttj.  That  is,  we  move  up  the  ith  column  and 
down  the  diagonal  extending  to  the right  of cell ttj simultaneously,  observing 
the  nonterminals  in  the  pairs  of cells  as  we  go. 

Since  S  is  in  t~5,  abaab  is  in L(G).  [B 

THEOREM 4.6 

If Algorithm  4.3 is applied  to  C N F   grammar  G and  input  string a~  ..  * a,, 

+ then  upon  termination,  A  is  in  t~j if and  only  if A  ==~ a s . . .   a~+~_~. 

Proof.  The  proof  is  a  straightforward  induction  on  j  and  is  left  for  the 
Exercises.  The  most  difficult  step  occurs  in  the  "if"  portion,  where  one must 

+ observe  that  i f j   >  1  and  A  ==~ a s . . .   at+i_~,  then  there  exist  nonterminals 

+ B  and  C  and  integer  k  such  that  A  ~  B C   is  in  P,  B  ~  a~  . . .   an+k_ 1,  and 
+ 
C  ~  at+ k  * * *  a,+j_ l.  E] 

Next,  we  show  that  Algorithm  4.3  can  be  executed  on  a  random  access 
computer  in  n 3 suitably  defined  elementary  operations.  For  this  purpose,  we 

SEC. 4.2  TABULAR PARSING  METHODS  31 '7 

shall  assume  that  we  have  several  integer  variables  available,  one  of  which 
is n, the input length. An  elementary  operation,  for the purposes  of this discus- 

sion,  is  one  of the  following' 

(1)  Setting  a  variable  to  a  constant,  to  the  value  held  by  some  variable, 
or  to  the  sum  or  difference  of the  value  of two  variables  or  constants; 

(2)  Testing  if two  variables  are  equal, 
(3)  Examining  and/or  altering  the  value  of  t~j, if  i  and j  are  the  current 
values  of two  integer  variables  or  constants,  or 

(4)  Examining  a,,  the  ith input  symbol,  if i is the  value  of some variable. 

We  note  that  operation  (3)  is a  finite  operation  if the  grammar  is  known 
in  advance.  As  the  grammar  becomes  more  complex,  the  amount  of  space 

necessary  to  store  t,.j and  the  amount  of  time  necessary  to  examine  it  both 
increase,  in terms  of reasonable  steps of a  more elementary nature.  However, 

here  we  are  interested  only  in  the  variation  of  time  with  input  length.  It  is 

left  to  the  reader  to  define  some  more  elementary  steps  to  replace  (3)  and 
find  the  functional  variation  of  the  computation  time  with  the  number  of 

nonterminais  and  productions  of the  grammar. 

CONVENTION 

We take  the  notation  '~f(n) is 0(g(n))"  to  mean  that there  exists a  constant 
k  such  that  for  all  n  ~  1, f(n)  ~  kg(n).  Thus,  when  we  say  that  Algorithm 

4.3  operates  in  time  0(n3),  we  mean  that  there  exists  a  constant  k  for  which 
it  never  takes  more  than  kn 3 elementary  operations  on  a  word  of length  n. 

THEOREM 4.7 

Algorithm 4.3 requires 0(n 3) elementary operations of the type enumerated 
above  to  compute  tij for  all  i and j. 

Proof  To  compute  tit  for  all  i merely  requires  that  we  set  i  =  1  [opera- 
tion(l)],  then repeatedly  set t~l to ~A]A  --~  a~ is in P~} [operations  (3) and  (4)], 

test  if  i  =  n  [operation  (2)],  and  if  not,  increment  i  by  I  [operation  (1)]. 
The  total  number  of elementary  operations  performed  is  0(n). 

Next,  we  must  perform  the  following  steps  to  compute  t~j" 

(1)  Set j  --  1. (2)  Test ifj  --  n.  If not,  increment j  by  1 and perform line(j),  a procedure 

to  be  defined  below. 

(3)  Repeat  step  (2)  until j  =  n. 

Exclusive  of  operations  required  for  line(j),  this  routine  involves  2 n -   2 
elementary  operations.  The  total  number  of elementary  operations  required 
for  Algorithm  4.3  is  thus  0(n)  plus  ~ = 2   l(j),  where  l(j)is  the  number  of 

elementary  operations  used  in  line(j).  We  shall  show  that  l(j)  is  0(n 2)  and 

thus  that  the  total  number  of operations  is 0(n0. 

318  GENERAL  PARSING  METHODS  CHAP.  4 

The  procedure  line(j)  computes  all  entries  t~j  such  that  1 _<  i  <  n -  
j  -I-- 1.  It  embodies  the  procedure  outlined  in Example 4.8  to compute  tij.  It is 

defined  as  follows  (we  assume  that  all  t~j  initially  have  value  N)' 

(1)  Let  i =   1 a n d j ' =   n  - - j - k   1. 
(2)  L e t k =   1. 
(3)  Let  k'  --  i  q-  k  and j "   =  j -   k. 
(4)  Examine  t;k  and  tk,i,,. Let 

t~j  ---  t~j  u  { A[A  --,  BC   is  in  P,  B  in  t,.k, and  C  in  tk,j,,]. 
(5)  Increment  k  by  1. 
(6)  If k  - - j ,  go  to  step  (7).  Otherwise,  go  to  step  (3). 
(7)  If  i-----j',  halt.  Otherwise  do  step  (8). 
(8)  Increment  i  by  1 and  go  to  step  (2). 

We  observe  that  the  above  routine  consists  of  an  inner  loop,  (3)-(6), 
and  an  outer  loop,  (2)-(8).  The  inner  loop  is  executedj  --  1 times  (for  values 

of  k  from  1  to j -   1)  each  time  it  is  entered.  At  the  end,  t,.j has  the  value 
defined  in  Algorithm  4.3.  It  consists  of  seven  elementary  operations,  and  so 
the  inner  loop  uses  0(j)  elementary  operations  each  time  it  is  entered. 

The  outer  loop  is  entered  n  -- j  -Jr- 1 times  and  consists  of 0(j)  elementary 
operations  each  time  it  is  entered.  Since j  ~  n,  each  computation  of line(j) 

takes  0(n 2)  operations. 

Since  line(j)  is  computed  n  times,  the  total  number  of elementary  opera- 
tions  needed  to  execute  Algorithm  4.3  is  thus  0(n3).  [--] 

We  shall  now  describe  how  to  find  a  left  parse  from  the  parse  table. 
The  method  is  given  by  Algorithm  4.4. 

ALGORITHM 4.4 

Left  parse  from  parse  table. 
Input.  A  Chomsky  normal  form  CF G   G - - ( N ,   E, P,  S)  in  which  the 
productions  in P  are  numbered  from  1 to p,  an  input  string  w  --  a~a 2 . . .   a,, 

and  the  parse  table  T  for  w constructed  by  Algorithm  4.3. 

Output.  A  left  parse  for  w or  the  signal  "error." 
Method .  We  shall  describe  a  recursive  routine  gen(i,j,  A)  to  generate 

+ a  left  parse  corresponding  to  the  derivation  A  =,aia~+  1  ...  ai+j_ 1.  The 

lm routine  gen(i, j,  A)  is  defined  as  follows" 

(1)  Ifj  --  1 and  the mth  production  in P  is A  --,  a,., then  emit  the  produc- 
tion  number  m. 

(2)  If j  >  1,  k  is  the  smallest  integer,  1 <  k  <  j,  such  that  for  some  B 
in  t;k  and  C  in  t~+k,j-k,  A  --*  BC  is  a  production  in  P,  say  the  mth.  (There 

SEC. 4.2  TABULAR PARSING  METHODS  31 9 
may  be  several  choices  for  A  --~  BC  here.  We  can  arbitrarily  choose  the  one 
with  the  smallest  m.)  Then  emit  the  production  number  m  and  execute 
gen(i,  k,  B),  followed  by  gen(i  -t-- k, j -   k,  C). 

Algorithm  4.4,  then,  is  to  execute  gen(1,  n, S),  provided  that  S  is in  t~,.  If 
S  is  not  in  t,,,  emit  the  message  "error." 

We  shall  extend  the  notion  of  an  elementary  operation  to  include  the 
writing  of  a  production  number  associated  with  a  production.  We  can  then 

show  the  following  result. 

THEOREM 4.8 

If Algorithm  4.4 is executed  with  input  string  a I  ..-   a,,  then  it will  termi- 
nate with some left parse  for the input if one exists. The number  of elementary 

steps  taken  by  Algorithm  4.4  is  0(nZ). 

Proof  An  induction  on  the  order  in  which  gen  is  called  shows  that 
whenever  gen(i,j,  A)  is  called,  then  A  is  in  t;j.  It  is  thus  straightforward  to 

show  that  Algorithm  4.4  produces  a  left  parse. 

t  To  show  that  Algorithm  4.4  operates  in time O(nZ),  we prove  by induction 

on j  that  for  all j  a  call  of gen(i, j,  A)  takes  no  more  than  e l i  2 steps for  some 
constant  c 1. The  basis, j  =  1, is trivial,  since  step  (1)  of Algorithm  4.4 applies 

and  uses  one  elementary  operation. 

For  the  induction,  a  call  of  gen(i,j,  A)  with j  >  1  causes  step  (2)  to  be 
executed.  The  reader  can  verify  that  there  is  a  constant  c z such  that  step  (2) 

takes no  more  than  czj elementary operations,  exclusive of calls.  If gen(i, k, B) 
and  gen(i  q - k , j -   k,  C)  are  called,  then  by  the  inductive  hypothesis,  no 

more than  elk z +  c1( j  --  k) z -?  c2j steps are taken  by gen(i, j,  A). This expres- 

sion  reduces  to  c~(j z  +  2k z  --  2kj)  +  czj.  Since  1 <  k  < j   and  j~>  2,  we 
know  that  2k z -   2kj <  2 -   2j <  --j.  Thus,  if  we  chose  c,  to  be  c 2  in  the 
inductive  hypothesis,  we  would  have  elk z  q-  c ~ ( j -   k) 2 -t-  czj  <  el i z.  Since 
we  are  free  to  make  this  choice  of  c~,  we  conclude  the  theorem.  [--] 

Example 4.9 

Let  G  be  the  grammar  with  the  productions 

(1)  S  ~  ~ AA 

(2)  S  ~ A S  
(3)  S  > b 
(4)  A  ~ SA 
(5)  A  ~ A S  
(6)  A  , a  

320  GENERAL PARSING METHODS  CHAP. 4 

Let  w =  abaab  be  the  input  string.  The parse  table  for  w is given in  Example 
4.8. 

Since S is in  T 15, w is in L(G).  To find a left parse for abaab  we call routine 
gen(1,  5,  S).  We  find  A  in  t~l  and  in  t24  and  the  production  S  ---~ A A   in  the 

set  of  productions.  Thus  we  emit  1  (the  production  number  for  S---~  A A )  
and  then  call  gen(1,  1, A)  and  gen(2,  4,  A).  gen(1,  1, A)  gives  the  production 
number  6. Since  S is in t21 and  A  is  in  t33  and A  --~  S A  is  the  fourth  produc- 
tion,  gen(2,  4,  A)  emits  4  and  calls  gen(2,  1, S)  followed  by  gen(3,  3, A). 

Continuing  in  this  fashion  we  obtain  the  left  parse  164356263. 
Note  that  G  is  ambiguous;  in  fact,  abaab  has  more  than  one  left  parse. 
It  is  not  in  general  possible  to  obtain  all  parses  of  the  input  from  a  parse 
table  in  less  than  exponential  time,  as  there  may  be  an  exponential  number 

of left  parses  for  the  input.  D 

We  should  mention  that  Algorithm  4.4  can  be  made  to  run  faster  if, 
when  we  construct  the  parse  table  and  add  a  new  entry,  we place  pointers  to 
those  entries  which  cause  the  new  entry  to  appear  (see  Exercise  4.2.21). 

4.2.2.  The  Parsing  Method  of  Earley 

In this section we shall present  a parsing  method  which will parse an input 
string  according  to  an  arbitrary  C FG  using time 0(n 3) and  space  0(n2),  where 
n  is  the  length  of  the  input  string.  Moreover,  if  the  C F G   is  unambiguous, 
the  time  variation  is  quadratic,  and  on  most  grammars  for  programming 

languages  the  algorithm  can  be  modified  so  that  both  the  time  and  space 
variations  are  linear  with  respect  to  input  length  (Exercise  4.2.18).  We  shall 
first  give the  basic  algorithm  informally  and  later  show  that  the  computation 

can  be  organized  in  such  a  manner  that  the  time  bounds  stated  above  can 
be  obtained. 

The  central  idea  of the  algorithm  is  the  following.  Let  G  =  (N,  Z, P,  S) 
be  a  C FG   and  let  w =  ala 2  .. .  a n be  an  input  string  in  Z*.  An  object  of 
the  form  [A--~  X i X 2 .. .  X k  *  Xk + l .. .  Xm,  i]  is  called  an  item  for  w  if 

A  --~ X'~  ..-  X'~ is a  production  in P  and  0 ~  i ~  n.  The  dot  between  X k  and 

Xk+~  is  a  metasymbol  not  in  N  or  Z.  The  integer  k  can  be  any  number 
including  0  (in  which  case  the  *  is  the  first  symbol)  or  m  (in  which  case  it 
is  the  last).t 

For  each  integer j,  0  ~  j  ~  n,  we  shall  construct  a  list  of  items  Ij  such 
that  [A  --~ t~ * fl, i] is  in  Ij  for  0  ~  i ~  j  if and  only  if for  some  }, and  ~,  we 

have  S  ~  ~.4~,  ~ ~  a~  .. .  a,,  and  tx ~  a,.l  " "   aj.  Thus  the  second  com- 
ponent  of  the  item  and  the  number  of  the  list  on  which  it  appears  bracket 
the  portion  of the  input  derived  from  the  string  ~.  The  other  conditions  on 
the  item  merely  assure  us  of  the  possibility  that  the  production  A  --~  t~fl 

tIf the production  is A ~  e, then the item is [A ~ . ,   i]. 

SEC. 4.2  TABULAR  PARSING  METHODS  321 

could  be  used  in  the  way  indicated  in  some  input  sequence  that  is  consistent 
with  w  up  to  position  j. 

The  sequence  of  lists  Io,  I1,.  * *,  I,  will  be  called  the  parse  lists  for  the 
input  string  w.  We  note  that  w  is  in L(G)  if and  only  if there  is  some  item  of 

the  form  [S  ~  ~ . ,   0]  in  I,. 

We  shall  now  describe  an  algorithm  which,  given  any  grammar,  will 
generate  the  parse  lists  for  any  input  string. 

ALGORITHM 4.5 

Earley's  parsing  algorithm. 
Input.  C F G   G  =  (N,  E,  P,  S)  and  an  input  string  w  --  ala 2  * ..  a,  in  Z*. 

Output.  The  parse  lists  Io,  I~ . . . . .   I,. 
Method.  First,  we  construct  Io  as  follows" 

(1)  If  S  --~  a  is  a  production  in  P,  add  [S  ~  *  0~, 0]  to  Io. 
Now   perform  steps  (2)  and  (3)  until  no  new  items  can  be  added  to  Io. 
(2)  If  [B  ~  y-,  0]  is  on  Io,t  add  [A  --~  ~B  *  p,  01  for  all  [A  ---  a  *  Bp,  0] 
on  I o. 

(3)  Suppose  that  [A  ~  ~ .   Bfl,  0]  is  an  item  in  I o.  Add  to  Io,  for  all 
productions  in P  of the form  B  ~  y, the  item  [B ~  *  y, 0]  (provided  this  item 
is  not  already  in  Io). 

We  now  construct  Ij,  having  constructed  I0, I 1 , . . . ,   Ij_ t- 
(4)  For  each  [B  --~  0~ *  aft,  i]  in  Ij_~  such  that  a  --  aj,  add  [B --.  0~a *  fl,  i] 
to  I v. 

N ow  perform  steps  (5)  and  (6)  until  no  new  items  can  be  added. 

(5)  Let  [A  --~  ~,.,  i]  be  an  item  in  Ij.  Examine  It  for  items  of  the  form 
[B ~  0~ *  Ap,  k].  For  each  one  found,  we  add  [B ~  ~A  *  fl,  k]  to  Ij. 

(6)  Let  [A  ~  0~-  Bfl,  i]  be  an  item  in  Ij.  For  all  B  ~  7  in  P,  we  add 
[B  ~  *  y, j]  to  Ij. 

Note  that  consideration  of  an  item  with  a  terminal  to  the  right  of the  dot 
yields  no  new  items  in  steps  (2),  (3),  (5)  and  (6). 

The  algorithm,  then,  is  to  construct  Ij  for  0  ~  j  _~  n.  [~] 

Example 4.10 

Let  us  consider  the  gra mma r  G  with  the  productions 

(1)  E  > T  +  E 

(2)  E--  > T 
(3)  T - - - >   F ,   T 
(4)  T  >  F 
(5)  F  ~  > (E) 

(6)  F --- ~   a 

tNote  that  ? can be e. This is the way rule (2) becomes applicable initially. 

322  GENERAL PARSING METHODS  CHAP. 4 
and  let  (a  q - a ) ,   a  be  the  input  string.  From  step  (1)  we  add  new  items 
[E----,.  T  +  E, 0]  and  [ E - - , .   T,  0]  to  I 0.  These  items  are  considered  by 
adding  to  I0  the  items  [T ~  * F  *  T, 0]  and  [T --,  * F,  0] from  rule  (3).  Con- 
tinuing,  we  then  add  [F ~  *  (E),  0]  and  [F--~  * a,  0].  No  more  items  can 
be  added  to  I 0. 

We  now  construct  I~.  By  rule  (4)  we  add  I F - - ,   (.  E),  0],  since  aa  ----- (. 
Then  rule  (6)  causes  [ E ~ . T + E ,   1],  [ E ~ . T ,   1],  [ T - - ~ . F , T ,   1], 
[T--,  * F,  1],  [F ~  *  (E),  1],  and  [F----,  * a,  1]  to  be  added.  Now,  no  more items  can  be  added  to  I~. 

To  construct  12,  we  note  that  a2  =  a  and  that  by  rule  (4)  [F----,  a . ,   1]  is 
to  be  added  to  Iz.  Then  by rule  (5),  we  consider  this  item  by  going  to  I~  and 
looking  for  items  with  F  immediately  to  the  right  of  the  dot.  We  find  two, 
and  add  [T--,  F .   *  T,  1]  and  [T--~  F . ,   1]  to  12.  Considering  the  first  of these  yields  nothing,  but  the  second  causes  us  to  again  examine  I~,  this  time 

for items  with.  T in  them.  Two more items are added  to/2,  [E ~  T .   q- E,  1] 
and  [E--~  T . ,   1].  Again  the  first  yields  nothing,  but  the  second  causes 
[F ~  (E  .),  0]  to  be  added  to  12.  Now  no  more  items  can  be  added  to  12, so  I2  is  complete. 

The  values  of all  the  lists  are  given  in  Fig.  4.9. 

Io  11  Iz [E---->  *  Tq-  E,  0]  IF--~  (.  E),0]  [F---.  a.,  1] 
[E~.  T, 0]  [E~.  T-t-E,  1]  [T~F..T,  1] 
[T---~  * F.  T, 0]  [E--~  *  T,  1]  [T--~  F.,  1] 
[T--~  * F, 0]  [ T - - ~ . F . T ,   1]  [ E - - ~ T .   WE,  l] 
[F--~  * (E),  01  [T----~  * F,  11  [E ~  T .,   1] [F ~  o a, 0]  [F ~  * (E),  1]  [F-o  (E  .),  0] 

[F ---~  .a,l] 
13  h  ls [E ~  T  q-  * E,  11  [F ~  a.,  3]  [F ~  (E).,  0] 
[E------~  *  T-t-  E,  31  [T----~ F.  *  T, 3]  [T----~ F.,  T, 0] 
[E ~  *  T, 31  [T ~  F.,  31  [T ~  F.,  0] 
[T--->  * F ,  T,  3]  [E ~  T .  +  E,  31  [E ~  T  .  +  E, 01 [T --~  * F, 3]  [E ~  T.,  3] 

[E --~  T. ,   0] [F---~  *  (E),  31  [E----*  T  +  E.,  1] 

[F ~  * a,  31  [F ~  (E.),  01 

16  I7 [ T - . F . .   T, 0]  [F---~  a.,  6] 
[T---,  * F ,   T, 6]  IT---, F.   * T, 61 
[T----~  * F,  6]  [T----~ F.,  61 
[F ~  * (E),  6]  [T ~  F,  T.,  01 
[F ~  * a,  6]  [E ----* T.  q-  E,  01 

[E--~  T.,0] 

Fig.  4.9  Parse  lists  for  Example  4.10. 

Since  [E --~  T. ,   0]  is  on  the  last  list,  the  input  is in L(G). 

SEC. 4.2  TABULAR  PARSING  METHODS  323 

We  shall  pursue  the  following  course  in  analyzing  Earley's  algorithm. 
First,  we  shall  show  that  the  informal  interpretation  of the  items  mentioned 
before  is  correct.  Then,  we  shall  show  that  with  a  reasonable  concept  of 

an  elementary  operation,  if  G  is  unambiguous,  then  the  time  of  execution  is 

quadratic  in  input  length.  Finally,  we  shall  show  how  to  construct  a  parse 
from  the  lists  and,  in  fact,  how  this  may  be  done  in  quadratic  time. 

THEOREM 4.9 

If  parse  lists  are  constructed  as  in  Algorithm  4.5,  then  [A  ---~ t~.  fl,  i]  is 
on  Ij  if  and  only  if  ~  =~  a~+l  "'"  aj  and,  moreover,  there  are  strings  y and  ,6 

such  that  S  =>  ~,AJ  and  7 =~  al  " "   a~. 

Proof 

Only  if:  We  shall  do  this  part  by induction  on  the  number  of items  which 

have  been  added  to  I0,  11,  . . . .   1 i  before  [A  ~  t~-  fl,  i]  is  added  to  Ij.  For 

the  basis,  which  we  take  to  be  all  of  I0,  we  observe  that  anything  added  to 

I 0 has  a  ~  e,  so  S  =-~ 7AJ  holds  with  ?  =  e. 

For  the  inductive  step,  assume  that  we  have  constructed  I0  and  that  the 
hypothesis  holds  for all items presently  on/~,  i_~j.  Suppose  that  [A ~  ~ .   fl, i] 
is  added  to  Ij  because  of  rule  (4).  Then  a  =  oc'aj  and  [A  ---~ t x'-ajfl,  i]  is 

on  Ij_  1.  By  the  inductive  hypothesis,  o~' ~  a~+l  * * "  a j_ 1  and  there  exist 

strings  ~,'  and  6'  such  that  S =-~ ?'AJ'  and  y ' = >   a 1 * * * a r  It  then  follows  that 

=  t~'aj ==~ at+ 1 "'"  aj,  and  the  inductive  hypothesis  is  satisfied  with  ~, =  ~,' 
and  ~  =  6'. 

Next,  suppose  that  [,4  ---.  a .   fl,  i]  is  added  by  rule  (5).  Then  0~ =  ogB 

for  some  B in N,  and  for  some  k,  [,4  ~  o~'  *  Bfl,  i]  is on I k. Also,  [B --,  1/.,  k] . 

is on  Ij  for  some  I/in  (N  u  E)*.  By the inductive  hypothesis,  r/~>  ak+ 1 "" *  aj 
and  0~'==~ a~+l...ak.  Thus,  tx  =  ogB=>  a~+l...a  j.  Also  by  hypothesis, 

there  exist  ~,' and  6'  such  that  S  ~  ?'A6'  and  ~,'=>  a 1 .--  ai.  Again,  the  rest 

of the  inductive  hypothesis  is  satisfied  with  y  =  ~,'  and  J  =  6'. 

The  remaining  case,  in  which  [A  ---.  ct.  fl,  i]  is  added  by  rule  (6),  has 
0c =  e  and  i  =  j.  Its  elementary  verification  is  left  to  the  reader,  and  we 
conclude  the  "only  if"  portion. 

If:  The  "if"  portion  is  the  proof  of the  statement 

(4.2.1)  If S  =~  ~,AO, ~, =~  a 1 . . .   a~, A  ---,  ctfl is  in P,  and  tz  =~  a~+l  "'"  aj, 

then  [A  --.  ~  *  fl,  i]  is  on  list  Ij 

We  must  prove  all  possible  instances  of  (4.2.1).  Any  instance  can  be 
characterized  by  specifying  the  strings  0~, fl,  y,  and  ~,  the  nonterminal  A,  and 

the  integers  i  and  j,  since  S  and  al  . . . a ,   are  fixed.  We  shall  denote  such 

324  GENERAL PARSING  METHODS  CHAP.  4 
an  instance  by  [~,  fl,  7,  $,  A,  i, j].  The  conclusion  to  be  drawn  from  the  above 
instance  is  that  [A  ----~ at  *  fl,  i]  is  on  list  I 1.  Note  that  7  and  J  do  not  figure 

explicitly  into  the  conclusion. 

The  p r o o f  will  turn  on  ranking  the  various  instances  and  proving  the 
result  by induction  on  the rank.  The rank  of the  instance  ~  =  [at, fl,  ~, $,  A,  i,j] 
is  computed  as  follows" 

Let  %(~)  be  the  length  of a  shortest  derivation  S  ==~ ?AJ. 
Let  ~2(~)  be  the  length  of a  shortest  derivation  7 ==~ a I  . . .   a~. . 
Let  %(t0  be  the  length  of a  shortest  derivation  at ==~ at+~  . ..  a v. 
The  rank  of  t~ is  defined  to  be  zl(t0  q-  2[.]  -+- z2(tt)  -q-  z~(~)]. 
We  now  prove  (4.2.1)  by  induction  on  the  rank  of  an  instance  g  = 
[at,  fl,  7,  t~, A,  i,j].  If  the  rank  is  0,  then  ~r~(g) =  zz(g)  =  %(g)  = j   =  0.  We 
can  conclude  that  at  =  7  =  $  =  e  and  that  A  =  S.  Then  we  need  to  show 
that  [S ~  *  ,0, 0]  is  on  list  0.  However,  this  follows  immediately  from  the 
first  rule  for  that  list,  as  S  ---~ fl  must  be  in  P. 

For  the  inductive  step,  let  ~,  as  above,  be  an  instance  of  (4.2.1)  of  some 
rank  r  >  0,  and  assume  that  (4.2.1)  is  true  for  instances  of  smaller  rank. 

Three  cases  arise,  depending  on  whether  at ends  in  a  terminal,  ends  in  a  non- 
terminal,  or  is  e. 

Case  1:  ot  =  at'a  for  some  a  in  E.  Since  ~  ==~ at+ 1  . . .   a~,  we  conclude 
that  a  =  a v.  Consider  the  instance  ~'  =  [at',  avfl ,  ~,, $,  A,  i ,j  --  1].  Since 
A  ~  at'avfl  is in P,  tt' is an  instance  of (4.2.1),  and  its  rank  is  easily seen  to  be 
r -   2.  We  may  conclude  that  [A  --~  at'.  a~fl,  i]  is  on  list  I v_ 1.  By  rule  (4), 

[A  ----~ at  *  fl,  i]  will  be  placed  on  list  I v. 

Case  2:  at  =  at'B  for  some  B  in  N.  There  is  some  k,  i  <  k  _< j,  such  that 
at'=,,  a~+ 1  .. .a  k  and  B  ==~ ak+ 1  .-.  ay.  F r o m   the  instance  of  lower  rank 
~'  =  [at',  B fl,  7,  $,  A,  i,  k]  we  conclude  that  [A  ~  at'.  B fl,  i]  is  on  list  Ik.  Let 

B  ==~ t / b e   the  first  step  in  a  m in i mu m   length  derivation  B  ==~ ak+ 1  * * *  aj. 

Consider  the  instance  a"  =  [t/,  e, 7at',  fl$,  B,  k,j].  Since  S  ==~ 7A$  ==~ 
7at'Bfl$,  we  conclude  that  zl(a")  _<  %(a)  -Jr  1.  Let  n 1 be  the  m in im um   num- . 

ber  of steps  in  a  derivation  at' ==~ at+ 1 ..  *  a k  and  n 2 be  the  m inim um   number 
in  a  derivation  B  =-~  ak+l  "'"  aj.  Then  %(~)  =  n I -4- n2.  Since  B  ==~ t/=-~ 
ak+ 1  . ."  a v,  we  conclude  that  z3(a")  =  n2  --  1.  It  is  straightforward  to  see 
that  %(tl")  =  %(~)  -b  n~.  Hence  %(~")  q-- zs(a")  =  %(a)  -k  nl  +  nz  --  1  = 
~:2(a) -Jr- z3(a)  --  1.  Thus  ~:l(a")  +  2[j  -q- z2(a")  .jr_ %(a")]  is less  than  r.  By  the 
inductive  hypothesis  for  a"  we  conclude  that  [B --~  1//.,  k]  is  on  list  Iv,  and 
with  [A ~  at'-  Bfl,  i]  on  list  Ik,  conclude  by  rule  (2)  or  (5)  that  [A ~  at.  fl,  i] 

is  on  list  I v. 

Case  3:  at  =  e.  We  may  conclude  that  i  =  j  and  z3(~)  =  0  in  this  case. 

SEC. 4.2  TABULAR  PARSING  METHODS  325 

Since  r  >  0,  we  may  conclude  'that  the  derivation  S  =~  yA,~  is  of  length 
greater  than  0.  If  it  were  of  length  0,  then  z l(a)  --  0.  Then  we  would  have 

7  ----- e,  so  x2(a)  =  i  --  0.  Since  i  =  j  and  z3(a)  =  0  have  been  shown  in  gen- 
eral  for  this  case,  we  would  have  r  --  0. 

We  can  thus  find  some  B  in  N  and  7',  7",  6',  and  ~"  in  (N  u  X)*  such 

that  S  =~  y'B6'=:>  y'7"A,~"6',  where  B  ---,  7" A6 "  is in P,  7  =  Y'7",  ~  =  ~"6', , 
and  7'B6'  is  the  penultimate  step  in  some  shortest  derivation  S = ~   7A6. 

Consider  the  instance  a ' - - [ 7 " ,   A6",  7',  6',  B,  k, j],  where  k  is  an  integer 

such  that  7 ' = ~   al  . " a k   and  7 " = ~ a k + l . . . a j .   Let  the  smallest  length  of 
the  latter  derivations  be nl  and  n2,  respectively.  Then  Zz(a')  --  n 1, "r3(a')  --  n 2, 

and  ~2(a)  --  n~  /  n 2.  We  have  already  argued  that  z3(a)  --  0,  and  B,  7',  and 

6'  were  selected  so  that  zl(a')  --  ~ ( a )   --  I.  It  follows  that  the  rank  of  a'  is 
r  --  1.  We  may  conclude  that  [B  --,  7"  *  Ad;",  k]  is  on  list  1 i.  By  rule  (6),  or 
rule  (3)  for  list  I0,  we  place  [A  --,  *  fl, j]  on  list  1 i.  D 

Note  that  as  a  special  case  of  Theorem  4.9,  [S  ~  a . ,   0]  is  on  list  I,  if 
and  only  if S  --,  a  is  in  P  and  a  =~  a I  -..   a,;  i.e.,  al  " "   a,  is  in  L(G)  if and 

only  if [S  --,  a.,  0]  is  on  list  I,  for  some  ~. 

We  shall  now  examine  the  running  time  of Algorithm  4.5.  We  leave  it  to 
the  reader  to  show  that  in  general  0(n0  suitably  defined  elementary  steps  are 

sufficient  to  parse  any  word  of  length  n  according  to  a  known  grammar. 
We  shall  concentrate  on  showing  that  if the  gr amm ar   is  unambiguous,  0(n 2) 

steps  are  sufficient. 

LEMMA 4.6 

Let  G  =  (N,  X, P,  S)  be  an  unambiguous  g ra m ma r  and  a 1 .. -  a,  a  string 
in  X*.  Then  when  executing  Algorithm  4.5,  we  attempt  to  add  an  item 

[A  ~  tz  *  fl,  i]  to  list  Ij  at  most  once  if ct ~  e. 

Pro of  This  item  can  be  added  only  in  steps  (2),  (4),  or  (5).  If  added  in 
step  (4),  the  last  symbol  of  ~  is  a  terminal,  and  if in  steps  (2)  or  (5),  the  last 

symbol  is a  nonterminal.  In  the  first case,  the  result  is  obvious.  In  the  second 

case,  suppose  that  [A  ---~ tx'B.  fl,  i]  is  added  to  list  Ij  when  two  distinct 

items,  [ B -- ,   7  ",  k]  and  [B----~ 6 . ,   l],  are  considered.  Then  it  must  be  that 

[A  ~  tz'  *  Bfl,  i]  is  on  both  list  k  and  list  1.  (The  case  k  =  l  is  not  ruled  out, 
however.) Suppose  that  k  ~  1.  By  Theorem  4.9,  there  exist  01,  02,  03,  and  04  such 

that  S  =-> 01A02  =>  01~'Bp02  ~ a l   " "   ajll02  and  S  =>  03AO 4 ~  03o(BflO 4 
=>  a ~ . " a j l l O , .   But  in  the  first  derivation,  0 ~ '   ==>  a l " ' ' a k ,   and  in  the 
second,  03~'  =-~  a l " " a v   Then  there  are  two  distinct  derivation  trees  for 
some  al  " "   an,  with  ~'B  deriving  ai+l  " "   aj  in  two  different  ways. 

326  GENERAL  PARSING  METHODS  CHAP. 4 

Now,  suppose  that  k  =  l.  Then  it  must  be  that  7  ~  ~.  It  is  again  easy 
to  find  two  distinct  derivation  trees  for  a 1 . . .   a,.  The  details  are  left  for 
the  Exercises.  E] 

We  now  examine  the  steps  of Algorithm  4.5.  We  shall  leave the  definition 
of  "elementary  operation"  for  this  algorithm  to  the  reader.  The  crucial  step 
in  showing  that  Algorithm  4.5  is  of  quadratic  time  complexity  is  not  how 

"elementary  operation"  is  defined--any  reasonable  set  of  list-processing 
primitives  will do.  The  crucial  step  in  the  argument  concerns  "bookkeeping" 

for  the  costs  involved.  We  here  assume  that  the  grammar  G  is  fixed,  so  that 
any  processes  concerning  its  symbols  can  be  considered  elementary.  As  in 
the previous  section,  the  matter  of time variation  with the  "size"  of the gram- 
mar  is  left  for  the  Exercises. 

For  I0,  step  (1)  clearly  can  be  done  in  a  fixed  number  of  elementary 
operations.  Step  (3)  for  I0  and  step  (6)  for  the  general  case  can  be  done  in 
a  finite  number  of  elementary  operations  each  time  an  item  is  considered, 
provided  we  keep  track  of  those  items  [A  ~  ~ .   fl, j]  which  have  been 

added  to  Ij.  Since  grammar  G  is  fixed,  this  information  can  be  kept  in  a 
finite  table  for  each j.  If  this  is done,  it is  not  necessary  to  scan  the  entire  list 

I v to  see  if items  are  already  on  the  list. 

For  steps  (2),  (4),  and  (5),  addition  of  items  to  1 i  is  facilitated  if  we  can 
scan  some  list  I~ such  that  i  <  j  for  all  those  items  having  a  desired  symbol 
to  the  right  of  the  dot,  the  desired  symbol  being  a  terminal  in  step  (4)  and 

a  nonterminal  in  steps  (2)  and  (5).  Thus  we  need  two  links  from  every  item on  a  list. 

The  first points  to the next item on the list.  This  link allows  us to  consider 
each  item  in  turn.  The  second  points  to  the next  item  with  the  same  symbol 
to  the  right  of the  dot.  It  is  this  link  which  allows  us  to  scan  a  list efficiently in  steps  (2),  (4),  and  (5). 

The  general  strategy  will  be  to  consider  each  item  on  a  list  once  to 
add  new  items.  However,  immediately  upon  adding  an  item  of  the  form [A  ~  oc.  Bfl,  i] 

to  I v,  we  consult  the  finite  table  for  I v  to  determine  if 
[B ~  7  ", J]  is  on  I v for  any  7.  If  so,  we  also  add  [A  ~  ~ B .   fl,  i]  to  I v. 

We observe  that there are a  fixed number  of strings,  say k, that  can appear 
as  the  first  half  of  an  item.  Thus  at  most  k (j   /  !)  items  appear  on I v.  If we 
can  show  that  Algorithm  4.5  spends  a  fixed  amount  of time,  say  c,  for  each 
item  on  a  list,  we  shall  show  that  the  amount  of time  taken  is 0(n2),  since 

c ~  k( j  +  1)  =  1/2ck(n  +  1 ) ( n + 2 ) ~ c ' n   2 j=0  for  some  constant  c' 
The  "bookkeeping  trick"  is  as  follows.  We  charge  time  to  an  item,  under 
certain  circumstances,  both  when  it is considered  and  when  it is entered  onto 

a  list.  The  maximum  amount  of time  charged  in  either  case  is fixed.  We  also 
charge  a  fixed  amount  of time  to  the  list  itself. 

SEC. 4.2  TABULAR  PARSING  METHODS  327 

We  leave  it  to  the  reader  to  show  that  I0  can  be  constructed  in  a  fixed 
amount  of time.  We  shall consider  the  items  on lists Ij,  for j  >  0.  In  step  (4) 

of Algorithm  4.5  for  Ij,  we  examine  aj  and  the  previous  list.  For  each  entry 
on Ij_ 1 with a~ to the right of the dot, we add an item to Ij. As we can examine 

only those items on Ij_ 1 satisfying that condition, we need charge only a finite 
amount  of time  to  each  item  added,  and  a  finite  amount  of  time  to  1~ for 

examining  a~ and  for finding the first item of Ij_ ~ with  * aj in it. 

Now,  we  consider  each  item  on  Ij  and  charge  time  to  it  in  order  to  see 
if step  (5)  or  step  (6)  applies.  We  can  accomplish  step  (6)  in  a  finite  amount 

of time, as we need examine only the table associated with Ij that tells whether 
all  [A  ~  * ~, j]  have  been  added  for  the  relevant  A.  This  table  can  be  exam- 
ined  in  fixed  time,  and  if necessary,  a  fixed  number  of items  are  added  to  Ij. 
This  time  is  all  charged  to  the  item  considered. 

If step  (5)  applies,  we  must  scan  some  list  I~, k < j ,   for  all  items  having 

* B  in  them  for  some  particular  B.  Each  time  one  is found,  an  item is  added 
to  list  lj,  and  the  time  is  charged  to  the  item  added,  not  the  one  being  con- 

sidered ! To show that the amount of time charged to any item on any list is bound- 

ed  above  by  some  finite  number,  we  need  observe  only  that  by  Lemma  4.6, 
if the  grammar  is  unambiguous,  only  one  attempt  will  ever be  made  to  add 

an item to a list.  This observation also ensures that in  step (5) we do not have 
to  spend  time  checking  to  see  if an  item  already  appears  on  a  list. 

THEOREM 4.10 

If  the  underlying  grammar  is  unambiguous,  then  Algorithm  4.5  can  be 
executed  in  0(n 2)  reasonably  defined  elementary  operations  when  the  input 
is  of length  n. 

Proof.  A  formalization  of the  above  argument  and  the  notion  of  an  ele- 
mentary  operation  is  left  for  the  Exercises.  D 

THEOREM 4.11 

In  all  cases,  Algorithm  4.5  can  be  executed  in  0(n 3)  reasonably  defined 
elementary  operations  when  the  input  is  of length  n. 

Proof.  Exercise. 

Our last portion of the analysis of Earley's algorithm concerns the method 
of  constructing  a  parse  from  the  completed  lists.  For  this  purpose  we  give 
Algorithm 4.6,  which  generates  a  right  parse  from  the  parse  lists.  We  choose 

to  produce  a  right  parse  because  the  algorithm  is  slightly  simpler.  A  left 
parse  can  also  be  found with  a  simple  alteration  in  the  algorithm. 

Also for the  sake of simplicity, we shall assume that  the grammar at hand 
has no  cycles.  If a  cycle does  exist in the  grammar,  then it is possible to  have 

328  GENERAL PARSING  METHODS  CHAP.  4 

arbitrarily  many  parses  for  some  input  strings.  However,  Algorithm  4.6  can 

be  modified  to  accommodate  grammars  with  cycles  (Exercise  4.2.23). 

It should be pointed  out that as for Algorithm  4.4,  we can make  Algorithm 
4.6  simpler  by  placing  pointers  with  each  item  added  to  a  list  in  Algorithm 
4.5.  Those  pointers  give  the  one  or  two  items  which lead  to its placement  on 
its  list. 

ALGORITHM 4.6 

Construction  of a  right  parse  from  the  parse  lists. 
Input.  A  cycle-free  C F G   G  =  (N,  X, P,  S)  with  the  productions  in  P 
numbered  from  1  to  p,  an  input  string  w  =  a 1 . . .   a n,  and  the  parse  lists 
I0,  I a , . . . ,   I,  for  w. 

Output.  7r, a  right  parse  for  w,  or  an  "error"  message. 
Method.  If  no  item  of  the  form  [S---~  0c.,  0]  is  on  In,  then  w  is  not  in 
L(G),  so  emit  "error"  and  halt.  Otherwise,  initialize  the  parse  7r to  e  and 

execute  the  routine  R([S----~  ~z.,  0],  n)  where  the  routine  R  is  defined  as 
follows: 

Routine  R([A  ---+ f l . ,   i], j):  

(1)  Let  n  be h  followed  by the  previous  value  of ~z, where  h  is the  number 
of  production  A  ---~ ft.  (We  assume  that  7r is  a  global  variable.) 

(2)  If fl  =  X i X z  . . .   Xm,  set  k  =  m  and  1 =  j. 
(3)  (a)  If Xk  ~  X,  subtract  1 from  both  k  a nd/. 

(b)  If  X k  ~  N,  find  an  item  [X k ~  7'  ", r]  in  I~ for  some  r  such  that 

[A  ~ X ~ X z   . . .   Xk_~  .  X k . . .   Xm,  i]  is  in  I,.  Then  execute 
R([Xk  ---' 7'  ", r],  l).  Subtract  1  from  k  and  set  l  =  r. 
(4)  Repeat  step  (3)  until  k  =  0.  Halt.  E] 

Algorithm  4.6  works  by  tracing  out  a  rightmost  derivation  of  the  input 
string  using  the  parse  lists  to  determine  the  productions  to  use.  The  routine 
R  called  with  arguments  [A  --~ f l . ,   i]  and j  appends  to  the  left  end  of  the 
current  partial  parse  the  number  corresponding  to  the  production  A  ~  ft. 
If fl  =  voBlvlB2v 2  . . .   B,v,,  where  B 1 , . . . ,   Bs  are  all the  nonterminals  in  fl, 
then  the  routine  R  determines  the  first  production  used  to  expand  each  Bt, 

say  B,---~  fl,,  and  the  position  in  the  input  string  w  immediately  before 
the  first  terminal  symbol  derived  from  B,.  The  following  recursive  calls  of 
R  are  then  made  in  the  order  shown: 

R([B, --,/~,., i,], j,) 
R([B,_, ~  /L-~ ", ~,-~], L-,) 

o 

SEC. 4.2  TABULAR  PARSING  METHODS  329 

where 

(1)  j,  = j -   Iv, I and 

(2)  jq  --  iq+x --[vq [for  1 ~  q  <  s. 

Example 4.11 

Let  us  apply  Algorithm  4.6  to  the  parse  lists  of Example  4.10  in  order  to 
produce  a right parse for the input  string (a  +  a)  *  a.  Initially,  we can execute 

R([E----~  T . ,   0],  7).  In  step  (1),  n  gets  value  2,  the  number  associated  with 
production  E  ~  T.  We  then  set  k  =  1  and  1 =  7  and  execute  step  (3b)  of 

Algorithm  4.6.  We  find  [T ----~ F  *  T . ,   0]  on  17  and  [E ~  *  T, 0]  on  Io.  Thus 
we  execute  R([T ~  F ,   T . ,   0],  7),  which  results  in  production  number  3 

being  appended  to  the  left  of  n.  Thus,  n  =  32.  Following  this  call  of  R,  in 

step  (2)  we  set  k  =  3  and  l  =  7. 

Step  (3b)  is  then  executed  with  k  =  3.  We  find  [T---,  F . ,   6]  on  I0  and 
IT ~  F  , .   T, 0]  on  /6,  so  we  call  R([T----, F . ,   6],  7).  After  completion  of 
this  call,  we  set  k  =  2  and  l  =  6.  In  Step  (3a)  we  consider  *  and  set  k  =  1 

and  1 =  5.  We  then  find  [ F- - ,   ( E ) . ,   0]  on  ls  and  [T---,.  FF, T, 0]  on  I0, 

so  we  call  R([F---~  ( E ) . ,   0],  5). 

Continuing  in  this  fashion  we  obtain  the  right  parse  64642156432. 
The  calls  of routine  R  are  shown  in  Fig.  4.10  superimposed  on  the  deri- 
vation  tree  for  (a  +  a)  *  a.  D 

E  R([E~ T-,01,7) ! 

i  R([T~F,T.,01,7) 

F  R([F~(E)-,0I,  5)  T 
(  ) i  R([,,,,,~,~E~ 

T+E.,  1],4) R([T~F-,ll,2)  T /  E  R([E~T.,3],4) 

I  + R([F~a.,11,2)  F  T  R([T~F.,31,4) 

R([T~ F.,  61,7) 
F  R([F~a  -,61,7) I 

a 

a  F  R([Foa., 31,4) 

a 
Fig. 4.10  Diagram of execution of Algorithm  4.6. 

330  GENERAL PARSING  METHODS  CHAP.  4 

THEOREM 4.12 

Algorithm  4.6  correctly  finds  a  right  parse  of  a 1  . . .   a,  if  one  exists,  and 
can  be  made  to  operate  in  time  0(n2). 

P r o o f   A  straightforward  induction  on  the  order  of  the  calls  of  routine 
R  shows  that  a  right  parse is  produced.  We  leave  this  portion  of the  p ro o f for 
the  Exercises. 

In  a  manner  analogous  to  Theorem  4.10,  we  can  show  that  a  call  of R([A 

--~ ft . ,   i], j)  takes  time  0 ( ( j -   0 2)  if  we  can  show  that  step  (3b)  takes 
0 ( j -   i)  elementary  operations.  To  do  so,  we  must  preprocess  the  lists in such 
a  way  that  the  time  taken  to  examine  all  the  finite  number  of  items  on  I k 

whose  second  com pon ent  is  l  requires  a  fixed  computation  time.  That  is,  for 
each  parse  list,  we  must  link  the  items  with  a  c o m m o n   second  co mpo nent  
and  establish  a  header  pointing  to  the  first  entry  on  that  list.  This  preprocess- 
ing  can  be  done  in  time  0(n 2)  in  an  obvious  way. 

In  step  (3b),  then,  we  examine  the  items  on  list  Iz with  second  comp onent 
r  =  l,  l -   1 , . . . ,   i  until  a  desired  item  of  the  form  [X k --,  7  ",  r]  is  found. 
The  verification  that  we  have  the  desired  item  takes  fixed time,  since  all  items 
with  second  compo nent  i  on  I,  can  be  found  in  finite  time.  The  total  a mo u n t 

of time  spent  in  step  (3b)  is  thus  proportional  to j -   i.  [--] 

E X E R C IS E S  
4.2.1.  Let  G  be  defined  by  S  --,  AS! b,  A  ---,  SAla.  Construct  the  parse  tables 

by  Algorithm  4.3  for  the  following  words: 

(a)  bbaab. 
(b)  ababab. 

(c)  aabba. 

4.2.2.  Use  Algorithm  4.4 to  obtain  left  parses  for  those  words  of Exercise  4.2.1 which  are  in 

L(G). 

4.2.3.  Construct  parse  lists  for  the  grammar  G  of Exercise  4.2.1  and  the  words 

of  that  exercise  using  Algorithm  4.5. 

4.2.4.  Use  Algorithm  4.6  to  construct  right  parses  for  those  words  of Exercise 4.2.1  which  are  in 

L(G). 

4.2.5.  Let  G  be  given  by  S-- ~  SS[ a.  Use  Algorithm  4.5  to  construct  a  few  of 

the parse  lists  Io, 11 . . . .   when  the  input  is aa  . . . .   How  many  elementary 

operations  are  needed  before  I;  is  computed ? 
4.2.6.  Prove  Theorem  4.6. 
4.2.7.  Prove  that  Algorithm  4.4  correctly  produces  a  left  parse. 
4.2.8.  Complete  the  "only  if"  portion  of Theorem  4.9. 
4.2.9.  Show  that  Earley's  algorithm  operates  in  time  0(n 3)  on  any  grammar. 

EXERCISES  331 
4.2.10.  Complete the  proof of Lemma 4.6. 
4.2.11.  Give  a  reasonable  set  of elementary  operations  for  Theorems  4.10-4.12. 

4.2.12.  Prove Theorem  4.10. 
4.2.13.  Prove Theorem  4.11. 
4.2.14.  Show  that  Algorithm  4.6  correctly produces  a  right parse. 
4.2.15.  Modify  Algorithm  4.3  to  work  on  non-CNF  grammars.  Hint:  Each  t~j 

must  hold  not  only  those  nonterminals  A  which  derive  a t...a~+j_ , 
but  also  certain  substrings  of  right  sides  of  productions  which  derive 

ai  *  *  *  ai+j-1. 
"4.2.16.  Show  that  if  the  underlying  grammar  is  linear,  then  a  modification  of 

Algorithm  4.3  can  be  made  to  work  in  time  0(n2). 

"4.2.17.  We  can  modify  Algorithm  4.3  to  use  "lookahead  strings"  of  length 

k  _~ 0.  Given  a  grammar  G  and  an  input  string  w  =  ala2...a,,  we 
create  a  parse  table  T  such  that  t~j  contains  A  if and  only  if 

(1)  S  ==~ o~Ax, + 
(2)  A ==~ ai  . ..  at+j-l,  and 
(3)  at+jai+j+l  ...  ai+~+k-1  --  FIRSTk(X). 

Thus,  A  would  be  placed  in  entry t~j provided  the k  input symbols to the 
right  of  input  symbol  a~+j_a  can  legitimately  appear  after  A  in  a  sen- 
tential  form.  Algorithm  4.3  uses  lookahead  strings  of  length  0.  Modify 
Algorithm  4.3  to  use  lookahead  strings  of  length  k  ~>  1.  What  is  the 
time  complexity  of  such  an  algorithm ? 

"4.2.18.  We can also modify Earley's algorithm to use lookahead.  Here we would 

use  items  of the  form  [A ---,  ~  * fl,  i, u]  where  u  is  a  lookahead string of 
length k.  We would  not  enter this  item  on list Ij  unless there is  a  deriva- 

tion S  ==~ ?Auv,  where  ~' ==~ al  -.-  at, *  ==~ a~+~ ..-  aj,  and FIRSTk(flu) 
contains  a~+l "''a j+k.  Complete  the  details  of  modifying  Earley's  al- 

gorithm to  incorporate lookahead  and then examine the time complexity 

of  the  algorithm. 

4.2.19.  Modify  Algorithm  4.4 to  produce  a  right  parse. 
4.2.20.  Modify Algorithm 4.6 to  produce  a  left parse. 
4.2.21.  Show  that  it  is  possible  to  modify  Algorithm  4.4  to  produce  a  parse  in 

linear  time  if,  in  constructing  the parse  table,  we  add  pointers  with  each 
A  in  ttj  to  the  B  in  t~k and  C  in  tt+k.j-k  that  caused  A  to  be  placed  in 

t~j  in  step  (2)  of Algorithm  4.3. 

4.2.22.  Show  that  if Algorithm 4.5  is  modified to  include pointers  from  an  item 

to  the  other  items  which  caused it  to  be  placed  on  a  list,  then  a  right 

(or left)parse  can  be  obtained  from the parse lists in  linear  time. 

4.2.23.  Modify  Algorithm  4.6  to  work  on  arbitrary  CFG's  (including  those 

with  cycles).  Hint:  Include  the  pointers  in  the  parse  lists  as  in  Exercise 
4.2.22. 

332  GENERAL PARSING  METHODS  CHAP.  4 

4.2.24.  What  is  the  maximum  number  of  items  that  can  appear  in  a  list Ij  in 

Algorithm 4.5 ? 

*4.2.25.  A  grammar  G  is  said  to  be  of finite  ambiguity if  there  is  a  constant  k 

such  that  if w is in L(G), then  w has  no  more  than  k  distinct  left parses. 

Show  that  Earley's  algorithm takes  time 0(n 2)  on  all  grammars  of finite 

ambiguity. 

Open Problems 

There  is  little  known  about  the  actual  time  necessary  to  parse  an 
arbitrary  context-free  grammar.  In  fact,  no  good  upper  bounds  are 
known  for  the  time  it  takes  to  recognize  sentences  in L(G) for  arbitrary 

CFG  G,  let  alone  parse  it.  We  therefore  propose  the  following  open 
problems  and  research  areas. 

4.2.26.  Does  there  exist  an  upper  bound  lower  than  0(n 3)  on  the  time  needed 

to  recognize  an  arbitrary  CFL  on  some reasonable  model  of  a  random 
access  computer  or  a  multitape  Turing  machine ? 

4.2.27.  Does  there  exist  an  upper  bound  better  than  0(n 2)  on  the  time  needed 

to  recognize unambiguous  CFL's ? 

Research Problems 

4.2.28.  Find  a  CFL  which  cannot  be  recognized  in  time  f(n)  on  a  random 

access  computer  or  Turing  machine  (the  latter  would  be  easier),  where f(n) 

grows  faster  than  n;  i.e.,  lim,_.~ (n/f (n)) =  0.  Can  you find  a  CFL 
which  appears  to  take  more  than  0(n 2) time for  recognition,  even if  you 

cannot  prove this  to  be so ? 

4.2.29.  Find  large  classes  of  CFG's  which  can  be  parsed  in  linear  time  by 

Eadey's  algorithm.  Find  large  classes  of  ambiguous  CFG's  which  can 
be  parsed  in  time  0(n 2)  by  Earley's  algorithm.  It  should  be  mentioned 
that  all the  deterministic CFL's  have  grammars in  the  former class. 

Programming  Exercises 

4.2.30.  Use  Earley's  algorithm  to  construct  a  parser  for  one  of  the  grammars 

in the Appendix. 

4.2.31.  Construct  a  program  that  takes  as  input  any  CFG  G  and  produces  as 

output  a  parser for  G that uses Earley's algorithm. 

B I B L I O G R A P H I C   N O T E S 
Algorithm  4.3  has  been  discovered  independently  by  a  number  of  people. 
Hays  [1967] reports  a  version  of it,  which  he  attributes  to  J. Cocke. 

Younger  [1967] uses  Algorithm  4.3  to  show  that  the  time  complexity  of  the 
membership problem  for  context-free languages  is  0(n3).  Kasami  [1965] also  gives 

a  similar  algorithm.  Algorithm  4.5  is  found  in  Earley's  Ph.D.  thesis  [1968]. An 
0(n 2) parsing  algorithm  for  unambiguous  CFG's  is reported  by  Kasami  and  Torii 

[1969]. 

5 

ONE -PAS S  N O  
BACKTRACK  PARSING 

In  Chapter  4  we  discussed  backtrack  techniques  that  could  be  used  to 
simulate  the  nondeterministic  left  and  right  parsers  for  large  classes  of con- 
text-free  grammars.  However,  we  saw  that  in  some  cases  such  a  simulation 
could  be  quite  extravagant  in  terms  of time.  In  this  chapter  we  shall  discuss 
classes  of context-free  grammars  for  which  we  can  construct  efficient  parsers 
--parsers  which  make  cln  operations  and  use  c2n  space  in  processing  an 

input  of length  n,  where  cl  and  c2  are  small  constants. 

We  shall  have  to  pay  a  price  for  this  efficiency,  as  none  of the  classes  of 
grammars  for  which  we  can  construct  these  efficient  parsers  generate  all 
the context-free languages.  However, there is strong evidence that the restrict- 
ed  classes  of grammars  for  which  we  can  construct  these  efficient parsers  are 
adequate  to  specify all  the  syntactic features  of programming  languages  that 
are  normally  specified  by  context-free  grammars. 

The  parsing  algorithms  to  be  discussed  are  characterized  by  the  facts 
that  the  input  string  is  scanned  once  from  left  to  right  and  that  the  parsing 
process  is  completely  deterministic.  In  effect,  we  are  merely  restricting  the 
class  of  CFG's  so  that  we  are  always  able  to  construct  a  deterministic  left 
parser  or  a  deterministic  right  parser  for  the  grammar  under  consideration. 

The  classes  of grammars  to  be  discussed  in  this  chapter  include 

(1)  The  LL(k)  grammars--those  for  which  the  left  parser  can  be  made 
to  work  deterministically  if it  is  allowed  to  look  at  k  input  symbols  to  the 
right  of its  current  input  position.t 

(2)  The  LR(k)  grammars--those  for  which  the  right  parser  can  be  made 

tThis  does not involve an extension of the definition of a DPDT.  The k  "lookahead symbols" are stored in the finite control. 

333 

334  ONE-PASS NO BACKTRACK PARSING  CHAP. 5 
to  work  deterministically if it  is  allowed  to  look  k  input  symbols beyond  its 

current  input  position. (3)  The  precedence  grammars--those  for  which  the  right  parser  can  find 

the  handle  of  a  right-sentential  form  by  looking  only  at  certain  relations 

between  pairs  of adjacent  symbols  of that  sentential  form. 

5.1.  LL(k)  GRAMMARS 

In  this  section we  shall  present  the  largest  "natural"  class  of left-parsable 
grammars--the  LL(k)  grammars. 

5.1.1.  Definition  of  LL(k)  Grammar 

As  an  introduction,  let  G  =  (N,  I#, P,  S)  be  an  unambiguous  grammar 
and  w =  ata 2  .. .  a,  a  sentence  in L(G).  Then  there  exists  a  unique  sequence 

of  left-sentential  forms  0%, e l , . - . ,   em  such  that  S  =  ~0,  ei P'==> ~i+1  for 
0  <  i  <  m  and  e m =   w.  The  left  parse  for  w  is PoP1  "'"  Pro-l" NOW,  suppose  that  we  want  to  find  this  left  parse  by  scanning  w  once 

from  left  to  right.  We  might  try  to  do  this  by  constructing  Co, e l , . . . ,   era, 
the  sequence  of left-sentential  forms.  If el  =  al  ""  " aiAfl,  then  at  this  point 
we  could  have  read  the  first j  input  symbols  and  compared  them  with  the 

first j  symbols of 0~i. It would  be  desirable  if ei+ i  could be  determined  know- 

ing  only  al  .. .  aj  (the  part  of  the  input  we  have  scanned  to  this  point), 
the next few input  symbols (aj+laj+2  . . .   aj+~  for  some fixed k),  and the non- 

terminal  A.  If these  three  quantities  uniquely  determine  which  production  is 
to  be used to  expand  A, we can then precisely determine  et+~ from e~ and  the 
k  input  symbols  aj+laj+ z  . . .   aj+ k. A  grammar  in  which  each  leftmost  derivation  has  this  property  is  said 

to  be  an  LL(k)  grammar.  We  shall  see  that  for  each  LL(k)  grammar  we can 
construct  a  deterministic  left  parser  which  operates  in  linear  time.  A  few 

definitions  are  needed  before  we  proceed. 

DEFINITION 

Let  e  =  xfl  be  a  left-sentential  form  in  some  grammar  G  =  (N,  ~, P,  S) 
such  that  x  is in  If*  and  fl  either  begins  with  a  nonterminal  or  is  e.  We  say 
that x is the closed portion  of e  and that  fl is the open portion  of e.  The bound- 

ary  between  x  and  ,8 will  be  referred  to  as  the  border. 

Example 5.1 

Let  ~  ---- abacAaB.  The  closed  portion  of  ~  is  abac;  the  open  portion  is 
A aB .  If  ~  ---- abc,  then  abc  is  its  closed  portion  and  e  its  open  portion.  Its 
border  is  at  the  right  end. 

sEc. 5.1  LL(k) GRAMMARS  335 

The  intuitive  idea  behind  LL(k)  grammars  is  that  if we  are  constructing 
a  leftmost  derivation  S =~  w and  we have  already  constructed  S  =~ ~  :=~ ~2 

lm  Im  lm 

=*  . . .   =~  et~ such  that  ~  =*  w,  then  we  can construct  ~+1,  the  next  step  of lm  lm  Ira 

the  derivation,  by  observing  op, ly  the  closed  portion  of  ~i  and  a  "little 
more,"  the  "little  more"  being  the  next  k  input  symbols  of  w.  (Note  that 
the  closed  portion  of  ~t  is  a  prefix  of  w.)  It  is  important  to  observe  that  if 
we  do  not  see  all  of  w when  ~,+1 is  constructed,  then we do not really know 
what  terminal  string  is  ultimately derived  from S.  Thus the LL(k)condition 

implies  that  ~;+i  is  substantially independent  (except  for the  next k  terminal 

symbols) of what is derived  from  the  open  portion  of ~. 

Viewed  in  terms  of a  derivation  tree,  we  can  construct  a  derivation  tree 
for a  sentence  wxy  in  an  LL(k) grammar  starting from  the  root  and working 
top-down  determfiaistieally.  Specifically,  if  we  have  constructed  the  partial 
derivation  tree  with  frontier  wArE,  then  knowing  w  and  the  first  k  symbols 

of xy  we  would  know  which  production  to  use  to  expand  A.  The  outline  of 
the  complete  tree  is  shown  in  Fig.  5.1. 

S 

w  x  y 
Fig. 5.1  Partial derivation tree for the sentence wxy. 

Recall that in Chapter  4 we defined for a  CF G  G  --  (N,  ~;, P,  S) the func- 
tion  FIRST,(a),  where  k  is  an  integer  and  0c  is  in  (N  u  Ig)*,  to  be 

[w  in  Z * [ e i t h e r [ w [ <   k  and  ct=*  w,  o r [ w  I--  k  and  ~ = *   wx  for  some  x]. G  G 

We  shall  delete  the  subscript  k  and/or  the  superscript  G  from  FIRST 
whenever  no  confusion  will  result. 

If  ~  consists  solely  of  terminals,  then  FIRSTk(~)  is just  {w], where  w  is 
the  first  k  symbols  of  ~  if  10c[ ~  k,  and  w =  ~  if  l~]  <  k.  We  shall  write 

336  ONE-PASS NO BACKTRACK PARSING  CHAP.  5 

FIRSTk(a )  =  w,  rather  than  {w}, in  this  case.  It  is  straightforward  to  deter- 
mine  FIRST~(00  for  particular  grammar  G.  We  shall  defer  an  algorithm  to 
Section  5.1.6. 

DEFINITION 

Let  G  =  (N,  Z, P,  S)  be  a  CFG.  We  say  that  G  is L L (k ) ,   for  some  fixed 
integer  k,  if whenever  there  are  two  leftmost  derivations 

(1)  S  =>  w A e   =~  wfla  =-~ w x   and 

lra  Im 

(2)  S  =-~  w A a  =-~ w~,a =-~  wy 

Ira  Ira 

such  that  FIRSTk(x )  =  FIRSTk(y),  it  follows  that  fl  =  7'. 

Stated  less  formally,  G  is  LL(k)  if  given  a  string  w A a   in  (N  U  Z)*  and 
the  first  k  terminal  symbols  (if they  exist)  to  be  derived  from  A a  there  is  at 
most  one  production  which  can  be  applied  to  A  to  yield  a  derivation  of any 
terminal  string  beginning  with  w  followed  by  those  k  terminals. 

We  say  that  a  grammar  is LL   if it  is  LL(k)  for  some  k. 

Example 5.2 

Let  G 1 be the grammar  S  ~  aAS]  b,  A  ---~ a  l bSA .  Intuitively,  G1 is LL(1) 
because  given  C,  the  leftmost  nonterminal  in  any  left-sentential  form,  and 

c,  the  next  input  symbol,  there  is  at  most  one  production  for  C  capable  of 
deriving  a  terminal  string  beginning  with  c.  Going  to  the  definition  of  an 

LL(1)  grammar,  if  S  =~  w Sa   ~  wfla  ~  wx  and  S  ~  w S a  =~  wTa  ~  wy 

lra  lm  lm  lrn  lm  lra and  x  and  y  start  with  the  same  symbol,  we must  have  fl  =  7'. Specifically,  if 

x  and  y  start  with  a,  then  production  S  ---~ aA S   was  used  and  fl  =  7' =  aA S. 

S---~  b  is  not  a  possible  alternative.  Conversely,  if  x  and  y  start  with  b, 
S  ~  b  must  be  the  production  used,  and  fl  =  7' =  b.  Note  that  x  =  y  =  e 
is  impossible,  since  S  does  not  derive  e  in  G~. 

A  similar  argument  prevails  when  we  consider  two  derivations 

S  ~  wAoc ~  wile  ~  wx  and  S  ~  wAoc  ~  wT'a  =-> wy.  [~ 

lm  lm  lm  tm  lrn  Ira 

The  grammar  in  Example  5.2 is an  example  of what  is known  as  a  simple 
LL(!)  grammar. 

DEFINITION 

A  context-flee  grammar  G  =  (N,  E, P,  S)  with  no  e-productions  such 
that for all A  ~  N  each alternate  for A  begins with a  distinct  terminal  symbol 
is  called  a  simple  LL(1)  grammar.  Thus  in  a  simple  LL(1)  grammar,  given 

a  pair  (A,  a),  where  A  ~  N  and  a  ~  Z,  there  is  at  most  one  production  of 
the  form  A  --,  aa. 

SEC. 5.1  LL(k) GRAMMARS  337 

Example 5.3 

Let  us  consider  the  more  complicated  case  of the  grammar  G 2 defined  by 
S  ---~ e tabA,  A  ~  S a a l b .   We shall show that  G2 is LL(2).  To do this,  we shall 
show  that  if  wBtx  is  any  left-sentential  form  of  G2  and  wx  is  a  sentence  in 
L(G),  then  there  is  at  most  one  production  B ~ f l   in  Gz  such  that 

FIRST2(flt~ )  contains  FIRST2(x ).  Suppose  that  S  ~  wSt~  ~  wflo~ ==~ wx 

lm  lm  lm 

and  S  ~  wSoc  :=~ w~,o~ ~  wy,  where the  first  two  symbols of x  and y  agree if 

lm  Im  lm they  exist.  Since  G2  is  a  linear  grammar,  ~  must  be  in  (a  +  b)*.  In  fact, 

we  can  say  more.  Either  w =  ~  =  e,  or  the  last  production  used  in  the  deri- 

vation  S  ~  wSoc  was  A  ~  Saa.  (There  is  no  other  way  for  S  to  "appear" 

lm in  a  sentential  form.)  Thus  either  ~  =  e  or  ~  begins  with  aa. 

Suppose  that  S  ---~ e  is  used  going from  wSt~  to  wilts.  Then  fl  =  e,  and  x 
is  either  e  or  begins  with  aa.  Likewise,  if  S  ~  e  is  used  going  from  wSt~  to 

w},~, then  ~  =  e  and  y  =  e,  or  y  begins  with  aa.  If  S  ~  abA  is  used  going 
from wSoc to  wflo~,  then  fl  =  abA,  and x  begins with ab.  Likewise, if S  ---~ abA 
is  used  going  from  wStx  to  wToc ,  then  ~, =  abA,  and  y  begins  with  ab.  There 

are thus  no possibilities  other than  x  =  y  =  e, x  and y  begin with aa,  or both 
begin with ab.  Any other condition  on the first two symbols of x  and y implies 
that  one  or  both  derivations  are  impossible.  In  the  first  two  cases,  S  ---~ e  is 

used  in  both  derivations,  and  fl  =  ?  =  e.  In  the  third  case,  S  ---~ abA  must 
be  used,  and  fl  =  ~, =  abA. 

It  is  left  for  the  Exercises  to  prove  that  the  situation  in  which  A  is  the 
symbol to  the  right  of the  border  of the  sentential  form in  question  does  not 
yield  a  contradiction  of  the  LL(2)  condition.  The  reader  should  also  verify 

that  G~  is  not  LL(1).  D 

Example  5.4 

Let  us  consider  the  grammar  G 3 =  ({S, A,  B}, {0,  1, a,  b}, P3,  S),  where 
P3  consists  of  S  --~  A  IB,  A  --~  aAb  l O,  B  ~  aBbb  l l.  L(G3)  is  the  language 

(a'Ob"in  ~  0}  u  {a"lb2"! n  ~  0}.  G 3  is  not  LL(k)  for  any  k.  Intuitively,  if 
we  begin  scanning  a  string  of  a's  which  is  arbitrarily  long,  we  do  not  know 

whether the production  S  ---~ A  or S  ~  B was first used until a  0  or  1 is seen. 

Referring  to  the  definition  of  LL(k)  grammar,  we  may  take  w  =  a  =  e, 
fl  =  A,  ~,  =  B,  x  =  akOb k,  and  y  =  a klb 2k  in  the  derivations 

0  , S  ~  S  ~  A  ~  akOb k 
lm  lm 

0  * S  ~  S  ~  B  ~  aklb 2k 

lm  lm 

to  satisfy conditions  (1)  and  (2)  of the  definition.  Moreover,  x  and y  agree  in 

338  ONE-PASS NO BACKTRACK PARSING  CHAP.  5 

the  first k  symbols.  However,  the  conclusion  that  fl  --  ~, is false.  Since k  can 
be  arbitrary  here,  we  may  conclude  that  G 3 is  not  an  LL  grammar.  In  fact, 
in  Chapter  8 we  shall  show  that  L(G3)  has  no  LL(k) grammar.  F---] 

5.1.2.  Predictive  Parsing Algorithms 

We shall show that we can parse LL(k) grammars very conveniently using 
what we call a k-predictive parsing algorithm.  A k-predictive parsing algorithm 
for  a  CFG  G  =  (N, X, P,  S)  uses  an  input  tape,  a  pushdown  list,  and  an 

output tape as shown in Fig.  5.2. The k-predictive parsing algorithm attempts 
to  trace  out  a  leftmost  derivation  of the  string placed  on its input  tape. 

i] 
7 

I  w  ,'l  /'/ 

F  ,i 

X 
Input head 

Input tape 

Output tape 
Fig. 5.2  Predictive parsing algorithm. 
The  input  tape  contains  the  input  string  to  be  parsed.  The  input  tape  is 
read  by an  input  head  capable  of reading  the  next  k  input  symbols  (whence 
the  k in k-predictive).  The  string  scanned  by the input  head  will  be called the lookahead string. 

In  Fig.  5.2 the substring  u of the input  string wux represents 
the  lookahead  string. 

The  pushdown  list  contains  a  string  X0c$, where  X'a is  a  string  of  push- 
down  symbols  and  $ is  a  special  symbol  used  as  a  bottom  of the  pushdown 
list  marker.  The  symbol  X is  on  top  of the  pushdown  list.  We  shall  use r'  to 

represent  the  alphabet  of pushdown  list  symbols  (excluding  $). 

The  output  tape  contains  a  string  7z of production  indices. 
We  shall  represent  the  configuration  of a  predictive  parsing  algorithm  by 
a  triple  (x, X~, n),  where 

(1)  x  represents  the  unused  portion  of  the  original  input  string. 
(2)  Xa  represents  the  string  on  the  pushdown  list  (with  X  on  top). 
(3)  n  is  the  string  on  the  output  tape. 
For  example,  the  configuration  in  Fig.  5.2 is (ux, Xa$,  ~r). 

SEC. 5.1  LL(k) GRAMMARS  339 

The  action  of a  k-predictive  parsing  algorithm  ~  is  dictated  by a  parsing 
table  M,  which  is  a  mapping  from  (F  u  {$}) x  Z.k  to  a  set  containing  the 

following  elements" 

(1)  (fl, i),  where  fl  is  in  F*  and  i  is  a  production  number.  Presumably, 
fl will  be  either  the  right  side  of production  i  or  a  representation  of it. 

(2)  pop. (3)  accept. 

(4)  error. 
The  parsing  algorithm  parses  an  input  by  making  a  sequence  of moves, 
each move being very similar to  a  move of a pushdown  transducer.  In a  move 
the  lookahead  string  u  and  the  symbol  X  on  top  of  the  pushdown  list  are 
determined.  Then the entry M(X,  u) in the parsing table is consulted  to deter- 
mine  the  actual  move  to  be  made.  As  would  be  expected,  we  shall  describe 
the  moves  of  the  parsing  algorithm  in  terms  of  a  relation  ~  on  the  set  of 
configurations.  Let  u  be  FIRSTk(x ).  We  write 

(1)  (x, X0c, 70  ~  (x, fl0~, 7ti)  if  M(X,  u)  =  (fl,  i).  Here  the  top  symbol X on the pushdown  list is replaced  by the  string  fl  ~  IF'*, and the  production 

number  i is  appended  to  the  output.  The  input  head  is  not  moved. 

(2)  (x, a0~, 70 1-- (x',  0c, n)  if M(a,  u) =  pop and x  =  ax'.  When the  symbol 
on  top  of  the  pushdown  list  matches  the  current  input  symbol  (the  first 
symbol of the  lookahead  string),  the  pushdown  list  is  popped  and  the  input 
head  is  moved  one  symbol  to  the  right. 

(3)  If  the  parsing  algorithm  reaches  configuration  (e,  $, n),  then  parsing 
ceases,  and  the  output  string  zt is  the parse  of the  original  input  string.  We 

shall  assume  that  M($,  e)  is  always  accept.  Configuration  (e,  $,  70  is  called accepting. 

(4)  If  the  parsing  algorithm  reaches  configuration  (x,  X0~, 10  and M(X, 

u)  =  error,  then  parsing  ceases,  and  an  error  is  reported.  The configu- 
ration  (x,  X~, n)  is  called  an  error  configuration. 

If  w  ~  E*  is  the  string  to  be  parsed,  then  the  initial configuration  of the 
parsing  algorithm  is  (w, X0$,  e),  where  Xo  is  a  designated  initial  symbol. 
If  (x,  X0$,  e)I-~--(e,  $, 70,  we  write  ~ ( w ) =   7t  and  call  n  the  output  of 
for  input  w.  If (w, X0$, e)  does  not  reach  an  accepting  configuration,  we  say 

that  ~(w)  is undefined.  The  translation  defined  by ~,  denoted  z(~),  is the  set 

of pairs  {(w, 7t) [~(w)  =  n}. 

We  say  that  ~  is  a  valid k-predictive parsing  algorithm  for  CFG  G if 

(1)  L(G)  =  {wta(w)  is  defined},  and 
(2)  If a(w)  =  n,  then  7t is  a  left  parse  of w. 

If  a  k-predictive  parsing  algorithm  ~  uses  a  parsing  table  M  and  a  is 
a  valid  parsing  algorithm  for  a  CFG  G,  we  say  that  M  is  a  valid parsing table 

for  G. 

340  ONE-PASS NO  BACKTRACK PARSING  CHAP.  5 

Example 5.5 

Let  us  construct  a  1-predictive  parsing  algorithm  et  for  G~,  the  simple 
LL(1)  grammar  of Example  5.2.  First,  let  us  number  the  productions  of  G1 as  follows: 

(1)  S -  ,~ aAS 
(2)  S  ~  b 

(3)  A  .~ a 
(4)  A~  ,~bSA 

A  parsing  table  for  a  is  shown  in  Fig.  5.3. 

Symbol on top of 
pushdown list 

Lookahead  string 
a  e 
aAS, 1 

a, 3 
pop 
error 
error 

b, 2 
bSA, 4 

error 

pop 
error 

error 

,  , 

error 

error 

error 

. 

accept 

Fig. 5.3  Parsing table for a. 
Using  this  table,  et would  parse  the  input  string  abbab  as  follows" 

(abbab,  S$,  e)  ~  (abbab,  aAS$,  1) 

1-  (bbab, ASS,  1) 
~-  (bbab, bSAS$,  14) 

k-  (bab,  SAS$,  14) 
}--- (bab, bASS,  142) 

k-- (ab, ASS,  142) 

k-  (ab, aSS,  1423) 

(b,  S$,  1423) 

(b, b$,  14232) 
l--- (e,  $,  14232) 

SEC. 5.1  LL(k) GRAMMARS  341 

For  the  first  move  M(S,  a)  =  (aAS,  1),  so  S  on  top  of the  pushdown  list  is 
replaced  by  aAS,  and  production  number  1  is  written  on  the  output  tape. 

For  the  next  move  M(a,  a)  =  pop  so  that  a  is  removed  from  the  pushdown 
list,  and  the  input  head  is  moved  one  position  to  the  right. 

Continuing  in  this  fashion,  we  obtain  the  accepting  configuration 
(e,  $,  14232).  It  should  be  clear  that  14232  is  a  left  parse  of  abbab,  and,  in 

fact,  a~ is  a  valid  1-predictive  parsing  algorithm  for  G 1. 

A  k-predictive  parsing  algorithm  for  a  CFG  G  can  be  simulated  by  a 
deterministic  pushdown  transducer  with  an  endmarker  on  the  input.  Since 

a  pushdown  transducer  can  look  only  at  one  input  symbol,  the  lookahead 
string  should  be  stored  as  part  of  the  state  of the  finite  control.  The  rest  of 

the  simulation  should  be  obvious. 

THEOREM 5.1 

Let a~ be a  k-predictive  parsing  algorithm for  a  CFG  G.  Then  there  exists 
a  deterministic pushdown transducer  T such that z(T)  =  ((w$, z01 a~(w) =  n~}. 

Proof.  Exercise.  [[] 

COROLLARY 

Let  ~  be  a  valid  k-predictive  parsing  algorithm  for  G.  Then  there  is 
a  deterministic  left  parser  for  G. 

Example 5.6 

Let  us  construct  a  deterministic  left parser  P~ for  the  1-predictive  parsing 
algorithm  in  Example  5.5.  Since  the  grammar  is  simple,  we  can  obtain 

a  smaller  DPDT  if we  move  the  input  head  one  symbol  to  the  right  on  each 

move.  The  left  parser  will use  $ as both  a  right  endmarker  on  the  input  tape 

and  as  a  bottom  of the  pushdown  list  marker. Let  P~ =  ([q0, q, accep(}, (a,  b,  $}, {S, A,  a,  b,  $~}, ~, q0,  $, [accept]),  where 

is defined  as  follows" 

~(q0,  e,  $)  =  (q,  S$,  e) 

~(q, a,  S)  =  (q, AS,  1) 

~(q,  b, S)  =  (q,  e, 2) 
~(q, a, A)  =  (q, e,  3) 

~(q, b, A)  =  (q, SA,  4) 

~(q,  $,  $)  =  (accept,  e, e) 

It  is  easy  to  see  that  (w$,  n)  ~  z(P~) if and  only  if  ~(w)  =  n.  I[] 

342  ONE-PASS  NO  BACKTRACK  PARSING  CHAP.  5 
5.1.3.  Implications  of  the  LL(k)  Definition 

We  shall  show  that  for  every  LL(k)  grammar  we  can  mechanically  con- 
struct  a  valid  k-predictive  parsing  algorithm.  Since  the  parsing  table  is  the 
heart  of the  predictive  parsing  algorithm,  we  shall  show  how  a  parsing  table 
can  be  constructed  from  the  grammar.  We  begin  by  examining  the  impli- 
cations  of the  LL(k)  definition. 

The  LL(k)  definition  states  that,  given  a  left-sentential  form  w a s ,   then 
w and  the  next  k  input  symbols  following  w will  uniquely  determine  which 
production  is  to  be  used to  expand  A.  Thus  at  first  glance  it  might  appear 

that  we  have  to  remember  all  of  w to  determine  which  production  is  to  be 

used next.  However,  this is not  the case.  The following theorem  is fundamen- 
tal  to  an  understanding  of  LL(k)  grammars. 

THEOREM 5.2 

Let G  =  (N,  E,  P,  S) be a  CFG.  Then  G is LL(k) if and  only if the follow- 
ing  condition  holds"  If  A  ---~ fl  and  A  ---~ ~,  are  distinct  productions  in  P, 

then  FIRSTk(fl0 0  ~  FIRSTk(T0 0  =  ~ '   for  all  wAtt  such  that  S = ~   wAtx. lm 

Pr o o f  

Only  if:  Suppose  that  there  exist  w,  A,  ~,  fl,  and  7  as  above,  but 
FIRSTk(fltx) ~  FIRSTk(7~)  contains  x.  Then  by  definition  of  FIRST,  we 

have  derivations  S  ~  wAoc =~  wflo~ =~  w xy  and  S  ~  wAoc =~  wTo~ =-~  w xz  Im  Im  lm  lm  lm  lm 
for  some  y  and  z.  (Note  that  here  we  need  the  fact  that  N  has  no  useless 

nonterminals,  as  we  assume  for  all  grammars.)  If  Ix I <   k,  then  y  =  z =  e. 

Since  fl  ~  7,  G  is  not  LL(k). 

lf:  Suppose  that  G  is  not  LL(k).  Then  there  exist  two  derivations 
S  ~  wAoc ~  wfloc  ~  wx  and  S  =~  wAoc =-~  wT~  =-~  wy  such  that  x  and  y lm  lm  lm  lm  lm  lm 
agree up to the first k  places,  but fl  ~  ~,. Then A  ---~ fl and A  ~  y are distinct 
productions  in  P,  and  FIRST(fiE)  and  FIRST(?~)  each  contain  the  string 

FIRST(x),  which  is  also  FIRST(y).  [~] 

Let  us  look  at  some  applications  of  Theorem  5.2  to  LL(1)  grammars. 
Suppose  that  G  =  (N,  E, P,  S)  is  an  e-free  CFG,  and  we  wish  to  determine 
whether  G  is  LL(1).  Theorem  5.2  implies  that  G  is  LL(1)  if  and  only  if for 
all  A  in  N  each  set  of  A-productions  A  --,  ~1 I ~ z l ' "   I~.  in  P  is  such  that 

FIRST~(gl),  F I RS T ~ ( a 2 ) ,. ..,   FIRSTI(~ .)  are  all  pairwise  disjoint.  (Note 
that  e-freedom  is  essential  here.) 

Example  5.7 

The grammar  G having the  two  productions  S---,  a S  a  cannot  be  EL(l), 
since  F I R S T 1 ( a S ) =   F I R S T , ( a ) =   a.  Intuitively,  in  parsing  a  string  begin- 

SEC. 5.1  LL(k) GRAMMARS  343 
ning  with  an  a,  looking  only  at  the  first  input  symbol  we  would  not  know 

whether to use S  ~  aS   or S  --~ a to expand  S.  On the other hand,  G is LL(2). 

Using  the  notation  in  Theorem  5.2,  if  S  =~  wAoc,  then  A  =  S  and  0c =  e. lm 
The  only  two  productions  for  S  are  as  given,  so  that  fl  =  a S  and  ~, =  a. 

Since  FIRST2(aS)  =  aa and  FIRST2(a )  =  a,  G is LL(2) by Theorem  5.2.  E] 

Let  us  consider  LL(1)  grammars  with  e-productions.  At  this  point  it  is 
convenient  to  introduce  the  function  FOLLOW~. 

DEFINITION 

Let  G  =  (N,  Z, P,  S)  be  a  CFG.  We  define  FOLLOW~(fl),  where  k  is 
an  integer  and  fl  is  in  (N  U  E)*,  to  be  the  set  {wl S  ~  eft?  and  w  is  in 

FIRST~(?)}.  As  is  customary,  we  shall  omit  k  and  G  whenever  they  are 
understood. 

Thus,  FOLLOW~(A) includes  the  set  of terminal  symbols  that  can  occur 
immediately to  the  right  of A  in  any  sentential  form,  and  if eA  is a  sentential 
form,  then  e  is  also  in  FOLLOW~(A). 

We  can  extend  the  functions  FIRST  and  FO L L OW   to  domains  which 
are  sets  of strings  rather  than  single  strings,  in  the  obvious  manner.  That  is, 
let  G  =  (N,  E, P,  S)  be a  CFG.  If L  ~  (N  U  Z)*,  then 

FIRST~(L)  =  {wl for  some  a  in  L,  w is  in  FIRS'r~(a)} 
and  FOLLOW~(L)  =  [wl for  some  a  in  L,  w is  in  FOLLOW~(a)}. 

For  LL(1)  grammars  we  can  make  the  following  important  observation. 

THEOREM 5.3 

A  C FG   G  =  (N,  E, P,  S)  is  LL(1)  if  and  only  if the  following  condition 
holds"  For  each  A  in  N,  if A  --,  fl  and  A  ~  7 are  distinct  productions,  then 

FIRST1( ~  FO LL O W i (A))  ~  FIRST1( 7  F O L L O W i ( A ) ) =   ~ .  

Proof.  Exercise.  [Z] 

Thus  we  can  show  that  a  grammar  G  is  LL(1)  if and  only  if for  each  set 
of A-productions  A  --~ tz~ [~21-''  I~n the  following  conditions  hold" 

(1)  FIRSTi(tz~),  FIRST i(tz2), .. .,   FIRSTi(tzn)  are  all  pairwise  disjoint. 

(2)  If  ~t ~  e,  then  FIRST~(~i)  A  FOLLOW~(A)  =  ~  for  1 < j   <  n, i ~ j .  

These  conditions  are  merely  a  restatement  of Theorem  5.3.  We  should  cau- 
tion  the  reader  that,  appealing  as  it  may  seem,  Theorem  5.3  does  not  gener- 

alize directly.  That  is, tet  G be a  C FG   such that  statement  (5.1.1)  holds" 

If A  --~  fl  and  A  --~ 7 are  distinct  A-productions,  then (5.1.1) 

FIRSTk( fl  FOLLOWk(A))  A  FIRSTk( 7  F O L L O W k (A ) )=  

344  ONE-PASS  NO  BACKTRACK  PARSING  CHAP. 5 

Such  a  grammar  is  called  a  strong  LL(k)  grammar.  Every  LL(1)  grammar  is 
strong.  However,  the  r~ext example  shows  that  when  k  >  1  there  are  LL(k) 

grammars  that  are  not  strong  LL(k)  grammars. 

Example 5.8 

Consider  the  grammar  C,  defined  by 

S  > aAaalbAba 
A  > ble  

Using  Theorem  5.2,  we  can  verify  that  G  is  an  LL(2)  grammar.  Consider 
the  derivation  S  ~  aAaa.  We  observe  that  FIRST2(baa )  ~  FIRSTz(aa )  = 

~ .   Using  the  notation  o f   Theorem  5.2  here,  ~  =  aa,  fl  =  b,  and  ?  =  e. 
Likewise,  if  S=>  bAba,  then  F I R S T 2 ( b b a ) ~   FIR ST 2(ba )=  ~ .   Since  all 
derivations  in  G  are  of length  2,  we  have  shown  G to  be  LL(2),  by  Theorem 

5.2.  But  FOLLOWz(A)  =  [aa,  ba},  so 

FIRSTz(b  FOLLOW2(A))  ~  FIRS Tz (FO LL OW z( A) )  =  {bali, 
violating  (5.1.1).  Hence  G  is  not  a  strong  LL(2)  grammar.  D 

One  important  consequence  of the  LL(k) definition  is that  a  left-recursive 
grammar  cannot  be  LL(k)  for  any  k  (Exercise  5.1.1). 

Example 5.9 

Consider  the  grammar  G  with  the  two  productions  S  ~  Sa[ b.  From 

i Theorem  5.2,  consider  the  derivation  S  ~  Sa t,  i ~  0  with  A  =  S,  ~  =  e, 

fl  =  Sa,  and  ~, =  b.  Then  for  i ~  k,  FIRSTk(Saa t)  A  FIRSTk(ba ~) =  ba k- ~. 
Thus,  G cannot  be  LL(k)  for  any  k.  [[] 

It is also important  to  observe  that  every LL(k) grammar  is unambiguous 
(Exercise  5.1.3).  Thus,  if we are given an  ambiguous  grammar,  we can  imme- 
diately  conclude  that  it  cannot  be  LL(k)  for  any  k. 

In  Chapter  8 we  shall  see that  many  deterministic  context-free  languages 
do not have an  LL(k) grammar.  For example,  {anObn[n ~  1~ u  {a"lbZn[n ~  1} 
is  such  a  language.  Also,  given  a  C FG   G  which  is not  LL(k)for  a  fixed k, 
it  is  undecidable  whether  G  has  an  equivalent  LL(k)  grammar.  But  in  spite 

of  these  obstacles,  there  are  several  situations  in  which  various  transforma- 
tions  can  be  applied  to  a  grammar  which  is  not  LL(1)  to  change  the  given 
grammar  into  an  equivalent  LL(1) grammar.  We  shall  give two  useful  exam- 
ples  of  such  transformations  here. 

The  first  is  the  elimination  of left  recursion.  We  shall  illustrate  the  tech- 
nique  with  an  example. 

Example 5.10 

Let  G be  the  grammar  S  ----~ Salb,  which  we  saw  in  Example  5.9  was  not 

SEC. 5.1  LL(k)  GRAMMARS  345 

LL.  We  can  replace  these  two  productions  by  the  three  productions 

S  > bS' 
S'  > aS'  l e 

to  obtain  an  equivalent  grammar  G'.  Using  Theorem  5.3,  we  can  readily 

show  G' to  be  LL(1).  E] 

Another  useful  transformation  is  left  factoring.  We  again  illustrate  the 
technique  through  an  example. 

Example 5.11 

Consider  the  LL(2)  grammar  G  with  the  two  productions  S---~  aS!  a. 
We  can  "factor"  these  two  productions  by  writing  them  as  S---~  a(S[  e). 
That  is,  we  assume  that  concatenation  distributes  over  alternation  (the 
vertical  bar).  We  can  then  replace  these  productions  by 

S  > aA 
A  > S i c  

to  obtain  an  equivalent  LL(1)  grammar. 

In  general,  the  process  of  left  factoring  involves  replacing  the  produc- 
tions  A --~ tzfll l " "   [tzfln by  A ----~ tzA' and  A' ----~ fll  1""  [fin. 

5.1.4.  Parsing  LL(1)  Grammars 

The  heart  of  a  k-predictive  parsing  algorithm  is  its  parsing  table  M. 
In this section and the next we show that  every LL(k) grammar G can be left- 
parsed  by  a  k-predictive  parsing  algorithm  by  showing how  a  valid  parsing 

table can be constructed from G. We shall first consider the important  special 
case  where  G is an  LL(1)  grammar. 

ALGORITHM  5.1 

A  parsing  table  for  an  LL(1)  grammar. 

Input.  An  LL(1)  CFG  G =  (N, X, P,  S). 

Output.  M,  a  valid parsing  table  for  G. 
Method.  We  shall  assume  that  $  is  the  bottom  of  the  pushdown  list 
marker.  M  is  defined  on  (N  U  X  u  {$}) x  (X  U  {e}) as  follows: 

(1)  If A --~ ~  is  the  ith  production  in  P,  then  M(A,  a)  =  (oc, i)  for  all  a 
in  FIRST 1(~), a  ~  e.  If e is  also  in  FIRST~ (00, then  M(A,  b)  =  (oc, i)  for  all 
b in  FOLLOWl(A). 

346  ONE-PASS NO  BACKTRACK PARSING  CHAP. 5 

(2)  M(a,  a)  =  pop  for  all  a  in  ~. 
(3)  M ($,   e)  =  accept. 
(4)  Otherwise  M ( X,   a)  :  error,  for  X in N  U  ~  U  {$}, a  in  E  U  [e}.  D 

Before  we  prove  that  Algorithm  5.1  does  produce  a  valid  parsing  table 
for  G,  let  us  consider  an  example  of Algorithm  5.1. 

Example  5.12 

Let  us  consider  producing  a  parsing  table  for  the  grammar  G  with  pro- 
ductions 

(1)  E  ~  TE'  (2)  E'  .--,  -b  T E  ' 
(3)  E'---~  e  (4)  T  ~  F T'  
(5)  T'---~  *  F T'   (6)  T'--~  e 
(7)  F  ~  (E)  (8)  F  ~  a 

Using  Theorem  5.3  the  reader  can  verify  that  G  is  an  LL(1)  grammar. 
In   fact,  the  discerning  reader  will  observe that  G has  been  obtained  from  Go 

using  the  transformation  eliminating  left  recursion  as  in  Example  5.10.  Go 
is  not  LL,  by the  way. 

Let us now compute the entries for the E-row  using step  (1) of Algorithm 

E 

t 

T 

it 

a  (  )  +  *  e 
TE',  1 

FT', 4 

a, 8 

pop 

TE',  1 
FT', 4 

(E),7 

pop 

e, 3 
e, 6 

pop 

+TE', 2 

,, 

e, 6 

pop 

* FT',  5 

pop 

e, 3 

e, 6 

accept 
Fig. 5.4  Parsing table for G. 

SEC. 5.1  LL(k)  GRAMMARS  347 

5.1.  Here,  FIRSTI[TE' ] =  {(, a},  so M[E,  (] =  [TE',  1] and  M[E,  a] =  [TE',  1]. 
All  other  entries  in  the  E-row  are  error.  Let  us  now  compute  the  entries for  the  E'-row.  We  note  FIRST1[-+-  TE']  =  -t-,  so  M[E',  .+]  =  [-q- TE',  2]. 

Since  E'--~  e  is  a  production,  we  must  compute  FOLLOWl[E' ] =  {e,)}. 
Thus,  M[E',  e] =  M[E',  )] =  [e, 3]. All  other  entries  for  E'  are error.  Continu- 
ing  in  this  fashion,  we  obtain  the  parsing  table  for  G  shown  in  Fig.  5.4. 

Error  entries  have  been  left  blank. 

The  1-predictive  parsing  algorithm  using  this  table  would  parse  the  input 
string  (a  *  a)  in  the  following  sequence  of  moves: 

THEOREM 5.4 

[(a  *  a),  E$,  e]  [(a *  a),  TE'$,  1] 

[(a *  a),  FT'E'$,  14] 
[(a  *  a),  (E)T'E'$,  147] 
[a *  a),  E)T'E'$,  147] 
[a *  a),  TE')T'E'$,  1471] 
]--  [a *  a),  FT'E')T'E'$,  14714] 

[a *  a), aT'E')T'E'$,  147148] 
[,  a),  T'E')T'E'$,  147148] 
]--  [.  a),.  FT'E')T'E'$,  1471485] 

[a), FT'E')T'E'$,  1471485] 

[a),  aT'E')T'E'$,  14714858] 
1-- D, T'E')T'E'$,  14714858] 

[), E')T'E' $,  147148586] 
i--  D, )T'E'$,  1471485863] 

[e,  T'E',  1471485863] 

[e, E'$,  14714858636] 
[e,  $,  147148586363] 

Algorithm  5.1  produces  a  valid  parsing  table  for  an  LL(I)  grammar  G. 
Proof.  We  first  note  that  if  G  is  an  LL(1)  grammar,  then  at  most  one 
value  is  defined  in  step  (1)  of  Algorithm  5.1  for  each  entry  M(A,  a)  of  the 
parsing  matrix.  This  observation  is  merely  a  restatement  of  Theorem  5.3. 

Next,  a  straightforward  induction  on  the  number  of  moves  executed  by 
a  1-predictive  parsing  algorithm  ~  using  the  parsing  table  M  shows  that  if (xy,  S$,  e)1--~-(y,  oc$, zt), 

then  S ~   x0~. Another  induction  on  the  number 
of  steps  in  a  leftmost  derivation  can  be  used  to  show  the  converse,  namely 

348  ONE-PASS  NO  BACKTRACK PARSING  CHAP.  5 
that  if  S"=~  xa,  where  a  is  the  open  portion  of  xa,  and  FIRSTI(y )  is  in 

FIRSTI(a),  then  (xy,  S$,  e) l *   (Y, aS,  n).  It  then  follows  that  (w,  S$,  e) 
I*-- (e,  $,  ~z) if  and  only  if  S  "=*  w.  Thus  ff  is  a  valid  parsing  algorithm  for 

G,  and  M  a  valid  parsing  table  for  G.  V-] 

5.1.5.  Parsing  LL(k)  Grammars 

Let  us  now  consider  the  construction  of  a  parsing  table  for  an  arbitrary 
LL(k)  grammar  G  =  (N,  E, P,  S),  where  k  >  1.  If G is a  strong  LL(k) gram- mar, 

then  we  can  use  Algorithm  5.1  with  lookahead  strings  of length  up  to 
k  symbols.  However,  the  situation  is  somewhat  more  complicated  when  G 
is  not  a  strong  LL(k)  grammar.  In  the  LL(1)-predictive  parsing  algorithm 
we  placed  only  symbols  in  N  u  E  on  the  pushdown  list,  and  we  found  that 
the  combination  of  the  nonterminal  symbol  on  top  of  the  pushdown  list 
and  the  current  input  symbol  was  sufficient  to  uniquely  determine  the  next 
production  to  be applied.  However,  when  G is not  strong,  we find that  a  non- 

terminal symbol and the lookahead  string are not always sufficient to uniquely 
determine  the  next  production. 

For  example,  consider  the  LL(2)  grammar 

S  > aAaa tbAba 
A  >hie 
of Example  5.8.  Given  the  nonterminal  A  and  the  lookahead  string  ba we  do 
not  know  whether  we  should  apply  production  A  ~  b  or  A  ~  e. 

We  can,  however,  resolve  uncertainties  of this  nature  by  associating  with 
each  nonterminal  and  the  portion  of a  left-sentential  form which  may appear 
to  its  right,  a  special  symbol  which  we  shall  call  an  LL(k)  table  (not  to  be 
confused  with  the  parsing  table).  The  LL(k)  table,  given  a  lookahead  string, 
will  uniquely  specify  which  production  is  to  be  applied  next  in  a  leftmost 

derivation  in  an  LL(k)  grammar. 

DEFINITION 

Let  E  be  an  alphabet.  If L 1 and  L 2  are  subsets  of  X*,  let 

L1 ~)k L2  =  {w I for  some  x  ~  L 1 and  y  ~  L2,  we  have  w  =  xy 

if [xyl  ~  k  and  w is  the  first  k  symbols  of xy  otherwise]. 

Example 5.13 

Let  L,  =  [e,  abb)  and  L2  :  [b,  bab].  Then  L1  O2  L2  =  {b, ba, ab}. 

The  ~ k   operator  is  similar  to  an  infix  FIRST  operator. 
LEMMA 5.1 

For  any  C FG   G  =  (N,  E,P,  S),  and  for  all  a  and  fl  in  (N  u  E)*, 
FIRST~(afl)  =  FIRST~(a)  @)~ FIRST~(fl). 

sEc. 5.1  LL(k)  GRAMMARS  349 

Proof  Exercise.  [B 
DEFINITION 

Let  G  =  (N,  Z, P,  S)  be  a  CFG.  For  each  A  in  N  and L  ~  Z *k we  define 
T,4.L,  the  L L ( k )   table  associated  with  A  and  L  to  be  a  function  which  given 
a  lookahead  string  u  in  Z *k returns  either  the  symbol  error  or  an  A-produc- 
tion  and  a  finite  list  of  subsets  of  Z *k. 

Specifically, 

(1)  TA,L(U)=  error  if  there  is  no  production  A--~  a  in  P  such  that 
FIRSTk(a )  @e L  contains  u. 

(2)  TA,L(U)  =  (A  ~  a,  (Y~,  Y 2 , . . . ,   Ym))  if  A  ~  a  is  the  unique  pro- 

duction  in  P  such  that  FIRSTk(a )  @e L  contains  u.  If 

a  =  x oB x x t Bz x  z  . . .   BmX m,  m > 0 ,  
where each Bt  ~  N  andxt  E  Z*, then  Yt =  FIRSTk(xiB,+ix~+~  "'"  Bmxm)@k L" 

We  shall  call  Y,  a  local follow  set  for  B,.  [If  m  =  0,  TA.L(U)  =  (A  --~  a,  ~).] 

(3)  TA,L(U)  is  undefined  if  there  are  two  or  more  productions 
A  --~  ~ , 1~ 1   "-"  1~.  such  that  FIRSTk(et)  @e L  contains  u,  for  1 _<  i  <  n, 
n  >  2.  This  situation  will  not  occur  if G  is an  LL(k)  grammar. 

Intuitively,  if  TA.L(U)=  error,  then  there  is  no  possible  derivation 

+ in  G  of  the  form  A x = - ~ u v   for  any  x E L   and  v  ~  Z*.  Whenever 

TA,L(U) =  (A --~ a, (Y1,  Y2,  . . . ,  Ym)),  there  is exactly one production,  A ---~ a, 

+ which  can  be  used  in  the  first  step  of  a  derivation  A x   ==~ uv  for  any  x  ~  L 

and  v  ~  Z*.  Each  set  of  strings  Yt  gives  all  possible  prefixes  of  length 
up  to  k  of  terminal  strings  which  can  follow  a  string  derived  from  B~  when 
we  use  the  production  A  ~  a,  where  a  =  x o B l x i B 2 x 2 . . .   BmXm,  in  any 

derivation  of the  form  A x   ~  ax  ~  uv,  with  x  in  L. lm 

By  Theorem  5.2,  G  =  (N,  Z, P,  S)  is not  LL(k)  if and  only  if there  exists 
in  (N  U  Z)*  such  that 

(1)  S  ==~ wAa,  and Im 
(2)  FIRSTk(fla)  ~  F I RS T k ( I,a )c   ~  for  some  fl  ~  ~, such  that  A  ~  fl 
and  A  --~ t'  are  in  P. 

By  Lemma  5.1  we  can  rephrase  condition  (2)  as 
(2')  If r  =  FIRSTk(a ), then  (FIRSTe(fl) Ok L)  C3 (FIRSTk(?) @k L) ~  ~ .  

Therefore,  if  G  is  LL(k),  and  we  have  the  derivation  S  =~  wAa  =~  wx, lm  lm 
then  TA,L(U) will uniquely determine  which production  is to  be used to  expand 

A,  where  u  is  FIRSTk(x)  and  L  is  FIRSTk(a ). 

350  ONE-PASS NO BACKTRACK PARSING  CHAP. 5 

Example 5.14 

Consider  the  LL(2)  grammar 

S  > aAaalbAba 
A  >ble 

Let  us  compute  the  LL(2)  table  Ts, te~, which  we  shall  denote  To.  Since S  ---~ aAaa 

is  a  production,  we  compute  FIRST2(aAaa ) Q2  {e} =  {aa, ab}. 
Likewise,  S---~ bAba  is  a  production,  and  FIRST2(bAba ) e 2   [e} =  {bb}. 
Thus  we  find  To(aa)=  (S---~ aAaa,  Y).  Y  is  the  local  follow  set  for  A; 

Y =   FIRST2(aa)02  [e]  =  [aa}. The  string  aa  is  the  string  to  the  right  of 
A  in  the  production  S---~  aAaa.  Continuing  in  this  fashion,  we  obtain  the 

aa 
ab 
bb 

table  To  shown  below" 

Table  To 
Production 

S  ~  aAaa 
S  ~  aAaa 
S  ~  bAba 

Sets 

{aa} {aa} 
{ba} 

For  each  u in  (a  -+ b) .2  not  shown,  To(u)  =  error.  D 

We  shall  now  provide  an  algorithm  to  compute  those  LL(k)  tables  for 
an  LL(k)  grammar  G which  are  needed  to  construct  a  parsing  table  for  G. 

It  should be  noted  that  if G is an  LL(1)  grammar,  this  algorithm might pro- 

duce ~nore than  one table per nonterminal.  However,  the parsers constructed 
by  Algorithms  5.1  and  5.2  will  be  quite  similar.  They  act  the  same  way  on 
inputs  in  the  language,  of course.  On  other  inputs,  the  parser  of Algorithm 

5.2 might detect the error while the parser of Algorithm 5.1 proceeds to make 

a  few more  moves. 

ALGORITHM 5.2 

Construction  of LL(k)  tables. 
Input.  An  LL(k)  CFG  G  =  (N,  E, P,  S). 

Output.  3,  the  set  of  LL(k)  tables  needed  to  construct  a  parsing  table 
for  G. 

Method. 
(1)  Construct  To,  the  LL(k)  table  associated  with  S  and  {e}. 

(2)  Initially set  ~ =  IT0}. (3)  For  each  LL(k)  table  T  in  3  with  entry 

r(u)  =  (a  ~  XoaxXxa~x~... a~x~, (r~,  r ~ , . . . ,   r , ) ) ,  

SEC. 5.1  LL(k) GRAMMARS  351 

add  to  3  the  LL(k)  table  Ts,,r,,  for  1 <  i <  m,  if  Ts,,r, is  not  already  in  3. 

(4)  Repeat  step  (3)  until  no  new  LL(k)  tables  can  be  added  to  3. 

Example  5.15 

Let  us  construct  the  relevant  set  of  LL(2)  tables  for  the  grammar 

S  ~ aAaalbAba 
A  >ble 

We  begin  with  3  =  [Ts.t,l~}. Since  Ts.t,~(aa)=  (S---~  aAaa,  {aa}),  we  must 
add  TA. taa~ tO  3.  Likewise,  since  To(bb)=  (S  ~  bAba,  {ha}),  we  must  also 

add  TA, Cbo~ to  3.  The  nonerror  entries  for  the  LL(2)  tables  TA. taa~ and  TA. Cbo~ 

are  shown  below' 

ba 
aa 

Table  Ta, taa~ 
Production 

A ----~ b 
a----~e 

Sets 

u 

ba bb 

Table  TA, tbal 
Production 

a  ----~ e 
A  ----~ b 

Sets 

At  this  point  3  =  {Ts,{e],  ZA,{aa} ,  ZA,{ba] ~  and  no  new  entries  can  be  added  to 

3  in  Algorithm  5.2  so  that  the  three  LL(2)  tables  in  3  are  the  relevant  LL(2) 
table  for  G. 

From  the  relevant  set  of  LL(k)  tables  for  an  LL(k)  grammar  G  we  can 
use  the  following  algorithm  to  construct  a  valid  parsing  table  for  G.  The  k- 
predictive  parsing  algorithm  using  this  parsing  table  will  actually  use  the 

LL(k)  tables  themselves  as  nonterminal  symbols  on  the  pushdown  list. 

ALGORITHM 5.3 

A  parsing  table  for  an  LL(k)  grammar  G  =  (N,  X, P,  S). 
Input.  An  LL(k)  C F G   G  =  (N,  X, P,  S)  and  3,  the  set  of  LL(k)  tables 
for  G. 

Output.  M,  a  valid  parsing  table  for  G. 
Method.  M  is  defined  on  (3  U  X  u  [$})  *  X.k  as  follows: 

(1)  If  A  --~ xoBlxlB2x2  ...  BmX m  is  the  ith  production  in  P  and  TA,L is 

352  ONE-PASS NO  BACKTRACK PARSING  CHAP.  5 
in  3,  then  for  all  u  such  that 

Ta,z(u)  =  (A  ---~ XoBlX~B2x 2  . . .   BmXm,  <Y,,  Y2,  . ..  ,  Ym)), 
we  have  M(TA,z,  u)  =  (xoTBl, rlxlTB,,v, X2  . . .   TB.,r  x m, i). (2) 

M(a,  av)  =  pop for  all  v in  X *~k- 1). (3)  M($,  e)  =  accept. 

(4)  Otherwise,  M ( X ,   u)  =  error. 
(5)  Ts, te~ is  the  initial  table.  D 

Example 5.16 

Let  us  construct  the  parsing  table  for  the  LL(2)  grammar 

(1)  S  > aAaa 
(2)  S  > bAba 
(3)  A  > b 
(4)  A  > e 

using the  relevant  set of LL(2)  tables  constructed  in  Example  5.15.  The  pars- 

ing  table  resulting  from  Algorithm  5.3  is  shown  in  Fig.  5.5.  In  Fig.  5.5, 

To  =  Ts, t,~,  T1  =  Ta,~aa~, and  T2  =  TA, Cba~- Blank  entries  indicate  error. 

aa  ab  a  ba  bb  b  e 
To 

T1 
T2 

aTlaa,  1 

e,4 

pop 

aTlaa,  1 

pop  pop 

b,3 
e,4 

pop 

b T2 ba, 2 

b,. 3 

pop  pop 

accept 

Fig. 5.5  Parsing table. 
The  2-predictive  parsing  algorithm  would  make  the  following  sequence 
of moves  with  input  bba" 

(bba,  To$ , e)  ~-  (bba,  bTzba$ ,  2) 

~- (ba, T~ba$, 2) 

F-  (ba,  ba$,  24) 

(a,  aS,  24) 
}-(e,  $, 24).  [-] 

SEC. 5.1  LL(k) GRAMMARS  353 
THEOREM 5.5 

If  G  =  (N,  E, P,  S)  is  an  LL(k)  grammar,  then  the  parsing  table  con- 
structed  in  Algorithm  5.3  is  a  valid  parsing  table  for  G under  a  k-predictive 
parsing  algorithm. 

Proof.  The  proof  is  similar  to  that  of  Theorem  5.4.  If  G  is  LL(k),  then 
no  conflicts  occur  in  the  construction  of  the  relevant  LL(k)  tables  for  G, 

:c since  if A  ----~ fl  and  A  ~  ~ are  in  P  and  S  :=> wAoc,  then 

lm 
(FIRSTkffl)  @~k FIRSTk(tx))  ~  (FIRSTk(?)  @~k FIRSTk(00)  =  ~ .  

In  the  construction  of the  relevant  LL(k) tables for  G, we compute  a  table 
Ta.z only if for some S,  w, and  ~, we have S  ~  wAot,  and L =  FIRST~(a).  That 

lm is,  L  will  be  a  local  follow  set  for  A.  Thus, if u is in  Z *k, then  there  is at  most 

one  production  A  ~  fl  such  that  u  is in  FIRSTk(fl)  OkL. 

Let  us  define  the  homomorphism  h  on  3  u  E  as  foIlows: 

h(a)  =  a  for  all  a  E 
h(T)  =  A  if  T  is  an  LL(k)  table  associated  with  A  and  L  for 

some  L. 

Note  that each  table in  3  must  have  at  least  one  entry  which  is  a  production 
index.  Thus,  A  is  uniquely  determined  by  T. 

We  shall  now  prove  that 

(5.1.2)  S "=~ x0c  if  and  only  if  there  is  some  ~'  in  (3  u  ~)*  such  that 

h(og)  =  ~,  and  (xy,  To$,  e)I--~- (y,  oc'$, n)  for  all  y  such  that 

0~ ~  y.  To  is  the  LL(k)  table  associated  with  S  and  {el. 

If."  From  the  manner  in  which  the  parsing  table  is constructed,  whenever 
a  production  number  i  is  emitted  corresponding  to  the  ith  production 
A ---~ fl, the parsing algorithm replaces a table T such that h(T)  =  A  by a string 
fl'  such  that  h ( f l ' ) =  ft.  The  "if"  portion  of  statement  (5.1.2)can  thus  be 
proved  by  a  straightforward  induction  on  the  number  of  moves  made  by 

the  parsing  algorithm. 

Only  if."  Here  we  shall  show  that 

(5.1.3)  If A ===~ x,  then  the  parsing  algorithm  will  make  the  sequence  of 

moves  (xy,  T,  e)  ~  (y,  e,  n)  for any LL(k)  table  T  associated  with 

A  and  L,  where  L  =  FIRSTk(0 0  for  some  ~  such  that  S  ==~ wAoc, lm 

and  y  is  in  L. 

The  proof  will  proceed  by  induction  on  In I.  If  A  ~  a la2  . . .   an,  then 

354  ONE-PASS NO  BACKTRACK  PARSING  CHAP*  5 

(ata z  . ..  any ,  T,  e)  ~-  (a t  .. .  any,  a 1  ...   a n,  i) 
since  T( u) =  (A  ---~ al a2. .,  a n,  f~)  for  all  u  in  FIRSTk(ala2  .. .   a,)@k  L. 
Then  (al  . ..   anY,  al  . ..  a n,  i)I.-~-  (y,  e,  i).  Now  suppose  that  statement 

(5.I.3)  is  true  for  all  leftmost  derivations  of  length  up  to  /,  and  suppose 
that  At==~x oBtxiB2x 2  . . .   BmX m  and  Bj='==~yj,  where  [rrj]  <  l.  Then 

(XoY~Xl  "''   YmX,nY,  T,  e)I -~- (XoYlX~  ""  YmXmY,  xoTax~  "'"  TmX m,  i),  since 
T (u )  :  (A  ~  xoBl X  1  .. .   BmX m,  <Yt  "'"  Ym>)  for  all  u  included  in 

FI RS Tk ( x oB ax a  .. "  B,nXm)  e e   L.  Each  Tj is the  LL(k)  table  associated  with 
Bj  and  Y~,  1  < j   <  m,  so  that  the  inductive  hypothesis  holds  for  each 

sequence  of moves  of the  form 

(yjxy  ...   YmXmY,  Tj,  e)[.-~-- (x i  . ..   YmXmY,  e,  ztj) 
Putting  in  the  popping  moves  for  the  x/ s  we  obtain 

(xoY~X~y2x2  "'"  YmxmY,  T,  e) 
~-  (XoYlXly2xz  ""  YmXmY,  x o Tl x lT zx z   "'"  TmXm,  i) 
~-- ( y lx 2 y 2x 2  .. .  y.,xmY ,  Tl x tT 2x z   .. .   Tmxm,  i) 
~--  ( xl yzxz  '''  YmXmY,  xI Tzxz  "'"  TmXm,  iffl) 

(YzX2 "'"  ymXmY,  T2x2  "'"  TmXm,  iffl) 
, 

[--  (y,  e,  ilrlrt2  .. .  Ztm) 
From  statement  (5.1.3)  we  have,  as  a  special  case,  that  if  S~==~ w,  then 
(w,  To $, e)[---- (e,  $, n).  E] 

As  another  example,  let  us  construct  the  parsing  table  for  the  LL(2) 
grammar  G 2 of Example  5.3. 

Example 5.17 

Consider  the  LL(2)  grammar  G2 

(1)  S  > e 
(2)  S  > abA 
(3)  A  ~  Saa 
(4)  A  >b 

Let  us  first  construct  the  relevant  LL(2)  tables  for  G 2.  We  begin  by  con- 
structing  To  =  Ts.t,~" 

see.  5.1 

Table  To 
Production 

e ab  S-----re S  ~  abA 

Sets 

m {e] 

LL(k) GRAMMARS  355 

From  To  we  obtain  T1  =  T,~.c,~" 

b 
aa 

ab 

Table T1 
Production 

A  --,  b A---~  Saa 
A---~  Saa 

Sets 

[aa} {aa} 

From  Ta  we  obtain  T2  =  Ts, ca=~" 

Table T2 
u  Production 

aa  S  ---~ e ab  S  ~  abA 

Sets 

m {aa} 

From  Tz  we  obtain  T3  =  TA.ta=~" 

aa 

ab ba 

Table T3 
Production 

A  ----~ Saa A.--,  Saa 
A  --,  b 

Sets 

{aa} {aa} 

From  these  LL(2)  tables  we  obtain  the  parsing  table  shown  in  Fig.  5,6. The  2-predictive  parsing  algorithm  using  this  parsing  table  would  parse 
the  input  string  abaa  by  the  following  sequence  of moves" 

(abaa,  To $,  e)  R  (abaa,  ab T1 $,  2) 

1-  (baa,  bT1 $,  2) 
R  (aa,  Ti $, 2) 

[--  (aa,  T2aa$,  23) 

i--  (aa,  aa$,  231) 
[-  (a,  aS,  231) 

t-  (e,  $,  231)  [B 

356  ONE-PASS NO BACKTRACK PARSING  CHAP.  5 

To 
Tl 

T2 

T3 

a 

b 

S 

aa  ab 

ab T1,  2 
T2aa,  3  T2aa,  3 

e,  1  ab T3 , 2 
T2aa,  3  T2aa,  3 

pop pop 

a  ba  bb  b  e 
pop 

b, 4 

b,4 

pop  pop  pop 

e,  1 

accept 
Fig. 5.6  Parsing table for G2. 

We  conclude  this  section  by  showing  that  the  k-predictive  parsing 
algorithm  parses  every input  string  in  linear  time. 

THEOREM 5.6 

The  number  of steps  executed  by  a  k-predictive  parsing  algorithm,  using 
the  parsing  table  resulting  from  Algorithm  5.3  for  an  LL(k)  context-free 
grammar  G  =  (N,  E, P,  S), with an input  of length n, is a linear function  of n. 

Proof.  If  G  is  an  LL(k)  grammar,  G  cannot  be  left-recursive.  From 
Lemma  4.1,  the  maximum  number  of  steps  in  a  derivation  of  the  form 

+ A ==> B~ is less  than  some  constant  c.  Thus  the  maximum  number  of  moves 

lm that  can  be  made  by  a  k-predictive  parsing  algorithm  t~ before  a  pop  move, 

which  consumes  another  input  symbol,  is  bounded  above  by  c.  Therefore, 

a  can  execute  at  most  O(n)  moves  in  processing  an  input  of  length  n.  D 

5.1.6.  Testing  for  the  LL(k)  Condition 

Given  a  grammar  G,  there  are  several  questions  we  might  naturally  ask 
about  G.  First,  one  might  ask  whether  G  is  LL(k)  for  a  given  value  of  k. 

Second,  is  G  an  LL  grammar?  That  is,  does  there  exist  some  value  of  k 
such  that  G is  LL(k)?  Finally,  since  the  left  parsers  for  LL(1)  grammars  are 
particularly  straightforward  to  construct,  we  might  ask,  if  G  is  not  LL(1), 
whether  there  is  an  LL(1)  grammar  G'  such  that  L(G')= L(G). 

Unfortunately  we  can  provide  an  algorithm  to  answer  only  the  first 
question.  It can be shown that the second and third  questions are undecidable. 

In  this  section  we  shall  provide  a  test  to  determine  whether  a  grammar  is 
LL(k)  for  a  specified  value  of  k.  If  k  =  1,  we  can  use  Theorem  5.3.  For 
arbitrary  k,  we  can  use  Theorem  5.2.  Here  we  shall  give  the  general  case. 

It  is  essentially  just  showing  that  Algorithm  5.3  succeeds  in  producing 
a  parsing  table  only  if  G is  LL(k). 

SEC. 5.1  LL(k)  GRAMMARS  357 

Recall  that  G =  (N, Z, P,  S)  is  not  LL(k)  if  and  only  if for  some  ~  in 
(N  u  Z)*  the  following  conditions  hold" 

(1)  S  ==~ wAoc, 

tm (2)  L  =  FIRSTk(a),  and 

(3)  (FIRSTk(fl)  Ok L)  A  (FIRSTk(?) ~  L)  c  ~ , 
for  some  fl  ~  ~, such  that  A ---~ fl and  .,4 --~ 7 are productions  in P. 

ALGORITHM 5.4 

Test  for  LL(k)-ness. 
lnput.  A  CFG  G  :  (N, Z, P,  S) and  an integer  k. 

Output.  "Yes"  if G is  LL(k).  "No,"  otherwise. 
Method. 
(1)  For  a  nonterminal  A  in  N  such  that  A  has  two  or  more  alternates, 
compute  a(A)  =  {L  ~  Z'k IS ==~ wan  and  L  =  FIRSTk(~)}.  (We  shall  pro- 

lm vide an  algorithm  to  do  this  subsequently.) 

(2)  If A ~  fl  and  A  --~ ?  are  distinct  A-productions,  compute,  for  each 
L  in a(A), f(L)  =  (FIRSTk(fl) @k L)  ~  (FIRSTk(?) @k L).  Iff(L)  ~  ~, then halt  and  return  "No."  If f ( L ) =   ~  for  all L  in  a(A),  repeat  step  (2)  for  all 

distinct  pairs  of A-productions. 

(3)  Repeat  steps  (1) and  (2) for all nonterminals  in  N. 
(4)  Return  "yes"  if  no  violation  of  the  LL(k)  condition  is  found. 

To  implement  Algorithm  5.4, we  must  be  able  to  compute  FIRST~(fl) 
for  any fl in  (N  u  Z)*  and  CFG  G  =  (N, Z, P,  S).  Second,  we must  be able 

to  find  the  sets  in  a(A)  ----- {L ~  E*k [there  exists  ~  such  that  S  =~ wAs,  and 

lm L - -   FIRSTk(~)}.  We  shall  now  provide  algorithms  to  compute  both  these 

items. 
AL6ORIrIaM 5.5 

Computation  of FIRSTk(fl). 
Input.  A  CFG  G  =  (N,  Z, P,  S)  and  a  string  f l = X i X z . . . X ,   in (N  W Z)*. 

Output.  FIRST~(fl). 
Method.  We  compute  FIRSTk(X~)  for  1 <  i ~  n  and  observe  that  by Lemma  5.1 

FIRSTk(fl)  --  FIRSTk(X1) O1, FIRSTk(X2) @)I, " "   Ok  FIRSTk(X,) 
It  will  thus  suffice to  show  how  to  find  FIRSTk(X )  when  X  is  in  N;  if X is 
in  Z  L) {e}, then  obviously  FIRSTk(X)  =  (X}. 

358  ONE-PASS NO BACKTRACK PARSING  CHAP.  5 

We  define  sets  Ft(X)  for  all  X  in  N  U  l~  and  for  increasing  values  of  i, 
i  >  0,  as  follows" 

(1)  F~(a) =  [a}  for  all  a  in  E  and  i  >  0. 
(2)  Fo(A)  =  {~x E  ~ *klA  ~  X0~ is  in  P,  where  either  Ix[  =  k  or  Ix[  <  k 
and  ~  =  e]. (3)  Suppose  that  F 0, F x , . . . ,   Ft-1  have  been  defined  for  all  A  in  N. 

Then 

F~(A) =  {xlA  ---~ Y ~. . .   Y,  is  in  P  and  x  is  in 

F,_ ,(g~)  ~ k   F,_ x(gz)  ~ k   "'"  ~)k F,_ ~(Y.)}  W F,_ i(A). 

(4)  As  F~_ ~(A) ~  Ft(A)  ~  E,k  for  all  A  and  i,  eventuaIly  we  must  reach 
an  i  for  which  Ft_i(A)  =  Ft(A)  for  all  A  in  N.  Let  FIRSTk(A )  =  Ft(A)  for 
that  value  of i.  E] 

Example  5.18 

Let  us  construct  the  sets  F~(X),  assuming  that  k  has  the  value  l,  for 
the  grammar  G  with  productions 

S  > BA 
A  >-q-BAle 

B  >  DC 
C  > ,   DCle 
D  > (S)la 
Initially, 

F0(S)  =  F0(B)  = 
Fo(A)  =  [-t-,  e} 
Fo(C)  =  [*,  e} 
Fo(D )  =  [(,  a} 

Then  F,(B)={(,  a}  and  Fi(X)  =  Fo(X)  for  all  other  X.  Then  F z ( S ) =   [(,  a} 
and  F 2 ( X ) =   El(X)  for  all  other  X.  F3(X)  =  F2(X)  for  all  X,  so  that 

FIRST(S)  =  FIRST(B)  =  FIRST(D)  =  [(,  a} 
FIRST(A)  =  [ + ,   e} 
FIRST(C)  =  {,,  e}  D 

THEOREM 5.7 

Algorithm  5.5  correctly  computes  FIRSTk(A ). 
Proof.  We  observe  that  if  for  all  X  in  N  U  ~,  F i_i(X) =  Ft(X),  then 

SEC. 5.1  LL(k) GRAMMARS  359 
Ft(X)  =  Fj(X)  for  all  j  >  i  and  all  X.  Thus  we  must  prove  that  x  is  in 

FIRSTk(A )  if and  only  if x  is  in  Fj(A)  for  some j. 

lf:  We  show  that  F / A ) ~   FIRSTk(A)  by  induction  on  j.  The  basis, 
j  =  0,  is  trivial.  Let  us  consider  a  fixed  value  of  j  and  assume  that  the 

hypothesis  is  true  for  smaller  values  ofj. 

If  x  is  in  Fj(A),  then  either  it  is  in  Fj_ i(A),  in  which  case  the  result  is 
immediate,  or  we  can  find  A  ~  Y1  . . .   Yn in  P,  with  xp  in  F~_ l(Yp),  where 

1 ~  p  ~  n,  such  that  x  =  FIRSTk(x 1 . . .   xn).  By  the  inductive  hypothesis, 

xp  is  in  FIRSTk(Yp).  Thus  there  exists,  for  each  p,  a  derivation  Y~ ==~ yp, 
where  xp  is  FIRSTk(yp).  Hence,  A  =-~ y l . . . y ~ .   We  must  now  show  that 
x  =  FIRSTk(y I  . . .   y~),  and  thus  conclude  that  x  is  in  FIRSTk(A ). 

Case  1:  tx~  . . .   xn[  <  k.  Then  yp  =  xp  for  each  p,  and  x  =  y t . . . y ~ .  
Since Yl  "'"  Yn is in  FIRSTk(A )  in  this  case,  x  is in  FIRSTk(A ). 

Case  2."  For  some  s  >  0,  Ix1  " "   x,I  <  k  but  Ix1  "..  x~+11>_  k.  Then 
yp  =  xp  for  1 ~  p  ~  s,  and  x  is  the  first  k  symbols  of  x~  . . .   x,+~.  Since 

x,+~  is  a  prefix  of Ys+l,  x  is  a  prefix  of  y1  . . .   y,+l  and  hence  of  y~  . . .   y~. Thus,  x  is  FIRSTk(A ). 

/. Only  if:  Let  x  be  in  FIRSTk(A).  Then  for  some  r,  A  ==~ y  and  x  = 

FIRSTk(y ).  We  show  by  induction  on  r  that  x  is  in F~(A).  The  basis,  r  =  1, 
is trivial,  since x  is in Fo(A ).  (In fact, the hypothesis  could  have been tightened 

somewhat,  but  there  is  no  point  in  doing  so.) 

Fix  r,  and  assume  that  the  hypothesis  is  true  for  smaller  r.  Then 

r-1 A ~ Y 1   "'"  Y n ~ Y ,  

rla where  y  =  y l  . . .   y~ and  Yp ==~ yp  for  1 <  p  ~  n.  Evidently,  rp  <  r.  By  the 
inductive  hypothesis,  xp  is  in  F~_I(Yp) ,  where  xp  =  FIRSTk(yp).  Thus, 

FIRSTk(x ~ . . .   xp),  which  is  x,  is  in  F~(A).  [Z 

In  the  next  algorithm  we  shall  see  a  method  of  computing,  for  a  given 
grammar  G  =  (N,  X, P,  S),  those  sets  L  ~  X *k  such  that  S  ==~ wAo~  and lm 

FIRSTk(e )  =  L  for  some  w,  A,  and  e. 

ALGORITHM  5.6 

Computation  of tr(A). 
Input.  A  C F G   G  =  (N,  X, P,  S). 

Output. 

a(A)  =  {L1L  ~  X*e  such  that  S  =-~ wAo~ and Im 

FIRSTk(~ )  =  L  for  some  w and  ~}. 

360  ONE-PASS NO  BACKTRACK  PARSING  CHAP.  5 

Method.  We  shall  compute,  for  all  A  and  B  in  N,  sets  a(A,  B)  such  that $ 
a ( A , B ) = { L [ L c E *   k,  and  for  some  x  and  a,  A = ~ x B a   and  L =  

lm FIRST(e)}.  We construct  sets  tri(A,  B) for  each  A  and  B  and  for  i  =  0,  1 . . . .  

as  follows" 

(1)  Let  ao (A , B )=   {L  ~  Z*k[A---~flBa  is  in  P  and  L =   FIRST(a)}. 

(2)  Assume  that  tr~_~(A, B)  has  been  computed  for  all  A  and  B.  Define at(A,  B) 

as  follows" 
(a)  If L  is  in  at_ i(A,  B),  place  L  in  tri(A,  B). 
(b)  If  there  is  a  production  A  ---~ X1  . "   X,  in  P,  place  L  in  at(A,  B) 

if for  some j,  1 _~j  <  n,  there  is  a  set  L'  in  trt_~(Xj, B)  and  L  = 
L'  ~ k   FIRST(Xj+i)  @~k  "'"  ~ k   FIRST(X,). 
(3)  When  for  some  i, an(A, B) =  a~_ x(A, B)  for  all  A  and  B,  let  a(A, B) = at(A, B). 

Since  for  all  i,  a~_~(A, B ) ~   at(A, B ) ~   (P(Z*k),  such  an  i  must 
exist. 

(4)  The  desired  set  is  a(S,  A).  [~] 

THEOREM 5.8 

In  Algorithm  5.6,  L  is  in  a(S,  A)  if  and  only  if  for  some  w  ~  Z*  and 
a  ~  (N  u  Z)*,  S  ==~ wAa  and  L  =  FIRSTk(a ). lm 

Proof  The  proof  is  similar  to  that  of  the  previous  theorem  and  is  left 
for  the  Exercises. 

Example 5.19 

Let  us  test  the  grammar  G  with  productions 

S  > AS[e 
A  ~ aAlb 
for  the  LL(1)  condition. 

We  begin  by  computing  FIRSTi(S )  =  [e,  a,  b]  and  FIRSTi(A)  =  {a, b}. 

We  then  must  compute  a(S)  =  a(S,  S)  and  a(A)  =  a(S,  A).  From  step 
(1)  of Algorithm  5.4  we  have 

ao(S,  S)  =  {{e}] 
ao(A,  S)  = 

ao(S,  A)  --  {{e, a,  b}} 
ao(A,  A)  =  {{e}} 

Fro m  step  (2)  we find  no  additions  to  these  sets.  For  example,  since  S  ~  AS 
is  a  production,  and  ao(A, A)  contains  {e}  we  must  add  to  a l(S,  A)  the  set 
L  =  {e} t~)l FIRST(S)  =  {e, a,  b]  by  step  (2b).  But  a~(S,  A)  already  contains 

{e, a,  b},  because  ao(S,  A)  contains  this  set. 

Thus,  a ( A ) =   {(e,  a,  b})  and  a ( S ) =   [[el}.  To  check  that  G  is  LL(1),  we 

EXERCISES  361 
have  to  verify  that  ( F I R S T ( A S ) ~ ,   (e})  ~  (FIRST(e)@),  ( e ) ) =   ~ .   [This  is 
for  the  two  S-productions  and  the  lone  member of a(S,  S).]  Since 

F I R S T ( A S ) - -   F I R S T ( A ) ~ ,   F I R S T ( S ) - -   (a,  b} 
and  F I R S T ( e ) =   [e],  we  indeed  verify  that  [a,  b}  5~  [ e ) =   ~ .  

For  the  two  A-productions,  we  must  show  that 

(FIRST(aA)  ~  [e,  a,  b))  ~  (FIRST(b)  @>~ [e,  a,  b))  --  ~ .  
This  relation  reduces  to  (a)  5~ {b]  --  ~ ,   which  is true.  Thus,  G is  EL(l).  [-7 

EXERCI SES 
5.1.1.  Show  that  if G  is  left-recursive,  then  G  is  not  an  LL  grammar. 
5.1.2.  Show  that  if  G  has  two  productions  A - ,   aOc]afl,  where  ~ ~  fl,  then 

G cannot  be  LL(1). 

5.1.3.  Show  that  every  LL  grammar is  unambiguous. 
5.1.4.  Show that every grammar obeying statement (5.1.1) on page 343 is LL(k). 

5.1.5.  Show  that  the  grammar with  productions 

S  ~  aAaBl bAbB 
A -   ~ alab 

B -   ~ a B l a  

is  LL(3)  but  not  LL(2). 
5.1.6.  Construct  the  LL(3)  tables  for  the  grammar  in  Exercise  5.1.5. 
5.1.7.  Construct  a  deterministic left  parser  for  the  grammar  in  Example  5.17. 

5.1.8.  Give  an  algorithm  to  compute  FOLLOW~(A)  for  nonterminal  A. 
"5.1.9.  Show  that  every  regular  set  has  an  LL(I)  grammar. 
5.1.10.  Show  that  G  =  (N,  Z,P,   S)  is  an  LL(1)  grammar  if  and  only  if  for 

each  set  of  A-productions  A  ~  0~ 1 1(~2[''"  ](~n  the  following  condi- 
tions  hold" 

(1)  FIRST?(0ct)  ~  FIRST~(~j)  =  ~5  for  i ~   j. 

(2)  If  0c; ==~ e,  FIRST~(0Cj)  ~  FOLLOW~(A)  --  ~  for  1 __~ j  -<  n, 
i :~ j.  Note  that  at  most  one  ct~ can  derive  e. 

*'5.1.11.  Show  that  it  is  undecidable  whether  there  exists  an  integer k  such  that 

a  CFG  is  LL(k).  [In  contrast,  if we  are  given  a  fixed  value  for  k,  we 

can  determine  if G is  LL(k)  for that  particular  value  of k.] 

"5.1.12.  Show  that  it  is  undecidable  whether  a  CFG  generates an  LL  language. 

"5.1.13.  The  definition  of  an  LL(k)  grammar  is  often  stated  in  the  following 

manner.  Let  G  - ( N ,   E, P,  S)  be  a  CFG.  If  S  =~  wax,  for  w  and  x 

362  ONE-PASS NO BACKTRACK  PARSING  CHAP. 5 

in ~*  and A  ~  N,  then for each y in Z *k there is at most one production 
A  ---~ a  such  that  y  is  in  FIRSTk(ax).  Show  that  this  definition  is equi- 
valent  to  the  one  given  in  Section  5.1.1. 

5.1.14.  Complete  the  proof of Theorem  5.4. 
5.1.15.  Prove  Lemma  5.1. 
5.1.16.  Prove  Theorem  5.8. 
*'5.1.17.  Show  that  if L  is  an  LL(k)  language,  then  L  has  an  LL(k)  grammar 

in  Chomsky  normal  form. 

5.1.18.  Show that  an  LL(0)  language  has  at  most  one member. 
"5.1.19.  Show  that  the  grammar  G  with  productions  S  ~  aaSbbla[e  is  LL(2). 

Find  an  equivalent  LL(1)  grammar  for  L(G). 

*'5.1.20.  Show  that  the  language  [a"0b"l n  >  0} LJ {a"lb2"l n >  0}  is  not  an  LL 

language. 

Exercises  5.1.21-5.1.24  are  done  in  Chapter  8.  The  reader  may 
wish  to  try  his  hand  at  them  now. 

*'5.1.21.  Show that  it is  decidable  for  two  LL(k)  grammars,  G1  and  G2, whether 

L(G,)  =  L(Gz). 
*'5.1.22.  Show  that  for  all  k  >  0,  there  exist  languages  which  are  LL(k  +  1) 

but  not  LL(k). 

*'5.1.23.  Show  that  every  LL(k)  language  has  an  LL(k  +  1)  grammar  with  no 

e-productions. 

*'5.1.24.  Show  that  every  LL(k)  language  has  an  LL(k  +  1)  grammar  in  Grei- 

bach  normal  form. 

"5.1.25.  Suppose  that  A  ~  0eft lay  are  two  productions  in  a  grammar  G  such 

that  a  does  not  derive  e  and  fl  and  ~  begin  with  different  symbols. 

Show  that  G  is  not  LL(1).  Under  what  conditions  will  the replacement 
of these  productions  by 

A  ------~ 0cA' 
h '   --->  ]31~, 

transform  G  into  an  equivalent  LL(1)  grammar 9. 
5.1.26.  Show  that  if  G  =  (N,  Z, P,  S)  is  an  LL(k)  grammar,  then,  for  all 

A  ~  N,  GA is  LL(k),  where  Ga  is  the  grammar  obtained  by  removing 
all useless  productions  and  symbols  from  the  grammar  (N,  Z, P,  A). 

Analogous  to  the  LL  grammars  there  is  a  class  of grammars,  called 
the  LC  grammars,  which  can  be  parsed  in  a  left-corner  manner  with 

a  deterministic  pushdown  transducer  scanning  the  input  from  left  to 
right.  Intuitively,  a  grammar  G  =  (N,  Z,P,  S)  is  LC(k)  if,  knowing , 

the  leftmost  derivation  S  ~  wAS,  we  can  uniquely  determine  that  the 

lm production  to  replace  A  is  A ~  X 1 . - .   X~  once  we  have  seen  the 

EXERCISES  363 
portion  of the  input  derived from  X1  (the  symbol  X1  is the left corned 
and  the  next  k  input  symbols.  In  the  formal  definition,  should  Xx  be 
a  terminal,  we  may  then  look  only  at  the  next  k -   1  symbols.  This 
restriction  is  made  for  the  sake  of  simplicity  in  stating  an  interesting 
theorem  which  will  be  Exercise  5.1.33.  In  Fig.  5.7,  we  would recognize 
production  A ~  X1  ...  Xp  after  seeing  wx  and  the  first  k  symbols 

(k  -  1 if Xa  is in E) of y. Note that if G were LL(k), we could recognize 
the  production  "sooner,"  specifically,  once  we  had  seen  w  and 

FIRSTk(Xy). 

w  x  y 

Fig. 5.7  Left-corner parsing. 

We  shall  make  use  of the  following  type  of derivation in the  defini- 
tion  of an  LC grammar. 

DEFINITION 

Let  G  be  a  CFG.  We  say  that  S=-~  wAS  if  S ~  wAS  and  the lc  lrn 
nonterminal  A  is  not  the  left  corner  of  the  production  which  intro- 

duced  it  into  a  left-sentential  form  of  the  sequence  represented  by 

S  :=-> wAo~. lm 

For  example,  in  Go,  E ~  E  +  T  is  false,  since  the  E  in  E  +  T le 
arises from the left corner  of the production  E  ~  E  +  T.  On  the  other , 
hand,  E ~  a  +  T is  true,  since  T is  not  the  left  corner  of  the  produc- lc 
tion  E  ~  E  +  T,  which  introduced  the  symbol  T  in  the  sequence  . 

E==-> E  +  T:=> a  +  T. 

DEFINITION 

CFG  G----(N,  Z,P,  S)  is  LC(k)  if  the  following  conditions  are , 
satisfied"  Suppose  that  S  ~  wAG.  Then  for  each  lookahead  string  u lc 

there is at  most  one production  B  ~  cx such  that  A  ~  B' F and 

364  ONE-PASS  NO  BACKTRACK PARSING  CHAP.  5 

(1)  (a)  If a  =  Cfl,  C  ~  N,  then  u  ~  FIRSTk(fl},d;),  and 

(b)  In  addition,  if  C  =  A,  then  u  is  not  in  FIRST,(d;); 
(2)  If a  does  not  begin with  a  nonterminal,  then  u  ~  FIRSTk(aT'~). 

Condition  (la)  guarantees  that  the  use  of  the  production  B  ~  Cfl 
can  be  uniquely  determined  once  we  have  seen  w,  the  terminal  string 

derived  from  C  (the  left  corner)  and  FIRSTk(fl)'~)  (the  lookahead 
string). Condition  (lb)  ensures  that  if  the  nonterminal  A  is  left-recursive 

(which is possible  in an  LC  grammar),  then we can tell after  an  instance 

of  A  has  been  found  whether  that  instance  is  the  left  corner  of  the 
production  B----~ AT'  or  the  A  in  the  left-sentential  form  wAa. 

Condition  (2)  states  that  FIRSTk(0~7't~)  uniquely  determines  that  the 
production  B  ---~ 0~ is  to  be used  next  in  a  left-corner  parse  after  having 
seen  wB,  when  ~  does  not  begin  with  a  nonterminal  symbol.  Note 
that  0c might  be  e  here. For  each  LC(k)  grammar  G  we  can  construct  a  deterministic  left 

corner parsing  algorithm  that  parses  an  input  string  recognizing the left 

corner  of  each  production  used  bottom-up,  and  the  remainder  of  the 
production  top-down. 

Here,  we  shall  outline  how  such  a  parser  can  be  constructed  for 
LC(1)  grammars.  Let  G = ( N , ~ , P , S )   be  an  L C( 1 ) g r a m m a r .  
From  G  we  shall  construct  a  left  corner  parser  ~  such  that  ~r(~)  -- {(x, n)lx  ~  L(G)and 

n;  is  a  left-corner  parse  for  x}.  ~  uses  an  input 
tape,  a  pushdown  list  and  an  output  tape  as  does a k-prodictive parser. 

The  set  of  pushdown  symbols  is  I"  =  N  U  ~  w  (N  x  N)  u  {$}. 
Initially,  the  pushdown  list  contains  S$  (with  S  on  top).  A  single 

nonterminai  or  terminal  symbol  appearing  on  top  of  the  pushdown  list 
can  be  interpreted  as  the  current  goal  to  be  recognized.  When  a  push- 

down  symbol is  a  pair  of nonterminals  of the  form  [A, B],  we  can  think 

of  the  first  component  A  as  the  current  goal  to  be  recognized  and 
the  second  component  B  as  a  left  comer  which  has  just  been  recog- 
nized. For  convenience,  we  shall  construct  a  left-corner  parsing  table  T 

which  is  a  mapping  from  F  *  (~  u  {e}) to  (F*  *  (P  u  {e}) u  {pop, 
accept,  error}.  This parsing table is similar to  a  1-predictive parsing table 

for  an  LL(1)  grammar.  A  configuration of M  will be a triple  (w,  X0~, zt), 

where  w  represents  the  remaining  input,  X0c  represents  the  pushdown 
list  with  X  e  F  on  top,  and  7t is the  output  to  this  point.  If  T(X,  a)  = 

(fl,  i),  X  in  N  u  (N  x  N),  then  we  write  (aw,  Xoc, rt)  ~  (aw,  floe, rti). If  T(a, 

a)  =  pop,  then  we write  (aw, aoc, 70  ~  (w, oc, 70.  We  say  that  7t 
is  a  (left-corner)  parse  of  x  if  (x,  S$,  e)I ~  (e,  $, 70. 

Let  G  =  (N,  E, P,  S)  be  an  LC(1)  grammar.  T  is  constructed  from 

G  as  follows: 

(1)  Suppose  that  B  ----~ tz is  the  ith  production  in P. 

(a)  If  0c =  Cfl,  where  C  is  a  nonterminal,  then  T([A,  C], a) =  (iliA,  B], i) 

for  all  A  ~  N  and  a  ~  FIRST i(fl)'~)  such 

EXERCISES  365 
that  S  ~  wAO  and  A ~  B~,.  Here,  12  recognizes  left lc 

corners  bottom-up.  Note  that  A  is  either  S  or  not  the  left 
corner  of  some  production,  so  at  some  point  in  the  parsing 

A will be  a  goal. 
(b)  If  0~  does  not  begin  with  a  nonterminal,  then  T(A,  a) 

=  (tz[A, B], i)  for  all  A  ~  N  and  a  ~  FIRSTi(~}'~)  such 

that  S  ~  wAO  and  A =-> BT. lc 
(2)  T([A, A], a ) =   (e, e)  for  all  A  ~  N  and  a  ~  FIRSTi(O)  such 
that  S  ~  wA~. le 

(3)  T(a,  a)  =  pop for  all a  E  E. 
(4)  T($,  e)  =  aecept. (5) 

T(X,  a)  =  error  otherwise. 

Example 5.20 

Consider  the following grammar  G with productions 

(1)  S--*  S  +  A  (2)  S--~  A (3)  A  --> A  .  B  (4)  A-->  B 

(5)  B--->  ( S )   (6)  B ~  a 
G  is  an  LC(1)  grammar.  G  is,  in  fact,  Go  slightly  disguised.  A  left- corner  parsing  table  for  G  is  shown  in  Fig.  5.8. 

The  parser  using  this  left-corner  parsing  table  would  make  the following  sequence  of moves  on  input  ~ a ,   a): 

((a  *  a),  S$, e)  ~--- ((a  *  a),  (S)[S,  B]$, 5) 

(a *  a),  ST[S,  B]$,  5) 
F-  (a *  a),  a[S,  B])[S,  B]$,  56) 
l-- (* a),  [S, B])[S,  B]$,  56) 

[-  (,a),  IS, A])[S,  B]$,  564) 
}-  (,a),  *  B[S,  A])[S,  B]$,  5643) 
t-  (a),  B[S, A])[S, B]$,  5643) 

(a), a[B, B][S, hi)IS, B]$, 56436) 
~-- (),  [B, B][S,  A])[S,  B]$,  56436) 

(),  [S, A])[S,  B]$,  56436) 
(),  IS, S])[S,  B]$,  564362) 
1-- (),)[S,  B]$,  564362) 

1-- (e, [S, B]$,  564362) 

[--- (e, [S, A]$,  5643624) 

(e, IS, S]$,  56436242) 
t-  (e,  $, 56436242) 

366  ONE-PASS  NO  BACKTRACK  PARSING  CHAP.  5 
Pushdown symbol 

S 
A 
B 
[S,S] 
Is, A] 

[S, BI 
[A,AI 

IA,B] 

[B, BI 

a 
< 

> 

+[ 

sl 

Input symbol 
a  <  > 

a[S,B],  6  < S>[ S,  B],  5 
p  . . . . . . . . . . . . . . .  

a[A,B] , 6  <S>[A,B]  , 5 
L . . . . . . . . . . .  a[B,B],  6  <S>[B,B],  5 

pop 

e,e 
[S, S],  2 

*  ,, 

[S,A],4 

e,e 
[A,A],4 

,  ,, 

pop 

e,e 

pop 

+ 
+A[S, SI,  l 

[S, SI, 2 

,, 

[S,A],4 

e,e 
[A,A],4 
., 

e,e 
p  . . . .  

pop 

i'i 
,  i~ 

,  ,,  ~h 

*B[S, A I, 3 

[S,A],4 
.B [A,A I,3 

.  . I h , a ] , 4  

.. 

e,e 
[S, S], 2 
[S,A],4 

e,e 
[A,AI,4 
e,e 

pop 

e,e 

accept 
Fig. 5.8  Left-corner parsing table for G. 

The reader  can  easily  verify that  56436242  is  the  correct  left  corner 
parse  for  (a  *  a). 

"5.1.27.  Show that  the  grammar with productions 

S----+  A[B 
A  ~  aAblO 

B -   > aBbb l 1 

is not  LC(k)  for  any  k. 
"5.1.28.  Show that  the following  grammar  is  LC(1)" 

E----->.E  +  T[T 

T--->"  T .   F[F 
F----> PI" FIP 
P----~"  (E) l a 

5.1.29.  Construct  a  left-corner  parser  for  the  grammar  in  Exercise  5.1.28. 

EXERCISES  367 
*'5.1.30.  Provide  an  algorithm  which  will  test whether an  arbitrary  grammar  is 

LC(1). 

*'5.1.31.  Show  that  every LL(k)  grammar is  LC  (k). 

5.1.32.  Give  an  example  of  an  LC(1)  grammar  which  is  not  LL. 
*'5.1.33.  Show  that  ff Algorithm  2.14 is applied to  a  grammar G to put  it in  Grei- 

bach  normal  form,  then  the  resulting  grammar  is  LL(k)  if  and  only  if 

G is LC(k).  Hence  the  class  of LC  languages  is  identical  to  the  class  of 
LL  languages. 

"5.1.34.  Provide  an  algorithm  to  construct  a  left-corner  parser  for  an  arbitrary 

LC(k)  grammar. 

Research Problems 

5.1.35.  Find transformations which can be used to convert non-LL(k) grammars 

into  equivalent  LL(1) grammars. 

Programming  Exercises 

5.1.36.  Write  a  program  that  takes  as  input  an  arbitrary  CFG  G  and  con- 

structs  a  1-predictive  parsing  table  for  G  if G  is LL(i). 

5.1.37.  Write a  program  that  takes  as  input  a  parsing table and  an  input  string 

and  parses  the  input  string  using  the  given parsing  table. 

5.1.38.  Transform one of the grammars in the Appendix into an LL(1) grammar. 

Then  construct  an  LL(1)  parser for  that  grammar. 

5.1.39.  Write  a  program  that  tests whether  a  grammar  is  LL(1). 

Let M  be a  parsing  table for  an  LL(1)  grammar  G.  Suppose that  we 
are parsing  an input  string  and  the parser has reached the configuration (ax,  Xoc, ~z). 

If  M(X,  a ) =   error,  we  would  like  to  announce  that  an 
error  occurred  at  this  input  position  and  transfer  to  an  error  recovery 
routine  which  modifies  the  contents  of  the  pushdown  list  and  input 
tape so that parsing can proceed normally.  Some possible error recovery 

strategies are 

(1)  Delete a  and  try to  continue parsing. 
(2)  Replace  a  by  a  symbol  b  such  that  M(X,  b ) ~   error  and  con- 
tinue parsing. 

(3)  Insert  a  symbol  b  in  front  of a  on  the  input  such  that  M(X,  b) 
error and  continue parsing.  This  third  technique  should  be used with 
care  since  an  infinite loop is easily possible. 

(4)  Scan  forward  on  the  input  until  some  designated  input  symbol 
b  is  found.  Pop  symbols  from  the  pushdown  list  until  a  symbol  X  is 

found  such  that  X : ~   bfl  for  some  ft.  Then  resume  normal  parsing. 

We  also  might  list  for  each  pair  (X, a)  such  that  M(X, a)  =  error, 
several  possible  error  recovery  methods  with  the  most  promising 
method  listed  first.  It  is  entirely  possible  that  in  some  situations  inser- 

368  ONE-PASS NO  BACKTRACK PARSING  CHAP.  5 

tion  of  a  symbol  may  be  the  most  reasonable  course  of  action,  while 

in  other  cases deletion  or  change would be most likely to  succeed. 

5.1.40.  Devise  an  error  recovery  algorithm  for  the  LL(1)  parser  constructed in Exercise 5.1.38. 

B I B L I O G R A P H I C   NO TE S  
LL(k)  grammars  were  first  defined  by  Lewis  and  Steams  [1968]. In  an  early version  of that  paper,  these grammars were called TD(k)  grammars,  TD  being  an 
acronym  for  top-down.  Simple LL(1)  grammars were first  investigated by  Koren- 
jak  and  Hopcroft  [1966], where they were called s-grammars. The theory  of LL(k)  grammars was extensively developed by Rosenkrantz  and 

Steams  [1970], and  the  answers  to  Exercises  5.1.21-5.1.24  can  be  found  there. LL(k)  grammars  and  other  versions  of  deterministic  top-down  grammars  have 
been  considered  by  Knuth  [1967], Kurki-Suonio  [1969], Wood  [1969a, 1970] and 

Culik [1968]. P.M.  Lewis,  R.E.  Stearns,  and  D.J.  Rosenkrantz  have  designed  compilers 

for  ALGOL  and  FORTRAN  whose  syntax  analysis  phase  is  based  on  an  LL(1) parser.  Details  of  the  ALGOL  compiler  are  given  by  Lewis  and  Rosenkrantz 

[1971]. This reference also contains an LL(1) grammar for ALGOL 60. LC(k)  grammars  were  first  defined  by  Rosenkrantz  and  Lewis  [1970]. Clues 

to  Exercises  5.1.27-5.1.34 can be found  there. 

5.2.  DETERMINISTIC  BOTTOM-UP  PARSING 

In the previous  section, we saw a class of grammars which could be parsed 
top-down  deterministically, while  scanning the input  from left to right.  There 
is  an  analogous  class  of languages  that  can  be  parsed  deterministically  bot- 
tom-up,  using  a  left-to-right  input  scan.  These  are  called  LR  grammars,  and 

their  development  closely  parallels  the  development  of  the  LL  grammars  in 
the  preceding  section. 

5.2.1.  Deterministic  Shift-Reduce  Parsing 

In  Chapter  4 we indicated  that  bottom-up  parsing  can proceed  in  a  shift- 
reduce  fashion  employing  two  pushdown  lists.  Shift-reduce  parsing  consists 

of  shifting  input  symbols  onto  a  pushdown  list  until  a  handle  appears  on 
top  of the  pushdown  list.  The  handle  is then  reduced.  If no  errors  occur,  this 
process is repeated until all of the input string is scanned and only the sentence 

symbol appears  on the  pushdown  list.  In  Chapter  4  we provided  a  backtrack 
algorithm  that  worked  in  essentially  this  fashion,  normally  making  some 
initially incorrect choices for  some handles,  but ultimately making the correct 

choices.  In  this  section  we  shall consider a  large class  of grammars for  which 

sEc. 5.2  DETERMINISTIC  BOTTOM-UP PARSING  369 
this  type  of  parsing  can  always  be  done  in  a  deterministic  manner.  These 

are the  LR(k) grammars  the largest class of grammars which can  be  "natu- 

rally"  parsed  bottom-up  using  a  deterministic  pushdown  transducer.  The  L 

stands  for  left-to-right  scanning  of  the  input,  the  R  for  producing  a  right 

parse,  and  k  for  the  number  of input  "lookahead"  symbols. 

We  shall  later  consider  various  subclasses  of LR(k)  grammars,  including 
precedence  grammars  and  bounded-right-context  grammars. 

Let  ~x  be  a  right-sentential  form  in  some  grammar,  and  suppose  that 
is either  the  empty  string  or  ends  with  a  nonterminal  symbol.  Then  we  shall 
call  ~  the  open  portion  of  0~x and  x  the  closed portion  of  0cx. The  boundary 

between  ~  and  x  is  called  the  border.  These  definitions  of  open  and  closed 
portion  of a  right-sentential  form  should  not  be  confused  with  the  previous 

definitions of open  and  closed portion,  which were for a  left-sentential form. 

A  "shift-reduce"  parsing  algorithm  can  be  considered  a  program  for 
an  extended  deterministic  pushdown  transducer  which  parses  bottom-up. 

Given  an  input  string  w,  the  DPDT  simulates  a  rightmost  derivation  in 
reverse.  Suppose  that 

S  ~-  oC o  ------~  o~1------- ~  . . .   ~.  O~m  =  W 

rm  rm  rm 

is  a  rightmost  derivation  of  w.  Each  right-sentential  form  at  is  stored  by 

the  DPDT  with the  open  portion  of ~t on  the  pushdown  list  and  the  closed 

portion  as  the  unexpended  input.  For  example,  if 0~ --  o~Ax,  then  0~A would 

be on the pushdown list (with A on top) and x would be the as yet unseanned 
portion  of the  original  input  string. 

Suppose  that  0ct_~--?Bz  and  that  the  production  B - - .   fly  is  used  in 
the  step  ~t-1  ~  ~,  where  yfl  =  o~A  and yz  =  x.  With  0~A on  the  pushdown 

rm list,  the  PDT  will shift  some number  (possibly  none)  of the  leading  symbols 

of x  onto  the  pushdown  list until  the  right  end  of the  handle  of o~t is  found. 
In  this  case,  the  string y  is  shifted  onto  the  pushdown  list. 

Then  the  PDT  must  locate  the  left end  of the  handle.  Once  this has  been 
done,  the PDT will replace the handle  (here fly),  which is on top of the push- 
down  list,  by the  appropriate  nonterminal  (here  B)  and  emit the  number  of 
the  production  B----~ fly.  The  PDT  now  has  yB  on  its  pushdown  list,  and 

the  unexpended  input  is  z.  These  strings  are  the  open  and  closed  portions, 
respectively,  of the  right-sentential  form  ~_ i. 

Note  that  the  handle  of ocAx  can  never  lie  entirely  within  ~,  although  it 
could  be  wholly within  x.  That  is,  ~t-1  could  be  of the  form  o~AxlBx2,  and 

a production of the form B ---~ y, where x l y x 2   =  x  could be applied to obtain 
0~. Since Xl  could be arbitrarily long, many shifts may occur before ~t can  be 
reduced  to  ~_ 1. To  sum  up,  there  are  three  decisions  which  a  shift-reduce  parsing 

370  ONE-PASS  NO  BACKTRACK PARSING  CHAP. 5 

algorithm  must make.  The  first is to  determine  before  each move whether to 

shift  an input  symbol onto  the  pushdown  list or to  call for  a  reduction.  This 
decision is really the determination  of where the right end of a handle  occurs 
in  a  right-sentential  form. 

The  second  and  third  decisions  occur  after  the  right  end  of a  handle  is 
located.  Once the handle is known to lie on top  of the pushdown  list,  the left 
end  of  the  handle  must  be  located  within  the  pushdown  list.  Then,  when 
the  handle  has been thus  isolated,  we must  find the  appropriate  nonterminal 
by which it is to  be replaced. 

A  grammar  in  which  no  two  distinct  productions  have  the  same  right 
side  is  said  to  be  uniquely  invertible  (UI)  or,  alternatively,  backwards  deter- 
ministic.  It  is not  difficult  to  show that  every context-free  language  is gener- 

ated  by  at  least  one  uniquely  invertible  context-free  grammar. 

If a grammar is uniquely invertible,  then once we have isolated the handle 
of a  right-sentential  form,  there  is  exactly  one  nonterminal  by which  it  can 
be  replaced.  However,  many  useful  grammars  are  not  uniquely  invertible, 

So in  general  we  must  have  some  mechanism  for  knowing  with  which  non- 
terminal  to  replace  a  handle. 

Example 5.21 

Let us  consider  the  grammar  G with  the  productions 

(1)  S  ~  SaSb 
(2)  S  ~e 

Consider  the  rightmost  derivation: 

S  ~  SaSb  ~  SaSaSbb  ~  SaSabb  ~  Saabb  ~  aabb 
Let  us  parse  the  sentence  aabb  using  a  pushdown  list  and  a  shift-reduce 
parsing algorithm.  We  shall  use  $ as an  endmarker for both  the  input  string 
and  the  bottom  of the  pushdown  list. 

We  shall  describe the  shift-reduce  parsing  algorithm  in terms  of configu- 
rations  consisting  of triples  of the  form  (~X, x, n),  where 

(1)  ~X represents the contents  of the pushdown list, with  X on top; 
(2)  x  is the  unexpended  input;  and 
(3)  n  is the  output  to  this  point. 

We  can  picture  this  configuration  as the  configuration  of an  extended  PDT 
with  the  state  omitted  and  the  pushdown  list  preceeding  the  input.  In 

Section  5.3.1 we  shall  give  a  formal  description  of  a  shift-reduce  parsing 
algorithm. 

Initially,  the  algorithm  will  be  in  configuration  ($, aabb$,  e).  The  algo- 

SEC. 5.2  DETERMINISTIC  BOTTOM-UP  PARSING  371 

rithm  must  then  recognize  that  the  handle  of  the  right-sentential  form  aabb 
is  e,  occurring  at  the  left  end,  and  that  this  handle  is  to  be  reduced  to  S. 
We  defer  describing  the  actual  mechanism  whereby  handle  recognition 

occurs.  Thus  the  algorithm  must  next  enter  the  configuration  ($S,  aabb$,  2). 
It  will  then  shift  an  input  symbol  on  top  of  the  pushdown  list  to  enter 
configuration  ($Sa,  abb$,  2).  Then  it  will  recognize  that  the  handle  e  is 
on  top  of  the  pushdown  list  and  make  a  reduction  to  enter  configuration ( $SaS,  abb $, 

22). 
Continuing  in  this  fashion,  the  algorithm  would  make  the  following 
sequence  of moves: 

($,  aabb $, e)  ~  ($S,  aabb $, 2) 

k-  ( $Sa,  abb $, 2) 
k-  ( $SaS,  abb $, 22) 
F-  ( $SaSa,  bb $, 22) 
k-  ( $SaSaS,  bb $, 222) 
k-  ( $SaSaSb,  b $, 222) 
?-  ( $SaS,  b $, 2221) 
?-  ( $SaSb,  $, 2221) 
?-  ($S,  $, 22211) 

accept  [~ 

5.2.2.  LR(k)  Grammars 

In  this  section we shall  define a  large class  of grammars for which we can 
always  construct  deterministic  right  parsers.  These  grammars  are  the  LR(k) 
grammars. 

Informally, we say that  a grammar is LR(k) if given a rightmost derivation 
S  =  ao - ~   al  =~  a2 ~  .. .  - ~   am =  z,  we  can  isolate  the  handle  of  each rm  rm  rm  rm 

right-sentential  form and  determine  which  nonterminal  is to  replace  the han- 
dle  by  scanning  ai  from  left  to  right,  but  only going at  most  k  symbols  past 
the  right  end  of the  handle  of ai. 

Suppose  that  a,_~ =  aAw  and  a~ =  aflw,  where  fl  is  the  handle  of  a~. 
Suppose  further  that  /3 =  X1X2...  X,..  If  the  grammar  is  LR(k),  then  we 
can  be  sure  of the  following facts: 

(1)  Knowing  aX~X~ ...  Xj  and  the  first  k  symbols  of  Xj+~...  X,w,  we 
can  be  certain  that  the  right  end  of  the  handle  has  not  been  reached  until 
j = r .  

(2)  Knowing aft and at most the first k  symbols of w, we can always deter- 
mine  that  fl  is  the  handle  and  that  fl  is  to  be  reduced  to  A. 

372  ONE-PASS NO BACKTRACK PARSING  CHAP.  5 

(3)  When  ~_ ~ =  S,  we  can  signal  with  certainty  that  the  input  string  is 
to  be  accepted. 

Note  that  in  going  through  the  sequence  ~m, ~ - ~ , ' ' ' ,   ~0,  we  begin  by 
looking  at  only  FIRSTk(~m)=  FIRSTk(w ).  At  each  step  our  lookahead 
string  will  consist  only  o f k   or  fewer  terminal  symbols. 

We  Shall  now  define  the  term  LR(k)  grammar.  But  before  we  do  so,  we 
first  introduce  the  simple  concept  of an  augmented  grammar. 

DEFINITION 

Let  G  =  (N,  Z, P,  S)  be  a  CFG.  We  define  the  augmented  grammar 
derived from  G  as  G'  :  (N  U  {S'}, ~, P  U  {S' ----~ S}, S').  The  augmented 
grammar  G' is merely  G with  a  new  starting  production  S ' - ~   S,  where  S'  is 

a  new  start  symbol,  not  in  N.  We  assume  that  S'  ---~ S  is  the  zeroth  pro- 
duction in G' and that  the other  productions  of G  are  numbered  1, 2 . . . .   , p. 
We  add  the  starting  production  so  that  when  a  reduction  using  the zeroth 
production  is  called  for,  we  can  interpret  this  "reduction"  as  a  signal  to 

accept. 

We  shall  now  give  the  precise  definition  of an  LR(k)  grammar. 
DEFINITION 

Let  G  =  (N,  E, P,  S)  be  a  CFG  and  let  G ' =   (N',  ~, P',  S')  be  its  aug- 
mented  grammar.  We  say  that  G  is  LR(k),  k  ~  0,  if  the  three  conditions 

(1)  S' ~  ocAw ~  ~pw, 

G'  rm  G p  rm 

(2)  S'  ~  ~Bx ~  oq~y, and 

G'  rm  G" rm 

(3)  FIRSTk(w )  =  FIRSTk(Y ) 

imply  that  aAy  =  ~,Bx. (That  is,  ~  =  ~,,  A  =  B,  and  x  =  y.) 

A  grammar  is LR  if it  is  LR(k)  for  some  k. 

Intuitively  this  definition  says  that  if  aflw  and  afly  are  right-sentential 
forms  of  the  augmented  grammar  with  FIRSTk(w )  =  FIRSTk(y )  and  if 
A  ---~ fl  is  the  last  production  used  to  derive  aflw  in  a  rightmost  derivation, 

then  A  ~  fl  must  also  be  used  to  reduce  afly  to  aAy  in  a  right  parse.  Since 
A  can  derive  fl  independently  of  w,  the  LR(k)  condition  says  that  there  is 

sufficient  information  in  FIRSTk(w )  to  determine  that  aft  was  derived  from 
aA.  Thus  there  can  never  be  any  confusion  about  how  to  reduce  any  right- 
sentential  form  of  the  augmented  grammar.  In  addition,  with  an  LR(k) 
grammar  we  will  always  know  whether  we  should  accept  the  present  input 

string  or  continue  parsing.  If the  start  symbol  does  not  appear  on  the  right 
side  of  any  production,  we  can  alternatively  define  an  LR(k)  grammar 

G  =  (N,  E, P,  S)  as  one  in  which  the  three  conditions 

sEc. 5.2  DETERMINISTIC  BOTTOM-UP  PARSING  373 

:g (1)  S  =:~ ~aw  ::~  ~pw, 
rill  I'm 
(2)  S  =:~ ?Bx  ~  ocpy,  and 

rlll  1" rrl. (3)  F IR ST k(w ) =   FIRSTk(y ) 

imply  that  otA y  =  yBx. 

The  reason  we cannot  always use  this definition  is that  if the  start  symbol 
appears  on the right side of some production  we may not be able to determine 
whether  we  have  reached  the  end  of  the  input  string  and  should  accept  or 
whether  we  should  continue  parsing. 

Example 5.22 

Consider  the  grammar  G  with  the  productions 

S  > S a [ a  
If  we  ignore  the  restriction  against  the  start  symbol  appearing  on  the  right 

side  of a  production,  i.e.,  use  the  alternative  definition,  G would  be  an  LR(0) 

grammar. 

However,  using  the  correct  definition,  G  is  not  LR(0),  since  the  three 
conditions 

0 (1)  S'"---~  S"----'-~ S, 

G t  rnl  G t  rrll (2) 
S'  ~  S  ~  Sa,  and 

G" rm  G' rm (3)  FIRST0(e )  =  FIRSTo(a )  =  e 

do  not  imply that  S'a  =  S.  Relating  this  situation  to  the  definition  we  would 
have  ~  =  e,  , 8 =   S,  w = e ,   7, =  e,  A  =  S',  B =   S,  x =   e,  a n d y = a .   The 
problem  here is that in the right-sentential  form  Sa of G' we cannot  determine 

whether  S  is the  handle  of Sa  (i.e.,  whether  to  accept  the  input  derived  from 

S)  looking  zero  symbols  past  the  S.  Intuitively,  G  should  not  be  an  LR(0) 
grammar  and  it  is  not,  if  we  use  the  first  definition.  Throughout  this  book, 

we  shall  use  the  first  definition  of LR(k)-ness.  [~ 

In  this  section  we  show  that  for  each  LR(k)  grammar  G  =  (N,  E, P,  S) 
we can  construct  a  deterministic  right  parser  which  behaves  in  the  following 
manner. 

First  of all,  the  parser  will  be  constructed  from  the  augmented  grammar 
G'.  The  parser  will  behave  very much  like the  shift-reduce  parser  introduced 
in  Example  5.21,  except  that  the  LR(k)  parser  will  put  special  information 

symbols,  called  LR(k)  tables,  on  the  pushdown  list  above  each  grammar 

symbol  on  the  pushdown  list.  These  LR(k)  tables  will  determine  whether 
a  shift  move  or  a  reduce  move  is  to  be  made  and,  in  the  case  of  a  reduce 
move,  which  production  is  to  be  used. 

374  ONE-PASS  NO  BACKTRACK  PARSING  CHAP.  5 

Perhaps  the  best  way  to  describe  the  behavior  of  an  LR(k)  parser  is  via 
a  running  example. 

Let  us  consider  the  g ram m ar  G  of  Example  5.21,  which  we  can  verify  is 
an  LR(1)  grammar.  The  augmented  g ram m ar  G'  is 

(0)  S ' - - +   S 
(1)  S  -  >S aS b 
(2)  S  > e 

An  LR(1)  parser  for  G  is  displayed  in  Fig.  5.9. 

To 

T1 

T2 

T3 

75 

T5 
r6 

T7 

Parsing action  Goto 

b  e  S  a  b 
2  X  2 

S  X  A 

2  2  X 
S  S  X 

2  2  X 

1  X  1 
S  S  X 

1  X 

T1  X  X 

X  T2  X 
T3  X  X 
X  T4  Ts 
r6  X  X 

X  X  X 
X  T4  TT, 

X  X  X 
Legend i  -= reduce using production i 
S  ~  shift A -= accept 
X  ~  error 

Fig. 5.9  LR(t)  parser for  G. 

An  LR(k)  parser  for  a  C F G   G  is  nothing  more  than  a  set  of  rows  in 
a  large  table,  where  each  row  is  called  an  "LR(k)  table."  One  row,  here  To, 
is  distinguished  as  the  initial  LR(k)  table.  Each  LR(k)  table  consists  of  two 
functions--a  parsing  action function f  and  a  goto function  g: 

(1)  A  parsing  action  function f  takes  a  string  u  in  E,k  as  argument  (this 
string  is  called  the  lookahead  string),  and  the  value  of f(u)  is  either  shift, 
reduce  i,  error,  or  accept. 

SEC. 5.2  DETERMINISTIC BO'I~rOM-UP PARSING  375 

(2)  A  goto  function g  takes  a  symbol  X  in  N  u  ]E as  argument  and  has 
as  value  either  the  name  of another  LR(k)  table  or  error. 

Admittedly,  we have  not explained how to  construct  such  a parser  at this 
point.  The  construction  is  delayed  until  Sections  5.2.3  and  5.2.4. 

The  LR parser  behaves  as a  shift-reduce parsing algorithm, using a push- 
down  list,  an  input  tape,  and  an  output  buffer.  At  the  start,  the  pushdown 

list contains  the  initial  LR(k)  table  To and  nothing else.  The  input  tape con- 

tains  the  word  to  be  parsed,  and  the  output  buffer  is  initially  empty.  If  we 

assume  that  the  input  word  to  be  parsed  is  aabb, then  the  parser  would 
initially be in  configuration 

(T0, aabb, e) 

Parsing  then  proceeds  by  performing  the  following  algorithm. 
ALGORITHM 5.7 

LR(k)  parsing  algorithm. 
Input.  A  set  3  of LR(k)  tables  for  an  LR(k)  grammar  G  =  (N,  ~, P, S), with  To  ~  3  designated  as  the  initial  table,  and  an  input  string  z  ~  ~*, 

which  is to  be parsed. 

Output.  If z  ~  L(G), the right parse of  G.  Otherwise,  an error indication. 
Method.  Perform  steps  (1)  and  (2)  until  acceptance  occurs  or  an  error  is 
encountered.  If acceptance  occurs,  the  string in the  output  buffer is the right 

parse  of z. 

(1)  The  lookahead  string  u,  consisting  of  the  next  k  input  symbols,  is 
determined. 

(2)  The  parsing  action  function f  of  the  table  on  top  of  the  pushdown 
list is  applied to  the  lookahead  string u. 

(a)  If  f ( u ) =   shift,  then  the  next  input  symbol,  say  a,  is  removed 

from  the  input  and  shifted  onto  the  pushdown  list.  The  goto 
function g  of the table  on top  of the pushdown list is applied to a 

to  determine  the  new  table  to  be  placed  on  top  of the  pushdown 

list.  We then  return  to  step (1).  If there is no next input symbol or 
g  (a) is undefined,  halt  and  declare  error. 
(b)  If f(u)  =  reduce i and production i is A  --~ ~, then 21~1 symbols? 

are  removed  from  the  top  of the  pushdown  list,  and  production 
number  i  is  placed  in  the  output  buffer.  A  new  table  T'  is  then 
exposed as the  top table  of the  pushdown  list,  and  the  goto func- 
tion of T' is applied  to A to determine  the  next table to be placed 

?If  0~ =  X~-.-  Xr,  at  this  point  the  top  of  the  pushdown  list  will  be  of  the  form ToX1T1XzTz...  X, Tr. 

Removing  2 I~1  symbols  removes  the  handle  from  the  top  of 
the  pushdown  list along  with  any intervening  LR  tables. 

376  ONE-PASS NO  BACKTRACK PARSING  CHAP.  5 

on  top  of  the  pushdown  list.  We  place  A  and  this  new  table  on 
top  of the  pushdown  list  and  return  to  step  (1). 
(c)  If  f ( u ) =   error,  we  halt  parsing  (and,  in  practice,  transfer  to 

an  error  recovery  routine). 
(d)  If  f ( u ) =   accept,  we  halt  and  declare  the  string  in  the  output 

buffer  to  be  the  right  parse  of the  original  input  string.  D 

Example 5.23 

Let  us  apply  Algorithm  5.7  to  the  initial  configuration  (To, aabb, e) 
using the  LR(1) tables  of Fig.  5.9.  The lookahead  string here is a. The parsing 
action  function  of  To  on  a  is reduce  2,  where  production  2 is  S  --~ e.  By  step 

(2b),  we are to  remove  2[el  =  0  symbols  from  the  pushdown  list and  emit  2. 
The  table  on  top  of  the  pushdown  list  after  this  process  is  still  T 0.  Since 
the  goto  part  of table  To with  argument  S  is  T~, we then  place STi  on top  of 
the  pushdown  list  to  obtain  the  configuration  (ToST~,  aabb, 2). 

Let  us  go  through  this  cycle  once  more.  The  lookahead  string  is  still  a. 
The  parsing  action  of  T~  on  a  is  shift,  so  we  remove  a  from  the  input  and 
place  a  on  the pushdown  list.  The  goto  function  of  Tt  on  a  is  T2, so after this 

step  we  have  reached  the  configuration  (ToSTlaT2,  abb, 2). 

Continuing  in  this  fashion,  the  LR  parser  would  make  the  following 
sequence  of moves: 

(To, aabb, e)  ~  (ToST1,  aabb, 2) 

}--- (ToSTiaT2,  abb, 2) 

(ToSTa aT2ST3, abb, 22) 
1--- (ToSTaaT2ST3aT4,  bb, 22) 
]--- (ToSTlaT2ST3aT4ST6,  bb, 222) 

(ToSTiaT2ST3aT4ST6bTT,  b, 222) 
}-  (ToST~aT~ST3,  b, 2221) 
1--- (ToST~aT2ST3bTs, e, 2221) 
[-- (ToSTa,  e, 22211) 

Note  that  these  steps  are  essentially  the  same  as  those  of  Example  5.21  and 

that  the  LR(1)  tables  explain  the  way  in  which  choices  were  made  in  that 

example.  D 

In  this  section  we  shall  develop  the  necessary  algorithms  to  be  able  to 
automatically  construct  an  LR  parser  of  this  form  for  each  LR  grammar. 

In  fact,  we shall  see that  a  grammar  G is  LR(k)  if and  only if it has  an  LR(k) 
parser.  But  first,  let  us  return  to  the  basic  definition  of  an  LR(k)  grammar 
and  examine  some  of its  consequences. 

SEC. 5.2  DETERMINISTIC  BOTTOM-UP  PARSING  377 

A  proof  that  Algorithm  5.7  correctly  parses  an  LR(k)  grammar  requires 
considerable  development  of  the  theory  of  LR(k)  grammars.  Let  us  first 
verify  that  our  intuitive  notions  of  what  a  deterministically  right-parsable 
grammar  ought  to  be  are  in  fact  implied  by  the  LR(k)  definition.  Suppose 
that  we  are  given  a  right-sentential  form  ~tflw of an  augmented  LR(k)  gram- 
mar  such  that  ~Aw:::~. ~flw.  We  shall  show  that  by  scanning  ~fl  and 

rm FIRST~(w),  there  can  be  no  confusion  as  to 

(1)  The  location  of the  right  end  of the  handle, 

(2)  The  location  of the  left  end  of the  handle,  or 
(3)  What  reduction  to  make  once  the  handle  has  been  isolated. 

(1)  Suppose  that  there  were  another  right-sentential  form  ~fly  such  that 
FIRSTk(y ) =  FIRSTk(W )  but  that  y  can  be  written  as Y lY2Y3,  where  B ~  Y2 
is  a  production  and  t~flyiBy3  is  a  right-sentential  form  such  that  ctfly~By3 =::* ~flY~Y2Y3. 

This  case  is  explicitly  ruled  out  by  the  LR(k)  definition.  This 
rm becomes  evident  when  we  let  x  =  Y3  and  ~,B =  ~flylB  in  that  definition. 

The  right  end  of  the  handle  might  occur  before  the  end  of ft.  That  is,  there 
may be another  right-sentential  form ~,~Bvy such that B ----~ ~'2 is a  production, 

FIRSTk(y )  =  FIRSTk(w ) and ~,lBvy ~  }'l~2vy  =  tufty. This case is also ruled 

rm out  if we  let  ~,B =  7,1B and  x  =  vy  in  the  LR(k)  definition. 

(2)  Now  suppose  that  we  know  where  the  right  end  of  the  handle  of 
a  right-sentential  form  is  but  that  there  is  confusion  about  its  left  end.  That 
is,  suppose  that  ~Aw  and  tx'A'y  are  right-sentential  forms  such  that 

FIRSTk(W )  =  FIRSTk(y )  a n d   ~Aw==~tzflw  and  t~'A'y=>~'fl'y=  txfly. 

rm  rill However,  the  LR (k)condi tion  stipulates  that  tzA  =  tz'A',  so  that  both 

fl  =  fl'  and  A  =  A'.  Thus  the  left end  of the  handle  is uniquely  specified. 

(3)  There  can  be  no  confusion  of  type  (3),  since  A  =  A'  above.  Thus 
the nonterminal  which is to replace the handle is always uniquely determined. 

Let  us  now  give  some  examples  of LR  and  non-LR  grammars. 

Example 5.24 

Let  G~  be  the  right-linear  grammar  having  the  productions 

S- - - ~   CID 
C  > aClb 
D  > aDlc 

We  shall  show  G~  to  be  LR(1).I" 

t  In fact,  G~ is LR(0). 

378  ONE-PASS  NO  BACKTRACK  PARSING  CHAP.  5 

Every  (rightmost)  derivation  in  G'i  (the  augmented  version  of G~) is  either 
of the  form 

i S'  ~  S  ~  C  ~  aiC ~  aib  for  i  ~  0 

or 

i S'-----~  S  ~  D  ~  a i d   ~  aic  for  i  ~  0 

Let  us  refer  to  the  LR(1)  definition,  and  suppose  that  we  have  derivation 
S'  ~  tzAw  =~, ocflw  and  S'  ==~ 7Bx  =~  t~fly.  Then  since  G'i  is  right-linear, VIII  rm  rm 

r/n we  must  have  w  =  x  =  e.  If  FIRSTi (w )  =  FIRST~(y),  then  y  =  e  also. 

We  must  now  show  that  txA  =  7,B;  i.e.,  ~  =  7'  and  A  =  B.  Let  B  ---~ ,5  be 
the  production  applied  going  from  3,Bx  to  ocfly.  There  are  three  cases  to  con- sider. 

Case  1:  A  =  S'  (i.e.,  the  derivation  S ' = ~   o~Aw  is  trivial).  Then  tx =  e 

rill and  fl =  S.  By  the  form  of derivations  in  G'i,  there  is  only  one  way  to  derive 

the  right-sentential  from  S,  so  7'  =  e  and  B  =  S',  as  was  to  be  shown. 

Case  2:  A  =  C.  Then  fl  is  either  aC  or  b.  In  the  first  case,  we  must  have 
B  =  C,  for  only  C  and  S  have  a  production  which  end  in  C.  If B  =  S,  then 
3' =  e  by  the  form  of  derivations  in  G'i.  Then  7B  ~  ~fl.  Thus  we  may  con- 
clude  that  B  =  C,  6  =  aC,  and  3'  =  ~.  In  the  second  case  (fl  b),  we  must 
have  B  =  C,  because  only  C  has  a  production  ending  in  b.  The  conclusion 
that  7'  =  tz and  B  A  is  again  immediate. 

Case  3"  A  ---- D.  This  case  is  symmetric  to  case  2. 
Note  that  G 1 is  not  LL.  D 

Example 5.25 

Let  G 2  be  the  left-linear  g ra mm ar   with  productions 

S=  > A b   l Bc 
A~  > A a l e  

B  > B a l e  

Note  that  L(G2)  =  L(G1)  for  G1  above.  However,  G 2 is  not  LR(k)  for  any  k. 

Suppose  that  G2  is  LR(k).  Consider  the  two  rightmost  derivations  in 
the  augmented  gram ma r  G~" 

S'~----~  S  ~  Aakb  ~  akb 

rm  rm  rm and 

S'  ~  S  ~  Bakc  ~  akc 

rm  rm  rm 

sec.  5.2  DETERMINISTIC  BOTTOM-UP  PARSING  379 

These  two  derivations  satisfy the hypotheses  of the  LR(k)  definition  with 
tz  =  e,  fl  =  e,  w  =  akb,  y =  e,  and  y  =  akc.  Since  A  ~  B,  G2  is  not  LR(k). 

Moreover,  this  violation  of  the  LR(k)  condition  holds  for  any  k,  so  that 

G 2 i s n o t L R .   5 

The  grammar  in  Example  5.25  is  not  uniquely  invertible,  and  although 
we know  where  the  handle  is in  any right-sentential  form,  we do  not  always 

know whether to reduce the first handle,  which is the  empty string, to A  or B 
if  we  allow  ourselves  to  scan  only  a  finite  number  of  terminal  symbols 

beyond  the  right  end  of the  handle. 

Example 5.26 

A  situation  in  which  the  location  of a  handle  cannot  be  uniquely  deter- 
mined  is  found  in  the  grammar  G 3 with  the  productions 

S - - - - - ~ A B  
A  ----~  a 

B  ~  C DI a E  
C  .-----~. ab 
D -   >bb 

E -   > bba 

G3 is not LR(1). We can see this by considering the two rightmost derivations 
in  the  augmented  grammar" 

S'  ~  S  ~  A B   ~  A C D   ~  A Cbb  ~  Aabbb 
and 

S'  ~  S  ~  AB   ~  A a E   ~  Aabb a 

In the right-sentential form Aa b w   we cannot determine whether the right end 

of the  handle  occurs  between  b  and  w (when  w =  bb)  or  to  the  right  of Aa b 

(when  w =  ha)  if we know only the first symbol of w. Note  that  G3 is LR(2), 

however.  El 

We can give an informal but appealing definition of an LR(k) grammar in 
terms  of its  parse  trees.  We  say  that  G  is  LR(k)  if when  examining  a  parse 
tree for G, we know which production is used at any interior node after seeing 

the  frontier  to  the  left  of  that  node,  what  is  derived  from  that  node,  and 

the next k  terminal  symbols. For example in Fig.  5.10 we can determine with 
certainty  which  production  is  used  at  node  A  by  examining  uv  and 

FIRSTk(W ).  In  contrast,  the  LL(k)  condition  states  that  the  production 
which is used at A  can be determined by examining u and  FIRSTk(VW). 

380  ONE-PASS  NO  BACKTRACK PARSING  CHAP.  5 

S 

u  A  w 

1J  Fig.  5.10  Parse  tree. 
In Example  5.25 we would argue that  G 2 is not LR(k) because after seeing 
the  first  k  a's,  we  cannot  determine  whether  production  A  ~  e  or  B  ---~ e is 
to  be  used  to  derive  the  empty  string  at  the  beginning  of  the  input.  We 
cannot  tell  which  production  is  used  until  we  see  the  last  input  symbol,  b 

or  c.  In  Chapter  8 we  shall  try  to  make  rigorous  arguments  of this  type,  but 
although  the  notion  is  intuitively appealing,  it is rather  difficult  to  formalize. 

5.2.3  Implications  of  the  LR(k)  Definition 

We  shall  now  develop  the  theory  necessary  to  construct  LR(k)  parsers. 
DEFINITION 

Suppose  that  S  ~  ~Aw  =~  ~flw  is a  rightmost  derivation  in  grammar  G. 

rm  rm We  say that  a  string  ~, is a 

viable prefix  of  G if ~, is  a  prefix  of ~fl.  That  is,  ~, 

is a  string  which  is a  prefix  of some  right-sentential  form  but  which  does  not 
extend  past  the  right  end  of  the  handle  of  that  right-sentential  form. 

The  heart  of  the  LR(k)  parser  is  a  set  of tables.  These  are  analogous  to 
the  LL  tables  for  LL  grammars,  which  told  us,  given  a  lookahead  string, 
what  production  might  be  applied  next.  For  an  LR(k)  grammar  the  tables 
are  associated  with  viable  prefixes.  The  table  associated  with  viable  prefix  ~, 
will  tell  us,  given  a  lookahead  string  consisting  of the  next  k  input  symbols, 
whether  we  have  reached  the  right  end  of  the  handle.  If  so,  it  tells  us  what 
the  handle  is and  which  production  is to  be  used  to  reduce  the  handle. 

Several  problems  arise.  Since  ~,  can  be  arbitrarily  long,  it  is  not  clear 
that  any  finite  set  of  tables  will  suffice.  The  LR(k)  condition  says  that  we 
can  uniquely  determine  the  handle  of  a  right-sentential  form  if  we  know 
all  of  the  right-sentential  form  in  front  of  the  handle  as  well  as  the  next  k 
input  symbols.  Thus  it  is  not  obvious  that  we  can  always  determine  the 

handle  by  knowing  only  a  fixed  amount  of information  about  the  string  in 

front  of the  handle.  Moreover,  if S  ~  ~Aw  ~  ~flw  and  the  question  "Can 

rm  rm txflw  be derived rightmost  by a  sequence  of productions ending in production 

p ?" can  be answered reasonably,  it may not be possible to calculate the tables 

SEC. 5.2  DETERMINISTIC BOTTOM-UP PARSING  381 

for aA  from those for  aft in a way that can be "implemented"  on a pushdown 

transducer  (or possibly in any other convenient way).  Thus  we must consider 

a table  that  includes  enough information to  computethe  table corresponding 
to  aA from that for aft if it is decided that  aAw  ==~ apw  for an appropriate  w. 

rm We  thus  make  the  following  definitions. 

DEFINITION 

Let G  =  (N,  E, P,  S) be a  CFG.  We say that [A  ---~ fll  *  flz,  u] is an LR(k) 
item  (for  k  and  G,  but  we  usually  omit  reference  to  these  parameters  when 
they  are  understood)  if A  ---~ fl~fl2  is  a  production  in  P  and  u  is  in  E *k.  We 

say  that  LR(k)  item  [A  ----~ fll  * flz,  u]  is  valid for  afl~,  a  viable  prefix  of  G, 

if  there  is  a  derivation  S  =-~ a A w = ,   ocfllflzw  such  that  u  =  FIRSTk(w ). 

rnl  rIi1 Note  that  fix  may  be  e  and  that  every  viable  prefix  has  at  least  one  valid 

LR(k)  item. 

Example 5.27 

Consider  grammar  G1  of Example  5.24.  Item  [C ---~ a  * C, e]  is  valid  for 
aaa,  since  there  is  a  derivation  S  ~  aaC => aaaC.  That  is,  ~  =  aa  and 

rill  rill w =  e  in  this  example. 

Note  the  similarity  of  our  definition  of  item  here  to  that  found  in  the 
description  of  Earley's  algorithm.  There  is  an  interesting  relation  between 
the  two  when  Earley's  algorithm  is  applied  to  an  LR(k)  grammar.  See 

Exercise  5.2.16. 

The  LR(k)  items  associated  with  the  viable  prefixes  of  a  grammar  are 
the  key  to  understanding  how  a  deterministic  right  parser  for  an  LR(k) 
grammar  works.  In  a  sense  we  are  primarily  interested  in  LR(k)  items  of 
the  form  [A ~  fl  -,  u],  where  the  dot  is  at  the  right  end  of the  production. 
These items indicate which productions can be used to  reduce right-sententia! 

forms.  The  next  definition  and  the  following  theorem  are  at  the  heart  of 

LR(k) parsing. 

DEFINITION 

We  define  the  e-free first  function,  EFF~(00  as  follows  (we  shall  delete 
the  k  and/or  G when  clear)" 

(1)  If a  does  not  begin  with  a  nonterminal,  then  EFFk(a)  =  FIRSTk(00. 

(2)  If a  begins  with  a  nonterminal,  then 

* EFFk(a)  =  (w  I there  is  a  derivation  a  ~  fl ~  wx, 

rlTl  rm where  B ~  Awx  for  any  nonterminal  A),  and  w =  FIRSTk(WX) 

Thus,  EFFk(a ) captures  all members  of FIRSTk(a ) whose derivation  does 

382  ONE-PASS NO BACKTRACK PARSING  CHAP.  5 
not  involve  replacing  a  leading  nonterminal  by  e  (equivalently,  whose  right- 
most  derivation  does  not  use  an  e-production  at  the last step,  when g  begins 
with  a  nonterminal). 

Example 5.28 

Consider  the  grammar  G with  the  productions 

S  > AB  
A  > Bale  

B~  > CbIC 
C  >cle 
FIRST2(S)  =  {e,  a,  b,  c, ab,  ac,  ba,  ca,  cb} 

EFF2(S )  =  {ca,  cb}  D 

Recall  that  in  Chapter  4  we  considered  a  bottom-up  parsing  algorithm 
which  would ~ not  work  on  grammars  having  e-productions.  For  LR(k) 
parsing  we can  permit  e-productions  in the  grammar,  but  we must  be careful 

when  we  reduce  the  empty  string  to  a  nonterminal. 

We  shall  see that  using  the  E F F  function  we  are  able  to  correctly  deter- 
mine  when  the  empty  string  is  the  handle  to  be  reduced  to  a  nonterminal. 

First,  however,  we  introduce  a  slight  revision  of  the  LR(k)  definition.  The 
two  derivations  involved in  that  definition  really  play  interchangeable  roles, 
and  we  can  therefore  assume  without  loss  of  generality  that  the  handle  of 
the  second  derivation  is  at  least  as  far  right  as  that  of the  first. 

LEMMA 5.2 

If  G  =  (N,  ~,  P,  S')  is  an  augmented  grammar  which  is  not  LR(k),  then 
there  exist  derivations  S' ~  gAw  ~  t~j3w  and  S'  ~  ?Bx  ~  ?#x  =  g/3y, 

rm  rm  rm  rill where  FIRSTk(w )  =  FIRSTk(y )  and  l r,~t  >_ Is,at  but  ?Bx  ~  gay. 

Proof.  We  know  by  the  LR(k)  definition  that  we  can  find  derivations 
satisfying  all  conditions,  except  possibly  the  condition  ]76[  >  I~Pl.  Thus, 
assume  that  I ral  <  I~Pl.  We  shall  show  that  there  is  another  counter- 
example  to  the  LR(k)  condition,  where  ?~  plays  the  role  of  t~,a in  that  con- 
dition. 

Since we are given that ?6x  =  ~fly and  i ral  <  I~/~ !, we find that for some 
z  in  E+,  we  can  write  gfl  =  ?6z.  Thus  we  have  the  derivations 

S'  ~  ?Bx  ~  ?~x, 

rill  rill and 

S'  ~  ~Aw  ~  ~pw  =  ?d~zw 

rill  rill 

SEC. 5.2  DETERMINISTIC  BOTTOM-UP  PARSING  383 

Now  z  was  defined  so  that  x - - z y .   Since  FIRSTk(w )  =  FIRSTk(y),  it 
follows  that  FIRSTk(x )  =  FIRST~(zw).  The  LR(k)  condition,  if  it  held, 
would  say  that  ocAw  =  7Bzw.  We  would  have  7Bz  =  ~A  and  7Bzy  =  o~Ay, 

using operations  of  "cancellation"  and  concatenation,  which  preserve  equal- 
ity.  But  zy  =  x,  so  we  have  shown  that  e~Ay  =  7Bx,  which  we  originally 

assumed  to  be  false.  If  we  relate  the  two  derivations  above  to  the  LR(k) 
condition,  we  see  that  they  satisfy  the  conditions  of  the  lemma,  when  the 
proper  substitutions  of  string  names  are  made,  of  course. 

LR(k)  parsing  techniques  are  based  on  the  following  theorem. 
THEOREM 5.9 

A  grammar  G  =  (N,  1~, P,  S)  is  LR(k)  if and  only if the  following  condi- 
tion  holds  for  each  u  in  ]~.k.  Let  ~fl  be  a  viable  prefix  of  a  right-sentential 
form  ~]~w of the  augmented  grammar  G'.  If LR(k)  item [A  --~ f l . ,   u] is valid 
for  ~p,  then  there  is  no  other  LR(k)  item  [Ax  ~  fl~  *  P2,  v]  which  is  valid 

for  ~fl  with  u  in  EFFk(flzv ).  (Note  that  f12  may  be  e.) 

Proof. 

Only  if:  Suppose  that  [A --~  p . ,   u]  and  [A~  ~  p~  *  P2,  v]  are  two  dis- 
tinct  items  valid  for  0eft. That  is to  say,  in  the  augmented  grammar 

S'  ~  ocAw ~  ocpw  with  FIRSTk(w )  :  u 

rm  rill 

S'  ~  0~tA ~x ~  ~ P 2 x   with  FIRSTk(x )  =  v 

rill  rm 

and  ~p  =  0c~p~. Moreover,  P2x ~  uy  for  some  y  in  a  (possibly  zero  step) 

rm derivation  in  which  a  leading  nonterminal  is  never  replaced  by  e. 

We  claim  that  G  cannot  be  LR(k).  To  see  this,  we  shall  examine  three 
cases  depending  on  whether  (1)  P2  =  e,  (2)  P2 is in  E+,  or  (3)  ,02 has  a  non- 
terminal. 

Case  1:  If P2  =  e,  then  u  =  v,  and  the  two  derivations  are 

and 

:o S'  ~  ocAw ~  ~pw 
rill  rill 

S'  ~  ~,AlX  ---->. ~ p ~ x  

rill  rill 

where  FIRSTk(W )  =  FIRSTk(x )  =  u  ---- v.  Since  the  two  items  are  distinct, 

either  A  ~  A~  or  fl  ~  fl~. In  either  case  we  have  a  violation  of  the  LR(k) 

definition. 

384  ONE-PASS  NO  BACKTRACK  PARSING  CHAP.  5 

Case  2:  If f12  =  z  for  some  z  in  E +,  then 

and 

S'  ~  a A w   ~  aflw 

rill  riD. 

S'  ~  a~A ~x ~  a~fl~zx 

rill  rm 

where  aft  =  a~fl~  and  FIRSTk(zx )  =  u.  But  then  G  is  not  LR(k),  since 
aA z x   cannot  be  equal  to  a~Alx  if z  ~  E+. 

Case  3:  Suppose  that  f12 contains  at least  one  nonterminal  symbol.  Then 
f12 ==~ u~Bu3  ==~ u~u2u3,  where  u~u2  ~  e,  since  a  leading  nonterminal  is  not rm  rm 

to  be  replaced  by  e  in  this  derivation.  Thus  we  would  have  two  derivations 

and 

S'  ~  a A w   ~  aflw rm  rill 

r m  rill 

alfllu~Bu3x  ~  alfllulu2u3x I'm  rm 

such  that  alfl~  =  aft  and  uluzu3x  =  uy.  The  LR(k)  definition  requires  that 
OCAUlUzU3X =  ~xfllulBu3x.  That  is,  aAu~u2  =  a~fl~u~B.  Substituting  aft  for 
a~fl~,  we  must  have  Aulu2  =  flu~B.  But  since  uluz  ~  e,  this  is impossible. 

Note  that  this  is  the  place  where  the  condition  that  u  is  in  EFFk(fl2v )  is 
required.  If we had  replaced  E F F   by  FIRST  in  the  statement  of the  theorem, 
then  u~u2  could  be  e  and  aAu~u2u3x  could  then  be  equal  to  a~fl~u~Bu3x  (if 

ulu2  =  e  and  fl  =  e). 

If:  Suppose  that  G  is  not  LR(k).  Then  there  are  two  derivations  in  the augmented  grammar 

(5.2.1)  S'  ~  aAw  ~  aflw 

rm  rm 

and 

(5.2.2)  S'  ~  ~Bx  - - ~   7~x  =  afly 

rill  rill 

such  that  FIRSTk(w )  =  FIRSTk(y )  =  u,  but  aAy  ~  ~,Bx.  Moreover,  we  can 
choose  these  derivations  such  that  aft  is  as  short  as  possible. 

By  Lemma  5.2,  we  may  assume  that  ]~,~[ ~  [aft[.  Let  alA1yx  be  the  last 

SEC. 5.2  DETERMINISTIC  BOTTOM-UP  PARSING  385 
right-sentential  form  in  the  derivation 

S'  ~  },Bx 

rm 

such  that  the  length  of its  open  portion  is  no  more  than  lafll  +  1.  That  is, I~A~  I _<[~fll  +  1. 

Then  we  can  write  (5.2.2)  as 

(5.2.3)  rm  rm  rm 
where  a,]31 =  aft.  By  our  choice  of  txlAly~,  we  have  [a,[_~  ]afll  _~ 1~'61. 

Moreover,  flzYl  =o. y  does  not  use  a  production  B  --~  e at  the  last  step  from 

rm our  choice  of  a lA~yl.  That  is  to  say,  if  B  ~  e  were  the  last  production 

applied,  then  alA ~yl  would  not  be  the  last  right-sentential  form  in  the  deri- 
vation  S=>  ?,Bx  whose  open  portion  is  no  longer  than  l aft[  +  1.  Thus,  u 

rm =  FIRSTk(y)  is  in  EFFk(fl2yl).  We  may  conclude  that  [A1  ~  fll  * f12, v]  is 

valid  for  aft  where  v  =  FIRST~(yi). 

From  derivation  (5.2.1),  [A--+  f l . ,   u]  is  also  valid  for  aft,  so  that  it 
remains  to  show  that  A~ ~  c/1-c/2 is  not  the  same  as  A  ~ / 3 - .  

To  show  this,  suppose  that  A1  ~  & . &   is  A  ~  #-.  Then  derivation 
(5.2.3)  is  of the  form 

S'--->  alAy  =--~ alfly 

i'm  i'm 

where  alfl  =  aft.  Thus  el  =  a  and  aAy  =  o~Bx, contrary  to  the  hypothesis that  G  is  not  LR(k).  [~] 

The  construction  of  a  deterministic  right  parser  for  an  LR(k)  grammar 
requires  knowing  how  to  find  all  valid  LR(k)  items  for  each  viable  prefix  of 
a  right-sentential  form. 

DEFINITION 

Let  G  be  a  C FG   and  y a  viable  prefix  of  G.  We  define  V~(~,) to  be the set 
of  LR(k)  items  valid  for  ~, with  respect  to  k  and  G.  We  again  delete  k  and/or 

G  if understood.  We  define  ,~  =  { a l a   -=  V~(~') for  some  viable  prefix  ?  of  G} 
as  the  collection  of  the  sets  of  valid LR(k)  items  for  G.  ~  contains  all  sets  of 

LR(k)  items  which  are  valid  for  some  viable  prefix  of  G. 

We  shall  next  present  an  algorithm  for  constructing  a  set  of LR(k)  items 
for  any  sentential  form,  followed  by  an  algorithm  to  construct  the  collection 

of the  sets  of  valid  items  for  any  grammar  G. 

386  ONE-PASS NO BACKTRACK PARSING  CHAP.  5 

ALGORITHM  5.8 

Construction  of  V~(?). 
Input.  CF G  G  =  (N,  X, P,  S)  and  7 in  (N  u  X)*. 

Output.  V~(r). 
Method.  If 7  =  X t X 2" ' "   X,,  we  construct  V~(7)by  constructing  Vk(e), 
v~ (x~ ),   v~ (x xx~ ) ,  .  .  .,   v~ ( x, x~   . ..   x. ) . 

(1)  We  construct  Vk(e)  as  follows" 

(a)  If S  ~  ~  is in P,  add  [S ----,  * ~, e]  to  Vk(e). 
(b)  If  [A --~  * B~,  u]  is  in  Ve(e )  and  B  ~  fl  is  in  P,  then  for  each  x 

in  FIRSTk(~u)  add  [ B - - , .   ,8, x]  to  Vk(e),  provided  it  is  not 
already  there. 
(c)  Repeat  step  (b)  until  no  more  new  items  can  be  added  to  Vk(e). 
(2)  Suppose  that  we  have  constructed  V, (X 1X2...   Xi_x),  i ~  n.  We 
construct  Vk(XtX2  "'"  Xi)  as  follows" 

(a)  If  [A---*  ~ - X ,  fl, u]  is  in  V k(Xx. ..  Xi_,),  add  [A--*  ~Xt"  fl,  u] 

to  Vk(X,  . . .   X,). 
(b)  If [A --*  ~  * Bfl,  u] has  been  placed  in  Vk(Xi  "'"  X3  and  B  ---, J 

is  in  P,  then  add  [B---..  J,  x]  to  V k ( X , . . .   X3  for  each  x  in FIRSTk(flu), 

provided  it  is  not  already  there. 
(c)  Repeat  step  (2b)  until  no  more  new  items  can  be  added  to 

v~ (x ,  .. .  x,).  El 

DEFINITION 

The  repeated  application  of step  (lb)  or  (2b)  of Algorithm  5.8  to  a  set  of 
items  is  called  taking  the  closure of that  set. 

We  shall  define  a  function  GOTO  on  sets  of items  for  a  grammar  G  = 
(N,  Z, P,  S).  If a  is a  set of items  such that  ~  =  V~(7) , where  7  ~  (N  U  Z)*, 
then  GOTO(~,  X)  is  that  ~t'  such  that  ~ t ' =   V~(?X),  where  X  ~  (N  u  Z). 
In  Algorithm  5.8  step  (2)  computes 

Vk(XaX2  .  .  .  X~)  =  G O TO ( Vk ( X ,  X2  . .  .  Xt_ ,),  Xt). 
Note that step (2) is really independent  of X1  . "   X~_ ,, depending only on the 
set  Vk(Xi  . ..   Xt-,)  itself. 

Example 5.29 

Let  us  construct  Va(e),  V,(S),  and  Vx(Sa )  for  the  augmented  grammar 

S'  > S 

S  > SaSb 
S  >e 

SEC. 5.2  DETERMINISTIC  BOTTOM-UP  PARSING  387 
(Note,  however,  that  Algorithm  5.8  does  not  require  that  the  grammar  be 
augmented.)  We  first  compute  V(e)  using  step  1  of  Algorithm  5.8.  In  step 
(la)  we  add  [S'  --,  *  S,  e]  to  V(e).  In  step  (lb)  we  add  [S ----~ *  SaSb,  e]  and 

[S  ~ . ,   e]  to  V(e).  Since  [S ----~ *  SaSb,  e]  is  now  in  V(e),  we  must  also  add 
IS  ---~  *  SaSb,  x]  and  [S  ---~.,  x]  to  V(e)  for  all  x  in  FIRST(aSb)  =  a.  Thus, 

V(e)  contains  the  following  items" 

IS'  ~  *  S,  e] 

[S  --->-.  SaSb,  e/a] 
[S~  > .,  e/a] 

Here  we  have  used  the  shorthand  notation  [A  ~  ~x.  ,8, x ~ / x 2 / " ' / x , ]   for 
the  set  of  items  [A  ----~ ~ .   ,8, x~],  [A  ---~ 0~ *  fl,  x 2 ] , . . . ,   [A  ~  ~ .   fl,  x,].  To 
obtain  V(S),  we  compute  GOTO(V(e),  S).  From  step  (2a)  we  add  the  three 
items  [S'  ---~ S . ,   e]  and  IS  ---~ S .   aSb,  e/a]  to  V(S).  Computing  the  closure 

adds  no  new  items  to  V(S),  so  V(S)  is 

IS'  > S . ,   e] 

[S  ~  S  .  aSb,  e/a] 

V(Sa)  is  computed  as  GOTO(V(S),  a).  V(Sa)  contains  the  following  six 
items" 

[S  > Sa  *  Sb,  e/a] 

[s  > *  SaSh,  a/b] 
[S  =  ~  .,  a/b]  [[] 

We  now  show  that  Algorithm  5.8  correctly  computes  V~(y). 
THrORnM  5.10 

An  LR(k)  item  is  in  V~(7) after  step  (2)  of  Algorithm  5.8  if  and  only  if 
that  item  is  valid  for  ?. 

Proof. 

If:  It  is  left  to  the  reader  to  show  that  Algorithm  5.8  terminates  and 
correctly  computes  Vk(e).  We  shall  show  that  if all  and  only  the  valid  items 
for  X1  "--  Xt_l  are  in  Vk(XxX2  . "   Xt-1),  then  all  and  only  the  valid  items 

for  X1  .."  X~ are  in  V~(Xi  . . .   Xt). 

Suppose  that  [A  ~  fl~  *  ,82, u]  is  valid  for  X1  .."  X~.  Then  there  exists 

a  derivation  S  ==~ o~Aw ~  ocfli[32w  such  that  0q81 =  X~Xz  . . .   Xt  and 

rill  rill u  =  FIRSTk(w ).  There  are  two  cases  to  consider. 

Suppose  ,8,  =  ,8'~X~. Then  [A  ---~ ,8~  *  X,82,  u]  is  valid  for  X~  . . .   Xt_~ 

308  ONE-PASS  NO  BACKTRACK  PARSING  CHAP.  5 
and,  by  the  inductive  hypothesis,  is  in  Vk(X~  . . .   Xt_x).  By  step  (2a)  of 
Algorithm  5.8,  [A  ---~ ]3'~X~  *  ,132, u]  is  added  to  Vk(X~  . ..   Xt). 

Suppose  that  fl~  =  e,  in  which  case  ~  =  X~  . . .   Xt.  Since  S  =>  ~Aw  is 

rm a  rightmost  derivation,  there  is  an  intermediate  step  in  this  derivation  in 

which  the  last  symbol  Xt  of  0c is  introduced.  Thus  we  can  write  S  ~  ogBy 

rm 

* 'TXfly  =-~ ocAw,  where  ~'~, =  X ~ . . .   X~_~, and  every  step  in  the  deri- rill  rill 

vation  ~'TXfly ~  ~A w  rewrites  a  nonterminal  to  the  right  of  the  explicitly 

rm shown  Xt.  Then  [B ~  7 .   Xfl,  v],  where  v  =  FIRSTk(y),  is  valid  for 

X~  . . .   Xt_ ~, and  by  the  inductive  hypothesis  is  in  Vk(X~  "'"  X~_ ~).  By  step 

(2a)  of  Algorithm  5.5,  [B ~  7X~.  ~,  v]  is  added  to  V k ( g~ ' ' "   Xt).  Since 

@=:,. Aw,  we can  find a  sequence  of nonterminals  D~, D2,...  , D m and  strings rm 
02 , . . . ,   Om  in  (N  W E)*  such  that  $  begins  with  D~,  A  =  Din, and  produc- 
tion  D~ ----~ Dr+ ~0~+~ is  in  P  for  1 ~  i  ~  m.  By  repeated  application  of  step 

(2b),  [A  ~  * f12, u]  is  added  to  Vk(X 1  . . .   X~).  The  detail  necessary  to  show 
that  u  is  a  valid  second  component  of  items  containing  A  ~  *  f12  is  left  to the  reader. 

Only  if:  Suppose  that  [A  ----~ fit  *  f12, u]  is  added  to  Vk(X~  . . .   Xt).  We 
show by induction  on the number  of items previously added to  Vk(Xt  . ..   Xt) 
that  this  item  is valid  for  X~  . . .   X~. 

The  basis,  zero  items  in  Vk(X~  "'"  Xt),  is  straightforward.  In  this  case 
[A  --~ 1/~  * f12, u]  must  be  placed  in  Vk(X~  "'"  X~)  in  step  (2a),  so  fl~  =  fl'~X~ 

and  [A  ~  fl'~  *  Xtfl~,  u]  is  in  Vk(X~  " "   X~_ ~).  Thus,  S  ~  txAw  =-~ o~fl'~X~flzw 

rm  rill and  0~fl'~ =  X ~ . . .   X~_~.  Hence,  [A  --~  i l l .   flz,  u]  is  valid  for  X ~ . . .   Xt. 

For  the  inductive  step,  if  [A  ---~ fl~  * flz,  u]  is  placed  in  Ve(X~  . . .   X~)  at 
step  (2a),  the  argument  is  the  same  as  for  the  basis.  If this  item  is  added  in 
step  (2b),  then  fl~  =  e,  and  there  is  an  item  [B --~ 7  *  A$,  v]  which  has  been 
previously  added  to  Vk(Xt  "'"  X~),  with  u  in  FIRST~($v).  By  the  inductive 
hypothesis  [B---~  ~, *  A$,  v]  is  valid  for  X~  . ..   X~,  so  there  is  a  derivation 

S  ~  rE'By ==~ tx'TAJy,  where  0c'7 =  X~  . . .   X~.  Then rm  rm 

S  =~  X ,   . . .   X , A @   =~ X ,  . . .   X, A z   =~ X ,  . . .   X, fl2z, 

rm  rm  rm 

where  u :  FiRST~(z).  Hence  [A ~  * ,8~, u]  is  valid  for  X~  . . .   X i. 

Algorithm  5.8  provides  a  method  for  constructing  the  set  of LR(k)  items 
valid  for  any viable  prefix.  In  the  construction  of a  right  parser  for  an  LR(k) 
grammar  G we are  interested  in the  sets  of items  which  are  valid for all viable 
prefixes  of  G,  namely  the  collection  of  the  sets  of  valid  items  for  G.  Since 

a  grammar  contains  a  finite  number  of  productions,  the  number  of  sets  of 

SEC. 5.2  DETERMINISTIC BOTTOM-UP PARSING  389 
items  is  also  finite,  but  often  very  large.  If  ?  is  a  viable  prefix  of  a  right 

sententiai  form  ?w,  then  we  shall  see  that  Vk(?) contains  all the  information 
about  ?  needed  to  continue  parsing  yw. 

The  following  algorithm  provides  a  systematic  method  for  computing 
the  sets  of LR(k)  items  for  G. 

ALGORITHM 5.9 

Collection  of sets  of valid  LR(k)  items  for  G. 
Input.  CFG  G  =  (N,  X, P,  S)  and  an  integer  k. 

Output.  S  =  [t~l(2  =  Vk(?),  and  ? is  a  viable  prefix  of  G}. 
Method.  Initially  S  is empty. 

(1)  Place  Vk(e)  in  g.  The  set  Vk(e)  is  initially  "unmarked." 
(2)  If  a  set  of  items  a  in  $  is  unmarked,  mark  6t  by  computing,  for 
each  X  in  N  U  X,  GOTO(~t,  X).  (Algorithm  5.8  can  be  used  here.)  If 

e t ' =   GOTO(a,  X)  is  nonempty  and  is  not  already  in  $,  then  add  a '  to  $ 
as  an  unmarked  set  of items. 

(3)  Repeat  step  (2)  until  all  sets  of items  in  $  are  marked.  D 

DEFINITION 

If  G  is  a  CFG,  then  the  collection  of  sets  of  valid  LR(k)  items  for  its 
augmented  grammar  will  be  called  the  canonical  collection  of sets  of  LR(k) 

items for  G. 

Note  that  it  is  never  necessary  to  compute  GOTO(~,  S'),  as  this  set  of 
items will  always  be  empty. 

Example  5.30 

Let  us  compute  the  canonical  collection  of  sets  of  LR(1)  items  for  the 
grammar  G whose augmented  grammar  contains  the  productions 

We  begin  by  computing  tg 0 

S t  =  >  S 

S  > SaSb 
S  >e 

=  V(e).  (This  was  done  in  Example  5.29.) 

(go:  IS'  >  *  S,  e] 

IS  > *  SaSb,  e/a] 

IS  >. ,  e/a] 

We  then  compute  GOTO((g o, X)  for  all  X  ~  (S,  a,  b}.  Let  GOTO(ao,  S) 
be  a~ 1. 

300  ONE-PASS NO  BACKTRACK  PARSING  CHAP. 5 

121"  [S'  > S . , e ]  

[S  > S  .  aSb,  e/a] 

GOTO(120,  a)  and  GOTO(a0,  b)  are  both  empty,  since  neither  a  nor  b  are 
viable prefixes  of G.  Next we must compute  GOTO(al,  X) for  X  ~  {S, a, b}. 

GOTO(~i,  S)  and  GOTO(~i,  b)  are  empty  and  a; 2 =  GOTO(12~, a)  is 

122"  [S  > Sa  *  Sb,  e/a] 

[S  > *  SaSb,  a/b] 
[S  > . ,   a/b] 

Continuing,  we  obtain  the  following sets  of items" 

t~3 :  [S  > S a S  .  b,  e/a] 

[S  > S.   aSb,  a/b] 
124:  [S  ,  S a.   Sb,  a/b] 

[S  >  .  SaSb,  a/b] 

125" 
12 6" 

[S  >  .,  a/b] 

[s  ~ S a S h . ,   e/a] 
[S  > S a S .   b,  a/b] 
[S  > S  .  aSb,  a/b] 
127"  [S  > SaSb  .,  a/b] 

The  GOTO  function  is  summarized  in  the  following table" 

Set of 
Items 

120 
121 
122 
123 
124 
125 

126 

127 

Grammar Symbol 

S  a  b 

121 
123 
126 

u  m 
122  m 

124  t25 

124  127 

Note that  GOTO(12,  X) will always be empty if all items in 12 have the  dot at 
the  right  end  of the  production.  Here,  125 and  127 are  examples  of such  sets 

of items. The  reader  should  note  the  similarity  in  the  GOTO  table  above  and 

the  GOTO  function  of the  LR(1)  parser  for  G in  Fig.  5.9.  [Z] 

SEC. 5.2  DETERMINISTIC  BOTTOM-UP  PARSING  391 

THEOREM 5.11 

Algorithm  5.9  correctly  determines  ~. 
Proof.  By Theorem  5.10 it  sumces  to  prove  that  a  set  of  items  (~ is placed 
in  S  if  and  only  if  there  exists  a  derivation  S  =*  ocAw =~  6cflw,  where  7  is 

rm  rm a  prefix  of  0eft and  6 - -   Vk(7). The  "only  if"  portion  is  a  straightforward 

induction  on  the  order  in  which  the  sets  of  items  are  placed  in  S.  The  "if" 
portion  is  a  no  less  straightforward  induction  on  the  length  of  7.  These  are 

both  left  for  the  Exercises.  [-] 

5.2.4.  Testing  for the  LR(k)  Condition 

It may be of interest  to  know  that  a  particular  grammar  is LR(k)  for  some 
given  value  of  k.  We  can  provide  an  algorithm  based  on  Theorem  5.9  and 

Algorithm  5.9. 

DEFINITION 

Let  G  --  (N,  Z, P,  S)  be  a  C F G  and  k  an  integer.  A  set  6  of  LR(k)  items 
for  G  is  said  to  be  consistent  if no  two  distinct  members  of 6  are  of the  form 

[A  --~ fl-,  u]  and  [B  ~  fll  "  f12,  v],  where  u  is  in  EFFk(flzv).  flz  may  be  e. 

ALGORITHM 5.10 

Test  for  LR(k)-ness. 
Input.  CF G  G  --  (N,  X, P,  S)  and  an  integer  k  ~  0. 

Output.  "Yes"  if  G  is  LR(k);  "no"  otherwise. 
Method. 

(1)  Using  Algorithm  5.9,  compute  g,  the  canonical  collection  of the  sets 
of  LR(k)  items  for  G. 

(2)  Examine  each  set  of  LR(k)  items  in  $  and  determine  whether  it  is 
consistent. 

(3)  If all  sets in  g  are  consistent,  output  Yes.  Otherwise,  declare  G  not  to 
be  LR(k)  for  this  particular  value  of k.  [---] 

The correctness  of Algorithm  5.10 is merely a  restatement of Theorem  5.9. 
Example  5.31 

Let  us  test  the  grammar  in  Example  5.30  for  LR(l)-ness.  We  have 
S  =  {60,  * * *,  67}.  The  only  sets  of  LR(1)  items  which  need  to  be  tested  are 
those  that  contain  a  dot  at  the  right  end  of a  production.  These  sets  of items 
are  60,  6  I, 6 z,  64,  65,  and  67. 

Let  us  consider  60.  I n   the  items  [S'  --,  *  S,  e]  and  [S - - ,  *  SaSb,  e/a]  in 
6o,  EFF(S)  and  EFF(Sa)  are  both  empty,  so  no  violation  of  consistency 

with  the  items  [S - - , . ,   e/a]  occurs. 

392  ONE-PASS NO  BACKTRACK PARSING  CHAP.  5 

Let  us  consider  a  1.  Here  EFF(aSb)=  EFF(aSba)=  a,  but  a  is  not 
a  lookahead  string  of  the  item  [S'----~ S . ,   e].  Therefore,  ~1  is  consistent. 

The  sets  of  items  ~2  and  ~4  are  consistent  because  EFF(Sbx)and EFF(SaSbx) 

are  both  empty  for  all x.  The  sets  of items ~5  and  ~7 are clearly consistent. 

Thus  all  sets in  S  are  consistent,  so  we have  shown  that  G is  LR(1).  D 
5.2.5.  Deterministic  Right  Parsers  for LR(k) 

Grammars 

In  this  section  we  shall  informally  describe  how  a  deterministic  extended 
pushdown  transducer  with  k  symbol  lookahead  can  be  constructed  from 
an  LR(k)  grammar  to  act  as  a  right  parser  for  that  grammar.  We  can  view 
the pushdown  transducer  described earlier as a  shift-reduce  parsing algorithm 
which  decides  on  the  basis  of  its  state,  the  top  pushdown  list  entry,  and 
the  lookahead  string whether  to  make  a  shift  or  a  reduction  and,  in the latter 
use,  what  reduction  to  make. 

To help  make  the  decisions,  the  parser  will have  in  every other  pushdown 
list  cell  an  "LR(k)  table,"  which  summarizes  the  parsing  information  which 
can  be  gleaned  from  a  set  of items.  In  particular,  if ~ is  a  prefix  of the  push- 

down  string  (top  is  on  the  right),  then  the  table  attached  to  the  rightmost 
symbol  of ~  comes  from  the  set  of items  Vk(00. The  essence  of the  construc- 
tion  of  the  right  parser,  then,  is  finding  the  LR(k)  table  associated  with a  set  of items. 

DEFINITION 

Let  G  be  a  C FG   and  let  S  be  a  collection  of  sets  of  LR(k)  items  for  G. 
T(~),  the  LR(k)  table associated with the  set  of  items  ~  in  S,  is  a  pair  of 
functions  (f,   g~.  f  is  called  the  parsing  action function  and  g  the  goto function. 

(1)  f  maps  E *k  to  {error,  shift,  accept}  U  {reduce  i{  i  is  the  number  of a  production  in P,  i  _~  1 }, where 

(a)  f ( u ) =   shift  if  [A  ~  ill"  f12, v]  is  in  ~,  f12  ~  e,  and  u  is  in 

EFFk(fl2v). 
(b)  f(u)  =  reduce  i if [A  ~  f l . ,   u]  is  in  ~  and  A  ---~ fl  is production iinP,  i ~   1. 

(c)  f(e)  =  accept  if [S'  --,  S . ,   e]  is  in  ~. 
(d)  f ( u ) =   error  otherwise. 
(2)  g,  the  goto  function,  determines  the  next  applicable  table.  Some  g 
will be invoked  immediately after each  shift  and reduction.  Formally, g  maps 
N  U  E  to  the  set  of tables  or  the  message  error,  g(X) is  the  table  associated 
with  GOTO(~,  X).  If  GOTO(~,  X)  is  the  empty  set,  then  g(X)=  error. 

We  should  emphasize  that  by  Theorem  5.9,  if  G  is  LR(k)  and  $  is  the 

sEc. 5.2  DETERMINISTIC BOTTOM-UP PARSING  393 

canonical  collection  of  sets  of  LR(k)  items  for  G,  then  there  can  be  no  con- 
flicts  between  actions  specified  by rules  (la),  (1 b),  and  (l c)  above. 

We  say  that  the  table  T(~)  is  associated  with  a  viable  prefix  ?  of  G  if 
a  = 

DEFINITION 

The  canonical set of LR(k) tables for an LR(k)  grammar  G  is the pair 
(~, To), where 5 the set of LR(k) tables associated with the canonical collec- 
tion of sets of LR(k) items for G. T o is the LR(k) table associated with V~(e). 

W e  shall usually represent a canonical LR(k) parser as a table, of which 
each row is an LR(k)  table. 

The LR(k) parsing algorithm given as Algorithm 5.7 using the canonical 
set of LR(k)  tables will be called the canonical LR(k)parsing algorithm or 
canonical LR(k) parser, for short. 

We  shall now  summarize the process of constructing the canonical set of 
LR(k) tables from an LR(k) grammar. 

ALGORITHM  5.11 

Construction  of  the  canonical  set  of  LR(k)  tables  from  an  LR(k)  gram- 
mar. 

Input.  An  LR(k)  grammar  G  =  (N,  Z, P,  S). 

Output.  The  canonical  set  of LR(k)  tables  for  G. 
Method. 
(1)  Construct  the  augmented  grammar 

G ' =   (N  U  IS'}, Z, P  U  {S'  --~  S},  S'). 
S ' - - ,   S  is  to  be  the  zeroth  production. 

(2)  From  G'  construct  S,  the  canonical  collection  of  sets  of valid  LR(k) 
items  for  G. 

(3)  Let  3  be  the  set  of  LR(k)  tables  for  G,  where  ~3 =  {T[T  =  T(a)  for 
some  Ct  ~  8}.  Let  To  =  T(Cto), where  Ct o =  V~(e).  [~ 

Example  5.32 

Let  us  construct  the  canonical  set  of  LR(1)  tables  for  the  grammar  G 
whose  augmented  grammar  is 

(o)  s' 

(1)  S  > SaSb 
(2)  S  > e 

The  canonical  collection  $  of sets  of  LR(1)  items  for  G is  given  in  Example 

5.30.  From  S  we  shall  construct  the  set  of  LR(k)  tables. 

394  ONE-PASS NO BACKTRACK  PARSING  CHAP.  5 

Let us construct  To =  <f0, go>, the  table  associated with a~ 0. Since k  =  1, 
the  possible  lookahead  strings  are  a,  b,  and  e.  Since  ~o  contains  the  items 

[S ~ . ,   e/a], f o ( e ) =   fo(a)=  reduce  2.  From  the  remaining  items  in  ~t o we 
determine  that  fo(b)=  error  [since  EFF(S00  is  empty].  To  compute  the 

GOTO  function  go,  we  note  that  GOTO(a~o,  S ) =   ~i  and  GOTO(~o,  X)  is 
empty  otherwise.  If  T1  is  the  name  given  to  T(~i),  then  go(S)=  Tx  and go(X) 

=  error  for  all  other  X.  We  have  now  completed  the  computation  of 
To.  We  can  represent  To  as follows" 

fo 

b 

To  2  X  2 

go 
,S c  a  b 

T1  X  X 
Here,  2  represents  reduce  using production  2,  and  X  represents  error. 

Let  us  now  compute  the  entries  for  T~ =  (f~,  g~).  Since  [S'--~  S . ,   e]  is 
in  ~t,  we  have  f~(e)=  accept.  Since  IS ~  S .   aSb,  e/a]  is  in  ~1,  f t ( a ) =  

shift.  Note  that  the  lookahead  strings  in  this  item  have  no  relevance  here. 
Then,  f j ( b ) =   error.  Since  GOTO(t~,  a ) =   t~ 2,  we  let  gx(a)=  T2,  where 

T~  =  T ( ~ ) .  

Continuing in this fashion we  obtain  the  set of LR(1)  tables given in  Fig. 
5.9  on  p.  374.  [[] 

In  Chapter  7 we  shall  discuss  a  number  of  other  methods  for  producing 
LR(k)  parsers  from  a  grammar.  These methods often produce parsers much 
smaller  than  the  canonical  LR(k)  parser.  However,  the  canonical  LR(k) 
parser  has  several outstanding  features,  and  these  will  be used as a yardstick 

by  which  other  LR(k)  parsers  will  be  evaluated.  We  will  mention  several 
features  concerning  the  behavior  of the  canonical  LR(k)  parsing  algorithm" 

(1)  A  simple  induction  on  the  number  of moves  made  shows  that  each 
table  on  the  pushdown  list is associated  with the  string of grammar  symbols 

to  its  left.  Thus  as  soon  as  the  first  k  input  symbols  of the  remaining  input 
are  such that  no possible suffix could yield a  sentence in L(G), the parser  will 
report error.  At all times the string of grammar symbols on the pushdown list 
must  be  a  viable  prefix  of  the  grammar.  Thus  an  LR(k)  parser  announces 

error  at  the  first  possible  opportunity  in  a  left  to  right  scan  of  the  input 
string. 

(2)  Let  Tj =  (~,  gj).  If f~(u)=  shift  and  the  parser  is  in  configuration 

(5.2.4)  (ToXaTaXzT z  . ..  XiT ~, x,  tO 
then  there  is  an  item  [B--, fl~  */~z, v] which  is valid for  XaX2  . ..  Xj,  with  u , 
in  EFF(flzv).  Thus  by  Theorem  5.9,  if  S' =~ XxX2  . ..  Xjuy  for  some y  in 

rm Y*,  then  the  right  end  of the  handle  of Xa  . . .   X~uy must  occur  somewhere 

to  the  right  of X~. 

sEc. 5.2  DETERMINISTIC  BOTTOM-UP  PARSING  395 

(3)  If  f j ( u ) =   reduce  i  in  configuration  (5.2.4)  and  production  i  is 
A ---~ YiY2  "'"  Yr,  then  the  string  Xj_r+iX'j_,+z  . . .   X 1 on  the  pushdown  list 
in  configuration  (5.2.4)  must  be  Y1  "'"  Yr,  since  the  set  of items  from  which 

table  T~  is  constructed  contains  the  item  [A  ----~ Y1Y2"'"  Y,  ", u].  Thus  in 
a  reduce  move  the  symbols  on  top  of  the  pushdown  list  do  not  need  to  be 
examined.  It  is  only  necessary  to  pop  2r  symbols  from  the  pushdown  list. 

(4)  If  f / u ) =   accept,  then  u  =  e.  The  pushdown  list  at  this  point  is 
ToST,  where  T  is the  LR(k)  table  associated  with  the  set  of  items  containing 
[ S ' - .   S  ., el. 

(5)  A  D PD T   with  an  endmarker  can  be  constructed  to  implement  the 
canonical  LR(k)  parsing  algorithm.  Once  we  realize  that  we  can  store 
the  lookahead  string  in  the  finite  control  of the  DPDT,  it  should  be  evident 

how  an  extended  D PD T   can  be  constructed  to  implement  Algorithm  5.7, 
the  LR(k)  parsing  algorithm. 

We  leave  the  proofs  of  these  observations  for  the  Exercises.  They  are 
essentially  restatements  of the  definitions  of valid  item  and  LR(k)  table.  We 
thus  have  the  following  theorem. 

THEOREM 5.12 

The  canonical  LR(k)  parsing  algorithm  correctly  produces  a  right  parse 
of its  input  if there  is one,  and  declares  "error"  otherwise. 

Proof.  Based on the  above  observations,  it follows  immediately  by induc- 
tion  on  the  number  of moves  made  by  the  parsing  algorithm  that  if e  is  the 

string  of grammar  symbols  on  its pushdown  list  and  x  the  unexpended  input 
including  the  lookahead  string,  then  otx :=~ w,  where  w is  the  original  input 

string  and  ~  is  the  current  output.  As  a  special  case,  if  it  accepts  w  and 
emits  output  nR,  then  S  ==~  w.  [Z] 

A  proof  of  the  unambiguity  of  an  LR  grammar  is  a  simple  application 
of the LR condition.  Given two distinct rightmost derivations  S ==~ el  =>  ..  * 

rill  rill =~  e, ==~ w  and  S  ==~ fl~  =~  . . .   ==~ tim :=~ W,  consider  the  smallest  i  such 

rm  rm  rm  rm  rm that  OCn-t ~  flm-r  A  violation  of the  LR(k)  definition  for  any k  is immediate. 

We leave details for the Exercises.  It follows  that the canonical  LR(k) parsing algorithm  for  an  LR(k)  grammar  G  produces  a  right  parse  for  an  input  w 
if and  only  if  w  ~  L(G). It may not  be  completely  obvious  at first that  the canonical  LR(k)  parser 

operates  in  linear  time,  even  when  the  elementary  operations  are  taken  to be  its  own  steps.  That  such  is  the  case  is  the  next  theorem. 

THEOREM  5.13 

The  number  of  steps  executed  by  the  canonical  LR(k)  parsing  algorithm 
in  parsing  an  input  of length  n  is  0(n). 

396  ONE-PASS  NO  BACKTRACK PARSING  CHAP.  5 

Pr oo f  Let  us  define  a  C-configuration  of the  parser  as  follows" 

(1)  An  initial  configuration  is  a  C-configuration. 
(2)  A  configuration  immediately  after  a  shift  move  is  a  C-configuration. 
(3)  A  configuration  immediately  after  a  reduction  which  makes  the  stack 

shorter  than  in  the  previous  C-configuration  is  a  C-configuration. 

In parsing  an  input  of length n  the  parser  can  enter  at  most  2n  C-configu- 
rations.  Let the  characteristic  of a  C-configuration  be  the  sum  of the  number 

of grammar  symbols  on  the  pushdown  list  plus  twice  the  number  of remain- 
ing  input  symbols.  If  C 1 and  C z  are  successive  C-configurations,  then  the 
characteristic  of  C1  is  at  least  one  more  than  the  characteristic  of  C2.  Since 
the  characteristic  of  the  initial  configuration  is  2n,  the  parser  can  enter  at 
most  2n  C-configurations. 

Now  it  suffices  to  show  that  there  is  a  constant  c such  that  the parser  can 
make  at  most  c  moves  between  successive  C-configurations.  To  prove  this 
let  us  simulate  the  LR(k)  parser  by  a  DPD A  which  keeps  the  pushdown  list 

of  the  algorithm  as  its  own  pushdown  list.  By  Theorem  2.22,  if  the  DPDA  
does  not  shift an  input  or  reduce  the  size of its  stack  in  a  constant  number  of 
moves,  then  it  is  in  a  loop.  Hence,  the  parsing  algorithm  is  also  in  a  loop. 

But  we have  observed  that  the  parsing  algorithm  detects  an  error  if there 
is  no  succeeding  input  that  completes  a  word  in  L(G).  Thus  there  is  some 
word  in  L(G)  with  arbitrarily  long  rightmost  derivations.  The  unambiguity 

of LR(k)  grammars  is  contradicted.  We  conclude  that  the  parsing  algorithm 
enters  no  loops  and  that  hence  the  constant  c  exists.  D 

5.2.6.  Implementation  of  LL(k)  and  LR(k)  Parsers 

Both  the  LL(k)  and  LR(k)  parser  implementations  seem  to  require  plac- 
ing  large  tables  on  the  pushdown  list.  Actually,  we  can  avoid  this  situation, 

as  follows: 

(1)  Make  one  copy  of each  possible  table  in memory.  Then,  on  the  push- 
down  list,  replace  the  tables  by  pointers  to  the  tables. 

(2)  Since  both  the  LL(k)  tables  and  LR(k)  tables  return  the  names  of 
other  tables,  we  can  use  pointers  to  the  tables  instead  of  names. 

We  note  that  the  grammar  symbols  are  actually  redundant  on  the  push- 
down  list  and  in  practice  would  not  be  written  there. 

E X E R C I S E S  
5.2.1.  Determine  which  of the  following  grammars  are  LR(1): 

(a)  Go. (b) 

S  --,  AB,  A  --.  OAlt e, B --~  1B[ 1. (c) 
S - - .   OSI l A,  A  --~  1All. 
(d)  S  ----~ S  +  A  I A,  A  ---~ (S) t a(S) l a. 

EXERCISES  397 
The  last grammar generates parenthesized expressions with  operator  + 
and  with  identifiers,  denoted  a,  possibly  singly  subscripted. 

5.2.2.  Which  of the grammars of Exercise  5.2.1  are  LR(0)? 
5.2.3.  Construct  the  sets of LR(1)  tables  for those  grammars of Exercise 5.2.1 

which  are  LR(1).  Do  not  forget to  augment  the  grammars first. 

5.2.4.  Give  the  sequence  of  moves  made  by  the  LR(1)  right  parser  for  Go 

with  input  (a  +  a) *  (a  +  (a  +  a ) .   a). 

* 5.2.5.  Prove  or  disprove  each  of the following: 

(a)  Every right-linear grammar is  LL. 
(b)  Every right-linear grammar is  LR. 

(c)  Every regular grammar is LL. 
(d)  Every regular grammar is  LR. 
(e)  Every regular set has  an  LL(1)  grammar. 

(f)  Every regular set has  an  LR(1)  grammar. 

(g)  Every regular set has  an  LR(0)  grammar. 

5.2.6.  Show  that  every LR  grammar is unambiguous. 
*5.2.7.  Let  G  =  (N,  E, P,  S),  and  define  GR  =  (N,  E, PR, S),  where  PR  is  P 

with  all  right  sides  reversed.  That  is,  PR  =  {A ~  0~RIA ~  t~ is  in P]. 

Give  an example to  show that  GR need  not  be  LR(k)  even though  G is. 

*5.2.8.  Let  G  =  (N, E, P,  S)  be  an  arbitrary  CFG.  Define  R~(i,  u),  for  u  in 

E,e  and  production  number  i,  to  be  {ocflulS ~  o~Aw ~  txflw,  where 

[m  rlTl u  =  FIRSTk(w)  and  production  i  is  A ~  fl}.  Show  that  R~(i,  u)  is 

regular. 
5.2.9.  Give  an  alternate  parsing  algorithm  for  LR(k)  grammars  by  keeping 

track  of  the  states  of  finite  automata  that  recognize  R~(i,  u)  for  the 

various  i and  u. 

'5.2.10.  Show  that  G is  LR(k)  if and  only  if for  all ~,  fl, u,  and  v,  ~  in  R~(i,  u) 

and  t~fl in  R~(j,  v)  implies  fl  =  e  and  i =   j. 

'5.2.11.  Show  that  G  is  LR(k)  if  and  only  if  G  is  unambiguous  and  for  all 

w, x, y,  and  z  in  E*,  the four  conditions  S  ~  way,  A  ~  x,  S  ~  wxz, 
and  FIRSTk(y)  =  FIRSTk(Z) imply that  S  ~  wAz. 
*'5.2.12.  Show  that  it  is  undecidable  whether  a  CFG  is  LR(k)  for  some k. 
*'5.2.13.  Show  that  it  is  undecidable  whether  an  LR(k)  grammar  is  an  LL 

grammar. 

5.2.14.  Show that  it is decidable whether an  LR(k) grammar is an LL(k)  gram- 

mar  for  the same value  of k. 

"5.2.15.  Show  that  every  e-free  CFL  is  generated  by  some  e-free  uniquely 

invertible CFG. 

'5.2.16.  Let  G  =  (N, E, P,  S)  be  a  CFG  grammar,  and  w =  al,  . ..   an  a  string 

in  E".  Suppose  that  when  applying  Earley's  algorithm  to  G,  we  find 

398  ONE-PASS  NO  BACKTRACK PARSING  CHAP.  5 

item  [A--~  0c-fl, j]  (in  the  sense  of  Earley's  algorithm)  on  list  Ii. . 

Show  that  there  is  a  derivation  S  ==~ yx  such  that  item  [A - ~   0c *  fl,  u] 

rm 

(in  the  LR  sense)  is  valid  for  y,  u  =  FIRSTk(x),  and  7  ~  a l  . ..   a~. 

"5.2.17.  Prove  the  converse  of Exercise  5.2.16. 

"5.2.18.  Use  Exercise  5.2.16  to  show  that  if  G  is  LR(k),  then  Earley's  algorithm 

with  k  symbol  lookahead  (see  Exercise  4.2.17)  takes  linear  time  and 
space. 

5.2.19.  Let  X  be  any  symbol.  Show  that  EFFk(Xa)  =  EFF~(X)  ~  FIRST~(a). 
5.2.20.  Use  Exercise  5.2.19  to  give  an  efficient  algorithm  to  compute  EFF(t~) 

for  any  0~. 

5.2.21.  Give  formal  details  to  show  that  cases  1  and  3  of  Theorem  5.9  yield 

violations  of the  LR(k)  condition. 

5.2.22.  Complete  the  proof  of Theorem  5.11. 
5.2.23.  Prove  the  correctness  of Algorithm  5.10. 
5.2.24.  Prove  the  correctness  of Algorithm  5.11  by  showing  each  of the  obser- 

vations  following  that  algorithm. 

In  Chapter  8 we shall  prove various  results  regarding  LR  grammars. 
The  reader  may  wish  to  try  his  hand  at  some  of  them  now  (Exercises 

5.2.25-5.2.28). 

* *5.2.25.  Show  that  every  LL(k)  grammar  is  an  LR(k)  grammar. 

* *5.2.26.  Show  that  every  deterministic  CFL  has  an  LR(1)  grammar. 

* 5.2.27.  Show  that  there  exist  grammars  which  are  (deterministically)  right- 

parsable  but  are  not  LR. 

* 5.2.28.  Show  that  there  exist  languages  which  are  LR  but  not  LL. 

* *5.2.29.  Show  that  every  LC(k)  grammar  is  an  LR(k)  grammar. 

* *5.2.30.  What  is  the maximum  number  of sets  of valid  items  an  LR(k)  grammar 

can  have as a  function  of the number of grammar  symbols, productions, 

and  the  length  of  the  longest  production ? 

"5.2.31.  Let  us  call  an  item  essential  if  it  has  its  dot  other  than  at  the  left  end 

[i.e.,  it  is  added  in  step  (2a)  of  Algorithm  5.8].  Show  that  other  than 
for  the  set  of items  associated  with  e,  and  for  reductions  of  the  empty 

string,  the  definition  of  the  LR(k)  table  associated  with  a  set  of  items 
could  have  restricted  attention  to  essential  items,  with  no  change in the 
table  constructed. 

* 5.2.32.  Show  that  the  action  of  an  LR(1)  table  on  symbol  a  is  shift  if  and 

only  if a  appears  immediately  to  the  right  of a  dot  in  some  item  in  the 
set  from  which  the  table  is  constructed. 

SEC. 5.3  PRECEDENCE GRAMMARS  399 
Programming  Exercises 

5.2.33,  Write  a  program  to  test  whether  an  arbitrary  grammar  is  LR(1).  Esti- 

mate how much  time and  space your program will require as a  function 

of the  size of the input  grammar. 

5.2.34.  Write a  program that uses an  LR(1) parsing table as in Fig.  5.9 to parse 

input  strings. 

5.2.35.  Write a  program that  generates an  LR(1) parser for an LR(1)  grammar. 

5.2.36.  Construct  an  LR(1)  parser  for  a  small  grammar. 
*5.2.37.  Write  a  program  that  tests  whether  an  arbitrary  set  of  LR(1)  tables 

forms  a  valid  parser for a  given CFG. 

Suppose  that  an  LR(1)  parser  is  in  the  configuration  (ocT, ax, 7t) 
and  that  the  parsing  action  associated  with  T  and  a  is  error.  As  in LL 
parsing,  at  this  point  we  would  like  to  announce  error  and  transfer  to 
an  error recovery routine that modifies  the input  and/or  the  pushdown 
list  so  that  the  LR(1)  parser  can  continue.  As  in  the  LL  case  we  can 

delete  the  input  symbol,  change  it,  or  insert  another  input  symbol 
depending  on  which  strategy  seems  most  promising  for  the  situation 
at  hand. 

Leinius  [1970]  describes  a  more  elaborate  strategy  in  which  LR(1) 
tabIes  stored in  the  pushdown  list  are  consulted. 

5.2.38.  Write an LR(1)  grammar for a  sma11 language. Devise an error recovery 

procedure  to  be  used  in  conjunction  with  an  LR(i)  parser  for  this 

grammar.  Evaluate  the  efficacy  of  your  procedure. 

B I B L I O G R A P H I C   N O T E S  
LR(k)  grammars were first defined by Knuth  [1965].  Unfortunately,  the method 
given  in  this  section  for  producing  an  LR  parser  will  result  in  very  large  parsers 
for  grammars  of  practical  interest.  In  Chapter  7  we  shall  investigate  techniques 

developed  by  De  Remer  [1969],  Korenjak  [1969],  and  Aho  and  Ullman  [i971], 
which  can  often  be  used  to  construct  much  smaller  LR  parsers. 

The  LR(k)  concept  has  also  been  extended  to  context-sensitive  grammars  by 

Walters  [1970]. The  answer to  Exercises  5.2.8-5.2.10  are given by Hopcroft  and  Ullman [1969]. 

Exercise 5.2.12 is from Knuth  [1965]. 

5.3.  PRECEDENCE  GRAMMA RS 

The  class  of shift-reduce  parsable grammars  includes the  LR(k)  gram mars 
and  various  subsets  of the  class  of  LR(k)  grammars.  In  this  section  we  shall 
precisely  define  a  shift-reduce  parsing  algorithm  and  consider  the  class  of 

400  ONE=PASS NO  BACKTRACK PARSING 

t  CHAP.  5 

precedence  grammars,  an  important  class  of grammars  which  can  be  parsed 
by  an  easily  implemented  shift-reduce  parsing  algorithm. 

5.3.1.  Formal  Shift-Reduce  Parsing  Algorithms 
DEHNITION 

Let  G  =  (N,  ~, P,  S)  be  a  C FG   in  which  the  productions  have  been 
numbered  from  I  to  p.  A  shift-reduce  parsing  algorithm  for  G  is  a  pair  of 
functions  (2 =  (f,  g),tt  where f  is  called  the  shift-reduce  function  and  g  the reduce function. 

These  functions  are  defined  as  follows: 

(1)  f  maps  V*  x  (~  U  [$})*  to  {shift,  reduce,  error,  accept],  where 
V =  N  U  ~  u  {$}, and  $  is  a  new  symbol,  the  endmarker. 

(2)  g  maps  V*  x  (E  u  [$})*  to  {1, 2 . . . .   , p,  error],  under  the  constraint 
that  if g(a,  w)  =  i,  then  the  right  side  of production  i  is  a  suffix  of  a. 

A  shift-reduce  parsing  algorithm  uses  a  left-to-right  input  scan  and 
a  pushdown  list.  The  function f  decides  on  the  basis  of what  is  on  the  push- 
down  list  and  what  remains  on  the  input  tape  whether  to  shift  the  current 
input  symbol  onto  the  pushdown  list  or  call  for  a  reduction.  If a  reduction 
is called for,  then  the function g  is invoked to  decide what  reduction  to make. 

We  can  view  the  action  of  a  shift-reduce  parsing  algorithm  in  terms  of 
configurations  which  are  triples  of the  form 

($Xi  "'"  Xm, al  " "   a.$,pl  "'"  P.) 
where 

(1)  $X1  " "   Xm  represents  the  pushdown  list,  with  Xm  on  top.  Each  X't 
is  in  N  W E,  and  $  acts  as  a  bottom  of the  pushdown  list  marker. 

(2)  al  . "   a,  is  the  remaining  portion  of the  original  input,  a 1 is  the  cur- 
rent  input  symbol,  and  $  acts  as  a  right  endmarker  for  the  input. 

(3)  p l  . . .   Pr is the  string  of production  numbers  used to  reduce  the  origi- 
nal  input  to  X~  . . .   Xma~  " "   a,. 

We  can  describe  the  action  of (2 by two  relations,  t--a- and  !-c-, on  configu- 
rations.  (The  subscript  (2  will  be  dropped  whenever  possible.) 

(1)  If f(a,   aw)  =  shift,  then  (a,  aw,  r~)p-  (aa,  w,  z~) for  all  a  in  V*,  w in 
(E  U  {$})*,  and  n  in  { 1 , . . . , p ] * .  

(2)  If f(afl,  w)  =  reduce,  g(o~fl,  w)  =  i,  and  production  i is  A  ~  fl,  then 
(aft,  w, n:) ~  (aA,  w,  n:i). 

(3)  If f(a,   w)  =  accept,  then  (a,  w, n ) ~   accept. 
(4)  Otherwise,  (a,  w, 70  ~  error. 

* lThese functions  are not the functions  associated with an LR(k) table. 

SEC. 5.3  PRECEDENCE  GRAMMARS  401 

We  define  1-  to  be the  union  of ~  and  ~--.. We  then  define  ~  and  ~  to 
have  their  usual  meanings. 

We  define  ~(w)  for  w  ~  ~*  to  be  n  if  ($,  w$,  e)  t----($S,  $,  re)  1--- accept, 
and  ~(w)  =  error  if no  such  n  exists. 

We  say  that  the  shift-reduce  parsing  algorithm  is  valid for  G  if 

(1)  L(G) =  [wl ~2(w) ~  error},  and 
(2)  If  ~(w)  =  zc, then  ~  is  a  right  parse  of  w. 

Example 5.33 

Let  us  construct  a  shift-reduce  parsing  algorithm  ~  =  (f, g)  for  the 
grammar  G  with  productions 

(i)  S-~-  > SaSb 

(2)  S  r e  

The  shift-reduce  function  f  is  specified  as  follows:  For  all  ~  ~  V*  and 
x  ~  (~:  u  [$})*, 

(1)  f(o~S, ex)  =  shift  if  c  ~  {a,  b}. 

(2)  f(ow, dx)  =  reduce  if e  ~  {a, b} and  d  ~  {a, b}. (3)  f  ( $ , ax) 

=  reduce. 
(4)  f($,  bx)  =  error. 

(5)  f(~zX,  $ ) =   error  for  X  ~  IS,  a}. 

(6)  f(~b,  $)  =  reduce. 
(7)  f($S,  $)  =  accept. 
(8)  f($,  $)  =  error. 

The  reduce  function  g  is  as  follows.  For  all  ~  ~  V*  and  x  ~  (~  U  [$})*, 

(1)  g($,  ax)  =  2 

(2)  g(oca, cx) =  2  for  c  ~  {a, b}. 

(3)  g($SaSb,  cx)=  1 for  c  ~  {a,  $}. 
(4)  g(ocaSaSb, cx)=  1 for  c  ~  {a, b}. 

(5)  Otherwise,  g(~,  x)  =  error. 

Let  us  parse  the  input  string  aabb using  ~.  The  parsing  algorithm  starts  off 
in  the  initial  configuration 

($,  aabb $, e) 
The  first  move  is  determined  by  f ($,  aabb$), which  we  see,  from  the  speci- 
fication  o ff ,  is  reduce.  To  determine  the  reduction  we  consult  g($,  aabb$), 
which  we  find  is  2.  Thus  the  first  move  is  the  reduction 

($, aabb$, e) ~  ($S, aabb$, 2) 

402  ONE-PASS NO  BACKTRACK PARSING  CrIAP. 5 

The  next  move is  determined  by  f($S,  aabb$), which  is  shift.  Thus  the  next 
move is 

($S, aabb$, 2) ~  ($Sa, abb$, 2) 

Continuing in this fashion,  the shift-reduce parsing algorithm e  would make 
the  following  sequence  of moves: 

($, aabb$, e) ~  ($S, aabb$, 2) 

~-  ( $Sa, abb $, 2) 
~-- ($SaS, abb$, 22) 

($SaSa, bb$, 22) 
($SaSaS, bb $, 222) 
.L  ($SaSaSb, b$, 222) 
_r_ ($SaS, b$, 2221) 

($SaSb,  $, 2221) 
.z_ ($S,  $, 22211) 

accept 

Thus,  e(aabb)  =  22211.  Clearly,  22211 is  the  right  parse  of aabb.  D 

In  practice,  we do  not  want  to  look  at  the  entire  string in the  pushdown 
list  and  all  of the  remaining  input  string  to  determine  what  the  next  move 

of a parsing algorithm  should be.  Usually, we want the  shift-reduce  function 
to  depend  only  on  the  top  few  symbols  on  the  pushdown  list  and  the  next 
few  input  symbols.  Likewise,  we  would  like  the  reduce  function  to  depend 

on  only  one  or  two  symbols  below  the  left  end  of the  handle  and  on  only 

one  or two  of the  next input  symbols. 

In  the  previous  example,  in  fact,  we  notice  that f  depends  only  on  the 
symbol  on  top  of the  pushdown  list  and  the  next  input  symbol.  The  reduce 
function g  depends  only on  one  symbol below  the  handle  and the  next input 

symbol. 

The  LR(k)  parser  that  we  constructed  in  the  previous  section  can  be 
viewed as a "shift-reduce parsing algorithm" in which the pushdown alphabet 
is augmented with  LR(k)  tables.  Treating the  LR(k)  parser  as  a  shift-reduce 
parsing  algorithm,  the  shift-reduce  function  depends  only  on  the  symbol 

on  top  of the  pushdown  list  [the  current  LR(k)  table]  and  the  next  k  input 

symbols.  The  reduce  function  depends  on  the  table  immediately  below  the 
handle  on  the  pushdown  list  and  on  zero  input  symbols.  However,  in  our 

present  formalism,  we  may  have  to  look  at  the  entire  contents  of the  stack 

in  order  to  determine  what  the  top  table  is.  The  algorithms  to  be  discussed 

SEC. 5.3  PRECEDENCE GRAMMARS  403 

subsequently  in  this  chapter  will  need  only  information  near  the  top  of the 
stack.  We  therefore  adopt  the  following  convention. 

CONVENTION 

I f f   and g  are  functions  of a  shift-reduce  parsing  algorithm  and  f(0~, w) 
is  defined,  then  we  assume  that  f(floc,  wx)  =  f(oc,  w)  for  all  fl  and  x,  unless 

otherwise  stated.  The  analogous  statement  applies  to g. 

5.3.2,  Simple  Precedence  Grammars 

The  simplest  class  of  shift-reduce  algorithms  are  based  on  "precedence 
relations."  In a  precedence grammar the  boundaries  of the  handle  of a  right- 

sentential  form  can  be  located  by  consulting  certain  (precedence)  relations 
that  hold  among  symbols  appearing  in  right-sentential  forms. 

Precedence-oriented  parsing  techniques  were  among  the  first  techniques 
to  be  used  in  the  construction  of  parsers  for  programming  languages  and 

a  number  of  variants  of  precedence  grammars  have  appeared  in  the 
literature.  We  shall  discuss  left-to-right  deterministic  precedence  parsing  in 
which  a  right  parse  is  to  be produced.  In  this  discussion  we  shall  introduce 
the  following  types  of precedence  grammars: 

(1)  Simple precedence. 
(2)  Extended  precedence. 
(3)  Weak  precedence. 
(4)  Mixed  strategy precedence. 
(5)  Operator  precedence. 

The  key  to  precedence  parsing  is  the  definition  of a  precedence  relation 
3>  between  grammar  symbols  such  that  scanning  from  left  to  right  a  right- 
sentential  form  ocflw, of which  fl is  the  handle,  the  precedence  relation  3>  is 
first  found  to  hold  between the  last  symbol  of fl  and  the  first  symbol  of w. 

If we  use  a  shift-reduce  parsing  algorithm,  then  the  decision  to  reduce 
will  occur whenever the precedence  relation  3> holds  between what is on top 

of the  pushdown  list and  the  first remaining  input  symbol.  If the  relation  3> 

does  not  hold,  then  a  shift  may be  called for. 

Thus  the  relation  .>  is used to  locate  the  right  end of a handle  in a right- 
sentential  form.  Location  of the  left end  of the  handle  and  determination  of 
the exact reduction  to be made is done  in one of  several ways, depending  on 
the  type  of precedence  being  used. 

The  so-called  "simple  precedence"  parsing  technique  uses  three  prece- 
dence relations  4 ,   -~-, and  3> to isolate  the handle  in a right-sentential  form ocflw. 

If fl is the handle,  then the relation  ~  or ~-- is to  hold between all pairs of  symbols  in  0~,  <~  is  to  hold  between  the  last  symbol  of  0c and  the  first 

symbol  of fl,  ~  is  to  hold  between  all  pairs  of symbols  in  the  handle  itself, 
and  the  relation  3>  is  to  hold  between  the  last  symbol  of  fl  and  the  first 
symbol  of w. 

404  ONE=PASS  NO  BACKTRACK  PARSING  CHAP. 5 

Thus the handle  of a right-sentential form of a simple precedence grammar 
can  be  located  by  scanning  the  sentential  form  from  left  to  right  until  the 
precedence  relation  >  is  first  encountered.  The  left  end  of  the  handle  is 
located  by  scanning  backward  until  the  precedence  relation  .~  holds.  The 

handle  is  the  string  between  <  and  3>.  If  we  assume  that  the  grammar  is 
uniquely  invertible,  then  the  handle  can  be  uniquely  reduced.  This  process 
can be repeated until the input  string is either reduced  to the  sentence  symbol 

or  no  further  reductions  are  possible. 

DEHNITION 

The  Wirth-Weber  precedence  relations  <~,  ~-,  and  3>  for  a  CFG  G  = 
(N,  ~, P,  S)  are  defined  on  N  u  ~  as  follows: 

(t)  We say that  X  . ~ Y   if there exists A  ---~ ~XBfl in P  such that B  ~  YT'. 
(2)  We  say  that  X-~-  Y if there  exists  A  --~ ~XYfl  in P. 
(3)  3>  is  defined  on  (N  U  E ) x   ~E,  since  the  symbol  immediately  to 
the  right  of a  handle  in  a  right-sentential  form  is  always  a  terminal.  We  say 
that  X  3> a  if  A  ---~ ~BYfl  is  in  P,  B ~  7,X, and  Y *~  a~.  Notice  that  Y 
will  be  a  in  the  case  Y ~  ad~. 

In  precedence  parsing  procedures  we  shall find it convenient  to  add  a  left 
and  right  endmarker  to  the  input  string.  We  shall  use  $  as  this  endmarker 

+ and  we  assume  that  $  .~  X  for  all  X  such  that  S  ==~ X~  and  Y 3>  $  for  all 

Y such  that  S  ~  ~ Y. 

The  calculation  of  Wirth-Weber  precedence  relations  is  not  hard.  We 
leave it  to  the  reader  to  devise  an  algorithm,  or  he  may  use  the  algorithm  to 
calculate  extended  precedence  relations  given  in  Section  5.3.3. 

DEFINITION 

A  CFG  G  =  (N,  E, P,  S)  which  is  proper, t  which  has  no  e-productions, 
and  in  which  at  most  one  Wirth-Weber  precedence  relation  exists  between 
any  pair  of  symbols  in  N  u  ~  is  called  a  precedence  grammar.  A  prece- 

dence  grammar  which  is  uniquely  invertible  is  called  a  simple  precedence 
grammar. 

By  our  usual  convention,  we  define  the  language  generated  by  a  (simple) 
precedence  grammar  to  be  a  (simple) precedence  language. 

Example 5.34 

Let  G  have  the  productions 

S  > aSSblc 

?Recall that a CFG G is proper if there is no derivation of the form A ~  A, if G has no  useless symbols, and  if there  are  no  e-productions except possibly S ~  e  in  which 
case S does not appear on the right side of any production. 

SEC. 5.3  PRECEDENCE  GRAMMARS  405 

The precedence  relations  for  G,  together with the  added  precedence  relations 
involving  the  endmarkers,  are  shown  in  the  precedence  matrix  of  Fig.  5.11. 

Each  entry  gives  the  precedence  relations  that  hold  between  the  symbol 
labeling  the  row  and  the  symbol  labeling  the  column.  Blank  entries  are 

interpreted  as  error. 

a  b  c 

*  <.  *  <. 

<.  ~  <. 

* >  .>  .>  .> 

* >  .>  .>  .> 
<.  <.  Fig. 5.11  Precedence relations. 

The  following  technique  is  a  systematic  approach  to  the  construction  of 
the  precedence  relations.  First,  "  is  easy  to  compute.  We  scan  the  right 

sides  of the  productions  and  find that  a  "  S,  S  "  S,  and  S  "  b. 

To  compute  <Z,  we  scan  the  right  sides  of  the  productions  for  adjacent 
pairs  XC.  Then  X is related  by  ~z  to  any leftmost  symbol of a  string  derived 
nontrivially  from  C.  (We  leave  it  to  the  reader  to  give  an  algorithm  to  find 

all  such  symbols.) In  our  example,  we  consider  the  pairs aS and  SS.  In  each 
case,  C  can  be  identified  with  S,  and  S  derives  strings  beginning  with  a  and 

c.  Thus,  X  ~  Y,  where  X  is  any  of a  or  S  and  Y is  any  of  a  or  e. 

To  compute  .>,  we  again  consider  adjacent  pairs  in  the  right  sides,  this 
time  of the  form  CX.  We  find  those  symbols  Y that  can  appear  at  the  end 
of a  string  derived  in one  or  more  steps  from  C and  those  terminals  d  at  the 
beginning  of  a  string  derived  in  zero  or  more  steps  from  X.  If  X  is  itself 
a  terminal,  then  X - - d   is  the  only  possibility.  Here,  SS  and  Sb  are  sub- 

strings  of this  form.  Y is  b  or  e,  and  d  is  a  or c in  the  first  case  and  b  in  the 
second. 

It  should  be  emphasized  that  ~----, ~z,  and  ->  do  not  have  the  properties 
normally  ascribed  to  = ,   < ,   and  >  on  the  reals,  integers,  etc.  For  example, 
"--  is  not  usually an equivalence  relation;  ~Z and  .>  are not  normally transi- 

tive,  and  they  may be  symmetric or  reflexive. 

Since  there  is  at  most  one  precedence  relation  in  each  entry  of Fig.  5.1 i, 
G  is  a  precedence  grammar.  Moreover,  all  productions  in  G  have  unique 
right  sides,  so  that  G  is  a  simple  precedence  grammar,  and  L(G)  is  a  simple 

precedence  language. 

Let us consider  $accb$, a right-sentential form of G delimited by endmark- 

406  ONE-PASS NO BACKTRACK PARSING  CHAP.  5 

ers.  We  have  $  <  a,  a  <  c,  and  c  3>  c.  The  handle  of  accb  is  the  first  c,  so 
the  precedence  relations  have  isolated  this  handle. 

We  can  often  represent  the  relevant  information  in  an  n  *  n  precedence 
matrix  by  two  vectors  of dimension  n.  We  shall  discuss  such  representations 

of precedence  matrices  in  Section  7.1. 

The  following  theorem  shows  that  the  precedence  relation  ~  occurs  at 
the beginning of a handle in a right-sentential form, ~  holds between adjacent 

symbols  of a  handle,  and  .>  holds  at  the  right  end  of  a  handle.  This  is  true 
for  all grammars  with  no  e-productions,  but  it is  only  in  a  precedence  gram- 

mar that there is at most  one precedence  relation between  any pair  of symbols 
in  a  viable  prefix  of a  right-sentential  form. 

First  we  shall  show  a  consequence  of  a  precedence  relation  holding 
between  two  symbols. 

LEMMA  5.3 

Let  G  =  (N,  E, P,  S)  be  a  proper  C F G   with  no  e-productions. 
(1)  I f X < A o r X   "  A a n d A ~ Y a i s i n P ,   t h e n X - <   Y. 

(2)  If A  <  a, A  "  a, or A  ->  a  and  A  ~  a  Yis a  production,  then  Y 3>  a. 

Proof  We  leave  (1)  for  the  Exercises  and  prove  (2).  If A  <  a,  then  there 
is  a  right  side  fllABfl2  such  that  B  ~  a?  for  some  7.  Since  A  ~  0~ Y,  Y ->  a 

is  immediate.  If A  "  a,  there  is  a  right  side  flIAafl2.  As  we  have  a  ==~ a  and 
A  ~  0~ Y,  it  follows  that  Y .2> a  again.  If  A  .>  a,  then  there  is  a  right  side 

flxBXfl2,  where  B  =*  ?A  and  X  *~  aO  for  some  7  and  ~.  Since  B  ~  ?ix Y, 
we  again  have  the  desired  conclusion.  [Z] 

THEOREM 5.14 

Let  G  =  (N,  E, P,  S)  be  a  proper  C F G   with  no  e-productions.  If 

ss$ @  ...  x  +  Aa,  ... 

XpXp_~  . . .   Xk+aX~,'"  X~a~ . . .   aq 

then 

(1)  For p  <  i  <  k,  either  X~+~ <  X~ or  Xt+~ "--- X~; 
(2)  Xk+a <~Xk; 
(3)  For  k  >  i ;>  1,  Xt+~ --" X~;  and 
(4)  X~  >  a~. 

Proof.  The  proof  will  proceed  by  induction  on  n.  For  n  =  0,  we  have 
$S$  ~  $Xk  . . .   Xa$.  From  the  definition  of  the  precedence  relations  we 

SEC. 5.3  PRECEDENCE GRAMMARS  407 
have  $ < X k ,   X~+,  "  X ~ f o r k > i ~ I   a n d X ,   >  $.  Note  that  X k . . . X   1 

cannot  be  the  empty  string,  since G is  assumed  to  be  free  of e-productions. 

For  the  inductive  step  suppose  that  the  statement  of the  theorem  is  true for  n.  Now  consider  a  derivation' 

n $S$~X,,...Xk+IAa,...a q 

Xp  .. .   Xk+lX1,  . . .  Xaal  .. .   a q 

Xp  . . .   X j + , Y ,   . . .   Y i X j _ ,   . . .   X1a I  ..  aq 

That  is,  Xj  is replaced  by  Yr  "'"  Y1  at  the  last  step.  Thus,  X j _ i , . . . ,   X,  are terminals;  the  case j  =  1 is  not  ruled  out. 

By the  inductive  hypothesis,  Xj+ a  ~  X j   or  Xj+ 1 "--  Xj.  Thus,  Xj+ ,  <;.  Y,. 
by Lemma  5.3(1).  Also,  Xj  is related by one  of the three relations to the  sym- 
bol  on  its  right  (which  may  be  a,).  Thus,  Ya  "> Xj_ 1,  or  Y1  .2> al  if j - -   1. 
We  have  Yr  "  Yr-a  "  "'"  "2--Yi,  since  Yr'"   Y1  is  a  right  side.  Finally, Xi+ 1 <  Xe or  X;+ 1  "  X; follows  by  the  inductive  hypothesis,  for  p  <  i  <  j. 

Thus  the  induction  is  complete. 
COROLLARY  1 

If  G  is  a  precedence  grammar,  then  conclusion  (1)  of Theorem  5.14  can 
be  strengthened  by  adding  "exactly  one  of  <  and  ~_."  Conclusions  (1)-(4) 
can  be  strengthened  by  appending  "and  no  other  relations  hold." 

Proof.  Immediate  from  the  definition  of a  precedence  grammar.  [Z] 

COROLLARY 2 

Every  simple  precedence  grammar  is  unambiguous. 
Proof.  All  we  need  to  do  is  observe  that  for  any  right-sentential  form  fl, 
other  than  S, the previous right-sentential  form a  such that  a  r~m fl is unique. 
From  Corollary  1 we know  that  the  handle  of fl can  be  uniquely  determined 
by  scanning  fl  surrounded  by  endmarkers  from  left  to  right  until  the  first  3> 
relation  is found,  and  then  scanning  back  until  a  <~ relation  is  encountered. 

The handle lies between these points.  Because a simple precedence  grammar is 
uniquely  invertible,  the  nonterminal  to  which  the  handle  is  to  be  reduced  is 

unique.  Thus,  ~  can  be  uniquely  found  from  ft.  D 

We  note  that  since  we  are  dealing  only  with  proper  grammars,  the  fact 
that  this  and  subsequent  parsing  algorithms  operate  in  linear  time  is  not 
difficult  to  prove.  The  proofs  are  left  for  the  Exercises. 

We shall now describe how a deterministic right parser can be constructed for  a  simple  precedence  grammar. 

408  ONE-PASS  NO  BACKTRACK PARSING  CI-IAP. 5 

ALGORITHM 5.12 

Shift-reduce  parsing  algorithm  for  a  simple  precedence  grammar. 
Input.  A  simple  precedence  grammar  G  =  (N,  X, P,  S)  in  which  the 
productions  in P  are  numbered  from  1 to p. 

Output.  CZ =  (f,  g),  a  shift-reduce  parsing  algorithm. 

Method. 

(1)  The  shift-reduce  parsing  algorithm  will employ  $ as  a  bottom  marker 
for  the  pushdown  list  and  a  right  endmarker  for  the  input. (2)  The  shift-reduce  function  f  will  be  independent  of  the  contents  of 

the  pushdown  list  except  for  the  topmost  symbol  and  independent  of  the 
remaining  input  except  for  the  leftmost  input  symbol.  Thus  we  shall  define 
f  only  on  (N  u  X  U  {$}) x  (X  u  [$}),  except  in  one  case  (rule  c). 

(a)  f(X, a)  =  shift  if  X  <  a  or  X--~  a. 

(b)  f ( X ,   a)  =  reduce  if  X  ->  a. 

(c)  f($S,  $)  =  accept.l" 

(d)  f ( X ,   a)  =  error  otherwise. (These  rules  can  be  implemented  by consulting  the  precedence  matrix  itself.) 

(3)  The  reduce  function  g  depends  only  on  the  string  on  top  of the  push- 
down  list  up  to  one  symbol  below  the  handle.  The  remaining  input  does  not 

affect  g.  Thus  we  define  g  only  on  (N  U  X  t,.) { $})*  as  follows: (a)  g(X~+lXkXk_l  "'"  XI,  e)  =  i if Xk+~ <  Xk,  Xj+~  "  Xj  for  k  >  j 

>  1,  and  production  i  is  A  ~  X k X k - ~ ' ' "   Xi.  (Note  that  the 
reduce  function  g  is  only  invoked  when  X~  3>  a,  where  a  is  the 

current  input  symbol.) 
(b)  g(~z, e)  =  error,  otherwise.  [Z] 

Example 5.35 

Let us construct a shift-reduce  parsing algorithm ~  =  (f,  g) for the gram- 
mar  G  with  productions 

(1)  S  > aSSb 

(2)  S  ~  c 

The  precedence  relations  for  G  are  given  in  Fig.  5.11  on  p.  405.  We  can 

use  the  precedence  matrix  itself  for  the  shift-reduce  function f.  The  reduce 

function  g  is  as  follows" 

(1)  g(XaSSb)  =  1 if  X  ~  IS,  a,  $]. 

(2)  g(Xc)  =  2  if  X  ~  [S,  a,  $}. 

(3)  g(00  =  error,  otherwise. 

tNote that this rule may take priority over rules (2a) and (2b) when X =  S and a =  $. 

SEC. 5.3  PRECEDENCE  GRAMMARS  409 

With  input  accb,  c~ would  make  the  following  sequence  of moves" 

($, accb $, e)  ~  ($a,  ccb $, e) 

(Sac,  cb $, e) 
_r_ ( $aS,  cb $, 2) 

($aSc,  b$,  2) 
._r_ ( $aSS,  b $, 22) 

($aSSb,  $, 22) 
.z_ ($S, $, 221) 

In  configuration  (Sac,  cb$,  e),  for  example,  we  have  f(c,  b ) =   reduce  and g(ac,  e)  =  2.  Thus 

(Sac,  cb $, e)  R  ( $aS,  cb $, 2) 
Let  us  examine  the  behavior  of  a  on  aeb,  an  input  not  in  L(G).  With acb 

as  input  a  would  make  the  following moves: 

($, acb $, e) ~  ( $a,  cb $, e) 

~2_ (Sac,  b $, e) 

[--($aS,  b$,2) 12- (SaSh,  $, 2) 

R  error 
In configuration  ($aSb, $, 2), f(b,  $) =  reduce.  Since $ <  a  and  a-~- S ~  b, 
we  can  make  a  reduction  only  if  aSb  is  the  right  side  of  some  production. 

However,  no  such  production  exists,  so g(aSb,  e)  =  error. 

In  practice  we  might  keep  a  list  of  "error  productions."  Whenever  an 
error is encountered  by the g  function,  we could  then  consult the  list of error 
productions  to see if a reduction by an  error production  can  be  made.  Other 
precedence-oriented  error  recovery  techniques  are  discussed  in  the  biblio- 
graphical  notes  at  the  end  of this  section.  D 

THEOREM 5.15 

Algorithm  5.12  constructs  a  valid  shift-reduce  parsing  algorithm  for 
a  simple  precedence  grammar. 

Proof.  The  proof  is  a  straightforward  consequence  of  Theorem  5.14, 
the  unique  invertibility  property,  and  the  construction  in  Algorithm  5.12. 
The  details  are  left  for  the  Exercises.  El 

410  ONE-PASS NO BACKTRACK PARSING  CHAP.  5 

It is interesting to consider the classes of languages which can be generated 
by  precedence  grammars  and  simple  precedence  grammars.  We  discover 
that  every CFL without  e has  a precedence  grammar  but  that  not  every CFL 
without  e  has  a  simple  precedence  grammar.  Moreover,  for  every  CFL 
without  e  we  can  find  an  e-free  uniquely  invertible  CFG.  Thus,  insisting 
that  grammars  be  both  precedence  and  uniquely  invertible  diminishes  their 

language-generating  capability.  Every  simple  precedence  grammar  is  an 

LR(1)  grammar,  but  the  LR(1)  language  (a0'l'l  i ~  1} w  {b0tl2'l i ~  1}  has 

no  simple  precedence  grammar,  as  we  shall  see  in  Section  8.3. 

5.3.3.  Extended  Precedence  Grammars 

It  is  possible  to  extend  the  definition  of  the  Wirth-Weber  precedence 
relations  to pairs  of strings rather than pairs  of symbols.  We shall give a  defi- 
nition  of  extended  precedence  relations  that  relate  strings  of  m  symbols  to 

strings  of n  symbols.  Our  definition  is  designed  with  shift-reduce  parsing  in 
mind. 

Understanding  the  motivation  of  extended  precedence  requires  that  we 
recall  the  two  roles  of  precedence  relations  in  a  shift-reduce  parsing 

algorithm" 

(1)  Let ~X be the m  symbols on top  of the pushdown  list (with  X  on top) 
and  aw  the  next  n  input  symbols.  If  ~X  <  aw  or  ~ X  "  aw,  then  a  is  to  be 

shifted  on  top  of the  pushdown  list.  If  ~X  3>  aw,  then  a  reduction  is  to  be 
made. 

(2)  Suppose that X~,  . ..   X 2Xi   is the  string on the pushdown list and  that 
a 1 . . .   a~  is  the  remaining  input  string  when  a  reduction  is  called  for  (i.e., 
Xm  "-"  Xi  3> a 1 . . .   a,).  If the  handle  is  Xk  "'"  X1,  we then  want 

for  k > j ~ l   and 

Xm+k' '"  Xk+l  <  Xk  ''"   Xl al   ""  a._k.'t 
Thus  parsing  according  to  a  uniquely  invertible  extended  precedence 
grammar is similar to parsing according to a  simple Wirth--Weber precedence 

grammar,  except  that  the  precedence  relation  between  a  pair  of  symbols  X 

and  Y is  determined  by  ~X  and  Yfl,  where  ~  is  the  m  --  1  symbols  to  the 
left  of X  and  fl is the n  --  1 symbols to  the right  of  Y. 

We  keep  shifting  symbols  onto  the  pushdown  list  until  a  3>  relation  is 
encountered  between the  string  on  top  of the  pushdown  list  and  the  remain- 

ing  input.  We  then  scan  back  into  the  pushdown  list  over  "  relations  until 

"tWo assume that  X, Xr-t  ...  Xlal  ...  a,_,.  is XrX,-1  ...  X,-n+l  if r ~  n. 

SEC. 5.3  PRECEDENCE GRAMMARS  4'1 1 
the  first  .~  relation  is  encountered.  The  handle  lies  between  the  <~  and  3> 
relations. 

This  discussion  motivates  the  following  definition. 

DEFINITION 

Let  G  =  (N,  E,  P,  S)  be  a  proper  C FG   with  no  e-production.  We 
define  the  (m, n)  precedence  relations  4 ,   ~ ,   and  3>  on  (N  U  E  U  { $})m * 

(N  U  ~  U  {$})" as  follows"  Let 

$r"s$"  ~  X pX p_   1  . ..  Xk+ aAa ~  . ..  aq 

rm 

X~,X~,_ i  " " "  Xk + l Xk  " " "  X l a i  .  .  .  aq 

be  any  rightmost  derivation.  Then, 

(1)  0c .~  fl  if  0c consists  of  the  last  m  symbols  of  XpXp_~  . . .   Xk+~,  and 
either 

(a)  fl  consists  of  the  first  n  symbols  of  Xk  " "   X~a~  ...   aq,  or 
(b)  X k  is  a  terminal  and  fl  is  in  FIRST,(Xk  * * * X~a~  . ..  aq). 
(2)  0c ~-  fl  for  all j,  k  >  j  ~  1,  such  that  0c consists  of the  last  m  symbols 
of X pX p_   ~  .. .  Xj+ ~,  and  either 

(a)  p  consists  of  the  first  n  symbols  of  X)Xj_~  . . .   X~a~  .. .  aq,  or 
(b)  X i  is  a  terminal  and  fl  is  in  FIRST,(XjXj_~  . . .   X ~a ~. ..  aq). 
(3)  X~Xm_~  ...   X~  3>  a~  ...   a,, 

We  say that  G is an  (m, n) pr ece den ce   gr am ma r  if G is a  proper  C FG   with 
no  e-production  and  the  relations  ~ ,   __~__, and  3>  are  pairwise  disjoint.  It 

should  be  clear from  Lemma  5.3  that  G is a  precedence  grammar  if and  only 
if  G  is  a  (1,  i)  precedence  grammar.  The  details  concerning  endmarkers  are 
easy to  handle.  Whenever  n  =  1, conditions  (lb)  and  (2b)  yield nothing  new. 

We also comment that the disjointness  of the portions  of <~ and  "  arising 
solely  from  definitions  (1 b) and  (2b)  do  not  really  affect  our  ability to  find  a 
shift-reduce  parsing  algorithm  for extended  precedence grammars.  We could 
have  given  a  more  complicated  but  less  restrictive  definition,  and  leave  the 

development  of such  a  class  of grammars  for  the  Exercises. 

We  shall give an  algorithm  to  compute  the  extended precedence  relations. 
It  is  clearly  applicable  to  Wirth-Weber  precedence  relations  also. 

ALGORITHM  5.13 

Construction  of (m, n)  precedence  relations. 
Input.  A  proper  C F G   G  =  (N,  E, P,  S)  with  no  e-production. 

Output.  The  (m, n)  precedence  relations  <~,  ~--, and  3>  for  G. 
M eth od .  We  begin  by  constructing  the  set  $  of  all  substrings  of  length 
rn  -+  n  that  can  appear  in  a  string  0q~u such  that  $mS$" ~  ~A w   =*.  ~ flw   and rm  rrn 

412  ONE-PASS NO  BACKTRACK  PARSING  CHAP. 5 

u  =  FIRST,(w).  The  following  steps  do  the  job" 

(1)  Let  S--{$ms$"-~,  $m-~s$"} *  The  two  strings  in  S  have  n o t   been 
"considered." (2)  If  ~  is  an  unconsidered  string  in  $,  "consider"  it  by  performing  the 

following  two  operations. (a)  If d~ is  not  of the  form  aAx,  where  Ix[  _< n,  do  nothing. 

(b)  If 5  =  aAx,  l xl  _< n,  and  A  ~  N,  add  to  $,  if not  already  there, 

those  strings  ?  such  that  there  exists  A  ----~ fl  in  P  and  ?  is  a  sub- 

string  of  length  m  -+- n  of  aflx.  Note  that  since  G  is  proper,  we 
have  l~Pxl>_ m q-- n.  New  strings  added  to  S  are  not  yet  consid- 
ered. (3)  Repeat  step  (2)  until  no  string  in  $  remains  unconsidered. 

From  set  S,  we  construct  the  relations  < ,   -~-, and  ->,  as  follows: 
(4)  For  each  string  aAw in  $  such  that  I *  I =  m  and  for  each  A  ---~ fl in P, 
let  a  <  6,  where  5  is  the  first  n  symbols  of  flw or  fl  begins  with  a  terminal 

and  d~ is  in  FIRST,(flw). (5)  For  each  string  aA  in  g  such  that  t~1= m  and  for  each  production 

A  ~  fllXYfl2  in  P,  let  ~1  ~  ~2,  where  51  is  the  last  m  symbols  of  affiX 

and  5z  is  the  first  n  symbols  of  Yfl2?,  or  Y is  a  terminal  and  ~2  =  Yw  for 

some  w  in  FIRST,_I(fl2?). (6)  For  each  string  aAw  in  S  such  that  twl  =  n  and  for  each  A  ---~ fl  in 

P,  let  ~  .>  w,  where  ,6 is  the  last  m  symbols  of aft.  D 

Example 5.36 

Consider  the  grammar  G  having  the  productions 

S  > 0 Sl l   1011 
The  (1,  1)  precedence  relations  for  G  are  shown  in  Fig.  5.12.  Since  1 ~-  1 

and  1  3>  1,  G  is  not  a  (1,  1) precedence  grammar. 

Let us use Algorithm  5.13 to compute  the  (2,  1) precedence  relations for  G. 
We  start  by  computing  S.  Initially,  $  =  [$S$,  $$S}.  We  consider  $S$  by 
adding $0S, 0S1,  S11,  11 $, (these  are  all the  substrings of $0S11 $ of length  3), 

S  0  1  $ 

"_,.>  .> 

Fig. 5.12  (1, 1) precedence relations. 

SEC. 5.3  PRECEDENCE GRAMMARS  413 
and  $01  and  011  (substrings  of  $0115  of  length  3).  Consideration  of  $$S 

adds  $$0.  Consideration  of  $0S  adds  $00,  00S,  and  001.  Consideration  of 
0S1  adds  111,  and  consideration  of 00S  adds  000.  These are all the members 

of  $. To  construct  <Z,  we  consider  those  strings  in  S  with  S  at  the  right.  We 

obtain  $$  <Z 0,  $0  ~  0,  and  00  ~  0.  To  construct  -~-, we  again  consider  the 
strings  in  S  with  S  at  the  right  and  find  $0  "  S,  0S  "  1,  S1  "  1,  $0  "  1, 
01  ~  1,  00-~-  S,  and  00-~-  1. To  construct  .>,  we  consider  strings  in  $  with  S  in  the  middle.  We  find 

11  3>  $  from  $S$  and  i 1 3>  1 from  0S1. The  (2,1)precedence  relations  for  G  are  shown  in  Fig.  5.13.  Strings  of 
length  2 which  are  not in the  domain  of ~-,  <Z, or  3>  do  not  appear. 

$$ 

$0 
0S 

00 
01 
SI 

S  0  l  $ 

<. 

l  <. 

11  i  ->  ">  Fig. 5.13  (2, 1) precedence relations. 
Since  there  are  no  (2,  1)  precedence  conflicts,  G  is  a  (2,  1)  precedence 
grammar.  D 

THEOREM 5.16 

Algorithm  5.13  correctly  computes  4 ,   -~-, and  3>. 
Proof  We  first  show  that  S  is  defined  correctly.  That  is,  7,  ~  S  if  and only ifl 

r  l =  m  +  n and 7' is a  substring  of t~flu, where  $mS$" *~rm t~Aw ==>,:m ~flW 
and  u  =  FIRST,(w). 

Only if:  The proof is by induction  on the  order in which  strings are added 
to  $.  The  basis,  the  first  two  members  of $,  is immediate.  For  the  induction, 

suppose  that  7 is added  to  $,  because  txAx is in  S  and  A  --~ fl is in P;  that  is, 
~, is  a  substring  of ctflx. Since  ~Ax  is  in  $,  from  the  inductive  hypothesis  we 

414  ONE-PASS NO  BACKTRACK PARSING  CHAP.  5 

have  the  derivation  $'S$"  *=* ~ 'A'w   =:> oc'fl'uv,  where  u  =  FIRST,(w)  and l-Ill  l-hi 
e'fl'u  can  be  written  as  JlOCAx~ 2  for  some  j~  and  ~2  in  (N  u  I~  U  {$})*. 

Since  G  is  proper,  there  is  some  y  ~  (I~  u  {$})*  such  that  J2  ~  Y-  Thus, 

$"S$"  ~  ~o~ Axyv  ~  ~ l e fl x yv .   Since  ~,  is  a  substring  of  eflx  of  length 
m  /  n,  it  is  certainly  a  substring  of eflz,  where  z  =  F I R S T , (x y v) .  

If:  An  induction  on  k  shows  that  if  $~S$"  =~k ~ Aw   :=> OCflw, then  every l'Ill  rill 
substring  of ocflu  of length  m  +  n  is  in  $,  where  u  =  FIRST,(w). 

That  steps  (4)-(6)  correctly  compute  <~,  --,   and  .>  is  a  straightforward 
consequence  of the  definitions  of these  relations.  [--] 

We may show the following  theorem,  which is the basis  of the  shift-reduce 
parser  for  uniquely  invertible  (m,  n)  precedence  grammars  analogous  to that  of  Algorithm  5.12. 

THEOREM  5.17 

Let  G - -   (N,  E, P,  S)  be  an  arbitrary  proper  CF G   and  let  m  and  n  be integers.  Let 

(5.3.1)  x , x , _   ,  . . .   x, +  xAax  ...  a, l-m 

l-m > XpXp_ 1  . . .   Xk + l Xk  . . .   Xl a 1  . . .   aq 

(1)  For  j  such  that  p -   m  ~ j   >  k,  let  0c  be  the  last  m  symbols  of 
X~Xp_~  . . .   Xj+~  and  let  fl  be  the  first  n  symbols  of  XjXj_~  . . .   Xaa~  . . .   aq. 

If  fl  ~  (E  u  {$})*,  then  either  ~  <Z fl  or  ~  "  ft. 

(2)  Xm+kXm+k_1  . . .   Xk+ 1 ~  fl,  where  fl  consists  of  the  first  n  symbols 
of  X~  . . .   X la  1  . . .   aq. 

(3)  For  k  >  j  ~>  1 let  ~  be  the  last  m  symbols  in XpXp_  1  "'"  Xj+ ~ and  let 
fl  be  the  first  n  symbols  of Xj X j_ i   . . .   Xaal  . . .   ap.  Then  ~  "  ft. 

(4)  X, , % , , _ 1 . . .   Xi  .>  a x . . . a , .  

Pr o o f  All  but  statement  (1)  are  immediate  consequences  of  the  defini- 
tions.  To  prove  (1),  we  observe  that  since j  >  k,  fl  does  not  consist  entirely 

of  $'s.  Thus  the  derivation  (5.3.1)  can  be  written  as 

(5.3.2) 

i $~S$"  ~  ~,Bw 
rm 

- - ~   ~ ' ~ 2 w  rm 

------> XpXp_l  . . .   Xk+lAa l  . . .   a q rm 

rm~XpXp_l  "'"  Xk+lXk  ' ' "  Xl a l   . ."  aq 

SEC. 5.3  PRECEDENCE GRAMMARS  41 5 

where  i  is  as  large  as  possible  such  that  B  --,  ~152 is  a  production  in  which 
62  ~  e,  B  derives  both  Xj+~ and  Xj,  and  ydi 1 =  XpXp_l  . . .   Xj+I. 

If the  first  symbol  of ~2  is  a  terminal,  say ~2  =  a83,  then  by  rule  2(b)  in 
the  definition  of--',  we  have  Xj+,,,Xj+,,,_i  . . .   Xj+i  "-~ fl,  where  fl  =  ax  and 
x  is  in  FIRST,_ a (63 w). 

If the  first  symbol of ~2 is  a  nonterminal,  let 6z  =  C~3.  Since Xj is a  ter- 
minal,  by hypothesis,  C  must  be  subsequently rewritten  after  several  steps  of 

derivation  (5.3.2)  as  De,  for  some  D  in  N  and  e  in  (N  W E)*.  Then,  D  is 
replaced  by  XiO  for  some  0,  and  the  desired  relation  follows  from  rule  2(b) 

of the  definition  of  <~.  D 

COROLLARY 

If  G  of  Theorem  5.17  is  an  (m, n)  precedence  grammar,  then  Theorem 
5.17 can be strengthened  by adding the condition that no  other relation holds 
between  the  strings  in  question  to  each  of  (1)-(4)  in  the  statement  of  the 
theorem.  D 

The  shift-reduce  parsing  algorithm  for  uniquely  invertible  extended 
precedence  grammars  is  exactly  analogous  to  Algorithm  5.12  for  simple 
precedence grammars,  and we shall only outline it here.  The first n unexpend- 

ed  input  symbols  can  be  kept  on  top  of  the  pushdown  list.  If  Xm  " "   X1 
appears  top,  al  -.  * a, is the first n input  symbols and  Xm  * ..  X1  "  al  " "   a, 

or Xm  " " " X~  ~  al  * ""  a,, then we shift. If Xm  "- * X  1 "> a~  ..  * a,, we reduce. 
Part  (1)  of Theorem  5.17  assures us that  one  of the  first two  cases will  occur 
whenever  the  handle  lies  to  the  right  of  X1.  By  part  (4)  of  Theorem  5.17, 

the  right  end  of  the  handle  has  been  reached  if  and  only  if  the  third  case 

applies. 

To  reduce,  we  search  backwards  through  ~--  relations  for  a  ~  relation, 
exactly  as  in  Algorithm  5.12.  Parts  (2)  and  (3)  of  Theorem  5.17  imply  that 
the  handle  will  be  correctly  isolated. 

5.3.4.  Weak  Precedence  Grammars 

Many naturally occurring grammars are not simple precedence grammars, 
and  in  many cases  rather  awkward  grammars  result  from  an  attempt  to  find 

a  simple  precedence  grammar  for  the  language  at  hand.  We  can  obtain 

a  larger  class  of grammars  which  can  be  parsed  using precedence  techniques 

by relaxing the restriction that the  <~ and ~-- precedence  relations be disjoint. 

We  still use  the  3>  relation  to  locate  the  right  end  of the  handle.  We  can 
then use the right sides of the productions  to locate the left end  of the handle 

by  finding  a  production  whose  right  side  matches  the  symbols  immediately 
to  the  left  of the  right  end  of the  handle.  This  is  not  much  more  expensive 
than  simple  precedence  parsing.  When  parsing  with  a  simple  precedence 

grammar,  once we had isolated the handle we still needed to determine which 

41 6  ONE-PASS NO BACKTRACK  PARSING  CHAP.  5 

production was to  be  used in  making  the  reduction,  and  thus  had  to  examine 
these  symbols  anyway. 

To  make  this  scheme  work,  we must  be  able  to  determine  which  produc- 
tion  to  use  in  case  the  right  side  of  one  production  is  a  suffix  of  the  right 

side  of another.  For  example,  suppose  that  ocflTw is a  right-sentential  form in 
which  the  right  end  of  the  handle  occurs  between  ?  and  w.  If  A----~ ?  and 
B ~ / 3 7   are two productions, then  it is not apparent  which production  should 

be  used  to  make  the  reduction. 

We  shall  restrict  ourselves  to  applying  the  longest  applicable  production. 
The weak precedence grammars are  one class of grammars for which this rule 
is  the  correct  one. 

DEFINITION 

Let  G  =  (N,  E, P,  S)  be  a  proper  C F G   with  no  e-productions.  We  say 
that  G  is  a  weak precedence grammar  if the  following  conditions  hold: 

(1)  The  relation  3>  is  disjoint  from  the  union  of  <  and  " 
(2)  If  A  --.  otXfl  and  B  --* fl  are  in  P  with  X  in  N  u  Z,  then  neither  of 
the  relations  X  <Z B  and  X  "  B  are  valid. 

Example  5.37 

The  grammar  G  with  the  following  productions  is  an  example  of a  weak 
precedence  grammart: 

E  > E  -t-  TI  +  TI T 

T  > T , F [ F  
F  > ( E) l a  

The  precedence  matrix  for  G  is  shown  in  Fig.  5.14. 

Note  that  the  only precedence  conflicts  are  between  <  and  ~-,  so  condi- 
tion  (1)  of  the  definition  of  a  weak  precedence  grammar  is  satisfied.  To 

see  that  condition  (2)  is  not  violated,  first  consider  the  three  productions 
E---+ E  +  T,  E  ~  -b T,  and  E---~ T.$  From  the  precedence  table  we  see  that 
no  precedence  relation  holds  between  E  and  E  or  between  -b  and  E  (with 

--b  on  the  left  side  of the  relation,  that  is).  Thus  these  three  productions  do 
not  cause  a  violation  of  condition  (2).  The  only  other  productions  having 

one right  side  a  suffix of  the  other are  T  ----~ T  *  F  and  T  ~  F.  Since  there  is 
no  precedence  relation  between  *  and  T,  condition  (2)  is  again  satisfied. 

Thus  G  is  a  weak  precedence  grammar. 

t  It  should  be  obvious  that  G is  related  to  our  favorite grammar  Go.  In  fact,  L(G) is  just  L(Go) with  superfluous unary +  signs, as  in  -q- a .   (q-- a  -k- a),  included.  Go  is 
another example of a uniquely invertible weak precedence grammar which is not a simple precedence grammar. 

:l:The fact that these three productions have the same left side is coincidental. 

SEC. 5.3 

E  T  F  a  ( 

PRECEDENCE  GRAMMARS 
) 

417 

* >  .>  .>  .> 

* >  .>  .>  .> 

)  i 

7 (  <., "=  <- 

* >  .>  .>  .> 

<.  <.  <.  <. 
+  <.,"  <.  <.  <. 

* _  <.  <. 
$  <-  <-  ~  <-  <.  <.  <. 

Fig. 5.14  Precedence matrix. 
Although  G is not a  simple precedence  grammar,  it does generate  a  simple 
precedence language.  Later we shall see that this is always true.  Every unique- 

ly  invertible  weak  precedence  grammar  generates  a  simple  precedence  lan- 

guage.  [Z] 

Let  us  now  verify  that  in  a  right-sentential  form  of  a  weak  precedence 
grammar  the  handle is always the right side  of the longest applicable  produc- 
tion. 

LEMMA 5.4 

Let  G  =  (N,  2;, P,  S)  be  a  weak  precedence  grammar,  and  let  P  contain 
production  B  ---~ ft.  Suppose  that  $S$  *~m ~,Cw =~'~m ~Xflw.  If  there  exists 

a  production  A  ~  tzXfl  for  any  g,  then  the  last  production  applied  was 
not  B  ---~ ft. 

Proof.  Assume  on  the  contrary  that  C  =  B  and  7  =  5X.  Then  X  ~  B 
or  X  "  B  by  Theorem  5.14  applied  to  derivation  S  *~  ~,Cw. This  follows rm 
because  the  handle  of ~,Cw ends  somewhere  to  the  right  of  C,  and  thus  C  is 

one  of  the  X's  of  Theorem  5.14.  But  we  then  have  an  immediate  violation 

of the  weak  precedence  condition.  [Z] 

LEMMA 5.5 

Let  G  be  as  in  Lemma  5.4,  and  suppose  that  G  is  uniquely  invertible. 
If  there  is  no  production  of  the  form  A--~  txXfl,  then  in  the  derivation 

418  ONE-PASS NO BACKTRACK  PARSING  CHAP.  5 

$S$  ~  ?Cw  ==> gXflw,  we  must  have  C  =  B  and  7' =  O X  (i.e.,  the  last l'Ill  rill 
production  used  was  B  ----~ fl). 

Proof  Obviously,  C  was  replaced  at  the  last  step.  The  left  end  of  the 
handle  of OXltw could  not  be  anywhere  to  the  left  of  X  by  the  nonexistence 

of  any  production  A  ~  o~X]3. If  the  handle  ends  somewhere  right  of  the 
first symbol  of fl,  then  a  violation  of Lemma  5.4 is seen to  occur with  B  ~  ,0 
playing  the  role  of A  ----~ ocXfl in  that  lemma.  Thus  the  handle  is  fl,  and  the 

result  follows  by  unique  invertibility.  [53 

Thus  the  essence  of  the  parsing  algorithm  for  uniquely  invertible  weak 
precedence  grammars  is that  we can  scan  a  right-sentential  form  (surrounded 

by  endmarkers)  from  left  to  right  until  we  encounter  the  first  3>  relation. 
This  relation  delimits  the  right  end  of the  handle.  We  then  examine  symbols 

one  at  a  time  to  the  left  of  .~.  Suppose  that  B ~  fl  is  a  production  and  we 
see  Xfl  to  the  left  of  the  .~  relation.  If  there  is  no  production  of  the  form 
A--*  ~X,8,  then  by  Lemma  5.5,  fl  is  the  handle.  If  there  is  a  production 

A  --.  ocXfl, then  we  can  infer  by  Lemma  5.4  that  B  --* fl  is  not  applicable. 
Thus  the  decision  whether  to  reduce  fl  can  be  made  examining  only  one 

symbol  to  the  left  of ft. 

We  can thus  construct  a  shift-reduce  parsing  algorithm  for  each  uniquely 
invertible  weak  precedence  grammar. 

ALGORITHM  5.14 

Shift-reduce  parsing  algorithm  for  weak  precedence  grammars. 
Input.  A  uniquely  invertible  weak precedence  grammar  G  =  (N,  Z, P,  S) 
in  which  the  productions  are  numbered  from  1 to p. 

Output.  ~Z =  (f,   g),  a  shift-reduce  parsing  algorithm  for  G. 
Method.  The  construction  is similar  to  Algorithm  5.12.  The  shift-reduce 
function f  is  defined  directly  from  the  precedence  relations: 

(1)  f ( X ,   a)  =  shift  if  X  <  a  or  X ~ -   a. 
(2)  f ( X ,   a)  =  reduce  if X  .>  a. 

(3)  f($S,  $)  =  accept. 
(3)  f ( X ,   a)  =  error  otherwise. 

The  reduce  function  g  is  defined  to  reduce  using  the  longest  applicable 
production" 

(4)  g(Xfl)  =  i if B  ~  fl is the ith production  in P  and  there is no  produc- 
tion  in P  of the  form  A  ~  ocXfl for  any  A  and  u. 

(5)  g(cx) =  error  otherwise. 

THEOREM 5.18 

Algorithm  5.14  constructs  a  valid  shift-reduce  parsing  algorithm  for  G. 

SEC. 5.3  PRECEDENCE  GRAMMARS  419 

Proof  The  proof  is  a  straightforward  consequence  of  Lemmas  5.4  and 
5.5,  the  definition  of  a  uniquely  invertible  weak  precedence  grammar,  and 
the  construction  of a  itself.  D 

There  are  several  transformations which  can  be used to eliminate preced- 
ence  conflicts  from  grammars.  Here  we  shall  present  some  useful  transfor- 
mations  of  this  nature  which  can  often  be  used  to  map  a  nonprecedence 

grammar into an equivalent  (1,  1) precedence grammar  or a  weak precedence 
grammar. Suppose  that  in  a  grammar  we  have  a  precedence  conflict  of  the  form 

X  "  Y and  X->  Y.  Since  X  "  Y there  exist  one  or  more  productions  in 
which  the  substring  X Y  appears  on the  right  side.  If in  these  productions we 
replace  X  by a  new  nonterminal  A,  we will eliminate  the  precedence  relation 

X  ~  Y  and  thus  resolve  this  precedence  conflict.  We  can  then  add  the 
production  A  ----~ X  to  the  grammar  to  preserve  equivalence.  If  X  alone  is 

not  the  right  side  of  any  other  production,  then  unique  invertibility  will  be 
preserved. 

Example 5.38 

Consider  the  grammar  G  having  the  productions 

S ~ 0 S l 1 1 0 1 1  
We  saw in  Example  5.36 that  G is not  a  simple precedence  grammar  because 

1  "  1  and  1 ->  1.  However,  if  we  substitute  the  new  nonterminal  A  for 
the  first  1 in  each  right  side  and  add  the  production  A  ~  1,  we  obtain  the 

simple  precedence  grammar  G'  with  productions 

S  > OSAltOA1 
A -   >1 

The  precedence  relations  for  G'  are  shown in  Fig.  5.15.  D 

S  A  0  1 

,  , 

S  "=  ~ 

Ai  "= 

I 0  ="  ~  <- 

1  "~  "~ 
$  <-  Fig. 5.15  Precedence relations for  G'. 

420  ONE-PASS NO  BACKTRACK PARSING  CHAP, 5 

Similar  transformations  can  be  used  to  eliminate  some  precedence  con- 
flicts  of the  form  .7( ~  Y,  X  .>  Y (and  also  of  the  form  X  ~  Y,  X ~  Y if 

simple  precedence  is  desired). 

When  these  techniques destroy unique  invertibility, it  may be possible  to 
resolve  precedence  conflicts  by eliminating  productions  as in  Lemma  2.14. 

Example 5.39 

Consider  the  grammar  G with  productions 

E  > E + T I T  

T  > T * F I F  
F  > al(E) la(L ) 
L  > L , E [ E  

In  this  grammar L  represents  a  list  of expressions,  and  variables  can  be  sub- 
scripted  by  an  arbitrary  sequence  of expressions. 

G is  not  a  weak  precedence  grammar  since  E  ~  )  and  E  3> ).  We  could 
eliminate this precedence conflict by replacing E in F---,  (E) by E' and adding 
the  production  E'----~ E.  But  then  we  would  have  two  productions  with  E 

as  the  right  side.  However,  if we instead  eliminate  the  production  F ~  a(L) 
from  G  by  substituting  for  L  as  in  Lemma  2.14,  we  obtain  the  equivalent 
grammar  G with  the  productions 

E  > E + T I T  

T  > T*  FIF 
F  > al(E) la(Z, E) Ia(E) 
L  >L, EIE 

Since L  no longer appears  to the left of),  we do  not have E  3> ) in this gram- 
mar.  We  can  easily verify that  G'  is  a  weak precedence  grammar.  D 

We can  use a  slight generalization  of these techniques  to  show that  every 
uniquely  invertible  weak  precedence  grammar  can  be  transformed  into 
a  simple precedence grammar.  Thus  the uniquely invertible  weak precedence 
grammars  are  no  more  powerful  than  the  simple  precedence  grammars  in 
their  language-generating  capability,  although,  as  we  saw  in  Example  5.37, 
there  are  uniquely  invertible  weak  precedence  grammars  which  are  not simple  precedence  grammars. 

THEOREM 5.19 

A  language is defined by a  uniquely invertible  weak precedence  grammar if and  only if it is  a  simple precedence  language. 

SEC. 5.3  PRECEDENCE GRAMMARS  421 

Proof. 

lf:  Let  G  =  (N,  l~, P,  S)  be  a  simple  precedence  grammar.  Then  clearly, 
condition  (1)  of  the  definition  of  weak  precedence  grammar  is  satisfied. 

Suppose that  condition  (2) were not satisfied.  That is,  there  exist  A  ~  txXYfl 
and  B  ---~ Yfl  in  P,  and  either  X  <  B  or  X ~  B.  Then  X  <~  Y,  by  Lemma 

5.3.  But  X = '   Y  because  of  the  production  A  ---,  o~XYfl.  This  situation  is 
impossible  because  G is  a  precedence  grammar. 

Only  if."  Let  G  =  (N,  E, P,  S)  be  a  uniquely  invertible  weak  precedence 
grammar.  We  construct  a  simple  precedence  grammar  G ' =   (N',  I;, P',  S) 

such  that  L(G')  =  L(G).  The  construction  of G' is  given  as  follows: 

(1)  Let  N'  be  N  plus  new  symbols  of the  form  [0c] for  each  e  ~  e  such 
that  A  ---~ fie  is  in P  for  some  A  and  ft. 

(2)  Let P '   consist  of the  following  productions: 

(a)  IX]----~ X  for  each  [X]  in  N'  such  that  X  is  in  N  U  l~. 

(b)  [X~] ----~ X[e] for each [Xe] in N', where  X is in N  U  E  and 0c ~  e. 

(c)  A  ~  [0c] for  each  A  ----~ t~ in P. 

We  shall  show  that  4 ,   ~-,  and  .>  for  the  grammar  G'  are  mutually  dis- 
joint.  No  conflicts  can  involve  the  endmarker.  Thus  let  X  and  Y  be  in 

N'  U  E.  We  observe  that 

(1)  If X <   Y,  t h e n X i s i n N U E ;  (2)  If X ~  Y, then  X  is in  N  U  E,  and  Y is in  N'   --  N,  since  right  sides 

of length  greater  than  one  only appear  in  rule  (2b);  and 

(3)  If X  .2>  Y, then  X  is  in  N'  U  E  and  Y is in  E. 

Part1:  "  n  3>  =  ~ . I f X   "  Y, then  Y is in N'  --  N.  I f X . >   Y, then  Y 
is  inE.  Clearly,  "  n  .>  =  ~ . 

Part  2:  .~  n  ->  =  ~.  Suppose  that  X-~  Y and  X  ->  Y.  Then  X  is  in 
N  u  E  and  Y is  in  E.  Since  X  <  Y in  G',  there  is  a  production  of the  form 

+ [Xel] --~ X[~i]  in P '   such that  [e~] 7  Yez for some  e2 in  (N'  U  E)*.  But  Xe~ 

, must be  the  suffix  of some  production  A  --~  0c3Xel in P.  Now  ea  ~  Ye'z for 

some  0c~ in  (N  U  E)*.  Thus  in  G we  have  X  "  Y or  X  <  Y. 

Now  consider  X->  Y in  G'.  There  must  be  a  production  [Bill]  ~  B[fll] +  + 

in P'  such  that  B  =~  fl2X  and  [fl~] ~  Yfl3  for  some  f12 in  (N  U  I~)*  and  f13 G' 
in  (N'  U  I~)*. In  G,  Bill  is  the  suffix  of  some  production  C  ~  )?Bfl~  in  P. 

Moreover,  B ~   fl2X  and  fla  ~  Yfl~  for  some  fl'3  in  (N  w  E)*.  Thus G  G 

X . >   Y i n G .  We  have  shown  that  if  in  G'  we  have  X  .~  Y  and  X  3>  Y,  then  in  G, 

either  X  ~  Y and  X  ->  Y or X--~  Y and  X  .>  Y. Either  situation contradicts 

422  ONE-PASS NO BACKTRACK PARSING  CHAP.  5 
the  assumption  that  G  is  a  weak  precedence  grammar.  Thus,  <~  n  ~  -- 
in  G'. 

Part  3:  <~  n  ---" --  ~ .   We  may  assume  that  X  <~ [Y~] and  X  ~  [Y~], 
for  some  X  in  N  u  E,  and  [Y~] in  N'  --  N.  This  implies  that  there  are  pro- + 

ductions  [XAfl]---,  X[A]3]  and  B - - ,   [Y~]  in  P'   such  that  [Aft]  ~  BTfl =~ (7' 
[Y0~]?fl for  some  ~, in  (N'  U  E)*,  A  and  B  in  N. 

This,  in  turn,  implies  that  there  are  productions  C--~6XA#  and 
B  ~  Ya in P  such that A *=~ B~,' for  some  ~.' in  (N  U  E)*.  Thus  in  G we  have G 

X  <~ B  or  X-~"  B.  (The  latter  occurs  if and  only  if B  z  A.) 

Now  consider  A" ~__ [Y~]. Then  there  is  a  production  [XY~]--~  X[Yoc]  in 
P',  and  thus  there  is  a  production  D  --~  eXY~  in  P. 

Therefore  if  X  <~  [Y~] and  X--"  [Y~] in  G',  then  there  are  two  produc- 
tions  in  P  of  the  form  B  --~  Y~  and  D  ---, eXY~,  and  X  <~ B  or  X ~  B  in 

G,  violating  condition  (2)  of  the  definition  of  weak  precedence  grammar. 

The  form  of  the  productions  in  P'  allows  us  to  conclude  immediately 
that  G'  is  uniquely  invertible  if  G  is.  Thus  we  have  that  L(G')  is  a  simple 
precedence  language.  A  proof that L(G')  --  L(G)  is quite  straightforward  and is  left  for  the  Exercises.  [~ 

COROLLARY 

Every  uniquely  invertible  weak  precedence  grammar  is  unambiguous. 
Proof  If  there  were  two  distinct  rightmost  derivations  in  G  of  Theorem 
5.19,  we  could  construct  distinct  rightmost  derivations  in  G'  in  a  straight- 
forward  manner.  [~ 

The  construction  in  the  proof  of  Theorem  5.19  is  more  appropriate  for 
a  theoretical  proof  than  a  practical  tool.  In  practice  we  could  use  a  far  less 
exhaustive  approach.  We  shall  give  a  simple algorithm  to  convert  a  uniquely 
invertible  weak  precedence  grammar  to  a  simple  precedence  grammar.  We 
leave  it  for  the  Exercises  to  show  that  the  algorithm  works. 

ALGORITHM  5.15 

Conversion  from  uniquely  invertible  weak  precedence  to  simple  preced- ence. 

Input.  A uniquely invertible weak precedence  grammar  G  =  (N,  E, P,  S). 

Output.  A  simple  precedence  grammar  G'  with L(G')  z  L(G). 
Method. 

(1)  Suppose  that  there  exists  a  particular  X  and  Y in  the  vocabulary  of 
G such that  X  ~  Y and  X  ~  Y. Remove from P  each production  of the form 
A  ~  ~xXY[3,  and  replace  it  by  A  --~  ocX[Yfl],  where  [Yfl]  is  a  new  nonter- minal. 

SEC. 5.3  PRECEDENCE GRAMMARS  423 

(2)  Fo r   each  [Yfl]  introduced  in  step  (1),  replace  any  produ ct ions   of 
the  form  B  ~  Yfl  by  B  ---~ [Yfl],  and  add  the  pro du cti on   [Yfl]  ---~ Yfl  to  P. 

(3)  Ret u rn   to  step  (1)  as  often  as  it  is  applicable.  Wh en   it  is  no  longer 
applicable,  let  the  resulting  g r a m m a r  be  G',  and  halt. 

Example  5.40 

Let  G  be  as  in  Example  5.37.  We  can  apply  Al go rit hm  5.15  to  obtain 
the  g r am m a r   G'  having  productions  

E  ~  E  +  [T]I  +  [T] [ [T] 

T  > T , F [ F  
F  :- ([E)]la 
IT]  :- T 
[g)]  ~  E) 

The  two  applications  of  step  (1)  are  to  the  pairs  X  =  (,  Y  =  E  an d  
X  =  + ,   Y  =  T.  The  precedence  relations  for  G'  are  given  in  Fig.  5.16.  [~] 

E  T  F  [TI  [E)]  a  (  )  +  *  $ 

o ] 

I  .> 

* >  .> 

*  o 

[TI  !  .>  .> 

* >  .>  .> 

* >  .>  .> 

* >  .> 

[E)I 

<.  < .j   <.  <.  <.  <. I 

o <"  <"  i  <"  <" 

J  "  I  <.  <- 
$  <-  <.  <.  <.  <.  <. 

<. 
<. 

,> 

* >  .> 

.> 

.> 
.> 

.> 

.> 

Fig. 5.16  Simple precedence matrix. 

424  ONE-PASS NO  BACKTRACK PARSING  CHAP.  5 

E X E R C IS E S  
5.3.1.  Which  of the  following  grammars  are  simple precedence grammars ? (a)  Go. 

(b)  S  ~  if E  then S  else S[a 

E  ~  E  or bib. (e)  S ~   ASlA 

A  ~  (S )I().  
(d)  S---,  SAIA 

A  ~  (S )l ().  

5.3.2.  Which  of  the  grammars  of  Exercise  5.3.1  are  weak  precedence  gram- 

mars ? 

5.3.3.  Which  of  the  grammars  of  Exercise  5.3.1  are  (2,  1)  precedence  gram- 

mars ? 

5.3.4.  Give examples  of precedence grammars for  which 

(a)  "  is neither reflexive, symmetric, nor  transitive. 

(b)  <  is neither irreflexive nor  transitive. 

(c)  3> is neither irreflexive nor  transitive. 

*5.3.5.  Show  that  every  regular  set  has  a  simple  precedence  grammar.  Hint" 

Make  sure your  grammar is uniquely  invertible. 

*5.3.6.  Show  that  every  uniquely  invertible  (m, n)  precedence  grammar  is  an 

LR  grammar. 

*5.3.7.  Show  that  every weak  precedence grammar is an  LR  grammar. 

5.3.8.  Prove that  Algorithm  5.12  correctly produces  a  right  parse. 

5.3.9.  Prove  that  G  is  a  precedence  grammar  if and  only  if  G  is  a  (1,  1)  pre- 

cedence grammar. 

5.3.10.  Prove Lemma  5.3(1). 

5.3.11.  Prove that  Algorithm  5.14  correctly produces  a  right parse. 

5.3.12.  Give a  right parsing algorithm for uniquely invertible (m, n) precedence 

grammars. 

5.3.13.  Prove the corollary to  Theorem  5.17. 
"5.3.14.  Show  that  the  construction  of  Algorithm  5.15  yields  a  simple preced- 

ence  grammar  equivalent  to  the  original. 

5.3.15.  For  those  grammars  of  Exercise  5.3.1  which  are  weak  precedence 

grammars,  give  equivalent  simple  precedence  grammars. 

"5.3.16.  Show  that  the  language  L  =  {a0"l"ln  ~  1 } U  {b0"12" In >_ 1 } is  not  a 

simple  precedence  language.  Hint:  Think  of  the  action  of  the  right 
parser  of  Algorithm  5.12  on  strings  of  the  form  a0"l"  and  b0"l ",  if L 

had  a  simple precedence grammar. 

"5.3.17.  Give  a  (2,  i)  precedence  grammar  for  the  language  of  Exercise  5.3.16. 

EXERCISES  425 
"5.3.18.  Give  a  simple  precedence  grammar  for  the  language  {0nalnln  >  1} 

u  {0nbl2nin  >__ 1}. 

'5.3.19.  Show  that  every  context-free  grammar  with  no  e-productions  can  be 

transformed  into  a  (1,  1) precedence  grammar. 

5.3.20.  For  CFG  G  =  (N,  X, P,  S),  define the relations 2,/z,  and  p  as follows: 

(1)  AAX if A  ---~ X~  is in P  for  some 0~. 
(2)  X/t Y if A ~  ~XYfl  is  in  P  for  some 0~ and  ft.  Also,  $/zS and 
S/z$. 

(3)  XpA  if A  ---~ ~X is in P  for  some  0~. 
Show  the  following  relations  between  the  Wirth-Weber  precedence 
relations  and  the  above  relations  ( +   denotes  the  transitive  closure; 

*  denotes reflexive and  transitive closure)' 
(a)  <  = / z 2  /. 

(b)  ~  u  {($, S),  (S,  $)}  =  ~. 
(c)  3>  =  p*ltA* n  ((N  u  ~)  x  ~). 

* "5.3.21.  Show  that  it  is  undecidable  whether  a  given  grammar  is  an  extended 

precedence  grammar  [i.e.,  whether  it  is  (m, n)  precedence  for  some  m 

and  n]. 

* 5.3.22.  Show  that  if  G  is  a  weak  precedence  grammar,  then  G  is  an  extended 

precedence grammar  (for some  m  and  n). 

5.3.23.  Show t ha t a   is in FOLLOWl(A)  if and  only if A  <  a,  A ~  a,  or A  3> a. 
5.3.24.  Generalize Lemma  5.3  to  extended precedence  grammars. 
5.3.25.  Suppose  that  we  relax  the  extended  precedence  conditions  to  permit 

0c <  w  and  0~ ~  w  if  they  are  generated  only  by  rules  (lb)  and  (2b). 
Give  a  shift-reduce  algorithm  to  parse  any  grammar  meeting  the 
relaxed definition. 

Research Problem 

5.3.26.  Find transformations which can be used to convert grammars into simple 

or  weak precedence grammars. 

Open Problem 

5.3.27.  Is  every  simple  precedence  language  generated  by  a  simple precedence 

grammar  in  which  the  start  symbol  does  not  appear  on  the  fight  side 

of  any  production?  It  would  be  nice  if  so,  as  otherwise,  we  might 
attempt  to reduce  when  $S is  on  the  pushdown  list and  $ on  the input. 

Programming Exercises 

5.3.28.  Write  a  program  to  construct  the  Wirth-Weber  precedence  relations 

for  a  context-free  grammar  G.  Use  your  program  on  the  grammar  for 
PL360  in the Appendix. 

5.3.29.  Write  a  program  that  takes  a  context-free  grammar  G  as  input  and 

constructs  a  shift-reduce  parsing  algorithm  for  G,  if  G  is  a  simple 
precedence grammar. Use your program to construct a parser for PL360. 

426  ONE-PASS  NO  BACKTRACK  PARSING  CHAP. 5 

5.3.30.  Write a  program  that  will test  whether  a  grammar is a uniquely invert- ible weak precedence  grammar. 
5.3.31.  Write  a  program  to  construct  a  shift-reduce  parsing  algorithm  for  a 

uniquely  invertible  weak  precedence  grammar. 

B I B L I O G R A P H I C   NOTE S 
The  origins  of  shift-reduce  parsing  appear  in  Floyd  [1961].  Our  treatment 
here  follows  Aho  et  al.  [1972].  Simple  precedence  grammars  were  defined  by 

Wirth and Weber  [1966] and  independently  by Pair  [1964]. The simple precedence 
concept  has  been  used  in  compilers  for  several  languages,  including  Euler  [Wirth 
and  Weber,  1966], ALGOL  W  [Bauer  et  al.,  1968], and  PL360  [Wirth,  1968]. Fischer [1969] proved  that  every CFL  without  e is generated  by a  (not  necessarily 

UI)  (1, 1)  precedence  grammar  (Exercise  5.3.19). 

Extended  precedence  was  suggested  by  Wirth  and  Weber.  Gray  [1969] points 
out  that  several  of  the  early  definitions  of  extended  precedence  were  incorrect. 
Because  of  its  large  memory  requirements,  (m, n)  extended  precedence  with 
m  +  n  >  3 seems to  have little practical utility.  McKeeman [1966] studies methods 
of reducing the table  size for  an  extended precedence parser.  Graham  [1970] gives 
the interesting theorem that  every deterministic language has a UI  (2,  1) precedence grammar. 

Weak  precedence  grammars  were  defined  by  Ichbiah  and  Morse  [1970]. Theorem  5.19  is from Aho  et  al.  [1972]. 

Several  error  recovery  schemes  are  possible  for  shift-reduce  parsers.  In  shift- 
reduce  parsing  an  error  can  be  announced  both  in  the  shift-reduce  phase  and 
in  the reduce  phase.  If an  error  is reported  by the  shift-reduce  function,  then  we 
can  make deletions,  changes,  and  insertions  as in  both  LL and  LR  parsing.  When 

a  reduce error  occurs,  it  is  possible  to  maintain  a  list  of error  productions  which can  then  be  applied  to  the  top  of  the  pushdown  list. 

Error  recovery  techniques  for  simple  precedence  grammars  are  discussed  by 
Wirth  [1968] and  Leinius  [1970]. To  enhance  the  error  detection  capability  of  a 
simple precedence parser,  Leinius  also  suggests checking after a  reduction  is made 
that  a  permissible  precedence  relation  holds  between  symbols  X  and  A,  where  A 
is the new nonterminal  on top  of the pushdown  list and  X the symbol immediately below. 

5.4,  OTHER  CLASSES  OF  SHIFT-REDUCE 

PARSABLE  GRAMMARS 

We  shall  mention  several  other  subclasses  of  the  LR  grammars  having 
shift-reduce  parsing  algorithms.  These  are  the  bounded-right-context  gram- 
mars,  mixed  strategy  precedence  grammars,  and  operator  precedence  gram- 
mars.  We  shall  also  consider  the  Floyd-Evans  production  language,  which 
is  essentially  a  programming  language  for  deterministic  parsing  algorithms. 

SEC. 5.4  OTHER  CLASSES OF  SHIFT-REDUCE PARSABLE GRAMMARS  427 
5.4.1.  Bounded-Right-Context  Grammars 

We would like to  enlarge the class of weak precedence  grammars that  we 
have  considered  by  relaxing  the  requirement  of  unique  invertibility.  We 
cannot  remove  the  requirement  of  unique  invertibility  altogether,  since 
an economical parsing algorithm is not  known  for all precedence grammars. 
However,  we can  parse  many grammars  using  the  weak precedence  concept 
to locate the right end of a handle and then using local context, if the grammar 
is not uniquely invertible, to locate the left end of the handle and to determine 
which  nonterminal  is to  replace  the  handle. 

A  large  class  of  grammars  which  can  be  parsed  in  this  fashion  are  the 
(m, n)-bounded-right-context  (BRC) grammars.  Informally,  G =  (N, Z, P,  S) 
is  an  (m, n)-BRC  grammar  if  whenever  there  is  a  rightmost  derivation 

S' ~  aaw  =~ apw, rm  rm 
in  the  augmented  grammar  G'  =  (N  t2  [S'}, Z, P  W {S' ---~ S}, S'),  then  the 
handle  fl  and  the  production  A ---~ fl  which  is  used  to  reduce  the  handle  in aflw can  be uniquely  determined  by 

(1)  Scanning aflw from left to  right until the handle is encountered. 

(2)  Basing  the  decision  of whether  ~  is  the  handle  of  aflw,  where  ~,~ is 
a  prefix  of aft,  only  on  ~,  the  m  symbols  to  the  left  of 6  and  n  symbols  to the  right  of 6. 

(3)  Choosing  for  the  handle  the  leftmost  substring  which  includes,  or  is 
to  the  right  of,  the  rightmost  nonterminal  of  aflw,  from  among  possible candidates  suggested  in  (2). 

For  notational  convenience  we shall  append  m  $'s to the  left and n $'s to 
the  right  of every right-sentential  form.  With  the  added  $'s  we can be sure 

that  there  will  always be  at  least  m  symbols  to  the  left  of and  n  symbols  to 
the  right  of the  handle  in  a  padded  right-sentential  form. 

DEFmmON 

G =  (N, Z, P,  S)  is  an  (m, n)-bounded  right-context  (BRC)  grammar  if the  four  conditions" 

(1)  $'ns'$" ~  aAw ~  aflw and G'  rm  G" rm 
(2)  $ms'$" ~  ?Bx ~  y~x =  ~' fly  are  rightmost  derivations  in  the G'  rm  G" rm 
augmented  grammar  G ' :   (N  U  {S'}, Z, P  U  {S' ---~ S}, S'). (3)  lxl_<lyl 

(4)  the  last  m  symbols  of  a  and  a'  coincide,  and  the  first  n  symbols  of 
w  and  y  coincide 

imply  that  t~'Ay----~Bx;  that  is,  a ' =   7,,  A  =  B,  and  y----x. 

A  grammar  is  BRC  if it  is  (m, n)-BRC  for  some  m  and  n. 

428  ONE-PASS NO BACKTRACK PARSING  CHAP. 5 

If we think of derivation  (2) as the "real" derivation  and of (1) as a possible 
cause  of  confusion,  then  condition  (3)  ensures  that  we  shall  not  encounter 
a  substfing  that  looks  like  a  handle  (p  surrounded  by  the  last  m  symbols  of 

and  the  first  n  of  w)  to  the  left  of the  real  handle  $.  Thus  we  can  choose 
as  the  handle  the  leftmost  substring  that  "looks"  like  a  handle.  Condition 

(4) assures  that  we  only use  m  symbols  of left context  and  n  symbols  of right 
context  to  decide  whether  something  is  a  handle  or  not. 

As  with  LR(k)  grammars,  the  use  of  the  augmented  grammar  in  the 
definition  is  required  only  when  S  appears  on  the  right  side  of  some 
production.  For  example,  the  grammar  G with  the  two  productions 

S  > Sala 
would  be  (1, 0)-BRC without  the  proviso  of an  augmented  grammar.  As  in 
Example  5.22 (p.  373),  we cannot  determine  whether  to accept  S in the  right- 
sentential  form  Sa  without  looking  ahead  one  symbol.  Thus  we  do not  want 

G  to  be  considered  (1, 0)-BRC. 

We shall prove  later  than  every (m, k)-BRC  grammar  is LR(k).  However, 
not  every LR(0)  grammar  is  (m, n)-BRC  for  any m  and  n,  intuitively  because 

the LR definition  allows us to use the entire portion  of a right-sentential  form 
to  the  left  of the  handle  to  make  our  parsing  decisions,  while  the  BRC  con- 
dition  fimits  the  portion  to  the  left  of  the  handle  which  we  may  use  to  m 
symbols.  Both  definitions  limit  the  use  of the  portion  to  the  right  of the  han- 
dle,  of course. 

Example 5.41 

The  grammar  G i  with  productions 

S  > aAc 
A  > Abblb 

is  a  (1, 0)-BRC  grammar.  The  right-sentential  forms  (other  than  S'  and  S) 
are  aAb2"c  for  all  n  >  0  and  ab2"+le  for  n  >  0.  The  possible  handles  are 

aAe, Abb, and b, and in each right-sentential  form the handle  can  be  uniquely 
determined  by  scanning  the  sentential  form  from  left  to  right  until  aAe  or 
Abb  is encountered  or b is encountered  with an  a  to  its left.  Note  that  neither 
b  in  Abb  could  possibly  be  a  handle  by  itself,  because  A  or  b  appears  to  its 

left. 

On  the  other  hand,  the  grammar  G2  with  productions 

S  > aAc 
A  > bAb[b 

generates  the  same  language  but  is  not  even  an  LR  grammar.  D 

SEC. 5.4  OTHER CLASSES OF SHIFT-REDUCE PARSABLE GRAMMARS  429 

Example 5.42 

The  grammar  G  with  productions 

S  > aAlbB 
A  ---+  0A l 1 

B----+  0BI  1 

is  an  LR(0)  grammar,  but  fails  to  be  BRC,  since  the  handle  in  either  of 

the right-sentential  forms a0"l  and  b0"l  is  !,  but  knowing only a fixed number 

of symbols immediately to the  left of  1 is  not  sufficient  to  determine  whether 
A  ~  1 or  B  ~  1 is to  be  used  to  reduce  the  handle. 

Formally,  we  have  derivations 

and 

smstsn ~  SmaOmASn ~  $maOml $" rill 

s msts "  z z ~   $mbOmB$" ~  SmbOml $" rm  rl'n 
Referring  to the BRC definition,  we note that  a  --  $ma0m, 0~' --- ~' ---  $mb0m, 
fl - - 6   =  1,  and  y  =  w - -   x  --  $n.  Then  ~  and  ~'  end  in  the  same m symbols, 
0 m; w and y begin  with  the  same n,  $"; and  [xl  <  ]y l, but ~'Ay  =/= ?Bx. (A  and 
B  are  themselves  in  the  BRC  definition.) 

The  grammar  with  productions 

S  > aA [bA 
A  ---~  0A I 1 

generates  the  same  language  and  is  (0, 0)-BRC.  [-7 

Condition  (3)  in  the  definition  of BRC  may  at  first  seem  odd.  However, 
it  is  this  condition  that  guarantees  that  if,  in  a  right-sentential  form  a'fly,  fl 
is  the  leftmost  substring  which  is  the  right  side  of  some  production  A  ~  fl 

and  the  left  and  right Context  of  fl  in  a'fly  is  correct,  then  the  string  a'Ay 
which  results  after  the  reduction  will  be  a  right-sentential  form. 

The  BRC  grammars  are  related  to  some  of  the  classes  of  grammars  we 
have  previously  considered  in  this  chapter.  As  mentioned,  they  are  a  subset 

of the LR grammars.  The BRC grammars are extended precedence grammars, 
and  every uniquely invertible  (m, n) precedence  grammar  is a  BRC  grammar. 
The  (1,  1)-BRC  grammars  include  all  uniquely  invertible  weak  precedence 
grammars.  We  shall  prove  this  relation  first. 

THEOREM 5.20 

If  G  =  (N,  E, P,  S)  is  a  uniquely  invertible  weak  precedence  grammar, 
then  it  is  a  (1,  1)-BRC  grammar. 

430  ONE-PASS NO BACKTRACK PARSING  CHAP.  5 

Proof  Suppose  that  we  have  a  violation  of  the  (1,  1)-BRC  condition, 
i.e.,  a  pair  of derivations 

$S'$ ~  aAw ~  aflw rm  riD. 
and 

$s'$  ~=  rBx ~  ~,ax =  a' fly 

where a  and a' end in the same symbol;  w and y  begin with the same  symbol; 
and  l xl  _< l yl,  but  ~,Bx ~  a'Ay.  Since  G  is  weak  precedence,  by  Theorem 

5.14  applied  to  75x,  we  encounter  the  3>  relation  first  between  5  and  x. 
Applying Theorem  5.14 to ~flw,  we encounter  3> between fl and  w, and  since 

w  and  y  begin  with  the  same  symbol,  we  encounter  3>  between  fl  and  y. 
Thus,  l a'13l  >_  I~'~ I.  Since  we  are  given  I xl  _< i y l,  we  must  have  a'fl  =  7di and  x  =  y. 

If we can show that  fl =  O, we shall have  a'  =  7-  But by unique invertibil- 
ity,  A  =  B.  We  would  then  contradict  the  hypothesis  that  7Bx  ~  ~'Ay. 

If  f l c   J,  then  one  is  a  suffix  of  the  other.  We  consider  cases  to  show / / =   ,~. 

Case  1:]3  =  eX5 for  some e and  X.  X is the  last  symbol of y,  and there- 
fore  we  have  X  <~ B  or  X  "  B  by  Theorem  5.14  applied  to  right-sentential 

form  ?Bx.  This  violates  the  weak  precedence  condition. 

Case 2:  J  =  eXfl for  some e and  X.  This case is symmetric to  the  above. 
We  conclude  that  fl  =  O and  that  G is  (1,  1)-BRC.  [Z 

THEOREM 5.21 

Every (m, k)-BRC  grammar  is  an  LR(k)  grammar. 
Proof.  Let  G  =  (N,  Z,P,  S)  be  (m, k)-BRC  but  not  LR(k).  Then  by 
Lemma  5.2,  we have  two  derivations in  the  augmented  grammar  G' 

S'  ~  aA w - - ~   oq3 w rill  rm 
and 

S'  ~rm  7 Bx ~  7OX =  efly 

where  I~Ol ~  laP[  and  FIRSTk(y )  =  FIRSTk(w),  but  7Bx ~  ~Ay.  If  we 

surround  all  strings  by  $'s  and  tet  ~ ' =   ~,  we  have  an  immediate  violation 
of the  (m, k)-BRC  condition.  [Z] 

COROLLARY 

Every  BRC  grammar  is  unambiguous.  D 

sEc. 5.4  OTHER CLASSES OF SHIFT-REDUCE PARSABLE GRAMMARS  431 

We  shall  now  give  a  shift-reduce  parsing  algorithm  for  BRC  grammars 
and  discuss  its  efficient  implementation.  Suppose  that  we  are  using  a  shift- 
reduce  parsing  algorithm  to  parse  a  BRC  grammar  G  and  that  the  parsing 
algorithm  is  in  configuration  (0c, w, z~). Then  we  can  define  sets  ~C  and  9Z, 
which  will tell us whether  the  handle  in the  right-sentential  form  t~w appears 

on top of the stack (i.e., is a  suffix of 00 or whether the right end of the handle 
is somewhere in  w (and  we need to  shift).  If the  handle  is on top  of the  stack, 
these  sets  will  also  tell  us  what  the  handle  is  and  what  production  is  to  be 
used  to  reduce  the  handle. 

DEFINITION 

Let  G  be  an  (m, n)-BRC  grammar.  ~ , , ( A ) ,   for  A  ~  N,  is  the  set  of 
triples  (~,  fl,  x)  such  that  l e l =   m,  Ix l =   n,  and  there  exists  a  derivation $"S'$"  *=*~m ~ocAxy ~  ~,ocflxy 

in  the  augmented  grammar. 
9Z G  is the  set  of pairs  (e,  x)  such  that m,  n 

(1)  Either  1~[  =  m  --t-/,  where  l is  the  length  of the  longest  right  side  in 
P  o r i e l   <  m  -q- I and  ~  begins  with  $". 

(2)  Ixl  =  n. (3)  There  is  a  derivation  $ms'$"  ~rm flay  :72 flYY, where  txx is a  substring 

of ]32,y positioned so that ~ lies within fl? and does not include the last symbol 
of fly. We  delete  G,  m,  and  n  from  ~(A)  and  9Z when  they  are  obvious. 

The  intention  is that  the appearance  of substring ocflx in scanning a  right- 
sentential  form  from  left  to  right  should  indicate  that  the  handle  is  fl  and 

that  it  is  to  be  reduced  to  A  whenever  (e,  fl, x)  is  in  ~C(A). The  appearance 

of ex,  when  (e,  x)  is in  9Z, indicates  that  we  do  not  have  the  handle  yet,  but 
it  is  possible  that  the  handle  exists  to  the  right  of  e.  The  following  lemma 
assures  us  that  this  is,  in  fact,  the  case. 

LEMMA 5.6 

G  =  (N,  E, P, S)  is  (m, n)-BRC  if and  only  if 
(1)  Let  A --,  fl  and  B  ~  6  be  distinct  productions.  Then  if (~, fl, x) is in 
~Cm,,(A) and (~,, 6, x) is in 5Cm.,(B ), then ~fl  is not a suffix of ~,O, or vice versa; 

(2)  For  all A  ~  N,  if (0~, fl, x)  is in  ~Cm.,(A), then  (Oo~fl, x)  is  not  in  9Zn, . 
for  any  0. 

Proof. 

If:  Suppose  that  G  is  not  (m, n)-BRC.  Then  we  can  find  derivations  in 
the  augmented  grammar  G 

$~S'$" ~  ~Aw ~  ~flw rIIl  rnl 

432  ONE-PASS NO BACKTRACK PARSING  CHAP. 5 

and 

$mstsn  ~rm  r Bx   r~m  r ~x  --  oF[3y 

where  0~ and  0F coincide  in  the  last  m  places,  w and  y  coincide  in  the  first  n 
places,  and  Ix I_<  ]y 1, but  7Bx  ~  ogAy.  Let  e  be  the  last  m  places  of 0~ and z 

the  first  n  places  of  w.  Then  (e,  fl,  z)  is in  ~(A).  If x  ~  y,  and  Ix[  _< ]y 1, we 
must have (Oefl, z) in 9Z for some 0 and  thus  condition  (2) is violated.  If x  =  y, 
then  07, 6, z)  is  in  ~(B),  where  I/is  the  last  m  symbols  of  7.  If  A  --> fl  and 
B  --~ d~ are  the  same,  then  with  x  =  y  we  conclude  7Bx  =  oFAy, contrary  to 
hypothesis.  But  since  one  of  ~6  or  eft  is  a  suffix  of  the  other,  we  have  a 
violation  of (1)  if A  --,  fl  and  B  ~  ~  are  distinct. 

Only  if:  Given  a  violation  of  (1)  or  (2),  a  violation  of  the  (m,  n)-BRC 
condition  is  easy  to  construct.  We  leave  this  part  for  the  Exercises.  [Z 

We  can  now  give  a  shift-reduce  parsing  algorithm  for  BRC  grammars. 
Since  it  involves  knowing  the  sets  ~(A)  and  9Z, we  shall  first  discuss  how  to compute  them. 

ALGORITHM 5.16 

Construction  of  ~m,,(A)  and  9Zm. .. 
Input.  A  proper  grammar  G  =  (N,  Z, P,  S). 

Output.  The  sets  ~m,,(A)  for  A  ~  N  and  9Z. 
Method. 

(1)  Let  I be  the  length  of the  longest  right  side  of a  production.  Compute $,  the  set  of  strings  7  such  that 

(a)  ]) , [ -- - - m - q- n +l ,  or  [~,[<m-q-n- Jr-I   and  y  begins  with  $m; 
(b))'  is  a  substring  of ocflu, where  ocflw is a  right-sentential  form  with 

handle  fl  and  u  ----- FIRST,(w);  and 
(c)  ~, contains  at  least  one  nonterminal. 
We  can  use  a  method  similar  to  the  first  part  of  Algorithm  5.13  here. 
(2)  For  A  ~  N,  let  5e(A)  be  the  set  of (ct, fl, x)  such  that  there  is a  string ~,ocAxy 

in  $,  A  --,  fl is in P,  [~[  =  m,  and  Ix]  =  n. 
(3)  Let  9Z be  the  set  of  (0~, x)  such  that  there  exists  ~,By in  $,  B  --~  ~  in 
P,  ctx is  a  substring  of ~,Oy, and  ~  is  within  ?O,  exclusive  of  the  last  symbol 

of ~,O. Of course,  we  also  require  that  I x ] =   n  and  that  [~[  --  m  / / ,   where  l 
is  the  longest  right  side  of  a  production  or  ~  begins  with  $"  and  [~[  < m-+- l.  F-] 

THEOREM 5.22 

Algorithm  5.6  correctly  computes  ~(A)  and  9Z. 
Proof  Exercise.  [Z] 

SEC. 5.4  OTHER CLASSES OF SHIFT-REDUCE PARSABLE GRAMMARS  4:33 
ALGORITHM  5.17 

Shift-reduce  parsing  algorithm  for  BRC  grammars. 
Input.  An  (m,n)-BRC  grammar  G =   ( N , X , P , S ) ,   with  augmented 
grammar  G'  =  (N',  X, P,  S'). 

Output.  et  =  (f,  g),  a  shift-reduce  parsing  algorithm  for  G. 
Method. 

(1)  Let  f (a,   w)  =  shift  if (~,  w)  is in  9Zm. .. 
(2)  f(a,  w)  =  reduce  if ~  =  axa2,  and  (~1,  a2,  w)  is  in  5Cm.,(A) for  some 
A,  unless  A  =  S',  a~  =  $,  and  a2  =  S. 

(3)  f($ss,  $")  =  accept. 
(4)  f (a,   w)  =  error  otherwise. 
(5)  g(a,  w)  =  i  if we  can  write  a  =  a~a2,  (aa,  ~2,  w)  is  in  ~(A),  and  the 
ith  production  is  A  --~  ~2. 

(6)  g(a,  w)  =  error  otherwise.  D 

THEOREM  5.23 

The  shift-reduce  parsing  algorithm  constructed  by  Algorithm  5.17  is 
valid  for  G. 

Proof.  By  Lemma  5.6,  there  is  never  any  ambiguity  in  defining f  and  g. 
By  definition  of  ~(A),  whenever  a  reduction  is  made,  the  string  reduced, 
t~2, is  the  handle  of  some  string  flocloczwz. If  it  is  the  handle  of  some  other 
string  fl'oclo~zwz', a  violation  of  the  BRC  condition  is  immediate.  The  only 
difficult  point  is ensuring  that  condition  (3),  [xl  _< lY l, of the  BRC  definition 
is  satisfied.  It  is  not  hard  to  show  that  the  derivations  of  floclet2wz and fl'oc~ctzwz' 

can be  made  the  first and  second  derivations  in the  BRC  definition 
in  one  order  or  the  other,  so  condition  (3)  will  hold.  [Z] 

The  shift-reduce  algorithm  of Algorithm  5.17 has f  and g functions  which 
clearly  depend  only  on  bounded  portions  of  the  argument  strings,  although 

one  must  look  at  substrings  of varying  lengths  at  different  times.  Let  us  dis- 
cuss  the  implementation  o f f   and  g  together  as  a  decision  tree.  First,  given 

on  the  pushdown  list  and  x  on  the  input,  one  might  branch  on  the  first  n 
symbols  of x.  For  each  such  sequence  of n  symbols,  one  might  then  scan 
backwards,  at  each  step  making  the  decision  whether  to  proceed  further  on 

0c or  announce  an  error,  a  shift,  or  a  reduction.  If a  reduction  is  called  for, 
we  have  enough  information  to  tell  exactly  which  production  is  used,  thus 

incorporating  the g  function  into  the  decision  tree  as  well.  It  is  also  possible 

to  generalize  Domolki's  algorithm  (See the  Exercises of Section 4.1.)  to  make 

the  decisions. 

434  ONE-PASS NO BACKTRACK PARSING  CHAP. 5 

Example 5.43 

Let  us  consider  the  grammar  G given  by 

(0)  s '   ~ s  
(1)  s  ,  OA 
(2)  S  ,~ 1S 
(3)  a  ~  OA 
(4)  A  71 

G  is  (1, 0)-BRC.  To  compute  5C(A), 5C(S),  and  ~ ,   we  need  the  set  of 
strings  of  length  3  or  less  that  can  appear  in  the  viable  prefix  of  a  right- 
sentential  form  and  have  a  nonterminal.  These  are  $S',  $S,  $0A,  SIS,  00A, 

11 S,  10A,  and  substrings  thereof. 

We  calculate 

5C,,o(S' )  -~  [($,  S,  e)} 

~,,o(S)  =  {($,  OA, e),  ($,  1S,  e),  (1,  OA, e),  (1,  1S,  e)} 
~C x,o (A)  =  [(0,  OA,  e),  (0,  1,  e)} 

i~  consists  of  the  pairs  (a,  e),  where  a  is  $,  $0,  $00,  000,  $1,  $11,  $10, 
111,100,  or  110.  The  functions f  and  g  are  given  in  Fig.  5.17.  By  "ending  of 
a"  we  mean  the  shortest  suffix  of  a  necessary  to  determine  f(a,   e)  and  to 
determine  g(a,  e)  if necessary. 

Ending of a  f(a, e)  g(a, e) 

$0A  reduce 10A  reduce 
00A  reduce $IS  reduce 

11S  reduce O1  reduce 

O0  shift $0  shift 
$10  shift 110  shift 

$1  shift $11  shift 
111  shift $  shift 

$S  accept 

Fig. 5.17  Shift-reduce functions. 

SEC. 5.4  OTHER  CLASSES OF  SHIFT-REDUCE  PARSABLE GRAMMARS  435 

A  decision  tree  implementing  f  and  g  is  shown  in  Fig.  5.18  on  p.  436. 
We  omit  nodes  with  labels  A  and  S  below  level  1.  They  would  all  have  the 

outcome  error,  of course.  [~ 

5.4.2,  Mixed  Strategy  Precedence  Grammars 

Unfortunately,  the  shift-reduce  algorithm  of  Algorithm  5.17  is  rather 
expensive  to  implement  because  of the  storage  requirements  for  the f  and  g 
functions.  We can define  a less costly shift-reduce  parsable  class  of grammars 

by  using  precedence  to  locate  the  right  end  of  the  handle  and  then  using 

local  context  to  both  isolate  the  left  end  of the  handle  and  determine  which 

nonterminal  is  to  replace  the  handle. 

Example  5.44 

Consider  the  (non-UI)  weak  precedence  grammar  G  with  productions 

S-  > aA l bB 
A  > CAIIC1 

B  > DBE1  [DE1 
C  >0 
D-  >0 

E  >1 

G generates the language [a0"l"ln  ~  1 } U  (b0"l 2"In >  1 ], which we shall show 
in  Chapter  8  not  to  be  a  simple  precedence  language.  Precedence  relations 
for  G  are  given  in  Fig.  5.19  (p.  437).  Note  that  G is  not  uniquely  invertible, 

because  0  appears  as  the  right  side  in  two  productions,  C  --~ 0  and  D  ~  0. 
However,  ~1,0(C)  =  [(a,  0,  e),  (C,  0,  e)}  and  ~l,0(D)  =  {(b, 0,  e),  (D,  0,  e)~ 
Thus,  if  we  have  isolated  0  as  the  handle  of  a  right-sentential  form,  then 

the  symbol  immediately  to  the  left  of  0  will  determine  whether  to  reduce 

the  0  to  C  or  to  D.  Specifically,  we  reduce  0  to  C  if  that  symbol  is  a  or  C 
and  we  reduce  0  to  D  if that  symbol  is  b  or  D.  [~] 

This  example  suggests  the  following  definition. 
DEFINITION 

Let  G  =  (N,  E, P,  S)  be  a  proper  CFG  with  no  e-productions.  We  say 
that  G  is  a  (p, q;  m,  n)  mixed strategy precedence  (MSP)  grammar  if 

(1)  The  extended  (p, q)  precedence  relation  .>  is  disjoint  from  the  union 
of the  (p, q)  precedence  relations  <  and  2__. 

(2)  If A  ~  ~fl and  B  ~  fl are  distinct  productions  in P,  then  the  follow- 
ing  three  conditions  can  never  be  all  true  simultaneously: 

/  o 

-  0 
L~ 

* ~'-~ -  ;,,~ 

1,1 
0 

,d 
o 
.,,.f 

0 

a0 

436 

SEe.  5.4  OTHER  CLASSES OF SHIFT-REDUCE PARSABLE GRAMMARS  437 

S  A  B  C  D  E  a  b  0  1  $ 

.> 
<.  .> 
<.  <. 

<.  "_  <. 

o 

-  <. 
i  o 

0  .>  .> 

1 ~  .>  .> 
$  <.  <. 

Fig. 5.19  Precedence relations  for G. 
(a)  ~m,n(A) contains  (7,  ~fl,  x). (b)  ~,,.,(B)  contains  (~,  fl,  x). 
(c)  ~  is  the  last  m  symbols  of ~,~. 
A  (I,  1;  1, 0)-MSP  grammar  will  be  called  a  simple  MSP  grammar.  For 
example, the grammar  of Example  5.44 is simple MSP.  In fact, every uniquely 
invertible  weak  precedence  grammar  is  a  simple  MSP  grammar. 

Let  l(A)  =  ~XI X  ~  A  or X--"  A}.  For a  simple  MSP grammar,  condition 
(2)  above  reduces  to  the  following: 

(2a)  If  A--~  fl'Xfl  and  B - ~   fl  are  productions,  then  X  is  not  in  I(B). 
(2b)  If  A  --~  fl  and  B  --~  fl  are  productions,  A ~  B,  then  I(A)  and  I(B) 
are  disjoint. 

Condition  (1)  above  and  condition  (2a)  are  recognizable  as  the  weak 
precedence  conditions.  Thus  a  simple  MSP  grammar  can  be  thought  of  as 

a  (possibly  non-UI)  weak  precedence  grammar  in  which  one  symbol  of  left 
context  is  sufficient  to  distinguish  between  productions  with  the  same  right 

side  [condition  (2b)]. 

ALGORITHM 5.18 

Parsing  algorithm  for  M s P   grammars. 

438  ONE-PASS  NO  BACKTRACK PARSING  CHAP.  5 

Input.  A  (p,  q;  m, n)-MSP  grammar  G  =  (N,  Z, P,  S)  in  which  the  pro- 
ductions  are  numbered. 

Output.  a  =  ( f,   g),  a  shift-reduce  parsing  algorithm  for  G. 
Method. 
(1)  Let  l al  =  p  and  ]xl  =  q.  Then 

(a)  f(a,  x)  =  shift  if a  ~  x  or  a  ~  x,  and 
(b)  f(a ,  x)  =  reduce  if 0c .>  x. 
(2)  f($PS,  $q)  ---- accept. 
(3)  f(7',  w ) =   error  otherwise. 
(4)  Let  ~Cm,n(A )  contain  (a,  fl,  x)  and  A  ~  fl  be  production  i.  Then 
g(~fl,  x)  =  i. 

(5)  g(?,  w)  =  error  otherwise.  [--] 

THEOREM 5.24 

Algorithm  5.18  is valid  for  G. 
Proof.  Exercise.  It  suffices  to  show  that  every  MSP  grammar  is  a  BRC 
grammar,  and  then  show  that  the  functions  of  Algorithm  5.18  agree  with 
those  of Algorithm  5.17.  D 

5.4.3.  Operator  Precedence  Grammars 

An  efficient parsing  procedure  can  be given for  a  class  of grammars  called 
operator  precedence  grammars.  Operator  precedence  parsing  is  simple  to 
implement  and  has  been  used  in  many  compilers. 

DEFINITION 

An  operator  grammar  is  a  proper  C FG   with  no  e-productions  in  which 
no  production  has  a  right  side  with  two  adjacent  nonterminals. 

For  an  operator  grammar  we  can  define  precedence  relations  on  the  set 
of terminal  symbols and  $, while ignoring nonterminals.  Let G  -~  (N,  E, P,  S) 
be  an  operator  grammar  and  let  $ be a  new  symbol.  We define  three  operator precedence  relations 

on  E  U  {$} as  follows: 

(1)  a  ~  b  if A  ~  o~a~bfl is  in  P  with  7' in  N  u  {e}. 
(2)  a  <"  b  if  A  --~  txaBfl  is  in  P  and  B  ~  7,bO, where  7'  is  in  N  U  {e}. 
(3)  a  .> b  if  A  ~  ~Bbfl  is  in  P  and  B  :=~ tSaT', where  7'  is  in  N  U  {e}. 

+ (4)  $  <-  a  if S  =~  7'a0c with  7' in  N  U  {e}. 

(5)  a  "> $  if S  ~  ~aT' with  7' in  N  u  {e}. 

G  is  an  operator  precedence  grammar  if  G  is  an  operator  grammar  and 
at  most  one  operator  precedence  relation  holds  between  any  pair  of terminal 
symbols. 

SEC. 5.4  OTHER  CLASSES OF SHIFT-REDUCE PARSABLE GRAMMARS  439 

Example  5.45 

The grammar  G o is a classic example of an operator precedence  grammar" 
(1)  E ~ E + T   (2)  E ~ T  
(3)  T ~ T , F   (4)  T ~ F  

(5)  F  --~ (E)  (6)  F  ---~ a 

The  operator  precedence  relations  are  given  in  Fig.  5.20.  5 

(  a  *  +  )  $ 

* >  .>  .>  .> 

* >  .>  .>  -> 
<.  <.  .>  .>  .>  .> 
<.  <.  <.  .>  .>  .> 

<.  <.  <.  <. 
<.  <.  <.  <.  Fig.  5.20  Operator  precedence  rela- 

tions  for  Go. 

We  can  produce  skeletal  parses  for  operator  precedence  grammars  very 
efficiently. The parsing principle is the same as for simple precedence  analysis. 
It  is  easy  to  verify  the  following theorem. 

THEOREM 5.25 

Let  G  =  (N,  E, P,  S)  be  an  operator  grammar,  and  let  us  suppose  that 
$S$  *~ txAw ~  txflw.  Then rm  rm 

(1)  The  operator  precedence  relation  <z  or  ~  holds between  consecutive 
terminals  (and  $)  of tx; (2)  The  operator  precedence  relation  <  holds  between  the  rightmost 

terminal  symbol  of  ~  and  the  leftmost  terminal  symbol  of  fl; 

(3)  The  relation  ~  holds  between  the  consecutive  terminal  symbols  of fl; 

(4)  The  relation  3>  holds  between  the  rightmost  terminal  symbol  of  fl 
and  the  first  symbol  of w. 

Proof.  Exercise.  [Z] 

COROLLARY 

If  G  is  an  operator  precedence  grammar,  then  we  can  add  to  (1)-(4)  of 
Theorem  5.25  that  "no  other  relations  hold." 

Proof.  By definition  of operator  precedence  grammar.  D 

440  ONE-PASS NO  BACKTRACK  PARSING  CHAP.  5 

Thus  we  can  readily  isolate  the  terminal  symbols  appearing  in  a  handle 
using  a  shift-reduce  parsing  algorithm.  However,  nonterminal  symbols 
cause  some  problems,  as  no  precedence  relations  are  defined  on  nonterminal 

symbols.  Nevertheless,  the  fact  that  we  have  an  operator  grammar  allows 
us  to  produce  a  "skeletal"  right  parse. 

Example 5.46 

Let  us  parse  the  string  (a  -+- a)  *  a  according  to  the  operator  precedence 
relations  of  Fig.  5.20  obtained  from  G o .  However,  we  shall  not  worry  about 
nonterminals  and  merely  keep  their  place  with  the  symbol  E.  That  way  we 

do  not  have  to  worry  about  whether  F  should  be  reduced  to  T,  or  T  to  F 

(although  in  this  particular  case,  we  could  handle  such  matters  by  going 
outside  the  methods  of  operator  precedence  parsing).  We  are  effectively 
parsing  according  to  the  grammar  G" 

(1)  E  > E  +  E 
(3)  E  > E ,   E 

(s)  E  ,  (E) 
(6)  E  > a  

derived  from  G o by  replacing  all  nonterminals  by  E  and  deleting  single  pro- 
ductions.  (Note  that  we  cannot  have  a  production  with  no  terminals  on  the 
right  side  in  an  operator  grammar  unless  it  is  a  single  production.) 

Obviously,  G  is  ambiguous,  but  the  operator  precedence  relations  will 
assure  us  that  a  unique  parse  is found.  The  shift-reduce  algorithm  which  we 
shall  use  on  grammar  G  is  given  by  the  functions f  and  g  below.  Note  that 
strings  which  form  arguments  for f  and  g  are  expected  to  consist  only  of 
the  terminals  of  Go  and  the  symbols  $  and  E.  Below,  ~,  is  either  E  or  the 
empty  string;  b  and  c  are  terminals  or  $. 

(1)  f(b},,  c)  = 

shift  if  b  ~  c  or  b  ~-.  c 

reduce  if  b  3>  c 

accept  if b  =  $,  7  =  E,  and  c  =  $ 
error  otherwise 

(2)  g(bTa,  x)  --  6  if b  <~ a 

g(bE  *  E,  x)  --  3  if b  <  * 
g(bE-+-  E , x ) - -   1  if b   ~  + 

g(bT(E ), x)  ----- 5  if b  <~  ( 

g(~,  x)  =  error  otherwise 

SEC. 5.4  OTHER  CLASSES OF  SHIFT-REDUCE  PARSABLE  GRAMMARS  441 

Thus  a~  would  make  the  following  sequence  of  moves  with  (a  --b a)  *  a 
as  input" 

[$,  (a  +  a)  *  aS,  e] ~  [$(,  a  +  a)  *  aS,  e] 

V-[$(a,  +  a)  *  aS,  e] 

[$(E,  +  a)  *  aS,  6] 
[$(E  + ,   a)  *  aS,  61 

[$(E  -+ a, )  *  aS,  6] 

[$(E  +  E, )  *  aS,  66] 
I--  IS(E, )  *  aS, 661] 

[$(E),  *  aS,  661] 
[--- [$E,  *  aS,  6615] 

[$E ,,  aS,  6615] 

[$E *  a,  $, 6615] 
]--- [$E *  E,  $,  66156] 

[$E,  $,  661563] 
t--  accept 

We  can  verify that  661563  is indeed  a  skeletal  right  parse  for  (a  q-- a)  *  a 
according  to  G.  We  can  view  this  skeletal  parse  as  a  tree  representation  of 

(a  +  a)  *  a,  as  shown  in  Fig.  5.21.  [ ]  

We  should  observe  that  it  is  possible  to  fill  in  the  skeletal  parse  tree  of 
Fig.  5.21  to  build  the  corresponding  tree  of Go.  But  in  a  practical  sense,  this 
is  not  often  necessary.  The  purpose  of  building  the  tree  is  for  translation, 

and  the  natural  translation  of E,  T and  F  in  G O is a  computer program  which 

computes  the  expression  derived  from  that  nonterminal.  Thus  when  produc- 
tion  E  ----~ T  or  T----~ F  is  applied,  the  translation  of  the  right  is  very  likely 

to  be  the  same  as  the  translation  of the  left. 

(  E  ) 
E  +  E I  I 

a  a  Fig. 5.21  Skeletal parse tree. 

442  ONE-PASS NO BACKTRACK PARSING  CHAP. 5 

Example  5.46 is a  special  case  of a  technique  that  works  for  many  gram- 
mars,  especially  those  that  define  languages  which  are  sets  of  arithmetic 
expressions.  Involved is the  construction  of a  new  grammar  with  all  nonter- 
minals  of the  old  grammar  replaced  by  one  nonterminal  and  single  produc- 
tions  deleted.  If  we  began  with  an  operator  precedence  grammar,  we  can 

always find  one parse  of each input  by a  shift-reduce  algorithm.  Quite  often 
the  new grammar  and its parser  are  sufficient for the purposes  of translation, 
and in such situations the  operator precedence parsing technique is a particu- 
larly  simple  and  efficient  one. 

DEFINITION 

Let G =  (N,  X, P,  S) be an operator grammar.  Define  Gs =  ([S},  X, P',  S), 
the  skeletal  grammar  for  G,  to  consist  of  all  productions  S----~ Xi  " "   Xm 
such  that  there is a  production A  ~  Y1  "" " Ym in P,  and  for  1 <  i <  m, 

(1)  Xt  =  Y, if Y,  ~  X. 
(2)  X~ =   S  if Yt  ~  N. 

However,  we  do  not  allow  S  ---~ S  in P'. 

We  should  warn  the  reader  that  L(G)  ~  L(G,) and  in  general L(G,) may 
contain  strings  not  in  L(G).  We  can  now  give  a  shift-reduce  algorithm  for 

operator  precedence  grammars. 

ALGORITHM 5.19 

Operator  precedence  parser. 
Input.  An  operator  precedence  grammar  G  =  (N,  X, P,  S). 

Output.  Shift-reduce  parsing  functions f  and  g  for  G,. 
Method.  Let  fl be  S  or  e. 

(1)  f(afl,  b)  =  shift if a  <  b  or  a  "  b. 

(2)  f(afl,  b)  =  reduce if a  ->  b. (3)  f($S,  $ ) =   accept. 

(4)  f ( a,  w)  =  error  otherwise. 
(5)  g(aflby,  w ) = / i f  

(a)  fl is  S  or  e; 
(b)  a  <  b; 
(c)  The  "  relation  holds  between  consecutive  terminal  symbols  of 

y, if any;  and 
(d)  Production  i  of G, is  S  --~ flby. 
(6)  g(g,  w)  =  error  otherwise.  D 

Example  5.46  is  an  example  of Algorithm  5.19  applied  to  G o .  To  show 
the  correctness  of Algorithm  5.19,  two  lemmas  are  needed. 

LEMMA 5.7 

If g  is  a  right-sentential  form  of an  operator  grammar,  then  g  does  not 
have  two  adjacent  nonterminals. 

SEC. 5.4  OTHER  CLASSES OF  SHIFT-REDUCE PARSABLE GRAMMARS  443 

Proof.  Elementary  induction  on  the  length  of  the  derivation  of  tz. 
LEMMA 5.8 

If  g  is  a  right-sentential  form  of  an  operator  grammar,  then  the  symbol 
appearing  immediately  to  the  left  of  the  handle  cannot  be  a  nonterminal. 

Proof.  If  it  were,  then  the  right-sentential  form  to  which  ~z is  reduced 
would  have  two  adjacent  nonterminals.  [Z] 

THEOREM 5.26 

Algorithm  5.19  parses  all  sentences  in  L(G). 
Proof.  By the  corollary  to  Theorem  5.25,  the  first  3>  and  the  previous  < 
correctly  isolate  a  handle.  Lemma  5.7 justifies  the  restriction  that  fl  be  only 
S  or  e  (rather  than  any  string  in  S*).  Lemma  5.8  justifies  inclusion  of  fl  in 
the  handle  in  rule  (5).  D 

5.4.4.  Floyd-Evans  Production  Language 

What  we  shall  next  discuss  is  not  another  parsing  algorithm,  but  rather 
a  language  in  which  deterministic  (nonbacktracking)  top-down  and  bottom- 

up  parsing  algorithms  can  be  described.  This  language  is  called  the  Floyd- 
Evans  production  language,  and  a  number  of  compilers  have  been  imple- 

mented  using  this  syntactic  metalanguage.  The  name  is  somewhat  of a  mis- 
nomer,  since  the  statements  of the  language  need  not  refer  to  any  particular 
productions  in  a  grammar.  A  program  written  in  Floyd-Evans  productions 
is a  specification  of a  parsing  algorithm  with  a  finite  state control  influencing 

decisions.'l" 

A  production  language  parser  is a  list of production  language  statements. 
Each  statement  has  a  label,  and  the  labels  can  be  considered  to  be  the  states 

of the  finite  control.  We  assume  that  no  two  statements  have  the  same  label. 
The  statements  act  on  an  input  string  and  a  pushdown  list  and  cause  a  right 
parse  to  be  constructed.  We  can  give  an  instantaneous  description  of  the 
parser  as  a  configuration  of the  form 

(q,  SXm  "'"  X1,  al  "'"  an$,  zt) 
where 

(1)  q  is the  label  of the  currently  active  statement; 

1"We might add that this is not the ultimate generalization  of shift-reduce algorithms. The LR(k) parsing algorithm uses a finite control and also  keeps extra information on its 
pushdown list.  In fact, a DPDT might really be considered  the most general kind of shift- reduce algorithm. However, as we saw in Section 3.4  the DPDT is not really constrained 

to parse by making reductions according to the grammar for which its output is a presumed parse,  as is the LR(k)  algorithm and  the algorithms  of Sections  5.3 and  5.4. 

Z 

444  ONE=PASS NO  BACKTRACK PARSING  CHAP.  5 

(2)  Xm  "'"  X1  is  the  contents  of the  pushdown  list  with  X1  on  top  ($  is 
used  as  a  bottom  of pushdown  list  marker); 

(3)  al  . . .   a n is  the  remaining  input  string  ($  is  also  used  as  a  right  end- 
marker  for  the  input  tape); 

(4)  zt  represents  the  output  of  the  parser  to  this  point,  presumably  the 
right  parse  of the  input  according  to  some  CFG. 

A  production  language  statement  is  of the  form 

(label)  : tzla  > ill(action)  *  (next  label) 
where  the  metasymbols  ---~ and  *  are  optional. 

Suppose  that  the  parser  is  in  configuration 

(L1, ~,~, ax,  n) 
and  statement  L1  is 

LI"  g l a   > Plemit  s ,   L2 
L1  says that  if the  string  on  top  of the pushdown  list is  ~ and  the  current 
input  symbol is a, then  replace  ~ by fl,  emit the  string s, move the input  head 

one  symbol  to  the  right  (indicated  by  the  presence  of  ,),  and  go  next  to 
statement  L2.  Thus  the  parser  would  enter  the  configuration  (L2, 7fl,  x,  ns). 
The symbol a may be e, in which case the current input symbol is not relevant, 
although  if the  *  is present,  an  input  symbol will be  shifted  anyway. 

If statement  L1  did  not  apply,  because  the  top  of the  pushdown  list  did 
not match  ~ or the current  input  symbol was not a, then the statement  imme- 
diately  following L1  on  the  list  of statements  must  be  applied  next. 

Both  labels  on  a  statement  are  optional  (although  we  assume  that  each 
statement  has  a  name  for  use in configurations).  If the  symbol  --~ is missing, 
then  the  pushdown  list is not  to  be  changed,  and  there  would  be  no  point  in 

having  fl  ~  e.  If  the  symbol  *  is  missing,  then  the  input  head  is  not  to  be 
moved.  If the  (next  label~ is missing,  the  next  statement  on the  list is always 

taken. 

Other  possible  actions  are  accept  and  error.  A  blank  in  the  action  field 
indicates that  no  action  is  to  be  taken  other  than  the  pattern  matching  and 
possible  reduction. 

Initially,  the  parser  is  in  configuration  (L,  $,  w$,  e),  where  w is  the  input 
string  to  be  parsed  and L  is a  designated  statement.  The  statements  are  then 
serially  checked  until  an  applicable  statement  is  found.  The  various  actions 
specified  by  this  statement  are  performed,  and  then  control  is  transferred  to 
the  statement  specified  by  the  next  label. 

The  parser  continues  until  an  error  or  accept  action  is  encountered.  The 
output  is valid  only when  the  accept  is  executed. 

SEC. 5.4  OTHER  CLASSES OF  SHIFT-REDUCE  PARSABLE GRAMMARS  445 

We  shall  discuss  production  language  in  the  context  of  shift-reduce 
parsing, but the reader should bear in mind that top-down parsing algorithms 
can  also  be  implemented  in  production  language.  There,  the  presumption 

is  that  we  can  write  a  :  a,a2a3  and  fl  :  a,Aa3  or  fl  :  o~,Aa3 a  if 
the  *  is  present.  Moreover,  A  ~  a2  is a  production  of the  grammar  we are 
attempting  to  parse,  and  the  output  s  is  just  the  number  of  production 

A  ---~ ~ z . Floyd-Evans  productions  can  be  modified to take  "semantic routines"  as 

actions.  Then, instead of emitting a parse,  the parser would perform a  syntax- 

directed  translation,  computing  the  translation  of  A  in  terms  of  the  trans- 
lations  of  the  various  components  of  a2.  Feldman  [1966] describes  such  a 

system. 

Example  5.47 

We  shall  construct  a  production  language  parser  for  the  grammar  Go 
with  the  productions 

(I)  E--~ E-1- T  (2)  E ~  T 
(3)  r---~ T ,  F  (4)  r---~ F 
(5)  F ~  (E)  (6)  F ---~ a 

The  symbol  ~  is  used  as  a  match  for  any  symbol.  It  is  presumed  to  repre- 

sent  the  same  symbol  on  both  sides  of  the  arrow.  L11  is  the  initial  state- 
ment. 

L0:  ( 

LI:  a 
L2:  T ,   F ~  

L3:  F..~ 
L4:  T  * 

L5:  E-t-TO 
L6:  T ~ 
L7:  E  -q- 
L8:  (E) 

L9:  $E$ 
L10:  $ 
L l l :  

#  ~ ( #  
#  ---~ y #  

> T ~  

T ~  
> T * ~  
----~ E ~  

> E ~  
#  > E + #  

#  ---~ F# 

> 

> 

emit  6 

emit  3 
emit  4 

emit  1 
emit  2 

emit  5 
accept 
error 

,L0 

L4 
,L0 

L7 

,L0 
,L2 

,LO 
The  parser  would  go  through  the  following  configurations  under  input (a+  a),a" 

446  ONE-PASS NO  BACKTRACK PARSING  CHAP.  5 

[L11,  $, (a  -t- a)  *  aS,  e]  ~- [LO, $(, a  +  a)  *  aS, e] 

R  [LO, $(a,  +  a)  *  aS, e] 
R  [L1,  $(a,  +  a)  *  aS,  e] 
R  [L2,  $(F +,  a)  *  aS,  6] 
R  [L3,  $(F +,  a) *  aS,  6] 
R  [L4,  $(T + ,   a) *  aS,  64] 
R  [L5,  $(T + ,   a) *  aS,  64] 

R  [L6,  $(T /,  a)  *  aS,  64] 
R  [L7,  $(E / ,  a) *  aS,  642] 
F- [L0,  $(E /  a, )  *  aS,  642] 
R  ILl,  $(E /  a, )  *  aS,  642] 
1- [L2,  $(E +  F),  *  aS,  6426] 
R  [L3,  $(E +  F),  *  aS,  6426] 
R  [L4,  $(E +  T),  *  aS,  64264] 

r--- [L5,  $(E /  T),  *  aS,  64264] 
R  [L7,  $(E),,  aS, 642641] 
R  [L8,  $(E),  *  aS,  642641] 
R  [L2,  $F, ,  aS,  6426415] 
l-  [L3,  SF ,,  aS,  6426415] 
I-- [L4,  ST ,,  aS,  64264154] 
R  [L0,  ST,  a,  $, 64264154] 
l--- [L1,  ST,  a,  $, 64264154] 
R  [L2,  ST,  F$,  e, 642641546] 
1-- [L4,  $T$,  e, 6426415463] 
1--- [L5,  $T$,  e, 6426415463] 
R  [L6,  $T$,  e, 6426415463] 

t-- [L7,  $E$,  e, 64264154632] 
R  [L8,  $E$,  e, 64264154632] 
t-  [L9,  $E$, e, 64264154632] 
R  accept  [Z 

It  should  be  observed  that  a  Floyd-Evans  parser  can  be  simulated  by a  DPDT.  Thus  each  Floyd-Evans  parser  recognizes  a  deterministic  CFL. 

However,  the language  recognized  may not  be  the  language  of the  grammar 

SEC.  5.4  OTHER  CLASSES OF  SHIFT-REDUCE PARSABLE GRAMMARS  447 

for  which the  Floyd-Evans  parser is constructing  a  parse.  This phenomenon 
occurs because the flow of control in the statements may cause certain reduc- 
tions  to  be  overlooked. 

Example 5.48 

Let  G consist  of the  productions 

(1)  S:  > aS 
(2)  S  > bS 
(3)  S  > a 

L(G)  =  (a  -t-- b)*a.  The  following  sequence  of  statements  parses  words  in b*a  according  to  G,  but  accepts  no  other  words: 

L0: 
LI:  a 
L2:  b 
L3:  $ 
L4:  aS 
L5:  bS 

L6:  $S 

L7: 

#  ~ # 1   , 

>  S  [  emit  3 ! 

[  error 
>  S  l  emit  1 

S  [  emit  2 
$  >  $S$[  accept  * 

I  error 

With  input  ba,  the  parser  makes  the following  moves" 

[L0,  $, ba$,  e]  }--- [L1,  $b, aS,  e] 

[L2,  $b, aS,  e] 
}-  [L0,  $b, a$,  e] 
l-  [L1,  $ba,  $,  e] 

[L4,  $bS,  $,  3] 
t--- [L5,  $bS,  $,  3] 

F-- [L4, $S,  $,  32] 
t-- [L5,  $S,  $,  32] 

[L6,  $S,  $,  32] 

L4 
L0 

L4 
L4 

The input is accepted at statement L6.  However, with input aa, the follow- 
ing sequence  of moves  is made" 

448  ONE-PASS NO  BACKTRACK PARSING  CHAP.  5 

[L0,  $, aa$,  e]  F- [L1,  Sa, aS, e] 

[L4,  $S, aS,  3] 
[L5,  $S, aS,  3] 
1-- [L6,  $S, aS,  3] 

[L7,  $S, aS,  3] 

An  error  is declared  at L7,  even though  aa  is in L(G). There  is  nothing  mysterious  going  on  in  Example  5.48.  Production 

language  programs  are  not  tied  to  the  grammar  that  they  are  parsing  in the  way that  the  other  parsing  algorithms  of  this  chapter  are  tied  to  their 
grammars.  [Z 

In  Chapter  7 we  shall  provide  an  algorithm for  mechanically generating a  Floyd-Evans  production  language  parser  for  a  uniquely  invertible  weak 

precedence  grammar. 

5.4.5.  Chapter  Summary 

The  diagram  in  Fig.  5.22  gives  the  hierarchy  of  grammars  encountered in  this  chapter. 

All  containments  in  Fig.  5.22  can  be  shown  to  be  proper.  Those  inclu- sions  which  have  not  been  proved  in  this  chapter  are  left  for  the  Exercises. 
The  inclusion  of  LL  grammars  in  the  LR  grammars  is  proved  in  Chapter 8. 

Insofar  as  the  classes  of languages  that  are  generated  by  these  classes  of grammars are concerned,  we can demonstrate  the following results: 

(1)  The  class  of languages  generated  by  each  of the  following  classes  of grammars is precisely the class  of deterministic context-free languages" 

(a)  LR.  (b)  LR(1). (c)  BRC.  (d)  (1,  1)-BRC. 
(e)  MSP.  (f)  Simple  MSP. (g)  Uniquely invertible  (2,  1)  (h)  Floyd-Evans  parsable. 

precedence. 
(2)  The  class  of  LL  languages  is  a  proper  subset  of  the  deterministic CFL's. 

(3)  The  uniquely  invertible  weak  precedence  grammars  generate  exactly the  simple precedence languages,  which  are 

(a)  A  proper  subset  of the  deterministic  CFL's  and 
(b)  Incommensurate  with  the  LL languages. 
(4)  The  class  of  languages  generated  by  operator  precedence  grammars is the  same as that  generated  by the  uniquely invertible  operator precedence 

grammars. This class of languages is properly contained in the class of simple precedence  languages. 

SEC. 5.4  OTHER  CLASSES OF  SHIFT-REDUCE  PARSABLE GRAMMARS  449 

Context-free grammars 

~ U ~ m b i g u o u s ~  
Floyd,- Evans ~ "   CFG's parsable  I  Operator 

R  precedence 

BRC  LR(I) ! I 

MSP 

Simple MSP Uniquely 
invertible extended 
p r e c e d e n c /  

Uniquely invertible 

weak precedence 

Fig.  5.22 

Simple precedence 
Hierarchy of grammars. 

Many  of these  results  on  languages  will  be  found  in  Chapter  8. The  reader  may  well  ask  which  class  of  grammars  is  best  suited  for 
describing  programming  languages  and  which  parsing  technique  is  best. 
There  is  no  clear'cut  answer  to  such  a  question.  The  simplest  class  of gram- 
mars  may  often  require  manipulating  a  given  grammar  in  order  to  make  it 

fall  into  that  class.  Often  the  grammar  becomes  unnatural  and  unsuitable 

for  use in  a  syntax-directed  translation  scheme. The  LL(1)  grammars  are  particularly  attractive  for  practical  use.  For 

each  LL(1)  grammar  we  can  find  a  parser  which  is  small,  fast,  and  pro- 

duces  a  left  parse,  which  is  advantageous  for  translation  purposes.  How- 
ever, there are  some  disadvantages.  An  LL(1) grammar  for  a  given language can be unnatural and difficult to construct.  Moreover, not every deterministic 

CFL  has  an  LL  grammar,  let  alone  an  LL(1)  grammar,  as  we  shall  see  in 

Chapter  8. Operator  precedence techniques  have  been  used  in  several compilers,  are 

450  ONE-PASS NO BACKTRACK  PARSING  CHAP. 5 

easy to implement and work quite efficiently. The (1, 1)-precedence  grammars 
are also easy to parse, but obtaining a (1, 1)-precedence grammar for a language 

often  requires  the  addition  of many  single productions  of  the  form  A  ---~ X 
to  make  the  precedence  relations  disjoint.  Also,  there  are  many  deter- 
ministic  CFL's  for  which  no  uniquely  invertible  simple  or  weak  precedence 

grammar  exists. 

The  LR(1)  technique  presented  in  this  chapter  closely  follows  Knuth's 
original  work.  The  resulting  parsers  can  be  extremely  large.  However,  the 
techniques  to  be  presented  in  Chapter  7  produce  LR(1)  parsers  whose  size 
and  operating  speed  are  competitive  with  precedence  parsers  for  a  wide 
variety  o f   programming  languages.  See  Lalonde  et  al.  [1971]  for  some 

empirical  results.  Since  the  LR(1)  grammars  embrace  a  large  class  of gram- 
mars,  LR(1)  parsing  techniques  are  also  attractive. 

Finally  we  should  point  out  that  it  is  often  possible  to  improve  the  per- 
formance  of any given parsing technique  in a  specific application.  In  Chapter 

7  we  shall  discuss  some  methods  which  can  be  used  to  reduce  the  size  and 
increase  the  speed  of parsers. 

EXERCI SES 
5.4.1.  Give a shift reduce parsing algorithm based  on the (1, 0)-BRC technique for  G1  of Example  5.41. 

5.4.2.  Which of the following grammars are (1, 1)-BRC? (a) 

S--~  aAIB A ---~ 0A1 la 

B ----~ 0B1 lb. (b)  S  --.  aA l bB 
,4  ~  0A ll O l  B ---, 0Bli 01. 
(c) E----, E /  TI E -   TIT 

T---, T .   F! T/F! F 

F---.  (E)I--  EIa. 

DEFINITION 

A  proper  CFG  G =  (N, Z, P, S)  is  an  (m, n)-bounded context  (BC) grammar  if the  three  conditions 

(1)  $mS'$" ~  t~Alyl  ==~ 0tlf1171 in  the  augmented  grammar, 

(2)  $'~S'$" ~  ot2A272 ~  ot2f1272 =  ~3fllY3 in the augmented gram- mar, and 

(3)  The  last  m  symbols of  etl  and  ~t3 agree  and  the  first  n  symbols of 71  and  73 agree 
imply that  ~3A173  =  0~A2~2. 

EXERCISES  451 
5.4.3.  Show that every (m, n)-BC grammar is an  (m, n)-BRC grammar. 

5.4.4.  Give  a  shift-reduce parsing algorithm for BC grammars. 

5.4.5.  Give an example of a  BRC  grammar that  is not BC. 
5.4.6.  Show  that  every  uniquely  invertible  extended  precedence  grammar  is 

BRC. 
5.4.7.  Show that  every BRC  grammar is extended precedence (not necessarily 

uniquely invertible,  of course). 

5.4.8.  For those grammars  of Exercise 5.4.2 which are (1, 1)-BRC,  give shift- reduce parsing algorithms and implement them with decision trees. 

5.4.9.  Prove the  "only if" portion of Lemma 5.6. 
5.4.10.  Prove Theorem 5.22. 
5.4.11.  Which of the following grammars are simple MSP  grammars ? 

(a)  Go. (b)  S--~  A[B 

A ~  0All01 B~  2B1  !1. 
(c)  S - - ,   A I B  A  -~  OAllOl 

B--~  OB1 I1. 
(d)  S--~  A I B  A ~  0A1 [01 

B  --~  01B1  [01. 
5.4.12.  Show  that  every  uniquely  invertible  weak  precedence  grammar  is  a 

simple MSP  grammar. 

5.4.13.  Is  every  (m, n; m, n)-MSP  grammar  an  (m, n)-BRC  grammar .9 

5.4.14.  Prove Theorem  5.24. 
5.4.15.  Are the following grammars  operator precedence ? 

(a)  The  grammar of Exercise 5.4.2(b). 

(b)  S  ~  if B  then  S  else  5' 

S  ~  if B  then  S 
S---~ s B---~  Bo r   b 

B---~ b. (c)  5' --~  if B  then  St  else  S 

S  --~  if B  then  S 
$1  --~  if B  then  $1  else  $1 

S - - ,   s 
S1----).  S B---~  Bor   b 

B - - l b .  The  intention  in  (b)  and  (c)is  that  the  terminal  symbols  are  if,  then, 
else,  or, b, and s. 

452  ONE-PASS NO BACKTRACK PARSING  CHAP.  5 

5.4.16.  Give  the  skeletal  grammars  for  the  grammars  of Exercise  5.4.15. 
5.4.17.  Give  shift-reduce  parsing  functions  for  those  grammars  of  Exercise 

5.4.15  which are  operator  precedence. 

5.4.18.  Prove Theorem  5.25. 
5.4.19.  Show  that  the  skeletal  grammar  G,  is  uniquely  invertible  for  every 

operator grammar  G. 

*5.4.20.  Show  that  every  operator  precedence  language  has  an  operator  prece- 

dence grammar  with  no  single productions. 

"5.4.21.  Show that every operator precedence language has  a  uniquely invertible 

operator precedence grammar. 

5.4.22.  Give  production  language  parsers  for  the  grammars  of  Exercise  5.4.2. 
5.4.23.  Show  that  every  BRC  grammar  has  a  Floyd-Evans  production  lan- 

guage parser. 

5.4.24.  Show  that  every  LL(k)  grammar  has  a  production  language  parser 

(generating left parses). 

**5.4.25.  Show that  it is  undecidable whether  a  grammar is 

(a)  BRC. (b)  BC. 

(c)  MSP. 
5.4.26.  Prove  that  a  grammar  G  =  (N,  E, P,  S)  is  simple  MSP  if  and  only  if 

it  is  a  weak  precedence  grammar  and  if A  ---~ ~  and  B  ~  0~ are  in  P, 
A  ~  B,  then I(A)  ~  I(B)  =  ~ .  

*5.4.27.  Suppose  we  relax  the  condition  on  an  operator  grammar  that  it  be 

proper and have no e-productions.  Show that under this new definition, 
L  is  an  operator  precedence  language  if  and  only  if  L -   [e}  is  an 

operator precedence language under  our  definition. 

5.4.28.  Extend Domolki's algorithm as presented in the Exercises of Section 4.1 

to  carry along  information  on  the pushdown  list  so that  it  can  be used 
to  parse BRC  grammars. 

DEFINITION 

We  can  generalize  the  idea  of  operator  precedence  to  utilize  for 
our  parsing  a  set  of  symbols  including  all  the  terminals  and,  perhaps, 
some  of  the  nonterminals  as  well.  Let  G  =  (N,  ~, P,  S)  be  a  proper 

CFG  with  no  e-production  and  T  a  subset  of  N  u  E,  with  E  ~  T. 

Let  V denote N  u  ~.  We  say  that  G  is  a  T-canonical  grammar  if 

(1)  For  each right  side of a  production,  say ocXYfl,  if X is not  in  T, then  Y is in  E,  and 

(2) If A  is in  T and  A  =~  ~,  then  tz has  a  symbol of  T. 
Thus  a  E-canonical  grammar  is  the  same  as  an  operator  grammar. 
If G  is  a  T-canonical  grammar,  we say  that  T  is  a  token  set  for  G. 

EXERCISES  453 
If  G  is  a  T-canonical  grammar,  we  define  T-canonical  precedence 
relations,  < ,   --~-, and  3>  on  T  U  {$}, as  follows: 

(1)  If  there  is  a  production  A  ---~ ~zX[JY?,  X  and  Y  are  in  T,  and 
/~  is  either  e  or  in  ( V -   T),  then  X  ~  Y. 

+ (2)  If  A  ----~ ~zXBt~  is  in P  and  B  ==~ ? YS,  where  X  and  Y are  in  T 

and  7  is  either  e  or  in  V -   T,  then  X  <  Y. 

(3)  Let  A  ~  oclBoc2Ztx3  be  in P,  where  ~z2 is  either  e  or  a  symbol 
of  V --  T.  Suppose  that Z  ~ *  flla[32,  where  fll  is either  e  or in  V --  T, 
and  a  is  in  X.  (Note  that  this  derivation  must  be  zero  steps  if  tx2 ~  e, 
by  the  T-canonical  grammar  definition.)  Suppose  also  that  there  is  a 
derivation 

B  ~  YiC1J1  =~" 7172C25251  ~  "'"  =~  71  "'"  ?k - lCk -l Jk -1   "'"  01 

=~  71  "'"  7k XJ k   "'"  51, 
where  the  C's  are replaced  at  each  step,  the  J's  are  all  in  {el  u  V -   T, 

and  X  is  in  T.  Then  we  say  that  X  .>  a. 

(4)  If  S  ~  aXfl  and  a  is  in  {e)  u  V - T ,   then  $ < X .   I f f l i s i n  
[el  kJ  V - -   T,  then  X->   $. 

Note  that  if  T  =  Z,  we  have  defined  the  operator  precedence  rela- 
tions,  and  if  T  =  V,  we  have  the  Wirth-Weber  relations. 

Example 5,49 

Consider  Go,  with  token  set  A  =  {F, a,  (,),  + ,   ,].  We  find  (..-~), 
since  (E)  is  a  right  side,  and  E  is  not  a  token.  We  have  -t-  <  *,  since 
there  is  a  right  side  E  --t-- T,  and  a  derivation  T ~  T .   F,  and  T  is  not 

a  token.  Also,  -4-  .>  +,   since  there  is  a  right  side  E  -4- T  and  a  deriva- 
tion  E  ~  E  +  T,  and  T  is  not  a  token.  The  A-canonical  relations  for 

Go  are  shown  in  Fig.  5.23. 

a  +  *  (  )  F  $ 

* >  .>  .>  .> 

<.  .>  <.  <.  .>  <.  .> 

<.  <. 

<.  <.  <5  <.  *  <. 

, 

)  .>  .>  .> 

F  ->  ->  "> 

$  <-  <.  <.  <.  <. 

.> 

.> 

Fig.  5.23  A-canonical  precedence 
relations. 

454  ONE-PASS NO BACKTRACK PARSING  CHAP.  5 

5.4.29.  Find  all  token  sets for  Go. 
5.4.30.  Show  that  E  is  a  token  set  for  G  =  (N,  ~, P, S)  if and  only  if  G  is  an 

operator  precedence  grammar.  Show  that  N  w  *  is  a  token  set  if and 
only if  G  is  a  precedence  grammar. 

DEFINITION 

Let  G  =  (N,  ZE, P,  S)  be  a  T-canonical  grammar.  The  T-skeletal 
grammar  for  G  is  formed  by  replacing  all  instances  of  symbols  in 

V -   T  by a  new  symbol  So  and  deleting  the  production  So  ---~ So  if it appears. 

Example 5.50 

Let  A  be  as  in  Example  5.49.  The  A-skeletal  grammar  for  Go  is 

S o   >S0  + S o I S 0 . F [ F  

F  > (S0) [a  D 

5.4.31.  Give  a  shift-reduce  parsing  algorithm  for  T-canonical  precedence 

grammars  whose  T-skeletal  grammar  is  uniquely  invertible.  Parses  in 
the  skeletal  grammar  are  produced,  of  course. 

Research Problem 

5.4.32.  Develop  transformations  which  can  be  used  to  make  grammars  BRC, 

simple precedence,  or  operator precedence. 

Programming Exercises 

5.4.33.  Write  a  program  that  tests  whether  a  given  grammar  is  an  operator 

precedence grammar. 

5.4.34.  Write  a  program  that  constructs  an  operator  precedence  parser  for  an 

operator  precedence  grammar. 

5.4.35.  Find  an  operator  precedence  grammar  for  one  of the  languages  in  the 

Appendix  and  then  construct  an  operator  precedence  parser  for  that language. 

5.4.36.  Write  a  program  that  constructs  a  bounded-right-context  parser  for  a 

grammar  G if G is (1,  1)-BRC. 

5.4.37.  Write  a  program  that  constructs  a  simple  mixed  strategy  precedence 

parser  for  a  grammar  G  if  G  is  simple  MSP. 

5.4.38.  Define  a  programming  language  centered  around  the  Floyd-Evans 

production  language.  Construct  a  compiler  for  this  programming language. 

BIBLIOGRAPHIC  NOTES  455 
B I B L I O G R A P H I C   NO TE S  
Various  precedence-oriented  parsing  techniques  were  employed  in  the  earliest 
compilers.  The  formalization  of  operator  precedence  is  due  to  Floyd  [1963]. 

Bounded context and bounded-right-context parsing methods were also defined  in the  early  1960's.  Most  of the  early development  of  bounded  context  parsing  and 

variants  of  it  is  reported  by  Eickel  et  al.  [1963], Floyd  [1963,  1964a, 1964b], 

Graham  [1964], Irons  [1964], and  Paul  [1962]. The  definition  of  bounded-right-context  grammar  here  is  equivalent  to  that 

given  by  Floyd [1964a]. An  algorithm  for  constructing  parsers  for  certain  classes of BRC grammars is given by Loeckx [1970]. An extension of Domolki's algorithm 
to  BRC  grammars  is  given  by  Wise  [1971]. Mixed  strategy  precedence  was  introduced  by  McKeeman  and  used  by 

McKeeman et al.  [I970]  as the basis of the XPL compiler writing system. 

Production  language  was  first  introduced  by  Floyd  [1961] and  later  modified by Evans  [1964]. Feldman  [1966] used  it  as the  basis  of a  compiler writing system 

called  Formal  Semantic  Language  (FSL)  by permitting general  semantic routines 

in  the  (action)  field of each production  language  statement. T-canonical  precedence  was  defined  by  Gray  and  Harrison  [1969]. Example 

5.47 is from  Hopgood  [1969]. 

6 

LIMITED  BACKTRACK 

PARSING  ALGORITHMS 

In  this  chapter  we  shall  discuss  several  parsing  algorithms  which,  like the  general top-down  and  bottom-up  algorithms  in  Section 4.1,  may involve 
backtracking.  However,  in  the  algorithms  of  this  chapter  the  amount  of backtracking  that  can  occur  is  limited.  As  a  consequence,  the  parsing 
algorithms to be presented here are more economical than those in Chapter 4. 
Nevertheless, these algorithms should not be used in situations where a deter- ministic nonbacktracking algorithm will suffice. 

In the first section we shall discuss two high-level languages in which top- 
down  parsing  algorithms  with  restricted  backtracking  capabilities  can  be written.  These languages,  called TDPL  and  GTDPL,  are  capable  of specify- 

ing recognizers for all deterministic context-free languages with an endmarker 
and,  because  of  the  restricted  backtracking,  even  some  non-context-free 
languages,  but  (probably)  not  all  context-free  languages.  We  shall  then discuss  a  method  of  constructing,  for  a  large  class  of  CFG's,  precedence- 

oriented  bottom-up  parsing  algorithms,  which  allow  a  limited  amount  of backtracking. 

6.1.  LIMITED  BACKTRACK  TOP-DOWN  PARSING 

In this  section we shall define two formalisms for limited backtrack pars- 
ing  algorithms  that  create  parse  trees  top-down,  exhaustively  trying  all alternates  for  each  nonterminal,  until  one  alternate  has  been  found  which 

derives a  prefix  of the  remaining  input.  Once  such  an  alternate  is found,  no 

other  alternates  will  be  tried.  Of course,  the  "wrong"  prefix  may  have  been 
found,  and in this case the  algorithm will  not backtrack but will fail.  Fortu- 

456 

SEC. 6.1  LIMITED  BACKTRACK  TOP-DOWN  PARSING  457 

nately,  this  aspect  of  the  algorithm  is  rarely  a  serious  problem  in  practical 

situations,  provided  we  order  the  alternates  so  that  the  longest  is  tried  first. 

We  shall  show  relationships  between  the  two  formalisms,  discuss  their 
implementation,  and  then  treat  them  briefly  as  mechanisms  which  define 

classes  of languages.  We  shall  discover  that  the  classes  of languages  defined 
are  different  from  the  class  of CFL's. 

6.1.1.  TDPL 

Consider the general  top-down parsing algorithm of Section 4.1.  Suppose 
we decide  to generate  a string  from  a  nonterminal A  and  that  ~1, ~z2, * * *,  ~, 
are  the  alternates  for  A.  Suppose  further  that  in  a  correct  parse  of  the 
input,  A  derives  some  prefix  x  of  the  remaining  input,  starting  with  the 

derivation  A =-~ ~m,  1 ~  rn <  n,  but  that  A ==~ ~j, for j  <  m,  does  not  lead lm  --  --  lm 
to  a  correct  parse. 

The  top-down  parsing  algorithm  in  Chapter  4  would  try  the  alternates 
~1, 0~2, * * *, 0Cm in  order.  After  each  ~zj failed,  j  <  m,  the  input  pointer  would 
be  reset,  and  a  new  attempt  would  be  made,  using  0~j+1. This  new  attempt 
would  be  made  regardless  of whether ~zj derived  a  terminal  string  which was 

a  prefix  of the  remaining  input. 

Here  we  shall  consider  a  parsing  technique  in  which  nonterminals  are 
treated  as  string-matching  procedures.  To  illustrate  this  technique  suppose 

that  a ~ . . .   a,.is  the  input  string  and  that  we  have  generated  a  partial  left 
parse  successfully matching  the  first  i -   1 input  symbols.  If  nonterminal  A 

is  to  be  expanded  next,  then  the  nonterminal  A  can  be  "called"  as  a  proce- 

dure,  with  input  position  i  as  an  argument.  If  A  derives  a  terminal  string 
that  is  a  prefix  of  a t af + ~ .. . a .,  then  A  is  said  to  succeed  starting  at  input 
position  i.  Otherwise,  A fails  at  position  i. 

These  procedures  call  themselves  recursively.  If  A  were  called  in  this 
manner,  A  itself  would  call  the  nonterminals  of its  first  alternate,  ~ .   If  0~ 
failed,  then  A  would  replace  the  input  pointer  to  where  it  was  when  A  was 
first called,  and then A would call 0cz, and so forth.  If ~j succeeds in matching 

at at+~ '"  ak,  then  A  returns  to  the  procedure  that  called  it  and  advances 
the  input  pointer  to  position  k  -c-  1. 

The  difference  between  the  current  algorithm  and  Algorithm  4.1  is  that 
should the latter fail to find a complete parse in which ~  derives a t * * * ak, then 
it  will  backtrack  and  try  derivations  beginning  with  productions  A ~  0c~/I, 
A --, ~j+2, and so forth, possibly deriving a  different prefix of a~.  * * a, from A. 

Our  algorithm  will  not  do  so.  Once  it  has  found  that  ~j  derives  a  prefix  of 
the  input  and  that  the  subsequent  derivation  fails  to  match  the  input,  our 

parsing  algorithm  returns  to  the  procedure  that  called  A,  reporting  failure. 

The  algorithm  will  act  as  if A  can  derive  no  prefix  whatsoever  of  at . . . a , , .  
Thus  our  algorithm  may  miss  some  parses  and  may  not  even  recognize 

458  LIMITED BACKTRACK PARSING ALGORITHMS  CHAP.  6 

the  same  language  as  its  underlying  CF G  defines.  We  shall  therefore  not  tie 

our  algorithm  to  a  particular  CFG,  but  will  treat  it  as  a  formalism  for  lan- 
guage  definition  and syntactic  analysis  in  its  own  right. 

Let  us  consider  a  concrete  example.  If 

S  > Ac 
A  >alab 

are  productions  and  the  alternates  are  taken  in  the  order  shown,  then  the 
limited  backtrackalgorithm  will  not  recognize  the  sentence  abc.  The  non- 
terminal  S  called  at  input  position  1 will  call  A  at  input  position  1.  Using 

the  first  alternate,  A  reports  success  and  moves the  input  pointer  to  position 
2.  However,  c does  not  match  the  second  input  symbol,  so  S  reports  failure 

starting  at  input  position  1.  Since  A  reported  success  the  first  time  it  was 
called,  it  will  not  be  called  to  try  the  second  alternate.  Note  that  we  can 
avoid  this  difficulty  by  writing  the  alternates  for  A  as 

A  >  abla 
We  shall  now  describe  the  "top-down  parsing  language,"  TDPL,  which 
can  be  used  to  describe  parsing  procedures  of  this  nature.  A  statement  (or 
rule)  of TDPL  is  a  string  of one  of the  following  forms" 

A  >  BC/D 
or 

A  > a  

where  A,  B,  C,  and  D  are  nonterminal  symbols  and  a  is  a  terminal  symbol, 
the  empty  string,  or  a  special  symbol f  (for failure). 

DEFINITION 

A  TDPL program  P  is  a  4-tuple  (N,  X, R,  S),  where 

(1)  N  and  X  are  finite  disjoint  sets  of nonterminals  and  terminals, 

(2)  R  is  a  sequence  of TDPL  statements  such  that  for  each  A  in  N  there 
is  at  most  one  statement  with  A  to  the  left  of the  arrow,  and 

(3)  S  in  N  is  the  start  symbol. 

A  TDPL program  can be likened to  a grammar in a  special normal form. 
A  statement  of the form A  ~  BC/D  is representative  of the two productions 
A  ---~ BC  and  A  ---~ D,  where  the  former  is  always  to  be  tried  first.  A  state- 
ment  of the  form  A  ----~ a  represents  a  production  of that  form  when  a  ~  X 

or a  =  e.  If a  =  f,  then  the  nonterminal  A has  a  special  meaning,  which  will 
be  described  later. 

sEc. 6.1  LIMITED BACKTRACK TOP-DOWN PARSING  459 

Alternatively,  we  can  describe  a  TDPL  program  as  a  set  of procedures 
(the  nonterminals)  which  are  called  recursively with  certain  inputs.  The  out- 
come  of  a  call  will  either  be  failure  (no  prefix  of  the  input  is  matched  or 
recognized)  or success  (some prefix  of the  input  is matched). 

The  following  sequence  of procedure  calls  results  from  a  call  of a  state- 
ment  of the  form  A  --~  BC/D,  with  input  w: 

(1)  First,  A  calls  B  with  input  w.  If  w =  xx '  and  B  matches  x,  then  B 
reports  success.  A  then  calls  C  with  input  x'. 

(a)  If  x ' =   y z   and  C  matches  y,  then  C  reports  success.  A  then 

returns  success and  reports  that  it has matched  the prefix x y   of w. 
(b)  If  C  does  not  match  any  prefix  of  x',  then  C  reports  failure.  A 

then calls  D  with input  w. Note  that  the  success  of B is undone  in this  case. 

(2)  If,  when  A  calls  B  with  input  w)  B  cannot  match  any  prefix  of  w, 
then  B  reports  failure.  A  then  calls  D  with  input  w. 

(3)  If D  has  been  called  with  input  w =  uv  and  D  matches  u,  a  prefix  of 
w,  then  D  reports  success.  A  then  returns  success  and  reports  that  it  has matched  the  prefix  u  of w. 

(4)  If  D  has  been  called  with  input  w and  D  cannot  match  any prefix  of 
w,  then  D  reports  failure.  A  then  reports  failure. 

Note  that  D  gets  called  unless  both  B  and  C  succeed.  We  shall  later 
explore  a  parsing  system in  which  D  is  called  only if B  fails.  Note  also  that 
if  both  B  and  C  succeed,  then  the  alternate  D  can  never  be  called.  This 
feature  distinguishes TDPL from the  general  top-down  parsing  algorithm  of Chapter  4. 

The special statements A  ---~ a, A  ~  e, and A  ---~ f a r e   handled as follows: 
(1)  If A  ----~ a  is  the  rule  for  A  with  a  ~  X~ and  A  is  called  on  an  input 
beginning  with  a,  then  A  succeeds  and  matches  this  a.  Otherwise,  A  fails. 

(2)  If A  ---~ e is the  rule  for  A,  then  A  succeeds  whenever it  is  called  and always matches  the  empty  string. 

(3)  If A  ~  f  is the  rule,  A  fails  whenever it  is  called. 
We  shall  now  formalize  the  notion  of a  nonterminal  "acting  on  an  input string. 

DEFINITION 

Let  P  =  (N,  T, R,  S)  be  a  TDPL  program.  We  define  a  set  of relations ~ 
=%.p from  nonterminals  to  pairs  of the  form  (x ~" y, r),  where  x  and  y  are  in 
X~* and  r  is  either  s  (for success)  or f  (for failure).  The  metasymbol  ~' is used 
to  indicate  the  position  of the  current  input  symbol.  We  shall  drop  the  sub- 
script P  wherever  possible. 

460  LIMITED BACKTRACK PARSING ALGORITHMS  CHAP. 6 

(1)  If A  ~  e  is  in  R,  then  A  =~- (r" w, s)  for  all  w  ~  Z*. 
(2)  If A  - - i f   is  in  R,  then  A  =~. (~ w ,f)  for  all  w  ~  Z*. 

(3)  If A  --~ a  is  in  R,  with  a  ~  Z,  then 

(a)  A  =~  (alx,  s)  for  all  x  ~  Z*. 
(b)  A  =~  (I Y, f )   for  all  those  y  ~  Z*  (including  e)  which  do  not begin  with  the  symbol  a. 

(4)  Let  A  --,  BC/D  be  in  R. 

(a)  A  m__n_~l (xy r'z, s)  if 

(i)  B  ~  (x r'yz, s)  and 
(ii)  C  ~  (y  l z, s). 

t (b)  A  =-~ (u l v, s),  with  i  =  m  -q- n  -if- p  -q-  1,  if 

(i)  s ~   (x l y,  s), 
(ii)  C  ~  (T" Y, f),  and 
(iii)  a  ~  (u l'v, s),  where  uv  =  xy. 

(c)  A  ~  (~" xy, f),  with  i  =  m  -b  n  -k p  if-  1,  if 

(i)  B=~  (x ~ y,  s), 
(ii)  C  ~  (T' Y, f),  and 
(iii)  D  =~  (I xy, f). 

(d)  A m___~_~? (x I' Y, s),  if 

(i)  B  =~ (~ xy, f),  and 
(ii)  D  =~,. (x r. y,  s). 
(e)  A '~----~1 (~' x, f),  if 

(i)  B  ~  (r' x, f),  and 
(ii)  D  ~  (~ x, f). 
(5)  The  relations  ~  do  not  hold  except  when  required  by  (1)-(4). 

Case  (4a)  takes  care  of  the  case  in  which  B  and  C  both  succeed.  In  (4b) 
and  (4c),  B  succeeds  but  C  fails.  In  (4d)  and  (4e),  B  fails.  In  the  last  four cases,  D  is  called  and  alternately  succeeds  and  fails.  Note  that  the  integer 

above  the  arrow  indicates  the  number  of  "calls"  which  were  made  before 
the  outcome  is  reached.  Observe  also  that  if  A  ~  (x T' Y, f),  then  x  =  e. That  is,  failure  always  resets  the  input  pointer  to  where  it  was  at  the  begin- 

ning  of the  call. 

We  define  A  ~  (x r' y, r)  if  and  only  if  A  ~  (x I' y,  r)  for  some  n  >  1 
The  language  defined  by  P,  denoted  L(P),  is  [w l S  =~  (w  ~, s)  and  w  ~  Z*}. 

SEC. 6.1  LIMITED  BACKTRACK  TOP-DOWN  PARSING  461 

Example 6.1 

Let  P  be  the  TDPL  program  (~S, A, B, C},  ~a, b}, R,  S),  where  R  is  the 
sequence  of  statements 

S  ~ AB/C 
A = > a  

B  ~  CB/A 

C----~  b 

Let  us  investigate  the  action  of P  on  the  input  string  aba  using  the  rela- 
tions  defined  above.  To  begin,  since  S  ----~ AB/C  is  the  rule  for  S,  S  calls  A 
with  input  aba.  A  recognizes  the  first  input  symbol  and  returns  success. 

Using  part  (3)  of the  previous  definition  we  can  write  A  =~  (a I ba, s).  Then, 
S  calls  B  with  input  ba.  Since  B  ---~ CB/A  is the  rule  for  B,  we  must  examine 
the  behavior  of C  on  ba.  We find that  C matches  b  and  returns  success.  Using 

(3)  we  write  C  =~  (b l a, s). 

Then  B  calls  itself  recursively with  input  a.  However,  C fails  on  a  and  so 
C  ~  (~" a, f).  B  then  calls  A  with  input  a.  Since  A  matches  a,  A  ~  (a  I, s). 
Since  A  succeeds,  the  second  call  of  B  succeeds.  Using  rule  (4d)  we  write B=~,. (a  ~',s). 

Returning  to  the  first  call of B  on  input  ba,  both  C  and  B have  succeeded, 
so  this  call  of  B  succeeds  and  we  can  write  B  ~  (ba  I, s)  using  rule  (4a). 

Now  returning  to  the  call  of  S,  both  A  and  B  have  succeeded.  Thus,  S 
matches  aba and  returns  success.  Using  rule  (4a)  we can write  S  ~  (aba  I, s). 
Thus,  aba  is  in  L(P). 

It  is  not  difficult  to  show  that  L(P)  =  ab*a  +  b.  [~] 

An  important  property  of  a  TDPL  is  that  the  outcome  of  any  program 
on  a  given  input  is  unique.  We  prove  this  in  the  following  lemma. 

LEMMA 6.1 

Suppose  that  P  =  (N,  E,  R,  S)  is  a  TDP L  program  such  that  for  some 
A  ~  N , A   =~,.(x 1 ~yl,  r l ) a n d A   =~(x2Iy2,  r2),where  x l y l = x 2 y z = w   ~  ~*. 

Then  we  must  have  x l  =  x2, y l  =  Y2,  and  r l =  r2. 

Proof  The  proof  is  a  simple  induction  on  the  minimum  of  n~  and  n 2, 
which  we  can  take  without  loss  of generality  to  be  na. 

Basis.  nl  =  1.  Then  the  rule  for  A  is A  ~  a,  a  ~  e,  or  A  --, f  The  con- 
clusion  is  immediate. 

Induction.  Assume  the  conclusion  for  n  <  n~,  and  let  n~  >  1.  Let  the 

462  LIMITED  BACKTRACK  PARSING  ALGORITHMS  CHAP.  6 

rule  for  A  be  A  ---~ BC]D.  Suppose  that  for  i  =  1  and  2,  A  ~  (xt ~" y~, ri) 
was  formed  by  rule  (4)  from  B  ~  (ui ~" %  tt)  and  (possibly)  C  =g  (u~ I v~, t~) 

and/or  D  ~  (u~' ~" vt", t'/).  Then  m 1 <  n 1,  so  the  inductive  hypothesis  applies 
to  give ua  =  u2, vl  =  v2, and  tl  =  t2.  N ow   two  cases,  depending  on  the  value 

of  t t,  have  to  be  considered.  tt  tt  tot /  tt 

Case  1:  t~  t2  = f  Then  since  l~  <  na,  we have  u~  =  u2,  =  %,  and 
t'~' =  t~'.  Since  x~  u~,  Yi  vi,  and  r~ =  t~  for  i  =  1  and  2  in  this  case, 
the  desired  result  follows. 

Case  2:  t 1  t~  =  s.  Then  ' ' =  --  u~v~  v~  for  i - -   1  and  2.  Since  k~  <  nl,  we 
may  conclude  that  u'a  =  u'  '  ' z,  v',  =  v~,  and  t',  =  t,.  If  t'l  =  s,  then  xi  =  u~u' ? 
yt  =  v~,  and  r,.  =  s  for  i - -   1  and  2.  We  reach  the  desired  conclusion.  If 

~t and  ~t t'l  =  f,  the  argument  proceeds  with  u,.  v,.  as  in  case  1.  U] 

It  should  also  be noted  that  a  TD P L  program  need  not  have  a  response  to 
every  input.  For  example,  any  program  having  the  rule  S  ~  S S /S ,   where  S 
is  the  start  symbol,  will  not  recognize  any  sentence  (that  is,  the  ~  relation  is 

empty). 

The  notation  we  have  used  for  a  T D P L   to  this  point  was  designed  for 
ease  of presentation.  In  practical  situations  it is  desirable  to  use  more  general 
rules.  For  this  purpose,  we  now  introduce  extended  TD P L   rules  and  define 
their  meaning  in  terms  of the  basic  rules" 

(1)  We  take  the  rule  A  ~  B C   to  stand  for  the  pair  of  rules  A  ~  B C /D  
and  D  ---~ f,  where  D  is  a  new  symbol. 

(2)  We  take  the  rule  A  ---~ B /C   to  stand  for  the  pair  of rules  A  ---~ B D /C  
and  D  ---~ e. 

(3)  We  take  the  rule  A  --~ B  to  stand  for  the  rules  A  ~  B C   and  C  ---~ e. 
(4)  We  take  the  rule  A  ---~ AtA2  " -   A,,  n  >  2,  to  stand  for  the  set  of 
rules  A  ~  A1B1,  B1  ~  AzB2,  .  .  .  , B._3  ~  A._2B._2,  B._z  ~  A . _ iA . . 

(5)  We  take  the  rule  A  ~  0~,/~2/"'/oc.,  where  the  oc's are  strings  of 
nonterminals,  to  stand  for  the  set  of  rules  A  ~  B~/C 1, C~  ~  Bz/Cz . .. ..  

C,_3  ~  B,_2/C,-2,  C,-z   ---" B ,_ i /B ,,   and  B1 ~  0cl, B2  ~  0cz . . . .   , B,  ---, 0c,. 
If  n - - 2 ,   these  rules  reduce  to  A- - ,   B1/Bz,  Ba  ~  oct,  and  Bz--~  ~ .   For 

1 <  i  ~  n  if  I~]  --  1,  we  can  let  B,. be  ~  and  eliminate  the  rule  B~ ~  ~. 

(6)  We  take  rule  A  ---,  ~ / 0 % / . . . / ~ , ,   where  the  ~'s  are  strings  of  non- 
terminals  and  terminals,  to  stand  for  the  set  of rules  A  ----, ~'~/0~/.../0c',,  and 
X~ ~  a  for  each  terminal  a,  where  ~'~ is  0~t with  each  terminal  a  replaced  by xo. 

Henceforth  we  shall  allow  extended  rules  of this  type  in  TD P L   programs. 
The  definitions  above  provide  a  mechanical  way  of  constructing  an  equiva- 

lent  T D P L   program  that  meets  the  original  definition. 

These  extended  rules  have  natural  meanings.  For  example,  if  A  has  the 
rule  A  ~  Y1 Y2  "'"  Y,,  then  A  succeeds  if  and  only  if  Y1  succeeds  at  the 

SEC. 6.1  LIMITED  BACKTRACK  TOP-DOWN  PARSING  463 
input  position  where  A  is  called,  Y2 succeeds  where  Y~ left  off,  Y3 succeeds 
where  Y2 left  off,  and  so  forth. Likewise,  if  A  has  the  rule  A ~  ~1/~2/""/~,,  then  A  succeeds  if  and 

only if ~a succeeds where A is called,  or if ~  fails, and ~2 succeeds where  A  is 

called,  and  so  forth. 

Example 6.2 

Consider the extended TDPL program P  =  ({E, F,  T}, {a, (,),  + ,   ,}, R, S), 
where  R  consists  of 

E  > T + E / T  

T  > F , T / F  
F  ~  (E)/a 

'Ihe  reader  should  convince  himself  that  L ( P ) =   L(Go),  where  G O is  our 

standard  grammar  for  arithmetic  expressions. To  convert P  to  standard  form,  we first  apply  rule  (6),  introducing  non- 

terminals  Xo, X~, X~, X+,  and  X,.  The  rules  become 

E  > TX+E/T 

T  > FX, T/F 
F  - - +   XcEX>/X,, 
Xa  >a 

x<  >( 

x,  >) 
x+ ---~  + 

X,  > ,  

By rule  (5),  the  first  rule  is  replaced  by  E  --~ B i/ T   and  B~ ---~ TX+E.  By rule 
(4),  B1 ---~ TX+E  is  replaced  by  B~ ---~ TB2  and  B2  ---~ X+E.  Then,  these  are 
replaced  by  B 1 ---~ TBz/D,  Bz  ~  X+E/D,  and  D----,f  Rule  E  ~  B i/ T   is replaced  by 

E---,  B a C/T  and  C  ~  e.  The  entire  set  of  rules  constructed  is 

Xa  >a  D -   > f  

X(  >(  E----~.B1C/T 

X>  > )  B1  >  TB2/D 
X+  > q-  B2  ~  X+E/D 

X,  > *  T  ~  B3C/F 

C  > e  B3  ~  FB4/D 

B4  > X,T/D 

F  >BsC/X~ 
B5  > X~B6/D 

B6  ~  > EX)/D 

464  LIMITED  BACKTRACK  PARSING  ALGORITHMS  CHAP.  6 

We  have  simplified  the  rules  by  identifying  each  nonterminal  X  that  has 
rule  X  ---~ e with  C  and each nonterminal  Y that  has rule  Y ----~ f  with  D. 

Whenever  a  TDPL  program  recognizes  a  sentence  w,  we  can  build  a 
"parse  tree"  for  that  sentence  top-down  by  tracing  through  the  sequence  of 
TDPL  statements  executed in recognizing  w. The  interior nodes  of this parse 
tree  correspond  to  nonterminals  which  are  called  during  the  execution  of 
the  program  and  which  report  success. 

ALGORITHM 6.1 

Derivation  tree  from  the  execution  of a  TDPL  program. 
Input.  A  TDPL  program  P  =  (N,  E,  R,  S)  and  a  sentence  w in  X*  such 
that  S  ~  (w r', s). 

output.  A  derivation  tree for  w. 
Method.  The heart  of the algorithm is a  recursive routine  buildtree  which 
takes  as  argument  a  statement  of the  form  A ~  (x ~' y, s)  and  builds  a  tree 
whose  root  is labeled  A  and  whose  frontier is x.  Routine  buildtree is initially 
called  with  the  statement  S  =%- (w  ~', s)  as  argument. 

Routine  buildtree:  Let  A ~  (x  r" Y, s)  be  input  to  the  routine. 

(1)  If the  rule for A  is  A  ---, a  or A  ---~ e,  then  create  a  node  with  label  A 
and  one  direct  descendant,  labeled  a  or  e, respectively.  Halt. 

(2)  If the  rule  for  A  is  A  ~  BC/D  and  we  can  write  x  =  x tx2  such  that ml  m~ 
B  =:~ (x 1 r" x2y,  s)  and  c  =:~ (x 21 Y, s),  create  a  node  labeled  A.  Execute 
routine  buildtree  with  argument  B  ~  (x 1 I xzY,  s),  and  then  with  argument mi 

C =:~ (x2  I Y, s).  Attach  the  resulting  trees  to  the  node  labeled  A,  so  that 
the roots  of the  trees  resulting from the  first and  second calls  are  the  left and 
right  direct  descendants  of the  node.  Halt. 

(3)  If the  rule  for  A  is A  ----~ BC/D  but  (2)  does  not  hold,  then  it  must be m3 
that  D  :=~ (x  ~' y, s).  Call  routine  buildtree  with  this  argument  and  make 
the  root  of  the  resulting  tree  the  lone  direct  descendant  of  a  created  node 
labeled  A.  Halt. 

Note  that  routine buildtree calls itself recursively only with  smaller values 
of m,  so  Algorithm  6.1  must  terminate. 

Example  6.3 

Let us use Algorithm 6.1  to  construct a parse tree generated  by the TDPL 
program  P  of  Example  6.1  for  the  input  sentence  aba. 

We  initially  call  routine  buildtree  with  the  statement  S  ~  (aba  I,  s)  as 
argument.  The  rule  S  ~  AB /C   succeeds  because  A  and  B  each  succeed, 

SEC. 6.1  LIMITED BACKTRACK TOP-DOWN PARSING  465 
recognizing  a  and  ba, respectively.  We  then  call  routine  buildtree  twice,  first 

with  argument  A  =~. (a  I, s)  and  then  with  argument  B ~  (ha I, s).  Thus 
the  tree  begins  as  shown  in  Fig.  6.I(a).  A  succeeds  directly  on  a,  so the  node 

labeled  A  is  given  one  descendant  labeled  a.  B  succeeds  because  its  rule  is B ~  CB/A 

and  C  and  B  succeed  on  b  and  a,  respectively.  Thus  the  tree 
grows  to  that  in  Fig.  6.1 (b). 

A/s   

S 

i  / \  a  C  B 
(a)  (b) 

s 

A  C  B I B  I /  

b  A I 

a 
(c) 
Fig. 6.1  Construction from parse tree in TDPL. 

C  succeeds  directly  on  b,  so  the  node  labeled  C  gets  one  descendant 
labeled b. B succeeds on a  because A  succeeds, so B gets a  descendant labeled 
A,  and  that  node  gets  a  descendant  labeled  a.  The  entire  tree  is  shown  in 

Fig.  6.1(c).  D 

We  note  that  if  the  set  of TDPL  rules  is  treated  as  a  parsing  program, 
then  whenever  a  nonterminal  succeeds,  a  translation  (which  may  later  have 
to  be  "canceled")can  be produced  for the portion  of input that it recognizes, 

in terms  of the translations  of its  "descendants"  in the  sense of the parse  tree 
just  described.  This  method  of  translation  is  similar  to  the  syntax-directed 

translation  for  context-free  grammars,  and  we shall  have  more  to  say  about 
this  in  Chapter  9. 

466  LIMITED BACKTRACK PARSING ALGORITHMS  CHAP. 6 

It is impossible  to  "prove"  Algorithm  6.1  correct,  since,  as we mentioned, 
it  itself  serves  as  the  constructive  definition  of  a  parse  tree  for  a  T DPL 
program.  However,  it  is  straightforward  to  show  that  the  frontier  of  the 
parse  tree  is  the  input.  One  shows  by  induction  on  the  number  of  calls  of 
routine  buildtree  that  when  called  with  argument  A  ~  (x  ~" y,  s)  the  result  is 

a  tree  with  frontier  x. 

Some  of the  successful  outcomes  will  be  "canceled."  That  is,  if we  have + A  ~  (xy  ~" z,  s) 

and  the  rule  for  A  is  A  --~ BC/D,  we  may  find  that  B ==~ (x  I yz,  s) 
but  that  C  *~ (r' yz, f).  Then  the  relationship  B  ~  (x  F" yz,  s)  and 
all the  successful  recognitions  that  served  to  build  up  B  ~  (x  ~" yz,  s)  are  not 
really  involved  in  defining  the  parse  tree  for  the  input  (except  in  a  negative 
way).  These  Successful  recognitions  are  not  reflected  in  the  parse  tree,  nor  is 
routine  buiidtree  Called with  argument  B  ~  (x  I yz,  s).  But  all  those  success- 
ful  recognitions  which  ultimately  contribute  to  the  successful  recognition  of 
the  input  are  included,  i 

6.1.2.  TDPL and  Deterministic  Context-Free  Languages 

It  can  be  quite  difficult  to  determine  what  language  is defined  by  a TDPL 
program.  To  get  some  feel for  the  power  of TDPL  programs,  we  shall  prove 
that  every  deterministic  CFL  with  an  endmarker  has  a  TDPL  program 
recognizing  it.  Moreover,  the  parse  trees  for  that  TDPL  program  are  closely 
related  to  the  parse  trees  from  the  "natural"  C FG   constructed  by  Lemma 
2.26  from  a  D PDA   for  the  language.  The  following  lemma  will  be  used  to 
simplify  the  representation  of  a  DPDA  in  this  section. 

LEMMA  6.2 

If  L  =  L,(M~)  for  D PDA   M~,  then  L  =  L,(M)  for  some  DPDA   M 
which  never  increases  the  length  of its  pushdown  list  by  more  than  1 on  any 

single  move. 

Proof.  A  proof  was  requested  in  Exercise  2.5.3.  Informally,  replace 
a  move  which  rewrites  Z  as  X ~ . . .   X,,  k  >  2,  by  moves  which  replace  Z 
by  YuXk,  Yk  by  Yk_~X'k_~,...,  Y4  by  Y3X3,  and  Y3  by  XiX 2.  The  Y's  are 

new  pushdown  symbols  and  the  pushdown  top  is  on  the  left.  D 

LEMMA  6.3 

If L  is  a  deterministic  CFL  and  $ is  a  new  symbol,  then  L$  =  L,(M)  for 
some  D PDA   M. 

Proof  The  proof  is  a  simple  extension  of  Lemma  2.22.  Let  L  =  L(M1). 
Then  M  simulates  M1,  keeping  the  next  input  symbol  in  its  finite  control. 
M  erases  its  pushdown  list  if M1  enters  a  final  state  and  $  is  the  next  input 

symbol.  No  other  moves  are  possible  or  needed,  so  M  is  deterministic.  D 

SEC. 6.1  LIMITED BACKTRACK  TOP-DOWN PARSING  467 

THEOREM  6.1 

Let  M  =  (Q, 2~, F,  ~,  q0,  Z0, F)  be  a  DPDA   with L , ( M )   =  L.  Then there 
exists  a  TDPL  program  P  such  that  L  =  L(P). 

Proof.  Assume that M  satisfies Lemma 6.2. We construct P  =  (N, 2~, R, S), 
an  extended  TDPL  program,  such  that L(P)  =  Le(M).  N  consists  of 

(1)  The  symbol  S; 
(2)  Symbols  of the  form  [qZp],  where  q  and p  are  in Q,  and  Z  ~  F;  and 

(3)  Symbols of the form [qZpL,  where q, Z,  and p  are as in (2), and  a  ~  ~. 

The  intention  is  that  a  call  of  the  nonterminal  [qZp]  will  succeed  and 
recognize  string  w if" and  only if (q, w, Z)[--~-. (p, e, e), and  [qZp] will fail under 
all  other  conditions,  including  the  case  where  (q, w, Z)[--~-(p',  e, e)  for  some 
p'  ~  p.  A  call  of  the  nonterminal  [qZpL  recognizes  a  string  w  if  and  only if 

(q, aw, Z)[-~--(p,  e,e).  The  rules  of P  are  defined  as  follows: 

(1)  The  rule  for  S  is  S  ~  [qoZoqo]/[qoZoq~]/"'"/[qoZoqA,  where  q0,  q~, 

* . . ,   qk  are  all  the  states  in  Q. 

(2)  If ,~(q, e, Z)  =  (p, e),  then  the  rule  for  [qZp]  is  [qZp]  ~  e,  and  for  all 
p'  ~  p,  [qZp']  --, f  is  a  rule. 

(3)  If  di(q, e, Z)  =  (p, X),  then  the  rule  for  [qZr]  is  [qZr] ~  [pXr]  for  all 
r i n Q .  (4)  If  ~(q,  e,  Z ) =   (p, 

X Y ),   then  for  each  r  in  Q,  the  rule  for  [qZr]  is 

[qZr]  ~  [pXqo][qoYr]/[pXqi][q~Yr]/  . . .   /[pXqk][qkYr],  where  qo,  qt,  .  . .  , qk 

are  all  the  states  in  Q. (5)  If  c~(q, e , z )   is  empty,  let  a ~ , . . . , a ~   be  the  symbols  in  X  for 

which  c~(q, a,  Z)  ~  ~ .   Then  for  r  ~  Q,  the  rule  for  nonterminal  [qZr]  is 

[qZr]  ----, al[qZr]aJa2[qZrL/."/al[qZr]o,.  If l =  0, the rule is [qZr]  ---~ f. 

(6)  If  ~(q,  a, Z)  =  (p,  e)  for  a  ~  ~,  then  we  have  rule  [qZpL---~  e,  and 
for p'  ~  p,  we  have  rule  [qZp']~  ---~ f. 

(7)  If cS(q, a, Z)  =  (p,  X),  then for  each  r  ~  Q, we have a  rule  of the form 
[qZr]~--~  [pXr]. 

(8)  If  c~(q, a , Z ) =   (p,  XY ) ,  then  for  each  r  ~  Q,  we  have  the  rule 

[qZr]a  ~  [pXqo][qo Yr]/[pXq ~][q ~ Yr]/  . . . /[pXqk][qkYr]. 

We  observe  that  because  M  is  deterministic,  these  definitions  are  consis- 
tent;  no  member of N  has more than  one rule.  We shall now show the follow- 

ing" 

(6.1.1)  [qZp]  ~  (w ~" x,  s),  for  any  x,  if and  only  if (q,  wx,  Z)[  /  (p, x,  e) 

(6.1.2)  If  (q,  wx,  Z)l-~-  (p,  x,  e),  then  for  all  p'  ~  p,  [qZp']  ~  (1 wx, f )  

We  shall  prove  (6.1.2)  and  the  "if"  portion  of  (6.1.1)  simultaneously  by 
induction  on  the  number  of  moves  made  by  M  going  from  configura- 

tion  (q,  wx,  Z)  to  (p,  x,  e).  If  one  move  is  made,  then  d~(q, a, Z)  =  (p,  e), 

i: 

468  LIMITED  BACKTRACK  PARSING  ALGORITHMS  CHAP. 6 
where  a  is  e  or  the  first  symbol  of  wx.  By  rule  (2)  or  rules  (5)  and  (6), [qZp]  ~  (a l y, s), 

where  ay  =  wx,  and  [qZp']  -----~ (r wx, f )   for  all  p ' ~   p. Thus  the  basis  is  proved. 

Suppose  that  the  result  is true  for  numbers  of moves  fewer  than  the  num- 
ber  required  to  go  from  configuration  (q,  wx, Z)  to  (p,  x,  e).  Let  w =  aw'  for 
some  a  ~  ~  U  [e}. There  are  two  cases  to  consider. 

Case  1:  The  first  move  is  (q,  wx, Z ) ~   (q',  w'x,  X)  for  some  X  in  F. 
By the  inductive  hypothesis,  [q'Xp]  ~  (w'  r" x,  s) and  [qZp']  ~  (I  w'x, f )   for 
p'  ~  p.  Thus  by  rule  (3)  and  rules  (5)  and  (7),  we  have  [qXp] =-~ (w  r" x, s) 

and  [qXp']  ~  (r" wx,  f)  for  all  p'  ~  p.  The  extended  rules  of  P  should  be 
first  translated  into  rules  of  the  original  type  to  prove  these  contentions 
rigorously. 

Case 2:  For  some  X  and  Y  in  I',  we  have,  assuming  that  w ' =   yw", (q,  wx, Z)  ~--- (q', w' x, X Y  ) I--e- (q",  w" x,  Y)I--~- (p,  x,  e), 

where  the  pushdown 
list  always  has  at  least  two  symbols  between  configurations  (q',  w'x,  XY) 
and  (q",  w"x,  Y).  By  the  inductive  hypothesis,  [q'Xq"]  ~  (y  I w"x,  s)  and [q"Yp]  ~  (w"  I x,  s). 

Also,  if p'  ~  q",  then  [q'Xp']  ~  (I  w'x, f).  Suppose 
first  that  a  =  c.  If We examine  rule  (4) and  use  the  definition  of the extended 
TD PL   statements,  we  see  that  every  sequence  of the  form  [q'Xp'][p'Yp]  fails 
for  p'  -~ q".  However,  [q'Xq"][q"Yp]  succeeds  and  so  [qZp] ~  (w  [" x,  s) as  desired. 

We  further  note  that  if  p'  -~ p,  then  [q"Yp']  =~. (F" w"x, f),  so  that  all 
terms  [q'Xp"][p"Yp']  fail.  (If p " ~   q",  then  [q'Xp"]  fails,  and  if p " =   q", 
then  [p"Yp']  fails.)  Thus,  [qZp']  ~  (r" wx, f )   for  p'  ~  p.  The  case  in  which 
a  ~  E  is  handled  similarly,  using  rules  (5)  and  (8). 

We  must  now  show  the  "only  if"  portion  of  (6.1.1).  If [qZp] ~  (w ~" x, s) 
then  [qZp] ~  (w I x, s)  for  some  n.f  We  prove  the  result  by  induction  on  n. 

If  n  =  1,  then  rule  (2)  must  have  been  used,  and  the  result  is  elementary. 

Suppose  that  it  true  for  n  <  no,  and  let  [qZp] ~  (w ~" x,  s). 

Case  1:  The  rule  for  [qZp] is  [qZp]---,  [q'Xp].  Then  J(q,  e, Z ) =   (p, X), 
and  [q'Xp]  ~  (w  I x,  s),  where  nl  <  no.  By  the  inductive  hypothesis, (q',  wx,  X)l.-~- (p,  x,  e). 

Thus,  (q,  wx, Z)I -e- (p,  x,  e). 

Case 2:  The  rule  for  [qZp] is  [qZp] ~  [q'Xqo][qoYp]/  ""  /[q'Xqk][qkYp]. 
Then  we  can  write  w =  w'w"  such  that  for  some p',  [q'Xp']  ~  (w'  I w"x,  s) 
and  [p'Yp]  ~  (w"  r" x,  s),  where  nl  and  nz  are  less  than  n 0.  By  hypothesis, (q',  w'w"x,  XY)[---(p',  w"x,  Y)[---(p, 

x,  e).  By  rule  (4),  ~(q,  e,  Z)  = (q', XY).  Thus,  (q,  wx, Z)  ~  (p,  x,  e). 

tThe step counting must be performed by converting the extended rules to the original form. 

SEC. 6.1  LIMITED BACKTRACK TOP-DOWN PARSING  469 

Case  3:  The  rule  for  [qZp]  is  defined  by rule  (5).  That  is,  O(q,  e, Z)  =  ~ .  
Then  it is not  possible  that  w  =  e, so let w  =  aw'.  If the rule for  nonterminal 

[qZp]o  is  [qZp]a ~  e, we know  that  O(q,  a, Z)  =  (p,  e),  so  w'  =  e,  w =  a,  and 

(q,  w, Z)  ~  (p,  e,  e).  The  situations  in  which  the  rule  for  [qZp],  is  defined  by 
(7)  or  (8)are  handled  analogously  to  cases  1  and  2,  respectively.  We  omit 
these  considerations. 

To  complete  the  proof  of  the  theorem,  we  note  that  S  ~  (w l,  s)  if  and 

only  if for  some p,  [qoZoP]  ~  (w  I, s).  By  (6.1.1),  [qoZoP]  ~  (w  l,  s)  if and 
only  if (q0,  w, Z0)[-~-- (p,  e,  e).  Thus,  L(P)  =  L,(M). 

COROLLARY 

If  L  is  a  deterministic  CF L  and  $  a  new  symbol,  then  L$  is  a  T DP L  
language. 

Proof.  From  Lemma  6.3.  D 

6,1.3,  A  Generalization  of  TDPL 

We  note  that  if  we  have  a  statement  A  ~  BC/D  in  TDPL,  then  D  is 
called  if either  B  or  C  fails.  There  is  no  way  to  cause  the  flow  of  control  to 

differ  in  the  cases  in  which  B  fails  or  in  which  B  succeeds  and  C  fails.  To 

overcome  this  defect  we  shall  define  another  parsing  language,  which  we  call 

G TD P L   (generalized  TDPL).  A  program  in  G T D P L   consists  of  a  sequence 
of  statements  of  one  of the  forms 

(1)  A  ---, B[C,  D] 
(2)  A  --~  a 
(3)  A  ~  e 

(4)  A  --~ f 

The  intuitive  meaning  of the  statement  A  ~  B[C,  D]  is  that  if A  is  called,  it 

calls  B.  If  B  succeeds,  C  is  called.  If B  fails,  D  is  called  at  the  point  on  the 
input  where  A  was  called.  The  outcome  of  A  is  the  outcome  of  C  or  D, 

whichever  gets  called.  Note  that  this  arrangement  differs  from  that  of  the 

T DPL   statement  A  ~  BC/D,  where  D  gets  called  if B  succeeds  but  C  fails. 

Statements  of types  (2),  (3),  and  (4)  have  the  same  meaning  as  in  TDPL. 
We  formalize  the  meaning  of  GT DP L   programs  as  follows. 

DEFINITION 

A  GTDPL  program  is  a  4-tuple  P  =  (N, ~,  R, S),  where  N,  ~,  and  S  are 
as  for  a  T DPL   program  and  R  is  a  list of  rules  of  the  forms  A  ~  B[C,  D], 
A  ~  a,  or  A  ~  f  Here,  A,  B,  C,  and  D  are  in  N,  a  is  in  ~  U  [e},  and f  is 

the  failure  metasymbol,  as  in  the TDP L  program.  There  is  at  most  one  rule 
with  any  particular  A  to  the  left  of the  arrow. 

We  define  relations  ~  as  for  the  TDP L  program" 

470  LIMITED BACKTRACK PARSING  ALGORITHMS  CHAP. 6 

(1)  If  A  has  rule  A  ---~ a,  for  a  in  Z  U  [e},  then  A  ~  (a  ~" w, s)  for  all 
w  ~  Z*,  and  A ~  (I  w, f )   for  all  w ~  Z*  which  do  not  have  prefix  a. 

(2)  If A  has  rule  A  ~  f,  then  A  =g. (~' w, f )   for  all  w ~  Z*. 
(3)  If A  has  rule  A  ----~ B[C,  D],  then  the  following  hold" 

(a)  If  B  =~  (w  I xy,  s)  and  C ~  (x  r" y,  s),  then  A  ,,___~i (wx  I Y, s). 
(b)  If B  ~  (w  r" x, s) and  C  ~  (F" x, f),  then  A"-----~2 (l  wx, f). 
(c)  If  B  =~  (~ wx, f )   and  D  ~  (w r' x,  s),  then  A  m____~)(W ~' X, S). 
(d)  If  B ~  (I  w, f )   and  D  ~  (r' w, f),   then  A m---e-~ ~ (I  w, f). 

We say that A  ~  (x  ~" y,  r)  if A  ~  (x  [' y,  r)  for some  n  ~  1. The language 
defined by P,  denoted  L(P), is  the  set  {w l S  =~  (w  I, s)}. 

Example 6.4 

Let  P  be  a  GTDPL  program  with  rules 

s  A[C,  El 
C-  > S[B, E] 
A  >a 

B  >b 
E  >e 

We  claim  that  P  recognizes  {a"b"]n ~  0}.  We  can  show  by  simultaneous induction  on  n  that  S  ~  (a"b"  ~" x,  s)  and  C =~ (a"b "+I I x,  s)  for  all  x  and 
n.  For  example,  with  input  aabb,  we  make  the  following  sequence  of observations" 

A  ~  ([" bb, f )  

E  ~  (~" bb, s) 

S  ~  (I bb, s) 
B  ___k. (b  I b,  s) 
c  (b I b, s) 
A  ~  (a I bb, s) 

S  ---2-.,. (ab  I b, s) 
B  (b  s) 

SEC. 6.1  LIMITED BACKTRACK TOP-DOWN PARSING  471 

C ~  (abb r', s) 
A  ~  (a r" abb, s) 

s  ~  (aabb l, s)  D 
The  next  example  is  a  G T D PL   program  that  defines  a  non-context-free 
language. 

Example 6.5 

We construct a G T D PL  program to recognize the non-CFL [0"1"2" In >  1}. 
By  Example  6.4,  we  know  how  to  write  rules  that  check  whether  the 
string  at  a  certain  point  begins  with 0"1"  or  1"2". Our  strategy  will  be  to  first 
check that  the input  has  a prefix of the form 0 ~ 1"2 for m  ~  0. If not,  we shall 
arrange  it  so  that  acceptance  cannot  occur.  If so,  we  shall  arrange  an  inter- 
mediate  failure  outcome  that  causes  the  input  to  be  reconsidered  from 
the  beginning.  We  shall  then  check  that  the  input  is  of  the  form  0'l J2 J. 

Thus  both  tests  will  be  met  if and  only if the  input  is  of  the  form  0"1"2"  for 
n ~ l .  

We  shall  need  nonterminals  that  recognize  a  single  terminal  or  cause 
immediate  success  or  failure;  let  us  list  them  first" 

(1)  X  > 0 

(2)  Y-  3 i  
(3)  Z  ~ 2 
(4)  E  >e 
(5)  r  > f  

We  can  utilize  the  program  of Example  6.4  to  recognize  0mlm2 by a non- 
terminal  Sa.  The  rules  associated  with  Sa  are 

(6)  S ,   ~ A[Z, Z] 
(7)  A  > X[B, El 
(8)  B - - /   A[Y, E] 

Rules  (7), (8),  (1),  (2),  and  (4) correspond  to  those  of Example  6.4 exactly. 
Rule  (6)  assures  that  S~ will recognize  what  A  recognizes  (0mlm), followed  by 
2.  Note  that  A  always  succeeds,  so  the  rule  for  $1  could  be  $1 ---~ A[Z, W] 
for  any  W. 

Next,  we  must  write  rules  that  recognize  0*  followed  by  1 ~2 J for  some j. 
The  following  rules  suffice" 

472  LIMITED  BACKTRACK  PARSING  ALGORITHMS  CHAP.  6 

(9)  $2  > X[S2,  C] 
(10)  C  ~ Y[D,  E] 

(11)  D  > C[Z,  El 

Rules  (10),  (11),  (2),  (3),  and  (4)  correspond  to  Example  6.4,  and  C 
recognizes  1 J2 j.  The  rule for $2 works  as follows.  As  long as  there  is a  prefix 

of O's  on  the  input,  $2  recognizes  one  of them  and  calls  itself further  along 
the  input.  When  X  fails,  i.e.,  the  input  pointer  has  shifted  over  the  O's, C  is 

called  and  recognizes  a prefix  of the  form  t j 2 j.  Note  that  C  always  succeeds, 

so  S z always  succeeds. We  must  now  put  the  subprograms  for  S~  and  $2  together.  We  first 

create  a  nonterrL  aal  $3,  which  never  consumes  any  input,  but  succeeds  or 
fails  as  S~ fails or  succeeds.  The  rule for $3  is 

(12)  $3  > S,[F,  E] 
Note  that  if S~  succeeds,  $3  will call F,  which  must  fail  and  retract  the  input 
pointer to the place where S~ was called.  If S~ fails, $3 calls E, which succeeds. 

Thus,  $3  uses  no  input  in  any  case.  Now  we  can  let  S  be  the  start  symbol, 
with  rule 

(13)  S  > Sa[F, $21 
If $1  succeeds,  then  $3 fails  and  $2 is called  at the  beginning  of the  input. 
Thus,  S  succeeds  whenever  $1  and  $2  succeed  on  the  same  input.  If $1  fails, 
then  S 3 succeeds,  so S fails.  If $1  succeeds but Sz fails,  then  S  also fails.  Thus 
the  program  recognizes  {0"1"2" In ~  1},  which  is  the  intersection  of  the  sets 
recognized  by  S~  and  $2.  Hence  there  are  languages  which  are  not  context- 
free which  can  be  defined  by  GTDPL  programs.  The  same  is true  for  TDPL 
programs, incidentally.  (See  Exercise  6.1.1.)  [Z] 

We  shall  now  investigate  some  properties  of  GTDPL  programs.  The 
following lemma  is  analogous  to  Lemma  6.1. 

LEMMA 6.4 

+ Let  P  =  (N,  X, R,  S)  be  any  GTDPL  program.  If  A  ==~ (xr'y,  r l)  and 

A  ~  (u  ~" v: r2),  where  xy  =  uv,  then  x  =  u,  y  =  v,  and  r l  =  r2. 

Proof.  Exercise.  5 

We  now  establish  two  theorems  about  GTDPL  programs.  First,  the class 
of TDPL  definable  languages  is  contained  in  the  class  of  GTDPL  definable 

languages.  Second,  every  language  defined  by  a  GTDPL  program  can  be 

recognized  in  linear  time  on  a  reasonable  random  access  machine. 

SEC. 6.1  LIMITED BACKTRACK TOP-DOWN PARSING  473 
THEOREM  6.2 

Every  TDPL  definable  language  is  a  GTDPL  definable  language. 
Proof  Let  L  =  L(P)  for  a  TDPL  program  P  =  (N,  Z,  R,  S).  We  define 
the  GTDPL  program  P '   =  (N', Z, R',  S),  where  R'  is  defined  as  follows" 

(1)  If A  ---~ e  is  in  R,  add  A  ---~ e  to  R'. 
(2)  If A  ----~ a  is  in  R,  add  A  ----~ a  to  R'. 
(3)  If A  ----~ f  is  in  R,  add  A  ~  f  to  R'. 
(4)  Create  nonterminals  E  and  F  and  add  rules  E  ~  e  and  F  ---~ f  to  R'. 
(Note  that  other  nonterminals  with  the  same  rules  can  be  identified  with 
these.) 

(5)  If A  ~  BC/D  is  in  R,  add 

A  > A'[E,  D] 
A'  > B[C, F] 

to  R',  where  A'  is  a  new  nonterminal. 

Let  N'  be  N  together  with  all  new  nonterminals  introduced  in  the  construc- 
tion  of R'. 

It is elementary to  observe  that  if B  and  C  succeed,  then  A'  succeeds,  and 
that  otherwise  A'  fails.  Thus,  A  succeeds  if  and  only  if A'  succeeds  (i.e.,  B 

and  C  succeed),  or  A'  fails  (i.e.,  B  fails  or  B  succeeds  and  C  fails)  and  D 
succeeds.  It  is  also  easy  to  check  that  B,  C,  and  D  are  called  at  the  same 
points  on  the  input  by  R'  that  they  are  called  by  R.  Since  R'  simulates  each 

rule  of R  directly,  we  conclude  that  S  ~  (w  T"  s)  if and  only if  S  ~  (w ~"  s), p  ~  p, 
a nd L(P)=L(P').  5 

Seemingly, the  GTDPL  programs  do more  in the way of recognition  than 
TDPL  programs.  For  example,  a  GTDPL  program  can  readily  be  written 
to  simulate  a  statement  of the  form 

A  > BC/(Di,  D2) 
in  which  D 1 is  to  be  called  if B  fails  and  D 2 is to  be  called if B  succeeds  and 

C  fails.  It  is  open,  however,  whether  the  containment  of  Theorem  6.2  is 
proper. 

As  with  TDPL,  we  can  embellish  GTDPL  with  extended  statements  (see 
Exercise  6.1.12). ~ For  example,  every  extended  TDPL  statement  can  be 
regarded  as  an  extended  form  of  GTDPL  statement. 

6.1.4.  Time  Complexity  of  GTDPL  Languages 

The  main  result  of  this  section  is  that  we  can  simulate  the  successful 
recognition  of an  input  sentence  by  a  GTDPL  program  (and  hence  a  TDPL 

474  LIMITED BACKTRACK PARSING ALGORITHMS  CHAP. 6 
program)  in  linear  time  on  a  machine  that  resembles  a  random  access  com- 
puter.  The  algorithm  recalls  both  the  Cocke-Younger-Kasami  algorithm 

and  Earley's  algorithm  of  Section  4.2,  and  works  backward  on  the  input 

string. 

ALGORITHM 6.2 

Recognition  of G TDP L  languages  in  linear  time. 
Input.  A  GTD PL   program  P  =  (N, Z, R,  S),  with  N  =  [A~, A z , .. .,   Ak}, 
and  S  =  A~,  and  an  input  string  w =  a~a2  .. .  a,  in  Z*.  We  assume  that 
a,+ ~ =  $,  a  right  endmarker. 

Output.  A  k  *  (n  +  1)  matrix  [ t j .  Each  entry  is  either  undefined,  an 
integer  m  such  that  0  <  m  <  n,  or  the  failure  symbol f.  If  t;~ =  m,  then 

A~ ~  (ajaj+~  .. .  aj+~_,  [" aj+m  "'"  a.,  s).  If  t,l  =  f,  At  =~" (r" aj  .  .  .  a,,  f ) . Otherwise,  t,j  is  undefined. 

Method.  We  compute  the  matrix  of  ttj's  as  follows.  Initially,  all  entries 
are  undefined. 

(1)  Do  steps  (2)-(4)  f o r j   =  n  +  1, n , . . . ,   1. 
(2)  For  each  i,  1 <  i <  k,  if As --* f  is  in  R,  set  t,j  =  f.  If A s ~  e  is  in 
R,  set  t;j  =  0.  If  A  --~  a~ is  in  R,  set  t;j  =  1,  and  if A  ~  b  is  in  R,  b  ~  aj, 

set t,j  - - f   (We take  a,+ ~ to  be a  symbol not in Z,  so A  --*  a,+ ~ is never in R.) 

(3)  Do  step  (4)  repeatedly  for  i - -   1, 2 , . . . ,   k,  until  no  changes  to  the 
t j s   occur  in  a  step. 

(4)  Let  the  rule  for  At  be  of  the  form  At  ~  Ap[Aq,  A,],  and  suppose 
that  tsj  is  not  yet  defined. 

(a)  If t~j  =  f a n d   trj =  X, then  set t,j  =  x,  where  x  is an  integer  or f 
(b)  If tpj  =  m~  and  tq<j+m,~ =  m2  ~  f,  set  ttj  =  m~  +  m2. 

(c)  If  t~j  =  m~  and  t~c~+m,) =  f,  set  t;~ =  f 
In  all  other  cases  do  nothing  to  t~j.  ~] 

THEOREM  6.3 

Algorithm  6.2  correctly  determines  the  tsj's. 
P r o o f   We  claim  that  after  execution  of  Algorithm  6.2  on  the  input 
string  w  =  al  . . .   a,,  tsj  =  f  if  and  only  if As  ~  (I a~  . ..   a,, f )   and  tsj  =  m 
if and  only if  At  ~  (aj  . ..   aj+m_ 1 I a~+m "'"  a,,  s).  A  straightforward  induc- 

tion  on  the  order  in  which  the  t j s   are  computed  shows  that  whenever  t~ 
is  given  a  value,  that  value  is  as  stated  above.  Conversely,  an  induction  on  l 

shows  that  if  At =L. ([' a j . . .   a , , f )   or  A,  =L.  (aj  . .  .  aj+m_ ~ 1 aj+m  "  " a,,  s), 
then  tt~  is  given  the  value f  or  m,  respectively.  Entry  tt~  is  left  undefined  if 
A t called  at positionj does  not  halt.  The  details  are  left for the  Exercises.  D 

Note  that  al  .."  a.  is  in L(P)  if and  only if t l~  =  n. 

sEc. 6.1  LIMITED BACKTRACK TOP-DOWN PARSING  475 

THEOREM  6.4 

For  each  GTDPL  program  there  is a  constant  c such  that  Algorithm  6.2 
takes  no  more  than  cn elementary  steps  on  an  input  string  of length  n  >  1, 

where  elementary  steps  are  of the  type  used  for  Algorithm  4.3. 

Proof  The crux of the proof is to observe that in step (3) we cycle  through 
all  the  nonterminals  at  most  k  times  for  any  given j.  D 

Last  we  observe  that  from  the  matrix  in  Algorithm  6.2  it  is  possible  to 
build  a  tree-like  parse  structure  for  accepted  inputs,  similar  to  the  structure 
that was built in Algorithm 6.1 for TDPL  programs.  Additionally, Algorithm 

6.2  can  be  modified  to  recognize  and  "parse"  according  to  a  TDPL  (rather 
than  GTDPL)  program. 

Example 6.6 

Let P  =  (N,  E,  R I, E),  where 

N =   [E,E+,T,T.,F,F',  X , Y , P , M , A , L ,   R], 
~E =  [a,  (,),  + ,   ,],  and  R1  consists  of 

(1)  g  *  T[E+, X] 
(2)  g+  ,PIE,  Y] 

(3)  Z  > F[T., X] 
(4)  T.  > M[T,  Y] 
(5)  F  * L[F', A] 
(6)  F'  *  E[R,  X] 

(7)  X---> f 
(8)  Y  > e 
(9)  P  > + 
(I0)  M  *  * 

(11)  A  *  a 

(12)  L:  > ( 
(13)  R  *  ) 

This  GTDPL  program  is  intended  to  recognize  arithmetic  expressions 
over  +  and  ,,  i.e., L(Go). E recognizes  an expression  consisting of a  sequence 
of terms  (T's)  separated  by  /'s.  The  nonterminal  E+ is intended to recognize 
an expression with the first term deleted.  Thus rule (2) says that E+ recognizes 
a  +  sign (P)  followed by any expression,  and  if there is no  +  sign, the empty 

476  LIMITED  BACKTRACK  PARSING  ALGORITHMS  CHAP.  6 

string (Y) serves. Then we can interpret  statement  (1) as saying that an expres- 
sion  is  a  term  followed  by  something  recognized  by  E/,  consisting  of either 
the  empty  string  or  an  alternating  sequence  of  q-'s  and  terms  beginning  with 

* q-  and  ending  in  a  term.  A  similar  relation  applies  to  statements  (3)  and  (4). 

Statements  (5)  and  (6)  say  that  a  factor  (F)  is  either  (  followed  by  an 
expression  followed  by  )  or,  if no  (  is  present,  a  single  symbol  a. 

Now,  suppose  that  (a  +  a)  *  a  is  the  input  to  Algorithm  6.2.  The  matrix 
[ t j   constructed  by  Algorithm  6.2  is  shown  in  Fig.  6.2. 

Let us compute  the  entries  in the  eighth  column  of the  matrix.  The  entries 
for  P,  M,  A,  L,  and  R  have  value f,  since  they  look  for  input  symbols  in 
and  the  eighth  input symbol  is  the  right  endmarker.  X" always  yields  value 
f,  and  Y always  yields  value  0.  Applying  step  (3)  of  Algorithm  6.2,  we  find 

that  in  the  first  cycle  through  step  (4)  the  values  for  E+,  T.,  and  F  can  be 
filled  in  and  are  0,  0,  and f,  respectively.  On  the  second  cycle,  T  is  given  the 

value f.  The  values  for  E  and  F'  can  be  computed  on  the  third  cycle. 

E E+ 

T 
T, F 

F' X 

Y P 
M A 
L R 

(  a  -4-  a  )  *  a  $ 
7  3  f  1  f  f  1  f 0  0  2  0  0  0  0  0 
7  1  f  1  f  f  1  f 0  0  0  0  0  2  0  0 

5  1  f  1  f  f  1  f f  4  f  2  f  f  f  f 
f  f  f  f  f  f  f  f 0  0  0  0  0  0  0  0 
f  f  1  f  f  f  f  f f  f  f  f  f  1  f  f 
f  1  f  1  f  f  1  f 1  f  f  f  f  f  f  f 

f  f  f  f  1  f  f  f 
Fig. 6.2  Recognition table from Algorithm 6.2. 

An  example  of a  less trivial computation  occurs  in column  3.  The  bottom 
seven  rows  are  easily  filled  in  by  statement  (2).  Then,  by  statement  (3),  since 
the P  entry  in  column  3 is  1, we  examine  the  E  entry  in  column  4  ( =   3  --t- 1) 

and find that this is also  1. Thus  the E/ entry in column  3 is 2 ( =   1 -q-  1).  D 

6.1.5.  Implementation  of  GTDPL  Programs 

In  practice,  implementation  of  GTDPL-like  parsing  systems  do  not  take 
the  tabular  form  of  Algorithm  6.2.  Instead,  a  trial-and-error  method  is 

normally  used.  In  this  section  we  shall  construct  an  automaton  that  "imple- 

ments"  the  recognition  portion  of  a  GT DPL   program.  We  shall  leave  it  to 

SEC. 6.1  LIMITED  BACKTRACK TOP-DOWN  PARSING  477 

the  reader  to  show  how  this  automaton  could  be  extended  to  a  transducer 
which  would  indicate  the  successful  sequence  of  routine  calls  from  which 
a  "parse"  or  translation  can  be  constructed. 

The  automaton  consists  of  an  input  tape  with  an  input  pointer,  which 
may  be  reset;  a  three-state  finite  control;  and  a  pushdown  list  consisting 

of  symbols  from  a  finite  alphabet  and  pointers  to  the  input.  The  device 
operates  in  a  way  that  exactly  implements  our  intuitive  idea  of  routines 
(nonterminals)  calling  one  another,  with  the  retraction  of  the  input  pointer 
required  on  each  failure.  The  retraction  is  to  the  point  at  which  the  input 
pointer  dwelt  when  the  call  of  the  failing  routine  occurred. 

DEFINITION 

A  parsing  machine is  a  6-tuple  M  =  (Q,  E,  F,  6,  begin,  Zo) ,  where 

(1)  Q  =  {success,  failure,  begin]. 
(2)  E  is  a  finite  set  of  input symbols. 
(3)  r  is  a  finite  set  of pushdown symbols. 
(4)  ~  is  a  mapping  from  Q x   (E u { e} )  x  F  to  Q  x F   .2,  which  is 
restricted  as  follows" 

(a)  If  q  is  success  or  failure,  then  0(q,  a, Z)  is  undefined  if  a  ~  E, 

and  ~(q,  e, Z)  is  of the  form  (begin,  Y)  for  some  Y ~  F. 
(b)  If ~(begin,  a, Z)  is  defined  for  some  a  ~  ~,  then  ~(begin,  b, Z)  is 

undefined  for  all  b  #=  a  in  E  U  [e}. 
(c)  For  a  in  E,  ~(begin,  a, Z)  can  only  be  (success,  e)  if it  is  defined. 
(d)  ~(begin,  e, Z)  can  only  be  of  the  forms  (begin,  YZ),  for  some  Y 

in  F,  or  of the  form  (q,  e),  for  q  :  success  or  failure. 
(5)  begin  is  the  initial  state. 
(6)  Z0  in  r  is  the  initial  pushdown  symbol. 

M  resembles  a  pushdown  automaton,  but  there  are  several  major  differ- 
ences.  We  can  think  of the  elements  of I" as  routines  that  either  call  or  trans- 
fer  to  each  other.  The  pushdown  list is  used  to  record  recursive  calls  and  the 
position  of the input  head each time a call was made.  The state begin normally 

causes a call of another routine,  reflected in that if ~(begin, e, Z)  =  (begin, YZ), 
where  Y  is  in  r  and  Z  is  on  top  of  the  list,  then  Y  will  be  placed  above 
Z  on  a  new  level  of  the  pushdown  list.  The  states  success  and  failure 

cause  transfers  to,  rather  than  calls  of,  another  routine.  If,  for  example, ~(success, 

e, Z)  :  (begin,  Y),  then  Y  merely  replaces  Z  on  top  of  the  list. 
We  formally  define  the  operation  of  M  as  follows. A  configuration 

of  M  is  a  triple  (q,  w I x,  ~,), where 

(1)  q  is  one  of success,  failure,  or  begin; 
(2)  w and  x  are  in  E*;  I  is  a  metasymbol,  the  input head; 

(3)  ~, is  a  pushdown  list  of  the  form  (Z~,  i~)  . . .   (Zm,  ira), where  Zj  ~  U 
and  ij is an integer, for  1 ~  j  ~  m. The top is at the left.  The Z's  are "routine" 
calls;  the  i's  are  input  pointers. 

478  LIMITED BACKTRACK PARSING  ALGORITHMS  CHAP.  6 

We define the I-~- relation,  or  ~  when M  is understood,  on configurations 
as  follows: 

(1)  Let  ~(begin,  e, Z)  =  (begin,  YZ)  for  Y in  F .  Then 

(begin,  w  ~" x,  (Z,  i)7 )  t-- (begin,  w  f x,  (Y, y)(z,  i)~,), 
where  j =   I wl.  Here,  Y  is  "called,"  and  the  position  of  the  input  head. 

when  Y is  called  is  recorded,  along  with  the  entry  on  the  pushdown  list 

for  Y. (2)  Let  ~(q,  e, Z)  =  (begin,  Y),  where  Y ~  F,  and q  =  success  or failure. 

Then  (q,  w I x,  (Z,  i)7)  t-  (begin, .w [" x,  (Y, i)r).  Here  Z  "transfers"  to  Y. 

The  input  position  associated  with  Y is  the  same  as  that  associated  with  Z. 

(3)  Let  ~(begin,  a, Z )  =  (q,  e)  for  a  in  E  u  {e}.  If  q  =  success,  then 
(begin,  w  Iax,  (Z,  i)~,) ~- (success,  wa ~ x,  ~,).  If  a  is  not  a  prefix  of  x  or 
q  =  failure,  then  (begin,  w I x,  (Z,  i)7)  ~  (failure,  u  I v, ~,),  where  uv  =  wx and  [ul  =  i.  In  the  latter  case  the  input  pointer  is  retracted  to  the  location 

given  by  the  pointer  on  top  of the pushdown  list. 

Note  that  if ~(begin,  a, Z)  =  (success, e), then the next state  of the parsing 
machine  is  success  if  the  unexpended  input  string  begins  with  a  and  failure 
otherwise. Let ~  be the transitive  closure  of  ~ .   The  language defined by M,  denoted 

L(M),  is {w[w  is in  E*  and  (begin,  I w, (Z0,  0))[ --~- (success,  w r', e)}. 

Example  6,7 

Let  M  =  (Q, (a, b}, (Z0,  Y, A,  B, E}, ~, begin,  Zo),  where  6  is  given  by 

(1)  6(begin,  e, Zo)  =  (begin,  YZo) 
(2)  O(success,  e, Z o ) =   (begin, Z0) (3)  $(failure,  e,  Z0)  =  (begin,  E) 

(4)  $(begin,  e,  Y)  =  (begin,  A Y) 
(5)  O(success,  e,  Y ) =   (begin,  Y) 
(6)  6(failure,  e,  Y)  =  (begin,  B) 

(7)  $(begin,  a,  A)  =  (success,  e) 
(8)  O(begin, b, B)  =  (success,  e) 
(9)  O(begin, e, E)  =  (success,  e) 

M  recognizes  e  or  any  string  of a's  and  b's  ending  in  b,  but  does  so  in a  peculiar  way.  A  and  B  recognize  a  and  b,  respectively.  When  Y begins,  it 

looks  for  an  a,  and  i f  Y finds  it,  Y  "transfers"  to  itself.  Thus  the  pushdown 

list  remains  intact,  and  a's  are  consumed  on  the  input.  If  b  or  the  end  of 
the  input  is  reached,  Y in  state  failure  causes  the  top  of the  pushdown  list 
to  be  erased.  That  is,  Y is  replaced  by  B,  and,  whether  B  succeeds  or  fails, 
that  B  is  eventually  erased. Z0  calls  Y and  transfers  to  itself  in  the  same  way  that  Y calls  A.  Thus 

SEC. 6.1  LIMITED  BACKTRACK TOP-DOWN PARSING  479 

any string of a's and  b's ending in b will eventually  cause Z0 to  be erased and state success entered.  The action  of M  on input  abaa is given by the following 
sequence  of configurations" 

(begin,  I abaa,  (Z o, 0))  t--- (begin,  I abaa,  (Y,  O)(Zo, 0)) 

t-  (begin,  l' abaa,  (A,  O)(Y,  O)(Zo, 0)) 

t-  (success,  a  ~' ban,  (Y,  O)(Zo, 0)) 

t-  (begin, a  [" ban,  (Y,  O)(Zo, 0)) 

(begin, a  ~" ban,  (A,  1)(Y, O)(Zo, 0)) 
(failure,  a  l ban,  (Y,  O)(Zo, 0)) 
(begin, a  ~" ban,  (B,  O)(Zo,  0)) 
(success,  ab  ~" an,  (Zo,  0)) 
(begin, ab  ~" an,  (Zo,  0)) 

(begin, ab  l an,  (Y,  2)(Zo,  0)) 
(begin, ab  I an,  (A,  2)(Y,  2)(Zo,  0)) 
[-- (success, aba  ~ a,  (Y,  2)(Zo, 0)) 
[-  (begin, aba  ~" a,  (Y,  2)(Z o, 0)) 

t-  (begin, aba  ~" a,  (A,  3)(Y,  2)(Zo, 0)) 

(success, abaa  I,  (Y,  2)(Zo, 0)) 

(begin, abaa  ~', (Y,  2)(Zo,  0)) 

(begin, abaa  l', (A,  4)(Y, 2)(Zo,  0)) 
[-  (failure,  abaa  ~', (Y,  2)(Zo, 0)) 

[-  (begin, abaa  I,  (B,  2)(Z o, 0)) 

t-- (failure,  ab  ~" an,  (Zo,  0)) 
[-  (begin, ab  [" aa,  (E,  0)) 
[-  (success, ab  ~ an,  e) 

Note  that  abaa  is  not  accepted  because  the  end  of  the  input  was  not reached at the last step.  However, ab alone would be accepted.  It is important 

also  to  note  that  in  the  fourth  from  last  configuration  B  is  not  "called"  but replaces  Y. Thus  the  number  2,  rather  than  4,  appears  on  top  of the list,  and 

when  B  fails,  the  input  head  backtracks.  [-7 

We  shall  now  prove  that  a  language  is  defined  by  a  parsing  machine  if 
and  only  if it  is  defined  by  a  GTDPL  program. 

LEMMA 6.5 

If L  --  L(M)  for  some  parsing  machine  M  --  (Q, E, r',  6, begin, z0),  then L  --  L(P) 

for  some  GTDPL  program  P. 

400  LIMITED BACKTRACK PARSING  ALGORITHMS  CHAP. 6 

Proof.  Let P  =  (N,  Z,  R, Z0), where N  =  F  u  {X} with X a new  symbol. 
Define  R  as  follows" 

(1)  X  has  no  rule. 
(2)  If ~(begin,  a,  Z)  =  (q,  e),  let  Z  --~ a  be  the  rule  for  Z  if q  =  success, 
and  Z--~ f  be  the  rule  if q  =  failure. 

(3)  For  the  other  Z's  in  F  define  Y~,  Y2,  and  Y3  as  follows' (a)  If  ~(begin,  e,  Z)  (begin,  YZ),  let  Y~  =  Y. 

(b)  If  ~(q,  e,  Z)  =  (begin,  Y),  let  Y2  =  Y  if  q  is  success  and  let  Y3 =  Y  if q  is  failure. 
(c)  If  Yt is not  defined  by  (a)  or  (b),  take  Yt to  be  X  for  each  Yt not 

otherwise  defined. 

Then  the  rule  for  Z  is Z  ---~ Y~[Y2,  Y3]. 

We  shall  show  that  the  following  statements  hold  for  all Z  in  r .  

(6.1.3) 

(6.1.4) 

Z  --->  (w  ~' x,  s)  if and  only  if 

(begin,  I wx,  (Z,  0))Ira-  (success,  w  I x,  e) 
Z  ~  ([' w, f )   if  and  only  if  (begin,  I w,  (Z,  0))[-~--  (failure,  I w,  e) 

We  prove  both  simultaneously  by  induction  on  the  length  of a  derivation  in 
P  or  computation  of M. 

Only  if:  The  bases  for  (6.1.3)  and  (6.1.4)  are  both  trivial  applications  of 
the  definition  of the  l-  relation. 

For  the  inductive  step  of  (6.1.3),  assume  that  Z  ~  (w ~' x, s)  and  that 
(6.1.3)  and  (6.1.4)  are true for  smaller n.  Since we may take n  >  1, let the rule 
for  Z  be Z  ~  Yi[Y2,  Y3]. 

Case  1:  w  =  wlw2,  Y1  ~  (wl  ~" w2x,  s)  and  Y2 ~  (w2 I x,  s).  Then  n 1 
and  n2 are  less than  n,  and  we have,  by the  inductive  hypothesis  (6.1.3), 

(6.1.5) 
and 

(begin,  ~" wx,  (Y1,  0))12-  (success,  w 1 I w2x,  e) 

(6.1.6)  (begin,  I w2x,  (Y2, 0))12-  (success,  w z I x,  e) 

We  must  now  observe  that  if  we  insert  some  string,  w 1 in  particular,  to 
the  left  of  the  input  head  of  M,  then  M  will  undergo  essentially  the  same 
action.  Thus  from  (6.1.6)  we  obtain 

(6.1.7)  (begin,  w 1 ~ w2x ,  (Y2, 0))12-  (success,  w  I x,  e) 

This  inference  requires  an  inductive  proof  in  its  own  right,  but  is  left  for the  Exercises. 

SEC. 6.1  LIMITED  BACKTRACK  TOP-DOWN  PARSING  481 

From  the  definition  of  R  we  know  that  ~(begin,  e,  Z ) =   (begin,  YtZ) 
and  ~(suceess,  e, Z)  =  (begin,  Y2). Thus 

(6.1.8)  (begin,  T' wx,  (Z,  0))  ~  (begin,  ~' wx,  (Y~, 0)(Z,  0)) 

and 

(6.1.9)  (success,  wl  r" w2x,  (Z,  0))  ~  (begin,  w 1 I w2x,  (Y2,  0)). 

Putting  (6.1.9),  (6.1.5),  (6.1.8),  and  (6.1.7)  together,  we  have 

as  desired. 

(begin,  [' wx,  (Z,  0))l-~-- (success,  w r' x,  e), 

Case  2:  Yl  ~  (~ wx, f )   and  Y3 ~  (w p x,  s).  The  proof  in  this  case  is 
similar  to  case  1 and  is left  for  the  reader. 

We  now turn  to  the  induction  for  (6.1.4).  We  assume  that  Z  ~  (~ w, f). 

Case  1 : Y 1   ~  (wl  ~ w2, s)  and  Yz ~  (~ wz, f),  where  w lw z =  w.  Then nl,  n2  <  n,  and  by  (6.1.3)  and  (6.1.4),  we  have 

(6.1.10) 
(6.1.11) 

(begin, p w,  (Y1,  0))[--~- (success, w 1 ~" w z,  e) 
(begin, r" w2,  (Y2,  0))I ~  (failure,  ~' wz,  e) 

If we insert  w a to  the  left  of 1" in  (6.1.11),  we  have 

(6.1.12)  (begin, wl  ~" w2,  (Yz,  0))I--~- (failure,  ~" w 1 wz,  e) 

The  truth  of this  implication  is left  for  the  Exercises.  One  has  to  observe 
that  when  (Yz, 0) is erased,  the input  head must be  set all the way to  the  left. 

Otherwise,  the  presence  of  w l  on  the  input  cannot  affect  matters,  because numbers  written  on  the  pushdown  list  above  (Y1, 0)  will have  1 w~ [ added  to 

them  [when  constructing  the  sequence  of steps  represented  by  (6.1.12)  from (6.1.11)],  and  thus  there  is  no  way  to  get  the  input  head  to  move  into  w x 
without  erasing  (Y~, 0). By  definition  of  Y~ and  Y2, we  have 

(6.1.13)  (begin,  ~' w,  (Z,  0))  ~  (begin,  ~" w,  (Y~,  O)(Z,  0)) 
and 

(6.1.14)  (success,  w l  ~" w2,  (Z,  0))  F-  (begin,  w t [" w2,  (Y2,  0)) 

Putting  (6.1.13),  (6.1.10),  (6.1.14),  and  (6.1.12)  together,  we  have 
(begin, [" w, (Z, 0))I --~- (failure,  1" w, e). 

482  LIMITED BACKTRACK PARSING ALGORITHMS  CHAP. 6 

Case  2 : Y1   ~  (~ w, f )   and  Y3 =%. ([' w, f).   This  case  is  similar  and  left 
to  the  reader. 

lf:  The  "if"  portion  of the proof is similar to  the foregoing,  and we leave 
the  details  for  the  Exercises. 

As a  special case of (6.1.3), Z0  =~  (w l', s) if and  only if (begin,  [' w, (Z0, 0)) ! -~-- (success,  w  [', e),  so  L(M)  =  L(P).  D 

LEMMA 6.6 

If L  =  L(P)  for  some  GTDPL  program  P,  then  L  =  L(M)  for  a  parsing 
machine  M. 

Proof.  Let  P  =  (N,  X, R,  S)  and  define  M  =  (Q, X, N,  ~, begin,  S). 
Define  ~  as  follows: 

(1)  If  R  contains  rule  A  ---~ B[C,  D],  let  ~(begin, e, A)  =  (begin,  BA), 
$(sueeess,  e, A)  =  (begin,  C)  and  dr(failure,  e, A)  =  (begin,  D). 

(2)  (a)  If  A----~ a  is  in  R,  where  a  is  in  X u  [e},  let  $(begin,  a, A ) =  (success,  e). 

(b)  If  A  ---~ f  is  in  R,  let  6(begin,  e,  A)  =  (failure,  e). 
A  proof that  L ( M ) =   L(G)  is  straightforward  and  left  for  the  Exercises. 

D 
THEOREM  6.5 

A  language  L  is  L(M)  for  some  parsing  machine  M  if  and  only  if  it  is L(P) 

for  some  GTDPL  program P. 

Proof  Immediate  from  Lemmas  6.5  and  6.6.  D 

E X E R C IS E S  
6.1.1.  Construct  TDPL  programs  to  recognize  the  following  languages" (a)  L(ao). 

(b)  The set of strings with an equal  number  of a's  and b's. (c)  {WCWR[ w ~ (a +  b)*}. 
*(d)  {a2"l n >  1}. Hint:  Consider  S  ~  aSa/aa. (e)  Some infinite subset of FORTRAN. 

6.1.2.  Construct  GTDPL programs to recognize the following languages: (a)  The languages in Exercise  6.1.1. 

(b)  The language generated by (with start symbol E) 

E----~  E  +  T[T 

T  > T , F I F  
F  > (E)II 

I----+ a[ a(L) 
L -   > a[a,L 
**(c)  {a"'ln ~  1}. 

EXERCISES  483 
"6.1.3.  Show that  for  every LL(1)  language there is  a  GTDPL  program  which 

recognizes the language with no  backtracking;  i.e.,  the parsing machine 
constructed  by  Lemrna  6.6  never  moves  the  input  pointer to  the  left 
between successive configurations. 

"6.1.4.  Show that it is undecidable whether a  TDPL  program P  =  (N,  X~, R, S) 

recognizes (a)  ~. 

(b)  X*. 
6.1.5.  Show  that  every  TDPL  or  GTDPL  program  is  equivalent  to  one  in 

which  every nonterminal  has  a  rule.  Hint:  Show  that  if A  has  no  rule, 
you can  give it rule  A  ~  A A / A   (or the equivalent in  GTDPL)  with no 
change in the language recognized. 

"6.1.6.  Give  a  TDPL  program  equivalent  to  the  following  extended  program. 

What  is  the  language  defined ?  From  a  practical  point  of  view,  what 

defects does this program have ? 

s  ~  Zl n/ C 
h  ------~ a 

B  -----~  SC A  

C- - +   b 

6.1.7.  Give a  formal proof of Lemma 6.3. 
6.1.8.  Prove Lemma  6.4, 
6.1.9.  Give  a  formal  proof that P  of Example  6.5  defines {0"l"2"ln >  1}. 
6.1.10.  Complete the proof of Theorem  6.2. 
6.1.11.  Use  Algorithm  6.2  to  show  that  the  string  ((a))  +  a  is  in  L(P),  where 

P  is given in Example  6.6. 

6.1.12.  GTDPL  statements  can  be  extended  in  much  the  same  manner  we 

extended  TDPL  statements.  For  example,  we  can  permit  GTDPL 

statements of the form 

A  > X~g~Xzg2  . . .  Xkgk 
where  each  Xt  is  a  terminal  or  nonterminal  and  each gt  is  either  e  or 
a  pair  of  the  form  [czi, fli],  where  ~zi  and  fit  are  strings  of  symbols. 
A  reports  success  if  and  only  if  each  Xtgt  succeeds  where  success  is 

defined  recursively,  as  follows.  The  string  ~.[0ct, fit]  succeeds  if  and only if 

(1)  ~  succeeds and  oct succeeds or 

(2)  Xi fails and  fit succeeds. 
(a)  Show  how  this extended statement can be replaced by an equivalent 

set of conventional GTDPL  statements. 
(b)  Show  that  every extended  TDPL  statement  can  be  replaced  by  an 

equivalent  set  of  (extended)  GTDPL  statements. 

484  LIMITED BACKTRACK PARSING ALGORITHMS  CHAP. 6 

6.1.13.  Show  that  there  are  TDPL  (and  GTDPL)  programs  in  which  the 

number  of  statements  executed  by  the  parsing  machine  of  Lemma  6.6 
is an  exponential  function  of the length  of the  input  string. 

6.1.14.  Construct  a  GTDPL  program  to  simulate  the  meaning  of  the  rule 

A  ~  BC/(D1,  D2)  mentioned  on  p.  473. 

6.1.15.  Find  a  GTDPL  program  which  defines  the  language  L (M ),  where  M 

is  the  parsing  machine  given  in  Example  6.7. 

6.1.16.  Find  parsing  machines  to  recognize  the  languages  of  Exercise  6.1.2. 

DEFINITION 

A  TDPL  or  GTDPL  program P  =  (N,  ~Z, R,  S)  has  a partial  accep- 
tance failure  on  w  if  w =  uv  such  that  v -~ e  and  S  ~  (u T" v, s).  We 
say  that  P  is  well formed  if for  every  w  in  E*,  either  S  ~  (1' w, f )  or s=~  (w, Ls). 

"6.1.17.  Show  that  if L  is  a  TDPL  language  (alt.  GTDPL  language)  and  $  is 

a  new  symbol,  then  L$  is  defined  by  a  TDPL  program  (alt.  GTDPL 
program)  with  no  partial  acceptance  failures. 

"6.1.18.  Let L1  be defined by a  TDPL  (alt.  GTDPL)  program  and  L2  by a  well- 

formed  TDPL  (alt.  GTDPL)  program.  Show  that 

(a)  L1  U  L2, 
(b)  L,2, 

(c)  L1  ~  L2,  and 
(d)  L1  -- .L2 
are TDPL  (alt.  GTDPL)  languages. 

"6.1.19.  Show  that  every GTDPL  program with  no  partial  acceptance  failure  is 

equivalent  to  a  well-formed  GTDPL  program.  Hint:  It  suffices to  look 
for  and  eliminate  "left  recursion."  That  is,  if we  have  a  normal  form 
GTDPL  program,  create  a  CFL  by  replacing  rules  ,4  --~  B[C,  D]  by 
productions  A  ~  BC  and  A  ~  D.  Let  A  ~  a  or  A  ~  e  be  produc- 
tions  of  the  CFL  also.  The  "left  recursion"  referred  to  is  in  the  CFL 

constructed. 

*'6.1.20.  Show  that  it  is  undecidable  for  a  well-formed  TDPL  program 

P  =  (N,  X, R,  S)  whether  L( P)   =  ~ .  Note:  The  natural  embedding  of 

Post's  correspondence  problem  proves  Exercise  6.1.4(a),  but  does  not 
always yield a  well-formed program. 

6.1.21.  Complete  the proof of Lemma  6.5. 
6.1.22.  Prove Lemma  6.6. 

Open  Problems 

6.1.23.  Does  there  exist  a  context-free  language  which  is  not  a  GTDPL  lan- 

guage ? 

6.1.24.  Are  the  TDPL  languages  closed  under  complementation ? 

s~c.  6.2  LIMITED  BACKTRACK  BOTTOM-UP  PARSING  485 

6.1.25.  Is  every TDPL  program  equivalent  to  a  well-formed  TDPL  program? 

6.1.26.  Is  every  GTDPL  program  equivalent  to  a  TDPL  program?  Here  we 

conjecture  that  {a"'ln _~ 1}  is  a  GTDPL  language  but  not  a  TDPL language. 

Programming  Exercises 

6.1.27.  Design  an interpreter for parsing machines.  Write a program that  takes an  extended  GTDPL  program  as  input  and  constructs  from  it  an 

equivalent  parsing  machine  which  the  interpreter  can  then  simulate. 
6.1.28.  Design  a  programming  language  centered  around  GTDPL  (or  TDPL) 

which  can  be  used  to  specify  translators.  A  source  program  would  be 
the  specification  of  a  translator  and  the  object  program  would  be the 
actual translator.  Construct  a  compiler for this  language. 

B I B L I OG R A P H I C   N OT E S  
TDPL  is an  abstraction  of the parsing language used by McClure  [1965] in  his compiler-compiler  TMG.I"  The  parsing  machine  in  Section  6.1.5  is  similar  to  the 
one  used  by  Knuth  [1967].  Most  of the  theoretical  results  concerning  TDPL  and 
GTDPL  reported  in  this  section  were  developed  by  Birman  and  Ullman  [1970]. 
The  solutions  to  many  of the  exercises  can  be  found  there. GTDPL  is a  model  of the META  family of compiler-compilers [Schorre,  1964] 

and  others. 

6.2.  LIMITED  BACKTRACK  BOTTOM-UP 

PARSING 

We  shall  discuss  possibilities  of parsing  deterministically  and  bottom-up 
in  ways  that  allow  more  freedom  t h a n  the  shift-reduce  methods  of  Chapter 

5.  In  particular,  we  allow  limited  backtracking  on  the  input,  and  the  parse 
produced  need  not  be  a  right  parse.  The  principal  method  to  be  discussed 
is  that  of  Colmerauer's  precedence-based  algorithm. 

6.2.1.  Noncanonical  Parsing 

There  are  several  techniques  which  might  be  used  to  deterministically 
parse  grammars  which  are  not  LR.  One  technique  would  be  to  permit  arbi- 
trarily  long  lookahead  by  allowing  the  input  pointer  to  migrate  forward 

along the input to resolve some ambiguity. When a decision has been reached, 

the  input  pointer finds  its way back  to the  proper place for a  reduction. 

tTMG comes from the word "transmogrify," whose meaning is "to change in appear- ance or form, especially, strangely or grotesquely.'" 

486  LIMITED BACKTRACK PARSING ALGORITHMS  CHAP. 6 

Example 6.8 

Consider  the  grammar  G with  productions 

S  *  AalBb 
A  > 0A1101 

B  > 0Bl11011 

G  generates  the  language  [O"Ya[n  >  1} U  {0"12"b ]n  >  1},  which  is  not  a 
deterministic  context-free  language.  However,  we  can  clearly  parse  G  by 
first  moving  the  input  pointer  to  the  end  of  an  input  string  to  see  whether 
the last  symbol is a  or b and  then  returning  to  the beginning  of the  string and 
parsing  as  though  the  string  were  0"1" or  0"12,,  as  appropriate.  [-7 

Another  parsing  technique  would  be  to  reduce  phrases  which  may  not be  handles. 

DEFINITION 

If G  =  (N,  E, P,  S)  is a  CFG,  then  fl is a phrase  of a  sentential  form  ~zfly 
if  there  is  a  derivation  S  ~  ~zAy :=~ ~zflT.  If  X t . . .   X k  and  X j . . .   X1  are 
phrases  of  a  sentential  form  X 1 . . -   X,,  we  say  that  phrase  X ~ . ..   Xk  is  to 

the left  of phrase  Xj  .-.  Xi  if i  <  j  or if i  =  j  and  k  <  l.  Thus,  if a  grammar 
is  unambiguous,  the  handle  is  the  leftmost  phrase  of a  right-sentential  form. 

Example 6.9 

Consider  the  grammar  G  having  productions 

S  > OABb[OaBc 
A  >a 

B ~   > B l l l  

L(G)  is  the  regular  set  0al+(b  +  c),  but  G is  not  LR.  However,  we  can  parse 

G bottom-up  if we  defer  the  decision  of whether  a  is  a  phrase  in  a  sentential 
form  until  we  have  scanned  the  last  input  symbol.  That  is,  an  input  string  of 
the  form  0al"  can  be  reduced  to  OaB independently  of whether  it is followed 

by  b  or  c.  In  the  former  case,  OaBb is  first  reduced  to  OABb  and  then  to  S. 
In  the  latter  case,  OaBc is reduced  directly  to  S.  Of course,  we  shall  not pro- duce  either  a  left  or  right  parse.  [Z] 

Let  G  =  (N,Z,  P,  S)  be  a  C FG   in  which  the  productions  are  numbered from  1 to p  and  let 
(6.2.1)  S :   ~o  ~ ~ 1 - - - - >' o ~ 2   :- " "  :-o~ n :  w 

sEc. 6.2  LIMITED BACKTRACK BOTTOM-UP PARSING  487 
be  a  derivation  of  w  from  S.  For  0  <  i  <  n,  let  at  =  fl~At6~,  suppose  that 
A~ ----~ Yt is  production  p~  and  suppose  that  this  production  is  used  to  derive 

a~+l =  fl~?i5~ by  replacing  the  explicitly  shown  A~.  We  can  represent  this 

step  of  the  derivation  by  the  pair  of  integers  (p,,  It),  where  1, =  I fl, I. Thus 
we  can  represent  the  derivation  (6.2.1)  by  the  string  of n  pairs 

(6.2.2)  (p0,/0)(p,,  t 0 - . .   (;._,,/._,) 
If  the  derivation  is  leftmost  or  rightmost,  then  the  second  components  in 

(6.2.2),  those  giving  the  position  of  the  nonterminal  to  be  expanded  in  the 
next  step  of the  derivation,  are  redundant. 

DEFINITION 

We  shall  call  a  string  of pairs  of the  form  (6.2.2)  a  (generalized)  top-down parse 

for  w.  Clearly,  a  left  parse  is  a  special  case  of a  top-down  parse.  Like- 
wise,  we  shall  call  the  reverse  of  this  string,  that  is, 

(P,-1,  t,-~)(P,-z,  l , - 2 ) " "   (Pa,  ll)(Po,  1o) 
a  (generalized)  bottom-up  parse  of  w.  Thus  a  right  parse  is  a  special  case  of 
a  bottom-up  parse. 

If  we  relax  the  restriction  of  scanning  the  input  string  only  from  left  to 
right,  but  instead  permit  backtracking  on  the  input,  then  we  can  determin- 
istically  parse  grammars  which  cannot  be  so  parsed  using  only  the  left-to- 

right  scan. 

6,2.2.  Two-Stack  Parsers 

To  model  some  backtracking  algorithms,  we  introduce  an  automaton 
with  two  pushdown  lists,  the  second  of  which  also  serves  the  function  of 

an  input  tape.  The  deterministic  version  of this  device  is  a  cousin  of the  two- 

stack parser  used in Algorithms  4. I  and 4.2 for general top-down  and  bottom- 
up  parsing.  We  shall,  however,  put  some  restrictions  on  the  device  which 

will  make  it  behave  as  a  bottom-up  precedence  parser. 

DEFINITION 

A  two-stack  (bottom-up)  parser  for  grammar  G  =  (N,  E, P,  S)  is  a  finite 
set  of  rules  of  the  form  (a,  fl)  ~  (y, 6),  where  a,  fl,  ~,,  and  ~  are  strings  of 

symbols  in  N  U  E  U  [$};  $ is a  new  symbol,  an  endmarker.  Each  rule  of the 
parser  (a,  fl)  ~  (y, 6)  must  be  of one  of two  forms:  either 

(1)  fl  =  X6  for  some  X  E  N  U  I:,  and  y  =  aX,  or 
(2)  a  =  ~,6 for  some  6 in  (N  u  E)*,  6  =  Aft,  and  A  ~  e  is  a  production 

in  P. 

In  general,  a  rule  (a,  fl)  ~  (Y, 6)  implies  that  if the  string  a  is  on  top  of 

488  LIMITED BACKTRACK PARSING  ALGORITHMS  CHAP.  6 

the  first  pushdown  list  and  if  the  string  fl  is  on  top  of  the  second,  then  we 
can  replace  0c by ~, on  the  first  pushd.own  list and  fl by y on the  second.  Rules 

of type  (1) correspond  to  a  shift in a  shift-reduce  parsing  algorithm.  Those  of 
type  (2)  are  related  to  reduce  moves;  the  essential  difference  is  that  the 

symbol  A,  which  is  the  left  side  of  the  production  involved,  winds  up  on 
the  top  of  the  second  pushdown  list  rather  than  the  first.  This  arrangement 

corresponds  to  limited  backtracking.  It  is  possible  to  move  symbols  from 
the  first  pushdown  list  to  the  second  (which  acts  as  the  input  tape),  but  only 

at  the  time  of a  reduction.  Of course,  rules  of type  (1) allow  symbols to  move 
from  the  second  list  to  the  first  at  any  time. 

A  configuration  of  a  two-stack  parser  T  is  a  triple  (0~, fl,  n),  where  tx 
$(N  u  E)*,  fl  ~  (N  u  E)* $,  and  rt is a  string of pairs  consisting of an integer 
and  a  production  number.  Thus,  n  could  be  part  of  a  parse  of  some  string 
in L(G).  We  say  that  (0c, fl, zt)]-T  (0c, fl',  n')  if 

(1)  ~  =  txltx2, fl  =  fl2fll,  (t~2, f12) "-~  (r,  ~)  is  a  rule  of T; 
(2)  ~'  =  0c17, fl'  =  ~flx ;  and (3)  If ((x2, f12) ~  (Y, d~) is  a  type  1 rule,  then  zt'  =  rt;  if a  type  2  rule  and 

production  i  is  the  applicable  production,  then  ~z'=  n(i, j),  where j  is  equal to  I ~ ' 1 -   1.t 

Note  that  the  first  stack  has  its  top  at  the  right  and  that  the  second  has 
its  at  the  left. 

We  define  I-~-, I~--, and  [-~- from  I T   in  the  usual  manner.  The  subscript 
T  will  be  dropped  whenever  possible. 

The  translation  defined  by  T,  denoted  z(T),  is  [(w,n)l($,  w$,  e)! ~ 
($,  S$,  zt)3. We  say  that  T  is  valid  for  G  if for  every  w  ~  L(G),  there  exists 
a  bottom-up  parse  n  of  w  such  that  (w, rt)  ~  z(T).  It  is  elementary  to  show 
that  if (w,  zt)  ~  z(T),  then  rt is  a  bottom-up  parse  of  w. 

T  is  deterministic  if  whenever  (0~1, ill)  ----~ (Yl, ~1) and  (~2, flz)  --~  (Y2, ~2) 
are  rules  such  that  0~  is  a  suffix  of  ~2  or  vice  versa  and  fll  is  a  prefix  of 
f12  or  vice  versa,  then  ~'1  =  ~'2  and  di~ =  ~2.  Thus  for  each  configuration  C, there  is  at  most  one  C'  such  that  C  ~-  C'. 

Example 6.10 

Consider  the  grammar  G  with  productions 

(1)  S  > aSA 
(2)  S  > bSA 
(3)  S  > b 
(4)  A-  >a 

tThe  --1  term is present  since ~' includes a left endmarker. 

SEC. 6.2  LIMITED  BACKTRACK  BOTTOM-UP  PARSING  489 
This  grammar  generates  the  nondeterministic  CFL  [wba"lw  ~  (a  +  b)* 
and  n  =  [wl}. We  can  design  a  (nondeterministic)  two-stack  transducer  which  can 

parse  sentences  according  to  G  by  first  putting  all  of  an  input  string  on the  first  pushdown  list  and  then  parsing  in  essence  from  right  to  left. 

The  rules  of  T  are  the  following: 

(1)  (e,  X)  ---~ (X,  e)  for  all  X  ~  {a, b,  S,  A}.  (Any  symbol  may  be  shifted from  the  second  pushdown  list  to  the  first.) 

(2)  (a,  e)  ---~ (e, A).  (An  a  may  be  reduced  to  A.) (3)  (b, e)  --~  (e,  S).  (A  b  may  be  reduced  to  S.) 
(4)  (aSA,  e) ---~ (e, S). (5)  (bSA,  e) ---, (e, S). 
[The  last  two  rules  allow  reductions  by  productions  (I)  and  (2).] 

Note  that  T  is  nondeterministic  and  that  many  parses  of  each  input  can be  achieved.  One  bottom-up  parse  of  abbaa  is  traced  out  in  the  following 

sequence  of  configurations" 

($, abbaa$, e)  ~  ($a, bbaa$,  e) 

R  ($ab, baa$,  e) 
k-  ($abb, aa$,  e) 
k-  ($ab,  Saa$,  (3, 2)) 
k-  ($abS,  aa$,  (3, 2)) 

($abSa,  aS,  (3, 2)) 
k-  ($abSaa,  $,  (3, 2)) 
[-  ($abSa,  AS,  (3, 2)(4,  4)) 
}--- ($abS,  AA$,  (3, 2)(4,  4)(4,  3)) 
[--- ($abSA,  AS,  (3,  2)(4,  4)(4,  3)) 
]-  ($a, SA$,  (3, 2)(4,  4)(4,  3)(2,  1)) 

R  ($aS,  AS,  (3, 2)(4,  4)(4,  3)(2,  1)) 

1--- ($aSA,  $, (3,  2)(4,  4)(4,  3)(2,  1)) 
~-  ($,  S$,  (3, 2)(4,  4)(4,  3)(2,  1)(1,  0)) 

The  string  (3, 2)(4,  4)(4,  3)(2,  1)(1, 0)  is  a  bottom-up  parse  of  abbaa, corresponding  to  the  derivation 

S  =~ aSA  =~ abSAA  =~ abSaA  =~ abSaa =:c,. abbaa.  [-q 
The  two-stack  parser  has  an  anomaly  in  common  with  the  general  shift- reduce  parsing  algorithms;  if  a  grammar  is  ambiguous,  it  may  still  be  pos- 

400  LIMITED  BACKTRACK PARSING ALGORITHMS  CHAP.  6 

sible  to  find  a  deterministic  two-stack  parser  for  it  by  ignoring  some  of 
the  possible  parses.  Later  developments  will  rule  out  this  problem. 

Example  6.11 

Let  G be  defined  by  the  productions 

S----~'AIB 
A  > aAla 

B  > Bala 

G  is  an  ambiguous  grammar  for  a /.  By  ignoring  B  and  its  productions, 
the  following  set  of rules  form  a  deterministic  two-stack  parser  for  G' 

(e,  a)  > (a,  e) 
(a,  $) ~  (e, A $) 
(a,  A)  > (aA,  e) 
(aA,  $)  > (e, AS) 

($,  A)  ----~ ($A,  e) 
($A,  $) ~  ($, S$)  G 

6.2.3.  Colmerauer  Precedence  Relations 

The  two-stack  parser  can  be  made  to  act  in  a  manner  somewhat  similar 
to  a  precedence  parser  by  assuming  the  existence  of three  disjoint  relations, 

< ,   ._~._, and  .~,  on  the  symbols  of a  grammar,  letting  .~  indicate  a  reduction, 
and  <  and  "  indicate  shifts.  When  reductions  are  made,  <  will  indicate 
the  left  end  of  a  phrase  (not  necessarily  a  handle).  It  should  be  emphasized 
that,  at  least  temporarily,  we  are  not  assuming  that  the  relations  < ,   -~--, and 

* ~  bear  any  connection  with  the  productions  of a  grammar.  Thus,  for  exam- 
ple,  we  could  have  X  Y  even  though  X  and  Y never  appear  together  on 
the  right  side  of a  production. 

DEFINITION 

Let  G  =  (N,  Z, P,  S)  be  a  CFG,  and  let  < ,   ~---, and  -~  be  three  disjoint 
relations  on  N  U  Z  U  [$},  where  $  is  a  new  symbol,  the  endmarker.  The 
two-stack  parser  induced  by  the  relations  < ,   ---~, and  -~  is  defined  by  the 
following  set  of rules" 

(1)  (X,  Y)  ~  (XY,  e)  if and  only  if  X  <  Y or  2"  "  Y. 
(2)  ( X Z 1 . . "   Zk,  Y ) ~   (X,  AY)   if  and  only  if  Zk  "~  Y:  Zt-=-Zt/~  for 
1 ~  i  <  k,  X  <  Z1,  and  A  ~  Z~  . ..   Zk  is  a  production. 

We  observe  that  if  G  is  uniquely  invertible,  then  the  induced  two-stack 
parser  is  deterministic,  and  conversely. 

sEc.  6.2  LIMITED BACKTRACK BOTTOM-UP PARSING  491 

Example 6.12 

Let  G  be  the  grammar  with  productions 

(1)  S  > aSA 

(2)  S  > bSA 
(3)  S  > b 
(4)  A  >a 
as  in  Example  6.i0. 

Let  < ,   ~--, and  3>  be  defined  by  Fig.  6.3. 

a  b  S  A  $ 
<.  <. 

<.  <.  *  .>  .> 

:i  ' <.  <.  *  .>  .> 

.>  .>  Fig. 6.3  "Precedence"  relations. 
These  relations  induce  the  two-stack  transducer  with  rules  defined  as 
follows" 

(x,  r )   - - ~   ( x r ,   e) 
(Xa,  Y)  > (x,  A Y) 
(Xb,  Y )   > (X, SY) 

(X,  S)  ~  (XS,  e) 

(S,  A )   > (SA,  e) 
(XaSA,  r )   > (X, SY) 

for  all  X  E  { $,  a,  b},  Y  ~  {a, b} 

for  all  X  ~  {$,  a,  b],  Y ~  {$, A} 
for  all  X  ~  [$,  a,  b},  Y ~   {$,  A} 
for  all  X  ~  [a,  b} 

for  X  ~  [ $,  a,  b}  and  Y ~  {A,  $} 
(XbSA,  Y)  > (X,  SY)  for  X  ~  { $,  a,  b} and  Y ~  {A,  $} 

T  accepts  a  string  wba" such  t h a t] w   l =   n  by  the  following  sequence  of 
moves" 

($,  wba"$, e)[2,+1 ($wba",  $, e) 

I  "  ($wb,  A"$,  (4,  2n)  ..-  (4,  n  +  1)) 

[  ($w,  SA"$,  (4,  2n)  * ..  (4,  n  -F  1)(3,  n)) 
]  3,  ($,  S$,  (4,  2n)  ...  (4,  n  -k  1)(3,  n)(i,,  n  --  1)...  (il,  0)) 

492  LIMITED BACKTRACK PARSING  ALGORITHMS  CHAP.  6 

where  i v is  1 or  2,  1 < j   _< n.  Note  the  last  3n  moves  alternately  shift  S  and 
A,  and  then  reduce  either  aSA  or  bSA  to  S. 

It is easy to  check that  T is deterministic, so no  other  sequences  of moves 
are  possible  with  words  in  L(G).  Since  all  reductions  of  T  are  according  to 
productions  of G,  it follows  that  T is  a  two-stack  parser  for  G.  D 

On  certain  grammars,  we  can  define  "precedence"  relations  such  that 
the  induced  two-stack  parser  is  both  deterministic  and  valid.  We  shall  make 

such  a  definition  here,  and  in  the  next  section  we  shall  give  a  simple  test  by 
which  we can  determine  whether  a  grammar  has  such  a  parser. 

DEFINITION 

Let G  =  (N,  E, P, S) be a  CFG.  We say that G is a  Colmerauer grammar if 
(1)  G is  unambiguous, 
(2)  G is  proper,  and 
(3)  There  exist  disjoint  relations  4 ,   .-~-, and  .>  on  N  U  E  U  {$} which 
induce  a  deterministic  two-stack  parser  which  is valid for  G. 

We  call  the  three  relations  above  Colmerauer precedence  relations.  Note 
that  condition  (3)  implies  that  a  Colmerauer  grammar  must  be  uniquely 

invertible. 

Example  6.13 

The  relations  of  Fig.  6.3  are  Colmerauer  precedence  relations,  and  G  of 
Examples  6.10 and  6.12 is therefore  a  Colmerauer  grammar.  D 

Example  6.14 

Every simple  precedence  grammar  is  a  Colmerauer  grammar.  Let  <~, -~-, 
and  3>  be  the  Wirth-Weber  precedence  relations  for  the  grammar  G  = 

(N,  ~, P,  S).  If  G  is  simple  precedence,  it  is  by  definition  proper  and  un- 
ambiguous.  The  induced  two-stack  parser  acts  almost  as  the  shift-reduce 
precedence  parser. 

However,  when a  reduction  of right-sentential  form ocflw to  0~A w is made, 
we  wind  up  with  $~  on  the  first  stack  and  Aw$  on  the  second,  whereas  in 
the  precedence  parsing  algorithm,  we  would  have  $~A  on  the  pushdown 
list  and  w$  on  the  input.  If Xis  the  last  symbol  of  $~, then  either  X  <  A  or 

X ~  A,  by  Theorem  5.14.  Thus  the next  move  of the  two-stack  parser  must 
shift  the  A  to  the  first  stack.  The  two-stack  parser  then  acts  as  the  simple 
precedence  parser  until  the  next  reduction. 

Note  that  if  the  Colmerauer  precedence  relations  are  the  Wirth-Weber 
ones,  then  the  induced  two-stack  parser  yields  rightmost  parses.  In  general, 
however,  we  cannot  always  expect  this  to  be  the  case.  D 

SEC. 6.2  LIMITED BACKTRACK BOTTOM-UP PARSING  493 

6.2.4.  Test  for  Colmerauer  Precedence 

We  shall  give  a  necessary  and  sufficient  condition  for  an  unambiguous, 
proper  grammar  to  be  a  Colmerauer  grammar.  The  condition  involves  three 
relations  which  we  shall  define  below.  We  should  recall,  however,  that  it  is 
undecidable  whether  a  CF G  is unambiguous  and  that,  as we  saw in Example 

6.12,  there  are  ambiguous  grammars  which  have  deterministic  two-stack 
parsers.  Thus  we  cannot  always  determine  whether  an  arbitrary  CFG  is 

a  Colmerauer  grammar  unless  we  know  a  priori  that  the  grammar  is  un- 
ambiguous. 

DEFINITION 

Let G  =  (N, X, P, S) be a  CFG.  We define three new relations  2  (for left), 
,u  (for  mutual  or  adjacent),  and  p  (for  right)  on  N  W X as  follows:  For  all 

X a n d   Yin  N  U  X,  A  in  N, 

(1)  A2Y if A  ~  Y0c is  a  production, 
(2)  XtuY  if A  ~  ocXYfl  is a  production,  and 

(3)  XpA  if A  ---~ ~zX is  a  production. 

o0 As is  customary,  for  each  relation  R  we  shall  use R / to  denote  U  Rt and 

i=1 oo 
R*  to  denote  U  RE  Recall  that  R / and  R*  can  be  conveniently  computed 

i=0 using  Algorithm  0.2. 

Note  that the Wirth-Weber precedence relations  < ,   ~--, and  .>  on N  W # 
can  be  defined  in  terms  of 2,/t,  and  p  as  follows: 

(1)  <  = / t 2   +. 
(2)  +-  =  u. 
(3)  >  = p + a 2 * m ( N u X )   x  #. 

The remainder  of this section is devoted to proving that an unambiguous, 
proper  CF G  has  Colmerauer  precedence  relations  if and  only  if 

(1)  p+,u  ~  lt2*  =  ~3,  and 

(2)  ,u ~  p*/.z2+ =  ~ .  

Example 6.15 

Consider  the  previous  grammar 

S  > aSA  [bSA  I b 
A  >a 

Here 

494  LIMITED BACKTRACK PARSING ALGORITHMS  CHAP. 6 

~, =  {(S,  a),  (S,  b),  (A,  a)} 
/t  =  {(a, S),  (S,  A),  (b,  S)} 

p  =  [(A,  S),  (b,  S),  (a,  A)] 
p+/,  =  [(A,  A),  (b, A),  (a,  A)} 

~u~,* =  {(a,  S),  (S,  A),  (b,  S),  (a,  a),  (a,  b),  (S,  a),  (b,  a),  (b,  b)} 
p*/z~ + --  [(a,  a),  (a,  b),  (b,  a),  (b,  b),  (S,  a),  (A,  a)} 

Since p+/t  A  ,uA* =  ~  and  p*,uA + ~  ,u  =  ~ ,   G  has  Colmerauer  precedence 
relations,  a  set  of which  we  saw  in  Fig.  4.3. 

We  shall  now  show  that  if a  grammar  G  contains  symbols  X  and  Y such 
that  X ltY   and  Xp*lt2+Y,  then  G  cannot  be  a  Colmerauer  grammar.  Here, 

X  and  Y need  not  be  distinct. 

LEMMA 6.7 

Let  G =  (N,  Z, P,  S)  be a  Colmerauer  grammar  with  Colmerauer  preced- 
ence  relations  < ,   ~-,  and  ->.  If  XItY,  then  X  '  Y. 

Proof  Since  G is proper,  there  exists  a  derivation  in  which  a  production 
A--~  aXYfl  is  used.  When  parsing  a  word  w  in  L(G)  whose  derivation 
involves  that  production,  at  some  time  aXYfl  must  appear  at  the  top  of 

stack  1 and  be  reduced.  This  can  happen  only  if  X_~-  Y.  [Z] 

LEMMA 6.8 

Let  G  --  (N,  Z, P,  S)  be  a  C F G  such  that  for  some  X  and  Y  in  N  u  Z, 
X  p*/zA +  Y and  X/z  Y.  Then  G is not  a  Colmerauer  grammar. 

Proof.  Suppose  that  G  is.  Let  G  have  Colmerauer  relations  < ,   --',  and 

* > ,   and let Tbe the induced two-stack parser.  Since G is assumed to be proper, 
there  exist  x  and  y  in  Z*  such  that  X *=~ x  and  Y *=~ y.  Since  X/z  Y, there 
is  a  production  A  ~  aXYfl  and  strings  wl,  w2,  w3,  and  w4  in  I;*  such 
that  S  *~ wlAw4 ==~ wlocXYflw4  *  * ==~ wlwzXYw3w 4 ==~ wlw2xyw3w4.  Since 
X  p*,uA +  Y,  there  exists  a  production  B  ~  7ZC5  such  that  Z  ~  ~,'X, C =~- 

YO',  and  for  some  zl,  z2,  z3,  and  z4,  we  have  S  ~  zaBz4 =-~ za?ZCSza 
zly?'XYO'Sz4  =~ zlz~ XYz3 z4 =~ zlz2xyz3z4. 

By  Lemma  6.7,  we  may  assume  that  X  "  Y.  Let  us watch  the  processing 
by  T  of the  two  strings  u  =  wlw2xyw 3 w4  and  v =  zlz2xyz3z 4.  In  particular, 
let  us  concentrate  on  the  strings  to  which  x  and  y  are  reduced  in  each  case, 

and  whether  these  strings  appear  on  stack  1,  stack  2,  or  spread  between 
them.  Let  01, 0 2 , . . .   be  the  sequence  of  strings  to  which  xy  is  reduced 
in  u  and  W1, W2,.  * * that  sequence  in v.  We  know  that  there  is  some j  such 

SEC. 6.2  LIMITED BACKTRACK BOTTOM-UP  PARSING  495 
that  0j  =  X Y,  because  since  G  is  assumed  to  be  unambiguous,  X  and  Y 
must  be  reduced  together  in  the  reduction  of  u.  If  ~F~ =  0~  for  1 _<  i < j ,  
then  when  this  Y is  reduced  in  the  processing  of v,  the  X  to  its  left  will  also 
be  reduced,  since  X  "  Y.  This  situation  cannot  be  correct,  since  C  =:~  Y6' 
for  some  8'  in  the  derivation  of v.t Therefore,  suppose  that  for  some  smallest  i,  1 <  i < j ,   either  0~ ~  tF t 

or  ~F~  does  not  exist  (because  the  next  reduction  of  a  symbol  of  ~F t  also 
involves  a  symbol  outside  of  W~). We  know  that  if  i  >  2,  then  the  break 

point  between  the  stacks when 0~_ 1 and tF~_ i were  constructed  by a  reduction 
was  at  the  same  position  in  0~-1 as  in  tF~_ 1.  Therefore,  if  the  break  point 

migrated  out  of 0t_ ~ before  0t  was  created,  it  did  so  for  tF~_ x,  and  it  left  in 
the  same  direction  in  each  case.  Taking into  account  the  case  i  =  2,  in which 
01  =  ~F1  =  xy,  we  know  that  immediately  before  the  creation  of 0~ and  ~F~, 

the  break  point  between  the  stacks  is  either 

(1)  Within  0t  and  ~F~, and  at  the  same  position  in  both  cases;  i.e.,  0,._1 
and  W~_ 1 straddle  the  two  stacks;  or 

(2)  To  the  right  of both  0~_ 1 and  tF~_ 1; i.e.,  both  are  on  stack  1. 

Note  that  it  is  impossible  for  the  break  point  to  be  left  of 0~_ 1 and  tF t_ 1 
and  still  have  these  altered  on  the  next  move.  Also,  the  number  of  moves 
between  the  creation  of  0~_1 and  0~  may  not  be  the  same  as  the  number 
between  ~F~_ ~ and  ~F~. We  do  not  worry  about  the  time  that  the  break  point 

spends  outside  these  substrings;  changes  of  these  strings  occur  only  when 
the  break  point  migrates  back  into  them. 

It  follows  that  since  0t  ~  ~F~, the  first  reduction  which  involves  a  symbol 
of tF~_ 1 must  involve  at  least  one  symbol  outside  of tF~_ 1 ;  i.e.,  tF t really does 
not  exist,  for  we  know  that  the  reduction  of 0~_ 1 to  0t involves  only  symbols 

of  0~_1, by  definition  of  the  0's.  If  the  next  reduction  involving  W~-i  were 
wholly  within  ~t-1,  the  result  would,  by  (1)  and  (2)  above,  have  to  be  that 

~ - 1   was  reduced  to  0t. Let  us  now  consider  several  cases,  depending  on  whether,  in  0~_ 1, x  has 

been  reduced  to  X  and/or  y  has  been  reduced  to  Y. 

Case  1"  Both  have  been  reduced.  This  is  impossible  because  we  chose 
i ~ j .  

Case  2:  y  has  not  been  reduced  to  Y, but  x  has  been  reduced  to  X.  Now 
the  reduction  of ~ _   1 involves  symbols  of ~ _   i  and  symbols  outside  of ~ _   1. 
Therefore,  the  breakpoint  is  written  0~-1 and  ~ - 1 ,   and  a  prefix  of  both  is 

reduced.  The  parser  on  input  u  thus  reduces  X  before  Y.  Since  we  have 

assumed  that  T  is  valid,  we  must  conclude  that  there  are  two  distinct  parse 

"l'Note that  we are  using symbols such  as  X and  Y to  represent  specific instances of that symbol in the derivations, i.e., particular nodes of the  derivation  tree. We trust that 
the intended meaning will be clear. 

496  LIMITED  BACKTRACK PARSING  ALGORITHMS  CHAP.  6 
trees for u, and thus that  G is ambiguous.  Since G is unambiguous,  we discard 
this  case. 

Case  3:  x  has  not  been  reduced  to  X,  but y  has  been  reduced to  Y.  Then 
0~_ ~ =  OY  for  some  0.  We  must  consider  the  position  of  the  stack  break 
point  in  two  subcases" 

(a)  If the  break  point  is  within  0i_ x,  and  hence  Wi_ x,  the  only  way that 
tF t  could  be  different  from  0~  occurs  when  the  reduction  of  0~_~ reduces 

a prefix of 0t_ ~, and  the  symbol to the left of 0~_ 1 is  <  related  to the  leftmost 

symbol  of  0~_~. However,  for  W~-i,  the  "  relation  holds,  so  a  different 
reduction  occurs.  But then, some symbols of Wt-1 that  have yet to be reduced 
to X are reduced  along  with  some  symbols outside  of W~_ ~. We  rule out this 
possibility  using  the  argument  of case  2. 

(b)  If the  break  point  is  to  the  right  of Ot_ 1, then  its  prefix  0  can  never 
reach the top of stack 1 without  Y being reduced, for the only way to decrease 

the  length  of  stack  1 is  to  perform  reductions  of its  top  symbols.  But  then, 
in the processing  of u,  by the time x  is reduced  to  X, the  Y has been reduced. 

However,  we know that  the X and  Y must be reduced  together in the  unique 

derivation  tree  for  u. We  are  forced  to  conclude  in  this  case, too,  that  either 
T  is  not  valid ~ or  G is ambiguous. 

Case  4:  Neither  x  nor  y  have  been  reduced  to  X  or  Y.  Here,  one  of 
the  arguments  of cases  2  and  3 must  apply. 

We  have  thus  ruled  out  all  possibilities  and  conclude  t h a t / t   ~  p*,u~, + 
must  be  empty  for  a  Colmerauer  grammar.  D 

We  now  show  that  if there  are  symbols  X  and  Y in  a  CFG  G  such  that 
X p+,u  Y and  X,u2*  Y,  then  G  cannot  be  a  Colmerauer  grammar. 

LEMMA 6.9 

Let  G  =  (N,  X, P,  S)  be  a  CF G  such  that  for  some  X  and  Y in  N  u  X, 
X p +g Y   and  Xg2*Y.  Then  G is  not  a  Colmerauer  grammar. 

Proof.  The  proof is  left  for  the  Exercises,  and  is  similar,  unfortunately, 
to  Lemma  6.8.  Since  X  p*g  Y,  we  can  find  A----~ o~ZYfl  in  P  such  that 
Z  *~  ~'X.  Since X g~+  Y, we can  find B  ~  yXC8  in P  such that  C ~  YS'. By 

the  properness  of G,  we  can  find  words  u  and  v in L(G)  such  that  each  deri- 
vation  of  u  involves  the  production  A  --,  o~ZY]3  and  the  derivation  of  ~'X 
from  that Z;  each  derivation  o f v   involves B  ~  7XC8  and  the  derivation  of 

YS'  from  C.  In  each  case,  X  derives  x  and  Y  derives  y  for  some  x  and  y 
in ~*. 

As in  Lemma  6.8,  we watch  what  happens  to  xy  in  u and  v.  In v, we find 
that  Y must  be  reduced  before  X,  while  in  u,  either  X  and  Y are  reduced  at 
the  same  time  (if Z  *~  ~'X  is  a  trivial  derivation)  or  X  is  reduced  before 

SEC. 6.2  LIMITED BACKTRACK  BOTTOM-UP  PARSING  497 

Y  (if Z  ~  0t'X).  Using  arguments  similar  to  those  of  the  previous  lemma, 
we  can  prove  that  as  soon  as  the  strings  to  which  xy  is  reduced  in  u  and  v 

differ,  one  derivation  or  the  other  has  gone  astray.  D 

Thus the conditions g  N  p*,u2 + =  ~3 and p*g  n  g2 + =  ~  are  necessary 
in  a  Colmerauer  grammar.  We  shall  now  proceed  to  show  that,  along  with 
unambiguity,  properness,  and  unique  invertibility,  they  are  sufficient. 

LEMMA 6.10 

Let  G  =  (N,  Z, P,  S)  be  any  proper  grammar.  Then  if  t~XYfl  is  any 
sentential  form  of G,  we  have  X  p*,u2*  Y. 

Proof.  Elementary induction  on the  length  of  a  derivation  of txXYfl.  D 

LEMMA 6.11 

Let G  =  (N,  Z, P,  S) be unambiguous and proper,  with ,u n  p*,u2 / = 
and  p+,u  n  , u 2 * =   ~3.  If  otYXt...  XkZfl  is  a  sentential  form  of  G,  then 
the conditions X1 ,u X2 . . . . .   X;,_i  ,u Xk,  Y p*,u2 + X1, and  Xk p+g2* Z  imply 
that  X~  .. .  Xk  is  a  phrase  of ~YXx  ""  XkZfl. 

Proof.  If not,  then  there  is  some  other  phrase  of  otYX~ ...  XkZfl  which 
includes  X1. 

Case 1:  Assume  that  X 2 , . . . ,   Xk  are  all  included  in  this  other  phrase. 
Then either  Y or Z  is also included,  since the phrase is not  X~  ..-  X~.  Assum- 

ing  that  Y is  included,  then  YgX~.  But  we  know  that  Y p*,uA + X~,  so  that 

,u n  p*,u2 + ~  $3. I f Z  is included,  then XkltZ.  But we also have Xk P+g~* Z. 
If  2*  represents  at  least  one  instance  of  2,  i.e.,  Xe  p+,u2 + Z,  then 

,u n  p*,u2 + ~  ~ .  If 2*  represents  zero  instances  of  2,  then  Xkp +ltZ.  Since 
Xk  g  Z,  we  have  Xk  g2*  Z,  SO  p+ g  N  g2*  ~  ~. 

Case 2:  X~ is in the phrase, but  Xg+ ~ is not for some i such that  1 <  i  < k .  
Let the phrase be reduced to A. Then by Lemma 6.10 applied to the sentential 
form  to  which  we  may  reduce  ttYX1...  XkZfl,  we  have  A p*,u2*  Xt+I,  and 
hence  Xg p+,u2*  X~+ ~.  But  we  already  have  X~ ,u Xg/l,  so  either  ,u n  p*,u2 + 

~  or  p+,u  n  , u 2 * ~   ~ ,   depending  on  whether  zero  or  more  instances 
of 2  are  represented  by  2*  in  p+/t2*.  D 

LEMMA 6.12 

Let  G  =  (N,  Z, P,  S)  be  a  CFG  which  is  unambiguous,  proper,  and 
uniquely  invertible  and  for  which  ,u n  p*,u2 + =  ~  and  p+,u  n  ,u2*  =  ~ .  

Then  G is  a  Colmerauer  grammar. 

Proof.  We  define  Colmerauer  precedence  relations  as  follows" 

498  LIMITED BACKTRACK PARSING ALGORITHMS  CHAP. 6 

(1)  X  "  Y if and  only  if X/2 Y. 
(2)  X  <  Y if and  only  if X/22+Y or  X  =  $,  Y :-#  $. 
(3)  X  3>  Y if and  only if X  c  $  and  Y =  $,  or  X p+/22"  Y but  X/2~ +  Y 
is  false. 

It  is  easy  to  show  that  these  relations  are  disjoint.  If  "  n  <  ~  ;2  or 
- : - n   .>  :c= ;2,  then  /2 n  p*/22 + :c= ;2  or  /2 n  p+/2 =  ;2,  in  which  case 

/22* n  p+/2 :c=  ;2.  If  <  ~  .>  c  ;2,  then  X/22 +  Y.  Also,  X/22 +  Y  is  false, 
an  obvious  impossibility. 

Suppose  that  T,  the  induced  two-stack  parser,  has  rule  ( YX ~ . . .  Xk, Z) 
--~(Y,  AZ).  Then  Y < X ~ ,   so  Y = $   or  Y/22+X~.  Also,  X~  '  X~+~ for 

1  <  i  <  k, so Xt/2 Xt+ ~. Finally, X~  .>  Z,  so Z  =  $ or X~ p+/22" Z.  Ignoring 
the  case  Y =  $  or  Z  =  $  for  the  moment,  Lemma  6.11  assures  us  that  if 
the  string  on  the  two  stacks  is  a  sentential  form  of  G,  then  X t . . .   Xk  is 
a  phrase  thereof.  The  cases  Y =  $  or  Z  =  $  are  easily  treated,  and  we  can 
conclude  that  every  reduction  performed  by  T  on  a  sentential  form  yields 
a  sentential  form. 

It  thus  suffices  to  show  that  when  started  with  w in L(G), T  will  continue 
to  perform  reductions  until  it  reduces  to  S. 

By Lemma 6.10,  if X  and  Y are  adjacent  symbols  of any  sentential  form, 
then  X  p*/22*  Y.  Thus  either  X/2  Y,  X  p+/2 Y,  X/2~+  Y,  or  X  p+/22+ Y.  In 
each case, Xa n d   Y are related by one of the Colmerauer precedence relations. 

A  straightforward  induction  on  the  number  of  moves  made  by  T  shows 
that  if  X  and  Y  are  adjacent  symbols  on  stack  I,  then  X  <  Y or  X  "  Y. 

The  argument  is,  essentially,  that  the  only way  X  and  Y could  become  adja- 
cent  is for  Y to  be  shifted  onto  stack  I  when  X  is the  top  symbol.  The  rules 
of T  imply that  X  <  Y or  X  "  Y. 

Since  $ remains  on  stack 2, there  is always some  pair  of adjacent  symbols 
on  stack  2  related  by  3>. Thus,  unless  configuration  ($,  S$,  zr) is  reached  by 
T, it will always shift until the tops  of stack  1 and  2 are related  by  .>. At that 
time,  since  the  <  relation  never  holds  between  adjacent  symbols  on  stack  1, 
a  reduction  is  possible  and  T  proceeds.  E] 

THEOREM  6.6 

A  grammar  is  Colmerauer  if  and  only  if  it  is  unambiguous,  proper, 
uniquely  invertible,  and/2  n  p*/22/  =  p//2  n / 2 2 "   =  ;2. 

Proof  Immediate  from  Lemmas  6.8,  6.9,  and  6.12. 

Example 6.16 

We  saw  in  Example  6.15  that  the  grammar  S  ~  aSAIbSAIb,  A  ~  a 
satisfies  the  desired  conditions.  Lemma  6.12  suggests  that  we  define 
Colmerauer precedence  relations  for this grammar according to Fig. 6.4.  [Z] 

b  s  A  $ 
<.  <.  <.  <. 

<.  <. 

<.  <. 

* _  .>  .> 

*  .>  .> 

S  <-  I  =  .> 
AL->  k  ">  -> 

EXERCISES  499 
Fig.  6.4  Colmerauer  precedence  rela- tions. 
EXERCISES 
6.2.1.  Which of the following are top-down  parses in Go ?  What  word is  derived 

if it  is ? 

(a)  (1, 0)  (3, 2) (5, 4) (2, 0) (4, 2) (2, 5) (4, 0)  (4,  5) (6,  5) (6, 2) (6, 0). 
(b)  (2, 0)  (4, 0)  (5, 0) (6,  1). 

6.2.2.  Give  a  two-stack  parser  valid  for  Go. 
6.2.3.  Which  of the  following  are  Colmerauer  grammars ? (a) 

Go (b) 

S  ~  aA l bB 
A  ~  OA1 I01 

B--+  0BIll011. (c) 

S - - + a A B I b  
A  ~  bSBla 

B -- ~ a .  

*6.2.4.  Show  that  if  G  is  proper,  uniquely  invertible,  /z  n  p*/tA + =  ~ ,   and 

p+,u  n / z ~ *   =  ~ ,   then  G  is  unambiguous.  Can  you  use  this  result  to 
strengthen  Theorem  6.6 ? 

6.2.5.  Show  that  every  uniquely  invertible  regular  grammar  is  a  Colmerauer 

grammar. 

6.2.6.  Show  that  every  uniquely  invertible  grammar  in  G NF   such  that 

p+,u  n / .t  =  ~  is  a  Colmerauer  grammar. 

6.2.7.  Show  that  the  two-stack  parser  of Example  6.12  is  valid. 
6.2.8.  Show,  using  Theorem  6.6,  that  every  simple  precedence  grammar  is  a 

Colmerauer  grammar. 

6.2.9.  Prove  Lemma  6.9. 
6.2.10.  Prove  Lemma  6.10. 
6.2.11.  Let  G  be  a  Colmerauer  grammar  with  Colmerauer  precedence  relations 

< ,   -~,  and  .>  such  that  the  induced  two-stack  parser  not  only  parses 
every  word  in  L(G),  but  correctly  parses  every  sentential  form  of  G. 

Show  that 

500  LIMITED BACKTRACK PARSING  ALGORITHMS  CHAP.  6 

(a)  /t  ~  ~tt. 
(b)  .u,,1, +  ~  <.  
(c)  p+.u  ~  .>. 
(d)  p+.u2 +  ~  <  u  .>. 

"6.2.12.  Let  G  be  a  Colmerauer  grammar,  and  let  o  be  any  subset  of 

p/,u2 / -- p//,t  -  ,u2 +.  Show  that  the  relations  --~ =  ,u,  M  =  p*,u2 / --  o, 
and  .>  =  p//t  u  o  are  Colmerauer  percedence  relations  capable  of 
parsing  any  sentential  form  of G. 

6.2.13.  Show  that  every  two-stack  parser  operates  in  time  0(n)  on  strings  of 

length n. 

"6.2.14.  Show  that  if L  has  a  Colmerauer  grammar,  then  L R has  a  Colmerauer 

grammar. 

"6.2.15.  Is  every  (1,  1)-BRC  grammar  a  Colmerauer  grammar? 
"6.2.16.  Show  that  there  exists  a  Colmerauer  language L  such  that  neither L  nor 

L R is a  deterministic  CFL.  Note  that  the language  [wba" 11 w i =  n~}, which 

we  have  been  using  as  an  example,  is  not  deterministic  but  that  its 
reverse is. 

* 6.2.17.  We  say  that  a  two-stack  parser  T recognizes the  domain  of  'r(T)  regard- 

less  of whether  a  parse  emitted  has  any  relation  to  the  input.  Show  that 

every  recursively  enumerable  language  is  recognized  by  some  determin- 
istic  two-stack  parser.  Hint:  It  helps  to  make  the  underlying  grammar 
ambiguous. 

Open Problem 

6.2.18.  Characterize  the  class  of CFG's,  unambiguous  or  not,  which  have  valid 

deterministic  two-stack  parsers  induced  by  disjoint  "precedence"  rela- 
tions. 

BI BLI OGRAPHI C  NOTES 
Colmerauer  grammars  and  Theorem  6.6  were first  given  by  Colmerauer  [1970]. 
These  ideas  were  related  to  the  token  set  concept  by  Gray  and  Harrison  [1969]. 
Cohen  and  Culik  [1971]  consider  an  LR(k)-based  scheme  which  effectively  incor- 
porates  backtrack. 

tThis  is a  special case  of Lemma  6.7 and  is included  only for  completeness. 

APP ENDI X 

The  Appendix  contains  the  syntactic  descriptions  of four  programming languages: 

(1)  A  simple  base  language  for  an  extensible  language. 
(2)  SNOBOL4,  a  string-manipulating  language. 
(3)  PL360,  a  high-level  machine  language  for  the  IBM  360  computers. 
(4)  PAL,  a  language  combining  lambda  calculus  with  assignment  state- ments. 

These languages were chosen for their diversity.  In addition,  the syntactic 
descriptions  of  these  languages  are  small  enough  to  be  used  in  some  of 
the  programming  exercises  throughout  this  book  without  consuming  an 
excessive  amount  of  time  (both  human  and  computer).  At  the  same  time 
the  languages  are  quite  sophisticated  and  will  provide  a  flavor  of the  prob- 
lems incurred in implementing  the  more  traditional  programming  languages 

such  as  ALGOL,  FORTRAN,  and  PL/I.  Syntactic  descriptions  of the  latter 
languages  can be found  in  the  following  references: 

(1)  ALGOL  60 in  Naur  [1963]. 
(2)  ALGOL  68 in  Van  Wijngaarden  [1969]. 
(3)  FORTRAN  in  [ANS  X3.9,  1966] (Also  see  ANSI-X3J3.) 

(4)  PL/I  in  the  IBM  Vienna  Laboratory  Technical  Report  TR  25.096. 

A.1.  SYNTAX  FOR  AN  EXTENSIBLE  BASE 

LANGUAGE 

The following language was proposed by Leavenwortht as a base language 
which can  be extended  by the  use of syntax macros.  We  shall give the  syntax 

?B. M.  Leavenworth, "Syntax Macros and  Extended Translation"  Comm.  A CM  9, No.  11  (November 1966), 790-793. Copyright (C)  1966, Association for  Computing 
Machinery, Inc. The syntax of the base language is reprinted here by permission of the Association for Computing Machinery. 

501 

502  APPENDIX 

of this language in two parts.  The  first part  consists of the high-level produc- 
tions  which  define  the  base  language.  This  base  language  can  be  used  as 

a  block-structured  algebraic  language  by  itself. 

The  second part  of the description is the  set of productions which defines 
the  extension  mechanism.  The  extension  mechanism  allows  new  forms  of 
statements  and  functions  to  be  declared  by  means of a  syntax  macro  defini- 
tion  statement  using  production  37.  This  production  states  that  an  instance 

of a  (statement)  can  be  a  (syntax  macro  definition),  which,  by productions 
39 and 40, can be either a (statement  macro definition)  or a (function  macro 
definition). In  productions  41  and  42  we  see  that  each  of  these  macro  definitions 

involves  a  (macro  structure)  and  a  (definition).  The  (macro  structure) 
portion  defines the  form  of the  new  syntactic  construct,  and  the  (definition) 

portion  gives  the  translation  that  is  to  be  associated  with  the  new  syntactic 

construct.  Both  the  (macro  structure)  and  (definition)  can  be  any  string  of 
nonterminal  and  terminal  symbols  except  that  each  nonterminal  in  the 

(definition)  portion  must  appear  in  the  (macro  structure).  (This  is  similar 
to  a  rule  in  an  SDTS  except  that  here  there  is  no  restriction  on  how  many 
times  one  n0nterminal  can  be  used  in  the  translation  element.) 

We  have  not  given  the  explicit  rules  for  (macro  structure)  and (defini- 
tion).  In  fact,  the  specification  that  each  nonterminal  in  the  (definition) 
portion appear in the (macro  structure) portion  of a syntax macro  definition 

cannot  be  specified  by  context-free  productions. 

Production  37  indicates  that  we  can  use  any  instance  of a  (macro  struc- 
ture)  defined  in  a  statement  macro  definition  wherever  (sentence)  appears 

in  a  sentential  form.  Likewise,  production  43  allows  us  to  use  any  instance 

of  a  (macro  structure)  defined  in  a  function  macro  definition  anywhere 
(primary)  appears  in  a  sentential  form. 

For  example,  we can  define a  sum  statement  by the  derivation 

(statement)  ~  (syntax  macro  definition) 

(statement  macro  definition) 

smacro  (macro  structure)  define  (definition)  endmacro 

A  possible  (macro  structure)  is  the  following" 

(2) sum (expression) ~1~ with (variable)  ~-- (expression)  to  (expression) C3~ 

We  can  define  a  translation  for  this  macro  structure  by  expanding  (defini- 
tion)  as 

begin local  t;  local  s;  local  r; 

t~  0; 

(2) (variable)  ~  (expression)  ; 

A.1  SYNTAX FOR AN  EXTENSIBLE  BASE LANGUAGE  503 

r"  if (variable)  >  (expression) ~3~ then  goto  s; 

t  <  t  +  (expression)~l~; 
(variable)  ,--  (variable)  +  1, 
goto  r; 
s"  result  t 
end 

Then  if we  write  the  statement 

sum  a  with  b  <  c to  d 
this  would  first  be  translated  into 

begin  local  t;  local  s;  local  r; 

t<  -0; 
b<  :c; 
r'  if  b  >  d  then  goto  s; 

t ~ - - - t . +   a; 
b<  b + l ;  
goto  r; 
s:  result  t 
end 

before  being  parsed  according  to  the  high-level productions. 

Finally,  the  nonterminals  (identifier),  (label),  and  (constant)are  lexical items  which  we  shall  leave  unspecified.  The  reader  is  invited  to  insert  his 

favorite  definitions  of these items or to treat  them as terminalsymbols. 

High-Level  Productions 

1  (program) 

(block) 2  (block)--~ 

begin  (opt  local  ids)  (statement  list)  end 3  (opt  local ids)--~ 
(opt  local  ids)  local  (identifier);  [ e 5  (statement  list) 
(statement)  ] (statement  list);  (statement) 7  (statement)  --~ 
(variable)  ~-- (expression) [ goto (identifier)[ 

if (expression)  then  (statement) [ (block) [ result  (expression) 1 (label):  (statement) 

504  APPENDIX 

13  (expression)  --~ (arithmetic  expression) (relation  op) (arithmetic  expression) ! 

(arithmetic  expression) 15  (arithmetic  expression)  --~ 

(arithmetic  expression) (add  op) (term) [ (term) 17  <term> --~ 
<term) <mult  op) (primary) [ <primary) 19  (primary)  --~ 

(variable) I (constant) [ ((expression)) I (block) 23  (variable) 

(identifier) t (identifier) ((expression  list)) 25  (expression  list) 
(expression  list),  (expression) I (expression) 27  (relation  op)--~ 

<1_<1=I~1>!>_ 33  (add  op) 
+ ! -  
35  (mult  op) ,1/ 

Extension  Mechanism 
37  (statement> (syntax  macro  definition>l(macro  structure) 
39  <syntax macro  definition> ---~ (statement  macro  definition) I (function  macro  definition) 
41  <statement macro  definition> smacro <macro  structure>  define (definition>  endmacro 

42  (function  macro  definition) fmacro <macro  structure>  define (definition)  endmacro 
43  (primary>--~ (macro  structure) 

Notes 

1.  (macro  structure)  and  (definition)  can  be  any  string  of nonterminal or  terminal  symbols.  However,  any  nonterminal  used  in  <definition>  must 

also  appear  in the  corresponding  <macro  structure>. 2.  <constant>,  (identifier>,  and  (label>  are  lexical  variables  which  have 
not  been  defined here. 

A.2  SYNTAX OF SNOBOL4 STATEMENTS  505 
A.2.  SYNTAX  OF  SNOBOL4  STATEMENTS 

Here  we  shall  define  the  syntactic  structure  of  SNOBOL4  statements  as described  by  Griswold  et  al.t  The  syntactic  description  is  in  two  parts.  The 
first part  contains  the context-free productions  describing the syntax in terms of  lexical  variables  which  are  described  in  the  second  part  using  regular 
definitions  of Chapter  3. The division  between  the  syntactic  and  lexical parts is  arbitrary  here,  and  the  syntactic  description  does  not  reflect  the  relative 
precedence  or  associativity  of  the  operators.  All  operators  associate  from left-to-right  except  --1,  !,  and  **.  The  precedence  of  the  operators  is  as 
follows: 

1.  &  2.  I  3.  <blanks> 4 . @   5 . + - -   6 . @  

7 . /   8 . ,   9 . %  10.  !**  11.  $.  12.--t  ? 

High-Level  Productions 

1  (statement>--~ (assignment  statement> i (matching  statement>! 

(replacement  statement>l(degenerate  statement>l<end  statement> 6  (assignment  statement> 
(optional  label> <subject  field> (equal> (object  field> (goto  field> <eos> 
7  (matching  statement> <optional  label> (subject  field> <pattern  field> <goto  field> (eos> 
8  (replacement  statement>  --~ <optional  label> <subject  field> <pattern  field> <equal> <object 

field> (goto  field> (eos> 9  (degenerate  statement>  ---~ 
(optional  label> <subject  field> (goto  field> <eos> I (optional  label> (goto  field> (eos> 
11  (end  statement>  --~ END  <eos> l END  <blanks> <label> (eos> I 

END  <blanks>  END  (eos> 14  <optional  label> 
(label> [ e 16  <subject  field> --, 
(blanks>  (element> 
tR. E.  Griswold,  J. F.  Poage,  and  I. P.  Polonsky,  The SNOBOL4  Programming  Lan- guage  (2nd ed.)  (Englewood  Cliffs,  N.J.:  Prentice-Hall,  Inc.,  1971) pp.  198-199. 

506  APPENDIX 

17  (equal)  ---~ (blanks)  = 
18  (object  field) (blanks)  (expression) 
19  (goto  field) (blanks)  : (optional  blanks)(basic  goto)l e 
21  (basic  goto)----~ (goto) ! S (goto) (optional  blanks) (optional  F  goto) I 

F (goto)(optional  blanks)(optional  S goto) 24  (goto)---~ 

((expression)) [ <  (expression)  > 26  (optional  S  goto)  ---~ 
S (goto) I e 28  (optional  F  goto)  --~ 
F(goto)le 30  (eos) 
(optional  blanks~ ;i (optional  blanks~ (eol~ 32  (pattern  field~ 

(blanks)  (expression~ 33  (element) 
(optional  unaries~ (basic  element~ 34  (optional  unaries~---~ 

(operator)  (optional  unaries) 1 e 36  ~basic element~ 
~identifier~ [ ~literal~ i (function  call) I (reference~ I ((expressionS) 41  (function  call~ ---~ 
(identifier~ ((arg  list~) 42  (reference~ 
(identifier)  <  (arg  list)  > 43  (arg  list~ ---~ 

(arg  list),  (expression~ I (expression) 45  (expression~----~ 
(optional  blanks~ (element) (optional  blanks) [ (optional  blanks~ (operation~ (optional  blanks) 1 
(optional  blanks~ 48  (optional  blanks~ 

(blanks)! e 50  (operation~ 
(element) (binary)  (element) l (element) (binary> (expression~ 
Regular  Definitions  for  Lexical  Syntax 

(digit~  = 0111213141516171819 

A.3  SYNTAX fOR PL360  507 

(letter)  = AtBICI...IZ 
(alphanumeric)  = 

(letter)  l (digit) 
(identifier)  = 

(letter)  ((alphanumeric)  i.  I  )* (blanks)  = 

(blank  character) + 
(integer)  = 

(digit) + 
(real)  = 

(integer).  (integer)  i (integer). (operator)  = 

~l?l$1.ltt%l,I/l#1+l-l@llt& (binary)  = 
(blanks 5 [(blanks)  (operator)  (blanks51 (blanks ~ ** (blanks) (sliteral 5  = 

' ((EBCDIC  charactery  --  ')*  't 
(dliteral)  = 

"( ( E B C DI C   character)  --  ")*  " (literal)  = 

(sliteral)  [ (dliteral)  [ (integer)  I (real) (label)  = 
(alphanumeric)  ((EBCDIC  character)  --  ((blank  character)  1;) )* --  END 

Lexical  Variables 

(blank  character) 
( EBCDI C  character) (eol~:l: 

A.3.  SYNTAX  FOR  PL360 

This  section  contains  the  syntactic  description  of  PL360,  a  high-level 
machine  language  devised  by  Nicklaus  Wirth  for  the  IBM  360  computers. 
The  syntactic  description  is  the  precedence  grammar  given  by Wirth  [1968].$ 

tThe minus sign is a metasymbol here and on the following lines. :l:end of line. 

$Niklaus Wirth,  "PL360, a Programming  Language for the 360 Computers"  J. ACM 15,  No.  1 (January,  1968), 37-74. Copyright  (C)  1968, Association for  Computing  Ma- 
chinery,  Inc.  The  syntax  is  reprinted  by  permission of the  Association for  Computing Machinery. 

508  APPENDIX 

High-Level Productions 

1  (register)--~ (identifier) 
2  (cell  identifier)  ----~ (identifier) 

3  (procedure  identifier)----~ (identifier) 
4  (function  identifier) (identifier) 

5  (cell)---~ (cell identifier) ] (celll))  l (cell2)) 

8  (celll)  --, (cell2) (arith  op) (number)! (cell3) (number) 
lO  (cell2) (cell3) (register) 

11  (cell3) (cell identifier)  ( 

12  (unary  op)--~ abs [ neg t neg  abs 

15  (arith  opt + i - I , i / ! + + i - -  
21  (logical  op)  ---~ and iorlxor 

24  (shift  op)---~ shla i shra I shll [ shrl 

28  (register  assignment)  --~ (register)  :=  (cell) 1 

(register)  :=  (number)l (register)  :=  (string)[ 

(register)  :=  (register) i (register)  "=  (unary 